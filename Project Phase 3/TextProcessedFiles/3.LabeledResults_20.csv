QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
52827853,Type double does not exist in PostgreSQL,"I have table like this:
CREATE TABLE workingtime_times
(
  workingno serial NOT NULL,
  checktime character(6) NOT NULL,
  timeoffset double precision DEFAULT 9
)
I create function like this:
CREATE OR REPLACE FUNCTION MyFName()
    RETURNS TABLE(
        CheckTime character varying,
        TimeOffset double 
    ) AS
$BODY$
BEGIN 
    RETURN QUERY 
    SELECT  t.CheckTime, t.TimeOffset
    FROM WorkingTime_Times t
    ORDER BY t.WorkingNo DESC
    limit 1;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION MyFName()
  OWNER TO postgres;
It make an error like this: 
  type double does not exist
Why we can create an table with column datatype double but return in the function fail. What type we can return in this case?
",<postgresql>,757,0,25,2914,3,19,43,75,16874,,417,1,13,2018-10-16 4:11,2018-10-16 4:21,2018-10-16 4:21,0.0,0.0,Intermediate,15
57078064,How to POST relation in Strapi,"I'm trying to do a POST to the Strapi API and can't seem to figure out how to attach a 'has and belongs to many' (many to many) relationship.
I've already tried the following body's:
events: [&quot;ID&quot;, &quot;ID&quot;]
name: &quot;name&quot;
&amp;
events: [ID, ID]
name: &quot;name&quot;
Which regarding the docs should be right, I think.
There's no error, I get a '200 OK' response. It adds the record but without the relations.
Event.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;events&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;event&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;artists&quot;: {
      &quot;collection&quot;: &quot;artist&quot;,
      &quot;via&quot;: &quot;events&quot;,
      &quot;dominant&quot;: true
    }
  }
}
Artist.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;artists&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;artist&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;required&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;events&quot;: {
      &quot;collection&quot;: &quot;event&quot;,
      &quot;via&quot;: &quot;artists&quot;
    }
  }
}
I'm using the standard SQLite database, strapi version 3.0.0-beta.13 and tried the request through Postman, HTML &amp; curl.
I would love to know how to attach the relation on POST
Update 23-07:
Did a fresh install of Strapi and now everything is working.
",<javascript><sqlite><strapi>,2083,1,56,213,2,3,12,80,17801,0.0,8,3,13,2019-07-17 14:18,2019-07-18 12:48,,1.0,,Advanced,33
50952467,COLLATE=utf8_unicode_ci getting removed from schema.rb after migrate,"Running rails 5.0.2
The tables in our schema.rb in source control seem to mostly have the format: 
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci"" 
do |t|
Note the COLLATE=utf8_unicode_ci"" at the end.
When I run migrations the generated schemaa.rb is mostly the same but chops off COLLATE=utf8_unicode_ci"" from those lines so it now looks like:
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8"" 
do |t|
Based on other SO posts I've tried two things to fix this
1) in my /etc/mysql/my.cnf, I added:
[mysqld]
character-set-server  = utf8
collation-server = utf8_unicode_ci
2) in my database.yml ive added collation: utf8_general_ci to all of the relevant environments
I then restarted mysql, dropped, created and migrated my db but still the collate line disappears.
Any thoughts on what configuration I need to change to have that bit autogenerated ?
",<mysql><sql><ruby-on-rails><rails-migrations>,974,0,17,953,0,11,27,75,3573,,61,2,13,2018-06-20 15:58,2019-09-24 14:58,,461.0,,Advanced,34
50745313,Check constraint that calls function does not work on update,"I created a constraint that prevents allocations in one table to exceed inventory in another table (see the details in my previous question). But for some reason the constraint works as expected only when I insert new allocations, but on update it does not prevent violating.
Here is my constraint:
([dbo].[fn_AllocationIsValid]([Itemid]) = 1)
And here is the function:
CREATE FUNCTION [dbo].[fn_AllocationIsValid] (@itemId as int)  
RETURNS int  AS  
BEGIN 
DECLARE @isValid bit;
SELECT @isValid = CASE WHEN ISNULL(SUM(Allocation), 0) &lt;= MAX(Inventory) THEN 1 ELSE 0 END
FROM Allocations A 
JOIN Items I ON I.Id = A.ItemId
WHERE I.Id = @itemId
GROUP BY I.Id;
RETURN @isValid;
END
Updated
Here are my tables:
CREATE TABLE [allocations] (
[userID] [bigint] NOT NULL ,
[itemID] [int] NOT NULL ,
[allocation] [bigint] NOT NULL ,
CONSTRAINT [PK_allocations] PRIMARY KEY  CLUSTERED 
(
    [userID],
    [itemID]
)  ON [PRIMARY] ,
CONSTRAINT [FK_allocations_items] FOREIGN KEY 
(
    [itemID]
) REFERENCES [items] (
    [id]
) ON DELETE CASCADE  ON UPDATE CASCADE ,
CONSTRAINT [CK_allocations] CHECK ([dbo].[fn_AllocationIsValid]([Itemid], [Allocation]) = 1)
) ON [PRIMARY]
CREATE TABLE [dbo].[Items](
[Id] [int] NOT NULL,
[Inventory] [int] NOT NULL
) ON [PRIMARY]
GO
INSERT INTO Items (Id, Inventory) VALUES (2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(4340, 2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(5895, 2692, 0)
The following statement execution should fail, but it does not:
update allocations set allocation = 5
where userid = 5895 and itemid = 2692
",<sql-server><constraints>,1610,1,44,5121,11,60,108,36,3505,0.0,244,1,13,2018-06-07 15:42,2018-06-07 18:06,2018-06-07 18:06,0.0,0.0,Advanced,33
59540432,How to Mock postgresql (pg) in node.js using jest,"I am new in node.js. I am writing code in node.js for postgresql using pg and pg-native for serverless app. I need to write unit test for it. I am unable to mock pg client using jest or sinon. 
My actual code is like this
const { Client } = require('pg');
export const getAlerts = async (event, context) =&gt; {
  const client = new Client({
    user: process.env.DB_USER,
    host: process.env.DB_HOST,
    database: process.env.DB_DATABASE,
    password: process.env.DB_PASSWORD,
    port: process.env.PORT
  });
  await client.connect();
  try {
    const result = await client.query(`SELECT * FROM public.alerts;`);
    console.log(result.rows);
    client.end();
    return success({ message: `${result.rowCount} item(s) returned`, data: result.rows, status: true });
  } catch (e) {
    console.error(e.stack);
    client.end();
    return failure({ message: e, status: false });
  }
};
How to mock pg client here?
",<node.js><postgresql><unit-testing><jestjs><sinon>,921,0,30,1145,7,17,44,44,33648,0.0,34,4,13,2019-12-31 6:28,2019-12-31 15:17,2019-12-31 15:17,0.0,0.0,Advanced,32
56977209,Azure Function creating too many connections to PostgreSQL,"I have an Azure Durable Function that interacts with a PostgreSQL database, also hosted in Azure.
The PostgreSQL database has a connection limit of 50, and furthermore, my connection string limits the connection pool size to 40, leaving space for super user / admin connections.
Nonetheless, under some loads I get the error
  53300: remaining connection slots are reserved for non-replication superuser connections
This documentation from Microsoft seemed relevant, but it doesn't seem like I can make a static client, and, as it mentions, 
  because you can still run out of connections, you should optimize connections to the database.
I have this method
private IDbConnection GetConnection()
{
    return new NpgsqlConnection(Environment.GetEnvironmentVariable(""PostgresConnectionString""));
}
and when I want to interact with PostgreSQL I do like this
using (var connection = GetConnection())
{
    connection.Open();
    return await connection.QuerySingleAsync&lt;int&gt;(settings.Query().Insert, settings);
}
So I am creating (and disposing) lots of NpgsqlConnection objects, but according to this, that should be fine because connection pooling is handled behind the scenes. But there may be something about Azure Functions that invalidates this thinking.
I have noticed that I end up with a lot of idle connections (from pgAdmin):
Based on that I've tried fiddling with Npgsql connection parameters like Connection Idle Lifetime, Timeout, and Pooling, but the problem of too many connections seems to persist to one degree or another. Additionally I've tried limiting the number of concurrent orchestrator and activity functions (see this doc), but that seems to partially defeat the purpose of Azure Functions being scalable. It does help - I get fewer of the too many connections error). Presumably If I keep testing it with lower numbers I may even eliminate it, but again, that seems like it defeats the point, and there may be another solution.
How can I use PostgreSQL with Azure Functions without maxing out connections?
",<postgresql><azure><azure-functions><npgsql><azure-durable-functions>,2037,5,13,6482,7,43,95,71,3816,0.0,1337,3,13,2019-07-10 19:10,2019-07-11 7:24,,1.0,,Advanced,33
52963079,"I can't start server PostgreSQL 11: ""pg_ctl: could not start server""","I am in CentOS Linux release 7.5.1804 (Core)
When I login as postgres and run:
bash-4.2$ /usr/pgsql-11/bin/initdb -D /var/lib/pgsql/11/data
The files belonging to this database system will be owned by user ""postgres"".
This user must also own the server process.
The database cluster will be initialized with locale ""en_US.UTF-8"".
The default database encoding has accordingly been set to ""UTF8"".
The default text search configuration will be set to ""english"".
Data page checksums are disabled.
creating directory /var/lib/pgsql/11/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
WARNING: enabling ""trust"" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    /usr/pgsql-11/bin/pg_ctl -D /var/lib/pgsql/11/data -l logfile start
bash-4.2$
then I run
bash-4.2$ /usr/pgsql-11/bin/pg_ctl start -l logfile -D /var/lib/pgsql/11/data
waiting for server to start..../bin/sh: logfile: Permission denied
 stopped waiting
pg_ctl: could not start server
Examine the log output.
bash-4.2$ date
Wed Oct 24 01:50:44 -05 2018
bash-4.2$
I search in GOOGLE for ""waiting for server to start..../bin/sh: logfile: Permission denied"" but this error only happened in MAC and no solutions is displayed...
Also I run
bash-4.2$ postgres --version;postmaster --version;
postgres (PostgreSQL) 11.0
postgres (PostgreSQL) 11.0
bash-4.2$
then I believe PostgreSQL 11 is fine installed, but I can't start server.
I install with this line:
yum install postgresql-jdbc.noarch postgresql-jdbc-javadoc.noarch postgresql-unit11.x86_64 postgresql-unit11-debuginfo.x86_64 postgresql11.x86_64 postgresql11-contrib.x86_64 postgresql11-debuginfo.x86_64 postgresql11-devel.x86_64 postgresql11-docs.x86_64 postgresql11-libs.x86_64 postgresql11-odbc.x86_64 postgresql11-plperl.x86_64 postgresql11-plpython.x86_64 postgresql11-pltcl.x86_64 postgresql11-server.x86_64 postgresql11-tcl.x86_64 postgresql11-test.x86_64
I didn't add [postgresql11-llvmjit.x86_64] because this requires many dependences.
CentOS Linux release 7.5.1804 + PostgreSQL 11 ?
Do I need install aditional software?
",<postgresql><centos7><postgresql-11>,2490,0,43,211,1,3,13,49,46847,0.0,11,4,13,2018-10-24 7:20,2018-10-24 17:07,,0.0,,Advanced,32
50451566,SQL Server In Memory Equivalent,"My team has recently decided to move from EF to Dapper. As such we are moving a lot of the logic that was done in EF into Stored Procedures as part of our SQL Server DB. This means that a lot of the Unit Tests that we have for EF are now Integration Level tests as they involve the DB. I am looking for a way to run these tests using an In-Memory DB so I don't have to stand up a DB externally as part of the tests. I looked into SQLite but without the SP support, it would not be a fair comparison. Are there any other In-Memory DBs that would be similar to SQL Server that can be used for testing?
",<sql-server><entity-framework><integration-testing><dapper>,600,0,0,733,1,8,20,59,7632,0.0,49,1,13,2018-05-21 15:01,2018-05-21 15:50,,0.0,,Advanced,34
51267470,"Unhandled rejection SequelizeDatabaseError: relation ""users"" does not exist","I am getting started with Sequelize.  I am following the documentation they are providing on their website :http://docs.sequelizejs.com/manual/installation/getting-started.html 
const Sequelize = require('sequelize');
const sequelize = new Sequelize('haha', 'postgres', 'postgres', {
  host: 'localhost',
  dialect: 'postgres',
  operatorsAliases: false,
  pool: {
    max: 5,
    min: 0,
    acquire: 30000,
    idle: 10000
  },
  // SQLite only
  storage: 'path/to/database.sqlite'
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
  const User = sequelize.define('user', {
    firstName: {
      type: Sequelize.STRING
    },
    lastName: {
      type: Sequelize.STRING
    }
  });
  // force: true will drop the table if it already exists
  User.sync({force: true}).then(() =&gt; {
    // Table created
    return User.create({
      firstName: 'John',
      lastName: 'Hancock'
    });
  });
Up until here, everything works perfectly. And the table ""user"" is correctly built and populated. (Although I do not understand Sequelize appends an ""s"" automatically to ""user"", any explanation.)
However when I add the following portion of code:
User.findAll().then(users =&gt; {
  console.log(users)
})
I get this error :  
  Unhandled rejection SequelizeDatabaseError: relation ""users"" does not
  exist
So my questions are:  
Why does Sequelize add an ""s"" to user. (I know it makes sense but shouldn't the developer decide that)  
What is causing that error? I followed the documentation but it still didn't work?
",<javascript><node.js><postgresql><sequelize.js><pgadmin>,1675,4,48,1743,6,33,77,47,48448,0.0,1026,8,13,2018-07-10 14:11,2018-09-11 2:08,,63.0,,Advanced,33
55538252,How to create a database backup in DBeaver and restore it?,"I need to create a SQL server database backup in DBeaver and restore it. Is that possible?
Using SQL Management Studio would not be a turnaround solution in this case, as we are not allowed to use it here.
",<sql-server><dbeaver>,206,0,0,1451,3,18,31,57,25277,,281,1,13,2019-04-05 14:58,2021-03-02 10:52,,697.0,,Intermediate,30
52783712,SQL Server Management Studio 2017 Database Diagram Folder Missing,"I have SSMS 2017 installed and I want to use it to create an ERD for some of my databases. The documentation online says you just need to right click on the ""Database Diagrams"" folder under your database in the navigation panel. However, that folder is simply not there for any of my databases. I cannot find any fix or work around. Does anyone have any ideas on how to fix this?
",<sql-server><ssms-2017>,380,0,0,465,1,4,18,56,12434,0.0,18,3,12,2018-10-12 16:38,2019-01-23 18:36,,103.0,,Advanced,32
52910437,Installed mysql@5.6 using brew mysql.server not a command,"I installed mysql@5.6 using brew. Below are the comands I ran
brew install mysql@5.6
sudo chmod -R 777 /usr/local/var/mysql
sudo ln -s /usr/local/Cellar/mysql\@5.6/5.6.41/bin/mysql /usr/local/bin/mysql
sudo cp /usr/local/Cellar/mysql\@5.6/5.6.41/homebrew.mxcl.mysql.plist /Library/LaunchAgents/
sudo chown root /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod 600 /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod +x /Library/LaunchAgents/homebrew.mxcl.mysql.plist
launchctl load -w /Library/LaunchAgents/homebrew.mxcl.mysql.plist
mysql.server start
  sh: mysql.server: command not found
this is the output I am getting.
mysql --version is giving output
  mysql  Ver 14.14 Distrib 5.6.41, for osx10.13 (x86_64) using  EditLine
  wrapper
If I start service via brew its starting
brew services start mysql@5.6
But when I run mysql -uroot I am getting 
  ERROR 2002 (HY000): Can't connect to local MySQL server through socket
  '/tmp/mysql.sock' (2)
",<mysql><macos><homebrew>,966,0,18,987,4,16,48,37,15874,0.0,189,2,12,2018-10-20 21:53,2018-10-20 21:59,2018-10-20 21:59,0.0,0.0,Advanced,34
64478510,Not able to take backup of hypertable TimescaleDB database using pg_dump PostgreSQL,"command used to take backup
C:\Program Files\PostgreSQL\12\bin&gt;pg_dump  -h localhost -U postgres -p 5432  -Fc -f &quot;D:\Database Backup\temp_10.bak&quot; GESEMS_Performace_Test.
Error :
pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied.
DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will
not copy any data.
Any suggestions to take backup of TimescaleDB hypertables?
",<postgresql><timescaledb>,449,1,1,153,1,2,7,68,6159,,2,1,12,2020-10-22 8:43,2020-10-22 10:22,2020-10-22 10:22,0.0,0.0,Advanced,33
53160535,Rails 4.2 Postgres 9.4.4 statement_timeout doesn't work,"I am trying to set a statement_timeout. I tried both setting in database.yml file like this
variables:
  statement_timeout: 1000
And this
ActiveRecord::Base.connection.execute(""SET statement_timeout = 1000"")
Tested with
ActiveRecord::Base.connection.execute(""select pg_sleep(4)"")
And they both don't have any effect.
I am running postgres 10 in my local and the statement_timeouts works just expected. But on my server that is running postgres 9.4.4, it simply doesn't do anything.
I've check Postgres' doc for 9.4 and statement_timeout is available. Anyone can shed some light?
",<ruby-on-rails><postgresql>,579,1,5,2544,0,21,29,72,1160,,705,1,12,2018-11-05 19:00,2020-04-04 11:09,,516.0,,Advanced,33
54515682,How to give ranks to multiple columns data in SQL Server?,"I have input table as shown below -
ID  Name q1     q2      q3      q4
1   a    2621   2036    1890    2300
2   b    18000  13000   14000   15000
3   c    100    200     300     400
I want ranking of columns(q1, q2, q3 and q4) data for each row. For example, if I consider last row of above input, then q4 column contains 400 value which is higher than other columns, so rank to q4 column will be 1, q3 rank will be 2, q2 rank will be 3 and q1 rank will be 4.
I am looking for output like -
id  name  q1  q2  q3  q4
1   a     1   3   4   2
2   b     1   4   3   2
3   c     4   3   2   1
There are more than 100,000 records present in input table.
I have created small SQL script for input table i.e.,
declare @temp table (ID int, Name varchar(10), q1 int, q2 int, q3 int, q4 int)
insert into @temp
select 1, 'a', 2621, 2036, 1890, 2300
union all
select 2, 'b', 18000, 13000, 14000, 15000
union all
select 3, 'c', 100, 200, 300, 400
select * from @temp
Please help me to find efficient way to solve this problem.
",<sql-server><sql-server-2012>,1013,0,29,2014,2,13,22,48,1478,,191,1,12,2019-02-04 12:02,2019-02-04 12:13,2019-02-04 12:13,0.0,0.0,Advanced,33
49702144,How do you properly handle SQL_VARIANT in Entity Framework Core?,"It looks like support has recently been added to Entity Framework Core in .NET Core 2.1 (preview) to allow the mapping of SQL_VARIANT columns (https://github.com/aspnet/EntityFrameworkCore/issues/7043).
It looks like the way to go about doing this is using the new HasConversion() method (https://learn.microsoft.com/en-us/ef/core/modeling/value-conversions).
So, in order to map my SQL_VARIANT column and treat the underlying datatype as being a VARCHAR of any length (I only care about reading it at this point), I can do the following (where the Value property here is of type object in the model):
entity.Property(e =&gt; e.Value).HasConversion(v =&gt; v.ToString(),
                                                            v =&gt; v.ToString());
This works, if the SQL_VARIANT's underlying datatype is a VARCHAR of any length.
However, being a SQL_VARIANT, the column could contain data of other types, such as DATETIME values.
For simplicity I've only specified DateTime and string here, but in theory I'd probably want to support the datatypes necessary to map whatever could be stored in a SQL_VARIANT column, if possible.
How would I go about determining which one of those two types (string and DateTime) I'd want to map to at runtime?  Is there a way to do this?
",<c#><.net-core><entity-framework-core><sql-variant>,1277,4,18,1338,3,21,52,35,5893,0.0,9,1,12,2018-04-06 23:15,2018-04-09 9:20,2018-04-09 9:20,3.0,3.0,Advanced,33
54387084,How to safely reindex primary key on postgres?,"We have a huge table that contains bloat on the primary key index. We constantly archive old records on that table. 
We reindex other columns by recreating the index concurrently and dropping the old one. This is to avoid interfering with production traffic.
But this is not possible for a primary key since there are foreign keys depending on it. At least based on what we have tried.
What's the right way to reindex the primary key safely without blocking DML statements on the table?
",<postgresql>,487,0,0,7418,5,41,78,48,8594,0.0,303,3,12,2019-01-27 10:21,2019-01-28 12:36,2020-07-24 10:30,1.0,544.0,Advanced,34
53133498,flask-sqlalchemy intellisense/autocomplete,"I'm searching for a way to actually get intellisense for flask-sqlalchemy. All the answers I have found online seem to tell us how to suppress the errors (no Column or query instance members)(switch linters, ignore the class in pylintrc, etc...), but I find it very hard to belive that the most widely used ORM in python does not actually have linting support. 
I am open to using pycharm or intellij community editions, but I'd rather stick with vs code, since I am a js developer who recently decided to learn python, and I'd like to stick with the tools I know and love. I hear that vs code has great python support, but sqlalchemy linting is a must!
",<python><sqlalchemy><visual-studio-code><flask-sqlalchemy>,654,0,0,469,0,3,12,75,5166,0.0,7,0,12,2018-11-03 16:53,,,,,Advanced,36
49016650,How to configure Spring boot for work with two databases?,"I am using Spring Boot 2.X with Hibernate 5 to connect two different MySQL databases (Bar and Foo) on different servers. I am trying to list all the information of an entity (own attributes and @OneToMany and @ManyToOne relations) from a method in a REST Controller. 
I have followed several tutorials to do this, thus, I am able to get all the information for my @Primary database (Foo), however, I always get an exception for my secondary database (Bar) when retrieving the @OneToMany sets. If I swap the @Primary annotation to the Bar database, I able to get the data from the Bar database but not for the Foo database . Is there a way to resolve this?
This is the exception I am getting:
...w.s.m.s.DefaultHandlerExceptionResolver :
Failed to write HTTP message: org.springframework.http.converter.HttpMessageNotWritableException: 
    Could not write JSON document: failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-com.foobar.bar.domain.Bar[""manyBars""]); 
    nested exception is com.fasterxml.jackson.databind.JsonMappingException:
        failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-&gt;com.foobar.bar.domain.Bar[""manyBars""])
My application.properties:
# MySQL DB - ""foo""
spring.datasource.url=jdbc:mysql://XXX:3306/foo?currentSchema=public
spring.datasource.username=XXX
spring.datasource.password=XXX
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
# MySQL DB - ""bar""
bar.datasource.url=jdbc:mysql://YYYY:3306/bar?currentSchema=public
bar.datasource.username=YYYY
bar.datasource.password=YYYY
bar.datasource.driver-class-name=com.mysql.jdbc.Driver
# JPA
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
My @Primary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""entityManagerFactory"",
        transactionManagerRef = ""transactionManager"",
        basePackages = {""com.foobar.foo.repo""})
public class FooDbConfig {
    @Primary
    @Bean(name = ""dataSource"")
    @ConfigurationProperties(prefix = ""spring.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Primary
    @Bean(name = ""entityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""dataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.foo.domain"")
                .persistenceUnit(""foo"")
                .build();
    }
    @Primary
    @Bean(name = ""transactionManager"")
    public PlatformTransactionManager transactionManager(
            @Qualifier(""entityManagerFactory"") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
My secondary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""barEntityManagerFactory"",
        transactionManagerRef = ""barTransactionManager"", basePackages = {""com.foobar.bar.repo""})
public class BarDbConfig {
    @Bean(name = ""barDataSource"")
    @ConfigurationProperties(prefix = ""bar.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Bean(name = ""barEntityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean barEntityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""barDataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.bar.domain"")
                .persistenceUnit(""bar"")
                .build();
    }
    @Bean(name = ""barTransactionManager"")
    public PlatformTransactionManager barTransactionManager(
            @Qualifier(""barEntityManagerFactory"") EntityManagerFactory barEntityManagerFactory) {
        return new JpaTransactionManager(barEntityManagerFactory);
    }
}
The REST Controller class:
@RestController
public class FooBarController {
    private final FooRepository fooRepo;
    private final BarRepository barRepo;
    @Autowired
    FooBarController(FooRepository fooRepo, BarRepository barRepo) {
        this.fooRepo = fooRepo;
        this.barRepo = barRepo;
    }
    @RequestMapping(""/foo"")
    public List&lt;Foo&gt; listFoo() {
        return fooRepo.findAll();
    }
    @RequestMapping(""/bar"")
    public List&lt;Bar&gt; listBar() {
        return barRepo.findAll();
    }
    @RequestMapping(""/foobar/{id}"")
    public String fooBar(@PathVariable(""id"") Integer id) {
        Foo foo = fooRepo.findById(id);
        Bar bar = barRepo.findById(id);
        return foo.getName() + "" "" + bar.getName() + ""!"";
    }
}
The Foo/Bar repositories:
@Repository
public interface FooRepository extends JpaRepository&lt;Foo, Long&gt; {
  Foo findById(Integer id);
}
@Repository
public interface BarRepository extends JpaRepository&lt;Bar, Long&gt; {
  Bar findById(Integer id);
}
The entities for the @Primary datasource. The entities of the second datasource are the same (only changing the class names):
@Entity
@Table(name = ""foo"")
public class Foo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @OneToMany(fetch = FetchType.LAZY, mappedBy = ""foo"")
    @JsonIgnoreProperties({""foo""})
    private Set&lt;ManyFoo&gt; manyFoos = new HashSet&lt;&gt;(0);
    // Constructors, Getters, Setters
}
@Entity
@Table(name = ""many_foo"")
public class ManyFoo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @ManyToOne(fetch = FetchType.LAZY)
    @JsonIgnoreProperties({""manyFoos""})
    private Foo foo;
    // Constructors, Getters, Setters
}  
Finally, my application main:
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
It is important to remark that the solution should keep the Lazy property for both databases in order to maintain an optimal performance.
Edit 1: If both catalogs (""databases"" in MySQL terminology) are in same database (""server"") the Rick James solution works!!
The problem remains when catalogs (MySQL databases) are in different databases (servers) and it is tried to keep Lazy  the property
Many thanks.
",<java><mysql><spring><hibernate><spring-boot>,6853,0,171,1280,1,15,43,49,4611,0.0,707,2,12,2018-02-27 19:38,2018-03-07 18:13,,8.0,,Advanced,34
53025787,Mysql Calculate Growth based on Quarters,"I have a database with two tables - companies and reports. I want to calculate the change from q1 (quarter 1) to q2 (quarter 2). I have tried to use the (following) sub-query, but then the main query fails...
FROM
    (SELECT revenue FROM reports WHERE quarter = 'q2' AND fiscal_year = 2018) AS q,
    (SELECT revenue FROM reports WHERE quarter = 'q1' AND fiscal_year = 2017) AS lq
Here is DB Fiddle to help you understand the problem and schema:
https://www.db-fiddle.com/f/eE8SNRojn45h7Rc1rPCEVN/4
Current Simple Query.
SELECT 
    c.name, r.quarter, r.fiscal_year, r.revenue, r.taxes, r.employees
FROM 
    companies c
JOIN
    reports r 
ON
    r.company_id = c.id
WHERE
    c.is_marked = 1;
Expected Results (this is what i need):
+---------+----------+----------------+----------+--------------+-----------+------------------+
|  Name   | Revenue  | Revenue_change |  Taxes   | Taxes_change | Employees | Employees_change |
+---------+----------+----------------+----------+--------------+-----------+------------------+
| ABC INC |    11056 | +54.77         | 35000.86 | -28.57%      |       568 | -32              |
| XYZ INC |     5000 | null           | null     | null         |        10 | +5               |
+---------+----------+----------------+----------+--------------+-----------+------------------+
I would really appreciate your help to build this query. Thanks in advance.
",<mysql><laravel>,1394,2,19,2756,7,44,76,76,1113,0.0,63,6,12,2018-10-27 19:59,2018-10-27 20:56,,0.0,,Intermediate,15
56180225,Keycloak server in docker fails to start in standalone mode?,"
Well, as the title suggests, this is more of an issue record. I was trying to follow the instructions on this README file of Keycloak docker server images, but encountered a few blockers. 
After pulling the image, below command to start a standalone instance failed. 
docker run jboss/keycloak
The error stack trace: 
-b 0.0.0.0
=========================================================================
  Using PostgreSQL database
=========================================================================
...
04:45:06,084 INFO  [io.smallrye.metrics] (MSC service thread 1-5) Converted [2] config entries and added [4] replacements
04:45:06,096 ERROR [org.jboss.as.controller.management-operation] (ServerService Thread Pool -- 33) WFLYCTL0013: Operation (""add"") failed - address: ([
    (""subsystem"" =&gt; ""datasources""),
    (""data-source"" =&gt; ""KeycloakDS"")
]) - failure description: ""WFLYCTL0113: '' is an invalid value for parameter user-name. Values must have a minimum length of 1 characters""
...
Caused by: java.lang.RuntimeException: Failed to connect to database
    at org.keycloak.connections.jpa.DefaultJpaConnectionProviderFactory.getConnection(DefaultJpaConnectionProviderFactory.java:382)
...
Caused by: javax.naming.NameNotFoundException: datasources/KeycloakDS -- service jboss.naming.context.java.jboss.datasources.KeycloakDS
    at org.jboss.as.naming.ServiceBasedNamingStore.lookup(ServiceBasedNamingStore.java:106)
...
I was wondering how it uses a PostgreSQL database, and assumed it might spin up its own instance. But the error looks like it has a problem connecting to the database. 
Changing to the embedded H2 DB made it work. 
docker run -e DB_VENDOR=""h2"" --name docker-keycloak-h2 jboss/keycloak
The docker-entrypoint.sh file shows that it uses below logic to determine what DB to use. 
if (getent hosts postgres &amp;&gt;/dev/null); then
        export DB_VENDOR=""postgres""
...
And further down the flow, this change-database.cli file indicates that it's actually expecting a running PostgreSQL instance to use. 
connection-url=jdbc:postgresql://${env.DB_ADDR:postgres}:${env.DB_PORT:5432}/${env.DB_DATABASE:keycloak}${env.JDBC_PARAMS:}
So I began wondering how PostgreSQL was chosen as a default initially. Executing below commands in a running Keycloak docker container revealed some interesting things. 
[root@71961b81189c bin]# getent hosts postgres
69.172.201.153  postgres.mbox.com
[root@71961b81189c bin]# echo $?
0
Not sure what this postgres.mbox.com is but apparently it's not an expected PostgreSQL server to be resolved by getent. Not sure whether this is a recent linux issue either. The hosts entry in the Name Service Switch Configuration file /etc/nsswitch.conf looks like below inside the container.
hosts:      files dns myhostname
It is the dns data source that resolved postgres to postgres.mbox.com. 
This is why the DB vendor determination logic failed which eventually caused the container failing to start. The instructions on this README file do not work as of the day this post is published. 
Below are the working commands to start a Keycloak server in docker properly with PostgreSQL as the database. 
docker network create keycloak-network
docker run -d --name postgres --net keycloak-network -e POSTGRES_DB=keycloak -e POSTGRES_USER=keycloak -e POSTGRES_PASSWORD=password postgres
docker run --name docker-keycloak-postgres --net keycloak-network -e DB_USER=keycloak -e DB_PASSWORD=password jboss/keycloak
",<postgresql><docker><keycloak>,3468,3,44,2467,1,24,42,81,17819,0.0,499,3,12,2019-05-17 5:36,2019-06-04 9:24,2019-06-04 9:24,18.0,18.0,Advanced,37
53469793,E_WARNING: Error while sending STMT_PREPARE packet. PID=*,"My Laravel 5.7 website has been experiencing a few problems that I think are related to each other (but happen at different times):
PDO::prepare(): MySQL server has gone away
E_WARNING: Error while sending STMT_PREPARE packet. PID=10
PDOException: SQLSTATE[23000]: Integrity constraint violation: 1062 Duplicate entry (My database often seems to try to write the same record twice in the same second. I've been unable to figure out why or how to reproduce it; it doesn't seem to be related to user behavior.)
Somehow, those first 2 types of errors only ever appear in my Rollbar logs but not on the text logs on the server or in my Slack notifications, as all errors are supposed to (and all others do).
For months, I've continued to see scary log messages like these, and I've been completely unable to reproduce these errors (and have been unable to diagnose and solve them).
I haven't yet found any actual symptoms or heard of any complaints from users, but the error messages seem non-trivial, so I really want to understand and fix the root causes.
I've tried changing my MySQL config to use max_allowed_packet=300M (instead of the default of 4M) but still get these exceptions frequently on days when I have more than a couple of visitors to my site.
I've also set (changed from 5M and 10M) the following because of this advice:
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
As further background:
My site has a queue worker that runs jobs (artisan queue:work --sleep=3 --tries=3 --daemon).
There are a bunch of queued jobs that can be scheduled to happen at the same moment based on the signup time of visitors. But the most I see that have happened simultaneously is 20.
There are no entries in the MySQL Slow Query Log.
I have a few cron jobs, but I doubt they're problematic. One runs every minute but is really simple. Another runs every 5 minutes to send certain scheduled emails if any are pending. And another runs every 30 minutes to run a report.
I've run various mysqlslap queries (I'm completely novice though) and haven't found anything slow even when simulating hundreds of concurrent clients.
I'm using Laradock (Docker).
My server is DigitalOcean 1GB RAM, 1 vCPU, 25GB SSD. I've also tried 2GB RAM with no difference.
The results from SHOW VARIABLES; and SHOW GLOBAL STATUS; are here.
My my.cnf is:
[mysql]
[mysqld]
sql-mode=&quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&quot;
character-set-server=utf8
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
max_allowed_packet=300M
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow_query_log.log
long_query_time = 10
log_queries_not_using_indexes = 0
Any ideas about what I should explore to diagnose and fix these problems? Thanks.
",<mysql><laravel><performance><laravel-5><laradock>,2793,3,23,22262,31,180,366,46,3476,0.0,13224,4,12,2018-11-25 17:02,2018-12-10 6:09,,15.0,,Advanced,33
59578211,MySQL to return records with a date/time of now minus 1 hour?,"can anybody help with a MySQL command to try and select all records within a table with a date/time equal to or more than now - 1hour?
Now I'm not 100% sure that that is the best way of describing this.
I basically have records with a date/time field (e.g. 2019-07-13 13:00:00) and I want to perform a MySQL select to find all records with a date/time of one hour ago. This is to trigger a function one hour after an event.
I currently have this, but not sure if it is along the right lines at all:
SELECT * FROM database.table_name
WHERE (NOW() - INTERVAL 1 HOUR) &gt;= 'date_of_event'
AND 'status' = 'Scheduled';
Any thoughts would be great!
",<mysql><datetime>,644,0,3,2813,8,29,32,38,15524,0.0,37,2,12,2020-01-03 11:56,2020-01-03 12:08,2020-01-03 12:09,0.0,0.0,Basic,10
52284555,FOR JSON PATH vs FOR JSON AUTO SQL Server,"I'm having an issue creating nested JSON in SQL Server.  I'm trying to create an output that looks like this:
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
        ""address1"": ""1 Oak Grove"",
        ""address2"": ""London"",
        ""address3"": ""UK""
    },
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
I've tried both FOR JSON AUTO and FOR JSON PATH:
SELECT
  d.SCHOOL_WEBSITE AS websiteURL
  ,d.SCHOOL_EMAIL AS email 
 ,d.SCHOOL_TELEPHONE AS phone
 ,d.[Address 1] AS 'address.address1'
 ,d.[Address 2] AS 'address.address2'
 ,d.[Address 3] AS 'address.address3'
 ,accreditations.[IndiUID] as name   
 ,accreditations.Value as value 
 FROM [TESTDB].[dbo].[DataValues] as d,[TESTDB].[dbo].[accreditations] as accreditations
 WHERE d.Code = accreditations.SchoolCode
 FOR JSON AUTO --PATH
FOR JSON AUTO creates this (address section is not nested (but accredidation is):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address.address1"": ""1 Oak Grove"",
    ""address.address2"": ""London"",
    ""address.address3"": ""UK"",
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
FOR JSON PATH creates this (address section is nested, but accreditation is not - the whole block repeats):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator1"",
    ""value"": ""True""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator2"",
    ""value"": ""False""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator3"",
    ""value"": ""False""
    }
]
I think the key to it is some sort of FOR JSON sub query around the accreditations but I haven't had any success with this.
Create sample data with the following:
    /****** Object:  Table [dbo].[accreditations]    Script Date: 11/09/2018 22:29:43 ******/
CREATE TABLE [dbo].[accreditations](
    [SchoolCode] [nvarchar](255) NULL,
    [IndiUID] [nvarchar](255) NULL,
    [Value] [nvarchar](255) NULL
) ON [PRIMARY]
GO
/****** Object:  Table [dbo].[DataValues]    Script Date: 11/09/2018 22:29:44 ******/
CREATE TABLE [dbo].[DataValues](
    [Code] [nvarchar](255) NULL,
    [SCHOOL_NAME_FORMAL] [nvarchar](255) NULL,
    [SCHOOL_WEBSITE] [nvarchar](255) NULL,
    [SCHOOL_EMAIL] [nvarchar](255) NULL,
    [SCHOOL_TELEPHONE] [float] NULL,
    [Address 1] [nvarchar](255) NULL,
    [Address 2] [nvarchar](255) NULL,
    [Address 3] [nvarchar](255) NULL
) ON [PRIMARY]
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator2', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator3', N'True')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'ABC', N'test', N'www.test.edu', N'hello@test.edu', 123456798, N'1 Oak Grove', N'London', N'UK')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'DEF', N'something', N'https://something.edu/fulltime', N'hello@something.edu', 987654321, N'23 Tree Road', N'Paris', N'France')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'GHI', N'university', N'http://www.university.ac.uk/', N'hello@university.ac.uk/', 123123123, N'57 Bonsai Lane', N'London', N'UK')
GO
",<json><sql-server><for-json>,5145,4,143,125,1,1,6,47,17562,,1,1,12,2018-09-11 21:39,2018-09-11 21:48,2018-09-11 21:48,0.0,0.0,Intermediate,23
51810460,Is proper event-time sessionization possible with Spark Structured Streaming?,"Been playing around Spark Structured Streaming and mapGroupsWithState (specifically following the StructuredSessionization example in the Spark source). I want to confirm some limitations I believe exist with mapGroupsWithState given my use case.
A session for my purposes is a group of uninterrupted activity for a user such that no two chronologically ordered (by event time, not processing time) events are separated by more than some developer-defined duration (30 minutes is common).
An example will help before jumping into code:
{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}
For the stream above, a session is defined with a 30 minute period of inactivity. In a streaming context, we should end up with one session (the second has yet to complete):
[
  {
    ""user_id"": ""mike"",
    ""startTimestamp"": ""2018-01-01T00:00:00"",
    ""endTimestamp"": ""2018-01-01T00:05:00""
  }
]
Now consider the following Spark driver program:
import java.sql.Timestamp
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.execution.streaming.MemoryStream
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout}
object StructuredSessionizationV2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master(""local[2]"")
      .appName(""StructredSessionizationRedux"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""WARN"")
    import spark.implicits._
    implicit val ctx = spark.sqlContext
    val input = MemoryStream[String]
    val EVENT_SCHEMA = new StructType()
      .add($""event_time"".string)
      .add($""user_id"".string)
    val events = input.toDS()
      .select(from_json($""value"", EVENT_SCHEMA).alias(""json""))
      .select($""json.*"")
      .withColumn(""event_time"", to_timestamp($""event_time""))
      .withWatermark(""event_time"", ""1 hours"")
    events.printSchema()
    val sessionized = events
      .groupByKey(row =&gt; row.getAs[String](""user_id""))
      .mapGroupsWithState[SessionState, SessionOutput](GroupStateTimeout.EventTimeTimeout) {
      case (userId: String, events: Iterator[Row], state: GroupState[SessionState]) =&gt;
        println(s""state update for user ${userId} (current watermark: ${new Timestamp(state.getCurrentWatermarkMs())})"")
        if (state.hasTimedOut) {
          println(s""User ${userId} has timed out, sending final output."")
          val finalOutput = SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = true
          )
          // Drop this user's state
          state.remove()
          finalOutput
        } else {
          val timestamps = events.map(_.getAs[Timestamp](""event_time"").getTime).toSeq
          println(s""User ${userId} has new events (min: ${new Timestamp(timestamps.min)}, max: ${new Timestamp(timestamps.max)})."")
          val newState = if (state.exists) {
            println(s""User ${userId} has existing state."")
            val oldState = state.get
            SessionState(
              startTimestampMs = math.min(oldState.startTimestampMs, timestamps.min),
              endTimestampMs = math.max(oldState.endTimestampMs, timestamps.max)
            )
          } else {
            println(s""User ${userId} has no existing state."")
            SessionState(
              startTimestampMs = timestamps.min,
              endTimestampMs = timestamps.max
            )
          }
          state.update(newState)
          state.setTimeoutTimestamp(newState.endTimestampMs, ""30 minutes"")
          println(s""User ${userId} state updated. Timeout now set to ${new Timestamp(newState.endTimestampMs + (30 * 60 * 1000))}"")
          SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = false
          )
        }
      }
    val eventsQuery = sessionized
      .writeStream
      .queryName(""events"")
      .outputMode(""update"")
      .format(""console"")
      .start()
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}""""""
    )
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}""""""
    )
    eventsQuery.processAllAvailable()
  }
  case class SessionState(startTimestampMs: Long, endTimestampMs: Long) {
    def durationMs: Long = endTimestampMs - startTimestampMs
  }
  case class SessionOutput(userId: String, startTimestampMs: Long, endTimestampMs: Long, durationMs: Long, expired: Boolean)
}
Output of that program is:
root
 |-- event_time: timestamp (nullable = true)
 |-- user_id: string (nullable = true)
state update for user mike (current watermark: 1969-12-31 19:00:00.0)
User mike has new events (min: 2018-01-01 00:00:00.0, max: 2018-01-01 00:05:00.0).
User mike has no existing state.
User mike state updated. Timeout now set to 2018-01-01 00:35:00.0
-------------------------------------------
Batch: 0
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514783100000|    300000|  false|
+------+----------------+--------------+----------+-------+
state update for user mike (current watermark: 2017-12-31 23:05:00.0)
User mike has new events (min: 2018-01-01 00:45:00.0, max: 2018-01-01 00:45:00.0).
User mike has existing state.
User mike state updated. Timeout now set to 2018-01-01 01:15:00.0
-------------------------------------------
Batch: 1
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514785500000|   2700000|  false|
+------+----------------+--------------+----------+-------+
Given my session definition, the single event in the second batch should trigger an expiry of session state and thus a new session. However, since the watermark (2017-12-31 23:05:00.0) has not passed the state's timeout (2018-01-01 00:35:00.0), state isn't expired and the event is erroneously added to the existing session despite the fact that more than 30 minutes have passed since the latest timestamp in the previous batch.
I think the only way for session state expiration to work as I'm hoping is if enough events from different users were received within the batch to advance the watermark past the state timeout for mike. 
I suppose one could also mess with the stream's watermark, but I can't think of how I'd do that to accomplish my use case.
Is this accurate? Am I missing anything in how to properly do event time-based sessionization in Spark?
",<apache-spark><apache-spark-sql><spark-structured-streaming>,7348,1,149,3780,3,24,31,54,2295,0.0,43,3,12,2018-08-12 15:56,2018-08-29 18:14,,17.0,,Intermediate,23
52864853,How to get MySql 8 to run with laravel?,"I'm having difficulty getting MySQL 8 to work. This is the error that appears everytime I attempt a php artisan migrate. I've reinstalled MySQL only once so far because I didn't want to hurt my head anymore on what was going on. I've edited the database.php from other possible answers, but that also doesn't seem to work. I saw a possible answer that it's because of MySQL 8's sha256 encryption of the root password, which is why I want to go back to MySQL 5.7 which I've looked up works with laravel just fine. Though, I want to keep the packages up to date and keep MySQL 8 only if i can get it to work with laravel.
PHP 7.2
How do I get MySQL 8 to work with Laravel?
 'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8',
            'collation' =&gt; 'utf8_unicode_ci',
            'prefix' =&gt; '',
            'prefix_indexes' =&gt; true,
            'strict' =&gt; true,
            'engine' =&gt; null,
            'version' =&gt; 8,
            'modes' =&gt; [
                'ONLY_FULL_GROUP_BY',
                'STRICT_TRANS_TABLES',
                'NO_ZERO_IN_DATE',
                'NO_ZERO_DATE',
                'ERROR_FOR_DIVISION_BY_ZERO',
                'NO_ENGINE_SUBSTITUTION',
            ],
        ],
``
Illuminate\Database\QueryException  : SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from information_schema.tables where table_schema = laravel_tut and table_name = migrations)
  at /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|
  Exception trace:
  1   PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
  2   PDO::__construct(""mysql:host=127.0.0.1;port=3306;dbname=laravel_tut"", ""root"", ""fdgkadgaf9g7ayaig9fgy9ad8fgu9adfg9adg"", [])
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
UPDATE ANOTHER FIX I DID TO FIX THIS:
With a fresh install of MySQL, i selected NO for encrypting passwords in the set up ( using legacy encryption, not the SHA encryption ) and it started to work with Laravel without any problems - Just use a long and strong password.
reference of the installation step:
https://www.percona.com/blog/wp-content/uploads/2018/05/Installing-MySQL-8.0-on-Ubuntu-2.png
",<php><mysql><laravel><ubuntu><server>,3347,2,46,1747,3,27,52,76,29783,0.0,307,2,12,2018-10-17 23:17,2018-10-18 0:26,2018-10-18 0:26,1.0,1.0,Intermediate,21
49074070,How to make sure my DataFrame frees its memory?,"I have a Spark/Scala job in which I do this:
1: Compute a big DataFrame df1 + cache it into memory
2: Use df1 to compute dfA
3: Read raw data into df2 (again, its big) + cache it
When performing (3), I do no longer need df1. I want to make sure its space gets freed. I cached at (1) because this DataFrame gets used in (2) and its the only way to make sure I do not recompute it each time but only once.
I need to free its space and make sure it gets freed. What are my options?
I thought of these, but it doesn't seem to be sufficient:
df=null
df.unpersist()
Can you document your answer with a proper Spark documentation link?
",<scala><apache-spark><garbage-collection><apache-spark-sql>,629,0,10,1500,1,19,31,77,17492,0.0,419,3,12,2018-03-02 17:11,2018-03-02 17:20,2018-03-02 20:56,0.0,0.0,Intermediate,24
55324144,Entity Framework Core SQLite Connection String Keyword not supported: version,"I created a ASP.NET MVC website using .NET Core 2.2 using a SQLite database. So far it's working well. Trouble begins when I want to add SQLite-specific keywords to the connection string, such as
Data Source=~\\App_Data\\MyDb.db; Version=3; DateTimeFormat=UnixEpoch; DateTimeKind=Utc
Now I get
  Keyword not supported: 'version'
I register the database like this
// ConfigureServices(IServiceCollection services)
var conn = Configuration.GetConnectionString(""MyDB"").Replace(""~"", _env.ContentRootPath);
services.AddDbContext&lt;MyDBContext&gt;(options =&gt; options.UseSqlite(conn));
Then MyDBContext has
public partial class MyDBContext : DbContext
{
    public MyDBContext() { }
    public SatrimonoContext(DbContextOptions&lt;MyDBContext&gt; options)
        : base(options) { }
    public virtual DbSet&lt;Book&gt; Book { get; set; }
}
Then I use it in my page Model
private SatrimonoContext _db;
public BookAccuracyListModel(SatrimonoContext dbContext)
{
    _db = dbContext ?? throw new ArgumentNullException(nameof(dbContext));
}
and from there I can access _db normally via LINQ.
Before posting here, I did plenty of research on the topic, and the best responses I found were this
  This provider is Microsoft.Data.Sqlite. Those connection strings are
  for System.Data.SQLite.
  We support the following keywords: Cache, Data Source, Mode.
and this
  The issue I was having was because I was trying to create a
  SqlConnection instead of a SQLiteConnection. Making that change solved
  my issue.
However, if I'm doing it ""right"", I'm not creating the SqlConnection and thus can't change it to SQLiteConnection. The other response doesn't include a solution.
So how do I get this to work the right way?
",<c#><sqlite><entity-framework-core>,1710,2,19,3544,6,29,61,62,13385,,72,2,12,2019-03-24 13:11,2019-03-24 13:45,2019-03-24 13:45,0.0,0.0,Intermediate,19
48105051,Docker - How to take a look at the Tables inside MySQL volume?,"I have imported an SQL file contains my schema and all its tables, By using:
services:
  db:
    image: mysql:5.7
    volumes:
      - db_data:/var/lib/mysql
      - ./resources/file.sql:/docker-entrypoint-initdb.d/file.sql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: db
The problem is, when I trying to retrieve data from some tables an exception in the backend appear:
  throws exception:
  com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table
  'db.Configuration' doesn't exist
      com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'db.Configuration' doesn't exist
And some tables work perfectly like user table.
Although I have tested the SQL file in MySQL Workbench.
The question is, Is there a way I can see what tables are inside the db_data volume?
",<mysql><docker><docker-compose>,813,0,13,2140,3,24,42,36,37761,0.0,211,4,12,2018-01-04 23:26,2018-01-05 11:14,2018-01-05 11:35,1.0,1.0,Basic,14
51298622,AWS Athena (Presto) OFFSET support,"I would like to know if there is support for OFFSET in AWS Athena. For mysql the following query is running but in athena it is giving me error. Any example would be helpful.
select * from employee where empSal >3000 LIMIT 300 OFFSET 20
",<sql><amazon-web-services><presto><amazon-athena>,237,0,0,301,1,7,19,73,13396,0.0,0,5,12,2018-07-12 6:26,2018-07-13 6:16,2018-07-13 9:42,1.0,1.0,Intermediate,23
52307790,Phpmyadmin export issue: count(): Parameter must be an array or an object that implements Countable,"I'm getting issue with PhpMyAdmin when exporting any database. It is coming every time.
Please help me if anyone has solution to resolve all these types of issues in PhpMyAdmin
",<mysql><phpmyadmin><php-7.2>,177,1,0,5554,5,28,52,49,11393,0.0,320,6,12,2018-09-13 6:29,2018-09-13 6:29,2018-09-13 6:29,0.0,0.0,Intermediate,15
59323938,How to automatically set timestamp in room SQLite database?,"I am trying to have SQLite create automatic timestamps with CURRENT_TIMESTAMP.
I took the liberty of using Google's code:
// roomVersion = '2.2.2'
@Entity
public class Playlist {
    @PrimaryKey(autoGenerate = true)
    long playlistId;
    String name;
    @Nullable
    String description;
    @ColumnInfo(defaultValue = ""normal"")
    String category;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String createdTime;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String lastModifiedTime;
}
@Dao
interface PlaylistDao {
    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insert(playlist: Playlist): Long
}
This translates into an SQLite-Statement:
CREATE TABLE `Playlist` (
    `playlistId` INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, 
    `name` TEXT, 
    `description` TEXT, 
    `category` TEXT DEFAULT 'normal', 
    `createdTime` TEXT DEFAULT CURRENT_TIMESTAMP, 
    `lastModifiedTime` TEXT DEFAULT CURRENT_TIMESTAMP
)
I did make one insert: 
mDb.playListDao().insert(Playlist().apply { name = ""Test 1"" })
But the timestamps are always Null.
With the DB Browser for SQLite I added another entry, here I get timestamps.
How do I insert without a Null-Timestamp in room?
(Info: createdTime is also always the same as lastModifiedTime. I think this has to be done with triggers in SQLite, but that is a different problem not to be discussed here).
",<sqlite><android-room>,1391,2,31,6094,2,47,67,46,8770,0.0,101,3,12,2019-12-13 13:53,2019-12-13 14:17,2019-12-13 16:38,0.0,0.0,Advanced,33
52817777,Google Big Query: alternatives to browser SQL editor?,"I'm currently working a lot with Google Big Query and I absolutely hate querying in-browser. I'm used to connect to regular DB's through editors like Toad, Microsoft SQL Studio, Teradata Studio Express or Databeaver. I'm look for a similar tool that you guys would recommend for using on Google Big Query.
Only alternative I've found so far are razorsql and jetbrains datagrip (whereas the latter requires a custom connection (https://blog.jetbrains.com/datagrip/2018/07/10/using-bigquery-from-intellij-based-ide/) 
Any idea's on alternatives out there? 
Thanks in advance
",<google-bigquery><datagrip><razorsql>,573,2,0,121,1,1,6,46,17610,0.0,2,4,12,2018-10-15 13:23,2018-10-22 23:19,,7.0,,Intermediate,21
61727582,How to avoid unresolved symbol with clj-kond when using hugSQL def-db-fns macro?,"I write Clojure using the VS Code Calva extension, which uses clj-kondo to perform static analysis of my code.
I'm using HugSQL to create Clojure functions from SQL queries and statements.
I'm aware that I could handle the database connection and the HugSQL integration with a library like conman, infact I used it in the past and I like it, but this time I wanted to keep things vanilla and talk to HugSQL myself.
HugSQL's def-db-fns macro takes a SQL file and creates Clojure functions based on the SQL queries and statements contained in that file.
My code below works, but clj-kondo complains that seed-mytable! is an unresolved symbol.
(ns my-app.db
  ""This namespace represents the bridge between the database world and the clojure world.""
  (:require [environ.core :refer [env]]
            [hugsql.core :as hugsql]
            [nano-id.core :refer [nano-id]]))
;; This create the function seed-mytable!, but clj-kondo doesn't (cannot?) know it.
(hugsql/def-db-fns ""sql/mytable.sql"")
;; The functions created by HugSQL can accept a db-spec, a connection, a connection pool,
;; or a transaction object. Let's keep it simple and use a db-spec for a SQLite database.
(def db-spec {:classname ""org.sqlite.JDBC""
              :subprotocol ""sqlite""
              :subname (env :database-subname)})
(defn db-seed
  ""Populate the table with some fakes.""
  []
  (let [fakes [[(nano-id) ""First fake title"" ""First fake content""]
               [(nano-id) ""Second fake title"" ""Second fake content""]]]
    ;; clj-kondo complains that seed-my-table! is an unresolved symbol
    (seed-mytable! db-spec {:fakes fakes})))
I understand why clj-kondo complains: seed-mytable! is not defined anywhere, it's ""injected"" in this namespace when calling the def-db-fns macro.
Is there a way to tell clj-kondo that after calling the hugsql/def-db-fns macro the symbol does indeed exist?
Probably it's not that useful, but this is the SQL file I'm loading with HugSQL.
-- :name seed-mytable!
-- :command :execute
-- :result :affected
-- :doc Seed the `mytable` table with some fakes.
INSERT INTO mytable (id, title, content)
VALUES :t*:fakes;
",<clojure><hugsql><vscode-calva><clj-kondo>,2123,4,34,4641,3,28,37,65,3712,,438,1,12,2020-05-11 10:32,2020-05-11 10:58,2020-05-11 10:58,0.0,0.0,Advanced,32
50735318,MySQL default value on NULL,"I have the following table: 
CREATE TABLE `Foo` (
  `id`         int NOT NULL,
  `FirstName`  varchar(255) NULL,
  `LastName`   varchar(255) NOT NULL DEFAULT 'NONE',
  PRIMARY KEY (`id`)
);
When I run the following query it take the default value of 'NONE':
INSERT INTO Foo (`FirstName`) VALUES('FOO');
When I run the following query: 
INSERT INTO Foo (`FirstName`, `LastName`) VALUES('FOO', NULL);
it gives an error:
  [Err] 1048 - Column 'LastName' cannot be null
What I want to achieve is that if a value is NULL then MySQL should use the DEFAULT value.
Does anybody know the solution?
",<mysql><default-value>,589,0,11,3001,17,69,124,47,18808,0.0,103,3,12,2018-06-07 7:29,2018-06-07 7:33,,0.0,,Advanced,33
49319731,PhalconPHP Database transactions fail on server,"I have developed a website using PhalconPHP. the website works perfectly fine on my local computer with the following specifications:
PHP Version 7.0.22
Apache/2.4.18
PhalconPHP 3.3.1
and also on my previous Server (with DirectAdmin):
PHP Version 5.6.26
Apache 2
PhalconPHP 3.0.1
But recently I have migrated to a new VPS. with cPanel:
CENTOS 7.4 vmware [server]
cPanel v68.0.30
PHP Version 5.6.34 (multiple versions available, this one selected by myself)
PhalconPHP 3.2.2
On the new VPS my website always gives me Error 500.
in my Apache Error logs file: [cgi:error] End of script output before headers: ea-php70, referer: http://mywebsitedomain.net
What I suspect is the new database System. the new one is not mySql. it is MariaDB 10.1. I tried to downgrade to MySQL 5.6 but the WHM says there is no way I could downgrade to lower versions.
this is my config file:
[database]
adapter  = Mysql
host     = localhost
username = root
password = XXXXXXXXXXXX
dbname   = XXXXXXXXXXXX
charset  = utf8
and my Services.php:
protected function initDb()
{
    $config = $this-&gt;get('config')-&gt;get('database')-&gt;toArray();
    $dbClass = 'Phalcon\Db\Adapter\Pdo\\' . $config['adapter'];
    unset($config['adapter']);
    return new $dbClass($config);
}
And in my controllers...
for example this code throws Error 500:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""id ASC"")-&gt;execute();
but changing id to fname fixes the problem:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""fname ASC"")-&gt;execute();
or even the following code throws error 500:
$user = Users::findFirst(array(
                         ""conditions"" =&gt; ""id = :id:"",
                         ""bind"" =&gt; array(""id"" =&gt; $this-&gt;session-&gt;get(""userID""))
                        ));
is there a problem with the compatibility of PhalconPHP and MariaDB?
",<php><mysql><mariadb><cpanel><phalcon>,1852,1,40,514,1,6,22,59,489,0.0,15,2,12,2018-03-16 11:30,2018-03-18 14:42,2018-03-18 14:42,2.0,2.0,Advanced,33
61630911,Microsoft SQL Server Management Studio error at startup,"I get this error when I try to run Microsoft SQL Server Management Studio: 
  The application has failed to start because its side-by-side configuration is incorrect. Please see the application event log or use the command-line sxstrace.exe tool for more detail.
SxSTrace detail:
1, 2 
What I did for solving the problem: 
reinstalled SQL Server
reinstalled Microsoft SQL Server Management Studio
updated at recent version of .NET Framework
reinstalled Visual C++ Redistributable 
And I still get that same error.
What should I do?
",<sql-server><configuration><ssms><startup-error>,532,3,0,605,2,6,21,46,19223,0.0,68,5,12,2020-05-06 8:41,2020-08-18 2:49,,104.0,,Advanced,33
49557830,Understanding ILIKE ANY element of an array - postgresql,"I have to select all the lines in a table (let's call it mytable) for which the value in a given column (let's call it mycolumn) is not equal to 'A' and not equal to 'S'.
So I tried something like
SELECT * FROM mytable WHERE mycolumn NOT ILIKE ANY(ARRAY['A','S'])
I prefer the use of ILIKE instead of the use of = to test string equalities because the values 'A' and 'S' may come in lower-case in my data, so I want the values 's' and 'a' to be excluded as well.
Strangely enough, the query above did return some lines for which the value inside mycolumn was equal to 'A'. I was very surprised.
Therefore, to understand what was happening I tried to carry out a very simple logical test:
SELECT ('A' ILIKE ANY(ARRAY['A','S'])) as logical_test ;
The statement above returns TRUE, which was expected.
But the following statement also returns TRUE and this is where I'm lost:
SELECT ('A' NOT ILIKE ANY(ARRAY['A','S'])) as logical_test ;
Could someone explain why 'A' NOT ILIKE ANY(ARRAY['A','S']) is considered TRUE by PostgreSQL?
",<sql><postgresql>,1028,0,9,973,2,9,26,55,19881,,46,1,12,2018-03-29 13:49,2018-03-29 13:53,2018-03-29 13:53,0.0,0.0,Intermediate,15
56162109,Data Truncation issue while importing excel from Azure Blob storage to Sql Server,"I'm trying to import the below excel file present in the azure blob storage into sql server
EXCEL File
Query
SELECT * 
    FROM OPENROWSET(
        BULK 'container/testfile.xlsx', 
        DATA_SOURCE = 'ExternalSrcImport',
        FORMATFILE='container/test.fmt', FORMATFILE_DATA_SOURCE = 'ExternalSrcImport',
        codepage = 1252,
        FIRSTROW = 1
        ) as data
Format file 
10.0  
4  
1       SQLCHAR       0       7       ""\t""     1     DepartmentID     """"  
2       SQLCHAR       0       100     ""\t""     2     Name             SQL_Latin1_General_CP1_CI_AS  
3       SQLCHAR       0       100     ""\t""     3     GroupName        SQL_Latin1_General_CP1_CI_AS  
4       SQLCHAR       0       24      ""\r\n""   4     ModifiedDate     """"  
Illustration of Format File 
when I execute the query, I'm getting the below error
  Msg 4863, Level 16, State 1, Line 210 Bulk load data conversion error
  (truncation) for row 1, column 1 (DepartmentID).
looks like field terminator in the format file is not working, any ideas to import the file ?
",<sql><sql-server><azure><azure-blob-storage><openrowset>,1051,2,14,92178,19,132,173,78,634,,2567,1,12,2019-05-16 6:27,2019-05-21 17:39,,5.0,,Advanced,32
60590746,Update jsonb object in postgres,"One of my column is jsonb and have value in the format. The value of a single row of column is below.
{
    ""835"": {
        ""cost"": 0, 
        ""name"": ""FACEBOOK_FB1_6JAN2020"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }, 
    ""876"": {
        ""cost"": 0, 
        ""name"": ""MARVEL_BLACK_WIDOW_4DEC2019"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }
}
I want to update campaign_info(column name) column's the inner key ""processed""  and ""models"" of the campaign_id is ""876"". 
I have tried this query:
update safe_vid_info 
set campaign_info -&gt; '835' --&gt; 'processed'='completed' 
where cid = 'kiywgh'; 
But it didn't work.
Any help is appreciated. Thanks.
",<sql><json><postgresql><sql-update><jsonb>,1024,0,27,395,1,3,13,55,18052,0.0,104,1,12,2020-03-08 18:31,2020-03-08 23:07,2020-03-08 23:07,0.0,0.0,Intermediate,21
53171685,@CreationTimestamp and @UpdateTimestamp is not working with LocalDateTime,"I am trying to use @CreationTimestamp and @UpdateTimestamp with LocalDateTime type, but it is giving me org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp exception.
I am using 5.0.12.hibernate version with java 8 LocalDataTime, 
Is there any way to use @UpdateTimestamp and @CreationTimestamp with Java 8 LocalDateTime ?
 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1080)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)
    at com.godigit.MotorDataService.main(MotorDataService.java:35)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:44)
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:22)
    at org.hibernate.cfg.annotations.PropertyBinder.instantiateAndInitializeValueGeneration(PropertyBinder.java:415)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotation(PropertyBinder.java:383)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotations(PropertyBinder.java:348)
    at org.hibernate.cfg.annotations.PropertyBinder.determineValueGenerationStrategy(PropertyBinder.java:323)
    at org.hibernate.cfg.annotations.PropertyBinder.makeProperty(PropertyBinder.java:269)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyAndValue(PropertyBinder.java:189)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyValueAndBind(PropertyBinder.java:199)
    at org.hibernate.cfg.AnnotationBinder.processElementAnnotations(AnnotationBinder.java:2225)
    at org.hibernate.cfg.AnnotationBinder.processIdPropertiesIfNotAlready(AnnotationBinder.java:911)
    at org.hibernate.cfg.AnnotationBinder.bindClass(AnnotationBinder.java:738)
    at org.hibernate.boot.model.source.internal.annotations.AnnotationMetadataSourceProcessorImpl.processEntityHierarchies(AnnotationMetadataSourceProcessorImpl.java:245)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess$1.processEntityHierarchies(MetadataBuildingProcess.java:222)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:265)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:847)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:874)
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:60)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:360)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:382)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:371)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:336)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1687)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1624)
    ... 21 common frames omitted
",<java><postgresql><hibernate><spring-boot><java-8>,6078,0,58,1053,1,12,16,36,20151,0.0,73,2,12,2018-11-06 12:13,2018-11-06 13:00,2018-11-06 13:00,0.0,0.0,Intermediate,17
52253791,Pytest: setup testclient and DB,"I'm trying to learn something about testing my flask app. In order to do that, I am using pytest and sqlalchemy.
I want to test a template, whose delivers route some SQL content. So in my opinion I need a testClient for testing the route itself and a DB fixture to manage the DB stuff included in the route.
Here is my fixture:
import pytest
from config import TestingConfig
from application import create_app, db
# ###########################
# ## functional tests
# ###########################
@pytest.fixture(scope='module')
def test_client():
    app = create_app(TestingConfig)
    # Flask provides a way to test your application by exposing the Werkzeug 
    # test Client and handling the context locals for you.
    testing_client = app.test_client()
    with app.app_context():
        db.create_all()
        yield testing_client  # this is where the testing happens!
        db.drop_all()
And this is my basic test:
def test_home_page(test_client):
    """"""
    GIVEN a Flask application
    WHEN the '/' page is requested (GET)
    THEN check the response is valid and contains rendered content
    """"""
    response = test_client.get('/')
    assert response.status_code == 200
    assert ""SOME CONTENT"" in response.data
Running my test fails with:
=================================================================================================== test session starts ===================================================================================================
platform linux -- Python 3.5.2, pytest-3.8.0, py-1.5.4, pluggy-0.7.1
rootdir: /home/dakkar/devzone/private/, inifile:
collected 2 items                                                                                                                                                                                                         
tests/test_main.py 
    SETUP    M test_client
        tests/test_main.py::test_home_page (fixtures used: test_client)F
        tests/test_main.py::test_valid_order_message (fixtures used: test_client).
    TEARDOWN M test_client
======================================================================================================== FAILURES =========================================================================================================
_____________________________________________________________________________________________________ test_home_page ______________________________________________________________________________________________________
self = &lt;sqlalchemy.engine.base.Connection object at 0x7f1c3f29b630&gt;, dialect = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;
constructor = &lt;bound method DefaultExecutionContext._init_compiled of &lt;class 'sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext'&gt;&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
args = (&lt;sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f1c3f29b6d8&gt;, [immutabledict({})]), conn = &lt;sqlalchemy.pool._ConnectionFairy object at 0x7f1c3f29b550&gt;
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def _execute_context(self, dialect, constructor,
                         statement, parameters,
                         *args):
        """"""Create an :class:`.ExecutionContext` and execute, returning
            a :class:`.ResultProxy`.""""""
        try:
            try:
                conn = self.__connection
            except AttributeError:
                # escape ""except AttributeError"" before revalidating
                # to prevent misleading stacktraces in Py3K
                conn = None
            if conn is None:
                conn = self._revalidate_connection()
            context = constructor(dialect, self, conn, *args)
        except BaseException as e:
            self._handle_dbapi_exception(
                e,
                util.text_type(statement), parameters,
                None, None)
        if context.compiled:
            context.pre_exec()
        cursor, statement, parameters = context.cursor, \
            context.statement, \
            context.parameters
        if not context.executemany:
            parameters = parameters[0]
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                statement, parameters = \
                    fn(self, cursor, statement, parameters,
                       context, context.executemany)
        if self._echo:
            self.engine.logger.info(statement)
            self.engine.logger.info(
                ""%r"",
                sql_util._repr_params(parameters, batches=10)
            )
        evt_handled = False
        try:
            if context.executemany:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        statement,
                        parameters,
                        context)
            elif not parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor,
                        statement,
                        context)
            else:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor,
                        statement,
                        parameters,
&gt;                       context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: order
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
The above exception was the direct cause of the following exception:
test_client = &lt;FlaskClient &lt;Flask 'application'&gt;&gt;
    def test_home_page(test_client):
        """"""
        GIVEN a Flask application
        WHEN the '/' page is requested (GET)
        THEN check the response is valid and contains rendered content
        """"""
&gt;       response = test_client.get('/')
tests/test_main.py:7: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.5/site-packages/werkzeug/test.py:830: in get
    return self.open(*args, **kw)
venv/lib/python3.5/site-packages/flask/testing.py:200: in open
    follow_redirects=follow_redirects
venv/lib/python3.5/site-packages/werkzeug/test.py:803: in open
    response = self.run_wsgi_app(environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:716: in run_wsgi_app
    rv = run_wsgi_app(self.application, environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:923: in run_wsgi_app
    app_rv = app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2309: in __call__
    return self.wsgi_app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2295: in wsgi_app
    response = self.handle_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1741: in handle_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:2292: in wsgi_app
    response = self.full_dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1815: in full_dispatch_request
    rv = self.handle_user_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1718: in handle_user_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:1813: in full_dispatch_request
    rv = self.dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1799: in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
application/main/routes.py:20: in index
    func.count(Order.id).label(""orders_count"")
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2947: in one
    ret = self.one_or_none()
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2917: in one_or_none
    ret = list(self)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2988: in __iter__
    return self._execute_and_instances(context)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:3011: in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:948: in execute
    return meth(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/sql/elements.py:269: in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1060: in _execute_clauseelement
    compiled_sql, distilled_params
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1200: in _execute_context
    context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1413: in _handle_dbapi_exception
    exc_info
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:265: in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:248: in reraise
    raise value.with_traceback(tb)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: in _execute_context
    context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: order [SQL: 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""'] (Background on this error at: http://sqlalche.me/e/e3q8)
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
=========================================================================================== 1 failed, 1 passed in 0.52 seconds ============================================================================================
which tells me: db.create_all() does not create all tables in my testing database.
Any hint, what I am doing wrong here?
Some additional info:
using sqlite at the moment
the database file itself gets created in the filesystem with 0byte
More Debugging:
I followed this guide here: https://xvrdm.github.io/2017/07/03/testing-flask-sqlalchemy-database-with-pytest/
this is where thing become strange:
Link from above:
&gt;&gt;&gt; db.engine.table_names()  # Check the tables currently on the engine
[]                           # no table found
&gt;&gt;&gt; db.create_all()          # Create the tables according to defined models
&gt;&gt;&gt; db.engine.table_names()
['users']                    # Now table 'users' is found
What happenes in my project:
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt;
Snipplet from models.py:
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
class Order(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(120), index=True, unique=True)
",<python><python-3.x><flask><sqlalchemy><pytest>,13612,3,252,5702,5,24,28,37,12551,0.0,30,2,12,2018-09-10 8:26,2018-09-16 12:53,2018-09-16 12:53,6.0,6.0,Intermediate,17
49490623,Datetime filtering with SQLAlchemy isn't working,"Basically, I need to build a function that will filter a query according to given dates and return a new query. I'm new to SQLAlchemy, I looked up similar questions but I still got the same error:
`Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 8, 3, 1, 438278)`
Here's my code:
def filter_dates(query_obj, datecols, start = None, end = None):
        if end is None:
            end = datetime.datetime.now()
        if start is None:
            start = end - datetime.timedelta(weeks=12)
        print(""%s to %s"" % (start, end))
        for datecol in datecols:
            print(""Filtrando datas!"")
            query_obj = query_obj.filter(datecol &gt;= start)
            query_obj = query_obj.filter(datecol &lt;= end)
            ReevTable.print_query(query_obj)
        return query_obj
datecols is an orm.attributes object. Suppose I have an Object called User with a Datetime attribute named created_at. This is the expected behaviour:
query = session.query(Company.name, Company.created_at, Company.number_of_employees, Company.email_bounce_rate)
query = filter_dates(query_obj=query, datecols = [Company.created_at, Company.email_events.created_at])
query.all()
Expected output is a table with Companies that were only created within the date range, and the bounce rate should only be calculated during that specified date range. This might seem weird, but I calculate a not just emails, but other kinds of interactions too, so I need to input a list of attributes instead of just a single one. This is why I need to separate this filtering with a method.
I've tried using pandas datetime and timedelta, the built-in python datetime module, and simple strings with pd.to_datetime, but without success. The same error gets raised everytime. My Company column is in DateTime, so I don't know what else to do.
class Company(Base)
    created_at = Column(DateTime, nullable=False)
I'm completely new to SQLAlchemy, what am I doing wrong?
Full traceback:
`Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""reev-data-science/tables/reevtable.py"", line 128, in import_data
    self.print_query(query_obj)
  File ""reev-data-science/tables/reevtable.py"", line 107, in print_query
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)
  File ""&lt;string&gt;"", line 1, in &lt;lambda&gt;
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 442, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 448, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 453, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 219, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 245, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/annotation.py"", line 80, in _compiler_dispatch
    self, visitor, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1815, in visit_select
    text, select, inner_columns, froms, byfrom, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1899, in _compose_select_body
    t = select._whereclause._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1080, in visit_binary
    return self._generate_generic_binary(binary, opstring, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1113, in _generate_generic_binary
    self, eager_grouping=eager_grouping, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1244, in visit_bindparam
    bindparam, within_columns_clause=True, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1277, in render_literal_bindparam
    return self.render_literal_value(value, bindparam.type)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1295, in render_literal_value
    ""Don't know how to literal-quote value %r"" % value)
NotImplementedError: Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 9, 24, 46, 54634)`
The print_query() method:
`def print_query(query_obj):
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)`
",<python><datetime><sqlalchemy>,6262,0,79,321,0,2,11,68,5960,,115,2,12,2018-03-26 11:42,2021-07-16 0:58,,1208.0,,Advanced,32
56016700,How to create an empty array of struct in hive?,"I have a view in Hive 1.1.0, based on a condition, it should return an empty array or an array of struct&lt;name: string, jobslots: int&gt;
Here is my code:
select
      case when &lt;condition&gt; 
             then array()
           else array(struct(t1.name, t1.jobslots))
       end
from table t1;
The problem here is, that the empty array array() is of type array&lt;string&gt;. So when I try to insert it into a table, it throws an error.
How can I change this to return an empty array of type array&lt;struct&lt;name: string, jobslots:int&gt;&gt; so that Hive's size() function returns 0 on this array?
",<sql><arrays><struct><hive><hiveql>,611,0,12,945,0,12,32,62,7759,0.0,53,1,12,2019-05-07 6:27,2020-12-19 19:22,,592.0,,Advanced,32
53552983,How to generate datasets dynamically based on schema?,"I have multiple schema like below with different column names and data types. 
I want to generate test/simulated data using DataFrame with Scala for each schema and save it to parquet file.
Below is the example schema (from a sample json) to generate data dynamically with dummy values in it.
val schema1 = StructType(
  List(
    StructField(""a"", DoubleType, true),
    StructField(""aa"", StringType, true)
    StructField(""p"", LongType, true),
    StructField(""pp"", StringType, true)
  )
)
I need rdd/dataframe like this with 1000 rows each based on number of columns in the above schema.
val data = Seq(
  Row(1d, ""happy"", 1L, ""Iam""),
  Row(2d, ""sad"", 2L, ""Iam""),
  Row(3d, ""glad"", 3L, ""Iam"")
)
Basically.. like this 200 datasets are there for which I need to generate data dynamically, writing separate programs for each scheme is merely impossible for me.
Pls. help me with your ideas or impl. as I am new to spark.
Is it possible to generate dynamic data based on schema of different types? 
",<scala><apache-spark><apache-spark-sql>,997,0,14,900,0,13,27,50,5013,0.0,423,3,12,2018-11-30 7:21,2018-12-14 20:20,2018-12-17 15:03,14.0,17.0,Basic,9
48326311,Sequelize: overlapping - Checking if any value in array matches any value in the passed array,"Using sequelize to check if the the db input property, which is array, has a given item.
Have a Postgres database with data Events.
Want to get one Event that will have any of these weekDays.
Type of weekDays is ARRAY(integer).
Events.findOne({
  where: {
    weekDays: {
      $contains: [2, 3],
    },
  },
});
Tried to do with $contains, $any, or $like $any but all the time got the same error message.
  TypeError: values.map is not a function
Sincerely thanks
",<sql><node.js><database><postgresql><sequelize.js>,465,0,7,515,1,5,17,81,7328,,13,1,12,2018-01-18 16:44,2018-01-19 10:45,2018-01-19 10:45,1.0,1.0,Advanced,32
53881132,How to change the search action in wordpress search bar?,"Create a new post and publish it.
The title is my test for search, content in it is as below:
no host route
Check what happen in wordpress database.
 select post_title from wp_posts
     where post_content like ""%no%""
       and post_content like ""%route%""
       and post_content like ""%to%""
       and post_content like ""%host%"";
The post named my test for search will not be in the select's result.
Type no route to host in wordpress search bar,and click enter.
The post named my test for search shown as result.
I found the reason that the webpage contain to ,in the left upper side corner ,there is a word Customize which contains the searched word to.
How to change such search action in wordpress serach bar?
I want to make the search behavior in wordpress saerch bar, for example ,when you type no route to host, equal to the following sql command.
select post_title from wp_posts where post_content like ""%no%route%to%host%"";
All the plugins in my wordpress.
CodePen Embedded Pens Shortcode
Crayon Syntax Highlighter
Disable Google Fonts
Quotmarks Replacer
SyntaxHighlighter Evolved
",<mysql><wordpress><search>,1092,1,20,474,42,145,298,74,988,,387,1,12,2018-12-21 8:01,2018-12-23 18:31,2018-12-23 18:31,2.0,2.0,Basic,9
54448139,Microsoft.SqlServer.Types incompatible with .NET Standard,"I'm attempting to convert all of our C# class libraries from .NET Framework to .NET Standard projects, as we are starting to leverage .NET Core so need these to be consumable by both .NET Core and .NET Framework apps (with the latter being ported over to Core in the upcoming months.)
I'm having trouble converting our data access layer code because we leverage Microsoft.SqlServer.Types extensively and the official NuGet package doesn't support .NET Standard. I tried an unofficial NuGet package by dotmorten but it's missing a lot of functionality. Below is a list of everything missing that we would need (thrown together to get the code building...)
public static class SqlMockExtensions
{
    public static SqlBytes STAsBinary(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography MakeValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static int STDimension(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static bool STIsValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static Nullable&lt;double&gt; EnvelopeAngle(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography ReorientObject(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography BufferWithTolerance(this SqlGeography geography, double arg1, int arg2, bool arg3) =&gt; throw new NotImplementedException();
    public static SqlGeography Reduce(this SqlGeography geography, double tolerance) =&gt; throw new NotImplementedException();
    public static SqlGeography EnvelopeCenter(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static double STDistance(this SqlGeography geography, SqlGeography point2) =&gt; throw new NotImplementedException();
    public static SqlBytes STAsBinary(this SqlGeometry geometry) =&gt; throw new NotImplementedException();
}
When I search SO for others trying to integrate Microsoft.SqlServer.Types into their .NET Core and Standard projects, I see mentions of including the official NuGet package and then doing something like this:
SqlServerTypes.Utilities.LoadNativeAssemblies(AppDomain.CurrentDomain.BaseDirectory);
However, it errors when you try to add a non-.NET Standard compliant NuGet package into a .NET Standard project, so I'm not clear how this is a solution.
This seems like a very common problem to have, there have to be a lot of developers out there who leverage Microsoft.SqlServer.Types for SqlGeography, SqlGeometry, etc... and are porting over to .NET Standard. So how are all of you accomplishing this?
",<c#><sql-server><.net-standard><sqlgeography><sqlgeometry>,2717,3,25,17807,38,132,203,58,4761,0.0,7782,1,12,2019-01-30 19:28,2021-07-25 13:26,,907.0,,Intermediate,18
48350843,How to connect from docker-compose to Host PostgreSQL?,"I have a server with installed PostgreSQL. All my services work in containers (docker-compose). I want to use my Host PostgreSQL from containers. Buy I have the error:
  Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://localhost:5432/shop-bd) for user 'shop-bd-user': Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  SQL State  : 08001
  Error Code : 0
  Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection refused (Connection refused)
My docker-compose is using host network_mode, like this:
version: '3'
services:
  shop:  
    container_name: shop
    build: ./shop
    hostname: shop
    restart: always   
    ports:
      - 8084:8084
    network_mode: ""host""
My database connection URL is: jdbc:postgresql://localhost:5432/shop-bd
",<postgresql><docker><networking><localhost><docker-compose>,1360,0,18,1813,3,26,39,68,19448,0.0,57,4,12,2018-01-19 23:11,2018-01-20 8:32,2018-04-15 7:25,1.0,86.0,Basic,9
50467598,Multi-homed SQL Server with High Availability Groups,"We have two servers (SQL-ATL01, SQL-ATL02) that make up a Failover Cluster, each running as part of a  SQL Server High Availability Group (HAG). Each server has two network cards. One is a 10Gbit card that is directly connected to the other server and is used for Synchronizing the HAG on a 192.168.99.x subnet. The other is a 1Gbit card that is used to connect the DB servers to a switch to communicate with the application servers on a 10.0.0.x subnet. The Listener is pointed to the  192.168.99.x subnet.
We want to add a third server (SQL-NYC01) in another physical location to the cluster and run it as an Async replica part of the HAG, but the VPN only routes traffic on the subnet on the 1Gbit network.
Is there any way to set up the Failover Cluster and High Availability Group to tell it:
Send synchronous replica traffic for SQL-ATL01 &lt;--> SQL-ATL02 over 192.168.99.x
Send asynchronous replica traffic for (SQL-ATL01, SQL-ATL02) &lt;--> SQL-NYC01 over 10.0.0.x
Or do we have to have all replica traffic going in and out on the same IP address/subnet?  
",<sql-server><high-availability>,1066,0,0,7461,3,32,33,72,479,,327,1,12,2018-05-22 12:23,2018-05-30 9:31,,8.0,,Advanced,32
51088034,Hibernate @Formula doesn't include Schema,"I have an entity with a property @Formula like this:
@Entity
@Table(name = ""areasAuxiliar"")
public final class AreaAuxiliar implements Serializable {
    @Id
    @Column(name = ""idArea"")
    private Integer idArea;
    @Formula(""RUTAAREA(idArea)"")
    private String ruta;
when I configure my hibernate to point to an Oracle DB I have no problem,
BUT, when I switch to an SQLServer, hibernate is not including the shema and the query fails,
the query generated for hibernate looks like this:
select
    areaauxili4_.idArea as idArea1_6_4_,
    rutaArea(areaauxili4_.idArea) as formula2_4_
from
    SIGAP.areasAuxiliar areaauxili4_ 
the param hibernate.default_schema=SIGAP is being read and included in the table but not in the function,
is there an option/annotation to force the shema in in that function?
I have tried hibernate 5.1 and 5.2 with the same result :(
",<java><sql-server><hibernate><jpa><hql>,867,0,15,568,3,7,27,68,2683,,221,4,12,2018-06-28 16:54,2018-07-25 16:15,2018-07-31 23:30,27.0,33.0,Advanced,35
57681970,PSQLException and lock issue when trigger added on table,"UPDATE: I eliminated Hibernate from the problem. I completely reworked description of problem to simplify it as much as possible.
I have master table with noop trigger and detail table with two relations between master and detail table:
create table detail (
  id bigint not null,
  code varchar(255) not null,
  primary key (id)
);
create table master (
  id bigint not null,
  name varchar(255),
  detail_id bigint, -- ""preferred"" detail is one-to-one relation
  primary key (id),
  unique (detail_id),
  foreign key (detail_id) references detail(id)
);
create table detail_candidate ( -- ""candidate"" details = many-to-many relation modeled as join table
  master_id bigint not null,
  detail_id bigint not null,
  primary key (master_id, detail_id),
  foreign key (detail_id) references detail(id),
  foreign key (master_id) references master(id)
);
create or replace function trgf() returns trigger as $$
begin
  return NEW;
end;
$$ language 'plpgsql';
create trigger trg
  before insert or update
  on master
  for each row execute procedure trgf();
insert into master (id, name) values (1000, 'x'); -- this is part of database setup
insert into detail (code, id) values ('a', 1);    -- this is part of database setup
In such setup, I open two terminal windows with psql and perform following steps:
in first terminal, change master (leave transaction open)
begin;
update master set detail_id=null, name='y' where id=1000;
in second terminal, add detail candidate to master in own transaction
begin;
set statement_timeout = 4000;
insert into detail_candidate (master_id, detail_id) values (1000, 1);
Last command in second terminal timeouts with message
ERROR:  canceling statement due to statement timeout
CONTEXT:  while locking tuple (0,1) in relation ""master""
SQL statement ""SELECT 1 FROM ONLY ""public"".""master"" x WHERE ""id"" OPERATOR(pg_catalog.=) $1 FOR KEY SHARE OF x""
My observation and questions (changes are independent):
when the db is setup without trigger, i.e. drop trigger trg on master; is called after initial setup, everything works fine.
Why the presence of noop trigger has such an influence? I don't get it.
when the db is setup without unique constraint on master.detail_id (i.e. alter table master drop constraint master_detail_id_key; is called after initial setup), everything works fine too. Why?
when I omit explicit detail=null assignment in update statement in first terminal (since there's null value from setup anyway), everything works fine too. Why?
Tried on Postgres 9.6.12 (embedded), 9.6.15 (in Docker), 11.5 (in Docker).
Problem is reproducible in Docker image tomaszalusky/trig-example which is available on DockerHub or can be built from this Dockerfile (instructions inside).
UPDATE 2: I found common behaviour of three observation above. I spawned the query select * from pgrowlocks('master') from pgrowlocks extension in second transaction. The row-level lock of updated row in master is FOR UPDATE in failing case but FOR NO KEY UPDATE in all three working cases. This is in perfect compliance with mode match table in documentation since FOR UPDATE mode is the stronger one and mode requested by insert statement is FOR KEY SHARE (which is apparent from error message, also invoking the select ... for key share command has same effect as insert command).
The documentation on FOR UPDATE mode says:
  The FOR UPDATE lock mode is also acquired by (...) an UPDATE that modifies the values on certain columns. Currently, the set of columns considered for the UPDATE case are those that have a unique index on them that can be used in a foreign key (...)
It is true for master.detail_id column. However, still it's not clear why FOR UPDATE mode isn't chosen independently on trigger presence and why trigger presence caused it.
",<postgresql><triggers><locking>,3772,4,65,11000,2,39,64,39,3384,,1413,1,12,2019-08-27 20:40,2019-09-05 21:42,2019-09-05 21:42,9.0,9.0,Advanced,35
55145223,SSIS job failed - An item with the same key has already been added,"Our SSIS package fails 2 seconds into the run with the following error:
An item with the same key has already been added.;   at System.ThrowHelper.ThrowArgumentException(ExceptionResource resource)
   at System.Collections.Generic.Dictionary`2.Insert(TKey key, TValue value, Boolean add)
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.StartPackage()
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.PerformOperation()    Transact-SQL stored procedure
We deploy our SSIS packages (2016) to our SSISDB on MSSQL: 13.0.4001.0, we also have environment variables in that same folder
The strange thing is that in this project I have 5 packages that run ok (different job) but only 1 fails (it has its’ own job)
I already deleted a recreated the folder/ environment variables/jobs  - same result
I made sure I have Different environment variables folder name
We run the job with different users and 2 of them were admins in the DB
We have other servers which have the same config and project (2 QA environments) and they work the same but OK!
Do I need to go directly to MSDB and delete a row? Where?
Appreciate any ideas – thank you all
I found the error and solution 😊
It seems I deployed a package with 2 same named SMTP connection (one as a project connection and the other with same name as a package connection)
I deleted the unnecessary (in my case deleted the package one) and deployed the fixed package.
Now the job run successfully calling on one only connection.
I was missing the correct error handling from the error itself since it did not direct me to the specific connection
The package failed at runtime on validations – so no error logs to assist
I run the query from [SSISDB].[catalog].[execution_parameter_values] and compared between a successful run to a failed then I noticed the same named connections   
Thank you for your comments!
Yoni 
",<sql-server><ssis>,1925,0,0,341,1,2,10,35,17411,0.0,2,3,12,2019-03-13 15:11,2019-03-14 14:05,,1.0,,Basic,13
53855219,"MySQL not updating information_schema, unless I manually run ANALYZE TABLE `myTable`","I have the need to get last id (primary key) of a table (InnoDB), and to do so I perform the following query:
SELECT (SELECT `AUTO_INCREMENT` FROM `information_schema`.`TABLES` WHERE `TABLE_SCHEMA` = 'mySchema' AND `TABLE_NAME` = 'myTable') - 1;
which returns the wrong AUTO_INCREMENT. The problem is the TABLES table of information_schema is not updated with the current value, unless I run the following query:
ANALYZE TABLE `myTable`;
Why doesn't MySQL update information_schema automatically, and how could I fix this behavior?
Running MySQL Server 8.0.13 X64.
",<mysql><innodb><information-schema><mysql-8.0>,565,0,2,928,2,13,33,64,15108,,32,3,12,2018-12-19 16:17,2018-12-19 16:44,2018-12-19 16:44,0.0,0.0,Intermediate,16
54821962,Connect to On Prem SQL server from Azure Web app,"I have .Net application at on prim. I want to host it at Azure but don't want to move database. I publish the application on Azure but it is not able to connect to on prim database. 
SQL server is in private netwrok.
For POC purpose, I am using MSDN subscription. I am facing below error,
A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error: 40 - Could not open a connection to SQL Server)
Thanks,
Umesh
",<c#><sql-server><azure><azure-hybrid-connections>,644,0,0,2915,5,16,18,43,18362,0.0,10,2,12,2019-02-22 7:19,2019-02-22 7:25,2019-02-22 7:51,0.0,0.0,Intermediate,22
56110263,pgAdmin argument formats can't be mixed,"Background
Ubuntu 18.04
Postgresql 11.2 in Docker 
pgAdmin4 3.5
Have a column named alias with type character varying[](64). Values have already been set on some rows before using psycopg2. Everything was alright then.
SQL = 'UPDATE public.""mytable"" SET alias=%s WHERE id=%s'
query = cursor.mogrify(SQL, ([values] , id))
cursor.execute(query)
conn.commit()
Recently, when I want to add more value using pgAdmin GUI as shown in the first figure, the error in the second figure happens, which says Argument formats can't be mixed:
Well, it turns out if insert the values using script such as psql or query tool in pgAdmin, the error does not happen, i.e., it only happens if using GUI of pgAdmin.
Example script:
UPDATE public.""mytable"" SET alias='{a, b}' WHERE id='myid'
But as the GUI is much easier to modify values, so really want to figure it out. Any idea?
",<postgresql><pgadmin>,861,2,7,2370,3,18,36,62,2083,,44,2,12,2019-05-13 10:13,2019-09-17 16:51,,127.0,,Basic,9
52143396,JDBC SQL Server : The value is not set for the parameter number,"I received the following error from the code that calls a stored procedure from java code:    
  Exception Trace {} org.springframework.jdbc.UncategorizedSQLException:
  CallableStatementCallback; uncategorized SQLException for SQL [{call
  test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, ,
  ?, ?, ?, ?, ?)}]; SQL state [null]; error code [0]; The value is not
  set for the parameter number 11.; nested exception is
  com.microsoft.sqlserver.jdbc.SQLServerException: The value is not set
  for the parameter number 11. at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:1095)
  at
  org.springframework.jdbc.core.JdbcTemplate.call(JdbcTemplate.java:1131)
The application is deployed on WAS 8.5.5 and using jdbc driver version 4.2. On restarting the server this issue did not occur again. The following call statement generated looks to be incorrect. There are consecutive commas without ? between them.
  {call test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?,
  ?)}
The stored procedure has 10 parameters. Following is the definition of the stored procedure:
CREATE PROCEDURE [test].[usp_xxx_GetCompanyDetails]
(
    @ANumber        int,
    @CompanyId      int,
    @UserRole       varchar(15),
    @RequestId      varchar(100),
    @CompanyCode    varchar(5),
    @BaseSystem     varchar(5),
    @PType          varchar(20),    
    @PId            varchar(40),
    @IsActive       bit,     
    @responseData   xml OUT
)
Following is the java code that makes a call to the stored proc. It is using spring data to make the call.
 private String executeProc(Integer aNumber,Integer companyId, String baseSystem, 
                String role,String companyCode,String requestId, String pType,String pId,
                boolean isActive ) throws SQLException {
            SQLXML responseData=null;
            Map&lt;String,Object&gt; inputParams= new HashMap&lt;&gt;();
            inputParams.put(""ANumber"", aNumber);
            inputParams.put(""CompanyId"", companyId);
            inputParams.put(""UserRole"", role);
            inputParams.put(""RequestId"", requestId);
            inputParams.put(""CompanyCode"", companyCode);
            inputParams.put(""BaseSystem"", baseSystem);
            inputParams.put(""PType"", pType);
            inputParams.put(""PId"", pId);
            inputParams.put(""IsActive"", isActive);
            inputParams.put(""ResponseData"", responseData);
            Map&lt;String, Object&gt; result = this.execute(inputParams);
            String responseXMLString = ((SQLXML) result.get(""ResponseData"")).getString();
            return responseXMLString;
        }
What could have gone wrong.
",<java><sql-server><jdbc><sql-server-2012><websphere>,3077,0,35,1235,0,16,36,59,5994,0.0,62,2,12,2018-09-03 5:02,2018-09-03 5:09,,0.0,,Advanced,32
52537741,multiprocessing / psycopg2 TypeError: can't pickle _thread.RLock objects,"I followed the below code in order to implement a parallel select query on a postgres database:
https://tech.geoblink.com/2017/07/06/parallelizing-queries-in-postgresql-with-python/
My basic problem is that I have ~6k queries that need to be executed, and I am trying to optimise the execution of these select queries. Initially it was a single query with the where id in (...) contained all 6k predicate IDs but I ran into issues with the query using up > 4GB of RAM on the machine it ran on, so I decided to split it out into 6k individual queries which when synchronously keeps a steady memory usage. However it takes a lot longer to run time wise, which is less of an issue for my use case. Even so I am trying to reduce the time as much as possible.
This is what my code looks like:
class PostgresConnector(object):
    def __init__(self, db_url):
        self.db_url = db_url
        self.engine = self.init_connection()
        self.pool = self.init_pool()
    def init_pool(self):
        CPUS = multiprocessing.cpu_count()
        return multiprocessing.Pool(CPUS)
    def init_connection(self):
        LOGGER.info('Creating Postgres engine')
        return create_engine(self.db_url)
    def run_parallel_queries(self, queries):
        results = []
        try:
            for i in self.pool.imap_unordered(self.execute_parallel_query, queries):
                results.append(i)
        except Exception as exception:
            LOGGER.error('Error whilst executing %s queries in parallel: %s', len(queries), exception)
            raise
        finally:
            self.pool.close()
            self.pool.join()
        LOGGER.info('Parallel query ran producing %s sets of results of type: %s', len(results), type(results))
        return list(chain.from_iterable(results))
    def execute_parallel_query(self, query):
        con = psycopg2.connect(self.db_url)
        cur = con.cursor()
        cur.execute(query)
        records = cur.fetchall()
        con.close()
        return list(records)
However whenever this runs, I get the following error:
TypeError: can't pickle _thread.RLock objects
I've read lots of similar questions regarding the use of multiprocessing and pickleable objects but I cant for the life of me figure out what I am doing wrong.
The pool is generally one per process (which I believe is the best practise) but shared per instance of the connector class so that its not creating a pool for each use of the parallel_query method.
The top answer to a similar question:
Accessing a MySQL connection pool from Python multiprocessing
Shows an almost identical implementation to my own, except using MySql instead of Postgres.
Am I doing something wrong?
Thanks!
EDIT: 
I've found this answer:
Python Postgres psycopg2 ThreadedConnectionPool exhausted
which is incredibly detailed and looks as though I have misunderstood what multiprocessing.Pool vs a connection pool such as ThreadedConnectionPool gives me. However in the first link it doesn't mention needing any connection pools etc. This solution seems good but seems A LOT of code for what I think is a fairly simple problem?
EDIT 2: 
So the above link solves another problem, which I would have likely run into anyway so I'm glad I found that, but it doesnt solve the initial issue of not being able to use imap_unordered down to the pickling error. Very frustrating. 
Lastly, I think its probably worth noting that this runs in Heroku, on a worker dyno, using Redis rq for scheduling, background tasks etc and a hosted instance of Postgres as the database. 
",<python><postgresql><sqlalchemy><psycopg2><python-multiprocessing>,3558,4,43,545,2,6,25,56,5165,0.0,21,1,12,2018-09-27 13:07,2018-10-08 13:33,2018-10-08 13:33,11.0,11.0,Advanced,32
52096692,Change sequelize timezone,"I want to make restful app in nodejs
Server: centos 7 64x
Database: postgresql
Additional: express, sequelize
Table: datetime with timezone
When I selecting rows with sequelize from database, created_at column gives me wrong time. 5 hour added to datetime.
I change timezone configuration of centos to +5 (Tashkent/Asia)
Also change postgresql timezone configuration to +5
Datetime is correct in database when shows.
But when I select it converts to like this
""createdAt"": ""2018-08-12T17:57:20.508Z""
In database column shows this
2018-08-12 22:57:20.508+05
config.json
""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""zablet"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""timezone"": ""Tashkent/Ashgabat"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    }
}
index.js
'use strict';
var fs = require('fs');
var path = require('path');
var Sequelize = require('sequelize');
var basename = path.basename(__filename);
var env = process.env.NODE_ENV || 'development';
var config = require('../config/config.json')[env];
var db = {};
if (config.use_env_variable) {
    var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
    var sequelize = new Sequelize(config.database, config.username, config.password, config);
}
fs
    .readdirSync(__dirname)
    .filter(file =&gt; {
        return (file.indexOf('.') !== 0) &amp;&amp; (file !== basename) &amp;&amp; (file.slice(-3) === '.js');
    })
    .forEach(file =&gt; {
        var model = sequelize['import'](path.join(__dirname, file));
        db[model.name] = model;
    });
Object.keys(db).forEach(modelName =&gt; {
    if (db[modelName].associate) {
        db[modelName].associate(db);
    }
});
db.sequelize = sequelize;
db.Sequelize = Sequelize;
module.exports = db;
updated config.json
{
    ""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""postgres"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    },
    ""dialectOptions"": {
        ""useUTC"": false
    },
    ""timezone"": ""+05:00""
}
}
How can I select rows from database in correct timezone format?
",<node.js><postgresql><express><sequelize.js>,2401,0,66,553,4,11,31,67,44370,0.0,37,2,12,2018-08-30 12:07,2018-08-30 13:53,2018-08-30 13:53,0.0,0.0,Basic,2
58905555,"Cannot connect to MySQL database (running on WSL2) from Windows Desktop Application ""MySQL Workbench""","I set up MySQL on Windows Subsystem for Linux (WSL2 version).  I'm relatively new to MySQL, but I have confirmed the following: 
It is running (ps ax | grep mysqld returns a value)
It is running on default host 127.0.0.1
It is running on default port 3306
To login to the mysql shell, I use the command sudo mysql -u root -p.  Without sudo, I am unable to login to the shell.
I assume that this issue has something to do with the host that the MySQL service is running on, but I have no idea how to change that and properly connect.  Below is a screenshot of the connection setup in MySQL Workbench.
And below is the error that I get when I use the settings shown and my root user password.
",<mysql-workbench><windows-subsystem-for-linux>,691,3,5,2164,2,17,26,58,23856,0.0,116,6,12,2019-11-17 21:51,2019-12-06 19:10,,19.0,,Basic,9
53800062,"""expected zero arguments for construction of ClassDict (for numpy.dtype)"" when calling UDF that returns FloatType()","I believe it is related to this one: Spark Error:expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
I have a dataframe
id col_1 col_2
1 [1,2] [1,3]
2 [2,1] [3,4]
I want to create another column that is a cosine distance between col_1 and col_2.
from scipy.spatial.distance import cosine
def cosine_distance(a,b):
    try:
        return cosine(a, b)
    except Exception as e:
        return 0.0 # in case division by zero
And I defined a udf:
cosine_distance_udf = udf (cosine_distance, FloatType())
And finally:
new_df = df.withColumn('cosine_distance', cosine_distance_udf('col_1', 'col_2'))
And I have the error: PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
What did I do wrong?
",<python><dataframe><pyspark><apache-spark-sql>,771,1,17,4480,11,46,76,35,9896,0.0,102,1,12,2018-12-16 6:52,2018-12-16 7:10,2018-12-16 7:10,0.0,0.0,Advanced,32
54454797,where IN clause - multi column (querydsl),"I have three integer values of two pairs.
I would like to use this as a list of IN in the WHERE clause.
  (2019, 5) (2019, 6) (2019, 7)
I want to use the above two-pair list in a query like this:
SELECT
    *
FROM
    WEEK_REPORT A
WHERE
    A.user_id IN ('test2','test5' ) AND
    (A.year, A.week_no) IN ((2019,4),(2019,5),(2019,6));
To that end, I wrote the source as follows:
// lastYear=2019, lastWeekNo=4
Tuple t = JPAExpressions.select(Expressions.constant(lastYear), Expressions.constant(lastWeekNo))
    .from(weekReport)
    .fetchOne();
// currentYear=2019, currentWeekNo=5        
Tuple t2 = JPAExpressions.select(Expressions.constant(currentYear), Expressions.constant(currentWeekNo))
    .from(weekReport)
    .fetchOne();
// nextYear=2019, nextWeekNo=4
Tuple t3 = JPAExpressions.select(Expressions.constant(nextYear), Expressions.constant(nextWeekNo))
    .from(weekReport)
    .fetchOne();
return queryFactory
    .select(weekReport)
    .from(weekReport)
    .where(weekReport.user.eq(user)
        .and(Expressions.list(weekReport.year, weekReport.weekNo).in(t, t2, t3)))
    .fetch();
However, the correct result is not output and an error occurs.
java.lang.UnsupportedOperationException: null
    at com.querydsl.jpa.JPASubQuery.fetchOne(JPASubQuery.java:66) ~[querydsl-jpa-4.1.4.jar:na]
I looked it up in the official document but it does not come out.
Is there a way?  
Thank you.
",<mysql><sql><querydsl><in-clause>,1402,0,30,323,0,2,9,67,2476,0.0,0,4,12,2019-01-31 6:44,2019-01-31 6:53,,0.0,,Basic,10
52803014,"Sqlite with real ""Full Text Search"" and spelling mistakes (FTS+spellfix together)","Let's say we have 1 million of rows like this:
import sqlite3
db = sqlite3.connect(':memory:')
c = db.cursor()
c.execute('CREATE TABLE mytable (id integer, description text)')
c.execute('INSERT INTO mytable VALUES (1, ""Riemann"")')
c.execute('INSERT INTO mytable VALUES (2, ""All the Carmichael numbers"")')
Background:
I know how to do this with Sqlite:
Find a row with a single-word query, up to a few spelling mistakes with the spellfix module and Levenshtein distance (I have posted a detailed answer here about how to compile it, how to use it, ...):
db.enable_load_extension(True)
db.load_extension('./spellfix')
c.execute('SELECT * FROM mytable WHERE editdist3(description, ""Riehmand"") &lt; 300'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(1, u'Riemann')]
With 1M rows, this would be super slow! As detailed here, postgresql might have an optimization with this using trigrams. A fast solution, available with Sqlite, is to use a VIRTUAL TABLE USING spellfix:
c.execute('CREATE VIRTUAL TABLE mytable3 USING spellfix1')
c.execute('INSERT INTO mytable3(word) VALUES (""Riemann"")')
c.execute('SELECT * FROM mytable3 WHERE word MATCH ""Riehmand""'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(u'Riemann', 1, 76, 0, 107, 7)], working!
Find an expression with a query matching one or multiple words with FTS (""Full Text Search""):
c.execute('CREATE VIRTUAL TABLE mytable2 USING fts4(id integer, description text)')
c.execute('INSERT INTO mytable2 VALUES (2, ""All the Carmichael numbers"")')
c.execute('SELECT * FROM mytable2 WHERE description MATCH ""NUMBERS carmichael""'); print c.fetchall()
#Query: 'NUMBERS carmichael'
#Answer: [(2, u'All the Carmichael numbers')]
It is case insensitive and you can even use a query with two words in the wrong order, etc.: FTS is quite powerful indeed. But the drawback is that each of the query-keyword must be correctly spelled, i.e. FTS alone doesn't allow spelling mistakes.
Question:
How to do a Full Text Search (FTS) with Sqlite and also allow spelling mistakes? i.e. ""FTS + spellfix"" together
Example: 
row in the DB: ""All the Carmichael numbers""
query: ""NUMMBER carmickaeel"" should match it!
How to do this with Sqlite? 
It is probably possible with Sqlite since this page states:
  Or, it [spellfix] could be used with FTS4 to do full-text search using potentially misspelled words.
Linked question: String similarity with Python + Sqlite (Levenshtein distance / edit distance)
",<python><sqlite><full-text-search><levenshtein-distance>,2431,4,30,42385,103,394,707,36,8839,0.0,3760,2,12,2018-10-14 13:12,2018-10-17 17:42,2018-10-17 17:42,3.0,3.0,Basic,10
56321781,Hangfire causing locks in SQL Server,"We are using Hangfire 1.7.2 within our ASP.NET Web project with SQL Server 2016. We have around 150 sites on our server, with each site using Hangfire 1.7.2. We noticed that when we upgraded these sites to use Hangfire, the DB server collapsed. Checking the DB logs, we found out there were multiple locking queries. We have identified one RPC Event  “sys.sp_getapplock;1” In the all blocking sessions. It seems like Hangfire is locking our DB rendering whole DB unusable. We noticed almost 670+ locking queries because of Hangfire.
This could possibly be due to these properties we setup:
   SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
   QueuePollInterval = TimeSpan.FromHours(5)
Each site has around 20 background jobs, a few of them run every minute, whereas others every hour, every 6 hours and some once a day.
I have searched the documentation but could not find anything which could explain these two properties or how to set them to avoid DB locks.
Looking for some help on this.
EDIT: The following queries are executed at every second:
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
select distinct(Queue) from [HangFire].JobQueue with (nolock)
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
irrespective of various combinations of timespan values we set. Here is the code of GetHangfirServers we are using:
  public static IEnumerable&lt;IDisposable&gt; GetHangfireServers()
    {
        // Reference for GlobalConfiguration.Configuration: http://docs.hangfire.io/en/latest/getting-started/index.html
        // Reference for UseSqlServerStorage: http://docs.hangfire.io/en/latest/configuration/using-sql-server.html#configuring-the-polling-interval
        GlobalConfiguration.Configuration
            .SetDataCompatibilityLevel(CompatibilityLevel.Version_170)
            .UseSimpleAssemblyNameTypeSerializer()
            .UseRecommendedSerializerSettings()
            .UseSqlServerStorage(ConfigurationManager.ConnectionStrings[""abc""]
                .ConnectionString, new SqlServerStorageOptions
            {
                CommandBatchMaxTimeout = TimeSpan.FromMinutes(5),
                SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
                QueuePollInterval = TimeSpan.FromHours(5), // Hangfire will poll after 5 hrs to check failed jobs.
                UseRecommendedIsolationLevel = true,
                UsePageLocksOnDequeue = true,
                DisableGlobalLocks = true
            });
        // Reference: https://docs.hangfire.io/en/latest/background-processing/configuring-degree-of-parallelism.html
        var options = new BackgroundJobServerOptions
        {
            WorkerCount = 5
        };
        var server = new BackgroundJobServer(options);
        yield return server;
    }
The worker count is set just to 5.
There are just 4 jobs and even those are completed (SELECT * FROM [HangFire].[State]):
Do you have any idea why the Hangfire is hitting so many queries at each second?
",<c#><sql-server><backgroundworker><hangfire>,3178,4,37,8900,6,84,109,67,12970,0.0,1086,3,12,2019-05-27 7:46,2019-06-03 8:46,,7.0,,Basic,10
50238568,How to group by time bucket in ClickHouse and fill missing data with nulls/0s,"Suppose I have a given time range. For explanation, let's consider something simple, like whole year 2018. I want to query data from ClickHouse as a sum aggregation for each quarter so the result should be 4 rows. 
The problem is that I have data for only two quarters so when using GROUP BY quarter, only two rows are returned.
SELECT
     toStartOfQuarter(created_at) AS time,
     sum(metric) metric
 FROM mytable
 WHERE
     created_at &gt;= toDate(1514761200) AND created_at &gt;= toDateTime(1514761200)
    AND
     created_at &lt;= toDate(1546210800) AND created_at &lt;= toDateTime(1546210800)
 GROUP BY time
 ORDER BY time
1514761200 – 2018-01-01
1546210800 – 2018-12-31
This returns:
time       metric
2018-01-01 345
2018-04-01 123
And I need:
time       metric
2018-01-01 345
2018-04-01 123
2018-07-01 0
2018-10-01 0
This is simplified example but in real use case the aggregation would be eg. 5 minutes instead of quarters and GROUP BY would have at least one more attribute like GROUP BY attribute1, time so desired result is
time        metric  attribute1
2018-01-01  345     1
2018-01-01  345     2
2018-04-01  123     1
2018-04-01  123     2
2018-07-01  0       1
2018-07-01  0       2
2018-10-01  0       1
2018-10-01  0       2
Is there a way to somehow fill the whole given interval? Like InfluxDB has fill argument for group or TimescaleDb's time_bucket() function with generate_series()  I tried to search ClickHouse documentation and github issues and it seems this is not implemented yet so the question perhaps is whether there's any workaround.
",<sql><clickhouse>,1570,2,36,11928,18,90,141,35,15729,0.0,1945,4,12,2018-05-08 16:46,2018-05-14 8:04,2018-05-14 8:04,6.0,6.0,Basic,2
52186125,Moving Wordpress site to Docker: Error establishing DB connection,"Ive been making new sites with Wordpress &amp; Docker recently and have a reasonable grasp of how it all works and Im now looking to move some established sites into Docker.
Ive been following this guide:
https://stephenafamo.com/blog/moving-wordpress-docker-container/
I have everything setup as it should be but when I go to my domain.com:1234 I get the error message 'Error establishing a database connection'. I have changed 'DB HOST' to 'mysql' in wp-config.php as advised and all the DB details from the site Im bringing in are correct.
I have attached to the mysql container and checked that the db is there and with the right user and also made sure the pw is correct via mysql CLI too.
SELinux is set to permissive and I havent changed any dir/file ownership nor permissions and for the latter dirs are all 755 and files 644 as they should be.
Edit: I should mention that database/data and everything under that seem to be owned by user/group 'polkitd input' instead of root. 
Docker logs aren't really telling me much either apart from the 500 error messages for the WP container when I browse the site on port 1234 (as expected though).
This is the docker-compose file:
version: '2'
services:
  example_db:
    image: mysql:latest
    container_name: example_db
    volumes:
      - ./database/data:/var/lib/mysql
      - ./database/initdb.d:/docker-entrypoint-initdb.d
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password123 # any random string will do
      MYSQL_DATABASE: mydomin_db # the name of your mysql database
      MYSQL_USER: my domain_me # the name of the database user
      MYSQL_PASSWORD: password123 # the password of the mysql user
  example:
    depends_on:
      - example_db
    image: wordpress:php7.1 # we're using the image with php7.1
    container_name: example
    ports:
      - ""1234:80""
    restart: always
    links:
      - example_db:mysql
    volumes:
      - ./src:/var/www/html
Suggestions most welcome as Im out of ideas!
",<mysql><wordpress><docker><docker-compose>,1991,2,28,377,2,8,17,71,34865,0.0,21,8,12,2018-09-05 13:17,2018-09-05 13:59,2018-09-05 13:59,0.0,0.0,Basic,3
58735389,Pyspark SQL Pandas Grouped Map without GroupBy?,"I have a dataset that I want to map over using several Pyspark SQL Grouped Map UDFs, at different stages of a larger ETL process that runs on ephemeral clusters in AWS EMR. The Grouped Map API requires that the Pyspark dataframe be grouped prior to the apply, but I have no need to actually group keys.
At the moment, I'm using an arbitrary grouping, which works, but results in:
An unnecessary shuffle.
Hacky code for an arbitrary groupby in each job.
My ideal solution allows a vectorized Pandas UDF apply without an arbitrary grouping, but if I could save the arbitrary grouping that would at least eliminate the shuffles.
EDIT:
Here's what my code looks like. I was originally using an arbitrary grouping, but am currently trying spark_partition_id() based on a comment below by @pault.
@pandas_udf(b_schema, PandasUDFType.GROUPED_MAP)
def transform(a_partition):
  b = a_partition.drop(""pid"", axis=1)
  # Some other transform stuff
  return b
(sql
  .read.parquet(a_path)
  .withColumn(""pid"", spark_partition_id())
  .groupBy(""pid"")
  .apply(transform)
  .write.parquet(b_path))
Using spark_partition_id() seems to still result in a shuffle. I get the following DAG:
Stage 1
Scan parquet
Project
Project
Exchange
Stage 2
Exchange
Sort
FlatMapGroupsInPandas
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,1262,1,14,1666,0,14,28,79,1863,0.0,162,1,12,2019-11-06 17:16,2020-02-11 15:32,2020-02-11 15:32,97.0,97.0,Intermediate,19
61419449,Unable to Instantiate Python Dataclass (Frozen) inside a Pytest function that uses Fixtures,"I'm following along with Architecture Patterns in Python by Harry Percival and Bob Gregory.
Around chapter three (3) they introduce testing the ORM of SQLAlchemy.
A new test that requires a session fixture, it is throwing AttributeError, FrozenInstanceError due to cannot assign to field '_sa_instance_state'
It may be important to note that other tests do not fail when creating instances of OrderLine, but they do fail if I simply include session into the test parameter(s).
Anyway I'll get straight into the code.
conftest.py
@pytest.fixture
def local_db():
    engine = create_engine('sqlite:///:memory:')
    metadata.create_all(engine)
    return engine
@pytest.fixture
def session(local_db):
    start_mappers()
    yield sessionmaker(bind=local_db)()
    clear_mappers()
model.py
@dataclass(frozen=True)
class OrderLine:
    id: str
    sku: str
    quantity: int
test_orm.py
def test_orderline_mapper_can_load_lines(session):
    session.execute(
        'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
        '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
        '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
        '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
    )
    expected = [
        model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
        model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
        model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
    ]
    assert session.query(model.OrderLine).all() == expected
Console error for pipenv run pytest test_orm.py
============================= test session starts =============================
platform linux -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/[redacted]/Documents/architecture-patterns-python
collected 1 item                                                              
test_orm.py F                                                           [100%]
================================== FAILURES ===================================
____________________ test_orderline_mapper_can_load_lines _____________________
session = &lt;sqlalchemy.orm.session.Session object at 0x7fd919ac5bd0&gt;
    def test_orderline_mapper_can_load_lines(session):
        session.execute(
            'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
            '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
            '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
            '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
        )
        expected = [
&gt;           model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
            model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
            model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
        ]
test_orm.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
&lt;string&gt;:2: in __init__
    ???
../../.local/share/virtualenvs/architecture-patterns-python-Qi2y0bev/lib64/python3.7/site-packages/sqlalchemy/orm/instrumentation.py:377: in _new_state_if_none
    self._state_setter(instance, state)
&lt;string&gt;:1: in set
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;[AttributeError(&quot;'OrderLine' object has no attribute '_sa_instance_state'&quot;) raised in repr()] OrderLine object at 0x7fd919a8cf50&gt;
name = '_sa_instance_state'
value = &lt;sqlalchemy.orm.state.InstanceState object at 0x7fd9198f7490&gt;
&gt;   ???
E   dataclasses.FrozenInstanceError: cannot assign to field '_sa_instance_state'
&lt;string&gt;:4: FrozenInstanceError
=========================== short test summary info ===========================
FAILED test_orm.py::test_orderline_mapper_can_load_lines - dataclasses.Froze...
============================== 1 failed in 0.06s ==============================
Additional Questions
I understand the overlying logic and what these files are doing, but correct my if my rudimentary understanding is lacking.
conftest.py (used for all pytest config) is setting up a session fixture, which basically sets up a temporary database in memory - using start and clear mappers to ensure that the orm model definitions are binding to the db isntance.
model.py simply a dataclass used to represent an atomic OrderLine object.
test_orm.py class for pytest to supply the session fixture, in order to setup, execute, teardown a db explicitly for the purpose of running tests.
Issue resolution provided by https://github.com/cosmicpython/code/issues/17
",<python><sqlalchemy><pytest><python-dataclasses>,4540,2,89,169,0,0,9,65,2442,0.0,20,2,12,2020-04-25 0:23,2021-04-04 13:22,2021-04-04 13:22,344.0,344.0,Advanced,41
