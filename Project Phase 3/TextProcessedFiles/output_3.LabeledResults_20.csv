QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
52827853,Type double does not exist in PostgreSQL,"I have table like this:
CREATE TABLE workingtime_times
(
  workingno serial NOT NULL,
  checktime character(6) NOT NULL,
  timeoffset double precision DEFAULT 9
)
I create function like this:
CREATE OR REPLACE FUNCTION MyFName()
    RETURNS TABLE(
        CheckTime character varying,
        TimeOffset double 
    ) AS
$BODY$
BEGIN 
    RETURN QUERY 
    SELECT  t.CheckTime, t.TimeOffset
    FROM WorkingTime_Times t
    ORDER BY t.WorkingNo DESC
    limit 1;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION MyFName()
  OWNER TO postgres;
It make an error like this: 
  type double does not exist
Why we can create an table with column datatype double but return in the function fail. What type we can return in this case?
",<postgresql>,757,0,25,2914,3,19,43,75,16874,,417,1,13,2018-10-16 4:11,2018-10-16 4:21,2018-10-16 4:21,0.0,0.0,Intermediate,15,"<postgresql>, Type double does not exist in PostgreSQL, I have table like this:
CREATE TABLE workingtime_times
(
  workingno serial NOT NULL,
  checktime character(6) NOT NULL,
  timeoffset double precision DEFAULT 9
)
I create function like this:
CREATE OR REPLACE FUNCTION MyFName()
    RETURNS TABLE(
        CheckTime character varying,
        TimeOffset double 
    ) AS
$BODY$
BEGIN 
    RETURN QUERY 
    SELECT  t.CheckTime, t.TimeOffset
    FROM WorkingTime_Times t
    ORDER BY t.WorkingNo DESC
    limit 1;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION MyFName()
  OWNER TO postgres;
It make an error like this: 
  type double does not exist
Why we can create an table with column datatype double but return in the function fail. What type we can return in this case?
","<postgresql>, type doubt exist postgresql, table like this: great table workingtime_tim ( working aerial null, checktim character(6) null, timeoffset doubt precise default 9 ) great function like this: great replace function myfname() return table( checktim character varying, timeoffset doubt ) $body$ begin return query select t.checktime, t.timeoffset workingtime_tim order t.working desk limit 1; end; $body$ language plpgsql volatile cost 100 row 1000; alter function myfname() owner postures; make error like this: type doubt exist great table column datatyp doubt return function fail. type return case?"
57078064,How to POST relation in Strapi,"I'm trying to do a POST to the Strapi API and can't seem to figure out how to attach a 'has and belongs to many' (many to many) relationship.
I've already tried the following body's:
events: [&quot;ID&quot;, &quot;ID&quot;]
name: &quot;name&quot;
&amp;
events: [ID, ID]
name: &quot;name&quot;
Which regarding the docs should be right, I think.
There's no error, I get a '200 OK' response. It adds the record but without the relations.
Event.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;events&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;event&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;artists&quot;: {
      &quot;collection&quot;: &quot;artist&quot;,
      &quot;via&quot;: &quot;events&quot;,
      &quot;dominant&quot;: true
    }
  }
}
Artist.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;artists&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;artist&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;required&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;events&quot;: {
      &quot;collection&quot;: &quot;event&quot;,
      &quot;via&quot;: &quot;artists&quot;
    }
  }
}
I'm using the standard SQLite database, strapi version 3.0.0-beta.13 and tried the request through Postman, HTML &amp; curl.
I would love to know how to attach the relation on POST
Update 23-07:
Did a fresh install of Strapi and now everything is working.
",<javascript><sqlite><strapi>,2083,1,56,213,2,3,12,80,17801,0.0,8,3,13,2019-07-17 14:18,2019-07-18 12:48,,1.0,,Advanced,33,"<javascript><sqlite><strapi>, How to POST relation in Strapi, I'm trying to do a POST to the Strapi API and can't seem to figure out how to attach a 'has and belongs to many' (many to many) relationship.
I've already tried the following body's:
events: [&quot;ID&quot;, &quot;ID&quot;]
name: &quot;name&quot;
&amp;
events: [ID, ID]
name: &quot;name&quot;
Which regarding the docs should be right, I think.
There's no error, I get a '200 OK' response. It adds the record but without the relations.
Event.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;events&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;event&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;artists&quot;: {
      &quot;collection&quot;: &quot;artist&quot;,
      &quot;via&quot;: &quot;events&quot;,
      &quot;dominant&quot;: true
    }
  }
}
Artist.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;artists&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;artist&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;required&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;events&quot;: {
      &quot;collection&quot;: &quot;event&quot;,
      &quot;via&quot;: &quot;artists&quot;
    }
  }
}
I'm using the standard SQLite database, strapi version 3.0.0-beta.13 and tried the request through Postman, HTML &amp; curl.
I would love to know how to attach the relation on POST
Update 23-07:
Did a fresh install of Strapi and now everything is working.
","<javascript><quite><strap>, post relate strap, i'm try post strap apt can't seem figure attach 'ha belong many' (man many) relationship. i'v already try follow body's: events: [&quit;id&quit;, &quit;id&quit;] name: &quit;name&quit; &amp; events: [id, id] name: &quit;name&quit; regard do right, think. there' error, get '200 ok' response. add record without relations. event.settings.son: { &quit;connection&quit;: &quit;default&quit;, &quit;collectionname&quit;: &quit;events&quit;, &quit;into&quit;: { &quit;name&quit;: &quit;event&quit;, &quit;description&quit;: &quit;&quit; }, &quit;option&quit;: { &quit;increments&quit;: true, &quit;timestamps&quit;: [ &quit;created_at&quit;, &quit;updated_at&quit; ], &quit;comment&quit;: &quit;&quit; }, &quit;attributes&quit;: { &quit;name&quit;: { &quit;type&quit;: &quit;string&quit; }, &quit;artists&quit;: { &quit;collection&quit;: &quit;artist&quit;, &quit;via&quit;: &quit;events&quit;, &quit;dominant&quit;: true } } } artist.settings.son: { &quit;connection&quit;: &quit;default&quit;, &quit;collectionname&quit;: &quit;artists&quit;, &quit;into&quit;: { &quit;name&quit;: &quit;artist&quit;, &quit;description&quit;: &quit;&quit; }, &quit;option&quit;: { &quit;increments&quit;: true, &quit;timestamps&quit;: [ &quit;created_at&quit;, &quit;updated_at&quit; ], &quit;comment&quit;: &quit;&quit; }, &quit;attributes&quit;: { &quit;name&quit;: { &quit;required&quit;: true, &quit;type&quit;: &quit;string&quit; }, &quit;events&quit;: { &quit;collection&quit;: &quit;event&quit;, &quit;via&quit;: &quit;artists&quit; } } } i'm use standard quite database, strap version 3.0.0-beta.13 try request potman, html &amp; curl. would love know attach relate post update 23-07: fresh instal strap every working."
50952467,COLLATE=utf8_unicode_ci getting removed from schema.rb after migrate,"Running rails 5.0.2
The tables in our schema.rb in source control seem to mostly have the format: 
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci"" 
do |t|
Note the COLLATE=utf8_unicode_ci"" at the end.
When I run migrations the generated schemaa.rb is mostly the same but chops off COLLATE=utf8_unicode_ci"" from those lines so it now looks like:
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8"" 
do |t|
Based on other SO posts I've tried two things to fix this
1) in my /etc/mysql/my.cnf, I added:
[mysqld]
character-set-server  = utf8
collation-server = utf8_unicode_ci
2) in my database.yml ive added collation: utf8_general_ci to all of the relevant environments
I then restarted mysql, dropped, created and migrated my db but still the collate line disappears.
Any thoughts on what configuration I need to change to have that bit autogenerated ?
",<mysql><sql><ruby-on-rails><rails-migrations>,974,0,17,953,0,11,27,75,3573,,61,2,13,2018-06-20 15:58,2019-09-24 14:58,,461.0,,Advanced,34,"<mysql><sql><ruby-on-rails><rails-migrations>, COLLATE=utf8_unicode_ci getting removed from schema.rb after migrate, Running rails 5.0.2
The tables in our schema.rb in source control seem to mostly have the format: 
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci"" 
do |t|
Note the COLLATE=utf8_unicode_ci"" at the end.
When I run migrations the generated schemaa.rb is mostly the same but chops off COLLATE=utf8_unicode_ci"" from those lines so it now looks like:
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8"" 
do |t|
Based on other SO posts I've tried two things to fix this
1) in my /etc/mysql/my.cnf, I added:
[mysqld]
character-set-server  = utf8
collation-server = utf8_unicode_ci
2) in my database.yml ive added collation: utf8_general_ci to all of the relevant environments
I then restarted mysql, dropped, created and migrated my db but still the collate line disappears.
Any thoughts on what configuration I need to change to have that bit autogenerated ?
","<myself><sal><ruby-on-rails><rails-migrations>, collar=utf8_unicode_ci get remove scheme.re migrate, run rail 5.0.2 table scheme.re source control seem mostly format: greatest ""app_files"", force: :cascade, option: ""engine=innodb default charge=utf collar=utf8_unicode_ci"" |t| note collar=utf8_unicode_ci"" end. run migrate genet scheme.re mostly chop collar=utf8_unicode_ci"" line look like: greatest ""app_files"", force: :cascade, option: ""engine=innodb default charge=utf"" |t| base post i'v try two thing fix 1) /etc/myself/my.cf, added: [myself] character-set-serve = utf collection-serve = utf8_unicode_ci 2) database.you give ad collection: utf8_general_ci rule environs start myself, dropped, great migrate do still collar line disappears. thought configur need change bit autogener ?"
50745313,Check constraint that calls function does not work on update,"I created a constraint that prevents allocations in one table to exceed inventory in another table (see the details in my previous question). But for some reason the constraint works as expected only when I insert new allocations, but on update it does not prevent violating.
Here is my constraint:
([dbo].[fn_AllocationIsValid]([Itemid]) = 1)
And here is the function:
CREATE FUNCTION [dbo].[fn_AllocationIsValid] (@itemId as int)  
RETURNS int  AS  
BEGIN 
DECLARE @isValid bit;
SELECT @isValid = CASE WHEN ISNULL(SUM(Allocation), 0) &lt;= MAX(Inventory) THEN 1 ELSE 0 END
FROM Allocations A 
JOIN Items I ON I.Id = A.ItemId
WHERE I.Id = @itemId
GROUP BY I.Id;
RETURN @isValid;
END
Updated
Here are my tables:
CREATE TABLE [allocations] (
[userID] [bigint] NOT NULL ,
[itemID] [int] NOT NULL ,
[allocation] [bigint] NOT NULL ,
CONSTRAINT [PK_allocations] PRIMARY KEY  CLUSTERED 
(
    [userID],
    [itemID]
)  ON [PRIMARY] ,
CONSTRAINT [FK_allocations_items] FOREIGN KEY 
(
    [itemID]
) REFERENCES [items] (
    [id]
) ON DELETE CASCADE  ON UPDATE CASCADE ,
CONSTRAINT [CK_allocations] CHECK ([dbo].[fn_AllocationIsValid]([Itemid], [Allocation]) = 1)
) ON [PRIMARY]
CREATE TABLE [dbo].[Items](
[Id] [int] NOT NULL,
[Inventory] [int] NOT NULL
) ON [PRIMARY]
GO
INSERT INTO Items (Id, Inventory) VALUES (2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(4340, 2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(5895, 2692, 0)
The following statement execution should fail, but it does not:
update allocations set allocation = 5
where userid = 5895 and itemid = 2692
",<sql-server><constraints>,1610,1,44,5121,11,60,108,36,3505,0.0,244,1,13,2018-06-07 15:42,2018-06-07 18:06,2018-06-07 18:06,0.0,0.0,Advanced,33,"<sql-server><constraints>, Check constraint that calls function does not work on update, I created a constraint that prevents allocations in one table to exceed inventory in another table (see the details in my previous question). But for some reason the constraint works as expected only when I insert new allocations, but on update it does not prevent violating.
Here is my constraint:
([dbo].[fn_AllocationIsValid]([Itemid]) = 1)
And here is the function:
CREATE FUNCTION [dbo].[fn_AllocationIsValid] (@itemId as int)  
RETURNS int  AS  
BEGIN 
DECLARE @isValid bit;
SELECT @isValid = CASE WHEN ISNULL(SUM(Allocation), 0) &lt;= MAX(Inventory) THEN 1 ELSE 0 END
FROM Allocations A 
JOIN Items I ON I.Id = A.ItemId
WHERE I.Id = @itemId
GROUP BY I.Id;
RETURN @isValid;
END
Updated
Here are my tables:
CREATE TABLE [allocations] (
[userID] [bigint] NOT NULL ,
[itemID] [int] NOT NULL ,
[allocation] [bigint] NOT NULL ,
CONSTRAINT [PK_allocations] PRIMARY KEY  CLUSTERED 
(
    [userID],
    [itemID]
)  ON [PRIMARY] ,
CONSTRAINT [FK_allocations_items] FOREIGN KEY 
(
    [itemID]
) REFERENCES [items] (
    [id]
) ON DELETE CASCADE  ON UPDATE CASCADE ,
CONSTRAINT [CK_allocations] CHECK ([dbo].[fn_AllocationIsValid]([Itemid], [Allocation]) = 1)
) ON [PRIMARY]
CREATE TABLE [dbo].[Items](
[Id] [int] NOT NULL,
[Inventory] [int] NOT NULL
) ON [PRIMARY]
GO
INSERT INTO Items (Id, Inventory) VALUES (2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(4340, 2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(5895, 2692, 0)
The following statement execution should fail, but it does not:
update allocations set allocation = 5
where userid = 5895 and itemid = 2692
","<sal-server><constraint>, check constraint call function work update, great constraint prevent allow one table exceed inventor not table (see detail previous question). reason constraint work expect insert new allocation, update prevent violating. constraint: ([do].[fn_allocationisvalid]([timid]) = 1) function: great function [do].[fn_allocationisvalid] (@timid in) return in begin declare @invalid bit; select @invalid = case skull(sum(allocation), 0) &it;= max(inventor) 1 else 0 end allow join item i.id = a.timid i.id = @timid group i.id; return @invalid; end update tables: great table [allocation] ( [used] [begin] null , [timid] [in] null , [allocation] [begin] null , constraint [pk_allocations] primary key cluster ( [used], [timid] ) [primary] , constraint [fk_allocations_items] foreign key ( [timid] ) refer [items] ( [id] ) delete cascade update cascade , constraint [ck_allocations] check ([do].[fn_allocationisvalid]([timid], [allocation]) = 1) ) [primary] great table [do].[items]( [id] [in] null, [inventor] [in] null ) [primary] go insert item (id, inventor) value (2692, 336) insert allow (used, timid, allocation) values(4340, 2692, 336) insert allow (used, timid, allocation) values(5895, 2692, 0) follow statement execute fail, not: update allow set allow = 5 used = 5895 timid = 2692"
59540432,How to Mock postgresql (pg) in node.js using jest,"I am new in node.js. I am writing code in node.js for postgresql using pg and pg-native for serverless app. I need to write unit test for it. I am unable to mock pg client using jest or sinon. 
My actual code is like this
const { Client } = require('pg');
export const getAlerts = async (event, context) =&gt; {
  const client = new Client({
    user: process.env.DB_USER,
    host: process.env.DB_HOST,
    database: process.env.DB_DATABASE,
    password: process.env.DB_PASSWORD,
    port: process.env.PORT
  });
  await client.connect();
  try {
    const result = await client.query(`SELECT * FROM public.alerts;`);
    console.log(result.rows);
    client.end();
    return success({ message: `${result.rowCount} item(s) returned`, data: result.rows, status: true });
  } catch (e) {
    console.error(e.stack);
    client.end();
    return failure({ message: e, status: false });
  }
};
How to mock pg client here?
",<node.js><postgresql><unit-testing><jestjs><sinon>,921,0,30,1145,7,17,44,44,33648,0.0,34,4,13,2019-12-31 6:28,2019-12-31 15:17,2019-12-31 15:17,0.0,0.0,Advanced,32,"<node.js><postgresql><unit-testing><jestjs><sinon>, How to Mock postgresql (pg) in node.js using jest, I am new in node.js. I am writing code in node.js for postgresql using pg and pg-native for serverless app. I need to write unit test for it. I am unable to mock pg client using jest or sinon. 
My actual code is like this
const { Client } = require('pg');
export const getAlerts = async (event, context) =&gt; {
  const client = new Client({
    user: process.env.DB_USER,
    host: process.env.DB_HOST,
    database: process.env.DB_DATABASE,
    password: process.env.DB_PASSWORD,
    port: process.env.PORT
  });
  await client.connect();
  try {
    const result = await client.query(`SELECT * FROM public.alerts;`);
    console.log(result.rows);
    client.end();
    return success({ message: `${result.rowCount} item(s) returned`, data: result.rows, status: true });
  } catch (e) {
    console.error(e.stack);
    client.end();
    return failure({ message: e, status: false });
  }
};
How to mock pg client here?
","<node.is><postgresql><unit-testing><jests><simon>, mock postgresql (pg) node.j use jest, new node.is. write code node.j postgresql use pg pg-native serverless pp. need write unit test it. unable mock pg client use jest simon. actual code like cost { client } = require('pg'); export cost getalert = async (event, context) =&it; { cost client = new client({ user: process.end.db_user, host: process.end.db_host, database: process.end.db_database, password: process.end.db_password, port: process.end.port }); await client.connect(); try { cost result = await client.query(`select * public.alert;`); console.log(result.rows); client.end(); return success({ message: `${result.rowcount} item(s) returned`, data: result.rows, status: true }); } catch (e) { console.error(e.stick); client.end(); return failure({ message: e, status: fall }); } }; mock pg client here?"
56977209,Azure Function creating too many connections to PostgreSQL,"I have an Azure Durable Function that interacts with a PostgreSQL database, also hosted in Azure.
The PostgreSQL database has a connection limit of 50, and furthermore, my connection string limits the connection pool size to 40, leaving space for super user / admin connections.
Nonetheless, under some loads I get the error
  53300: remaining connection slots are reserved for non-replication superuser connections
This documentation from Microsoft seemed relevant, but it doesn't seem like I can make a static client, and, as it mentions, 
  because you can still run out of connections, you should optimize connections to the database.
I have this method
private IDbConnection GetConnection()
{
    return new NpgsqlConnection(Environment.GetEnvironmentVariable(""PostgresConnectionString""));
}
and when I want to interact with PostgreSQL I do like this
using (var connection = GetConnection())
{
    connection.Open();
    return await connection.QuerySingleAsync&lt;int&gt;(settings.Query().Insert, settings);
}
So I am creating (and disposing) lots of NpgsqlConnection objects, but according to this, that should be fine because connection pooling is handled behind the scenes. But there may be something about Azure Functions that invalidates this thinking.
I have noticed that I end up with a lot of idle connections (from pgAdmin):
Based on that I've tried fiddling with Npgsql connection parameters like Connection Idle Lifetime, Timeout, and Pooling, but the problem of too many connections seems to persist to one degree or another. Additionally I've tried limiting the number of concurrent orchestrator and activity functions (see this doc), but that seems to partially defeat the purpose of Azure Functions being scalable. It does help - I get fewer of the too many connections error). Presumably If I keep testing it with lower numbers I may even eliminate it, but again, that seems like it defeats the point, and there may be another solution.
How can I use PostgreSQL with Azure Functions without maxing out connections?
",<postgresql><azure><azure-functions><npgsql><azure-durable-functions>,2037,5,13,6482,7,43,95,71,3816,0.0,1337,3,13,2019-07-10 19:10,2019-07-11 7:24,,1.0,,Advanced,33,"<postgresql><azure><azure-functions><npgsql><azure-durable-functions>, Azure Function creating too many connections to PostgreSQL, I have an Azure Durable Function that interacts with a PostgreSQL database, also hosted in Azure.
The PostgreSQL database has a connection limit of 50, and furthermore, my connection string limits the connection pool size to 40, leaving space for super user / admin connections.
Nonetheless, under some loads I get the error
  53300: remaining connection slots are reserved for non-replication superuser connections
This documentation from Microsoft seemed relevant, but it doesn't seem like I can make a static client, and, as it mentions, 
  because you can still run out of connections, you should optimize connections to the database.
I have this method
private IDbConnection GetConnection()
{
    return new NpgsqlConnection(Environment.GetEnvironmentVariable(""PostgresConnectionString""));
}
and when I want to interact with PostgreSQL I do like this
using (var connection = GetConnection())
{
    connection.Open();
    return await connection.QuerySingleAsync&lt;int&gt;(settings.Query().Insert, settings);
}
So I am creating (and disposing) lots of NpgsqlConnection objects, but according to this, that should be fine because connection pooling is handled behind the scenes. But there may be something about Azure Functions that invalidates this thinking.
I have noticed that I end up with a lot of idle connections (from pgAdmin):
Based on that I've tried fiddling with Npgsql connection parameters like Connection Idle Lifetime, Timeout, and Pooling, but the problem of too many connections seems to persist to one degree or another. Additionally I've tried limiting the number of concurrent orchestrator and activity functions (see this doc), but that seems to partially defeat the purpose of Azure Functions being scalable. It does help - I get fewer of the too many connections error). Presumably If I keep testing it with lower numbers I may even eliminate it, but again, that seems like it defeats the point, and there may be another solution.
How can I use PostgreSQL with Azure Functions without maxing out connections?
","<postgresql><azure><azure-functions><npgsql><azure-unable-functions>, azur function great man connect postgresql, azur rural function interact postgresql database, also host azure. postgresql database connect limit 50, furthermore, connect string limit connect pool size 40, leave space super user / admit connections. nonetheless, load get error 53300: remain connect slot reserve non-reply humerus connect document microsoft seem relevant, seem like make static client, and, mentions, still run connections, optic connect database. method privat idbconnect getconnection() { return new npgsqlconnection(environment.getenvironmentvariable(""postgresconnectionstring"")); } want interact postgresql like use (war connect = getconnection()) { connection.open(); return await connection.querysingleasync&it;in&it;(settings.query().insert, settings); } great (and disposing) lot npgsqlconnect objects, accord this, fine connect pool hand behind scenes. may cometh azur function invalid thinking. notice end lot ill connect (from pgadmin): base i'v try fiddle npgsql connect parapet like connect ill lifetime, timeout, pooling, problem man connect seem persist one degree another. admit i'v try limit number concur orchestra active function (see do), seem partial defeat purpose azur function capable. help - get fewer man connect error). presume keep test lower number may even limit it, again, seem like defeat point, may not solution. use postgresql azur function without max connections?"
52963079,"I can't start server PostgreSQL 11: ""pg_ctl: could not start server""","I am in CentOS Linux release 7.5.1804 (Core)
When I login as postgres and run:
bash-4.2$ /usr/pgsql-11/bin/initdb -D /var/lib/pgsql/11/data
The files belonging to this database system will be owned by user ""postgres"".
This user must also own the server process.
The database cluster will be initialized with locale ""en_US.UTF-8"".
The default database encoding has accordingly been set to ""UTF8"".
The default text search configuration will be set to ""english"".
Data page checksums are disabled.
creating directory /var/lib/pgsql/11/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
WARNING: enabling ""trust"" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    /usr/pgsql-11/bin/pg_ctl -D /var/lib/pgsql/11/data -l logfile start
bash-4.2$
then I run
bash-4.2$ /usr/pgsql-11/bin/pg_ctl start -l logfile -D /var/lib/pgsql/11/data
waiting for server to start..../bin/sh: logfile: Permission denied
 stopped waiting
pg_ctl: could not start server
Examine the log output.
bash-4.2$ date
Wed Oct 24 01:50:44 -05 2018
bash-4.2$
I search in GOOGLE for ""waiting for server to start..../bin/sh: logfile: Permission denied"" but this error only happened in MAC and no solutions is displayed...
Also I run
bash-4.2$ postgres --version;postmaster --version;
postgres (PostgreSQL) 11.0
postgres (PostgreSQL) 11.0
bash-4.2$
then I believe PostgreSQL 11 is fine installed, but I can't start server.
I install with this line:
yum install postgresql-jdbc.noarch postgresql-jdbc-javadoc.noarch postgresql-unit11.x86_64 postgresql-unit11-debuginfo.x86_64 postgresql11.x86_64 postgresql11-contrib.x86_64 postgresql11-debuginfo.x86_64 postgresql11-devel.x86_64 postgresql11-docs.x86_64 postgresql11-libs.x86_64 postgresql11-odbc.x86_64 postgresql11-plperl.x86_64 postgresql11-plpython.x86_64 postgresql11-pltcl.x86_64 postgresql11-server.x86_64 postgresql11-tcl.x86_64 postgresql11-test.x86_64
I didn't add [postgresql11-llvmjit.x86_64] because this requires many dependences.
CentOS Linux release 7.5.1804 + PostgreSQL 11 ?
Do I need install aditional software?
",<postgresql><centos7><postgresql-11>,2490,0,43,211,1,3,13,49,46847,0.0,11,4,13,2018-10-24 7:20,2018-10-24 17:07,,0.0,,Advanced,32,"<postgresql><centos7><postgresql-11>, I can't start server PostgreSQL 11: ""pg_ctl: could not start server"", I am in CentOS Linux release 7.5.1804 (Core)
When I login as postgres and run:
bash-4.2$ /usr/pgsql-11/bin/initdb -D /var/lib/pgsql/11/data
The files belonging to this database system will be owned by user ""postgres"".
This user must also own the server process.
The database cluster will be initialized with locale ""en_US.UTF-8"".
The default database encoding has accordingly been set to ""UTF8"".
The default text search configuration will be set to ""english"".
Data page checksums are disabled.
creating directory /var/lib/pgsql/11/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
WARNING: enabling ""trust"" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    /usr/pgsql-11/bin/pg_ctl -D /var/lib/pgsql/11/data -l logfile start
bash-4.2$
then I run
bash-4.2$ /usr/pgsql-11/bin/pg_ctl start -l logfile -D /var/lib/pgsql/11/data
waiting for server to start..../bin/sh: logfile: Permission denied
 stopped waiting
pg_ctl: could not start server
Examine the log output.
bash-4.2$ date
Wed Oct 24 01:50:44 -05 2018
bash-4.2$
I search in GOOGLE for ""waiting for server to start..../bin/sh: logfile: Permission denied"" but this error only happened in MAC and no solutions is displayed...
Also I run
bash-4.2$ postgres --version;postmaster --version;
postgres (PostgreSQL) 11.0
postgres (PostgreSQL) 11.0
bash-4.2$
then I believe PostgreSQL 11 is fine installed, but I can't start server.
I install with this line:
yum install postgresql-jdbc.noarch postgresql-jdbc-javadoc.noarch postgresql-unit11.x86_64 postgresql-unit11-debuginfo.x86_64 postgresql11.x86_64 postgresql11-contrib.x86_64 postgresql11-debuginfo.x86_64 postgresql11-devel.x86_64 postgresql11-docs.x86_64 postgresql11-libs.x86_64 postgresql11-odbc.x86_64 postgresql11-plperl.x86_64 postgresql11-plpython.x86_64 postgresql11-pltcl.x86_64 postgresql11-server.x86_64 postgresql11-tcl.x86_64 postgresql11-test.x86_64
I didn't add [postgresql11-llvmjit.x86_64] because this requires many dependences.
CentOS Linux release 7.5.1804 + PostgreSQL 11 ?
Do I need install aditional software?
","<postgresql><cents><postgresql-11>, can't start server postgresql 11: ""pg_ctl: could start server"", cent line release 7.5.1804 (core) login poster run: base-4.2$ /us/pgsql-11/bin/initdb -d /war/limb/pgsql/11/data file belong database system own user ""postures"". user must also server process. database cluster into local ""ends.utf-8"". default database end accordingly set ""utf"". default text search configur set ""english"". data page checks disabled. great director /war/limb/pgsql/11/data ... ok great subdirectori ... ok select default max_connect ... 100 select default shared_buff ... 128mb select dream share memory implement ... six great configur file ... ok run bootstrap script ... ok perform post-bootstrap into ... ok son data disk ... ok warning: enable ""trust"" authentic local connect change edit pg_hba.cone use option -a, --auto-low --auto-host, next time run initdb. success. start database server using: /us/pgsql-11/bin/pg_ctl -d /war/limb/pgsql/11/data -l logic start base-4.2$ run base-4.2$ /us/pgsql-11/bin/pg_ctl start -l logic -d /war/limb/pgsql/11/data wait server start..../bin/s: logfile: permits den stop wait pg_ctl: could start server examine log output. base-4.2$ date wed oct 24 01:50:44 -05 2018 base-4.2$ search good ""wait server start..../bin/s: logfile: permits denied"" error happen mac slut displayed... also run base-4.2$ poster --version;postmaster --version; poster (postgresql) 11.0 poster (postgresql) 11.0 base-4.2$ believe postgresql 11 fine installed, can't start server. instal line: sum instal postgresql-job.north postgresql-job-javadoc.north postgresql-united.x86_64 postgresql-united-debuginfo.x86_64 postgresql11.x86_64 postgresql11-control.x86_64 postgresql11-debuginfo.x86_64 postgresql11-devil.x86_64 postgresql11-docs.x86_64 postgresql11-lips.x86_64 postgresql11-duc.x86_64 postgresql11-paper.x86_64 postgresql11-plpython.x86_64 postgresql11-place.x86_64 postgresql11-server.x86_64 postgresql11-til.x86_64 postgresql11-test.x86_64 add [postgresql11-llvmjit.x86_64] require man dependence. cent line release 7.5.1804 + postgresql 11 ? need instal admit software?"
50451566,SQL Server In Memory Equivalent,"My team has recently decided to move from EF to Dapper. As such we are moving a lot of the logic that was done in EF into Stored Procedures as part of our SQL Server DB. This means that a lot of the Unit Tests that we have for EF are now Integration Level tests as they involve the DB. I am looking for a way to run these tests using an In-Memory DB so I don't have to stand up a DB externally as part of the tests. I looked into SQLite but without the SP support, it would not be a fair comparison. Are there any other In-Memory DBs that would be similar to SQL Server that can be used for testing?
",<sql-server><entity-framework><integration-testing><dapper>,600,0,0,733,1,8,20,59,7632,0.0,49,1,13,2018-05-21 15:01,2018-05-21 15:50,,0.0,,Advanced,34,"<sql-server><entity-framework><integration-testing><dapper>, SQL Server In Memory Equivalent, My team has recently decided to move from EF to Dapper. As such we are moving a lot of the logic that was done in EF into Stored Procedures as part of our SQL Server DB. This means that a lot of the Unit Tests that we have for EF are now Integration Level tests as they involve the DB. I am looking for a way to run these tests using an In-Memory DB so I don't have to stand up a DB externally as part of the tests. I looked into SQLite but without the SP support, it would not be a fair comparison. Are there any other In-Memory DBs that would be similar to SQL Server that can be used for testing?
","<sal-server><entity-framework><integration-testing><damper>, sal server memory equivalent, team recent decide move of damper. move lot logic done of store procedure part sal server do. mean lot unit test of inter level test involve do. look way run test use in-memory do stand do externa part tests. look quite without s support, would fair comparison. in-memory do would similar sal server use testing?"
51267470,"Unhandled rejection SequelizeDatabaseError: relation ""users"" does not exist","I am getting started with Sequelize.  I am following the documentation they are providing on their website :http://docs.sequelizejs.com/manual/installation/getting-started.html 
const Sequelize = require('sequelize');
const sequelize = new Sequelize('haha', 'postgres', 'postgres', {
  host: 'localhost',
  dialect: 'postgres',
  operatorsAliases: false,
  pool: {
    max: 5,
    min: 0,
    acquire: 30000,
    idle: 10000
  },
  // SQLite only
  storage: 'path/to/database.sqlite'
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
  const User = sequelize.define('user', {
    firstName: {
      type: Sequelize.STRING
    },
    lastName: {
      type: Sequelize.STRING
    }
  });
  // force: true will drop the table if it already exists
  User.sync({force: true}).then(() =&gt; {
    // Table created
    return User.create({
      firstName: 'John',
      lastName: 'Hancock'
    });
  });
Up until here, everything works perfectly. And the table ""user"" is correctly built and populated. (Although I do not understand Sequelize appends an ""s"" automatically to ""user"", any explanation.)
However when I add the following portion of code:
User.findAll().then(users =&gt; {
  console.log(users)
})
I get this error :  
  Unhandled rejection SequelizeDatabaseError: relation ""users"" does not
  exist
So my questions are:  
Why does Sequelize add an ""s"" to user. (I know it makes sense but shouldn't the developer decide that)  
What is causing that error? I followed the documentation but it still didn't work?
",<javascript><node.js><postgresql><sequelize.js><pgadmin>,1675,4,48,1743,6,33,77,47,48448,0.0,1026,8,13,2018-07-10 14:11,2018-09-11 2:08,,63.0,,Advanced,33,"<javascript><node.js><postgresql><sequelize.js><pgadmin>, Unhandled rejection SequelizeDatabaseError: relation ""users"" does not exist, I am getting started with Sequelize.  I am following the documentation they are providing on their website :http://docs.sequelizejs.com/manual/installation/getting-started.html 
const Sequelize = require('sequelize');
const sequelize = new Sequelize('haha', 'postgres', 'postgres', {
  host: 'localhost',
  dialect: 'postgres',
  operatorsAliases: false,
  pool: {
    max: 5,
    min: 0,
    acquire: 30000,
    idle: 10000
  },
  // SQLite only
  storage: 'path/to/database.sqlite'
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
  const User = sequelize.define('user', {
    firstName: {
      type: Sequelize.STRING
    },
    lastName: {
      type: Sequelize.STRING
    }
  });
  // force: true will drop the table if it already exists
  User.sync({force: true}).then(() =&gt; {
    // Table created
    return User.create({
      firstName: 'John',
      lastName: 'Hancock'
    });
  });
Up until here, everything works perfectly. And the table ""user"" is correctly built and populated. (Although I do not understand Sequelize appends an ""s"" automatically to ""user"", any explanation.)
However when I add the following portion of code:
User.findAll().then(users =&gt; {
  console.log(users)
})
I get this error :  
  Unhandled rejection SequelizeDatabaseError: relation ""users"" does not
  exist
So my questions are:  
Why does Sequelize add an ""s"" to user. (I know it makes sense but shouldn't the developer decide that)  
What is causing that error? I followed the documentation but it still didn't work?
","<javascript><node.is><postgresql><sequelae.is><pgadmin>, unhandl reject sequelizedatabaseerror: relate ""users"" exist, get start sequelae. follow document proved west :http://docs.sequelizejs.com/manual/installation/getting-started.html cost sequel = require('sequelae'); cost sequel = new sequelae('hata', 'postures', 'postures', { host: 'localhost', dialect: 'postures', operatorsaliases: false, pool: { max: 5, min: 0, acquire: 30000, idle: 10000 }, // quite storage: 'path/to/database.quite' }); sequel .authenticity() .then(() =&it; { console.log('connect establish successfully.'); }) .catch(err =&it; { console.error('un connect database:', err); }); cost user = sequelae.define('user', { firstname: { type: sequelae.sir }, lastname: { type: sequelae.sir } }); // force: true drop table already exist user.son({force: true}).then(() =&it; { // table great return user.create({ firstname: 'john', lastname: 'hancocks' }); }); here, every work perfectly. table ""user"" correctly built populated. (although understand sequel happened ""s"" automatic ""user"", explanation.) howe add follow portion code: user.finally().then(us =&it; { console.log(users) }) get error : unhandl reject sequelizedatabaseerror: relate ""users"" exist question are: sequel add ""s"" user. (i know make sens develop decide that) cause error? follow document still work?"
55538252,How to create a database backup in DBeaver and restore it?,"I need to create a SQL server database backup in DBeaver and restore it. Is that possible?
Using SQL Management Studio would not be a turnaround solution in this case, as we are not allowed to use it here.
",<sql-server><dbeaver>,206,0,0,1451,3,18,31,57,25277,,281,1,13,2019-04-05 14:58,2021-03-02 10:52,,697.0,,Intermediate,30,"<sql-server><dbeaver>, How to create a database backup in DBeaver and restore it?, I need to create a SQL server database backup in DBeaver and restore it. Is that possible?
Using SQL Management Studio would not be a turnaround solution in this case, as we are not allowed to use it here.
","<sal-server><beaver>, great database back beaver restore it?, need great sal server database back beaver restore it. possible? use sal manage studio would turnaround slut case, allow use here."
52783712,SQL Server Management Studio 2017 Database Diagram Folder Missing,"I have SSMS 2017 installed and I want to use it to create an ERD for some of my databases. The documentation online says you just need to right click on the ""Database Diagrams"" folder under your database in the navigation panel. However, that folder is simply not there for any of my databases. I cannot find any fix or work around. Does anyone have any ideas on how to fix this?
",<sql-server><ssms-2017>,380,0,0,465,1,4,18,56,12434,0.0,18,3,12,2018-10-12 16:38,2019-01-23 18:36,,103.0,,Advanced,32,"<sql-server><ssms-2017>, SQL Server Management Studio 2017 Database Diagram Folder Missing, I have SSMS 2017 installed and I want to use it to create an ERD for some of my databases. The documentation online says you just need to right click on the ""Database Diagrams"" folder under your database in the navigation panel. However, that folder is simply not there for any of my databases. I cannot find any fix or work around. Does anyone have any ideas on how to fix this?
","<sal-server><sums-2017>, sal server manage studio 2017 database diagram older missing, sum 2017 instal want use great end database. document online say need right click ""database diagram"" older database having panel. however, older simple database. cannot find fix work around. anyone idea fix this?"
52910437,Installed mysql@5.6 using brew mysql.server not a command,"I installed mysql@5.6 using brew. Below are the comands I ran
brew install mysql@5.6
sudo chmod -R 777 /usr/local/var/mysql
sudo ln -s /usr/local/Cellar/mysql\@5.6/5.6.41/bin/mysql /usr/local/bin/mysql
sudo cp /usr/local/Cellar/mysql\@5.6/5.6.41/homebrew.mxcl.mysql.plist /Library/LaunchAgents/
sudo chown root /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod 600 /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod +x /Library/LaunchAgents/homebrew.mxcl.mysql.plist
launchctl load -w /Library/LaunchAgents/homebrew.mxcl.mysql.plist
mysql.server start
  sh: mysql.server: command not found
this is the output I am getting.
mysql --version is giving output
  mysql  Ver 14.14 Distrib 5.6.41, for osx10.13 (x86_64) using  EditLine
  wrapper
If I start service via brew its starting
brew services start mysql@5.6
But when I run mysql -uroot I am getting 
  ERROR 2002 (HY000): Can't connect to local MySQL server through socket
  '/tmp/mysql.sock' (2)
",<mysql><macos><homebrew>,966,0,18,987,4,16,48,37,15874,0.0,189,2,12,2018-10-20 21:53,2018-10-20 21:59,2018-10-20 21:59,0.0,0.0,Advanced,34,"<mysql><macos><homebrew>, Installed mysql@5.6 using brew mysql.server not a command, I installed mysql@5.6 using brew. Below are the comands I ran
brew install mysql@5.6
sudo chmod -R 777 /usr/local/var/mysql
sudo ln -s /usr/local/Cellar/mysql\@5.6/5.6.41/bin/mysql /usr/local/bin/mysql
sudo cp /usr/local/Cellar/mysql\@5.6/5.6.41/homebrew.mxcl.mysql.plist /Library/LaunchAgents/
sudo chown root /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod 600 /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod +x /Library/LaunchAgents/homebrew.mxcl.mysql.plist
launchctl load -w /Library/LaunchAgents/homebrew.mxcl.mysql.plist
mysql.server start
  sh: mysql.server: command not found
this is the output I am getting.
mysql --version is giving output
  mysql  Ver 14.14 Distrib 5.6.41, for osx10.13 (x86_64) using  EditLine
  wrapper
If I start service via brew its starting
brew services start mysql@5.6
But when I run mysql -uroot I am getting 
  ERROR 2002 (HY000): Can't connect to local MySQL server through socket
  '/tmp/mysql.sock' (2)
","<myself><faces><hebrew>, instal myself@5.6 use grew myself.serve command, instal myself@5.6 use grew. command ran grew instal myself@5.6 so child -r 777 /us/local/war/myself so in -s /us/local/cellar/myself\@5.6/5.6.41/bin/myself /us/local/bin/myself so up /us/local/cellar/myself\@5.6/5.6.41/hebrew.much.myself.list /library/launchagents/ so shown root /library/launchagents/hebrew.much.myself.list so child 600 /library/launchagents/hebrew.much.myself.list so child +x /library/launchagents/hebrew.much.myself.list launchctl load -w /library/launchagents/hebrew.much.myself.list myself.serve start s: myself.server: command found output getting. myself --version give output myself ver 14.14 district 5.6.41, osx10.13 (x86_64) use edition wrapped start service via grew start grew service start myself@5.6 run myself -root get error 2002 (hy000): can't connect local myself server socket '/tm/myself.sock' (2)"
64478510,Not able to take backup of hypertable TimescaleDB database using pg_dump PostgreSQL,"command used to take backup
C:\Program Files\PostgreSQL\12\bin&gt;pg_dump  -h localhost -U postgres -p 5432  -Fc -f &quot;D:\Database Backup\temp_10.bak&quot; GESEMS_Performace_Test.
Error :
pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied.
DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will
not copy any data.
Any suggestions to take backup of TimescaleDB hypertables?
",<postgresql><timescaledb>,449,1,1,153,1,2,7,68,6159,,2,1,12,2020-10-22 8:43,2020-10-22 10:22,2020-10-22 10:22,0.0,0.0,Advanced,33,"<postgresql><timescaledb>, Not able to take backup of hypertable TimescaleDB database using pg_dump PostgreSQL, command used to take backup
C:\Program Files\PostgreSQL\12\bin&gt;pg_dump  -h localhost -U postgres -p 5432  -Fc -f &quot;D:\Database Backup\temp_10.bak&quot; GESEMS_Performace_Test.
Error :
pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied.
DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will
not copy any data.
Any suggestions to take backup of TimescaleDB hypertables?
","<postgresql><timescaledb>, all take back hyper timescaledb database use pg_dump postgresql, command use take back c:\program files\postgresql\12\bin&it;pg_dump -h localhost -u poster -p 5432 -ff -f &quit;d:\database back\temp_10.back&quit; gesems_performace_test. error : pg_dump: notice: hyper data chinks, data copied. detail: data hyper store chink hyper copy hyper copy data. suggest take back timescaledb hypertables?"
53160535,Rails 4.2 Postgres 9.4.4 statement_timeout doesn't work,"I am trying to set a statement_timeout. I tried both setting in database.yml file like this
variables:
  statement_timeout: 1000
And this
ActiveRecord::Base.connection.execute(""SET statement_timeout = 1000"")
Tested with
ActiveRecord::Base.connection.execute(""select pg_sleep(4)"")
And they both don't have any effect.
I am running postgres 10 in my local and the statement_timeouts works just expected. But on my server that is running postgres 9.4.4, it simply doesn't do anything.
I've check Postgres' doc for 9.4 and statement_timeout is available. Anyone can shed some light?
",<ruby-on-rails><postgresql>,579,1,5,2544,0,21,29,72,1160,,705,1,12,2018-11-05 19:00,2020-04-04 11:09,,516.0,,Advanced,33,"<ruby-on-rails><postgresql>, Rails 4.2 Postgres 9.4.4 statement_timeout doesn't work, I am trying to set a statement_timeout. I tried both setting in database.yml file like this
variables:
  statement_timeout: 1000
And this
ActiveRecord::Base.connection.execute(""SET statement_timeout = 1000"")
Tested with
ActiveRecord::Base.connection.execute(""select pg_sleep(4)"")
And they both don't have any effect.
I am running postgres 10 in my local and the statement_timeouts works just expected. But on my server that is running postgres 9.4.4, it simply doesn't do anything.
I've check Postgres' doc for 9.4 and statement_timeout is available. Anyone can shed some light?
","<ruby-on-rails><postgresql>, rail 4.2 poster 9.4.4 statement_timeout work, try set statement_timeout. try set database.you file like variable: statement_timeout: 1000 activerecord::base.connection.execute(""set statement_timeout = 1000"") test activerecord::base.connection.execute(""select pg_sleep(4)"") effect. run poster 10 local statement_timeout work expected. server run poster 9.4.4, simple anything. i'v check postures' do 9.4 statement_timeout available. anyone shed light?"
54515682,How to give ranks to multiple columns data in SQL Server?,"I have input table as shown below -
ID  Name q1     q2      q3      q4
1   a    2621   2036    1890    2300
2   b    18000  13000   14000   15000
3   c    100    200     300     400
I want ranking of columns(q1, q2, q3 and q4) data for each row. For example, if I consider last row of above input, then q4 column contains 400 value which is higher than other columns, so rank to q4 column will be 1, q3 rank will be 2, q2 rank will be 3 and q1 rank will be 4.
I am looking for output like -
id  name  q1  q2  q3  q4
1   a     1   3   4   2
2   b     1   4   3   2
3   c     4   3   2   1
There are more than 100,000 records present in input table.
I have created small SQL script for input table i.e.,
declare @temp table (ID int, Name varchar(10), q1 int, q2 int, q3 int, q4 int)
insert into @temp
select 1, 'a', 2621, 2036, 1890, 2300
union all
select 2, 'b', 18000, 13000, 14000, 15000
union all
select 3, 'c', 100, 200, 300, 400
select * from @temp
Please help me to find efficient way to solve this problem.
",<sql-server><sql-server-2012>,1013,0,29,2014,2,13,22,48,1478,,191,1,12,2019-02-04 12:02,2019-02-04 12:13,2019-02-04 12:13,0.0,0.0,Advanced,33,"<sql-server><sql-server-2012>, How to give ranks to multiple columns data in SQL Server?, I have input table as shown below -
ID  Name q1     q2      q3      q4
1   a    2621   2036    1890    2300
2   b    18000  13000   14000   15000
3   c    100    200     300     400
I want ranking of columns(q1, q2, q3 and q4) data for each row. For example, if I consider last row of above input, then q4 column contains 400 value which is higher than other columns, so rank to q4 column will be 1, q3 rank will be 2, q2 rank will be 3 and q1 rank will be 4.
I am looking for output like -
id  name  q1  q2  q3  q4
1   a     1   3   4   2
2   b     1   4   3   2
3   c     4   3   2   1
There are more than 100,000 records present in input table.
I have created small SQL script for input table i.e.,
declare @temp table (ID int, Name varchar(10), q1 int, q2 int, q3 int, q4 int)
insert into @temp
select 1, 'a', 2621, 2036, 1890, 2300
union all
select 2, 'b', 18000, 13000, 14000, 15000
union all
select 3, 'c', 100, 200, 300, 400
select * from @temp
Please help me to find efficient way to solve this problem.
","<sal-server><sal-server-2012>, give rank multiple column data sal server?, input table shown - id name qu qu qu qu 1 2621 2036 1890 2300 2 b 18000 13000 14000 15000 3 c 100 200 300 400 want rank columns(qu, qu, qu qu) data row. example, consider last row input, qu column contain 400 value higher columns, rank qu column 1, qu rank 2, qu rank 3 qu rank 4. look output like - id name qu qu qu qu 1 1 3 4 2 2 b 1 4 3 2 3 c 4 3 2 1 100,000 record present input table. great small sal script input table i.e., declare @hemp table (id in, name varchar(10), qu in, qu in, qu in, qu in) insert @hemp select 1, 'a', 2621, 2036, 1890, 2300 union select 2, 'b', 18000, 13000, 14000, 15000 union select 3, 'c', 100, 200, 300, 400 select * @hemp pleas help find effect way sole problem."
49702144,How do you properly handle SQL_VARIANT in Entity Framework Core?,"It looks like support has recently been added to Entity Framework Core in .NET Core 2.1 (preview) to allow the mapping of SQL_VARIANT columns (https://github.com/aspnet/EntityFrameworkCore/issues/7043).
It looks like the way to go about doing this is using the new HasConversion() method (https://learn.microsoft.com/en-us/ef/core/modeling/value-conversions).
So, in order to map my SQL_VARIANT column and treat the underlying datatype as being a VARCHAR of any length (I only care about reading it at this point), I can do the following (where the Value property here is of type object in the model):
entity.Property(e =&gt; e.Value).HasConversion(v =&gt; v.ToString(),
                                                            v =&gt; v.ToString());
This works, if the SQL_VARIANT's underlying datatype is a VARCHAR of any length.
However, being a SQL_VARIANT, the column could contain data of other types, such as DATETIME values.
For simplicity I've only specified DateTime and string here, but in theory I'd probably want to support the datatypes necessary to map whatever could be stored in a SQL_VARIANT column, if possible.
How would I go about determining which one of those two types (string and DateTime) I'd want to map to at runtime?  Is there a way to do this?
",<c#><.net-core><entity-framework-core><sql-variant>,1277,4,18,1338,3,21,52,35,5893,0.0,9,1,12,2018-04-06 23:15,2018-04-09 9:20,2018-04-09 9:20,3.0,3.0,Advanced,33,"<c#><.net-core><entity-framework-core><sql-variant>, How do you properly handle SQL_VARIANT in Entity Framework Core?, It looks like support has recently been added to Entity Framework Core in .NET Core 2.1 (preview) to allow the mapping of SQL_VARIANT columns (https://github.com/aspnet/EntityFrameworkCore/issues/7043).
It looks like the way to go about doing this is using the new HasConversion() method (https://learn.microsoft.com/en-us/ef/core/modeling/value-conversions).
So, in order to map my SQL_VARIANT column and treat the underlying datatype as being a VARCHAR of any length (I only care about reading it at this point), I can do the following (where the Value property here is of type object in the model):
entity.Property(e =&gt; e.Value).HasConversion(v =&gt; v.ToString(),
                                                            v =&gt; v.ToString());
This works, if the SQL_VARIANT's underlying datatype is a VARCHAR of any length.
However, being a SQL_VARIANT, the column could contain data of other types, such as DATETIME values.
For simplicity I've only specified DateTime and string here, but in theory I'd probably want to support the datatypes necessary to map whatever could be stored in a SQL_VARIANT column, if possible.
How would I go about determining which one of those two types (string and DateTime) I'd want to map to at runtime?  Is there a way to do this?
","<c#><.net-core><entity-framework-core><sal-variant>, properly hand sql_variant entity framework core?, look like support recent ad entity framework core .net core 2.1 (review) allow map sql_variant column (http://github.com/spent/entityframeworkcore/issues/7043). look like way go use new hasconversion() method (http://learn.microsoft.com/en-us/of/core/modeling/value-conversion). so, order map sql_variant column treat underlip datatyp varchar length (i care read point), follow (where value property type object model): entity.property( =&it; e.value).hasconversion(v =&it; v.string(), v =&it; v.string()); works, sql_variant' underlip datatyp varchar length. however, sql_variant, column could contain data types, datetim values. simple i'v specific datetim string here, theory i'd probably want support datatyp necessary map what could store sql_variant column, possible. would go determine one two type (string daytime) i'd want map auntie? way this?"
54387084,How to safely reindex primary key on postgres?,"We have a huge table that contains bloat on the primary key index. We constantly archive old records on that table. 
We reindex other columns by recreating the index concurrently and dropping the old one. This is to avoid interfering with production traffic.
But this is not possible for a primary key since there are foreign keys depending on it. At least based on what we have tried.
What's the right way to reindex the primary key safely without blocking DML statements on the table?
",<postgresql>,487,0,0,7418,5,41,78,48,8594,0.0,303,3,12,2019-01-27 10:21,2019-01-28 12:36,2020-07-24 10:30,1.0,544.0,Advanced,34,"<postgresql>, How to safely reindex primary key on postgres?, We have a huge table that contains bloat on the primary key index. We constantly archive old records on that table. 
We reindex other columns by recreating the index concurrently and dropping the old one. This is to avoid interfering with production traffic.
But this is not possible for a primary key since there are foreign keys depending on it. At least based on what we have tried.
What's the right way to reindex the primary key safely without blocking DML statements on the table?
","<postgresql>, safe render primary key postures?, huge table contain boat primary key index. constantly archive old record table. render column retreat index concur drop old one. avoid inter product traffic. possible primary key since foreign key depend it. least base tried. what' right way render primary key safe without block del statement table?"
53133498,flask-sqlalchemy intellisense/autocomplete,"I'm searching for a way to actually get intellisense for flask-sqlalchemy. All the answers I have found online seem to tell us how to suppress the errors (no Column or query instance members)(switch linters, ignore the class in pylintrc, etc...), but I find it very hard to belive that the most widely used ORM in python does not actually have linting support. 
I am open to using pycharm or intellij community editions, but I'd rather stick with vs code, since I am a js developer who recently decided to learn python, and I'd like to stick with the tools I know and love. I hear that vs code has great python support, but sqlalchemy linting is a must!
",<python><sqlalchemy><visual-studio-code><flask-sqlalchemy>,654,0,0,469,0,3,12,75,5166,0.0,7,0,12,2018-11-03 16:53,,,,,Advanced,36,"<python><sqlalchemy><visual-studio-code><flask-sqlalchemy>, flask-sqlalchemy intellisense/autocomplete, I'm searching for a way to actually get intellisense for flask-sqlalchemy. All the answers I have found online seem to tell us how to suppress the errors (no Column or query instance members)(switch linters, ignore the class in pylintrc, etc...), but I find it very hard to belive that the most widely used ORM in python does not actually have linting support. 
I am open to using pycharm or intellij community editions, but I'd rather stick with vs code, since I am a js developer who recently decided to learn python, and I'd like to stick with the tools I know and love. I hear that vs code has great python support, but sqlalchemy linting is a must!
","<patron><sqlalchemy><visual-studio-code><flask-sqlalchemy>, flask-sqlalchemi intelligence/autocomplete, i'm search way actual get intelligent flask-sqlalchemy. answer found online seem tell us suppress error (no column query instant members)(switch winters, ignore class pylintrc, etc...), find hard believe wide use or patron actual lint support. open use charm intellij common editions, i'd rather stick vs code, since is develop recent decide learn patron, i'd like stick tool know love. hear vs code great patron support, sqlalchemi lint must!"
49016650,How to configure Spring boot for work with two databases?,"I am using Spring Boot 2.X with Hibernate 5 to connect two different MySQL databases (Bar and Foo) on different servers. I am trying to list all the information of an entity (own attributes and @OneToMany and @ManyToOne relations) from a method in a REST Controller. 
I have followed several tutorials to do this, thus, I am able to get all the information for my @Primary database (Foo), however, I always get an exception for my secondary database (Bar) when retrieving the @OneToMany sets. If I swap the @Primary annotation to the Bar database, I able to get the data from the Bar database but not for the Foo database . Is there a way to resolve this?
This is the exception I am getting:
...w.s.m.s.DefaultHandlerExceptionResolver :
Failed to write HTTP message: org.springframework.http.converter.HttpMessageNotWritableException: 
    Could not write JSON document: failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-com.foobar.bar.domain.Bar[""manyBars""]); 
    nested exception is com.fasterxml.jackson.databind.JsonMappingException:
        failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-&gt;com.foobar.bar.domain.Bar[""manyBars""])
My application.properties:
# MySQL DB - ""foo""
spring.datasource.url=jdbc:mysql://XXX:3306/foo?currentSchema=public
spring.datasource.username=XXX
spring.datasource.password=XXX
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
# MySQL DB - ""bar""
bar.datasource.url=jdbc:mysql://YYYY:3306/bar?currentSchema=public
bar.datasource.username=YYYY
bar.datasource.password=YYYY
bar.datasource.driver-class-name=com.mysql.jdbc.Driver
# JPA
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
My @Primary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""entityManagerFactory"",
        transactionManagerRef = ""transactionManager"",
        basePackages = {""com.foobar.foo.repo""})
public class FooDbConfig {
    @Primary
    @Bean(name = ""dataSource"")
    @ConfigurationProperties(prefix = ""spring.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Primary
    @Bean(name = ""entityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""dataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.foo.domain"")
                .persistenceUnit(""foo"")
                .build();
    }
    @Primary
    @Bean(name = ""transactionManager"")
    public PlatformTransactionManager transactionManager(
            @Qualifier(""entityManagerFactory"") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
My secondary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""barEntityManagerFactory"",
        transactionManagerRef = ""barTransactionManager"", basePackages = {""com.foobar.bar.repo""})
public class BarDbConfig {
    @Bean(name = ""barDataSource"")
    @ConfigurationProperties(prefix = ""bar.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Bean(name = ""barEntityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean barEntityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""barDataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.bar.domain"")
                .persistenceUnit(""bar"")
                .build();
    }
    @Bean(name = ""barTransactionManager"")
    public PlatformTransactionManager barTransactionManager(
            @Qualifier(""barEntityManagerFactory"") EntityManagerFactory barEntityManagerFactory) {
        return new JpaTransactionManager(barEntityManagerFactory);
    }
}
The REST Controller class:
@RestController
public class FooBarController {
    private final FooRepository fooRepo;
    private final BarRepository barRepo;
    @Autowired
    FooBarController(FooRepository fooRepo, BarRepository barRepo) {
        this.fooRepo = fooRepo;
        this.barRepo = barRepo;
    }
    @RequestMapping(""/foo"")
    public List&lt;Foo&gt; listFoo() {
        return fooRepo.findAll();
    }
    @RequestMapping(""/bar"")
    public List&lt;Bar&gt; listBar() {
        return barRepo.findAll();
    }
    @RequestMapping(""/foobar/{id}"")
    public String fooBar(@PathVariable(""id"") Integer id) {
        Foo foo = fooRepo.findById(id);
        Bar bar = barRepo.findById(id);
        return foo.getName() + "" "" + bar.getName() + ""!"";
    }
}
The Foo/Bar repositories:
@Repository
public interface FooRepository extends JpaRepository&lt;Foo, Long&gt; {
  Foo findById(Integer id);
}
@Repository
public interface BarRepository extends JpaRepository&lt;Bar, Long&gt; {
  Bar findById(Integer id);
}
The entities for the @Primary datasource. The entities of the second datasource are the same (only changing the class names):
@Entity
@Table(name = ""foo"")
public class Foo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @OneToMany(fetch = FetchType.LAZY, mappedBy = ""foo"")
    @JsonIgnoreProperties({""foo""})
    private Set&lt;ManyFoo&gt; manyFoos = new HashSet&lt;&gt;(0);
    // Constructors, Getters, Setters
}
@Entity
@Table(name = ""many_foo"")
public class ManyFoo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @ManyToOne(fetch = FetchType.LAZY)
    @JsonIgnoreProperties({""manyFoos""})
    private Foo foo;
    // Constructors, Getters, Setters
}  
Finally, my application main:
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
It is important to remark that the solution should keep the Lazy property for both databases in order to maintain an optimal performance.
Edit 1: If both catalogs (""databases"" in MySQL terminology) are in same database (""server"") the Rick James solution works!!
The problem remains when catalogs (MySQL databases) are in different databases (servers) and it is tried to keep Lazy  the property
Many thanks.
",<java><mysql><spring><hibernate><spring-boot>,6853,0,171,1280,1,15,43,49,4611,0.0,707,2,12,2018-02-27 19:38,2018-03-07 18:13,,8.0,,Advanced,34,"<java><mysql><spring><hibernate><spring-boot>, How to configure Spring boot for work with two databases?, I am using Spring Boot 2.X with Hibernate 5 to connect two different MySQL databases (Bar and Foo) on different servers. I am trying to list all the information of an entity (own attributes and @OneToMany and @ManyToOne relations) from a method in a REST Controller. 
I have followed several tutorials to do this, thus, I am able to get all the information for my @Primary database (Foo), however, I always get an exception for my secondary database (Bar) when retrieving the @OneToMany sets. If I swap the @Primary annotation to the Bar database, I able to get the data from the Bar database but not for the Foo database . Is there a way to resolve this?
This is the exception I am getting:
...w.s.m.s.DefaultHandlerExceptionResolver :
Failed to write HTTP message: org.springframework.http.converter.HttpMessageNotWritableException: 
    Could not write JSON document: failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-com.foobar.bar.domain.Bar[""manyBars""]); 
    nested exception is com.fasterxml.jackson.databind.JsonMappingException:
        failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-&gt;com.foobar.bar.domain.Bar[""manyBars""])
My application.properties:
# MySQL DB - ""foo""
spring.datasource.url=jdbc:mysql://XXX:3306/foo?currentSchema=public
spring.datasource.username=XXX
spring.datasource.password=XXX
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
# MySQL DB - ""bar""
bar.datasource.url=jdbc:mysql://YYYY:3306/bar?currentSchema=public
bar.datasource.username=YYYY
bar.datasource.password=YYYY
bar.datasource.driver-class-name=com.mysql.jdbc.Driver
# JPA
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
My @Primary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""entityManagerFactory"",
        transactionManagerRef = ""transactionManager"",
        basePackages = {""com.foobar.foo.repo""})
public class FooDbConfig {
    @Primary
    @Bean(name = ""dataSource"")
    @ConfigurationProperties(prefix = ""spring.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Primary
    @Bean(name = ""entityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""dataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.foo.domain"")
                .persistenceUnit(""foo"")
                .build();
    }
    @Primary
    @Bean(name = ""transactionManager"")
    public PlatformTransactionManager transactionManager(
            @Qualifier(""entityManagerFactory"") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
My secondary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""barEntityManagerFactory"",
        transactionManagerRef = ""barTransactionManager"", basePackages = {""com.foobar.bar.repo""})
public class BarDbConfig {
    @Bean(name = ""barDataSource"")
    @ConfigurationProperties(prefix = ""bar.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Bean(name = ""barEntityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean barEntityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""barDataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.bar.domain"")
                .persistenceUnit(""bar"")
                .build();
    }
    @Bean(name = ""barTransactionManager"")
    public PlatformTransactionManager barTransactionManager(
            @Qualifier(""barEntityManagerFactory"") EntityManagerFactory barEntityManagerFactory) {
        return new JpaTransactionManager(barEntityManagerFactory);
    }
}
The REST Controller class:
@RestController
public class FooBarController {
    private final FooRepository fooRepo;
    private final BarRepository barRepo;
    @Autowired
    FooBarController(FooRepository fooRepo, BarRepository barRepo) {
        this.fooRepo = fooRepo;
        this.barRepo = barRepo;
    }
    @RequestMapping(""/foo"")
    public List&lt;Foo&gt; listFoo() {
        return fooRepo.findAll();
    }
    @RequestMapping(""/bar"")
    public List&lt;Bar&gt; listBar() {
        return barRepo.findAll();
    }
    @RequestMapping(""/foobar/{id}"")
    public String fooBar(@PathVariable(""id"") Integer id) {
        Foo foo = fooRepo.findById(id);
        Bar bar = barRepo.findById(id);
        return foo.getName() + "" "" + bar.getName() + ""!"";
    }
}
The Foo/Bar repositories:
@Repository
public interface FooRepository extends JpaRepository&lt;Foo, Long&gt; {
  Foo findById(Integer id);
}
@Repository
public interface BarRepository extends JpaRepository&lt;Bar, Long&gt; {
  Bar findById(Integer id);
}
The entities for the @Primary datasource. The entities of the second datasource are the same (only changing the class names):
@Entity
@Table(name = ""foo"")
public class Foo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @OneToMany(fetch = FetchType.LAZY, mappedBy = ""foo"")
    @JsonIgnoreProperties({""foo""})
    private Set&lt;ManyFoo&gt; manyFoos = new HashSet&lt;&gt;(0);
    // Constructors, Getters, Setters
}
@Entity
@Table(name = ""many_foo"")
public class ManyFoo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @ManyToOne(fetch = FetchType.LAZY)
    @JsonIgnoreProperties({""manyFoos""})
    private Foo foo;
    // Constructors, Getters, Setters
}  
Finally, my application main:
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
It is important to remark that the solution should keep the Lazy property for both databases in order to maintain an optimal performance.
Edit 1: If both catalogs (""databases"" in MySQL terminology) are in same database (""server"") the Rick James solution works!!
The problem remains when catalogs (MySQL databases) are in different databases (servers) and it is tried to keep Lazy  the property
Many thanks.
","<cava><myself><spring><liberate><spring-boot>, configur spring boot work two database?, use spring boot 2.x wiberd 5 connect two differ myself database (bar foo) differ serves. try list inform entity (own attribute @onetomani @manytoon relations) method rest controller. follow never tutor this, thus, all get inform @primary database (foo), however, away get except secondary database (bar) retrieve @onetomani sets. swap @primary cannot bar database, all get data bar database foo database . way resolve this? except getting: ...w.s.m.s.defaulthandlerexceptionresolv : fail write http message: org.springframework.http.converted.httpmessagenotwritableexception: could write son document: fail lazily into collect role: com.forbear.bar.domain.bar.magyars, could into prove - session (through refer chain: cava.until.arraylist[0]-com.forbear.bar.domain.bar[""magyars""]); nest except com.fasterxml.jackson.databind.jsonmappingexception: fail lazily into collect role: com.forbear.bar.domain.bar.magyars, could into prove - session (through refer chain: cava.until.arraylist[0]-&it;com.forbear.bar.domain.bar[""magyars""]) application.properties: # myself do - ""foo"" spring.datasource.curl=job:myself://xxx:3306/foo?currentschema=pull spring.datasource.surname=xxx spring.datasource.password=xxx spring.datasource.driver-class-name=com.myself.job.drive # myself do - ""bar"" bar.datasource.curl=job:myself://yyyy:3306/bar?currentschema=pull bar.datasource.surname=yyyy bar.datasource.password=yyyy bar.datasource.driver-class-name=com.myself.job.drive # pa spring.pa.show-sal=true spring.pa.liberate.del-auto=non spring.pa.properties.liberate.dialect=org.liberate.dialect.mysql5dialect @primary datasourc configuration: @configur @enabletransactionmanag @enablejparepositories(entitymanagerfactoryref = ""entitymanagerfactory"", transactionmanagerref = ""transactionmanager"", basepackag = {""com.forbear.foo.rep""}) public class foodbconfig { @primary @bean(am = ""datasource"") @configurationproperties(prefix = ""spring.datasource"") public datasourc datasource() { return datasourcebuilder.create().build(); } @primary @bean(am = ""entitymanagerfactory"") public localcontainerentitymanagerfactorybean entitymanagerfactory( entitymanagerfactorybuild builder, @qualified(""datasource"") datasourc datasource) { return builder .datasource(datasource) .packages(""com.forbear.foo.domain"") .persistenceunit(""foo"") .build(); } @primary @bean(am = ""transactionmanager"") public platformtransactionmanag transactionmanager( @qualified(""entitymanagerfactory"") entitymanagerfactori entitymanagerfactory) { return new jpatransactionmanager(entitymanagerfactory); } } secondary datasourc configuration: @configur @enabletransactionmanag @enablejparepositories(entitymanagerfactoryref = ""barentitymanagerfactory"", transactionmanagerref = ""bartransactionmanager"", basepackag = {""com.forbear.bar.rep""}) public class bardbconfig { @bean(am = ""bardatasource"") @configurationproperties(prefix = ""bar.datasource"") public datasourc datasource() { return datasourcebuilder.create().build(); } @bean(am = ""barentitymanagerfactory"") public localcontainerentitymanagerfactorybean barentitymanagerfactory( entitymanagerfactorybuild builder, @qualified(""bardatasource"") datasourc datasource) { return builder .datasource(datasource) .packages(""com.forbear.bar.domain"") .persistenceunit(""bar"") .build(); } @bean(am = ""bartransactionmanager"") public platformtransactionmanag bartransactionmanager( @qualified(""barentitymanagerfactory"") entitymanagerfactori barentitymanagerfactory) { return new jpatransactionmanager(barentitymanagerfactory); } } rest control class: @restcontrol public class foobarcontrol { privat final foorepositori forego; privat final barrepositori barred; @autowir foobarcontroller(foorepositori forego, barrepositori barred) { this.forego = forego; this.barred = barred; } @requestmapping(""/foo"") public list&it;foo&it; listfoo() { return forego.finally(); } @requestmapping(""/bar"") public list&it;bar&it; lister() { return barred.finally(); } @requestmapping(""/forbear/{id}"") public string forbear(@pathvariable(""id"") inter id) { foo foo = forego.findbyid(id); bar bar = barred.findbyid(id); return foo.getname() + "" "" + bar.getname() + ""!""; } } foo/bar depositaries: @depositors public interface foorepositori extend jparepository&it;foo, long&it; { foo findbyid(inter id); } @depositors public interface barrepositori extend jparepository&it;bar, long&it; { bar findbyid(inter id); } entity @primary datasource. entity second datasourc (only change class names): @entity @table(am = ""foo"") public class foo { @id @generatedvalue(strategic = identity) @column(am = ""id"", unique = true, nullabl = false) privat inter id; @column(am = ""name"") privat string name; @onetomany(fetch = fetchtype.lazy, mapped = ""foo"") @jsonignoreproperties({""foo""}) privat set&it;manyfoo&it; manyfoo = new asset&it;&it;(0); // contractors, letters, setter } @entity @table(am = ""many_foo"") public class manyfoo { @id @generatedvalue(strategic = identity) @column(am = ""id"", unique = true, nullabl = false) privat inter id; @column(am = ""name"") privat string name; @manytoone(fetch = fetchtype.lazy) @jsonignoreproperties({""manyfoos""}) privat foo foo; // contractors, letters, setter } finally, applied main: @springbootappl public class applied { public static void main(string[] arms) { springapplication.run(application.class, arms); } } import remark slut keep lazy property database order maintain optic performance. edit 1: catalogue (""database"" myself terminology) database (""server"") rich same slut works!! problem remain catalogue (myself database) differ database (serves) try keep lazy property man thanks."
53025787,Mysql Calculate Growth based on Quarters,"I have a database with two tables - companies and reports. I want to calculate the change from q1 (quarter 1) to q2 (quarter 2). I have tried to use the (following) sub-query, but then the main query fails...
FROM
    (SELECT revenue FROM reports WHERE quarter = 'q2' AND fiscal_year = 2018) AS q,
    (SELECT revenue FROM reports WHERE quarter = 'q1' AND fiscal_year = 2017) AS lq
Here is DB Fiddle to help you understand the problem and schema:
https://www.db-fiddle.com/f/eE8SNRojn45h7Rc1rPCEVN/4
Current Simple Query.
SELECT 
    c.name, r.quarter, r.fiscal_year, r.revenue, r.taxes, r.employees
FROM 
    companies c
JOIN
    reports r 
ON
    r.company_id = c.id
WHERE
    c.is_marked = 1;
Expected Results (this is what i need):
+---------+----------+----------------+----------+--------------+-----------+------------------+
|  Name   | Revenue  | Revenue_change |  Taxes   | Taxes_change | Employees | Employees_change |
+---------+----------+----------------+----------+--------------+-----------+------------------+
| ABC INC |    11056 | +54.77         | 35000.86 | -28.57%      |       568 | -32              |
| XYZ INC |     5000 | null           | null     | null         |        10 | +5               |
+---------+----------+----------------+----------+--------------+-----------+------------------+
I would really appreciate your help to build this query. Thanks in advance.
",<mysql><laravel>,1394,2,19,2756,7,44,76,76,1113,0.0,63,6,12,2018-10-27 19:59,2018-10-27 20:56,,0.0,,Intermediate,15,"<mysql><laravel>, Mysql Calculate Growth based on Quarters, I have a database with two tables - companies and reports. I want to calculate the change from q1 (quarter 1) to q2 (quarter 2). I have tried to use the (following) sub-query, but then the main query fails...
FROM
    (SELECT revenue FROM reports WHERE quarter = 'q2' AND fiscal_year = 2018) AS q,
    (SELECT revenue FROM reports WHERE quarter = 'q1' AND fiscal_year = 2017) AS lq
Here is DB Fiddle to help you understand the problem and schema:
https://www.db-fiddle.com/f/eE8SNRojn45h7Rc1rPCEVN/4
Current Simple Query.
SELECT 
    c.name, r.quarter, r.fiscal_year, r.revenue, r.taxes, r.employees
FROM 
    companies c
JOIN
    reports r 
ON
    r.company_id = c.id
WHERE
    c.is_marked = 1;
Expected Results (this is what i need):
+---------+----------+----------------+----------+--------------+-----------+------------------+
|  Name   | Revenue  | Revenue_change |  Taxes   | Taxes_change | Employees | Employees_change |
+---------+----------+----------------+----------+--------------+-----------+------------------+
| ABC INC |    11056 | +54.77         | 35000.86 | -28.57%      |       568 | -32              |
| XYZ INC |     5000 | null           | null     | null         |        10 | +5               |
+---------+----------+----------------+----------+--------------+-----------+------------------+
I would really appreciate your help to build this query. Thanks in advance.
","<myself><travel>, myself call growth base quarters, database two table - company reports. want call change qu (quarter 1) qu (quarter 2). try use (following) sub-query, main query fails... (select revenue report quarter = 'qu' fiscal_year = 2018) q, (select revenue report quarter = 'qu' fiscal_year = 2017) ll do fiddle help understand problem scheme: http://www.do-fiddle.com/f/ee8snrojn45h7rc1rpcevn/4 current simple query. select c.name, r.quarter, r.fiscal_year, r.revenue, r.taxes, r.employe company c join report r r.company_id = c.id c.is_mark = 1; expect result (the need): +---------+----------+----------------+----------+--------------+-----------+------------------+ | name | revenue | revenue_chang | tax | taxes_chang | employe | employees_chang | +---------+----------+----------------+----------+--------------+-----------+------------------+ | abc in | 11056 | +54.77 | 35000.86 | -28.57% | 568 | -32 | | by in | 5000 | null | null | null | 10 | +5 | +---------+----------+----------------+----------+--------------+-----------+------------------+ would really appreci help build query. thank advance."
56180225,Keycloak server in docker fails to start in standalone mode?,"
Well, as the title suggests, this is more of an issue record. I was trying to follow the instructions on this README file of Keycloak docker server images, but encountered a few blockers. 
After pulling the image, below command to start a standalone instance failed. 
docker run jboss/keycloak
The error stack trace: 
-b 0.0.0.0
=========================================================================
  Using PostgreSQL database
=========================================================================
...
04:45:06,084 INFO  [io.smallrye.metrics] (MSC service thread 1-5) Converted [2] config entries and added [4] replacements
04:45:06,096 ERROR [org.jboss.as.controller.management-operation] (ServerService Thread Pool -- 33) WFLYCTL0013: Operation (""add"") failed - address: ([
    (""subsystem"" =&gt; ""datasources""),
    (""data-source"" =&gt; ""KeycloakDS"")
]) - failure description: ""WFLYCTL0113: '' is an invalid value for parameter user-name. Values must have a minimum length of 1 characters""
...
Caused by: java.lang.RuntimeException: Failed to connect to database
    at org.keycloak.connections.jpa.DefaultJpaConnectionProviderFactory.getConnection(DefaultJpaConnectionProviderFactory.java:382)
...
Caused by: javax.naming.NameNotFoundException: datasources/KeycloakDS -- service jboss.naming.context.java.jboss.datasources.KeycloakDS
    at org.jboss.as.naming.ServiceBasedNamingStore.lookup(ServiceBasedNamingStore.java:106)
...
I was wondering how it uses a PostgreSQL database, and assumed it might spin up its own instance. But the error looks like it has a problem connecting to the database. 
Changing to the embedded H2 DB made it work. 
docker run -e DB_VENDOR=""h2"" --name docker-keycloak-h2 jboss/keycloak
The docker-entrypoint.sh file shows that it uses below logic to determine what DB to use. 
if (getent hosts postgres &amp;&gt;/dev/null); then
        export DB_VENDOR=""postgres""
...
And further down the flow, this change-database.cli file indicates that it's actually expecting a running PostgreSQL instance to use. 
connection-url=jdbc:postgresql://${env.DB_ADDR:postgres}:${env.DB_PORT:5432}/${env.DB_DATABASE:keycloak}${env.JDBC_PARAMS:}
So I began wondering how PostgreSQL was chosen as a default initially. Executing below commands in a running Keycloak docker container revealed some interesting things. 
[root@71961b81189c bin]# getent hosts postgres
69.172.201.153  postgres.mbox.com
[root@71961b81189c bin]# echo $?
0
Not sure what this postgres.mbox.com is but apparently it's not an expected PostgreSQL server to be resolved by getent. Not sure whether this is a recent linux issue either. The hosts entry in the Name Service Switch Configuration file /etc/nsswitch.conf looks like below inside the container.
hosts:      files dns myhostname
It is the dns data source that resolved postgres to postgres.mbox.com. 
This is why the DB vendor determination logic failed which eventually caused the container failing to start. The instructions on this README file do not work as of the day this post is published. 
Below are the working commands to start a Keycloak server in docker properly with PostgreSQL as the database. 
docker network create keycloak-network
docker run -d --name postgres --net keycloak-network -e POSTGRES_DB=keycloak -e POSTGRES_USER=keycloak -e POSTGRES_PASSWORD=password postgres
docker run --name docker-keycloak-postgres --net keycloak-network -e DB_USER=keycloak -e DB_PASSWORD=password jboss/keycloak
",<postgresql><docker><keycloak>,3468,3,44,2467,1,24,42,81,17819,0.0,499,3,12,2019-05-17 5:36,2019-06-04 9:24,2019-06-04 9:24,18.0,18.0,Advanced,37,"<postgresql><docker><keycloak>, Keycloak server in docker fails to start in standalone mode?, 
Well, as the title suggests, this is more of an issue record. I was trying to follow the instructions on this README file of Keycloak docker server images, but encountered a few blockers. 
After pulling the image, below command to start a standalone instance failed. 
docker run jboss/keycloak
The error stack trace: 
-b 0.0.0.0
=========================================================================
  Using PostgreSQL database
=========================================================================
...
04:45:06,084 INFO  [io.smallrye.metrics] (MSC service thread 1-5) Converted [2] config entries and added [4] replacements
04:45:06,096 ERROR [org.jboss.as.controller.management-operation] (ServerService Thread Pool -- 33) WFLYCTL0013: Operation (""add"") failed - address: ([
    (""subsystem"" =&gt; ""datasources""),
    (""data-source"" =&gt; ""KeycloakDS"")
]) - failure description: ""WFLYCTL0113: '' is an invalid value for parameter user-name. Values must have a minimum length of 1 characters""
...
Caused by: java.lang.RuntimeException: Failed to connect to database
    at org.keycloak.connections.jpa.DefaultJpaConnectionProviderFactory.getConnection(DefaultJpaConnectionProviderFactory.java:382)
...
Caused by: javax.naming.NameNotFoundException: datasources/KeycloakDS -- service jboss.naming.context.java.jboss.datasources.KeycloakDS
    at org.jboss.as.naming.ServiceBasedNamingStore.lookup(ServiceBasedNamingStore.java:106)
...
I was wondering how it uses a PostgreSQL database, and assumed it might spin up its own instance. But the error looks like it has a problem connecting to the database. 
Changing to the embedded H2 DB made it work. 
docker run -e DB_VENDOR=""h2"" --name docker-keycloak-h2 jboss/keycloak
The docker-entrypoint.sh file shows that it uses below logic to determine what DB to use. 
if (getent hosts postgres &amp;&gt;/dev/null); then
        export DB_VENDOR=""postgres""
...
And further down the flow, this change-database.cli file indicates that it's actually expecting a running PostgreSQL instance to use. 
connection-url=jdbc:postgresql://${env.DB_ADDR:postgres}:${env.DB_PORT:5432}/${env.DB_DATABASE:keycloak}${env.JDBC_PARAMS:}
So I began wondering how PostgreSQL was chosen as a default initially. Executing below commands in a running Keycloak docker container revealed some interesting things. 
[root@71961b81189c bin]# getent hosts postgres
69.172.201.153  postgres.mbox.com
[root@71961b81189c bin]# echo $?
0
Not sure what this postgres.mbox.com is but apparently it's not an expected PostgreSQL server to be resolved by getent. Not sure whether this is a recent linux issue either. The hosts entry in the Name Service Switch Configuration file /etc/nsswitch.conf looks like below inside the container.
hosts:      files dns myhostname
It is the dns data source that resolved postgres to postgres.mbox.com. 
This is why the DB vendor determination logic failed which eventually caused the container failing to start. The instructions on this README file do not work as of the day this post is published. 
Below are the working commands to start a Keycloak server in docker properly with PostgreSQL as the database. 
docker network create keycloak-network
docker run -d --name postgres --net keycloak-network -e POSTGRES_DB=keycloak -e POSTGRES_USER=keycloak -e POSTGRES_PASSWORD=password postgres
docker run --name docker-keycloak-postgres --net keycloak-network -e DB_USER=keycloak -e DB_PASSWORD=password jboss/keycloak
","<postgresql><doctor><keycloak>, keycloak server doctor fail start standalon mode?, well, till suggests, issue record. try follow instruct ready file keycloak doctor server images, count blocked. pull image, command start standalon instant failed. doctor run boss/keycloak error stick trace: -b 0.0.0.0 ========================================================================= use postgresql database ========================================================================= ... 04:45:06,084 into [to.smaller.merits] (ms service thread 1-5) convert [2] confirm entry ad [4] replace 04:45:06,096 error [org.boss.as.controller.management-operation] (serverservic thread pool -- 33) wflyctl0013: over (""add"") fail - address: ([ (""subsystem"" =&it; ""datasources""), (""data-source"" =&it; ""keycloakds"") ]) - failure description: ""wflyctl0113: '' invalid value parapet user-name. value must minimum length 1 characters"" ... cause by: cava.long.runtimeexception: fail connect database org.keycloak.connections.pa.defaultjpaconnectionproviderfactory.getconnection(defaultjpaconnectionproviderfactory.cava:382) ... cause by: naval.naming.namenotfoundexception: datasources/keycloakd -- service boss.naming.context.cava.boss.datasources.keycloakd org.boss.as.naming.servicebasednamingstore.lockup(servicebasednamingstore.cava:106) ... wonder use postgresql database, assume might spin instance. error look like problem connect database. change ebbed he do made work. doctor run -e db_vendor=""he"" --name doctor-keycloak-he boss/keycloak doctor-entrypoint.s file show use logic determine do use. (event host poster &amp;&it;/de/null); export db_vendor=""postures"" ... flow, change-database.coli file india actual expect run postgresql instant use. connection-curl=job:postgresql://${end.db_addr:postures}:${end.duport:5432}/${end.db_database:keycloak}${end.jdbc_params:} began wonder postgresql chosen default initially. execute command run keycloak doctor contain reveal interest things. [root@71961b81189c bin]# event host poster 69.172.201.153 postures.box.com [root@71961b81189c bin]# echo $? 0 sure postures.box.com appear expect postgresql server resolve event. sure whether recent line issue either. host entry name service switch configur file /etc/switch.cone look like inside container. hosts: file in myhostnam in data source resolve poster postures.box.com. do vendor determine logic fail events cause contain fail start. instruct ready file work day post published. work command start keycloak server doctor properly postgresql database. doctor network great keycloak-network doctor run -d --name poster --net keycloak-network -e postgres_db=keycloak -e postgres_user=keycloak -e postgres_password=password poster doctor run --name doctor-keycloak-poster --net keycloak-network -e db_user=keycloak -e db_password=password boss/keycloak"
53469793,E_WARNING: Error while sending STMT_PREPARE packet. PID=*,"My Laravel 5.7 website has been experiencing a few problems that I think are related to each other (but happen at different times):
PDO::prepare(): MySQL server has gone away
E_WARNING: Error while sending STMT_PREPARE packet. PID=10
PDOException: SQLSTATE[23000]: Integrity constraint violation: 1062 Duplicate entry (My database often seems to try to write the same record twice in the same second. I've been unable to figure out why or how to reproduce it; it doesn't seem to be related to user behavior.)
Somehow, those first 2 types of errors only ever appear in my Rollbar logs but not on the text logs on the server or in my Slack notifications, as all errors are supposed to (and all others do).
For months, I've continued to see scary log messages like these, and I've been completely unable to reproduce these errors (and have been unable to diagnose and solve them).
I haven't yet found any actual symptoms or heard of any complaints from users, but the error messages seem non-trivial, so I really want to understand and fix the root causes.
I've tried changing my MySQL config to use max_allowed_packet=300M (instead of the default of 4M) but still get these exceptions frequently on days when I have more than a couple of visitors to my site.
I've also set (changed from 5M and 10M) the following because of this advice:
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
As further background:
My site has a queue worker that runs jobs (artisan queue:work --sleep=3 --tries=3 --daemon).
There are a bunch of queued jobs that can be scheduled to happen at the same moment based on the signup time of visitors. But the most I see that have happened simultaneously is 20.
There are no entries in the MySQL Slow Query Log.
I have a few cron jobs, but I doubt they're problematic. One runs every minute but is really simple. Another runs every 5 minutes to send certain scheduled emails if any are pending. And another runs every 30 minutes to run a report.
I've run various mysqlslap queries (I'm completely novice though) and haven't found anything slow even when simulating hundreds of concurrent clients.
I'm using Laradock (Docker).
My server is DigitalOcean 1GB RAM, 1 vCPU, 25GB SSD. I've also tried 2GB RAM with no difference.
The results from SHOW VARIABLES; and SHOW GLOBAL STATUS; are here.
My my.cnf is:
[mysql]
[mysqld]
sql-mode=&quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&quot;
character-set-server=utf8
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
max_allowed_packet=300M
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow_query_log.log
long_query_time = 10
log_queries_not_using_indexes = 0
Any ideas about what I should explore to diagnose and fix these problems? Thanks.
",<mysql><laravel><performance><laravel-5><laradock>,2793,3,23,22262,31,180,366,46,3476,0.0,13224,4,12,2018-11-25 17:02,2018-12-10 6:09,,15.0,,Advanced,33,"<mysql><laravel><performance><laravel-5><laradock>, E_WARNING: Error while sending STMT_PREPARE packet. PID=*, My Laravel 5.7 website has been experiencing a few problems that I think are related to each other (but happen at different times):
PDO::prepare(): MySQL server has gone away
E_WARNING: Error while sending STMT_PREPARE packet. PID=10
PDOException: SQLSTATE[23000]: Integrity constraint violation: 1062 Duplicate entry (My database often seems to try to write the same record twice in the same second. I've been unable to figure out why or how to reproduce it; it doesn't seem to be related to user behavior.)
Somehow, those first 2 types of errors only ever appear in my Rollbar logs but not on the text logs on the server or in my Slack notifications, as all errors are supposed to (and all others do).
For months, I've continued to see scary log messages like these, and I've been completely unable to reproduce these errors (and have been unable to diagnose and solve them).
I haven't yet found any actual symptoms or heard of any complaints from users, but the error messages seem non-trivial, so I really want to understand and fix the root causes.
I've tried changing my MySQL config to use max_allowed_packet=300M (instead of the default of 4M) but still get these exceptions frequently on days when I have more than a couple of visitors to my site.
I've also set (changed from 5M and 10M) the following because of this advice:
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
As further background:
My site has a queue worker that runs jobs (artisan queue:work --sleep=3 --tries=3 --daemon).
There are a bunch of queued jobs that can be scheduled to happen at the same moment based on the signup time of visitors. But the most I see that have happened simultaneously is 20.
There are no entries in the MySQL Slow Query Log.
I have a few cron jobs, but I doubt they're problematic. One runs every minute but is really simple. Another runs every 5 minutes to send certain scheduled emails if any are pending. And another runs every 30 minutes to run a report.
I've run various mysqlslap queries (I'm completely novice though) and haven't found anything slow even when simulating hundreds of concurrent clients.
I'm using Laradock (Docker).
My server is DigitalOcean 1GB RAM, 1 vCPU, 25GB SSD. I've also tried 2GB RAM with no difference.
The results from SHOW VARIABLES; and SHOW GLOBAL STATUS; are here.
My my.cnf is:
[mysql]
[mysqld]
sql-mode=&quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&quot;
character-set-server=utf8
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
max_allowed_packet=300M
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow_query_log.log
long_query_time = 10
log_queries_not_using_indexes = 0
Any ideas about what I should explore to diagnose and fix these problems? Thanks.
","<myself><travel><performance><travel-5><laradock>, warning: error send stmt_prepar packet. did=*, travel 5.7 west experience problem think relate (but happen differ times): do::prepare(): myself server gone away warning: error send stmt_prepar packet. did=10 pdoexception: sqlstate[23000]: inter constraint violation: 1062 public entry (mi database often seem try write record twice second. i'v unable figure reproduce it; seem relate user behavior.) somehow, first 2 type error ever appear collar log text log server slack modifications, error suppose (and other do). months, i'v continue see scar log message like these, i'v complete unable reproduce error (and unable diagnose sole them). yet found actual symptom heard complaint users, error message seem non-trivial, really want understand fix root causes. i'v try change myself confirm use max_allowed_packet=300m (instead default am) still get except frequent day couple visitor site. i'v also set (change am him) follow advice: innodb_buffer_pool_chunk_size=218m innodb_buffer_pool_s = 218m background: site queue worker run job (artisans queue:work --sleep=3 --tries=3 --demon). bunch que job schedule happen moment base sign time visitors. see happen simulate 20. entry myself slow query log. iron jobs, doubt they'r problematical. one run every minute really simple. not run every 5 minute send certain schedule email pending. not run every 30 minute run report. i'v run various mysqlslap query (i'm complete novice though) found any slow even soul under concur clients. i'm use laradock (doctor). server digitalocean go ram, 1 cup, 25gb sad. i'v also try go ram difference. result show variable; show global status; here. my.cf is: [myself] [myself] sal-mode=&quit;strict_trans_tables,no_zero_in_date,error_for_division_by_zero,no_engine_substitution&quit; character-set-server=utf innodb_buffer_pool_chunk_size=218m innodb_buffer_pool_s = 218m max_allowed_packet=300m slow_query_log = 1 slow_query_log_fil = /war/log/myself/slow_query_log.log long_query_tim = 10 log_queries_not_using_index = 0 idea explore diagnose fix problems? thanks."
59578211,MySQL to return records with a date/time of now minus 1 hour?,"can anybody help with a MySQL command to try and select all records within a table with a date/time equal to or more than now - 1hour?
Now I'm not 100% sure that that is the best way of describing this.
I basically have records with a date/time field (e.g. 2019-07-13 13:00:00) and I want to perform a MySQL select to find all records with a date/time of one hour ago. This is to trigger a function one hour after an event.
I currently have this, but not sure if it is along the right lines at all:
SELECT * FROM database.table_name
WHERE (NOW() - INTERVAL 1 HOUR) &gt;= 'date_of_event'
AND 'status' = 'Scheduled';
Any thoughts would be great!
",<mysql><datetime>,644,0,3,2813,8,29,32,38,15524,0.0,37,2,12,2020-01-03 11:56,2020-01-03 12:08,2020-01-03 12:09,0.0,0.0,Basic,10,"<mysql><datetime>, MySQL to return records with a date/time of now minus 1 hour?, can anybody help with a MySQL command to try and select all records within a table with a date/time equal to or more than now - 1hour?
Now I'm not 100% sure that that is the best way of describing this.
I basically have records with a date/time field (e.g. 2019-07-13 13:00:00) and I want to perform a MySQL select to find all records with a date/time of one hour ago. This is to trigger a function one hour after an event.
I currently have this, but not sure if it is along the right lines at all:
SELECT * FROM database.table_name
WHERE (NOW() - INTERVAL 1 HOUR) &gt;= 'date_of_event'
AND 'status' = 'Scheduled';
Any thoughts would be great!
","<myself><daytime>, myself return record date/tim mind 1 hour?, anybody help myself command try select record within table date/tim equal - hour? i'm 100% sure best way describe this. basic record date/tim field (e.g. 2019-07-13 13:00:00) want perform myself select find record date/tim one hour ago. trigger function one hour event. current this, sure along right line all: select * database.table_nam (now() - inter 1 hour) &it;= 'date_of_event' 'status' = 'schedule'; thought would great!"
52284555,FOR JSON PATH vs FOR JSON AUTO SQL Server,"I'm having an issue creating nested JSON in SQL Server.  I'm trying to create an output that looks like this:
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
        ""address1"": ""1 Oak Grove"",
        ""address2"": ""London"",
        ""address3"": ""UK""
    },
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
I've tried both FOR JSON AUTO and FOR JSON PATH:
SELECT
  d.SCHOOL_WEBSITE AS websiteURL
  ,d.SCHOOL_EMAIL AS email 
 ,d.SCHOOL_TELEPHONE AS phone
 ,d.[Address 1] AS 'address.address1'
 ,d.[Address 2] AS 'address.address2'
 ,d.[Address 3] AS 'address.address3'
 ,accreditations.[IndiUID] as name   
 ,accreditations.Value as value 
 FROM [TESTDB].[dbo].[DataValues] as d,[TESTDB].[dbo].[accreditations] as accreditations
 WHERE d.Code = accreditations.SchoolCode
 FOR JSON AUTO --PATH
FOR JSON AUTO creates this (address section is not nested (but accredidation is):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address.address1"": ""1 Oak Grove"",
    ""address.address2"": ""London"",
    ""address.address3"": ""UK"",
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
FOR JSON PATH creates this (address section is nested, but accreditation is not - the whole block repeats):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator1"",
    ""value"": ""True""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator2"",
    ""value"": ""False""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator3"",
    ""value"": ""False""
    }
]
I think the key to it is some sort of FOR JSON sub query around the accreditations but I haven't had any success with this.
Create sample data with the following:
    /****** Object:  Table [dbo].[accreditations]    Script Date: 11/09/2018 22:29:43 ******/
CREATE TABLE [dbo].[accreditations](
    [SchoolCode] [nvarchar](255) NULL,
    [IndiUID] [nvarchar](255) NULL,
    [Value] [nvarchar](255) NULL
) ON [PRIMARY]
GO
/****** Object:  Table [dbo].[DataValues]    Script Date: 11/09/2018 22:29:44 ******/
CREATE TABLE [dbo].[DataValues](
    [Code] [nvarchar](255) NULL,
    [SCHOOL_NAME_FORMAL] [nvarchar](255) NULL,
    [SCHOOL_WEBSITE] [nvarchar](255) NULL,
    [SCHOOL_EMAIL] [nvarchar](255) NULL,
    [SCHOOL_TELEPHONE] [float] NULL,
    [Address 1] [nvarchar](255) NULL,
    [Address 2] [nvarchar](255) NULL,
    [Address 3] [nvarchar](255) NULL
) ON [PRIMARY]
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator2', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator3', N'True')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'ABC', N'test', N'www.test.edu', N'hello@test.edu', 123456798, N'1 Oak Grove', N'London', N'UK')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'DEF', N'something', N'https://something.edu/fulltime', N'hello@something.edu', 987654321, N'23 Tree Road', N'Paris', N'France')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'GHI', N'university', N'http://www.university.ac.uk/', N'hello@university.ac.uk/', 123123123, N'57 Bonsai Lane', N'London', N'UK')
GO
",<json><sql-server><for-json>,5145,4,143,125,1,1,6,47,17562,,1,1,12,2018-09-11 21:39,2018-09-11 21:48,2018-09-11 21:48,0.0,0.0,Intermediate,23,"<json><sql-server><for-json>, FOR JSON PATH vs FOR JSON AUTO SQL Server, I'm having an issue creating nested JSON in SQL Server.  I'm trying to create an output that looks like this:
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
        ""address1"": ""1 Oak Grove"",
        ""address2"": ""London"",
        ""address3"": ""UK""
    },
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
I've tried both FOR JSON AUTO and FOR JSON PATH:
SELECT
  d.SCHOOL_WEBSITE AS websiteURL
  ,d.SCHOOL_EMAIL AS email 
 ,d.SCHOOL_TELEPHONE AS phone
 ,d.[Address 1] AS 'address.address1'
 ,d.[Address 2] AS 'address.address2'
 ,d.[Address 3] AS 'address.address3'
 ,accreditations.[IndiUID] as name   
 ,accreditations.Value as value 
 FROM [TESTDB].[dbo].[DataValues] as d,[TESTDB].[dbo].[accreditations] as accreditations
 WHERE d.Code = accreditations.SchoolCode
 FOR JSON AUTO --PATH
FOR JSON AUTO creates this (address section is not nested (but accredidation is):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address.address1"": ""1 Oak Grove"",
    ""address.address2"": ""London"",
    ""address.address3"": ""UK"",
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
FOR JSON PATH creates this (address section is nested, but accreditation is not - the whole block repeats):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator1"",
    ""value"": ""True""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator2"",
    ""value"": ""False""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator3"",
    ""value"": ""False""
    }
]
I think the key to it is some sort of FOR JSON sub query around the accreditations but I haven't had any success with this.
Create sample data with the following:
    /****** Object:  Table [dbo].[accreditations]    Script Date: 11/09/2018 22:29:43 ******/
CREATE TABLE [dbo].[accreditations](
    [SchoolCode] [nvarchar](255) NULL,
    [IndiUID] [nvarchar](255) NULL,
    [Value] [nvarchar](255) NULL
) ON [PRIMARY]
GO
/****** Object:  Table [dbo].[DataValues]    Script Date: 11/09/2018 22:29:44 ******/
CREATE TABLE [dbo].[DataValues](
    [Code] [nvarchar](255) NULL,
    [SCHOOL_NAME_FORMAL] [nvarchar](255) NULL,
    [SCHOOL_WEBSITE] [nvarchar](255) NULL,
    [SCHOOL_EMAIL] [nvarchar](255) NULL,
    [SCHOOL_TELEPHONE] [float] NULL,
    [Address 1] [nvarchar](255) NULL,
    [Address 2] [nvarchar](255) NULL,
    [Address 3] [nvarchar](255) NULL
) ON [PRIMARY]
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator2', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator3', N'True')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'ABC', N'test', N'www.test.edu', N'hello@test.edu', 123456798, N'1 Oak Grove', N'London', N'UK')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'DEF', N'something', N'https://something.edu/fulltime', N'hello@something.edu', 987654321, N'23 Tree Road', N'Paris', N'France')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'GHI', N'university', N'http://www.university.ac.uk/', N'hello@university.ac.uk/', 123123123, N'57 Bonsai Lane', N'London', N'UK')
GO
","<son><sal-server><for-son>, son path vs son auto sal server, i'm issue great nest son sal server. i'm try great output look like this: [ { ""websiteurl"": ""www.test.ed"", ""email"": ""hello@test.ed"", ""phone"": 123456798, ""address"": { ""address"": ""1 oak grove"", ""address"": ""london"", ""address"": ""up"" }, ""accreditations"": [ { ""name"": ""indicator"", ""value"": ""true"" }, { ""name"": ""indicator"", ""value"": ""false"" }, { ""name"": ""indicator"", ""value"": ""false"" } ] } ] i'v try son auto son path: select d.school_websit websiteurl ,d.school_email email ,d.school_telephon phone ,d.[address 1] 'address.address' ,d.[address 2] 'address.address' ,d.[address 3] 'address.address' ,accreditations.[individu] name ,accreditations.value value [test].[do].[datavalues] d,[test].[do].[accreditations] credit d.code = accreditations.schoolboy son auto --path son auto great (address section nest (but accredited is): [ { ""websiteurl"": ""www.test.ed"", ""email"": ""hello@test.ed"", ""phone"": 123456798, ""address.address"": ""1 oak grove"", ""address.address"": ""london"", ""address.address"": ""up"", ""accreditations"": [ { ""name"": ""indicator"", ""value"": ""true"" }, { ""name"": ""indicator"", ""value"": ""false"" }, { ""name"": ""indicator"", ""value"": ""false"" } ] } ] son path great (address section rested, credit - whole block repeats): [ { ""websiteurl"": ""www.test.ed"", ""email"": ""hello@test.ed"", ""phone"": 123456798, ""address"": { ""address"": ""1 oak grove"", ""address"": ""london"", ""address"": ""up"" }, ""name"": ""indicator"", ""value"": ""true"" }, { ""websiteurl"": ""www.test.ed"", ""email"": ""hello@test.ed"", ""phone"": 123456798, ""address"": { ""address"": ""1 oak grove"", ""address"": ""london"", ""address"": ""up"" }, ""name"": ""indicator"", ""value"": ""false"" }, { ""websiteurl"": ""www.test.ed"", ""email"": ""hello@test.ed"", ""phone"": 123456798, ""address"": { ""address"": ""1 oak grove"", ""address"": ""london"", ""address"": ""up"" }, ""name"": ""indicator"", ""value"": ""false"" } ] think key sort son sub query around credit success this. great sample data following: /****** object: table [do].[accreditations] script date: 11/09/2018 22:29:43 ******/ great table [do].[accreditations]( [schoolcode] [nvarchar](255) null, [individu] [nvarchar](255) null, [value] [nvarchar](255) null ) [primary] go /****** object: table [do].[datavalues] script date: 11/09/2018 22:29:44 ******/ great table [do].[datavalues]( [code] [nvarchar](255) null, [school_name_formal] [nvarchar](255) null, [school_website] [nvarchar](255) null, [school_email] [nvarchar](255) null, [school_telephone] [float] null, [address 1] [nvarchar](255) null, [address 2] [nvarchar](255) null, [address 3] [nvarchar](255) null ) [primary] go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'abc', n'indicator', n'true') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'abc', n'indicator', n'false') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'abc', n'indicator', n'false') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'def', n'indicator', n'true') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'def', n'indicator', n'false') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'def', n'indicator', n'false') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'hi', n'indicator', n'true') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'hi', n'indicator', n'true') go insert [do].[accreditations] ([schoolcode], [individu], [value]) value (n'hi', n'indicator', n'true') go insert [do].[datavalues] ([code], [school_name_formal], [school_website], [school_email], [school_telephone], [address 1], [address 2], [address 3]) value (n'abc', n'test', n'www.test.ed', n'hello@test.ed', 123456798, n'1 oak grove', n'london', n'up') go insert [do].[datavalues] ([code], [school_name_formal], [school_website], [school_email], [school_telephone], [address 1], [address 2], [address 3]) value (n'def', n'something', n'http://something.ed/fulltime', n'hello@something.ed', 987654321, n'23 tree road', n'paris', n'france') go insert [do].[datavalues] ([code], [school_name_formal], [school_website], [school_email], [school_telephone], [address 1], [address 2], [address 3]) value (n'hi', n'university', n'http://www.university.ac.up/', n'hello@university.ac.up/', 123123123, n'57 bons lane', n'london', n'up') go"
51810460,Is proper event-time sessionization possible with Spark Structured Streaming?,"Been playing around Spark Structured Streaming and mapGroupsWithState (specifically following the StructuredSessionization example in the Spark source). I want to confirm some limitations I believe exist with mapGroupsWithState given my use case.
A session for my purposes is a group of uninterrupted activity for a user such that no two chronologically ordered (by event time, not processing time) events are separated by more than some developer-defined duration (30 minutes is common).
An example will help before jumping into code:
{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}
For the stream above, a session is defined with a 30 minute period of inactivity. In a streaming context, we should end up with one session (the second has yet to complete):
[
  {
    ""user_id"": ""mike"",
    ""startTimestamp"": ""2018-01-01T00:00:00"",
    ""endTimestamp"": ""2018-01-01T00:05:00""
  }
]
Now consider the following Spark driver program:
import java.sql.Timestamp
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.execution.streaming.MemoryStream
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout}
object StructuredSessionizationV2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master(""local[2]"")
      .appName(""StructredSessionizationRedux"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""WARN"")
    import spark.implicits._
    implicit val ctx = spark.sqlContext
    val input = MemoryStream[String]
    val EVENT_SCHEMA = new StructType()
      .add($""event_time"".string)
      .add($""user_id"".string)
    val events = input.toDS()
      .select(from_json($""value"", EVENT_SCHEMA).alias(""json""))
      .select($""json.*"")
      .withColumn(""event_time"", to_timestamp($""event_time""))
      .withWatermark(""event_time"", ""1 hours"")
    events.printSchema()
    val sessionized = events
      .groupByKey(row =&gt; row.getAs[String](""user_id""))
      .mapGroupsWithState[SessionState, SessionOutput](GroupStateTimeout.EventTimeTimeout) {
      case (userId: String, events: Iterator[Row], state: GroupState[SessionState]) =&gt;
        println(s""state update for user ${userId} (current watermark: ${new Timestamp(state.getCurrentWatermarkMs())})"")
        if (state.hasTimedOut) {
          println(s""User ${userId} has timed out, sending final output."")
          val finalOutput = SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = true
          )
          // Drop this user's state
          state.remove()
          finalOutput
        } else {
          val timestamps = events.map(_.getAs[Timestamp](""event_time"").getTime).toSeq
          println(s""User ${userId} has new events (min: ${new Timestamp(timestamps.min)}, max: ${new Timestamp(timestamps.max)})."")
          val newState = if (state.exists) {
            println(s""User ${userId} has existing state."")
            val oldState = state.get
            SessionState(
              startTimestampMs = math.min(oldState.startTimestampMs, timestamps.min),
              endTimestampMs = math.max(oldState.endTimestampMs, timestamps.max)
            )
          } else {
            println(s""User ${userId} has no existing state."")
            SessionState(
              startTimestampMs = timestamps.min,
              endTimestampMs = timestamps.max
            )
          }
          state.update(newState)
          state.setTimeoutTimestamp(newState.endTimestampMs, ""30 minutes"")
          println(s""User ${userId} state updated. Timeout now set to ${new Timestamp(newState.endTimestampMs + (30 * 60 * 1000))}"")
          SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = false
          )
        }
      }
    val eventsQuery = sessionized
      .writeStream
      .queryName(""events"")
      .outputMode(""update"")
      .format(""console"")
      .start()
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}""""""
    )
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}""""""
    )
    eventsQuery.processAllAvailable()
  }
  case class SessionState(startTimestampMs: Long, endTimestampMs: Long) {
    def durationMs: Long = endTimestampMs - startTimestampMs
  }
  case class SessionOutput(userId: String, startTimestampMs: Long, endTimestampMs: Long, durationMs: Long, expired: Boolean)
}
Output of that program is:
root
 |-- event_time: timestamp (nullable = true)
 |-- user_id: string (nullable = true)
state update for user mike (current watermark: 1969-12-31 19:00:00.0)
User mike has new events (min: 2018-01-01 00:00:00.0, max: 2018-01-01 00:05:00.0).
User mike has no existing state.
User mike state updated. Timeout now set to 2018-01-01 00:35:00.0
-------------------------------------------
Batch: 0
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514783100000|    300000|  false|
+------+----------------+--------------+----------+-------+
state update for user mike (current watermark: 2017-12-31 23:05:00.0)
User mike has new events (min: 2018-01-01 00:45:00.0, max: 2018-01-01 00:45:00.0).
User mike has existing state.
User mike state updated. Timeout now set to 2018-01-01 01:15:00.0
-------------------------------------------
Batch: 1
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514785500000|   2700000|  false|
+------+----------------+--------------+----------+-------+
Given my session definition, the single event in the second batch should trigger an expiry of session state and thus a new session. However, since the watermark (2017-12-31 23:05:00.0) has not passed the state's timeout (2018-01-01 00:35:00.0), state isn't expired and the event is erroneously added to the existing session despite the fact that more than 30 minutes have passed since the latest timestamp in the previous batch.
I think the only way for session state expiration to work as I'm hoping is if enough events from different users were received within the batch to advance the watermark past the state timeout for mike. 
I suppose one could also mess with the stream's watermark, but I can't think of how I'd do that to accomplish my use case.
Is this accurate? Am I missing anything in how to properly do event time-based sessionization in Spark?
",<apache-spark><apache-spark-sql><spark-structured-streaming>,7348,1,149,3780,3,24,31,54,2295,0.0,43,3,12,2018-08-12 15:56,2018-08-29 18:14,,17.0,,Intermediate,23,"<apache-spark><apache-spark-sql><spark-structured-streaming>, Is proper event-time sessionization possible with Spark Structured Streaming?, Been playing around Spark Structured Streaming and mapGroupsWithState (specifically following the StructuredSessionization example in the Spark source). I want to confirm some limitations I believe exist with mapGroupsWithState given my use case.
A session for my purposes is a group of uninterrupted activity for a user such that no two chronologically ordered (by event time, not processing time) events are separated by more than some developer-defined duration (30 minutes is common).
An example will help before jumping into code:
{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}
For the stream above, a session is defined with a 30 minute period of inactivity. In a streaming context, we should end up with one session (the second has yet to complete):
[
  {
    ""user_id"": ""mike"",
    ""startTimestamp"": ""2018-01-01T00:00:00"",
    ""endTimestamp"": ""2018-01-01T00:05:00""
  }
]
Now consider the following Spark driver program:
import java.sql.Timestamp
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.execution.streaming.MemoryStream
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout}
object StructuredSessionizationV2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master(""local[2]"")
      .appName(""StructredSessionizationRedux"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""WARN"")
    import spark.implicits._
    implicit val ctx = spark.sqlContext
    val input = MemoryStream[String]
    val EVENT_SCHEMA = new StructType()
      .add($""event_time"".string)
      .add($""user_id"".string)
    val events = input.toDS()
      .select(from_json($""value"", EVENT_SCHEMA).alias(""json""))
      .select($""json.*"")
      .withColumn(""event_time"", to_timestamp($""event_time""))
      .withWatermark(""event_time"", ""1 hours"")
    events.printSchema()
    val sessionized = events
      .groupByKey(row =&gt; row.getAs[String](""user_id""))
      .mapGroupsWithState[SessionState, SessionOutput](GroupStateTimeout.EventTimeTimeout) {
      case (userId: String, events: Iterator[Row], state: GroupState[SessionState]) =&gt;
        println(s""state update for user ${userId} (current watermark: ${new Timestamp(state.getCurrentWatermarkMs())})"")
        if (state.hasTimedOut) {
          println(s""User ${userId} has timed out, sending final output."")
          val finalOutput = SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = true
          )
          // Drop this user's state
          state.remove()
          finalOutput
        } else {
          val timestamps = events.map(_.getAs[Timestamp](""event_time"").getTime).toSeq
          println(s""User ${userId} has new events (min: ${new Timestamp(timestamps.min)}, max: ${new Timestamp(timestamps.max)})."")
          val newState = if (state.exists) {
            println(s""User ${userId} has existing state."")
            val oldState = state.get
            SessionState(
              startTimestampMs = math.min(oldState.startTimestampMs, timestamps.min),
              endTimestampMs = math.max(oldState.endTimestampMs, timestamps.max)
            )
          } else {
            println(s""User ${userId} has no existing state."")
            SessionState(
              startTimestampMs = timestamps.min,
              endTimestampMs = timestamps.max
            )
          }
          state.update(newState)
          state.setTimeoutTimestamp(newState.endTimestampMs, ""30 minutes"")
          println(s""User ${userId} state updated. Timeout now set to ${new Timestamp(newState.endTimestampMs + (30 * 60 * 1000))}"")
          SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = false
          )
        }
      }
    val eventsQuery = sessionized
      .writeStream
      .queryName(""events"")
      .outputMode(""update"")
      .format(""console"")
      .start()
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}""""""
    )
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}""""""
    )
    eventsQuery.processAllAvailable()
  }
  case class SessionState(startTimestampMs: Long, endTimestampMs: Long) {
    def durationMs: Long = endTimestampMs - startTimestampMs
  }
  case class SessionOutput(userId: String, startTimestampMs: Long, endTimestampMs: Long, durationMs: Long, expired: Boolean)
}
Output of that program is:
root
 |-- event_time: timestamp (nullable = true)
 |-- user_id: string (nullable = true)
state update for user mike (current watermark: 1969-12-31 19:00:00.0)
User mike has new events (min: 2018-01-01 00:00:00.0, max: 2018-01-01 00:05:00.0).
User mike has no existing state.
User mike state updated. Timeout now set to 2018-01-01 00:35:00.0
-------------------------------------------
Batch: 0
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514783100000|    300000|  false|
+------+----------------+--------------+----------+-------+
state update for user mike (current watermark: 2017-12-31 23:05:00.0)
User mike has new events (min: 2018-01-01 00:45:00.0, max: 2018-01-01 00:45:00.0).
User mike has existing state.
User mike state updated. Timeout now set to 2018-01-01 01:15:00.0
-------------------------------------------
Batch: 1
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514785500000|   2700000|  false|
+------+----------------+--------------+----------+-------+
Given my session definition, the single event in the second batch should trigger an expiry of session state and thus a new session. However, since the watermark (2017-12-31 23:05:00.0) has not passed the state's timeout (2018-01-01 00:35:00.0), state isn't expired and the event is erroneously added to the existing session despite the fact that more than 30 minutes have passed since the latest timestamp in the previous batch.
I think the only way for session state expiration to work as I'm hoping is if enough events from different users were received within the batch to advance the watermark past the state timeout for mike. 
I suppose one could also mess with the stream's watermark, but I can't think of how I'd do that to accomplish my use case.
Is this accurate? Am I missing anything in how to properly do event time-based sessionization in Spark?
","<apache-spark><apache-spark-sal><spark-structures-streaming>, proper event-tim session possible spark structure streaming?, play around spark structure stream mapgroupswithst (specie follow structuredsession example spark source). want confirm limit believe exist mapgroupswithst given use case. session purpose group interrupt active user two chronolog order (i event time, process time) event spear developer-define murat (30 minute common). example help jump code: {""event_time"": ""2018-01-01t00:00:00"", ""user_id"": ""mike""} {""event_time"": ""2018-01-01t00:01:00"", ""user_id"": ""mike""} {""event_time"": ""2018-01-01t00:05:00"", ""user_id"": ""mike""} {""event_time"": ""2018-01-01t00:45:00"", ""user_id"": ""mike""} stream above, session define 30 minute period inactivity. stream context, end one session (the second yet complete): [ { ""user_id"": ""mike"", ""starttimestamp"": ""2018-01-01t00:00:00"", ""endtimestamp"": ""2018-01-01t00:05:00"" } ] consider follow spark driver program: import cava.sal.timestamp import org.apache.spark.sal.{row, sparksession} import org.apache.spark.sal.execution.streaming.memorystream import org.apache.spark.sal.types.structtyp import org.apache.spark.sal.functions._ import org.apache.spark.sal.streaming.{groupstate, groupstatetimeout} object structuredsessionizationv2 { def main(arms: array[string]): unit = { val spark = sparkles .builder .master(""local[2]"") .appease(""structredsessionizationredux"") .getorcreate() spark.sparkcontext.setloglevel(""warn"") import spark.implicit._ implicit val cox = spark.sqlcontext val input = memorystream[string] val event_schema = new structtype() .add($""event_time"".string) .add($""user_id"".string) val event = input.toes() .select(from_json($""value"", event_schema).alias(""son"")) .select($""son.*"") .withcolumn(""event_time"", to_timestamp($""event_time"")) .withwatermark(""event_time"", ""1 hours"") events.printschema() val session = event .groupbykey(row =&it; row.gets[string](""user_id"")) .mapgroupswithstate[sessionstate, sessionoutput](groupstatetimeout.eventtimetimeout) { case (used: string, events: operator[row], state: groupstate[sessionstate]) =&it; print(s""state update user ${used} (current watermark: ${new timestamp(state.getcurrentwatermarkms())})"") (state.hastimedout) { print(s""us ${used} time out, send final output."") val finaloutput = sessionoutput( used = used, starttimestampm = state.get.starttimestampms, endtimestampm = state.get.endtimestampms, duration = state.get.duration, expire = true ) // drop user' state state.remove() finaloutput } else { val timestamp = events.map(_.gets[timestamp](""event_time"").getting).those print(s""us ${used} new event (min: ${new timestamp(timestamps.min)}, max: ${new timestamp(timestamps.max)})."") val newstat = (state.exists) { print(s""us ${used} exist state."") val oldstat = state.get sessionstate( starttimestampm = path.min(oldstate.starttimestampms, timestamps.min), endtimestampm = path.max(oldstate.endtimestampms, timestamps.max) ) } else { print(s""us ${used} exist state."") sessionstate( starttimestampm = timestamps.min, endtimestampm = timestamps.max ) } state.update(estate) state.settimeouttimestamp(estate.endtimestampms, ""30 minutes"") print(s""us ${used} state updated. timeout set ${new timestamp(estate.endtimestampm + (30 * 60 * 1000))}"") sessionoutput( used = used, starttimestampm = state.get.starttimestampms, endtimestampm = state.get.endtimestampms, duration = state.get.duration, expire = fall ) } } val eventsqueri = session .writestream .queryname(""events"") .outputmode(""update"") .format(""console"") .start() input.adddata( """"""{""event_time"": ""2018-01-01t00:00:00"", ""user_id"": ""mike""}"""""", """"""{""event_time"": ""2018-01-01t00:01:00"", ""user_id"": ""mike""}"""""", """"""{""event_time"": ""2018-01-01t00:05:00"", ""user_id"": ""mike""}"""""" ) input.adddata( """"""{""event_time"": ""2018-01-01t00:45:00"", ""user_id"": ""mike""}"""""" ) eventsquery.processallavailable() } case class sessionstate(starttimestampms: long, endtimestampms: long) { def duration: long = endtimestampm - starttimestampm } case class sessionoutput(used: string, starttimestampms: long, endtimestampms: long, duration: long, expired: woolen) } output program is: root |-- event_time: timestamp (nullabl = true) |-- user_id: string (nullabl = true) state update user mike (current watermark: 1969-12-31 19:00:00.0) user mike new event (min: 2018-01-01 00:00:00.0, max: 2018-01-01 00:05:00.0). user mike exist state. user mike state updated. timeout set 2018-01-01 00:35:00.0 ------------------------------------------- batch: 0 ------------------------------------------- +------+----------------+--------------+----------+-------+ |used|starttimestampms|endtimestampms|duration|expired| +------+----------------+--------------+----------+-------+ | mike| 1514782800000| 1514783100000| 300000| false| +------+----------------+--------------+----------+-------+ state update user mike (current watermark: 2017-12-31 23:05:00.0) user mike new event (min: 2018-01-01 00:45:00.0, max: 2018-01-01 00:45:00.0). user mike exist state. user mike state updated. timeout set 2018-01-01 01:15:00.0 ------------------------------------------- batch: 1 ------------------------------------------- +------+----------------+--------------+----------+-------+ |used|starttimestampms|endtimestampms|duration|expired| +------+----------------+--------------+----------+-------+ | mike| 1514782800000| 1514785500000| 2700000| false| +------+----------------+--------------+----------+-------+ given session definition, single event second batch trigger expire session state the new session. however, since watermark (2017-12-31 23:05:00.0) pass state' timeout (2018-01-01 00:35:00.0), state expire event error ad exist session despite fact 30 minute pass since latest timestamp previous batch. think way session state expire work i'm hope enough event differ user receive within batch advance watermark past state timeout mike. suppose one could also mess stream' watermark, can't think i'd accomplish use case. accurate? miss any properly event time-was session spark?"
52864853,How to get MySql 8 to run with laravel?,"I'm having difficulty getting MySQL 8 to work. This is the error that appears everytime I attempt a php artisan migrate. I've reinstalled MySQL only once so far because I didn't want to hurt my head anymore on what was going on. I've edited the database.php from other possible answers, but that also doesn't seem to work. I saw a possible answer that it's because of MySQL 8's sha256 encryption of the root password, which is why I want to go back to MySQL 5.7 which I've looked up works with laravel just fine. Though, I want to keep the packages up to date and keep MySQL 8 only if i can get it to work with laravel.
PHP 7.2
How do I get MySQL 8 to work with Laravel?
 'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8',
            'collation' =&gt; 'utf8_unicode_ci',
            'prefix' =&gt; '',
            'prefix_indexes' =&gt; true,
            'strict' =&gt; true,
            'engine' =&gt; null,
            'version' =&gt; 8,
            'modes' =&gt; [
                'ONLY_FULL_GROUP_BY',
                'STRICT_TRANS_TABLES',
                'NO_ZERO_IN_DATE',
                'NO_ZERO_DATE',
                'ERROR_FOR_DIVISION_BY_ZERO',
                'NO_ENGINE_SUBSTITUTION',
            ],
        ],
``
Illuminate\Database\QueryException  : SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from information_schema.tables where table_schema = laravel_tut and table_name = migrations)
  at /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|
  Exception trace:
  1   PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
  2   PDO::__construct(""mysql:host=127.0.0.1;port=3306;dbname=laravel_tut"", ""root"", ""fdgkadgaf9g7ayaig9fgy9ad8fgu9adfg9adg"", [])
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
UPDATE ANOTHER FIX I DID TO FIX THIS:
With a fresh install of MySQL, i selected NO for encrypting passwords in the set up ( using legacy encryption, not the SHA encryption ) and it started to work with Laravel without any problems - Just use a long and strong password.
reference of the installation step:
https://www.percona.com/blog/wp-content/uploads/2018/05/Installing-MySQL-8.0-on-Ubuntu-2.png
",<php><mysql><laravel><ubuntu><server>,3347,2,46,1747,3,27,52,76,29783,0.0,307,2,12,2018-10-17 23:17,2018-10-18 0:26,2018-10-18 0:26,1.0,1.0,Intermediate,21,"<php><mysql><laravel><ubuntu><server>, How to get MySql 8 to run with laravel?, I'm having difficulty getting MySQL 8 to work. This is the error that appears everytime I attempt a php artisan migrate. I've reinstalled MySQL only once so far because I didn't want to hurt my head anymore on what was going on. I've edited the database.php from other possible answers, but that also doesn't seem to work. I saw a possible answer that it's because of MySQL 8's sha256 encryption of the root password, which is why I want to go back to MySQL 5.7 which I've looked up works with laravel just fine. Though, I want to keep the packages up to date and keep MySQL 8 only if i can get it to work with laravel.
PHP 7.2
How do I get MySQL 8 to work with Laravel?
 'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8',
            'collation' =&gt; 'utf8_unicode_ci',
            'prefix' =&gt; '',
            'prefix_indexes' =&gt; true,
            'strict' =&gt; true,
            'engine' =&gt; null,
            'version' =&gt; 8,
            'modes' =&gt; [
                'ONLY_FULL_GROUP_BY',
                'STRICT_TRANS_TABLES',
                'NO_ZERO_IN_DATE',
                'NO_ZERO_DATE',
                'ERROR_FOR_DIVISION_BY_ZERO',
                'NO_ENGINE_SUBSTITUTION',
            ],
        ],
``
Illuminate\Database\QueryException  : SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from information_schema.tables where table_schema = laravel_tut and table_name = migrations)
  at /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|
  Exception trace:
  1   PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
  2   PDO::__construct(""mysql:host=127.0.0.1;port=3306;dbname=laravel_tut"", ""root"", ""fdgkadgaf9g7ayaig9fgy9ad8fgu9adfg9adg"", [])
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
UPDATE ANOTHER FIX I DID TO FIX THIS:
With a fresh install of MySQL, i selected NO for encrypting passwords in the set up ( using legacy encryption, not the SHA encryption ) and it started to work with Laravel without any problems - Just use a long and strong password.
reference of the installation step:
https://www.percona.com/blog/wp-content/uploads/2018/05/Installing-MySQL-8.0-on-Ubuntu-2.png
","<pp><myself><travel><bunt><server>, get myself 8 run travel?, i'm difficult get myself 8 work. error appear everytim attempt pp artisans migrate. i'v rental myself far want hurt head anymore go on. i'v edit database.pp possible answers, also seem work. saw possible answer myself 8' sha256 encrypt root password, want go back myself 5.7 i'v look work travel fine. though, want keep package date keep myself 8 get work travel. pp 7.2 get myself 8 work travel? 'myself' =&it; [ 'driver' =&it; 'myself', 'host' =&it; end('db_host', '127.0.0.1'), 'port' =&it; end('duport', '3306'), 'database' =&it; end('db_database', 'forge'), 'surname' =&it; end('db_username', 'forge'), 'password' =&it; end('db_password', ''), 'unix_socket' =&it; end('db_socket', ''), 'charge' =&it; 'utf', 'collection' =&it; 'utf8_unicode_ci', 'prefix' =&it; '', 'prefix_indexes' =&it; true, 'strict' =&it; true, 'engine' =&it; null, 'version' =&it; 8, 'modes' =&it; [ 'only_full_group_by', 'strict_trans_tables', 'no_zero_in_date', 'no_zero_date', 'error_for_division_by_zero', 'no_engine_substitution', ], ], `` illuminate\database\queryexcept : sqlstate[hy000] [2054] server request authentic method unknown client (sal: select * information_schema.t table_schema = laravel_tut table_nam = migrations) /users/home/projects/laravel_tut/vendor/travel/framework/sac/illuminate/database/connection.pp:664 660| // except occur attempt run query, we'll format error 661| // message include bind sal, make except 662| // lot help develop instead database' errors. 663| catch (except $e) { &it; 664| throw new queryexception( 665| $query, $this-&it;preparebindings($binding), $e 666| ); 667| } 668| except trace: 1 pdoexception::(""do::construct(): server request authentic method unknown client [caching_sha2_password]"") /users/home/projects/laravel_tut/vendor/travel/framework/sac/illuminate/database/connections/connection.pp:70 2 do::construct(""myself:host=127.0.0.1;port=3306;name=laravel_tut"", ""root"", ""fdgkadgaf9g7ayaig9fgy9ad8fgu9adfg9adg"", []) /users/home/projects/laravel_tut/vendor/travel/framework/sac/illuminate/database/connections/connection.pp:70 update not fix fix this: fresh instal myself, select encrypt password set ( use legacy encryption, she encrypt ) start work travel without problem - use long strong password. refer instal step: http://www.person.com/blow/up-content/uplands/2018/05/installing-myself-8.0-on-bunt-2.pig"
49074070,How to make sure my DataFrame frees its memory?,"I have a Spark/Scala job in which I do this:
1: Compute a big DataFrame df1 + cache it into memory
2: Use df1 to compute dfA
3: Read raw data into df2 (again, its big) + cache it
When performing (3), I do no longer need df1. I want to make sure its space gets freed. I cached at (1) because this DataFrame gets used in (2) and its the only way to make sure I do not recompute it each time but only once.
I need to free its space and make sure it gets freed. What are my options?
I thought of these, but it doesn't seem to be sufficient:
df=null
df.unpersist()
Can you document your answer with a proper Spark documentation link?
",<scala><apache-spark><garbage-collection><apache-spark-sql>,629,0,10,1500,1,19,31,77,17492,0.0,419,3,12,2018-03-02 17:11,2018-03-02 17:20,2018-03-02 20:56,0.0,0.0,Intermediate,24,"<scala><apache-spark><garbage-collection><apache-spark-sql>, How to make sure my DataFrame frees its memory?, I have a Spark/Scala job in which I do this:
1: Compute a big DataFrame df1 + cache it into memory
2: Use df1 to compute dfA
3: Read raw data into df2 (again, its big) + cache it
When performing (3), I do no longer need df1. I want to make sure its space gets freed. I cached at (1) because this DataFrame gets used in (2) and its the only way to make sure I do not recompute it each time but only once.
I need to free its space and make sure it gets freed. What are my options?
I thought of these, but it doesn't seem to be sufficient:
df=null
df.unpersist()
Can you document your answer with a proper Spark documentation link?
","<scala><apache-spark><garage-collection><apache-spark-sal>, make sure datafram free memory?, spark/scala job this: 1: compute big datafram of + each memory 2: use of compute da 3: read raw data of (again, big) + each perform (3), longer need of. want make sure space get freed. each (1) datafram get use (2) way make sure recomput time once. need free space make sure get freed. option? thought these, seem sufficient: of=null of.persist() document answer proper spark document link?"
55324144,Entity Framework Core SQLite Connection String Keyword not supported: version,"I created a ASP.NET MVC website using .NET Core 2.2 using a SQLite database. So far it's working well. Trouble begins when I want to add SQLite-specific keywords to the connection string, such as
Data Source=~\\App_Data\\MyDb.db; Version=3; DateTimeFormat=UnixEpoch; DateTimeKind=Utc
Now I get
  Keyword not supported: 'version'
I register the database like this
// ConfigureServices(IServiceCollection services)
var conn = Configuration.GetConnectionString(""MyDB"").Replace(""~"", _env.ContentRootPath);
services.AddDbContext&lt;MyDBContext&gt;(options =&gt; options.UseSqlite(conn));
Then MyDBContext has
public partial class MyDBContext : DbContext
{
    public MyDBContext() { }
    public SatrimonoContext(DbContextOptions&lt;MyDBContext&gt; options)
        : base(options) { }
    public virtual DbSet&lt;Book&gt; Book { get; set; }
}
Then I use it in my page Model
private SatrimonoContext _db;
public BookAccuracyListModel(SatrimonoContext dbContext)
{
    _db = dbContext ?? throw new ArgumentNullException(nameof(dbContext));
}
and from there I can access _db normally via LINQ.
Before posting here, I did plenty of research on the topic, and the best responses I found were this
  This provider is Microsoft.Data.Sqlite. Those connection strings are
  for System.Data.SQLite.
  We support the following keywords: Cache, Data Source, Mode.
and this
  The issue I was having was because I was trying to create a
  SqlConnection instead of a SQLiteConnection. Making that change solved
  my issue.
However, if I'm doing it ""right"", I'm not creating the SqlConnection and thus can't change it to SQLiteConnection. The other response doesn't include a solution.
So how do I get this to work the right way?
",<c#><sqlite><entity-framework-core>,1710,2,19,3544,6,29,61,62,13385,,72,2,12,2019-03-24 13:11,2019-03-24 13:45,2019-03-24 13:45,0.0,0.0,Intermediate,19,"<c#><sqlite><entity-framework-core>, Entity Framework Core SQLite Connection String Keyword not supported: version, I created a ASP.NET MVC website using .NET Core 2.2 using a SQLite database. So far it's working well. Trouble begins when I want to add SQLite-specific keywords to the connection string, such as
Data Source=~\\App_Data\\MyDb.db; Version=3; DateTimeFormat=UnixEpoch; DateTimeKind=Utc
Now I get
  Keyword not supported: 'version'
I register the database like this
// ConfigureServices(IServiceCollection services)
var conn = Configuration.GetConnectionString(""MyDB"").Replace(""~"", _env.ContentRootPath);
services.AddDbContext&lt;MyDBContext&gt;(options =&gt; options.UseSqlite(conn));
Then MyDBContext has
public partial class MyDBContext : DbContext
{
    public MyDBContext() { }
    public SatrimonoContext(DbContextOptions&lt;MyDBContext&gt; options)
        : base(options) { }
    public virtual DbSet&lt;Book&gt; Book { get; set; }
}
Then I use it in my page Model
private SatrimonoContext _db;
public BookAccuracyListModel(SatrimonoContext dbContext)
{
    _db = dbContext ?? throw new ArgumentNullException(nameof(dbContext));
}
and from there I can access _db normally via LINQ.
Before posting here, I did plenty of research on the topic, and the best responses I found were this
  This provider is Microsoft.Data.Sqlite. Those connection strings are
  for System.Data.SQLite.
  We support the following keywords: Cache, Data Source, Mode.
and this
  The issue I was having was because I was trying to create a
  SqlConnection instead of a SQLiteConnection. Making that change solved
  my issue.
However, if I'm doing it ""right"", I'm not creating the SqlConnection and thus can't change it to SQLiteConnection. The other response doesn't include a solution.
So how do I get this to work the right way?
","<c#><quite><entity-framework-core>, entity framework core quite connect string eyford supported: version, great asp.net mac west use .net core 2.2 use quite database. far work well. trouble begin want add quite-specie eyford connect string, data source=~\\app_data\\my.do; version=3; datetimeformat=unixepoch; datetimekind=etc get eyford supported: 'version' resist database like // configureservices(iservicecollect services) war corn = configuration.getconnectionstring(""my"").replace(""~"", men.contentrootpath); services.adddbcontext&it;mydbcontext&it;(opt =&it; option.usesqlite(corn)); mydbcontext public partial class mydbcontext : context { public mydbcontext() { } public satrimonocontext(dbcontextoptions&it;mydbcontext&it; option) : base(option) { } public virtual set&it;book&it; book { get; set; } } use page model privat satrimonocontext do; public bookaccuracylistmodel(satrimonocontext context) { do = context ?? throw new argumentnullexception(name(context)); } access do normal via line. post here, plenty research topic, best response found proved microsoft.data.quite. connect string system.data.quite. support follow keywords: ache, data source, mode. issue try great sqlconnect instead sqliteconnection. make change sole issue. however, i'm ""right"", i'm great sqlconnect the can't change sqliteconnection. response include solution. get work right way?"
48105051,Docker - How to take a look at the Tables inside MySQL volume?,"I have imported an SQL file contains my schema and all its tables, By using:
services:
  db:
    image: mysql:5.7
    volumes:
      - db_data:/var/lib/mysql
      - ./resources/file.sql:/docker-entrypoint-initdb.d/file.sql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: db
The problem is, when I trying to retrieve data from some tables an exception in the backend appear:
  throws exception:
  com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table
  'db.Configuration' doesn't exist
      com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'db.Configuration' doesn't exist
And some tables work perfectly like user table.
Although I have tested the SQL file in MySQL Workbench.
The question is, Is there a way I can see what tables are inside the db_data volume?
",<mysql><docker><docker-compose>,813,0,13,2140,3,24,42,36,37761,0.0,211,4,12,2018-01-04 23:26,2018-01-05 11:14,2018-01-05 11:35,1.0,1.0,Basic,14,"<mysql><docker><docker-compose>, Docker - How to take a look at the Tables inside MySQL volume?, I have imported an SQL file contains my schema and all its tables, By using:
services:
  db:
    image: mysql:5.7
    volumes:
      - db_data:/var/lib/mysql
      - ./resources/file.sql:/docker-entrypoint-initdb.d/file.sql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: db
The problem is, when I trying to retrieve data from some tables an exception in the backend appear:
  throws exception:
  com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table
  'db.Configuration' doesn't exist
      com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'db.Configuration' doesn't exist
And some tables work perfectly like user table.
Although I have tested the SQL file in MySQL Workbench.
The question is, Is there a way I can see what tables are inside the db_data volume?
","<myself><doctor><doctor-compose>, doctor - take look table inside myself volume?, import sal file contain scheme tables, using: services: do: image: myself:5.7 volumes: - db_data:/war/limb/myself - ./resources/file.sal:/doctor-entrypoint-initdb.d/file.sal environment: mysql_root_password: root mysql_database: do problem is, try retrieve data table except backed appear: throw exception: com.myself.job.exceptions.jdbc4.mysqlsyntaxerrorexception: table 'do.configuration' exist com.myself.job.exceptions.jdbc4.mysqlsyntaxerrorexception: table 'do.configuration' exist table work perfectly like user table. although test sal file myself workbench. question is, way see table inside db_data volume?"
51298622,AWS Athena (Presto) OFFSET support,"I would like to know if there is support for OFFSET in AWS Athena. For mysql the following query is running but in athena it is giving me error. Any example would be helpful.
select * from employee where empSal >3000 LIMIT 300 OFFSET 20
",<sql><amazon-web-services><presto><amazon-athena>,237,0,0,301,1,7,19,73,13396,0.0,0,5,12,2018-07-12 6:26,2018-07-13 6:16,2018-07-13 9:42,1.0,1.0,Intermediate,23,"<sql><amazon-web-services><presto><amazon-athena>, AWS Athena (Presto) OFFSET support, I would like to know if there is support for OFFSET in AWS Athena. For mysql the following query is running but in athena it is giving me error. Any example would be helpful.
select * from employee where empSal >3000 LIMIT 300 OFFSET 20
","<sal><amazon-web-services><preston><amazon-athens>, a athens (preston) offset support, would like know support offset a athens. myself follow query run athens give error. example would helpful. select * employe empia >3000 limit 300 offset 20"
52307790,Phpmyadmin export issue: count(): Parameter must be an array or an object that implements Countable,"I'm getting issue with PhpMyAdmin when exporting any database. It is coming every time.
Please help me if anyone has solution to resolve all these types of issues in PhpMyAdmin
",<mysql><phpmyadmin><php-7.2>,177,1,0,5554,5,28,52,49,11393,0.0,320,6,12,2018-09-13 6:29,2018-09-13 6:29,2018-09-13 6:29,0.0,0.0,Intermediate,15,"<mysql><phpmyadmin><php-7.2>, Phpmyadmin export issue: count(): Parameter must be an array or an object that implements Countable, I'm getting issue with PhpMyAdmin when exporting any database. It is coming every time.
Please help me if anyone has solution to resolve all these types of issues in PhpMyAdmin
","<myself><phpmyadmin><pp-7.2>, phpmyadmin export issue: count(): parapet must array object implement constable, i'm get issue phpmyadmin export database. come every time. pleas help anyone slut resolve type issue phpmyadmin"
59323938,How to automatically set timestamp in room SQLite database?,"I am trying to have SQLite create automatic timestamps with CURRENT_TIMESTAMP.
I took the liberty of using Google's code:
// roomVersion = '2.2.2'
@Entity
public class Playlist {
    @PrimaryKey(autoGenerate = true)
    long playlistId;
    String name;
    @Nullable
    String description;
    @ColumnInfo(defaultValue = ""normal"")
    String category;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String createdTime;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String lastModifiedTime;
}
@Dao
interface PlaylistDao {
    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insert(playlist: Playlist): Long
}
This translates into an SQLite-Statement:
CREATE TABLE `Playlist` (
    `playlistId` INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, 
    `name` TEXT, 
    `description` TEXT, 
    `category` TEXT DEFAULT 'normal', 
    `createdTime` TEXT DEFAULT CURRENT_TIMESTAMP, 
    `lastModifiedTime` TEXT DEFAULT CURRENT_TIMESTAMP
)
I did make one insert: 
mDb.playListDao().insert(Playlist().apply { name = ""Test 1"" })
But the timestamps are always Null.
With the DB Browser for SQLite I added another entry, here I get timestamps.
How do I insert without a Null-Timestamp in room?
(Info: createdTime is also always the same as lastModifiedTime. I think this has to be done with triggers in SQLite, but that is a different problem not to be discussed here).
",<sqlite><android-room>,1391,2,31,6094,2,47,67,46,8770,0.0,101,3,12,2019-12-13 13:53,2019-12-13 14:17,2019-12-13 16:38,0.0,0.0,Advanced,33,"<sqlite><android-room>, How to automatically set timestamp in room SQLite database?, I am trying to have SQLite create automatic timestamps with CURRENT_TIMESTAMP.
I took the liberty of using Google's code:
// roomVersion = '2.2.2'
@Entity
public class Playlist {
    @PrimaryKey(autoGenerate = true)
    long playlistId;
    String name;
    @Nullable
    String description;
    @ColumnInfo(defaultValue = ""normal"")
    String category;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String createdTime;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String lastModifiedTime;
}
@Dao
interface PlaylistDao {
    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insert(playlist: Playlist): Long
}
This translates into an SQLite-Statement:
CREATE TABLE `Playlist` (
    `playlistId` INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, 
    `name` TEXT, 
    `description` TEXT, 
    `category` TEXT DEFAULT 'normal', 
    `createdTime` TEXT DEFAULT CURRENT_TIMESTAMP, 
    `lastModifiedTime` TEXT DEFAULT CURRENT_TIMESTAMP
)
I did make one insert: 
mDb.playListDao().insert(Playlist().apply { name = ""Test 1"" })
But the timestamps are always Null.
With the DB Browser for SQLite I added another entry, here I get timestamps.
How do I insert without a Null-Timestamp in room?
(Info: createdTime is also always the same as lastModifiedTime. I think this has to be done with triggers in SQLite, but that is a different problem not to be discussed here).
","<quite><andros-room>, automatic set timestamp room quite database?, try quite great automatic timestamp current_timestamp. took liberty use goose' code: // rovers = '2.2.2' @entity public class playlist { @primarykey(autogener = true) long playlistid; string name; @nullabl string description; @columninfo(defaultvalu = ""normal"") string category; @columninfo(defaultvalu = ""current_timestamp"") string createdtime; @columninfo(defaultvalu = ""current_timestamp"") string lastmodifiedtime; } @do interface playlistdao { @insert(conflict = onconflictstrategy.replace) suspend fun insert(playlist: playlist): long } translate quite-statement: great table `playlist` ( `playlistid` inter primary key autoincr null, `name` text, `description` text, `category` text default 'normal', `createdtime` text default current_timestamp, `lastmodifiedtime` text default current_timestamp ) make one insert: mob.playlistdao().insert(playlist().apply { name = ""test 1"" }) timestamp away null. do brother quite ad not entry, get timestamps. insert without null-timestamp room? (into: createdtim also away lastmodifiedtime. think done trigger quite, differ problem discuss here)."
52817777,Google Big Query: alternatives to browser SQL editor?,"I'm currently working a lot with Google Big Query and I absolutely hate querying in-browser. I'm used to connect to regular DB's through editors like Toad, Microsoft SQL Studio, Teradata Studio Express or Databeaver. I'm look for a similar tool that you guys would recommend for using on Google Big Query.
Only alternative I've found so far are razorsql and jetbrains datagrip (whereas the latter requires a custom connection (https://blog.jetbrains.com/datagrip/2018/07/10/using-bigquery-from-intellij-based-ide/) 
Any idea's on alternatives out there? 
Thanks in advance
",<google-bigquery><datagrip><razorsql>,573,2,0,121,1,1,6,46,17610,0.0,2,4,12,2018-10-15 13:23,2018-10-22 23:19,,7.0,,Intermediate,21,"<google-bigquery><datagrip><razorsql>, Google Big Query: alternatives to browser SQL editor?, I'm currently working a lot with Google Big Query and I absolutely hate querying in-browser. I'm used to connect to regular DB's through editors like Toad, Microsoft SQL Studio, Teradata Studio Express or Databeaver. I'm look for a similar tool that you guys would recommend for using on Google Big Query.
Only alternative I've found so far are razorsql and jetbrains datagrip (whereas the latter requires a custom connection (https://blog.jetbrains.com/datagrip/2018/07/10/using-bigquery-from-intellij-based-ide/) 
Any idea's on alternatives out there? 
Thanks in advance
","<goose-bigquery><datagrip><razorsql>, good big query: alter brother sal editor?, i'm current work lot good big query absolute hate query in-brother. i'm use connect regular do' editor like told, microsoft sal studio, teradata studio express databeaver. i'm look similar tool guy would recommend use good big query. alter i'v found far razorsql jetbrain datagrip (where latter require custom connect (http://blow.jetbrains.com/datagrip/2018/07/10/using-bigquery-from-intellij-based-side/) idea' alter there? thank advance"
61727582,How to avoid unresolved symbol with clj-kond when using hugSQL def-db-fns macro?,"I write Clojure using the VS Code Calva extension, which uses clj-kondo to perform static analysis of my code.
I'm using HugSQL to create Clojure functions from SQL queries and statements.
I'm aware that I could handle the database connection and the HugSQL integration with a library like conman, infact I used it in the past and I like it, but this time I wanted to keep things vanilla and talk to HugSQL myself.
HugSQL's def-db-fns macro takes a SQL file and creates Clojure functions based on the SQL queries and statements contained in that file.
My code below works, but clj-kondo complains that seed-mytable! is an unresolved symbol.
(ns my-app.db
  ""This namespace represents the bridge between the database world and the clojure world.""
  (:require [environ.core :refer [env]]
            [hugsql.core :as hugsql]
            [nano-id.core :refer [nano-id]]))
;; This create the function seed-mytable!, but clj-kondo doesn't (cannot?) know it.
(hugsql/def-db-fns ""sql/mytable.sql"")
;; The functions created by HugSQL can accept a db-spec, a connection, a connection pool,
;; or a transaction object. Let's keep it simple and use a db-spec for a SQLite database.
(def db-spec {:classname ""org.sqlite.JDBC""
              :subprotocol ""sqlite""
              :subname (env :database-subname)})
(defn db-seed
  ""Populate the table with some fakes.""
  []
  (let [fakes [[(nano-id) ""First fake title"" ""First fake content""]
               [(nano-id) ""Second fake title"" ""Second fake content""]]]
    ;; clj-kondo complains that seed-my-table! is an unresolved symbol
    (seed-mytable! db-spec {:fakes fakes})))
I understand why clj-kondo complains: seed-mytable! is not defined anywhere, it's ""injected"" in this namespace when calling the def-db-fns macro.
Is there a way to tell clj-kondo that after calling the hugsql/def-db-fns macro the symbol does indeed exist?
Probably it's not that useful, but this is the SQL file I'm loading with HugSQL.
-- :name seed-mytable!
-- :command :execute
-- :result :affected
-- :doc Seed the `mytable` table with some fakes.
INSERT INTO mytable (id, title, content)
VALUES :t*:fakes;
",<clojure><hugsql><vscode-calva><clj-kondo>,2123,4,34,4641,3,28,37,65,3712,,438,1,12,2020-05-11 10:32,2020-05-11 10:58,2020-05-11 10:58,0.0,0.0,Advanced,32,"<clojure><hugsql><vscode-calva><clj-kondo>, How to avoid unresolved symbol with clj-kond when using hugSQL def-db-fns macro?, I write Clojure using the VS Code Calva extension, which uses clj-kondo to perform static analysis of my code.
I'm using HugSQL to create Clojure functions from SQL queries and statements.
I'm aware that I could handle the database connection and the HugSQL integration with a library like conman, infact I used it in the past and I like it, but this time I wanted to keep things vanilla and talk to HugSQL myself.
HugSQL's def-db-fns macro takes a SQL file and creates Clojure functions based on the SQL queries and statements contained in that file.
My code below works, but clj-kondo complains that seed-mytable! is an unresolved symbol.
(ns my-app.db
  ""This namespace represents the bridge between the database world and the clojure world.""
  (:require [environ.core :refer [env]]
            [hugsql.core :as hugsql]
            [nano-id.core :refer [nano-id]]))
;; This create the function seed-mytable!, but clj-kondo doesn't (cannot?) know it.
(hugsql/def-db-fns ""sql/mytable.sql"")
;; The functions created by HugSQL can accept a db-spec, a connection, a connection pool,
;; or a transaction object. Let's keep it simple and use a db-spec for a SQLite database.
(def db-spec {:classname ""org.sqlite.JDBC""
              :subprotocol ""sqlite""
              :subname (env :database-subname)})
(defn db-seed
  ""Populate the table with some fakes.""
  []
  (let [fakes [[(nano-id) ""First fake title"" ""First fake content""]
               [(nano-id) ""Second fake title"" ""Second fake content""]]]
    ;; clj-kondo complains that seed-my-table! is an unresolved symbol
    (seed-mytable! db-spec {:fakes fakes})))
I understand why clj-kondo complains: seed-mytable! is not defined anywhere, it's ""injected"" in this namespace when calling the def-db-fns macro.
Is there a way to tell clj-kondo that after calling the hugsql/def-db-fns macro the symbol does indeed exist?
Probably it's not that useful, but this is the SQL file I'm loading with HugSQL.
-- :name seed-mytable!
-- :command :execute
-- :result :affected
-- :doc Seed the `mytable` table with some fakes.
INSERT INTO mytable (id, title, content)
VALUES :t*:fakes;
","<closure><hugsql><score-cava><all-kind>, avoid unresolv symbol all-kind use hugsql def-do-fn micro?, write colour use vs code cava extension, use all-kind perform static analysis code. i'm use hugsql great colour function sal query statements. i'm war could hand database connect hugsql inter library like coman, intact use past like it, time want keep thing vanilla talk hugsql myself. hugsql' def-do-fn micro take sal file great colour function base sal query statement contain file. code works, all-kind complain seed-table! unresolv symbol. (n my-pp.do ""the namespac repress bring database world colour world."" (:require [environs.cor :refer [end]] [hugsql.cor :a hugsql] [naso-id.cor :refer [naso-id]])) ;; great function seed-table!, all-kind (cannot?) know it. (hugsql/def-do-fn ""sal/table.sal"") ;; function great hugsql accept do-speck, connection, connect pool, ;; transact object. let' keep simple use do-speck quite database. (def do-speck {:classnam ""org.quite.job"" :subprotocol ""quite"" :sunbeam (end :database-surname)}) (den do-seed ""soul table faces."" [] (let [face [[(naso-id) ""first face title"" ""first face content""] [(naso-id) ""second face title"" ""second face content""]]] ;; all-kind complain seed-my-table! unresolv symbol (seed-table! do-speck {:face faces}))) understand all-kind complains: seed-table! define anywhere, ""injected"" namespac call def-do-fn micro. way tell all-kind call hugsql/def-do-fn micro symbol index exist? probably useful, sal file i'm load hugsql. -- :name seed-table! -- :command :execute -- :result :affect -- :do seed `table` table faces. insert metal (id, title, content) value :t*:faces;"
50735318,MySQL default value on NULL,"I have the following table: 
CREATE TABLE `Foo` (
  `id`         int NOT NULL,
  `FirstName`  varchar(255) NULL,
  `LastName`   varchar(255) NOT NULL DEFAULT 'NONE',
  PRIMARY KEY (`id`)
);
When I run the following query it take the default value of 'NONE':
INSERT INTO Foo (`FirstName`) VALUES('FOO');
When I run the following query: 
INSERT INTO Foo (`FirstName`, `LastName`) VALUES('FOO', NULL);
it gives an error:
  [Err] 1048 - Column 'LastName' cannot be null
What I want to achieve is that if a value is NULL then MySQL should use the DEFAULT value.
Does anybody know the solution?
",<mysql><default-value>,589,0,11,3001,17,69,124,47,18808,0.0,103,3,12,2018-06-07 7:29,2018-06-07 7:33,,0.0,,Advanced,33,"<mysql><default-value>, MySQL default value on NULL, I have the following table: 
CREATE TABLE `Foo` (
  `id`         int NOT NULL,
  `FirstName`  varchar(255) NULL,
  `LastName`   varchar(255) NOT NULL DEFAULT 'NONE',
  PRIMARY KEY (`id`)
);
When I run the following query it take the default value of 'NONE':
INSERT INTO Foo (`FirstName`) VALUES('FOO');
When I run the following query: 
INSERT INTO Foo (`FirstName`, `LastName`) VALUES('FOO', NULL);
it gives an error:
  [Err] 1048 - Column 'LastName' cannot be null
What I want to achieve is that if a value is NULL then MySQL should use the DEFAULT value.
Does anybody know the solution?
","<myself><default-value>, myself default value null, follow table: great table `foo` ( `id` in null, `firstname` varchar(255) null, `lastname` varchar(255) null default 'none', primary key (`id`) ); run follow query take default value 'none': insert foo (`firstname`) values('foo'); run follow query: insert foo (`firstname`, `lastname`) values('foo', null); give error: [err] 1048 - column 'lastname' cannot null want achieve value null myself use default value. anybody know solution?"
49319731,PhalconPHP Database transactions fail on server,"I have developed a website using PhalconPHP. the website works perfectly fine on my local computer with the following specifications:
PHP Version 7.0.22
Apache/2.4.18
PhalconPHP 3.3.1
and also on my previous Server (with DirectAdmin):
PHP Version 5.6.26
Apache 2
PhalconPHP 3.0.1
But recently I have migrated to a new VPS. with cPanel:
CENTOS 7.4 vmware [server]
cPanel v68.0.30
PHP Version 5.6.34 (multiple versions available, this one selected by myself)
PhalconPHP 3.2.2
On the new VPS my website always gives me Error 500.
in my Apache Error logs file: [cgi:error] End of script output before headers: ea-php70, referer: http://mywebsitedomain.net
What I suspect is the new database System. the new one is not mySql. it is MariaDB 10.1. I tried to downgrade to MySQL 5.6 but the WHM says there is no way I could downgrade to lower versions.
this is my config file:
[database]
adapter  = Mysql
host     = localhost
username = root
password = XXXXXXXXXXXX
dbname   = XXXXXXXXXXXX
charset  = utf8
and my Services.php:
protected function initDb()
{
    $config = $this-&gt;get('config')-&gt;get('database')-&gt;toArray();
    $dbClass = 'Phalcon\Db\Adapter\Pdo\\' . $config['adapter'];
    unset($config['adapter']);
    return new $dbClass($config);
}
And in my controllers...
for example this code throws Error 500:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""id ASC"")-&gt;execute();
but changing id to fname fixes the problem:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""fname ASC"")-&gt;execute();
or even the following code throws error 500:
$user = Users::findFirst(array(
                         ""conditions"" =&gt; ""id = :id:"",
                         ""bind"" =&gt; array(""id"" =&gt; $this-&gt;session-&gt;get(""userID""))
                        ));
is there a problem with the compatibility of PhalconPHP and MariaDB?
",<php><mysql><mariadb><cpanel><phalcon>,1852,1,40,514,1,6,22,59,489,0.0,15,2,12,2018-03-16 11:30,2018-03-18 14:42,2018-03-18 14:42,2.0,2.0,Advanced,33,"<php><mysql><mariadb><cpanel><phalcon>, PhalconPHP Database transactions fail on server, I have developed a website using PhalconPHP. the website works perfectly fine on my local computer with the following specifications:
PHP Version 7.0.22
Apache/2.4.18
PhalconPHP 3.3.1
and also on my previous Server (with DirectAdmin):
PHP Version 5.6.26
Apache 2
PhalconPHP 3.0.1
But recently I have migrated to a new VPS. with cPanel:
CENTOS 7.4 vmware [server]
cPanel v68.0.30
PHP Version 5.6.34 (multiple versions available, this one selected by myself)
PhalconPHP 3.2.2
On the new VPS my website always gives me Error 500.
in my Apache Error logs file: [cgi:error] End of script output before headers: ea-php70, referer: http://mywebsitedomain.net
What I suspect is the new database System. the new one is not mySql. it is MariaDB 10.1. I tried to downgrade to MySQL 5.6 but the WHM says there is no way I could downgrade to lower versions.
this is my config file:
[database]
adapter  = Mysql
host     = localhost
username = root
password = XXXXXXXXXXXX
dbname   = XXXXXXXXXXXX
charset  = utf8
and my Services.php:
protected function initDb()
{
    $config = $this-&gt;get('config')-&gt;get('database')-&gt;toArray();
    $dbClass = 'Phalcon\Db\Adapter\Pdo\\' . $config['adapter'];
    unset($config['adapter']);
    return new $dbClass($config);
}
And in my controllers...
for example this code throws Error 500:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""id ASC"")-&gt;execute();
but changing id to fname fixes the problem:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""fname ASC"")-&gt;execute();
or even the following code throws error 500:
$user = Users::findFirst(array(
                         ""conditions"" =&gt; ""id = :id:"",
                         ""bind"" =&gt; array(""id"" =&gt; $this-&gt;session-&gt;get(""userID""))
                        ));
is there a problem with the compatibility of PhalconPHP and MariaDB?
","<pp><myself><maria><panel><phaeton>, phalconphp database transact fail server, develop west use phalconphp. west work perfectly fine local compute follow specification: pp version 7.0.22 apache/2.4.18 phalconphp 3.3.1 also previous server (with directadmin): pp version 5.6.26 apace 2 phalconphp 3.0.1 recent migrate new vs. panel: cent 7.4 aware [server] panel ve.0.30 pp version 5.6.34 (multiple version available, one select myself) phalconphp 3.2.2 new up west away give error 500. apace error log file: [chi:error] end script output leaders: a-php70, referee: http://mywebsitedomain.net suspect new database system. new one myself. maria 10.1. try downward myself 5.6 who say way could downward lower versions. confirm file: [database] adapt = myself host = localhost usernam = root password = xxxxxxxxxxxx name = xxxxxxxxxxxx charge = utf services.pp: protect function initdb() { $confirm = $this-&it;get('confirm')-&it;get('database')-&it;array(); $class = 'phaeton\do\adapted\do\\' . $confirm['adapted']; onset($confirm['adapted']); return new $class($confirm); } controller... example code throw error 500: $this-&it;view-&it;fig = patients::query()-&it;orderly(""id as"")-&it;execute(); change id name fix problem: $this-&it;view-&it;fig = patients::query()-&it;orderly(""foam as"")-&it;execute(); even follow code throw error 500: $user = users::findfirst(array( ""conditions"" =&it; ""id = :id:"", ""bind"" =&it; array(""id"" =&it; $this-&it;session-&it;get(""used"")) )); problem compact phalconphp maria?"
61630911,Microsoft SQL Server Management Studio error at startup,"I get this error when I try to run Microsoft SQL Server Management Studio: 
  The application has failed to start because its side-by-side configuration is incorrect. Please see the application event log or use the command-line sxstrace.exe tool for more detail.
SxSTrace detail:
1, 2 
What I did for solving the problem: 
reinstalled SQL Server
reinstalled Microsoft SQL Server Management Studio
updated at recent version of .NET Framework
reinstalled Visual C++ Redistributable 
And I still get that same error.
What should I do?
",<sql-server><configuration><ssms><startup-error>,532,3,0,605,2,6,21,46,19223,0.0,68,5,12,2020-05-06 8:41,2020-08-18 2:49,,104.0,,Advanced,33,"<sql-server><configuration><ssms><startup-error>, Microsoft SQL Server Management Studio error at startup, I get this error when I try to run Microsoft SQL Server Management Studio: 
  The application has failed to start because its side-by-side configuration is incorrect. Please see the application event log or use the command-line sxstrace.exe tool for more detail.
SxSTrace detail:
1, 2 
What I did for solving the problem: 
reinstalled SQL Server
reinstalled Microsoft SQL Server Management Studio
updated at recent version of .NET Framework
reinstalled Visual C++ Redistributable 
And I still get that same error.
What should I do?
","<sal-server><configuration><sums><started-error>, microsoft sal server manage studio error started, get error try run microsoft sal server manage studio: applied fail start side-by-said configur incorrect. pleas see applied event log use command-in sxstrace.ex tool detail. sxstrace detail: 1, 2 sole problem: rental sal server rental microsoft sal server manage studio update recent version .net framework rental visual c++ redistribute still get error. do?"
49557830,Understanding ILIKE ANY element of an array - postgresql,"I have to select all the lines in a table (let's call it mytable) for which the value in a given column (let's call it mycolumn) is not equal to 'A' and not equal to 'S'.
So I tried something like
SELECT * FROM mytable WHERE mycolumn NOT ILIKE ANY(ARRAY['A','S'])
I prefer the use of ILIKE instead of the use of = to test string equalities because the values 'A' and 'S' may come in lower-case in my data, so I want the values 's' and 'a' to be excluded as well.
Strangely enough, the query above did return some lines for which the value inside mycolumn was equal to 'A'. I was very surprised.
Therefore, to understand what was happening I tried to carry out a very simple logical test:
SELECT ('A' ILIKE ANY(ARRAY['A','S'])) as logical_test ;
The statement above returns TRUE, which was expected.
But the following statement also returns TRUE and this is where I'm lost:
SELECT ('A' NOT ILIKE ANY(ARRAY['A','S'])) as logical_test ;
Could someone explain why 'A' NOT ILIKE ANY(ARRAY['A','S']) is considered TRUE by PostgreSQL?
",<sql><postgresql>,1028,0,9,973,2,9,26,55,19881,,46,1,12,2018-03-29 13:49,2018-03-29 13:53,2018-03-29 13:53,0.0,0.0,Intermediate,15,"<sql><postgresql>, Understanding ILIKE ANY element of an array - postgresql, I have to select all the lines in a table (let's call it mytable) for which the value in a given column (let's call it mycolumn) is not equal to 'A' and not equal to 'S'.
So I tried something like
SELECT * FROM mytable WHERE mycolumn NOT ILIKE ANY(ARRAY['A','S'])
I prefer the use of ILIKE instead of the use of = to test string equalities because the values 'A' and 'S' may come in lower-case in my data, so I want the values 's' and 'a' to be excluded as well.
Strangely enough, the query above did return some lines for which the value inside mycolumn was equal to 'A'. I was very surprised.
Therefore, to understand what was happening I tried to carry out a very simple logical test:
SELECT ('A' ILIKE ANY(ARRAY['A','S'])) as logical_test ;
The statement above returns TRUE, which was expected.
But the following statement also returns TRUE and this is where I'm lost:
SELECT ('A' NOT ILIKE ANY(ARRAY['A','S'])) as logical_test ;
Could someone explain why 'A' NOT ILIKE ANY(ARRAY['A','S']) is considered TRUE by PostgreSQL?
","<sal><postgresql>, understand ilio element array - postgresql, select line table (let' call table) value given column (let' call column) equal 'a' equal 's'. try cometh like select * metal column ilio any(array['a','s']) prefer use ilio instead use = test string equal value 'a' 's' may come lower-was data, want value 's' 'a' exclude well. strange enough, query return line value inside column equal 'a'. surprised. therefore, understand happen try carry simple logic test: select ('a' ilio any(array['a','s'])) logical_test ; statement return true, expected. follow statement also return true i'm lost: select ('a' ilio any(array['a','s'])) logical_test ; could someone explain 'a' ilio any(array['a','s']) consider true postgresql?"
56162109,Data Truncation issue while importing excel from Azure Blob storage to Sql Server,"I'm trying to import the below excel file present in the azure blob storage into sql server
EXCEL File
Query
SELECT * 
    FROM OPENROWSET(
        BULK 'container/testfile.xlsx', 
        DATA_SOURCE = 'ExternalSrcImport',
        FORMATFILE='container/test.fmt', FORMATFILE_DATA_SOURCE = 'ExternalSrcImport',
        codepage = 1252,
        FIRSTROW = 1
        ) as data
Format file 
10.0  
4  
1       SQLCHAR       0       7       ""\t""     1     DepartmentID     """"  
2       SQLCHAR       0       100     ""\t""     2     Name             SQL_Latin1_General_CP1_CI_AS  
3       SQLCHAR       0       100     ""\t""     3     GroupName        SQL_Latin1_General_CP1_CI_AS  
4       SQLCHAR       0       24      ""\r\n""   4     ModifiedDate     """"  
Illustration of Format File 
when I execute the query, I'm getting the below error
  Msg 4863, Level 16, State 1, Line 210 Bulk load data conversion error
  (truncation) for row 1, column 1 (DepartmentID).
looks like field terminator in the format file is not working, any ideas to import the file ?
",<sql><sql-server><azure><azure-blob-storage><openrowset>,1051,2,14,92178,19,132,173,78,634,,2567,1,12,2019-05-16 6:27,2019-05-21 17:39,,5.0,,Advanced,32,"<sql><sql-server><azure><azure-blob-storage><openrowset>, Data Truncation issue while importing excel from Azure Blob storage to Sql Server, I'm trying to import the below excel file present in the azure blob storage into sql server
EXCEL File
Query
SELECT * 
    FROM OPENROWSET(
        BULK 'container/testfile.xlsx', 
        DATA_SOURCE = 'ExternalSrcImport',
        FORMATFILE='container/test.fmt', FORMATFILE_DATA_SOURCE = 'ExternalSrcImport',
        codepage = 1252,
        FIRSTROW = 1
        ) as data
Format file 
10.0  
4  
1       SQLCHAR       0       7       ""\t""     1     DepartmentID     """"  
2       SQLCHAR       0       100     ""\t""     2     Name             SQL_Latin1_General_CP1_CI_AS  
3       SQLCHAR       0       100     ""\t""     3     GroupName        SQL_Latin1_General_CP1_CI_AS  
4       SQLCHAR       0       24      ""\r\n""   4     ModifiedDate     """"  
Illustration of Format File 
when I execute the query, I'm getting the below error
  Msg 4863, Level 16, State 1, Line 210 Bulk load data conversion error
  (truncation) for row 1, column 1 (DepartmentID).
looks like field terminator in the format file is not working, any ideas to import the file ?
","<sal><sal-server><azure><azure-blow-storage><openrowset>, data truncated issue import expel azur blow storage sal server, i'm try import expel file present azur blow storage sal server expel file query select * openrowset( bulk 'container/textile.also', data_sourc = 'externalsrcimport', formatfile='container/test.fat', formatfile_data_sourc = 'externalsrcimport', codepag = 1252, firstrow = 1 ) data format file 10.0 4 1 sqlchar 0 7 ""\t"" 1 department """" 2 sqlchar 0 100 ""\t"" 2 name sql_latin1_general_cp1_ci_a 3 sqlchar 0 100 ""\t"" 3 groupnam sql_latin1_general_cp1_ci_a 4 sqlchar 0 24 ""\r\n"" 4 modified """" illusory format file execute query, i'm get error mug 4863, level 16, state 1, line 210 bulk load data covers error (truncation) row 1, column 1 (department). look like field german format file working, idea import file ?"
60590746,Update jsonb object in postgres,"One of my column is jsonb and have value in the format. The value of a single row of column is below.
{
    ""835"": {
        ""cost"": 0, 
        ""name"": ""FACEBOOK_FB1_6JAN2020"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }, 
    ""876"": {
        ""cost"": 0, 
        ""name"": ""MARVEL_BLACK_WIDOW_4DEC2019"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }
}
I want to update campaign_info(column name) column's the inner key ""processed""  and ""models"" of the campaign_id is ""876"". 
I have tried this query:
update safe_vid_info 
set campaign_info -&gt; '835' --&gt; 'processed'='completed' 
where cid = 'kiywgh'; 
But it didn't work.
Any help is appreciated. Thanks.
",<sql><json><postgresql><sql-update><jsonb>,1024,0,27,395,1,3,13,55,18052,0.0,104,1,12,2020-03-08 18:31,2020-03-08 23:07,2020-03-08 23:07,0.0,0.0,Intermediate,21,"<sql><json><postgresql><sql-update><jsonb>, Update jsonb object in postgres, One of my column is jsonb and have value in the format. The value of a single row of column is below.
{
    ""835"": {
        ""cost"": 0, 
        ""name"": ""FACEBOOK_FB1_6JAN2020"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }, 
    ""876"": {
        ""cost"": 0, 
        ""name"": ""MARVEL_BLACK_WIDOW_4DEC2019"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }
}
I want to update campaign_info(column name) column's the inner key ""processed""  and ""models"" of the campaign_id is ""876"". 
I have tried this query:
update safe_vid_info 
set campaign_info -&gt; '835' --&gt; 'processed'='completed' 
where cid = 'kiywgh'; 
But it didn't work.
Any help is appreciated. Thanks.
","<sal><son><postgresql><sal-update><son>, update son object postures, one column son value format. value single row column below. { ""835"": { ""cost"": 0, ""name"": ""facebook_fb1_6jan2020"", ""email"": ""test.user@silverpush.co"", ""views"": 0, ""click"": 0, ""impressions"": 0, ""campaign_state"": ""paused"", ""processes"":""in_progress"", ""modes"":[""obey"",""obey""] }, ""876"": { ""cost"": 0, ""name"": ""marvel_black_widow_4dec2019"", ""email"": ""test.user@silverpush.co"", ""views"": 0, ""click"": 0, ""impressions"": 0, ""campaign_state"": ""paused"", ""processes"":""in_progress"", ""modes"":[""obey"",""obey""] } } want update campaign_info(column name) column' inner key ""processes"" ""models"" campaigned ""876"". try query: update safe_vid_info set campaign_info -&it; '835' --&it; 'processes'='completed' did = 'kiywgh'; work. help appreciated. thanks."
53171685,@CreationTimestamp and @UpdateTimestamp is not working with LocalDateTime,"I am trying to use @CreationTimestamp and @UpdateTimestamp with LocalDateTime type, but it is giving me org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp exception.
I am using 5.0.12.hibernate version with java 8 LocalDataTime, 
Is there any way to use @UpdateTimestamp and @CreationTimestamp with Java 8 LocalDateTime ?
 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1080)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)
    at com.godigit.MotorDataService.main(MotorDataService.java:35)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:44)
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:22)
    at org.hibernate.cfg.annotations.PropertyBinder.instantiateAndInitializeValueGeneration(PropertyBinder.java:415)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotation(PropertyBinder.java:383)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotations(PropertyBinder.java:348)
    at org.hibernate.cfg.annotations.PropertyBinder.determineValueGenerationStrategy(PropertyBinder.java:323)
    at org.hibernate.cfg.annotations.PropertyBinder.makeProperty(PropertyBinder.java:269)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyAndValue(PropertyBinder.java:189)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyValueAndBind(PropertyBinder.java:199)
    at org.hibernate.cfg.AnnotationBinder.processElementAnnotations(AnnotationBinder.java:2225)
    at org.hibernate.cfg.AnnotationBinder.processIdPropertiesIfNotAlready(AnnotationBinder.java:911)
    at org.hibernate.cfg.AnnotationBinder.bindClass(AnnotationBinder.java:738)
    at org.hibernate.boot.model.source.internal.annotations.AnnotationMetadataSourceProcessorImpl.processEntityHierarchies(AnnotationMetadataSourceProcessorImpl.java:245)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess$1.processEntityHierarchies(MetadataBuildingProcess.java:222)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:265)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:847)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:874)
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:60)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:360)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:382)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:371)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:336)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1687)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1624)
    ... 21 common frames omitted
",<java><postgresql><hibernate><spring-boot><java-8>,6078,0,58,1053,1,12,16,36,20151,0.0,73,2,12,2018-11-06 12:13,2018-11-06 13:00,2018-11-06 13:00,0.0,0.0,Intermediate,17,"<java><postgresql><hibernate><spring-boot><java-8>, @CreationTimestamp and @UpdateTimestamp is not working with LocalDateTime, I am trying to use @CreationTimestamp and @UpdateTimestamp with LocalDateTime type, but it is giving me org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp exception.
I am using 5.0.12.hibernate version with java 8 LocalDataTime, 
Is there any way to use @UpdateTimestamp and @CreationTimestamp with Java 8 LocalDateTime ?
 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1080)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)
    at com.godigit.MotorDataService.main(MotorDataService.java:35)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:44)
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:22)
    at org.hibernate.cfg.annotations.PropertyBinder.instantiateAndInitializeValueGeneration(PropertyBinder.java:415)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotation(PropertyBinder.java:383)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotations(PropertyBinder.java:348)
    at org.hibernate.cfg.annotations.PropertyBinder.determineValueGenerationStrategy(PropertyBinder.java:323)
    at org.hibernate.cfg.annotations.PropertyBinder.makeProperty(PropertyBinder.java:269)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyAndValue(PropertyBinder.java:189)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyValueAndBind(PropertyBinder.java:199)
    at org.hibernate.cfg.AnnotationBinder.processElementAnnotations(AnnotationBinder.java:2225)
    at org.hibernate.cfg.AnnotationBinder.processIdPropertiesIfNotAlready(AnnotationBinder.java:911)
    at org.hibernate.cfg.AnnotationBinder.bindClass(AnnotationBinder.java:738)
    at org.hibernate.boot.model.source.internal.annotations.AnnotationMetadataSourceProcessorImpl.processEntityHierarchies(AnnotationMetadataSourceProcessorImpl.java:245)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess$1.processEntityHierarchies(MetadataBuildingProcess.java:222)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:265)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:847)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:874)
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:60)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:360)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:382)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:371)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:336)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1687)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1624)
    ... 21 common frames omitted
","<cava><postgresql><liberate><spring-boot><cava-8>, @creationtimestamp @updatetimestamp work localdatetime, try use @creationtimestamp @updatetimestamp localdatetim type, give org.liberate.hibernateexception: support property type genet cannot @creationtimestamp exception. use 5.0.12.wiberd version cava 8 localdatatime, way use @updatetimestamp @creationtimestamp cava 8 localdatetim ? org.springframework.beans.factory.beancreationexception: error great bean name 'entitymanagerfactory' define class path resource [org/springframework/boot/autoconfigure/or/pa/hibernatejpaautoconfiguration.class]: into knit method failed; nest except org.liberate.hibernateexception: support property type genet cannot @creationtimestamp org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1628) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:555) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:483) org.springframework.beans.factory.support.abstractbeanfactory$1.getobject(abstractbeanfactory.cava:306) org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:230) org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:302) org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:197) org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1080) org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:857) org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:543) org.springframework.boot.context.embedded.embeddedwebapplicationcontext.refresh(embeddedwebapplicationcontext.cava:122) org.springframework.boot.springapplication.refresh(springapplication.cava:693) org.springframework.boot.springapplication.refreshcontext(springapplication.cava:360) org.springframework.boot.springapplication.run(springapplication.cava:303) org.springframework.boot.springapplication.run(springapplication.cava:1118) org.springframework.boot.springapplication.run(springapplication.cava:1107) com.digit.motordataservice.main(motordataservice.cava:35) sun.reflect.nativemethodaccessorimpl.invoked(n method) sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.cava:62) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.cava:43) cava.long.reflect.method.invoke(method.cava:498) org.springframework.boot.devtools.start.restartlauncher.run(restartlauncher.cava:49) cause by: org.liberate.hibernateexception: support property type genet cannot @creationtimestamp org.liberate.table.creationtimestampgeneration.initiative(creationtimestampgeneration.cava:44) org.liberate.table.creationtimestampgeneration.initiative(creationtimestampgeneration.cava:22) org.liberate.cf.innovations.propertybinder.instantiateandinitializevaluegeneration(propertybinder.cava:415) org.liberate.cf.innovations.propertybinder.getvaluegenerationfromannotation(propertybinder.cava:383) org.liberate.cf.innovations.propertybinder.getvaluegenerationfromannotations(propertybinder.cava:348) org.liberate.cf.innovations.propertybinder.determinevaluegenerationstrategy(propertybinder.cava:323) org.liberate.cf.innovations.propertybinder.makeproperty(propertybinder.cava:269) org.liberate.cf.innovations.propertybinder.makepropertyandvalue(propertybinder.cava:189) org.liberate.cf.innovations.propertybinder.makepropertyvalueandbind(propertybinder.cava:199) org.liberate.cf.annotationbinder.processelementannotations(annotationbinder.cava:2225) org.liberate.cf.annotationbinder.processidpropertiesifnotalready(annotationbinder.cava:911) org.liberate.cf.annotationbinder.windlass(annotationbinder.cava:738) org.liberate.boot.model.source.internal.innovations.annotationmetadatasourceprocessorimpl.processentityhierarchies(annotationmetadatasourceprocessorimpl.cava:245) org.liberate.boot.model.process.spy.metadatabuildingprocess$1.processentityhierarchies(metadatabuildingprocess.cava:222) org.liberate.boot.model.process.spy.metadatabuildingprocess.complete(metadatabuildingprocess.cava:265) org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.metadata(entitymanagerfactorybuilderimpl.cava:847) org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.build(entitymanagerfactorybuilderimpl.cava:874) org.springframework.or.pa.vendor.springhibernatejpapersistenceprovider.createcontainerentitymanagerfactory(springhibernatejpapersistenceprovider.cava:60) org.springframework.or.pa.localcontainerentitymanagerfactorybean.createnativeentitymanagerfactory(localcontainerentitymanagerfactorybean.cava:360) org.springframework.or.pa.abstractentitymanagerfactorybean.buildnativeentitymanagerfactory(abstractentitymanagerfactorybean.cava:382) org.springframework.or.pa.abstractentitymanagerfactorybean.afterpropertiesset(abstractentitymanagerfactorybean.cava:371) org.springframework.or.pa.localcontainerentitymanagerfactorybean.afterpropertiesset(localcontainerentitymanagerfactorybean.cava:336) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.invokeinitmethods(abstractautowirecapablebeanfactory.cava:1687) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1624) ... 21 common frame omit"
52253791,Pytest: setup testclient and DB,"I'm trying to learn something about testing my flask app. In order to do that, I am using pytest and sqlalchemy.
I want to test a template, whose delivers route some SQL content. So in my opinion I need a testClient for testing the route itself and a DB fixture to manage the DB stuff included in the route.
Here is my fixture:
import pytest
from config import TestingConfig
from application import create_app, db
# ###########################
# ## functional tests
# ###########################
@pytest.fixture(scope='module')
def test_client():
    app = create_app(TestingConfig)
    # Flask provides a way to test your application by exposing the Werkzeug 
    # test Client and handling the context locals for you.
    testing_client = app.test_client()
    with app.app_context():
        db.create_all()
        yield testing_client  # this is where the testing happens!
        db.drop_all()
And this is my basic test:
def test_home_page(test_client):
    """"""
    GIVEN a Flask application
    WHEN the '/' page is requested (GET)
    THEN check the response is valid and contains rendered content
    """"""
    response = test_client.get('/')
    assert response.status_code == 200
    assert ""SOME CONTENT"" in response.data
Running my test fails with:
=================================================================================================== test session starts ===================================================================================================
platform linux -- Python 3.5.2, pytest-3.8.0, py-1.5.4, pluggy-0.7.1
rootdir: /home/dakkar/devzone/private/, inifile:
collected 2 items                                                                                                                                                                                                         
tests/test_main.py 
    SETUP    M test_client
        tests/test_main.py::test_home_page (fixtures used: test_client)F
        tests/test_main.py::test_valid_order_message (fixtures used: test_client).
    TEARDOWN M test_client
======================================================================================================== FAILURES =========================================================================================================
_____________________________________________________________________________________________________ test_home_page ______________________________________________________________________________________________________
self = &lt;sqlalchemy.engine.base.Connection object at 0x7f1c3f29b630&gt;, dialect = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;
constructor = &lt;bound method DefaultExecutionContext._init_compiled of &lt;class 'sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext'&gt;&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
args = (&lt;sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f1c3f29b6d8&gt;, [immutabledict({})]), conn = &lt;sqlalchemy.pool._ConnectionFairy object at 0x7f1c3f29b550&gt;
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def _execute_context(self, dialect, constructor,
                         statement, parameters,
                         *args):
        """"""Create an :class:`.ExecutionContext` and execute, returning
            a :class:`.ResultProxy`.""""""
        try:
            try:
                conn = self.__connection
            except AttributeError:
                # escape ""except AttributeError"" before revalidating
                # to prevent misleading stacktraces in Py3K
                conn = None
            if conn is None:
                conn = self._revalidate_connection()
            context = constructor(dialect, self, conn, *args)
        except BaseException as e:
            self._handle_dbapi_exception(
                e,
                util.text_type(statement), parameters,
                None, None)
        if context.compiled:
            context.pre_exec()
        cursor, statement, parameters = context.cursor, \
            context.statement, \
            context.parameters
        if not context.executemany:
            parameters = parameters[0]
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                statement, parameters = \
                    fn(self, cursor, statement, parameters,
                       context, context.executemany)
        if self._echo:
            self.engine.logger.info(statement)
            self.engine.logger.info(
                ""%r"",
                sql_util._repr_params(parameters, batches=10)
            )
        evt_handled = False
        try:
            if context.executemany:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        statement,
                        parameters,
                        context)
            elif not parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor,
                        statement,
                        context)
            else:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor,
                        statement,
                        parameters,
&gt;                       context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: order
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
The above exception was the direct cause of the following exception:
test_client = &lt;FlaskClient &lt;Flask 'application'&gt;&gt;
    def test_home_page(test_client):
        """"""
        GIVEN a Flask application
        WHEN the '/' page is requested (GET)
        THEN check the response is valid and contains rendered content
        """"""
&gt;       response = test_client.get('/')
tests/test_main.py:7: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.5/site-packages/werkzeug/test.py:830: in get
    return self.open(*args, **kw)
venv/lib/python3.5/site-packages/flask/testing.py:200: in open
    follow_redirects=follow_redirects
venv/lib/python3.5/site-packages/werkzeug/test.py:803: in open
    response = self.run_wsgi_app(environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:716: in run_wsgi_app
    rv = run_wsgi_app(self.application, environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:923: in run_wsgi_app
    app_rv = app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2309: in __call__
    return self.wsgi_app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2295: in wsgi_app
    response = self.handle_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1741: in handle_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:2292: in wsgi_app
    response = self.full_dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1815: in full_dispatch_request
    rv = self.handle_user_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1718: in handle_user_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:1813: in full_dispatch_request
    rv = self.dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1799: in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
application/main/routes.py:20: in index
    func.count(Order.id).label(""orders_count"")
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2947: in one
    ret = self.one_or_none()
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2917: in one_or_none
    ret = list(self)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2988: in __iter__
    return self._execute_and_instances(context)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:3011: in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:948: in execute
    return meth(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/sql/elements.py:269: in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1060: in _execute_clauseelement
    compiled_sql, distilled_params
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1200: in _execute_context
    context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1413: in _handle_dbapi_exception
    exc_info
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:265: in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:248: in reraise
    raise value.with_traceback(tb)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: in _execute_context
    context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: order [SQL: 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""'] (Background on this error at: http://sqlalche.me/e/e3q8)
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
=========================================================================================== 1 failed, 1 passed in 0.52 seconds ============================================================================================
which tells me: db.create_all() does not create all tables in my testing database.
Any hint, what I am doing wrong here?
Some additional info:
using sqlite at the moment
the database file itself gets created in the filesystem with 0byte
More Debugging:
I followed this guide here: https://xvrdm.github.io/2017/07/03/testing-flask-sqlalchemy-database-with-pytest/
this is where thing become strange:
Link from above:
&gt;&gt;&gt; db.engine.table_names()  # Check the tables currently on the engine
[]                           # no table found
&gt;&gt;&gt; db.create_all()          # Create the tables according to defined models
&gt;&gt;&gt; db.engine.table_names()
['users']                    # Now table 'users' is found
What happenes in my project:
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt;
Snipplet from models.py:
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
class Order(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(120), index=True, unique=True)
",<python><python-3.x><flask><sqlalchemy><pytest>,13612,3,252,5702,5,24,28,37,12551,0.0,30,2,12,2018-09-10 8:26,2018-09-16 12:53,2018-09-16 12:53,6.0,6.0,Intermediate,17,"<python><python-3.x><flask><sqlalchemy><pytest>, Pytest: setup testclient and DB, I'm trying to learn something about testing my flask app. In order to do that, I am using pytest and sqlalchemy.
I want to test a template, whose delivers route some SQL content. So in my opinion I need a testClient for testing the route itself and a DB fixture to manage the DB stuff included in the route.
Here is my fixture:
import pytest
from config import TestingConfig
from application import create_app, db
# ###########################
# ## functional tests
# ###########################
@pytest.fixture(scope='module')
def test_client():
    app = create_app(TestingConfig)
    # Flask provides a way to test your application by exposing the Werkzeug 
    # test Client and handling the context locals for you.
    testing_client = app.test_client()
    with app.app_context():
        db.create_all()
        yield testing_client  # this is where the testing happens!
        db.drop_all()
And this is my basic test:
def test_home_page(test_client):
    """"""
    GIVEN a Flask application
    WHEN the '/' page is requested (GET)
    THEN check the response is valid and contains rendered content
    """"""
    response = test_client.get('/')
    assert response.status_code == 200
    assert ""SOME CONTENT"" in response.data
Running my test fails with:
=================================================================================================== test session starts ===================================================================================================
platform linux -- Python 3.5.2, pytest-3.8.0, py-1.5.4, pluggy-0.7.1
rootdir: /home/dakkar/devzone/private/, inifile:
collected 2 items                                                                                                                                                                                                         
tests/test_main.py 
    SETUP    M test_client
        tests/test_main.py::test_home_page (fixtures used: test_client)F
        tests/test_main.py::test_valid_order_message (fixtures used: test_client).
    TEARDOWN M test_client
======================================================================================================== FAILURES =========================================================================================================
_____________________________________________________________________________________________________ test_home_page ______________________________________________________________________________________________________
self = &lt;sqlalchemy.engine.base.Connection object at 0x7f1c3f29b630&gt;, dialect = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;
constructor = &lt;bound method DefaultExecutionContext._init_compiled of &lt;class 'sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext'&gt;&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
args = (&lt;sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f1c3f29b6d8&gt;, [immutabledict({})]), conn = &lt;sqlalchemy.pool._ConnectionFairy object at 0x7f1c3f29b550&gt;
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def _execute_context(self, dialect, constructor,
                         statement, parameters,
                         *args):
        """"""Create an :class:`.ExecutionContext` and execute, returning
            a :class:`.ResultProxy`.""""""
        try:
            try:
                conn = self.__connection
            except AttributeError:
                # escape ""except AttributeError"" before revalidating
                # to prevent misleading stacktraces in Py3K
                conn = None
            if conn is None:
                conn = self._revalidate_connection()
            context = constructor(dialect, self, conn, *args)
        except BaseException as e:
            self._handle_dbapi_exception(
                e,
                util.text_type(statement), parameters,
                None, None)
        if context.compiled:
            context.pre_exec()
        cursor, statement, parameters = context.cursor, \
            context.statement, \
            context.parameters
        if not context.executemany:
            parameters = parameters[0]
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                statement, parameters = \
                    fn(self, cursor, statement, parameters,
                       context, context.executemany)
        if self._echo:
            self.engine.logger.info(statement)
            self.engine.logger.info(
                ""%r"",
                sql_util._repr_params(parameters, batches=10)
            )
        evt_handled = False
        try:
            if context.executemany:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        statement,
                        parameters,
                        context)
            elif not parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor,
                        statement,
                        context)
            else:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor,
                        statement,
                        parameters,
&gt;                       context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: order
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
The above exception was the direct cause of the following exception:
test_client = &lt;FlaskClient &lt;Flask 'application'&gt;&gt;
    def test_home_page(test_client):
        """"""
        GIVEN a Flask application
        WHEN the '/' page is requested (GET)
        THEN check the response is valid and contains rendered content
        """"""
&gt;       response = test_client.get('/')
tests/test_main.py:7: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.5/site-packages/werkzeug/test.py:830: in get
    return self.open(*args, **kw)
venv/lib/python3.5/site-packages/flask/testing.py:200: in open
    follow_redirects=follow_redirects
venv/lib/python3.5/site-packages/werkzeug/test.py:803: in open
    response = self.run_wsgi_app(environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:716: in run_wsgi_app
    rv = run_wsgi_app(self.application, environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:923: in run_wsgi_app
    app_rv = app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2309: in __call__
    return self.wsgi_app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2295: in wsgi_app
    response = self.handle_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1741: in handle_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:2292: in wsgi_app
    response = self.full_dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1815: in full_dispatch_request
    rv = self.handle_user_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1718: in handle_user_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:1813: in full_dispatch_request
    rv = self.dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1799: in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
application/main/routes.py:20: in index
    func.count(Order.id).label(""orders_count"")
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2947: in one
    ret = self.one_or_none()
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2917: in one_or_none
    ret = list(self)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2988: in __iter__
    return self._execute_and_instances(context)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:3011: in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:948: in execute
    return meth(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/sql/elements.py:269: in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1060: in _execute_clauseelement
    compiled_sql, distilled_params
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1200: in _execute_context
    context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1413: in _handle_dbapi_exception
    exc_info
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:265: in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:248: in reraise
    raise value.with_traceback(tb)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: in _execute_context
    context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: order [SQL: 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""'] (Background on this error at: http://sqlalche.me/e/e3q8)
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
=========================================================================================== 1 failed, 1 passed in 0.52 seconds ============================================================================================
which tells me: db.create_all() does not create all tables in my testing database.
Any hint, what I am doing wrong here?
Some additional info:
using sqlite at the moment
the database file itself gets created in the filesystem with 0byte
More Debugging:
I followed this guide here: https://xvrdm.github.io/2017/07/03/testing-flask-sqlalchemy-database-with-pytest/
this is where thing become strange:
Link from above:
&gt;&gt;&gt; db.engine.table_names()  # Check the tables currently on the engine
[]                           # no table found
&gt;&gt;&gt; db.create_all()          # Create the tables according to defined models
&gt;&gt;&gt; db.engine.table_names()
['users']                    # Now table 'users' is found
What happenes in my project:
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt;
Snipplet from models.py:
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
class Order(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(120), index=True, unique=True)
","<patron><patron-3.x><flask><sqlalchemy><test>, test: set testclient do, i'm try learn cometh test flask pp. order that, use test sqlalchemy. want test temple, whose devil rout sal content. opinion need testclient test rout do fixture manage do stuff include route. fixture: import test confirm import testingconfig applied import create_app, do # ########################### # ## function test # ########################### @test.fixture(scope='module') def test_client(): pp = create_app(testingconfig) # flask proved way test applied expose werkzeug # test client hand context local you. testing_cli = pp.test_client() pp.app_context(): do.create_all() yield testing_cli # test happens! do.drop_all() basic test: def test_home_page(test_client): """""" given flask applied '/' page request (get) check response valid contain render content """""" response = test_client.get('/') assert response.status_cod == 200 assert ""some content"" response.data run test fail with: =================================================================================================== test session start =================================================================================================== platform line -- patron 3.5.2, test-3.8.0, by-1.5.4, plunge-0.7.1 rootdir: /home/makar/devote/private/, inifile: collect 2 item tests/test_main.i set test_client tests/test_main.by::test_home_pag (fixture used: test_client)f tests/test_main.by::test_valid_order_messag (fixture used: test_client). tearworn test_client ======================================================================================================== failure ========================================================================================================= _____________________________________________________________________________________________________ test_home_pag ______________________________________________________________________________________________________ self = &it;sqlalchemy.engine.base.connect object 0x7f1c3f29b630&it;, dialect = &it;sqlalchemy.dialect.quite.pysqlite.sqlitedialect_pysqlit object 0x7f1c3f2c4ba8&it; construction = &it;bound method defaultexecutioncontext._init_compil &it;class 'sqlalchemy.dialect.quite.base.sqliteexecutioncontext'&it;&it; statement = 'select sum(""order"".col2_count) orders_col2, sum(""order"".col1_count) orders_col1, count(""order"".id) orders_count \from ""order""', parapet = () are = (&it;sqlalchemy.dialect.quite.base.sqlitecompil object 0x7f1c3f29b6d8&it;, [immutabledict({})]), corn = &it;sqlalchemy.pool._connectionfairi object 0x7f1c3f29b550&it; context = &it;sqlalchemy.dialect.quite.base.sqliteexecutioncontext object 0x7f1c3f29b6a0&it; def _execute_context(self, dialect, construction, statement, parameter, *arms): """"""great :class:`.executioncontext` execute, return :class:`.resultproxy`."""""" try: try: corn = self.connect except attributeerror: # escape ""except attributeerror"" revealed # prevent mislead stacktrac pack corn = none corn none: corn = self._revalidate_connection() context = construction(dialect, self, corn, *arms) except baseexcept e: self._handle_dbapi_exception( e, until.text_type(statement), parameter, none, none) context.complied: context.pre_exec() curses, statement, parapet = context.curses, \ context.statement, \ context.parapet context.executemany: parapet = parameter[0] self._has_ev self.engine._has_events: fn self.dispatch.before_cursor_execute: statement, parapet = \ fn(self, curses, statement, parameter, context, context.executemany) self.echo: self.engine.longer.into(statement) self.engine.longer.into( ""%r"", sql_util._repr_params(parameter, patches=10) ) evt_handl = fall try: context.executemany: self.dialect._has_events: fn self.dialect.dispatch.do_executemany: fn(curses, statement, parameter, context): evt_handl = true break evt_handled: self.dialect.do_executemany( curses, statement, parameter, context) if parapet context.no_parameters: self.dialect._has_events: fn self.dialect.dispatch.do_execute_no_params: fn(curses, statement, context): evt_handl = true break evt_handled: self.dialect.do_execute_no_params( curses, statement, context) else: self.dialect._has_events: fn self.dialect.dispatch.do_execute: fn(curses, statement, parameter, context): evt_handl = true break evt_handled: self.dialect.do_execute( curses, statement, parameter, &it; context) vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:1193: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = &it;sqlalchemy.dialect.quite.pysqlite.sqlitedialect_pysqlit object 0x7f1c3f2c4ba8&it;, curses = &it;sqlite3.curses object 0x7f1c3f2c2ce0&it; statement = 'select sum(""order"".col2_count) orders_col2, sum(""order"".col1_count) orders_col1, count(""order"".id) orders_count \from ""order""', parapet = () context = &it;sqlalchemy.dialect.quite.base.sqliteexecutioncontext object 0x7f1c3f29b6a0&it; def do_execute(self, curses, statement, parameter, context=none): &it; curses.execute(statement, parameter) e sqlite3.operationalerror: table: order vent/limb/python3.5/site-packages/sqlalchemy/engine/default.by:509: operationalerror except direct cause follow exception: test_client = &it;flaskclient &it;flask 'application'&it;&it; def test_home_page(test_client): """""" given flask applied '/' page request (get) check response valid contain render content """""" &it; response = test_client.get('/') tests/test_main.by:7: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ vent/limb/python3.5/site-packages/werkzeug/test.by:830: get return self.open(*arms, **w) vent/limb/python3.5/site-packages/flask/testing.by:200: open follow_redirects=follow_redirect vent/limb/python3.5/site-packages/werkzeug/test.by:803: open response = self.run_wsgi_app(environs, suffered=suffered) vent/limb/python3.5/site-packages/werkzeug/test.by:716: run_wsgi_app re = run_wsgi_app(self.application, environs, suffered=suffered) vent/limb/python3.5/site-packages/werkzeug/test.by:923: run_wsgi_app app_rv = pp(environs, start_response) vent/limb/python3.5/site-packages/flask/pp.by:2309: __call__ return self.wsgi_app(environs, start_response) vent/limb/python3.5/site-packages/flask/pp.by:2295: wsgi_app response = self.handle_exception(e) vent/limb/python3.5/site-packages/flask/pp.by:1741: handle_except raise(exc_type, exc_value, to) vent/limb/python3.5/site-packages/flask/compact.by:35: remain rays value vent/limb/python3.5/site-packages/flask/pp.by:2292: wsgi_app response = self.full_dispatch_request() vent/limb/python3.5/site-packages/flask/pp.by:1815: full_dispatch_request re = self.handle_user_exception(e) vent/limb/python3.5/site-packages/flask/pp.by:1718: handle_user_except raise(exc_type, exc_value, to) vent/limb/python3.5/site-packages/flask/compact.by:35: remain rays value vent/limb/python3.5/site-packages/flask/pp.by:1813: full_dispatch_request re = self.dispatch_request() vent/limb/python3.5/site-packages/flask/pp.by:1799: dispatch_request return self.view_functions[rule.endpoint](**red.view_args) application/main/routes.by:20: index fun.count(order.id).label(""orders_count"") vent/limb/python3.5/site-packages/sqlalchemy/or/query.by:2947: one met = self.one_or_none() vent/limb/python3.5/site-packages/sqlalchemy/or/query.by:2917: one_or_non met = list(self) vent/limb/python3.5/site-packages/sqlalchemy/or/query.by:2988: __iter__ return self._execute_and_instances(context) vent/limb/python3.5/site-packages/sqlalchemy/or/query.by:3011: _execute_and_inst result = corn.execute(querycontext.statement, self._params) vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:948: execute return met(self, multiparams, parts) vent/limb/python3.5/site-packages/sqlalchemy/sal/elements.by:269: _execute_on_connect return connection._execute_clauseelement(self, multiparams, parts) vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:1060: _execute_clauseel compiled_sql, distilled_param vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:1200: _execute_context context) vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:1413: _handle_dbapi_except exc_info vent/limb/python3.5/site-packages/sqlalchemy/until/compact.by:265: raise_from_caus raise(type(exception), exception, to=excite, cause=cause) vent/limb/python3.5/site-packages/sqlalchemy/until/compact.by:248: remain rays value.with_traceback(to) vent/limb/python3.5/site-packages/sqlalchemy/engine/base.by:1193: _execute_context context) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = &it;sqlalchemy.dialect.quite.pysqlite.sqlitedialect_pysqlit object 0x7f1c3f2c4ba8&it;, curses = &it;sqlite3.curses object 0x7f1c3f2c2ce0&it; statement = 'select sum(""order"".col2_count) orders_col2, sum(""order"".col1_count) orders_col1, count(""order"".id) orders_count \from ""order""', parapet = () context = &it;sqlalchemy.dialect.quite.base.sqliteexecutioncontext object 0x7f1c3f29b6a0&it; def do_execute(self, curses, statement, parameter, context=none): &it; curses.execute(statement, parameter) e sqlalchemy.etc.operationalerror: (sqlite3.operationalerror) table: order [sal: 'select sum(""order"".col2_count) orders_col2, sum(""order"".col1_count) orders_col1, count(""order"".id) orders_count \from ""order""'] (background error at: http://sqlalche.me/e/esq) vent/limb/python3.5/site-packages/sqlalchemy/engine/default.by:509: operationalerror =========================================================================================== 1 failed, 1 pass 0.52 second ============================================================================================ tell me: do.create_all() great table test database. hint, wrong here? admit into: use quite moment database file get great filesystem bite debugging: follow guide here: http://xvrdm.github.to/2017/07/03/testing-flask-sqlalchemy-database-with-test/ thing become strange: link above: &it;&it;&it; do.engine.table_names() # check table current engine [] # table found &it;&it;&it; do.create_all() # great table accord define model &it;&it;&it; do.engine.table_names() ['users'] # table 'users' found happen project: &it;&it;&it; do.engine.table_names() [] &it;&it;&it; do.create_all() &it;&it;&it; do.engine.table_names() [] &it;&it;&it; nipple models.by: flask_sqlalchemi import sqlalchemi do = sqlalchemy() class order(do.model): id = do.column(do.inter, primary_key=true) email = do.column(do.string(120), index=true, unique=true)"
49490623,Datetime filtering with SQLAlchemy isn't working,"Basically, I need to build a function that will filter a query according to given dates and return a new query. I'm new to SQLAlchemy, I looked up similar questions but I still got the same error:
`Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 8, 3, 1, 438278)`
Here's my code:
def filter_dates(query_obj, datecols, start = None, end = None):
        if end is None:
            end = datetime.datetime.now()
        if start is None:
            start = end - datetime.timedelta(weeks=12)
        print(""%s to %s"" % (start, end))
        for datecol in datecols:
            print(""Filtrando datas!"")
            query_obj = query_obj.filter(datecol &gt;= start)
            query_obj = query_obj.filter(datecol &lt;= end)
            ReevTable.print_query(query_obj)
        return query_obj
datecols is an orm.attributes object. Suppose I have an Object called User with a Datetime attribute named created_at. This is the expected behaviour:
query = session.query(Company.name, Company.created_at, Company.number_of_employees, Company.email_bounce_rate)
query = filter_dates(query_obj=query, datecols = [Company.created_at, Company.email_events.created_at])
query.all()
Expected output is a table with Companies that were only created within the date range, and the bounce rate should only be calculated during that specified date range. This might seem weird, but I calculate a not just emails, but other kinds of interactions too, so I need to input a list of attributes instead of just a single one. This is why I need to separate this filtering with a method.
I've tried using pandas datetime and timedelta, the built-in python datetime module, and simple strings with pd.to_datetime, but without success. The same error gets raised everytime. My Company column is in DateTime, so I don't know what else to do.
class Company(Base)
    created_at = Column(DateTime, nullable=False)
I'm completely new to SQLAlchemy, what am I doing wrong?
Full traceback:
`Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""reev-data-science/tables/reevtable.py"", line 128, in import_data
    self.print_query(query_obj)
  File ""reev-data-science/tables/reevtable.py"", line 107, in print_query
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)
  File ""&lt;string&gt;"", line 1, in &lt;lambda&gt;
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 442, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 448, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 453, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 219, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 245, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/annotation.py"", line 80, in _compiler_dispatch
    self, visitor, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1815, in visit_select
    text, select, inner_columns, froms, byfrom, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1899, in _compose_select_body
    t = select._whereclause._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1080, in visit_binary
    return self._generate_generic_binary(binary, opstring, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1113, in _generate_generic_binary
    self, eager_grouping=eager_grouping, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1244, in visit_bindparam
    bindparam, within_columns_clause=True, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1277, in render_literal_bindparam
    return self.render_literal_value(value, bindparam.type)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1295, in render_literal_value
    ""Don't know how to literal-quote value %r"" % value)
NotImplementedError: Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 9, 24, 46, 54634)`
The print_query() method:
`def print_query(query_obj):
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)`
",<python><datetime><sqlalchemy>,6262,0,79,321,0,2,11,68,5960,,115,2,12,2018-03-26 11:42,2021-07-16 0:58,,1208.0,,Advanced,32,"<python><datetime><sqlalchemy>, Datetime filtering with SQLAlchemy isn't working, Basically, I need to build a function that will filter a query according to given dates and return a new query. I'm new to SQLAlchemy, I looked up similar questions but I still got the same error:
`Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 8, 3, 1, 438278)`
Here's my code:
def filter_dates(query_obj, datecols, start = None, end = None):
        if end is None:
            end = datetime.datetime.now()
        if start is None:
            start = end - datetime.timedelta(weeks=12)
        print(""%s to %s"" % (start, end))
        for datecol in datecols:
            print(""Filtrando datas!"")
            query_obj = query_obj.filter(datecol &gt;= start)
            query_obj = query_obj.filter(datecol &lt;= end)
            ReevTable.print_query(query_obj)
        return query_obj
datecols is an orm.attributes object. Suppose I have an Object called User with a Datetime attribute named created_at. This is the expected behaviour:
query = session.query(Company.name, Company.created_at, Company.number_of_employees, Company.email_bounce_rate)
query = filter_dates(query_obj=query, datecols = [Company.created_at, Company.email_events.created_at])
query.all()
Expected output is a table with Companies that were only created within the date range, and the bounce rate should only be calculated during that specified date range. This might seem weird, but I calculate a not just emails, but other kinds of interactions too, so I need to input a list of attributes instead of just a single one. This is why I need to separate this filtering with a method.
I've tried using pandas datetime and timedelta, the built-in python datetime module, and simple strings with pd.to_datetime, but without success. The same error gets raised everytime. My Company column is in DateTime, so I don't know what else to do.
class Company(Base)
    created_at = Column(DateTime, nullable=False)
I'm completely new to SQLAlchemy, what am I doing wrong?
Full traceback:
`Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""reev-data-science/tables/reevtable.py"", line 128, in import_data
    self.print_query(query_obj)
  File ""reev-data-science/tables/reevtable.py"", line 107, in print_query
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)
  File ""&lt;string&gt;"", line 1, in &lt;lambda&gt;
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 442, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 448, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 453, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 219, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 245, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/annotation.py"", line 80, in _compiler_dispatch
    self, visitor, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1815, in visit_select
    text, select, inner_columns, froms, byfrom, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1899, in _compose_select_body
    t = select._whereclause._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1080, in visit_binary
    return self._generate_generic_binary(binary, opstring, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1113, in _generate_generic_binary
    self, eager_grouping=eager_grouping, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1244, in visit_bindparam
    bindparam, within_columns_clause=True, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1277, in render_literal_bindparam
    return self.render_literal_value(value, bindparam.type)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1295, in render_literal_value
    ""Don't know how to literal-quote value %r"" % value)
NotImplementedError: Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 9, 24, 46, 54634)`
The print_query() method:
`def print_query(query_obj):
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)`
","<patron><daytime><sqlalchemy>, datetim filter sqlalchemi working, basically, need build function filter query accord given date return new query. i'm new sqlalchemy, look similar question still got error: `don't know literal-quit value daytime.daytime(2018, 1, 1, 8, 3, 1, 438278)` here' code: def filter_dates(query_obj, datecols, start = none, end = none): end none: end = daytime.daytime.now() start none: start = end - daytime.timedelta(weeks=12) print(""% %s"" % (start, end)) datecol datecols: print(""filtrando data!"") query_obj = query_obj.filter(datecol &it;= start) query_obj = query_obj.filter(datecol &it;= end) reputable.print_query(query_obj) return query_obj datecol or.attribute object. suppose object call user datetim attribute name created_at. expect behaviour: query = session.query(company.name, company.created_at, company.number_of_employees, company.email_bounce_rate) query = filter_dates(query_obj=query, datecol = [company.created_at, company.email_events.created_at]) query.all() expect output table company great within date range, bound rate call specific date range. might seem weird, call email, kind interact too, need input list attribute instead single one. need spear filter method. i'v try use and datetim timedelta, built-in patron datetim module, simple string pp.to_datetime, without success. error get rays everytime. company column daytime, know else do. class company(base) created_at = column(daytime, syllable=false) i'm complete new sqlalchemy, wrong? full traceback: `traceback (most recent call last): file ""&it;stain&it;"", line 1, &it;module&it; file ""rev-data-science/tables/reputable.by"", line 128, import_data self.print_query(query_obj) file ""rev-data-science/tables/reputable.by"", line 107, print_queri print(colors.okblu + colors.bold + sir(query_obj.statement.compile(compile_kwargs={""literal_binds"":true})) + colors.end) file ""&it;string&it;"", line 1, &it;labia&it; file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/elements.by"", line 442, compel return self.compilers(dialect, bind=bind, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/elements.by"", line 448, compel return dialect.statement_compiler(dialect, self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 453, __init__ complied.__init__(self, dialect, statement, **wars) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 219, __init__ self.sir = self.process(self.statement, **compile_kwargs) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 245, process return obs._compiler_dispatch(self, **wars) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/annexation.by"", line 80, _compiler_dispatch self, visitor, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/visitors.by"", line 81, _compiler_dispatch return met(self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1815, visit_select text, select, inner_columns, from, byrom, wars) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1899, _compose_select_bodi = select._whereclause._compiler_dispatch(self, **wars) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/visitors.by"", line 81, _compiler_dispatch return met(self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 829, visit_clauselist c clauselist.clauses) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 826, &it;genexpr&it; file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 829, &it;genexpr&it; c clauselist.clauses) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/visitors.by"", line 81, _compiler_dispatch return met(self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 829, visit_clauselist c clauselist.clauses) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 826, &it;genexpr&it; file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 829, &it;genexpr&it; c clauselist.clauses) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/visitors.by"", line 81, _compiler_dispatch return met(self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1080, visit_binari return self._generate_generic_binary(binary, string, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1113, _generate_generic_binari self, eager_grouping=eager_grouping, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/visitors.by"", line 81, _compiler_dispatch return met(self, **w) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1244, visit_bindparam bindparam, within_columns_clause=true, **wars) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1277, render_literal_bindparam return self.render_literal_value(value, bindparam.type) file ""/us/local/limb/python3.5/list-packages/sqlalchemy/sal/compilers.by"", line 1295, render_literal_valu ""don't know literal-quit value %r"" % value) notimplementederror: know literal-quit value daytime.daytime(2018, 1, 1, 9, 24, 46, 54634)` print_query() method: `def print_query(query_obj): print(colors.okblu + colors.bold + sir(query_obj.statement.compile(compile_kwargs={""literal_binds"":true})) + colors.end)`"
56016700,How to create an empty array of struct in hive?,"I have a view in Hive 1.1.0, based on a condition, it should return an empty array or an array of struct&lt;name: string, jobslots: int&gt;
Here is my code:
select
      case when &lt;condition&gt; 
             then array()
           else array(struct(t1.name, t1.jobslots))
       end
from table t1;
The problem here is, that the empty array array() is of type array&lt;string&gt;. So when I try to insert it into a table, it throws an error.
How can I change this to return an empty array of type array&lt;struct&lt;name: string, jobslots:int&gt;&gt; so that Hive's size() function returns 0 on this array?
",<sql><arrays><struct><hive><hiveql>,611,0,12,945,0,12,32,62,7759,0.0,53,1,12,2019-05-07 6:27,2020-12-19 19:22,,592.0,,Advanced,32,"<sql><arrays><struct><hive><hiveql>, How to create an empty array of struct in hive?, I have a view in Hive 1.1.0, based on a condition, it should return an empty array or an array of struct&lt;name: string, jobslots: int&gt;
Here is my code:
select
      case when &lt;condition&gt; 
             then array()
           else array(struct(t1.name, t1.jobslots))
       end
from table t1;
The problem here is, that the empty array array() is of type array&lt;string&gt;. So when I try to insert it into a table, it throws an error.
How can I change this to return an empty array of type array&lt;struct&lt;name: string, jobslots:int&gt;&gt; so that Hive's size() function returns 0 on this array?
","<sal><array><struck><hive><hive>, great empty array struck hive?, view hive 1.1.0, base condition, return empty array array struck&it;name: string, jobslots: in&it; code: select case &it;condition&it; array() else array(struck(to.name, to.jobslots)) end table to; problem is, empty array array() type array&it;string&it;. try insert table, throw error. change return empty array type array&it;struck&it;name: string, jobslots:in&it;&it; hive' size() function return 0 array?"
53552983,How to generate datasets dynamically based on schema?,"I have multiple schema like below with different column names and data types. 
I want to generate test/simulated data using DataFrame with Scala for each schema and save it to parquet file.
Below is the example schema (from a sample json) to generate data dynamically with dummy values in it.
val schema1 = StructType(
  List(
    StructField(""a"", DoubleType, true),
    StructField(""aa"", StringType, true)
    StructField(""p"", LongType, true),
    StructField(""pp"", StringType, true)
  )
)
I need rdd/dataframe like this with 1000 rows each based on number of columns in the above schema.
val data = Seq(
  Row(1d, ""happy"", 1L, ""Iam""),
  Row(2d, ""sad"", 2L, ""Iam""),
  Row(3d, ""glad"", 3L, ""Iam"")
)
Basically.. like this 200 datasets are there for which I need to generate data dynamically, writing separate programs for each scheme is merely impossible for me.
Pls. help me with your ideas or impl. as I am new to spark.
Is it possible to generate dynamic data based on schema of different types? 
",<scala><apache-spark><apache-spark-sql>,997,0,14,900,0,13,27,50,5013,0.0,423,3,12,2018-11-30 7:21,2018-12-14 20:20,2018-12-17 15:03,14.0,17.0,Basic,9,"<scala><apache-spark><apache-spark-sql>, How to generate datasets dynamically based on schema?, I have multiple schema like below with different column names and data types. 
I want to generate test/simulated data using DataFrame with Scala for each schema and save it to parquet file.
Below is the example schema (from a sample json) to generate data dynamically with dummy values in it.
val schema1 = StructType(
  List(
    StructField(""a"", DoubleType, true),
    StructField(""aa"", StringType, true)
    StructField(""p"", LongType, true),
    StructField(""pp"", StringType, true)
  )
)
I need rdd/dataframe like this with 1000 rows each based on number of columns in the above schema.
val data = Seq(
  Row(1d, ""happy"", 1L, ""Iam""),
  Row(2d, ""sad"", 2L, ""Iam""),
  Row(3d, ""glad"", 3L, ""Iam"")
)
Basically.. like this 200 datasets are there for which I need to generate data dynamically, writing separate programs for each scheme is merely impossible for me.
Pls. help me with your ideas or impl. as I am new to spark.
Is it possible to generate dynamic data based on schema of different types? 
","<scala><apache-spark><apache-spark-sal>, genet dataset dream base scheme?, multiple scheme like differ column name data types. want genet test/soul data use datafram scala scheme save parquet file. example scheme (from sample son) genet data dream dummy value it. val scheme = structtype( list( structfield(""a"", doubletype, true), structfield(""a"", stringtype, true) structfield(""p"", longtype, true), structfield(""pp"", stringtype, true) ) ) need red/datafram like 1000 row base number column scheme. val data = see( row(d, ""happy"", ll, ""am""), row(d, ""sad"", ll, ""am""), row(d, ""glad"", ll, ""am"") ) basically.. like 200 dataset need genet data dynamically, write spear program scheme mere impose me. pus. help idea imply. new spark. possible genet dream data base scheme differ types?"
48326311,Sequelize: overlapping - Checking if any value in array matches any value in the passed array,"Using sequelize to check if the the db input property, which is array, has a given item.
Have a Postgres database with data Events.
Want to get one Event that will have any of these weekDays.
Type of weekDays is ARRAY(integer).
Events.findOne({
  where: {
    weekDays: {
      $contains: [2, 3],
    },
  },
});
Tried to do with $contains, $any, or $like $any but all the time got the same error message.
  TypeError: values.map is not a function
Sincerely thanks
",<sql><node.js><database><postgresql><sequelize.js>,465,0,7,515,1,5,17,81,7328,,13,1,12,2018-01-18 16:44,2018-01-19 10:45,2018-01-19 10:45,1.0,1.0,Advanced,32,"<sql><node.js><database><postgresql><sequelize.js>, Sequelize: overlapping - Checking if any value in array matches any value in the passed array, Using sequelize to check if the the db input property, which is array, has a given item.
Have a Postgres database with data Events.
Want to get one Event that will have any of these weekDays.
Type of weekDays is ARRAY(integer).
Events.findOne({
  where: {
    weekDays: {
      $contains: [2, 3],
    },
  },
});
Tried to do with $contains, $any, or $like $any but all the time got the same error message.
  TypeError: values.map is not a function
Sincerely thanks
","<sal><node.is><database><postgresql><sequelae.is>, sequelae: overlap - check value array match value pass array, use sequel check do input property, array, given item. poster database data events. want get one event weekdays. type weekly array(inter). events.finding({ where: { weekdays: { $contains: [2, 3], }, }, }); try $contains, $any, $like $and time got error message. typeerror: values.map function since thank"
53881132,How to change the search action in wordpress search bar?,"Create a new post and publish it.
The title is my test for search, content in it is as below:
no host route
Check what happen in wordpress database.
 select post_title from wp_posts
     where post_content like ""%no%""
       and post_content like ""%route%""
       and post_content like ""%to%""
       and post_content like ""%host%"";
The post named my test for search will not be in the select's result.
Type no route to host in wordpress search bar,and click enter.
The post named my test for search shown as result.
I found the reason that the webpage contain to ,in the left upper side corner ,there is a word Customize which contains the searched word to.
How to change such search action in wordpress serach bar?
I want to make the search behavior in wordpress saerch bar, for example ,when you type no route to host, equal to the following sql command.
select post_title from wp_posts where post_content like ""%no%route%to%host%"";
All the plugins in my wordpress.
CodePen Embedded Pens Shortcode
Crayon Syntax Highlighter
Disable Google Fonts
Quotmarks Replacer
SyntaxHighlighter Evolved
",<mysql><wordpress><search>,1092,1,20,474,42,145,298,74,988,,387,1,12,2018-12-21 8:01,2018-12-23 18:31,2018-12-23 18:31,2.0,2.0,Basic,9,"<mysql><wordpress><search>, How to change the search action in wordpress search bar?, Create a new post and publish it.
The title is my test for search, content in it is as below:
no host route
Check what happen in wordpress database.
 select post_title from wp_posts
     where post_content like ""%no%""
       and post_content like ""%route%""
       and post_content like ""%to%""
       and post_content like ""%host%"";
The post named my test for search will not be in the select's result.
Type no route to host in wordpress search bar,and click enter.
The post named my test for search shown as result.
I found the reason that the webpage contain to ,in the left upper side corner ,there is a word Customize which contains the searched word to.
How to change such search action in wordpress serach bar?
I want to make the search behavior in wordpress saerch bar, for example ,when you type no route to host, equal to the following sql command.
select post_title from wp_posts where post_content like ""%no%route%to%host%"";
All the plugins in my wordpress.
CodePen Embedded Pens Shortcode
Crayon Syntax Highlighter
Disable Google Fonts
Quotmarks Replacer
SyntaxHighlighter Evolved
","<myself><wordpress><search>, change search action wordpress search bar?, great new post publish it. till test search, content below: host rout check happen wordpress database. select post_titl wp_post post_cont like ""%no%"" post_cont like ""%route%"" post_cont like ""%to%"" post_cont like ""%host%""; post name test search select' result. type rout host wordpress search bar,and click enter. post name test search shown result. found reason webpag contain ,in left upper side corner ,there word custom contain search word to. change search action wordpress search bar? want make search behavior wordpress search bar, example ,when type rout host, equal follow sal command. select post_titl wp_post post_cont like ""%no%route%to%host%""; plain wordpress. codein ebbed pen shortcod carbon santa highlight distal good font quotmark replace syntaxhighlight evolve"
54448139,Microsoft.SqlServer.Types incompatible with .NET Standard,"I'm attempting to convert all of our C# class libraries from .NET Framework to .NET Standard projects, as we are starting to leverage .NET Core so need these to be consumable by both .NET Core and .NET Framework apps (with the latter being ported over to Core in the upcoming months.)
I'm having trouble converting our data access layer code because we leverage Microsoft.SqlServer.Types extensively and the official NuGet package doesn't support .NET Standard. I tried an unofficial NuGet package by dotmorten but it's missing a lot of functionality. Below is a list of everything missing that we would need (thrown together to get the code building...)
public static class SqlMockExtensions
{
    public static SqlBytes STAsBinary(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography MakeValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static int STDimension(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static bool STIsValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static Nullable&lt;double&gt; EnvelopeAngle(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography ReorientObject(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography BufferWithTolerance(this SqlGeography geography, double arg1, int arg2, bool arg3) =&gt; throw new NotImplementedException();
    public static SqlGeography Reduce(this SqlGeography geography, double tolerance) =&gt; throw new NotImplementedException();
    public static SqlGeography EnvelopeCenter(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static double STDistance(this SqlGeography geography, SqlGeography point2) =&gt; throw new NotImplementedException();
    public static SqlBytes STAsBinary(this SqlGeometry geometry) =&gt; throw new NotImplementedException();
}
When I search SO for others trying to integrate Microsoft.SqlServer.Types into their .NET Core and Standard projects, I see mentions of including the official NuGet package and then doing something like this:
SqlServerTypes.Utilities.LoadNativeAssemblies(AppDomain.CurrentDomain.BaseDirectory);
However, it errors when you try to add a non-.NET Standard compliant NuGet package into a .NET Standard project, so I'm not clear how this is a solution.
This seems like a very common problem to have, there have to be a lot of developers out there who leverage Microsoft.SqlServer.Types for SqlGeography, SqlGeometry, etc... and are porting over to .NET Standard. So how are all of you accomplishing this?
",<c#><sql-server><.net-standard><sqlgeography><sqlgeometry>,2717,3,25,17807,38,132,203,58,4761,0.0,7782,1,12,2019-01-30 19:28,2021-07-25 13:26,,907.0,,Intermediate,18,"<c#><sql-server><.net-standard><sqlgeography><sqlgeometry>, Microsoft.SqlServer.Types incompatible with .NET Standard, I'm attempting to convert all of our C# class libraries from .NET Framework to .NET Standard projects, as we are starting to leverage .NET Core so need these to be consumable by both .NET Core and .NET Framework apps (with the latter being ported over to Core in the upcoming months.)
I'm having trouble converting our data access layer code because we leverage Microsoft.SqlServer.Types extensively and the official NuGet package doesn't support .NET Standard. I tried an unofficial NuGet package by dotmorten but it's missing a lot of functionality. Below is a list of everything missing that we would need (thrown together to get the code building...)
public static class SqlMockExtensions
{
    public static SqlBytes STAsBinary(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography MakeValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static int STDimension(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static bool STIsValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static Nullable&lt;double&gt; EnvelopeAngle(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography ReorientObject(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography BufferWithTolerance(this SqlGeography geography, double arg1, int arg2, bool arg3) =&gt; throw new NotImplementedException();
    public static SqlGeography Reduce(this SqlGeography geography, double tolerance) =&gt; throw new NotImplementedException();
    public static SqlGeography EnvelopeCenter(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static double STDistance(this SqlGeography geography, SqlGeography point2) =&gt; throw new NotImplementedException();
    public static SqlBytes STAsBinary(this SqlGeometry geometry) =&gt; throw new NotImplementedException();
}
When I search SO for others trying to integrate Microsoft.SqlServer.Types into their .NET Core and Standard projects, I see mentions of including the official NuGet package and then doing something like this:
SqlServerTypes.Utilities.LoadNativeAssemblies(AppDomain.CurrentDomain.BaseDirectory);
However, it errors when you try to add a non-.NET Standard compliant NuGet package into a .NET Standard project, so I'm not clear how this is a solution.
This seems like a very common problem to have, there have to be a lot of developers out there who leverage Microsoft.SqlServer.Types for SqlGeography, SqlGeometry, etc... and are porting over to .NET Standard. So how are all of you accomplishing this?
","<c#><sal-server><.net-standard><sqlgeography><sqlgeometry>, microsoft.sqlserver.type incompat .net standard, i'm attempt convert c# class library .net framework .net standard projects, start several .net core need consume .net core .net framework pp (with latter port core upon months.) i'm trouble convert data access layer code several microsoft.sqlserver.type extent office puget package support .net standard. try unofficial puget package dotmorten miss lot functionality. list every miss would need (thrown together get code building...) public static class sqlmockextens { public static sqlbyte stasbinary(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static sqlgeographi makevalid(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static in dimension(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static book stisvalid(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static syllable&it;double&it; envelopeangle(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static sqlgeographi reorientobject(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static sqlgeographi bufferwithtolerance(the sqlgeographi geography, doubt are, in are, book are) =&it; throw new notimplementedexception(); public static sqlgeographi reduce(the sqlgeographi geography, doubt tolerable) =&it; throw new notimplementedexception(); public static sqlgeographi envelopecenter(the sqlgeographi geography) =&it; throw new notimplementedexception(); public static doubt distance(the sqlgeographi geography, sqlgeographi point) =&it; throw new notimplementedexception(); public static sqlbyte stasbinary(the sqlgeometri geometry) =&it; throw new notimplementedexception(); } search other try inter microsoft.sqlserver.type .net core standard projects, see mention include office puget package cometh like this: sqlservertypes.utilities.loadnativeassemblies(appdomain.currentdomain.basedirectory); however, error try add non-.net standard complaint puget package .net standard project, i'm clear solution. seem like common problem have, lot develop several microsoft.sqlserver.type sqlgeography, sqlgeometry, etc... port .net standard. accomplish this?"
48350843,How to connect from docker-compose to Host PostgreSQL?,"I have a server with installed PostgreSQL. All my services work in containers (docker-compose). I want to use my Host PostgreSQL from containers. Buy I have the error:
  Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://localhost:5432/shop-bd) for user 'shop-bd-user': Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  SQL State  : 08001
  Error Code : 0
  Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection refused (Connection refused)
My docker-compose is using host network_mode, like this:
version: '3'
services:
  shop:  
    container_name: shop
    build: ./shop
    hostname: shop
    restart: always   
    ports:
      - 8084:8084
    network_mode: ""host""
My database connection URL is: jdbc:postgresql://localhost:5432/shop-bd
",<postgresql><docker><networking><localhost><docker-compose>,1360,0,18,1813,3,26,39,68,19448,0.0,57,4,12,2018-01-19 23:11,2018-01-20 8:32,2018-04-15 7:25,1.0,86.0,Basic,9,"<postgresql><docker><networking><localhost><docker-compose>, How to connect from docker-compose to Host PostgreSQL?, I have a server with installed PostgreSQL. All my services work in containers (docker-compose). I want to use my Host PostgreSQL from containers. Buy I have the error:
  Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://localhost:5432/shop-bd) for user 'shop-bd-user': Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  SQL State  : 08001
  Error Code : 0
  Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection refused (Connection refused)
My docker-compose is using host network_mode, like this:
version: '3'
services:
  shop:  
    container_name: shop
    build: ./shop
    hostname: shop
    restart: always   
    ports:
      - 8084:8084
    network_mode: ""host""
My database connection URL is: jdbc:postgresql://localhost:5432/shop-bd
","<postgresql><doctor><networking><localhost><doctor-compose>, connect doctor-compose host postgresql?, server instal postgresql. service work contain (doctor-compose). want use host postgresql container. buy error: unable obtain job connect datasourc (job:postgresql://localhost:5432/shop-by) user 'shop-by-user': connect localhost:5432 refused. check hostnam port correct postmaster accept top/in connections. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ sal state : 08001 error code : 0 message : connect localhost:5432 refused. check hostnam port correct postmaster accept top/in connections. connect localhost:5432 refused. check hostnam port correct postmaster accept top/in connections. connect refuse (connect refused) doctor-compose use host network_mode, like this: version: '3' services: shop: container_name: shop build: ./shop hostage: shop start: away ports: - 8084:8084 network_mode: ""host"" database connect curl is: job:postgresql://localhost:5432/shop-by"
50467598,Multi-homed SQL Server with High Availability Groups,"We have two servers (SQL-ATL01, SQL-ATL02) that make up a Failover Cluster, each running as part of a  SQL Server High Availability Group (HAG). Each server has two network cards. One is a 10Gbit card that is directly connected to the other server and is used for Synchronizing the HAG on a 192.168.99.x subnet. The other is a 1Gbit card that is used to connect the DB servers to a switch to communicate with the application servers on a 10.0.0.x subnet. The Listener is pointed to the  192.168.99.x subnet.
We want to add a third server (SQL-NYC01) in another physical location to the cluster and run it as an Async replica part of the HAG, but the VPN only routes traffic on the subnet on the 1Gbit network.
Is there any way to set up the Failover Cluster and High Availability Group to tell it:
Send synchronous replica traffic for SQL-ATL01 &lt;--> SQL-ATL02 over 192.168.99.x
Send asynchronous replica traffic for (SQL-ATL01, SQL-ATL02) &lt;--> SQL-NYC01 over 10.0.0.x
Or do we have to have all replica traffic going in and out on the same IP address/subnet?  
",<sql-server><high-availability>,1066,0,0,7461,3,32,33,72,479,,327,1,12,2018-05-22 12:23,2018-05-30 9:31,,8.0,,Advanced,32,"<sql-server><high-availability>, Multi-homed SQL Server with High Availability Groups, We have two servers (SQL-ATL01, SQL-ATL02) that make up a Failover Cluster, each running as part of a  SQL Server High Availability Group (HAG). Each server has two network cards. One is a 10Gbit card that is directly connected to the other server and is used for Synchronizing the HAG on a 192.168.99.x subnet. The other is a 1Gbit card that is used to connect the DB servers to a switch to communicate with the application servers on a 10.0.0.x subnet. The Listener is pointed to the  192.168.99.x subnet.
We want to add a third server (SQL-NYC01) in another physical location to the cluster and run it as an Async replica part of the HAG, but the VPN only routes traffic on the subnet on the 1Gbit network.
Is there any way to set up the Failover Cluster and High Availability Group to tell it:
Send synchronous replica traffic for SQL-ATL01 &lt;--> SQL-ATL02 over 192.168.99.x
Send asynchronous replica traffic for (SQL-ATL01, SQL-ATL02) &lt;--> SQL-NYC01 over 10.0.0.x
Or do we have to have all replica traffic going in and out on the same IP address/subnet?  
","<sal-server><high-availability>, multi-him sal server high avail groups, two server (sal-atlas, sal-atlas) make failed cluster, run part sal server high avail group (had). server two network cards. one 10gbit card directly connect server use synchron had 192.168.99.x subject. bit card use connect do server switch common applied server 10.0.0.x subject. listen point 192.168.99.x subject. want add third server (sal-nyc01) not physics local cluster run async replied part had, van rout traffic subject bit network. way set failed cluster high avail group tell it: send synchron replied traffic sal-atlas &it;--> sal-atlas 192.168.99.x send asynchron replied traffic (sal-atlas, sal-atlas) &it;--> sal-nyc01 10.0.0.x replied traffic go in address/subject?"
51088034,Hibernate @Formula doesn't include Schema,"I have an entity with a property @Formula like this:
@Entity
@Table(name = ""areasAuxiliar"")
public final class AreaAuxiliar implements Serializable {
    @Id
    @Column(name = ""idArea"")
    private Integer idArea;
    @Formula(""RUTAAREA(idArea)"")
    private String ruta;
when I configure my hibernate to point to an Oracle DB I have no problem,
BUT, when I switch to an SQLServer, hibernate is not including the shema and the query fails,
the query generated for hibernate looks like this:
select
    areaauxili4_.idArea as idArea1_6_4_,
    rutaArea(areaauxili4_.idArea) as formula2_4_
from
    SIGAP.areasAuxiliar areaauxili4_ 
the param hibernate.default_schema=SIGAP is being read and included in the table but not in the function,
is there an option/annotation to force the shema in in that function?
I have tried hibernate 5.1 and 5.2 with the same result :(
",<java><sql-server><hibernate><jpa><hql>,867,0,15,568,3,7,27,68,2683,,221,4,12,2018-06-28 16:54,2018-07-25 16:15,2018-07-31 23:30,27.0,33.0,Advanced,35,"<java><sql-server><hibernate><jpa><hql>, Hibernate @Formula doesn't include Schema, I have an entity with a property @Formula like this:
@Entity
@Table(name = ""areasAuxiliar"")
public final class AreaAuxiliar implements Serializable {
    @Id
    @Column(name = ""idArea"")
    private Integer idArea;
    @Formula(""RUTAAREA(idArea)"")
    private String ruta;
when I configure my hibernate to point to an Oracle DB I have no problem,
BUT, when I switch to an SQLServer, hibernate is not including the shema and the query fails,
the query generated for hibernate looks like this:
select
    areaauxili4_.idArea as idArea1_6_4_,
    rutaArea(areaauxili4_.idArea) as formula2_4_
from
    SIGAP.areasAuxiliar areaauxili4_ 
the param hibernate.default_schema=SIGAP is being read and included in the table but not in the function,
is there an option/annotation to force the shema in in that function?
I have tried hibernate 5.1 and 5.2 with the same result :(
","<cava><sal-server><liberate><pa><he>, wiberd @formula include scheme, entity property @formula like this: @entity @table(am = ""areasauxiliar"") public final class areaauxiliar implement serializ { @id @column(am = ""area"") privat inter area; @formula(""rutaarea(area)"") privat string ruts; configur wiberd point oral do problem, but, switch sqlserver, wiberd include she query fails, query genet wiberd look like this: select areaauxili4_.area idarea1_6_4_, rutaarea(areaauxili4_.area) formula2_4_ sight.areasauxiliar areaauxili4_ parma liberate.default_schema=sight read include table function, option/cannot for she function? try wiberd 5.1 5.2 result :("
57681970,PSQLException and lock issue when trigger added on table,"UPDATE: I eliminated Hibernate from the problem. I completely reworked description of problem to simplify it as much as possible.
I have master table with noop trigger and detail table with two relations between master and detail table:
create table detail (
  id bigint not null,
  code varchar(255) not null,
  primary key (id)
);
create table master (
  id bigint not null,
  name varchar(255),
  detail_id bigint, -- ""preferred"" detail is one-to-one relation
  primary key (id),
  unique (detail_id),
  foreign key (detail_id) references detail(id)
);
create table detail_candidate ( -- ""candidate"" details = many-to-many relation modeled as join table
  master_id bigint not null,
  detail_id bigint not null,
  primary key (master_id, detail_id),
  foreign key (detail_id) references detail(id),
  foreign key (master_id) references master(id)
);
create or replace function trgf() returns trigger as $$
begin
  return NEW;
end;
$$ language 'plpgsql';
create trigger trg
  before insert or update
  on master
  for each row execute procedure trgf();
insert into master (id, name) values (1000, 'x'); -- this is part of database setup
insert into detail (code, id) values ('a', 1);    -- this is part of database setup
In such setup, I open two terminal windows with psql and perform following steps:
in first terminal, change master (leave transaction open)
begin;
update master set detail_id=null, name='y' where id=1000;
in second terminal, add detail candidate to master in own transaction
begin;
set statement_timeout = 4000;
insert into detail_candidate (master_id, detail_id) values (1000, 1);
Last command in second terminal timeouts with message
ERROR:  canceling statement due to statement timeout
CONTEXT:  while locking tuple (0,1) in relation ""master""
SQL statement ""SELECT 1 FROM ONLY ""public"".""master"" x WHERE ""id"" OPERATOR(pg_catalog.=) $1 FOR KEY SHARE OF x""
My observation and questions (changes are independent):
when the db is setup without trigger, i.e. drop trigger trg on master; is called after initial setup, everything works fine.
Why the presence of noop trigger has such an influence? I don't get it.
when the db is setup without unique constraint on master.detail_id (i.e. alter table master drop constraint master_detail_id_key; is called after initial setup), everything works fine too. Why?
when I omit explicit detail=null assignment in update statement in first terminal (since there's null value from setup anyway), everything works fine too. Why?
Tried on Postgres 9.6.12 (embedded), 9.6.15 (in Docker), 11.5 (in Docker).
Problem is reproducible in Docker image tomaszalusky/trig-example which is available on DockerHub or can be built from this Dockerfile (instructions inside).
UPDATE 2: I found common behaviour of three observation above. I spawned the query select * from pgrowlocks('master') from pgrowlocks extension in second transaction. The row-level lock of updated row in master is FOR UPDATE in failing case but FOR NO KEY UPDATE in all three working cases. This is in perfect compliance with mode match table in documentation since FOR UPDATE mode is the stronger one and mode requested by insert statement is FOR KEY SHARE (which is apparent from error message, also invoking the select ... for key share command has same effect as insert command).
The documentation on FOR UPDATE mode says:
  The FOR UPDATE lock mode is also acquired by (...) an UPDATE that modifies the values on certain columns. Currently, the set of columns considered for the UPDATE case are those that have a unique index on them that can be used in a foreign key (...)
It is true for master.detail_id column. However, still it's not clear why FOR UPDATE mode isn't chosen independently on trigger presence and why trigger presence caused it.
",<postgresql><triggers><locking>,3772,4,65,11000,2,39,64,39,3384,,1413,1,12,2019-08-27 20:40,2019-09-05 21:42,2019-09-05 21:42,9.0,9.0,Advanced,35,"<postgresql><triggers><locking>, PSQLException and lock issue when trigger added on table, UPDATE: I eliminated Hibernate from the problem. I completely reworked description of problem to simplify it as much as possible.
I have master table with noop trigger and detail table with two relations between master and detail table:
create table detail (
  id bigint not null,
  code varchar(255) not null,
  primary key (id)
);
create table master (
  id bigint not null,
  name varchar(255),
  detail_id bigint, -- ""preferred"" detail is one-to-one relation
  primary key (id),
  unique (detail_id),
  foreign key (detail_id) references detail(id)
);
create table detail_candidate ( -- ""candidate"" details = many-to-many relation modeled as join table
  master_id bigint not null,
  detail_id bigint not null,
  primary key (master_id, detail_id),
  foreign key (detail_id) references detail(id),
  foreign key (master_id) references master(id)
);
create or replace function trgf() returns trigger as $$
begin
  return NEW;
end;
$$ language 'plpgsql';
create trigger trg
  before insert or update
  on master
  for each row execute procedure trgf();
insert into master (id, name) values (1000, 'x'); -- this is part of database setup
insert into detail (code, id) values ('a', 1);    -- this is part of database setup
In such setup, I open two terminal windows with psql and perform following steps:
in first terminal, change master (leave transaction open)
begin;
update master set detail_id=null, name='y' where id=1000;
in second terminal, add detail candidate to master in own transaction
begin;
set statement_timeout = 4000;
insert into detail_candidate (master_id, detail_id) values (1000, 1);
Last command in second terminal timeouts with message
ERROR:  canceling statement due to statement timeout
CONTEXT:  while locking tuple (0,1) in relation ""master""
SQL statement ""SELECT 1 FROM ONLY ""public"".""master"" x WHERE ""id"" OPERATOR(pg_catalog.=) $1 FOR KEY SHARE OF x""
My observation and questions (changes are independent):
when the db is setup without trigger, i.e. drop trigger trg on master; is called after initial setup, everything works fine.
Why the presence of noop trigger has such an influence? I don't get it.
when the db is setup without unique constraint on master.detail_id (i.e. alter table master drop constraint master_detail_id_key; is called after initial setup), everything works fine too. Why?
when I omit explicit detail=null assignment in update statement in first terminal (since there's null value from setup anyway), everything works fine too. Why?
Tried on Postgres 9.6.12 (embedded), 9.6.15 (in Docker), 11.5 (in Docker).
Problem is reproducible in Docker image tomaszalusky/trig-example which is available on DockerHub or can be built from this Dockerfile (instructions inside).
UPDATE 2: I found common behaviour of three observation above. I spawned the query select * from pgrowlocks('master') from pgrowlocks extension in second transaction. The row-level lock of updated row in master is FOR UPDATE in failing case but FOR NO KEY UPDATE in all three working cases. This is in perfect compliance with mode match table in documentation since FOR UPDATE mode is the stronger one and mode requested by insert statement is FOR KEY SHARE (which is apparent from error message, also invoking the select ... for key share command has same effect as insert command).
The documentation on FOR UPDATE mode says:
  The FOR UPDATE lock mode is also acquired by (...) an UPDATE that modifies the values on certain columns. Currently, the set of columns considered for the UPDATE case are those that have a unique index on them that can be used in a foreign key (...)
It is true for master.detail_id column. However, still it's not clear why FOR UPDATE mode isn't chosen independently on trigger presence and why trigger presence caused it.
","<postgresql><trigger><locking>, psqlexcept lock issue trigger ad table, update: limit wiberd problem. complete work rescript problem simplify much possible. master table noon trigger detail table two relate master detail table: great table detail ( id begin null, code varchar(255) null, primary key (id) ); great table master ( id begin null, name varchar(255), detailed begin, -- ""preferred"" detail one-to-on relate primary key (id), unique (detailed), foreign key (detailed) refer detail(id) ); great table detail_candid ( -- ""candidate"" detail = many-to-man relate model join table mastered begin null, detailed begin null, primary key (mastered, detailed), foreign key (detailed) refer detail(id), foreign key (mastered) refer master(id) ); great replace function true() return trigger $$ begin return new; end; $$ language 'plpgsql'; great trigger try insert update master row execute procedure true(); insert master (id, name) value (1000, 'x'); -- part database set insert detail (code, id) value ('a', 1); -- part database set set, open two german window pool perform follow steps: first terminal, change master (leave transact open) begin; update master set detailed=null, name='y' id=1000; second terminal, add detail candid master transact begin; set statement_timeout = 4000; insert detail_candid (mastered, detailed) value (1000, 1); last command second german timeout message error: cancel statement due statement timeout context: lock up (0,1) relate ""master"" sal statement ""select 1 ""public"".""master"" x ""id"" operator(pg_catalog.=) $1 key share x"" observe question (change independent): do set without trigger, i.e. drop trigger try master; call into set, every work fine. present noon trigger influence? get it. do set without unique constraint master.detailed (i.e. alter table master drop constraint master_detail_id_key; call into set), every work fine too. why? omit explicit detail=null assign update statement first german (since there' null value set anyway), every work fine too. why? try poster 9.6.12 (embedded), 9.6.15 (in doctor), 11.5 (in doctor). problem reproduce doctor image tomaszalusky/trip-example avail dockerhub built dockerfil (instruct inside). update 2: found common behaviour three observe above. spain query select * pgrowlocks('master') pgrowlock extent second transaction. row-level lock update row master update fail case key update three work cases. perfect compliance mode match table document since update mode stronger one mode request insert statement key share (which appear error message, also invoke select ... key share command effect insert command). document update mode says: update lock mode also acquire (...) update modify value certain columns. currently, set column consider update case unique index use foreign key (...) true master.detailed column. however, still clear update mode chosen indeed trigger present trigger present cause it."
55145223,SSIS job failed - An item with the same key has already been added,"Our SSIS package fails 2 seconds into the run with the following error:
An item with the same key has already been added.;   at System.ThrowHelper.ThrowArgumentException(ExceptionResource resource)
   at System.Collections.Generic.Dictionary`2.Insert(TKey key, TValue value, Boolean add)
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.StartPackage()
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.PerformOperation()    Transact-SQL stored procedure
We deploy our SSIS packages (2016) to our SSISDB on MSSQL: 13.0.4001.0, we also have environment variables in that same folder
The strange thing is that in this project I have 5 packages that run ok (different job) but only 1 fails (it has its’ own job)
I already deleted a recreated the folder/ environment variables/jobs  - same result
I made sure I have Different environment variables folder name
We run the job with different users and 2 of them were admins in the DB
We have other servers which have the same config and project (2 QA environments) and they work the same but OK!
Do I need to go directly to MSDB and delete a row? Where?
Appreciate any ideas – thank you all
I found the error and solution 😊
It seems I deployed a package with 2 same named SMTP connection (one as a project connection and the other with same name as a package connection)
I deleted the unnecessary (in my case deleted the package one) and deployed the fixed package.
Now the job run successfully calling on one only connection.
I was missing the correct error handling from the error itself since it did not direct me to the specific connection
The package failed at runtime on validations – so no error logs to assist
I run the query from [SSISDB].[catalog].[execution_parameter_values] and compared between a successful run to a failed then I noticed the same named connections   
Thank you for your comments!
Yoni 
",<sql-server><ssis>,1925,0,0,341,1,2,10,35,17411,0.0,2,3,12,2019-03-13 15:11,2019-03-14 14:05,,1.0,,Basic,13,"<sql-server><ssis>, SSIS job failed - An item with the same key has already been added, Our SSIS package fails 2 seconds into the run with the following error:
An item with the same key has already been added.;   at System.ThrowHelper.ThrowArgumentException(ExceptionResource resource)
   at System.Collections.Generic.Dictionary`2.Insert(TKey key, TValue value, Boolean add)
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.StartPackage()
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.PerformOperation()    Transact-SQL stored procedure
We deploy our SSIS packages (2016) to our SSISDB on MSSQL: 13.0.4001.0, we also have environment variables in that same folder
The strange thing is that in this project I have 5 packages that run ok (different job) but only 1 fails (it has its’ own job)
I already deleted a recreated the folder/ environment variables/jobs  - same result
I made sure I have Different environment variables folder name
We run the job with different users and 2 of them were admins in the DB
We have other servers which have the same config and project (2 QA environments) and they work the same but OK!
Do I need to go directly to MSDB and delete a row? Where?
Appreciate any ideas – thank you all
I found the error and solution 😊
It seems I deployed a package with 2 same named SMTP connection (one as a project connection and the other with same name as a package connection)
I deleted the unnecessary (in my case deleted the package one) and deployed the fixed package.
Now the job run successfully calling on one only connection.
I was missing the correct error handling from the error itself since it did not direct me to the specific connection
The package failed at runtime on validations – so no error logs to assist
I run the query from [SSISDB].[catalog].[execution_parameter_values] and compared between a successful run to a failed then I noticed the same named connections   
Thank you for your comments!
Yoni 
","<sal-server><suis>, si job fail - item key already added, si package fail 2 second run follow error: item key already added.; system.throwhelper.throwargumentexception(exceptionresourc resource) system.collections.genetic.dictionary`2.insert(they key, talk value, woolen add) microsoft.sqlserver.integrationservices.server.isserverexec.projectoperator.startpackage() microsoft.sqlserver.integrationservices.server.isserverexec.projectoperator.performoperation() transact-sal store procedure deploy si package (2016) ssisdb mssql: 13.0.4001.0, also environs variable older strange thing project 5 package run ok (differ job) 1 fail (it its’ job) already delete retreat older/ environs variable/job - result made sure differ environs variable older name run job differ user 2 admit do server confirm project (2 a environment) work ok! need go directly made delete row? where? appreci idea – thank found error slut 😊 seem deploy package 2 name sat connect (one project connect name package connection) delete unnecessary (in case delete package one) deploy fix package. job run success call one connection. miss correct error hand error since direct specie connect package fail until valid – error log assist run query [ssisdb].[catalogue].[execution_parameter_values] compare success run fail notice name connect thank comments! yon"
53855219,"MySQL not updating information_schema, unless I manually run ANALYZE TABLE `myTable`","I have the need to get last id (primary key) of a table (InnoDB), and to do so I perform the following query:
SELECT (SELECT `AUTO_INCREMENT` FROM `information_schema`.`TABLES` WHERE `TABLE_SCHEMA` = 'mySchema' AND `TABLE_NAME` = 'myTable') - 1;
which returns the wrong AUTO_INCREMENT. The problem is the TABLES table of information_schema is not updated with the current value, unless I run the following query:
ANALYZE TABLE `myTable`;
Why doesn't MySQL update information_schema automatically, and how could I fix this behavior?
Running MySQL Server 8.0.13 X64.
",<mysql><innodb><information-schema><mysql-8.0>,565,0,2,928,2,13,33,64,15108,,32,3,12,2018-12-19 16:17,2018-12-19 16:44,2018-12-19 16:44,0.0,0.0,Intermediate,16,"<mysql><innodb><information-schema><mysql-8.0>, MySQL not updating information_schema, unless I manually run ANALYZE TABLE `myTable`, I have the need to get last id (primary key) of a table (InnoDB), and to do so I perform the following query:
SELECT (SELECT `AUTO_INCREMENT` FROM `information_schema`.`TABLES` WHERE `TABLE_SCHEMA` = 'mySchema' AND `TABLE_NAME` = 'myTable') - 1;
which returns the wrong AUTO_INCREMENT. The problem is the TABLES table of information_schema is not updated with the current value, unless I run the following query:
ANALYZE TABLE `myTable`;
Why doesn't MySQL update information_schema automatically, and how could I fix this behavior?
Running MySQL Server 8.0.13 X64.
","<myself><innodb><information-scheme><myself-8.0>, myself update information_schema, unless manual run analyze table `table`, need get last id (primary key) table (innodb), perform follow query: select (select `auto_increment` `information_schema`.`tables` `table_schema` = 'myschema' `table_name` = 'table') - 1; return wrong auto_increment. problem table table information_schema update current value, unless run follow query: analyze table `table`; myself update information_schema automatically, could fix behavior? run myself server 8.0.13 x."
54821962,Connect to On Prem SQL server from Azure Web app,"I have .Net application at on prim. I want to host it at Azure but don't want to move database. I publish the application on Azure but it is not able to connect to on prim database. 
SQL server is in private netwrok.
For POC purpose, I am using MSDN subscription. I am facing below error,
A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error: 40 - Could not open a connection to SQL Server)
Thanks,
Umesh
",<c#><sql-server><azure><azure-hybrid-connections>,644,0,0,2915,5,16,18,43,18362,0.0,10,2,12,2019-02-22 7:19,2019-02-22 7:25,2019-02-22 7:51,0.0,0.0,Intermediate,22,"<c#><sql-server><azure><azure-hybrid-connections>, Connect to On Prem SQL server from Azure Web app, I have .Net application at on prim. I want to host it at Azure but don't want to move database. I publish the application on Azure but it is not able to connect to on prim database. 
SQL server is in private netwrok.
For POC purpose, I am using MSDN subscription. I am facing below error,
A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error: 40 - Could not open a connection to SQL Server)
Thanks,
Umesh
","<c#><sal-server><azure><azure-horrid-connections>, connect pre sal server azur web pp, .net applied prim. want host azur want move database. publish applied azur all connect prim database. sal server privat network. pot purpose, use man subscription. face error, network-red instance-specie error occur establish connect sal server. server found accessible. verify instant name correct sal server configur allow remote connections. (provider: name pipe provider, error: 40 - could open connect sal server) thanks, mesh"
56110263,pgAdmin argument formats can't be mixed,"Background
Ubuntu 18.04
Postgresql 11.2 in Docker 
pgAdmin4 3.5
Have a column named alias with type character varying[](64). Values have already been set on some rows before using psycopg2. Everything was alright then.
SQL = 'UPDATE public.""mytable"" SET alias=%s WHERE id=%s'
query = cursor.mogrify(SQL, ([values] , id))
cursor.execute(query)
conn.commit()
Recently, when I want to add more value using pgAdmin GUI as shown in the first figure, the error in the second figure happens, which says Argument formats can't be mixed:
Well, it turns out if insert the values using script such as psql or query tool in pgAdmin, the error does not happen, i.e., it only happens if using GUI of pgAdmin.
Example script:
UPDATE public.""mytable"" SET alias='{a, b}' WHERE id='myid'
But as the GUI is much easier to modify values, so really want to figure it out. Any idea?
",<postgresql><pgadmin>,861,2,7,2370,3,18,36,62,2083,,44,2,12,2019-05-13 10:13,2019-09-17 16:51,,127.0,,Basic,9,"<postgresql><pgadmin>, pgAdmin argument formats can't be mixed, Background
Ubuntu 18.04
Postgresql 11.2 in Docker 
pgAdmin4 3.5
Have a column named alias with type character varying[](64). Values have already been set on some rows before using psycopg2. Everything was alright then.
SQL = 'UPDATE public.""mytable"" SET alias=%s WHERE id=%s'
query = cursor.mogrify(SQL, ([values] , id))
cursor.execute(query)
conn.commit()
Recently, when I want to add more value using pgAdmin GUI as shown in the first figure, the error in the second figure happens, which says Argument formats can't be mixed:
Well, it turns out if insert the values using script such as psql or query tool in pgAdmin, the error does not happen, i.e., it only happens if using GUI of pgAdmin.
Example script:
UPDATE public.""mytable"" SET alias='{a, b}' WHERE id='myid'
But as the GUI is much easier to modify values, so really want to figure it out. Any idea?
","<postgresql><pgadmin>, pgadmin argument format can't mixed, background bunt 18.04 postgresql 11.2 doctor pgadmin4 3.5 column name asia type character varying[](64). value already set row use psycopg2. every alright then. sal = 'update public.""table"" set alias=% id=%s' query = curses.modify(sal, ([values] , id)) curses.execute(query) corn.commit() recently, want add value use pgadmin gun shown first figure, error second figure happens, say argument format can't mixed: well, turn insert value use script pool query tool pgadmin, error happen, i.e., happen use gun pgadmin. example script: update public.""table"" set alias='{a, b}' id='maid' gun much easier modify values, really want figure out. idea?"
52143396,JDBC SQL Server : The value is not set for the parameter number,"I received the following error from the code that calls a stored procedure from java code:    
  Exception Trace {} org.springframework.jdbc.UncategorizedSQLException:
  CallableStatementCallback; uncategorized SQLException for SQL [{call
  test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, ,
  ?, ?, ?, ?, ?)}]; SQL state [null]; error code [0]; The value is not
  set for the parameter number 11.; nested exception is
  com.microsoft.sqlserver.jdbc.SQLServerException: The value is not set
  for the parameter number 11. at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:1095)
  at
  org.springframework.jdbc.core.JdbcTemplate.call(JdbcTemplate.java:1131)
The application is deployed on WAS 8.5.5 and using jdbc driver version 4.2. On restarting the server this issue did not occur again. The following call statement generated looks to be incorrect. There are consecutive commas without ? between them.
  {call test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?,
  ?)}
The stored procedure has 10 parameters. Following is the definition of the stored procedure:
CREATE PROCEDURE [test].[usp_xxx_GetCompanyDetails]
(
    @ANumber        int,
    @CompanyId      int,
    @UserRole       varchar(15),
    @RequestId      varchar(100),
    @CompanyCode    varchar(5),
    @BaseSystem     varchar(5),
    @PType          varchar(20),    
    @PId            varchar(40),
    @IsActive       bit,     
    @responseData   xml OUT
)
Following is the java code that makes a call to the stored proc. It is using spring data to make the call.
 private String executeProc(Integer aNumber,Integer companyId, String baseSystem, 
                String role,String companyCode,String requestId, String pType,String pId,
                boolean isActive ) throws SQLException {
            SQLXML responseData=null;
            Map&lt;String,Object&gt; inputParams= new HashMap&lt;&gt;();
            inputParams.put(""ANumber"", aNumber);
            inputParams.put(""CompanyId"", companyId);
            inputParams.put(""UserRole"", role);
            inputParams.put(""RequestId"", requestId);
            inputParams.put(""CompanyCode"", companyCode);
            inputParams.put(""BaseSystem"", baseSystem);
            inputParams.put(""PType"", pType);
            inputParams.put(""PId"", pId);
            inputParams.put(""IsActive"", isActive);
            inputParams.put(""ResponseData"", responseData);
            Map&lt;String, Object&gt; result = this.execute(inputParams);
            String responseXMLString = ((SQLXML) result.get(""ResponseData"")).getString();
            return responseXMLString;
        }
What could have gone wrong.
",<java><sql-server><jdbc><sql-server-2012><websphere>,3077,0,35,1235,0,16,36,59,5994,0.0,62,2,12,2018-09-03 5:02,2018-09-03 5:09,,0.0,,Advanced,32,"<java><sql-server><jdbc><sql-server-2012><websphere>, JDBC SQL Server : The value is not set for the parameter number, I received the following error from the code that calls a stored procedure from java code:    
  Exception Trace {} org.springframework.jdbc.UncategorizedSQLException:
  CallableStatementCallback; uncategorized SQLException for SQL [{call
  test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, ,
  ?, ?, ?, ?, ?)}]; SQL state [null]; error code [0]; The value is not
  set for the parameter number 11.; nested exception is
  com.microsoft.sqlserver.jdbc.SQLServerException: The value is not set
  for the parameter number 11. at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:1095)
  at
  org.springframework.jdbc.core.JdbcTemplate.call(JdbcTemplate.java:1131)
The application is deployed on WAS 8.5.5 and using jdbc driver version 4.2. On restarting the server this issue did not occur again. The following call statement generated looks to be incorrect. There are consecutive commas without ? between them.
  {call test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?,
  ?)}
The stored procedure has 10 parameters. Following is the definition of the stored procedure:
CREATE PROCEDURE [test].[usp_xxx_GetCompanyDetails]
(
    @ANumber        int,
    @CompanyId      int,
    @UserRole       varchar(15),
    @RequestId      varchar(100),
    @CompanyCode    varchar(5),
    @BaseSystem     varchar(5),
    @PType          varchar(20),    
    @PId            varchar(40),
    @IsActive       bit,     
    @responseData   xml OUT
)
Following is the java code that makes a call to the stored proc. It is using spring data to make the call.
 private String executeProc(Integer aNumber,Integer companyId, String baseSystem, 
                String role,String companyCode,String requestId, String pType,String pId,
                boolean isActive ) throws SQLException {
            SQLXML responseData=null;
            Map&lt;String,Object&gt; inputParams= new HashMap&lt;&gt;();
            inputParams.put(""ANumber"", aNumber);
            inputParams.put(""CompanyId"", companyId);
            inputParams.put(""UserRole"", role);
            inputParams.put(""RequestId"", requestId);
            inputParams.put(""CompanyCode"", companyCode);
            inputParams.put(""BaseSystem"", baseSystem);
            inputParams.put(""PType"", pType);
            inputParams.put(""PId"", pId);
            inputParams.put(""IsActive"", isActive);
            inputParams.put(""ResponseData"", responseData);
            Map&lt;String, Object&gt; result = this.execute(inputParams);
            String responseXMLString = ((SQLXML) result.get(""ResponseData"")).getString();
            return responseXMLString;
        }
What could have gone wrong.
","<cava><sal-server><job><sal-server-2012><websphere>, job sal server : value set parapet number, receive follow error code call store procedure cava code: except trace {} org.springframework.job.uncategorizedsqlexception: callablestatementcallback; uncategor sqlexcept sal [{call test.usp_xxx_getcompanydetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?, ?)}]; sal state [null]; error code [0]; value set parapet number 11.; nest except com.microsoft.sqlserver.job.sqlserverexception: value set parapet number 11. org.springframework.job.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.cava:84) org.springframework.job.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.cava:81) org.springframework.job.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.cava:81) org.springframework.job.core.jdbctemplate.execute(jdbctemplate.cava:1095) org.springframework.job.core.jdbctemplate.call(jdbctemplate.cava:1131) applied deploy 8.5.5 use job driver version 4.2. start server issue occur again. follow call statement genet look incorrect. consent comma without ? them. {call test.usp_xxx_getcompanydetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?, ?)} store procedure 10 parameter. follow definite store procedure: great procedure [test].[usp_xxx_getcompanydetails] ( @numb in, @company in, @userrol varchar(15), @requested varchar(100), @companycod varchar(5), @basesystem varchar(5), @type varchar(20), @did varchar(40), @sat bit, @responsedata all ) follow cava code make call store pro. use spring data make call. privat string executeproc(inter number,inter company, string basesystem, string role,sir companycode,sir requested, string type,sir did, woolen sat ) throw sqlexcept { sqlxml responsedata=null; map&it;string,object&it; inputparams= new hashmap&it;&it;(); inputparams.put(""number"", number); inputparams.put(""company"", company); inputparams.put(""userrole"", role); inputparams.put(""requested"", requested); inputparams.put(""companycode"", companycode); inputparams.put(""basesystem"", basesystem); inputparams.put(""type"", type); inputparams.put(""did"", did); inputparams.put(""inactive"", inactive); inputparams.put(""responsedata"", responsedata); map&it;string, object&it; result = this.execute(inputparams); string responsexmlstr = ((sqlxml) result.get(""responsedata"")).getting(); return responsexmlstring; } could gone wrong."
52537741,multiprocessing / psycopg2 TypeError: can't pickle _thread.RLock objects,"I followed the below code in order to implement a parallel select query on a postgres database:
https://tech.geoblink.com/2017/07/06/parallelizing-queries-in-postgresql-with-python/
My basic problem is that I have ~6k queries that need to be executed, and I am trying to optimise the execution of these select queries. Initially it was a single query with the where id in (...) contained all 6k predicate IDs but I ran into issues with the query using up > 4GB of RAM on the machine it ran on, so I decided to split it out into 6k individual queries which when synchronously keeps a steady memory usage. However it takes a lot longer to run time wise, which is less of an issue for my use case. Even so I am trying to reduce the time as much as possible.
This is what my code looks like:
class PostgresConnector(object):
    def __init__(self, db_url):
        self.db_url = db_url
        self.engine = self.init_connection()
        self.pool = self.init_pool()
    def init_pool(self):
        CPUS = multiprocessing.cpu_count()
        return multiprocessing.Pool(CPUS)
    def init_connection(self):
        LOGGER.info('Creating Postgres engine')
        return create_engine(self.db_url)
    def run_parallel_queries(self, queries):
        results = []
        try:
            for i in self.pool.imap_unordered(self.execute_parallel_query, queries):
                results.append(i)
        except Exception as exception:
            LOGGER.error('Error whilst executing %s queries in parallel: %s', len(queries), exception)
            raise
        finally:
            self.pool.close()
            self.pool.join()
        LOGGER.info('Parallel query ran producing %s sets of results of type: %s', len(results), type(results))
        return list(chain.from_iterable(results))
    def execute_parallel_query(self, query):
        con = psycopg2.connect(self.db_url)
        cur = con.cursor()
        cur.execute(query)
        records = cur.fetchall()
        con.close()
        return list(records)
However whenever this runs, I get the following error:
TypeError: can't pickle _thread.RLock objects
I've read lots of similar questions regarding the use of multiprocessing and pickleable objects but I cant for the life of me figure out what I am doing wrong.
The pool is generally one per process (which I believe is the best practise) but shared per instance of the connector class so that its not creating a pool for each use of the parallel_query method.
The top answer to a similar question:
Accessing a MySQL connection pool from Python multiprocessing
Shows an almost identical implementation to my own, except using MySql instead of Postgres.
Am I doing something wrong?
Thanks!
EDIT: 
I've found this answer:
Python Postgres psycopg2 ThreadedConnectionPool exhausted
which is incredibly detailed and looks as though I have misunderstood what multiprocessing.Pool vs a connection pool such as ThreadedConnectionPool gives me. However in the first link it doesn't mention needing any connection pools etc. This solution seems good but seems A LOT of code for what I think is a fairly simple problem?
EDIT 2: 
So the above link solves another problem, which I would have likely run into anyway so I'm glad I found that, but it doesnt solve the initial issue of not being able to use imap_unordered down to the pickling error. Very frustrating. 
Lastly, I think its probably worth noting that this runs in Heroku, on a worker dyno, using Redis rq for scheduling, background tasks etc and a hosted instance of Postgres as the database. 
",<python><postgresql><sqlalchemy><psycopg2><python-multiprocessing>,3558,4,43,545,2,6,25,56,5165,0.0,21,1,12,2018-09-27 13:07,2018-10-08 13:33,2018-10-08 13:33,11.0,11.0,Advanced,32,"<python><postgresql><sqlalchemy><psycopg2><python-multiprocessing>, multiprocessing / psycopg2 TypeError: can't pickle _thread.RLock objects, I followed the below code in order to implement a parallel select query on a postgres database:
https://tech.geoblink.com/2017/07/06/parallelizing-queries-in-postgresql-with-python/
My basic problem is that I have ~6k queries that need to be executed, and I am trying to optimise the execution of these select queries. Initially it was a single query with the where id in (...) contained all 6k predicate IDs but I ran into issues with the query using up > 4GB of RAM on the machine it ran on, so I decided to split it out into 6k individual queries which when synchronously keeps a steady memory usage. However it takes a lot longer to run time wise, which is less of an issue for my use case. Even so I am trying to reduce the time as much as possible.
This is what my code looks like:
class PostgresConnector(object):
    def __init__(self, db_url):
        self.db_url = db_url
        self.engine = self.init_connection()
        self.pool = self.init_pool()
    def init_pool(self):
        CPUS = multiprocessing.cpu_count()
        return multiprocessing.Pool(CPUS)
    def init_connection(self):
        LOGGER.info('Creating Postgres engine')
        return create_engine(self.db_url)
    def run_parallel_queries(self, queries):
        results = []
        try:
            for i in self.pool.imap_unordered(self.execute_parallel_query, queries):
                results.append(i)
        except Exception as exception:
            LOGGER.error('Error whilst executing %s queries in parallel: %s', len(queries), exception)
            raise
        finally:
            self.pool.close()
            self.pool.join()
        LOGGER.info('Parallel query ran producing %s sets of results of type: %s', len(results), type(results))
        return list(chain.from_iterable(results))
    def execute_parallel_query(self, query):
        con = psycopg2.connect(self.db_url)
        cur = con.cursor()
        cur.execute(query)
        records = cur.fetchall()
        con.close()
        return list(records)
However whenever this runs, I get the following error:
TypeError: can't pickle _thread.RLock objects
I've read lots of similar questions regarding the use of multiprocessing and pickleable objects but I cant for the life of me figure out what I am doing wrong.
The pool is generally one per process (which I believe is the best practise) but shared per instance of the connector class so that its not creating a pool for each use of the parallel_query method.
The top answer to a similar question:
Accessing a MySQL connection pool from Python multiprocessing
Shows an almost identical implementation to my own, except using MySql instead of Postgres.
Am I doing something wrong?
Thanks!
EDIT: 
I've found this answer:
Python Postgres psycopg2 ThreadedConnectionPool exhausted
which is incredibly detailed and looks as though I have misunderstood what multiprocessing.Pool vs a connection pool such as ThreadedConnectionPool gives me. However in the first link it doesn't mention needing any connection pools etc. This solution seems good but seems A LOT of code for what I think is a fairly simple problem?
EDIT 2: 
So the above link solves another problem, which I would have likely run into anyway so I'm glad I found that, but it doesnt solve the initial issue of not being able to use imap_unordered down to the pickling error. Very frustrating. 
Lastly, I think its probably worth noting that this runs in Heroku, on a worker dyno, using Redis rq for scheduling, background tasks etc and a hosted instance of Postgres as the database. 
","<patron><postgresql><sqlalchemy><psycopg2><patron-multiprocessing>, multiprocess / psycopg2 typeerror: can't pick thread.clock objects, follow code order implement parallel select query poster database: http://teach.geoblink.com/2017/07/06/parallelizing-queried-in-postgresql-with-patron/ basic problem ~k query need executed, try optimism execute select queried. into single query id (...) contain k predict id ran issue query use > go ram machine ran on, decide split k individu query synchron keep steady memory usage. howe take lot longer run time wise, less issue use case. even try reduce time much possible. code look like: class postgresconnector(object): def __init__(self, db_url): self.db_url = db_url self.engine = self.init_connection() self.pool = self.init_pool() def init_pool(self): cup = multiprocessing.cpu_count() return multiprocessing.pool(pus) def init_connection(self): longer.into('or poster engine') return create_engine(self.db_url) def run_parallel_queries(self, queried): result = [] try: self.pool.imap_unordered(self.execute_parallel_query, queried): results.happened(i) except except exception: longer.error('error whilst execute %s query parallel: %s', len(queried), exception) rays finally: self.pool.close() self.pool.join() longer.into('parallel query ran produce %s set result type: %s', len(results), type(results)) return list(chain.from_iterable(results)) def execute_parallel_query(self, query): con = psycopg2.connect(self.db_url) our = con.curses() our.execute(query) record = our.fetchall() con.close() return list(records) howe when runs, get follow error: typeerror: can't pick thread.clock object i'v read lot similar question regard use multiprocess pickleabl object can life figure wrong. pool genet one per process (which believe best practise) share per instant connection class great pool use parallel_queri method. top answer similar question: access myself connect pool patron multiprocess show almost went implement own, except use myself instead postures. cometh wrong? thanks! edit: i'v found answer: patron poster psycopg2 threadedconnectionpool exhaust inured detail look though misunderstood multiprocessing.pool vs connect pool threadedconnectionpool give me. howe first link mention need connect pool etc. slut seem good seem lot code think fairly simple problem? edit 2: link sole not problem, would like run anyway i'm glad found that, doesn sole into issue all use imap_unord pick error. frustration. lastly, think probably worth note run hero, worker no, use red re schelling, background task etc host instant poster database."
52096692,Change sequelize timezone,"I want to make restful app in nodejs
Server: centos 7 64x
Database: postgresql
Additional: express, sequelize
Table: datetime with timezone
When I selecting rows with sequelize from database, created_at column gives me wrong time. 5 hour added to datetime.
I change timezone configuration of centos to +5 (Tashkent/Asia)
Also change postgresql timezone configuration to +5
Datetime is correct in database when shows.
But when I select it converts to like this
""createdAt"": ""2018-08-12T17:57:20.508Z""
In database column shows this
2018-08-12 22:57:20.508+05
config.json
""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""zablet"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""timezone"": ""Tashkent/Ashgabat"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    }
}
index.js
'use strict';
var fs = require('fs');
var path = require('path');
var Sequelize = require('sequelize');
var basename = path.basename(__filename);
var env = process.env.NODE_ENV || 'development';
var config = require('../config/config.json')[env];
var db = {};
if (config.use_env_variable) {
    var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
    var sequelize = new Sequelize(config.database, config.username, config.password, config);
}
fs
    .readdirSync(__dirname)
    .filter(file =&gt; {
        return (file.indexOf('.') !== 0) &amp;&amp; (file !== basename) &amp;&amp; (file.slice(-3) === '.js');
    })
    .forEach(file =&gt; {
        var model = sequelize['import'](path.join(__dirname, file));
        db[model.name] = model;
    });
Object.keys(db).forEach(modelName =&gt; {
    if (db[modelName].associate) {
        db[modelName].associate(db);
    }
});
db.sequelize = sequelize;
db.Sequelize = Sequelize;
module.exports = db;
updated config.json
{
    ""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""postgres"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    },
    ""dialectOptions"": {
        ""useUTC"": false
    },
    ""timezone"": ""+05:00""
}
}
How can I select rows from database in correct timezone format?
",<node.js><postgresql><express><sequelize.js>,2401,0,66,553,4,11,31,67,44370,0.0,37,2,12,2018-08-30 12:07,2018-08-30 13:53,2018-08-30 13:53,0.0,0.0,Basic,2,"<node.js><postgresql><express><sequelize.js>, Change sequelize timezone, I want to make restful app in nodejs
Server: centos 7 64x
Database: postgresql
Additional: express, sequelize
Table: datetime with timezone
When I selecting rows with sequelize from database, created_at column gives me wrong time. 5 hour added to datetime.
I change timezone configuration of centos to +5 (Tashkent/Asia)
Also change postgresql timezone configuration to +5
Datetime is correct in database when shows.
But when I select it converts to like this
""createdAt"": ""2018-08-12T17:57:20.508Z""
In database column shows this
2018-08-12 22:57:20.508+05
config.json
""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""zablet"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""timezone"": ""Tashkent/Ashgabat"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    }
}
index.js
'use strict';
var fs = require('fs');
var path = require('path');
var Sequelize = require('sequelize');
var basename = path.basename(__filename);
var env = process.env.NODE_ENV || 'development';
var config = require('../config/config.json')[env];
var db = {};
if (config.use_env_variable) {
    var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
    var sequelize = new Sequelize(config.database, config.username, config.password, config);
}
fs
    .readdirSync(__dirname)
    .filter(file =&gt; {
        return (file.indexOf('.') !== 0) &amp;&amp; (file !== basename) &amp;&amp; (file.slice(-3) === '.js');
    })
    .forEach(file =&gt; {
        var model = sequelize['import'](path.join(__dirname, file));
        db[model.name] = model;
    });
Object.keys(db).forEach(modelName =&gt; {
    if (db[modelName].associate) {
        db[modelName].associate(db);
    }
});
db.sequelize = sequelize;
db.Sequelize = Sequelize;
module.exports = db;
updated config.json
{
    ""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""postgres"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    },
    ""dialectOptions"": {
        ""useUTC"": false
    },
    ""timezone"": ""+05:00""
}
}
How can I select rows from database in correct timezone format?
","<node.is><postgresql><express><sequelae.is>, change sequel timezone, want make rest pp nodes server: cent 7 six database: postgresql additional: express, sequel table: datetim timezon select row sequel database, created_at column give wrong time. 5 hour ad daytime. change timezon configur cent +5 (tashkent/asia) also change postgresql timezon configur +5 datetim correct database shows. select convert like ""created"": ""2018-08-12t17:57:20.508z"" database column show 2018-08-12 22:57:20.508+05 confirm.son ""development"": { ""surname"": ""postures"", ""password"": ""postures"", ""database"": ""tablet"", ""host"": ""127.0.0.1"", ""dialect"": ""postures"", ""timezone"": ""tashkent/ashgabat"", ""define"": { ""charge"": ""utf"", ""dialectoptions"": { ""collar"": ""utf8_general_ci"" }, ""freezetablename"": true } } index.j 'use strict'; war is = require('is'); war path = require('path'); war sequel = require('sequelae'); war basenam = path.basename(filename); war end = process.end.node_env || 'development'; war confirm = require('../confirm/confirm.son')[end]; war do = {}; (confirm.use_env_variable) { war sequel = new sequelae(process.end[confirm.use_env_variable], confirm); } else { war sequel = new sequelae(confirm.database, confirm.surname, confirm.password, confirm); } is .readdirsync(__dirname) .filter(fig =&it; { return (file.index('.') !== 0) &amp;&amp; (file !== basename) &amp;&amp; (file.slice(-3) === '.is'); }) .french(fig =&it; { war model = sequelae['import'](path.join(__dirname, file)); do[model.name] = model; }); object.keys(do).french(modelnam =&it; { (do[modelname].associate) { do[modelname].associate(do); } }); do.sequel = sequelae; do.sequel = sequelae; module.export = do; update confirm.son { ""development"": { ""surname"": ""postures"", ""password"": ""postures"", ""database"": ""postures"", ""host"": ""127.0.0.1"", ""dialect"": ""postures"", ""define"": { ""charge"": ""utf"", ""dialectoptions"": { ""collar"": ""utf8_general_ci"" }, ""freezetablename"": true }, ""dialectoptions"": { ""useutc"": fall }, ""timezone"": ""+05:00"" } } select row database correct timezon format?"
58905555,"Cannot connect to MySQL database (running on WSL2) from Windows Desktop Application ""MySQL Workbench""","I set up MySQL on Windows Subsystem for Linux (WSL2 version).  I'm relatively new to MySQL, but I have confirmed the following: 
It is running (ps ax | grep mysqld returns a value)
It is running on default host 127.0.0.1
It is running on default port 3306
To login to the mysql shell, I use the command sudo mysql -u root -p.  Without sudo, I am unable to login to the shell.
I assume that this issue has something to do with the host that the MySQL service is running on, but I have no idea how to change that and properly connect.  Below is a screenshot of the connection setup in MySQL Workbench.
And below is the error that I get when I use the settings shown and my root user password.
",<mysql-workbench><windows-subsystem-for-linux>,691,3,5,2164,2,17,26,58,23856,0.0,116,6,12,2019-11-17 21:51,2019-12-06 19:10,,19.0,,Basic,9,"<mysql-workbench><windows-subsystem-for-linux>, Cannot connect to MySQL database (running on WSL2) from Windows Desktop Application ""MySQL Workbench"", I set up MySQL on Windows Subsystem for Linux (WSL2 version).  I'm relatively new to MySQL, but I have confirmed the following: 
It is running (ps ax | grep mysqld returns a value)
It is running on default host 127.0.0.1
It is running on default port 3306
To login to the mysql shell, I use the command sudo mysql -u root -p.  Without sudo, I am unable to login to the shell.
I assume that this issue has something to do with the host that the MySQL service is running on, but I have no idea how to change that and properly connect.  Below is a screenshot of the connection setup in MySQL Workbench.
And below is the error that I get when I use the settings shown and my root user password.
","<myself-workbench><windows-subsystem-for-line>, cannot connect myself database (run will) window desktop applied ""myself workbench"", set myself window subsystem line (will version). i'm red new myself, confirm following: run (p ax | grew myself return value) run default host 127.0.0.1 run default port 3306 login myself shell, use command so myself -u root -p. without so, unable login shell. assume issue cometh host myself service run on, idea change properly connect. screenshot connect set myself workbench. error get use set shown root user password."
53800062,"""expected zero arguments for construction of ClassDict (for numpy.dtype)"" when calling UDF that returns FloatType()","I believe it is related to this one: Spark Error:expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
I have a dataframe
id col_1 col_2
1 [1,2] [1,3]
2 [2,1] [3,4]
I want to create another column that is a cosine distance between col_1 and col_2.
from scipy.spatial.distance import cosine
def cosine_distance(a,b):
    try:
        return cosine(a, b)
    except Exception as e:
        return 0.0 # in case division by zero
And I defined a udf:
cosine_distance_udf = udf (cosine_distance, FloatType())
And finally:
new_df = df.withColumn('cosine_distance', cosine_distance_udf('col_1', 'col_2'))
And I have the error: PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
What did I do wrong?
",<python><dataframe><pyspark><apache-spark-sql>,771,1,17,4480,11,46,76,35,9896,0.0,102,1,12,2018-12-16 6:52,2018-12-16 7:10,2018-12-16 7:10,0.0,0.0,Advanced,32,"<python><dataframe><pyspark><apache-spark-sql>, ""expected zero arguments for construction of ClassDict (for numpy.dtype)"" when calling UDF that returns FloatType(), I believe it is related to this one: Spark Error:expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
I have a dataframe
id col_1 col_2
1 [1,2] [1,3]
2 [2,1] [3,4]
I want to create another column that is a cosine distance between col_1 and col_2.
from scipy.spatial.distance import cosine
def cosine_distance(a,b):
    try:
        return cosine(a, b)
    except Exception as e:
        return 0.0 # in case division by zero
And I defined a udf:
cosine_distance_udf = udf (cosine_distance, FloatType())
And finally:
new_df = df.withColumn('cosine_distance', cosine_distance_udf('col_1', 'col_2'))
And I have the error: PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
What did I do wrong?
","<patron><dataframe><spark><apache-spark-sal>, ""expect zero argument construct classic (for jump.type)"" call utf return floattype(), believe relate one: spark error:expect zero argument construct classic (for jump.core.multiarray._reconstruct) datafram id cold cold 1 [1,2] [1,3] 2 [2,1] [3,4] want great not column cousin distance cold cold. city.spatial.list import cousin def cosine_distance(a,b): try: return cousine(a, b) except except e: return 0.0 # case davis zero define utf: cosine_distance_udf = utf (cosine_distance, floattype()) finally: new_df = of.withcolumn('cosine_distance', cosine_distance_udf('cold', 'cold')) error: pickleexception: expect zero argument construct classic (for jump.type) wrong?"
54454797,where IN clause - multi column (querydsl),"I have three integer values of two pairs.
I would like to use this as a list of IN in the WHERE clause.
  (2019, 5) (2019, 6) (2019, 7)
I want to use the above two-pair list in a query like this:
SELECT
    *
FROM
    WEEK_REPORT A
WHERE
    A.user_id IN ('test2','test5' ) AND
    (A.year, A.week_no) IN ((2019,4),(2019,5),(2019,6));
To that end, I wrote the source as follows:
// lastYear=2019, lastWeekNo=4
Tuple t = JPAExpressions.select(Expressions.constant(lastYear), Expressions.constant(lastWeekNo))
    .from(weekReport)
    .fetchOne();
// currentYear=2019, currentWeekNo=5        
Tuple t2 = JPAExpressions.select(Expressions.constant(currentYear), Expressions.constant(currentWeekNo))
    .from(weekReport)
    .fetchOne();
// nextYear=2019, nextWeekNo=4
Tuple t3 = JPAExpressions.select(Expressions.constant(nextYear), Expressions.constant(nextWeekNo))
    .from(weekReport)
    .fetchOne();
return queryFactory
    .select(weekReport)
    .from(weekReport)
    .where(weekReport.user.eq(user)
        .and(Expressions.list(weekReport.year, weekReport.weekNo).in(t, t2, t3)))
    .fetch();
However, the correct result is not output and an error occurs.
java.lang.UnsupportedOperationException: null
    at com.querydsl.jpa.JPASubQuery.fetchOne(JPASubQuery.java:66) ~[querydsl-jpa-4.1.4.jar:na]
I looked it up in the official document but it does not come out.
Is there a way?  
Thank you.
",<mysql><sql><querydsl><in-clause>,1402,0,30,323,0,2,9,67,2476,0.0,0,4,12,2019-01-31 6:44,2019-01-31 6:53,,0.0,,Basic,10,"<mysql><sql><querydsl><in-clause>, where IN clause - multi column (querydsl), I have three integer values of two pairs.
I would like to use this as a list of IN in the WHERE clause.
  (2019, 5) (2019, 6) (2019, 7)
I want to use the above two-pair list in a query like this:
SELECT
    *
FROM
    WEEK_REPORT A
WHERE
    A.user_id IN ('test2','test5' ) AND
    (A.year, A.week_no) IN ((2019,4),(2019,5),(2019,6));
To that end, I wrote the source as follows:
// lastYear=2019, lastWeekNo=4
Tuple t = JPAExpressions.select(Expressions.constant(lastYear), Expressions.constant(lastWeekNo))
    .from(weekReport)
    .fetchOne();
// currentYear=2019, currentWeekNo=5        
Tuple t2 = JPAExpressions.select(Expressions.constant(currentYear), Expressions.constant(currentWeekNo))
    .from(weekReport)
    .fetchOne();
// nextYear=2019, nextWeekNo=4
Tuple t3 = JPAExpressions.select(Expressions.constant(nextYear), Expressions.constant(nextWeekNo))
    .from(weekReport)
    .fetchOne();
return queryFactory
    .select(weekReport)
    .from(weekReport)
    .where(weekReport.user.eq(user)
        .and(Expressions.list(weekReport.year, weekReport.weekNo).in(t, t2, t3)))
    .fetch();
However, the correct result is not output and an error occurs.
java.lang.UnsupportedOperationException: null
    at com.querydsl.jpa.JPASubQuery.fetchOne(JPASubQuery.java:66) ~[querydsl-jpa-4.1.4.jar:na]
I looked it up in the official document but it does not come out.
Is there a way?  
Thank you.
","<myself><sal><querydsl><in-clause>, class - multi column (querydsl), three inter value two pairs. would like use list clause. (2019, 5) (2019, 6) (2019, 7) want use two-pair list query like this: select * week_report a.user_id ('test','test' ) (a.year, a.weekend) ((2019,4),(2019,5),(2019,6)); end, wrote source follows: // lastyear=2019, lastweekno=4 up = jpaexpressions.select(expressions.constant(lastyear), expressions.constant(lastweekno)) .from(weekreport) .fetching(); // currentyear=2019, currentweekno=5 up to = jpaexpressions.select(expressions.constant(currentyear), expressions.constant(currentweekno)) .from(weekreport) .fetching(); // nextyear=2019, nextweekno=4 up to = jpaexpressions.select(expressions.constant(nextyear), expressions.constant(nextweekno)) .from(weekreport) .fetching(); return queryfactori .select(weekreport) .from(weekreport) .where(weekreport.user.e(user) .and(expressions.list(weekreport.year, weekreport.weeks).in(t, to, to))) .fetch(); however, correct result output error occurs. cava.long.unsupportedoperationexception: null com.querydsl.pa.jpasubquery.fetching(jpasubquery.cava:66) ~[querydsl-pa-4.1.4.jar:na] look office document come out. way? thank you."
52803014,"Sqlite with real ""Full Text Search"" and spelling mistakes (FTS+spellfix together)","Let's say we have 1 million of rows like this:
import sqlite3
db = sqlite3.connect(':memory:')
c = db.cursor()
c.execute('CREATE TABLE mytable (id integer, description text)')
c.execute('INSERT INTO mytable VALUES (1, ""Riemann"")')
c.execute('INSERT INTO mytable VALUES (2, ""All the Carmichael numbers"")')
Background:
I know how to do this with Sqlite:
Find a row with a single-word query, up to a few spelling mistakes with the spellfix module and Levenshtein distance (I have posted a detailed answer here about how to compile it, how to use it, ...):
db.enable_load_extension(True)
db.load_extension('./spellfix')
c.execute('SELECT * FROM mytable WHERE editdist3(description, ""Riehmand"") &lt; 300'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(1, u'Riemann')]
With 1M rows, this would be super slow! As detailed here, postgresql might have an optimization with this using trigrams. A fast solution, available with Sqlite, is to use a VIRTUAL TABLE USING spellfix:
c.execute('CREATE VIRTUAL TABLE mytable3 USING spellfix1')
c.execute('INSERT INTO mytable3(word) VALUES (""Riemann"")')
c.execute('SELECT * FROM mytable3 WHERE word MATCH ""Riehmand""'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(u'Riemann', 1, 76, 0, 107, 7)], working!
Find an expression with a query matching one or multiple words with FTS (""Full Text Search""):
c.execute('CREATE VIRTUAL TABLE mytable2 USING fts4(id integer, description text)')
c.execute('INSERT INTO mytable2 VALUES (2, ""All the Carmichael numbers"")')
c.execute('SELECT * FROM mytable2 WHERE description MATCH ""NUMBERS carmichael""'); print c.fetchall()
#Query: 'NUMBERS carmichael'
#Answer: [(2, u'All the Carmichael numbers')]
It is case insensitive and you can even use a query with two words in the wrong order, etc.: FTS is quite powerful indeed. But the drawback is that each of the query-keyword must be correctly spelled, i.e. FTS alone doesn't allow spelling mistakes.
Question:
How to do a Full Text Search (FTS) with Sqlite and also allow spelling mistakes? i.e. ""FTS + spellfix"" together
Example: 
row in the DB: ""All the Carmichael numbers""
query: ""NUMMBER carmickaeel"" should match it!
How to do this with Sqlite? 
It is probably possible with Sqlite since this page states:
  Or, it [spellfix] could be used with FTS4 to do full-text search using potentially misspelled words.
Linked question: String similarity with Python + Sqlite (Levenshtein distance / edit distance)
",<python><sqlite><full-text-search><levenshtein-distance>,2431,4,30,42385,103,394,707,36,8839,0.0,3760,2,12,2018-10-14 13:12,2018-10-17 17:42,2018-10-17 17:42,3.0,3.0,Basic,10,"<python><sqlite><full-text-search><levenshtein-distance>, Sqlite with real ""Full Text Search"" and spelling mistakes (FTS+spellfix together), Let's say we have 1 million of rows like this:
import sqlite3
db = sqlite3.connect(':memory:')
c = db.cursor()
c.execute('CREATE TABLE mytable (id integer, description text)')
c.execute('INSERT INTO mytable VALUES (1, ""Riemann"")')
c.execute('INSERT INTO mytable VALUES (2, ""All the Carmichael numbers"")')
Background:
I know how to do this with Sqlite:
Find a row with a single-word query, up to a few spelling mistakes with the spellfix module and Levenshtein distance (I have posted a detailed answer here about how to compile it, how to use it, ...):
db.enable_load_extension(True)
db.load_extension('./spellfix')
c.execute('SELECT * FROM mytable WHERE editdist3(description, ""Riehmand"") &lt; 300'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(1, u'Riemann')]
With 1M rows, this would be super slow! As detailed here, postgresql might have an optimization with this using trigrams. A fast solution, available with Sqlite, is to use a VIRTUAL TABLE USING spellfix:
c.execute('CREATE VIRTUAL TABLE mytable3 USING spellfix1')
c.execute('INSERT INTO mytable3(word) VALUES (""Riemann"")')
c.execute('SELECT * FROM mytable3 WHERE word MATCH ""Riehmand""'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(u'Riemann', 1, 76, 0, 107, 7)], working!
Find an expression with a query matching one or multiple words with FTS (""Full Text Search""):
c.execute('CREATE VIRTUAL TABLE mytable2 USING fts4(id integer, description text)')
c.execute('INSERT INTO mytable2 VALUES (2, ""All the Carmichael numbers"")')
c.execute('SELECT * FROM mytable2 WHERE description MATCH ""NUMBERS carmichael""'); print c.fetchall()
#Query: 'NUMBERS carmichael'
#Answer: [(2, u'All the Carmichael numbers')]
It is case insensitive and you can even use a query with two words in the wrong order, etc.: FTS is quite powerful indeed. But the drawback is that each of the query-keyword must be correctly spelled, i.e. FTS alone doesn't allow spelling mistakes.
Question:
How to do a Full Text Search (FTS) with Sqlite and also allow spelling mistakes? i.e. ""FTS + spellfix"" together
Example: 
row in the DB: ""All the Carmichael numbers""
query: ""NUMMBER carmickaeel"" should match it!
How to do this with Sqlite? 
It is probably possible with Sqlite since this page states:
  Or, it [spellfix] could be used with FTS4 to do full-text search using potentially misspelled words.
Linked question: String similarity with Python + Sqlite (Levenshtein distance / edit distance)
","<patron><quite><full-text-search><levenshtein-distance>, quite real ""full text search"" spell mistake (its+spellfix together), let' say 1 million row like this: import sqlite3 do = sqlite3.connect(':memory:') c = do.curses() c.execute('or table metal (id inter, rescript text)') c.execute('insert metal value (1, ""remain"")') c.execute('insert metal value (2, ""all carmichael numbers"")') background: know quite: find row single-word query, spell mistake spellfix model levenshtein distance (i post detail answer compel it, use it, ...): do.enable_load_extension(true) do.load_extension('./spellfix') c.execute('select * metal editdist3(description, ""richmond"") &it; 300'); print c.fetchall() #query: 'richmond' #answer: [(1, u'remain')] am rows, would super slow! detail here, postgresql might optic use programs. fast solution, avail quite, use virtual table use spellfix: c.execute('or virtual table mytable3 use spellfix1') c.execute('insert mytable3(word) value (""remain"")') c.execute('select * mytable3 word match ""richmond""'); print c.fetchall() #query: 'richmond' #answer: [(u'remain', 1, 76, 0, 107, 7)], working! find express query match one multiple word ft (""full text search""): c.execute('or virtual table mytable2 use its(id inter, rescript text)') c.execute('insert mytable2 value (2, ""all carmichael numbers"")') c.execute('select * mytable2 rescript match ""number carmichael""'); print c.fetchall() #query: 'number carmichael' #answer: [(2, u'al carmichael numbers')] case intensity even use query two word wrong order, etc.: ft quit power indeed. drawback query-eyford must correctly spelled, i.e. ft along allow spell mistakes. question: full text search (its) quite also allow spell mistakes? i.e. ""ft + spellfix"" together example: row do: ""all carmichael numbers"" query: ""number carmickaeel"" match it! quite? probably possible quite since page states: or, [spellfix] could use its full-text search use potent missed words. link question: string similar patron + quite (levenshtein distance / edit distance)"
56321781,Hangfire causing locks in SQL Server,"We are using Hangfire 1.7.2 within our ASP.NET Web project with SQL Server 2016. We have around 150 sites on our server, with each site using Hangfire 1.7.2. We noticed that when we upgraded these sites to use Hangfire, the DB server collapsed. Checking the DB logs, we found out there were multiple locking queries. We have identified one RPC Event  “sys.sp_getapplock;1” In the all blocking sessions. It seems like Hangfire is locking our DB rendering whole DB unusable. We noticed almost 670+ locking queries because of Hangfire.
This could possibly be due to these properties we setup:
   SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
   QueuePollInterval = TimeSpan.FromHours(5)
Each site has around 20 background jobs, a few of them run every minute, whereas others every hour, every 6 hours and some once a day.
I have searched the documentation but could not find anything which could explain these two properties or how to set them to avoid DB locks.
Looking for some help on this.
EDIT: The following queries are executed at every second:
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
select distinct(Queue) from [HangFire].JobQueue with (nolock)
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
irrespective of various combinations of timespan values we set. Here is the code of GetHangfirServers we are using:
  public static IEnumerable&lt;IDisposable&gt; GetHangfireServers()
    {
        // Reference for GlobalConfiguration.Configuration: http://docs.hangfire.io/en/latest/getting-started/index.html
        // Reference for UseSqlServerStorage: http://docs.hangfire.io/en/latest/configuration/using-sql-server.html#configuring-the-polling-interval
        GlobalConfiguration.Configuration
            .SetDataCompatibilityLevel(CompatibilityLevel.Version_170)
            .UseSimpleAssemblyNameTypeSerializer()
            .UseRecommendedSerializerSettings()
            .UseSqlServerStorage(ConfigurationManager.ConnectionStrings[""abc""]
                .ConnectionString, new SqlServerStorageOptions
            {
                CommandBatchMaxTimeout = TimeSpan.FromMinutes(5),
                SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
                QueuePollInterval = TimeSpan.FromHours(5), // Hangfire will poll after 5 hrs to check failed jobs.
                UseRecommendedIsolationLevel = true,
                UsePageLocksOnDequeue = true,
                DisableGlobalLocks = true
            });
        // Reference: https://docs.hangfire.io/en/latest/background-processing/configuring-degree-of-parallelism.html
        var options = new BackgroundJobServerOptions
        {
            WorkerCount = 5
        };
        var server = new BackgroundJobServer(options);
        yield return server;
    }
The worker count is set just to 5.
There are just 4 jobs and even those are completed (SELECT * FROM [HangFire].[State]):
Do you have any idea why the Hangfire is hitting so many queries at each second?
",<c#><sql-server><backgroundworker><hangfire>,3178,4,37,8900,6,84,109,67,12970,0.0,1086,3,12,2019-05-27 7:46,2019-06-03 8:46,,7.0,,Basic,10,"<c#><sql-server><backgroundworker><hangfire>, Hangfire causing locks in SQL Server, We are using Hangfire 1.7.2 within our ASP.NET Web project with SQL Server 2016. We have around 150 sites on our server, with each site using Hangfire 1.7.2. We noticed that when we upgraded these sites to use Hangfire, the DB server collapsed. Checking the DB logs, we found out there were multiple locking queries. We have identified one RPC Event  “sys.sp_getapplock;1” In the all blocking sessions. It seems like Hangfire is locking our DB rendering whole DB unusable. We noticed almost 670+ locking queries because of Hangfire.
This could possibly be due to these properties we setup:
   SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
   QueuePollInterval = TimeSpan.FromHours(5)
Each site has around 20 background jobs, a few of them run every minute, whereas others every hour, every 6 hours and some once a day.
I have searched the documentation but could not find anything which could explain these two properties or how to set them to avoid DB locks.
Looking for some help on this.
EDIT: The following queries are executed at every second:
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
select distinct(Queue) from [HangFire].JobQueue with (nolock)
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
irrespective of various combinations of timespan values we set. Here is the code of GetHangfirServers we are using:
  public static IEnumerable&lt;IDisposable&gt; GetHangfireServers()
    {
        // Reference for GlobalConfiguration.Configuration: http://docs.hangfire.io/en/latest/getting-started/index.html
        // Reference for UseSqlServerStorage: http://docs.hangfire.io/en/latest/configuration/using-sql-server.html#configuring-the-polling-interval
        GlobalConfiguration.Configuration
            .SetDataCompatibilityLevel(CompatibilityLevel.Version_170)
            .UseSimpleAssemblyNameTypeSerializer()
            .UseRecommendedSerializerSettings()
            .UseSqlServerStorage(ConfigurationManager.ConnectionStrings[""abc""]
                .ConnectionString, new SqlServerStorageOptions
            {
                CommandBatchMaxTimeout = TimeSpan.FromMinutes(5),
                SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
                QueuePollInterval = TimeSpan.FromHours(5), // Hangfire will poll after 5 hrs to check failed jobs.
                UseRecommendedIsolationLevel = true,
                UsePageLocksOnDequeue = true,
                DisableGlobalLocks = true
            });
        // Reference: https://docs.hangfire.io/en/latest/background-processing/configuring-degree-of-parallelism.html
        var options = new BackgroundJobServerOptions
        {
            WorkerCount = 5
        };
        var server = new BackgroundJobServer(options);
        yield return server;
    }
The worker count is set just to 5.
There are just 4 jobs and even those are completed (SELECT * FROM [HangFire].[State]):
Do you have any idea why the Hangfire is hitting so many queries at each second?
","<c#><sal-server><backgroundworker><hangfire>, hangfir cause lock sal server, use hangfir 1.7.2 within asp.net web project sal server 2016. around 150 site server, site use hangfir 1.7.2. notice upgrade site use hangfire, do server collapsed. check do logs, found multiple lock queried. identify one rec event “says.sp_getapplock;1” block sessions. seem like hangfir lock do render whole do unable. notice almost 670+ lock query hangfire. could possible due property set: slidinginvisibilitytimeout = lifespan.fromminutes(30), queuepollinterv = lifespan.fromhours(5) site around 20 background jobs, run every minute, where other every hour, every 6 hour day. search document could find any could explain two property set avoid do locks. look help this. edit: follow query execute every second: even sp_executesql n'select count(*) [hangfire].[set] (readcommittedlock, foreseen) [key] = @key',n'@key nvarchar(4000)',@key=n'retires' select distinct(queue) [hangfire].jobqueu (clock) even sp_executesql n'select count(*) [hangfire].[set] (readcommittedlock, foreseen) [key] = @key',n'@key nvarchar(4000)',@key=n'retires' respect various combine lifespan value set. code gethangfirserv using: public static innumerable&it;idisposable&it; gethangfireservers() { // refer globalconfiguration.configuration: http://docs.hangfire.to/en/latest/getting-started/index.html // refer usesqlserverstorage: http://docs.hangfire.to/en/latest/configuration/using-sal-server.html#configuring-the-polling-inter globalconfiguration.configur .setdatacompatibilitylevel(compatibilitylevel.version_170) .usesimpleassemblynametypeserializer() .userecommendedserializersettings() .usesqlserverstorage(configurationmanager.connectionstrings[""abc""] .connectionstring, new sqlserverstorageopt { commandbatchmaxtimeout = lifespan.fromminutes(5), slidinginvisibilitytimeout = lifespan.fromminutes(30), queuepollinterv = lifespan.fromhours(5), // hangfir poll 5 he check fail jobs. userecommendedisolationlevel = true, usepagelocksondequeu = true, disablegloballock = true }); // reference: http://docs.hangfire.to/en/latest/background-processing/configuring-degree-of-parallels.html war option = new backgroundjobserveropt { workercount = 5 }; war server = new backgroundjobserver(option); yield return server; } worker count set 5. 4 job even complete (select * [hangfire].[state]): idea hangfir hit man query second?"
50238568,How to group by time bucket in ClickHouse and fill missing data with nulls/0s,"Suppose I have a given time range. For explanation, let's consider something simple, like whole year 2018. I want to query data from ClickHouse as a sum aggregation for each quarter so the result should be 4 rows. 
The problem is that I have data for only two quarters so when using GROUP BY quarter, only two rows are returned.
SELECT
     toStartOfQuarter(created_at) AS time,
     sum(metric) metric
 FROM mytable
 WHERE
     created_at &gt;= toDate(1514761200) AND created_at &gt;= toDateTime(1514761200)
    AND
     created_at &lt;= toDate(1546210800) AND created_at &lt;= toDateTime(1546210800)
 GROUP BY time
 ORDER BY time
1514761200 – 2018-01-01
1546210800 – 2018-12-31
This returns:
time       metric
2018-01-01 345
2018-04-01 123
And I need:
time       metric
2018-01-01 345
2018-04-01 123
2018-07-01 0
2018-10-01 0
This is simplified example but in real use case the aggregation would be eg. 5 minutes instead of quarters and GROUP BY would have at least one more attribute like GROUP BY attribute1, time so desired result is
time        metric  attribute1
2018-01-01  345     1
2018-01-01  345     2
2018-04-01  123     1
2018-04-01  123     2
2018-07-01  0       1
2018-07-01  0       2
2018-10-01  0       1
2018-10-01  0       2
Is there a way to somehow fill the whole given interval? Like InfluxDB has fill argument for group or TimescaleDb's time_bucket() function with generate_series()  I tried to search ClickHouse documentation and github issues and it seems this is not implemented yet so the question perhaps is whether there's any workaround.
",<sql><clickhouse>,1570,2,36,11928,18,90,141,35,15729,0.0,1945,4,12,2018-05-08 16:46,2018-05-14 8:04,2018-05-14 8:04,6.0,6.0,Basic,2,"<sql><clickhouse>, How to group by time bucket in ClickHouse and fill missing data with nulls/0s, Suppose I have a given time range. For explanation, let's consider something simple, like whole year 2018. I want to query data from ClickHouse as a sum aggregation for each quarter so the result should be 4 rows. 
The problem is that I have data for only two quarters so when using GROUP BY quarter, only two rows are returned.
SELECT
     toStartOfQuarter(created_at) AS time,
     sum(metric) metric
 FROM mytable
 WHERE
     created_at &gt;= toDate(1514761200) AND created_at &gt;= toDateTime(1514761200)
    AND
     created_at &lt;= toDate(1546210800) AND created_at &lt;= toDateTime(1546210800)
 GROUP BY time
 ORDER BY time
1514761200 – 2018-01-01
1546210800 – 2018-12-31
This returns:
time       metric
2018-01-01 345
2018-04-01 123
And I need:
time       metric
2018-01-01 345
2018-04-01 123
2018-07-01 0
2018-10-01 0
This is simplified example but in real use case the aggregation would be eg. 5 minutes instead of quarters and GROUP BY would have at least one more attribute like GROUP BY attribute1, time so desired result is
time        metric  attribute1
2018-01-01  345     1
2018-01-01  345     2
2018-04-01  123     1
2018-04-01  123     2
2018-07-01  0       1
2018-07-01  0       2
2018-10-01  0       1
2018-10-01  0       2
Is there a way to somehow fill the whole given interval? Like InfluxDB has fill argument for group or TimescaleDb's time_bucket() function with generate_series()  I tried to search ClickHouse documentation and github issues and it seems this is not implemented yet so the question perhaps is whether there's any workaround.
","<sal><clickhouse>, group time bucket clickhous fill miss data null/is, suppose given time range. explanation, let' consider cometh simple, like whole year 2018. want query data clickhous sum agree quarter result 4 rows. problem data two quarter use group quarter, two row returned. select tostartofquarter(created_at) time, sum(merit) merit metal created_at &it;= today(1514761200) created_at &it;= todatetime(1514761200) created_at &it;= today(1546210800) created_at &it;= todatetime(1546210800) group time order time 1514761200 – 2018-01-01 1546210800 – 2018-12-31 returns: time merit 2018-01-01 345 2018-04-01 123 need: time merit 2018-01-01 345 2018-04-01 123 2018-07-01 0 2018-10-01 0 simplify example real use case agree would eg. 5 minute instead quarter group would least one attribute like group attributed, time desire result time merit attributed 2018-01-01 345 1 2018-01-01 345 2 2018-04-01 123 1 2018-04-01 123 2 2018-07-01 0 1 2018-07-01 0 2 2018-10-01 0 1 2018-10-01 0 2 way somehow fill whole given interval? like influx fill argument group timescaledb' time_bucket() function generate_series() try search clickhous document github issue seem implement yet question perhaps whether there' workaround."
52186125,Moving Wordpress site to Docker: Error establishing DB connection,"Ive been making new sites with Wordpress &amp; Docker recently and have a reasonable grasp of how it all works and Im now looking to move some established sites into Docker.
Ive been following this guide:
https://stephenafamo.com/blog/moving-wordpress-docker-container/
I have everything setup as it should be but when I go to my domain.com:1234 I get the error message 'Error establishing a database connection'. I have changed 'DB HOST' to 'mysql' in wp-config.php as advised and all the DB details from the site Im bringing in are correct.
I have attached to the mysql container and checked that the db is there and with the right user and also made sure the pw is correct via mysql CLI too.
SELinux is set to permissive and I havent changed any dir/file ownership nor permissions and for the latter dirs are all 755 and files 644 as they should be.
Edit: I should mention that database/data and everything under that seem to be owned by user/group 'polkitd input' instead of root. 
Docker logs aren't really telling me much either apart from the 500 error messages for the WP container when I browse the site on port 1234 (as expected though).
This is the docker-compose file:
version: '2'
services:
  example_db:
    image: mysql:latest
    container_name: example_db
    volumes:
      - ./database/data:/var/lib/mysql
      - ./database/initdb.d:/docker-entrypoint-initdb.d
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password123 # any random string will do
      MYSQL_DATABASE: mydomin_db # the name of your mysql database
      MYSQL_USER: my domain_me # the name of the database user
      MYSQL_PASSWORD: password123 # the password of the mysql user
  example:
    depends_on:
      - example_db
    image: wordpress:php7.1 # we're using the image with php7.1
    container_name: example
    ports:
      - ""1234:80""
    restart: always
    links:
      - example_db:mysql
    volumes:
      - ./src:/var/www/html
Suggestions most welcome as Im out of ideas!
",<mysql><wordpress><docker><docker-compose>,1991,2,28,377,2,8,17,71,34865,0.0,21,8,12,2018-09-05 13:17,2018-09-05 13:59,2018-09-05 13:59,0.0,0.0,Basic,3,"<mysql><wordpress><docker><docker-compose>, Moving Wordpress site to Docker: Error establishing DB connection, Ive been making new sites with Wordpress &amp; Docker recently and have a reasonable grasp of how it all works and Im now looking to move some established sites into Docker.
Ive been following this guide:
https://stephenafamo.com/blog/moving-wordpress-docker-container/
I have everything setup as it should be but when I go to my domain.com:1234 I get the error message 'Error establishing a database connection'. I have changed 'DB HOST' to 'mysql' in wp-config.php as advised and all the DB details from the site Im bringing in are correct.
I have attached to the mysql container and checked that the db is there and with the right user and also made sure the pw is correct via mysql CLI too.
SELinux is set to permissive and I havent changed any dir/file ownership nor permissions and for the latter dirs are all 755 and files 644 as they should be.
Edit: I should mention that database/data and everything under that seem to be owned by user/group 'polkitd input' instead of root. 
Docker logs aren't really telling me much either apart from the 500 error messages for the WP container when I browse the site on port 1234 (as expected though).
This is the docker-compose file:
version: '2'
services:
  example_db:
    image: mysql:latest
    container_name: example_db
    volumes:
      - ./database/data:/var/lib/mysql
      - ./database/initdb.d:/docker-entrypoint-initdb.d
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password123 # any random string will do
      MYSQL_DATABASE: mydomin_db # the name of your mysql database
      MYSQL_USER: my domain_me # the name of the database user
      MYSQL_PASSWORD: password123 # the password of the mysql user
  example:
    depends_on:
      - example_db
    image: wordpress:php7.1 # we're using the image with php7.1
    container_name: example
    ports:
      - ""1234:80""
    restart: always
    links:
      - example_db:mysql
    volumes:
      - ./src:/var/www/html
Suggestions most welcome as Im out of ideas!
","<myself><wordpress><doctor><doctor-compose>, move wordpress site doctor: error establish do connection, give make new site wordpress &amp; doctor recent reason grasp work in look move establish site doctor. give follow guide: http://stephenafamo.com/blow/moving-wordpress-doctor-container/ every set go domain.com:1234 get error message 'error establish database connection'. change 'do host' 'myself' up-confirm.pp advise do detail site in bring correct. attach myself contain check do right user also made sure pp correct via myself coli too. selinux set permits haven change did/fig ownership permits latter did 755 file 644 be. edit: mention database/data every seem own user/group 'polite input' instead root. doctor log really tell much either apart 500 error message up contain brows site port 1234 (a expect though). doctor-compose file: version: '2' services: example_db: image: myself:latest container_name: example_db volumes: - ./database/data:/war/limb/myself - ./database/initdb.d:/doctor-entrypoint-initdb.d start: away environment: mysql_root_password: password123 # random string mysql_database: mydomin_db # name myself database mysql_user: domain # name database user mysql_password: password123 # password myself user example: depends_on: - example_db image: wordpress:pp.1 # we'r use image pp.1 container_name: example ports: - ""1234:80"" start: away links: - example_db:myself volumes: - ./sac:/war/www/html suggest welcome in ideas!"
58735389,Pyspark SQL Pandas Grouped Map without GroupBy?,"I have a dataset that I want to map over using several Pyspark SQL Grouped Map UDFs, at different stages of a larger ETL process that runs on ephemeral clusters in AWS EMR. The Grouped Map API requires that the Pyspark dataframe be grouped prior to the apply, but I have no need to actually group keys.
At the moment, I'm using an arbitrary grouping, which works, but results in:
An unnecessary shuffle.
Hacky code for an arbitrary groupby in each job.
My ideal solution allows a vectorized Pandas UDF apply without an arbitrary grouping, but if I could save the arbitrary grouping that would at least eliminate the shuffles.
EDIT:
Here's what my code looks like. I was originally using an arbitrary grouping, but am currently trying spark_partition_id() based on a comment below by @pault.
@pandas_udf(b_schema, PandasUDFType.GROUPED_MAP)
def transform(a_partition):
  b = a_partition.drop(""pid"", axis=1)
  # Some other transform stuff
  return b
(sql
  .read.parquet(a_path)
  .withColumn(""pid"", spark_partition_id())
  .groupBy(""pid"")
  .apply(transform)
  .write.parquet(b_path))
Using spark_partition_id() seems to still result in a shuffle. I get the following DAG:
Stage 1
Scan parquet
Project
Project
Exchange
Stage 2
Exchange
Sort
FlatMapGroupsInPandas
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,1262,1,14,1666,0,14,28,79,1863,0.0,162,1,12,2019-11-06 17:16,2020-02-11 15:32,2020-02-11 15:32,97.0,97.0,Intermediate,19,"<python><pandas><apache-spark><pyspark><apache-spark-sql>, Pyspark SQL Pandas Grouped Map without GroupBy?, I have a dataset that I want to map over using several Pyspark SQL Grouped Map UDFs, at different stages of a larger ETL process that runs on ephemeral clusters in AWS EMR. The Grouped Map API requires that the Pyspark dataframe be grouped prior to the apply, but I have no need to actually group keys.
At the moment, I'm using an arbitrary grouping, which works, but results in:
An unnecessary shuffle.
Hacky code for an arbitrary groupby in each job.
My ideal solution allows a vectorized Pandas UDF apply without an arbitrary grouping, but if I could save the arbitrary grouping that would at least eliminate the shuffles.
EDIT:
Here's what my code looks like. I was originally using an arbitrary grouping, but am currently trying spark_partition_id() based on a comment below by @pault.
@pandas_udf(b_schema, PandasUDFType.GROUPED_MAP)
def transform(a_partition):
  b = a_partition.drop(""pid"", axis=1)
  # Some other transform stuff
  return b
(sql
  .read.parquet(a_path)
  .withColumn(""pid"", spark_partition_id())
  .groupBy(""pid"")
  .apply(transform)
  .write.parquet(b_path))
Using spark_partition_id() seems to still result in a shuffle. I get the following DAG:
Stage 1
Scan parquet
Project
Project
Exchange
Stage 2
Exchange
Sort
FlatMapGroupsInPandas
","<patron><hands><apache-spark><spark><apache-spark-sal>, spark sal and group map without group?, dataset want map use never spark sal group map us, differ stage larger etc process run schemer cluster a mr. group map apt require spark datafram group prior apply, need actual group keys. moment, i'm use arbitrary grouping, works, result in: unnecessary scuffle. hack code arbitrary group job. ideal slut allow vector and utf apply without arbitrary grouping, could save arbitrary group would least limit snuffles. edit: here' code look like. origin use arbitrary grouping, current try spark_partition_id() base comment @fault. @pandas_udf(b_schema, pandasudftype.grouped_map) def transform(partition): b = partition.drop(""did"", axis=1) # transform stuff return b (sal .read.parquet(path) .withcolumn(""did"", spark_partition_id()) .group(""did"") .apply(transform) .write.parquet(path)) use spark_partition_id() seem still result scuffle. get follow day: stage 1 scan parquet project project exchange stage 2 exchange sort flatmapgroupsinpanda"
61419449,Unable to Instantiate Python Dataclass (Frozen) inside a Pytest function that uses Fixtures,"I'm following along with Architecture Patterns in Python by Harry Percival and Bob Gregory.
Around chapter three (3) they introduce testing the ORM of SQLAlchemy.
A new test that requires a session fixture, it is throwing AttributeError, FrozenInstanceError due to cannot assign to field '_sa_instance_state'
It may be important to note that other tests do not fail when creating instances of OrderLine, but they do fail if I simply include session into the test parameter(s).
Anyway I'll get straight into the code.
conftest.py
@pytest.fixture
def local_db():
    engine = create_engine('sqlite:///:memory:')
    metadata.create_all(engine)
    return engine
@pytest.fixture
def session(local_db):
    start_mappers()
    yield sessionmaker(bind=local_db)()
    clear_mappers()
model.py
@dataclass(frozen=True)
class OrderLine:
    id: str
    sku: str
    quantity: int
test_orm.py
def test_orderline_mapper_can_load_lines(session):
    session.execute(
        'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
        '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
        '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
        '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
    )
    expected = [
        model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
        model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
        model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
    ]
    assert session.query(model.OrderLine).all() == expected
Console error for pipenv run pytest test_orm.py
============================= test session starts =============================
platform linux -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/[redacted]/Documents/architecture-patterns-python
collected 1 item                                                              
test_orm.py F                                                           [100%]
================================== FAILURES ===================================
____________________ test_orderline_mapper_can_load_lines _____________________
session = &lt;sqlalchemy.orm.session.Session object at 0x7fd919ac5bd0&gt;
    def test_orderline_mapper_can_load_lines(session):
        session.execute(
            'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
            '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
            '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
            '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
        )
        expected = [
&gt;           model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
            model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
            model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
        ]
test_orm.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
&lt;string&gt;:2: in __init__
    ???
../../.local/share/virtualenvs/architecture-patterns-python-Qi2y0bev/lib64/python3.7/site-packages/sqlalchemy/orm/instrumentation.py:377: in _new_state_if_none
    self._state_setter(instance, state)
&lt;string&gt;:1: in set
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;[AttributeError(&quot;'OrderLine' object has no attribute '_sa_instance_state'&quot;) raised in repr()] OrderLine object at 0x7fd919a8cf50&gt;
name = '_sa_instance_state'
value = &lt;sqlalchemy.orm.state.InstanceState object at 0x7fd9198f7490&gt;
&gt;   ???
E   dataclasses.FrozenInstanceError: cannot assign to field '_sa_instance_state'
&lt;string&gt;:4: FrozenInstanceError
=========================== short test summary info ===========================
FAILED test_orm.py::test_orderline_mapper_can_load_lines - dataclasses.Froze...
============================== 1 failed in 0.06s ==============================
Additional Questions
I understand the overlying logic and what these files are doing, but correct my if my rudimentary understanding is lacking.
conftest.py (used for all pytest config) is setting up a session fixture, which basically sets up a temporary database in memory - using start and clear mappers to ensure that the orm model definitions are binding to the db isntance.
model.py simply a dataclass used to represent an atomic OrderLine object.
test_orm.py class for pytest to supply the session fixture, in order to setup, execute, teardown a db explicitly for the purpose of running tests.
Issue resolution provided by https://github.com/cosmicpython/code/issues/17
",<python><sqlalchemy><pytest><python-dataclasses>,4540,2,89,169,0,0,9,65,2442,0.0,20,2,12,2020-04-25 0:23,2021-04-04 13:22,2021-04-04 13:22,344.0,344.0,Advanced,41,"<python><sqlalchemy><pytest><python-dataclasses>, Unable to Instantiate Python Dataclass (Frozen) inside a Pytest function that uses Fixtures, I'm following along with Architecture Patterns in Python by Harry Percival and Bob Gregory.
Around chapter three (3) they introduce testing the ORM of SQLAlchemy.
A new test that requires a session fixture, it is throwing AttributeError, FrozenInstanceError due to cannot assign to field '_sa_instance_state'
It may be important to note that other tests do not fail when creating instances of OrderLine, but they do fail if I simply include session into the test parameter(s).
Anyway I'll get straight into the code.
conftest.py
@pytest.fixture
def local_db():
    engine = create_engine('sqlite:///:memory:')
    metadata.create_all(engine)
    return engine
@pytest.fixture
def session(local_db):
    start_mappers()
    yield sessionmaker(bind=local_db)()
    clear_mappers()
model.py
@dataclass(frozen=True)
class OrderLine:
    id: str
    sku: str
    quantity: int
test_orm.py
def test_orderline_mapper_can_load_lines(session):
    session.execute(
        'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
        '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
        '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
        '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
    )
    expected = [
        model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
        model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
        model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
    ]
    assert session.query(model.OrderLine).all() == expected
Console error for pipenv run pytest test_orm.py
============================= test session starts =============================
platform linux -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/[redacted]/Documents/architecture-patterns-python
collected 1 item                                                              
test_orm.py F                                                           [100%]
================================== FAILURES ===================================
____________________ test_orderline_mapper_can_load_lines _____________________
session = &lt;sqlalchemy.orm.session.Session object at 0x7fd919ac5bd0&gt;
    def test_orderline_mapper_can_load_lines(session):
        session.execute(
            'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
            '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
            '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
            '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
        )
        expected = [
&gt;           model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
            model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
            model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
        ]
test_orm.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
&lt;string&gt;:2: in __init__
    ???
../../.local/share/virtualenvs/architecture-patterns-python-Qi2y0bev/lib64/python3.7/site-packages/sqlalchemy/orm/instrumentation.py:377: in _new_state_if_none
    self._state_setter(instance, state)
&lt;string&gt;:1: in set
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;[AttributeError(&quot;'OrderLine' object has no attribute '_sa_instance_state'&quot;) raised in repr()] OrderLine object at 0x7fd919a8cf50&gt;
name = '_sa_instance_state'
value = &lt;sqlalchemy.orm.state.InstanceState object at 0x7fd9198f7490&gt;
&gt;   ???
E   dataclasses.FrozenInstanceError: cannot assign to field '_sa_instance_state'
&lt;string&gt;:4: FrozenInstanceError
=========================== short test summary info ===========================
FAILED test_orm.py::test_orderline_mapper_can_load_lines - dataclasses.Froze...
============================== 1 failed in 0.06s ==============================
Additional Questions
I understand the overlying logic and what these files are doing, but correct my if my rudimentary understanding is lacking.
conftest.py (used for all pytest config) is setting up a session fixture, which basically sets up a temporary database in memory - using start and clear mappers to ensure that the orm model definitions are binding to the db isntance.
model.py simply a dataclass used to represent an atomic OrderLine object.
test_orm.py class for pytest to supply the session fixture, in order to setup, execute, teardown a db explicitly for the purpose of running tests.
Issue resolution provided by https://github.com/cosmicpython/code/issues/17
","<patron><sqlalchemy><test><patron-dataclasses>, unable instant patron dataclass (frozen) inside test function use fixture, i'm follow along architecture pattern patron harris peri bob gregory. around chapter three (3) introduce test or sqlalchemy. new test require session fixture, throw attributeerror, frozeninstanceerror due cannot assign field '_sa_instance_state' may import note test fail great instant orderlies, fail simple include session test parameter(s). anyway i'll get straight code. contest.i @test.fixture def local_db(): engine = create_engine('quite:///:memory:') metadata.create_all(engine) return engine @test.fixture def session(local_db): start_mappers() yield sessionmaker(bind=local_db)() clear_mappers() model.i @dataclass(frozen=true) class orderlies: id: sir sky: sir quantity: in test_orm.i def test_orderline_mapper_can_load_lines(session): session.execute( 'insert order_lin (ordered, sky, quantity) value ' '(&quit;order&quit;, &quit;red-chair&quit;, 12),' '(&quit;order&quit;, &quit;red-table&quit;, 13),' '(&quit;order&quit;, &quit;blue-lipstick&quit;, 14)' ) expect = [ model.orderlies(&quit;order&quit;, &quit;red-chair&quit;, 12), model.orderlies(&quit;order&quit;, &quit;red-table&quit;, 13), model.orderlies(&quit;order&quit;, &quit;blue-lipstick&quit;, 14), ] assert session.query(model.orderlies).all() == expect console error pipe run test test_orm.i ============================= test session start ============================= platform line -- patron 3.7.6, test-5.4.1, by-1.8.1, plunge-0.13.1 rootdir: /home/[reacted]/documents/architecture-patterns-patron collect 1 item test_orm.i f [100%] ================================== failure =================================== ____________________ test_orderline_mapper_can_load_lin _____________________ session = &it;sqlalchemy.or.session.less object 0x7fd919ac5bd0&it; def test_orderline_mapper_can_load_lines(session): session.execute( 'insert order_lin (ordered, sky, quantity) value ' '(&quit;order&quit;, &quit;red-chair&quit;, 12),' '(&quit;order&quit;, &quit;red-table&quit;, 13),' '(&quit;order&quit;, &quit;blue-lipstick&quit;, 14)' ) expect = [ &it; model.orderlies(&quit;order&quit;, &quit;red-chair&quit;, 12), model.orderlies(&quit;order&quit;, &quit;red-table&quit;, 13), model.orderlies(&quit;order&quit;, &quit;blue-lipstick&quit;, 14), ] test_orm.by:13: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ &it;string&it;:2: __init__ ??? ../../.local/share/virtualenvs/architecture-patterns-patron-qi2y0bev/lib64/python3.7/site-packages/sqlalchemy/or/instrumentation.by:377: _new_state_if_non self._state_setter(instance, state) &it;string&it;:1: set ??? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = &it;[attributeerror(&quit;'orderlies' object attribute '_sa_instance_state'&quit;) rays rear()] orderly object 0x7fd919a8cf50&it; name = '_sa_instance_state' value = &it;sqlalchemy.or.state.instances object 0x7fd9198f7490&it; &it; ??? e dataclasses.frozeninstanceerror: cannot assign field '_sa_instance_state' &it;string&it;:4: frozeninstanceerror =========================== short test summary into =========================== fail test_orm.by::test_orderline_mapper_can_load_lin - dataclasses.froze... ============================== 1 fail 0.06 ============================== admit question understand overlie logic file doing, correct rudimentari understand lacking. contest.i (use test confirm) set session fixture, basic set temporary database memory - use start clear mapped ensue or model definite bind do instance. model.i simple dataclass use repress atom orderly object. test_orm.i class test supply session fixture, order set, execute, tearworn do explicitly purpose run tests. issue resolute proved http://github.com/cosmicpython/code/issues/17"
