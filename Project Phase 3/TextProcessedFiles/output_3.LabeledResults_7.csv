QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
48481028,Building SQL Server Database Project In Ubuntu,"I'm building an ASP.NET Core 2.0 Web API application that is hosted in an Ubuntu environment. So far, I've had great success getting things building and running (for the .NET Core app) in Ubuntu.
For the database, I have a SqlProj included in my solution. The project includes typical things such as tables, SPs, and pre/post deployment scripts. I'm using the following command (on my Windows-based dev machine) to build and deploy this project:
msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
When I take this approach, everything builds and deploys properly; however, since I will be taking advantage of the .NET Core CLI commands + CI/CD that targets an Ubuntu environment, I'd like to do something more like:
dotnet msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
In Windows, I immediately get the error:
error MSB4019: The imported project ""C:\Program Files\dotnet\sdk\2.1.4\Microsoft\VisualStudio\v11.0\SSDT\Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
Basically, what I'm asking is how to successfully build and deploy a SqlProj project in an Ubuntu environment. I've tried Googling, but I have had zero luck thus far. All of the similar issues that I've found were for individuals who were editing their .proj file to target their VS folder's SSDT. All of these individuals were fixing the issue in Windows. This approach will not work in Ubuntu, since the targets file uses Windows registry keys.
EDIT: I'm aware that SSDT is needed in order to perform such a deployment using MSBuild. I've found no evidence that installing/using SSDT is even possible in Ubuntu. If it is not, perhaps there is an alternative solution?
FYI, I'm aware that using a code-first approach with EF Core is possible. I'm attempting to take the raw SP approach (along with leveraging indexes) and keep track of all of my code using SqlProj instead. This will all be stored and CI/CDed from a Git repo.
",<sql-server><ubuntu><msbuild><.net-core><sql-server-data-tools>,2151,0,3,669,0,5,21,62,5222,0.0,146,7,23,2018-01-27 22:05,2018-01-28 18:10,2020-04-28 18:14,1.0,822.0,Intermediate,17,"<sql-server><ubuntu><msbuild><.net-core><sql-server-data-tools>, Building SQL Server Database Project In Ubuntu, I'm building an ASP.NET Core 2.0 Web API application that is hosted in an Ubuntu environment. So far, I've had great success getting things building and running (for the .NET Core app) in Ubuntu.
For the database, I have a SqlProj included in my solution. The project includes typical things such as tables, SPs, and pre/post deployment scripts. I'm using the following command (on my Windows-based dev machine) to build and deploy this project:
msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
When I take this approach, everything builds and deploys properly; however, since I will be taking advantage of the .NET Core CLI commands + CI/CD that targets an Ubuntu environment, I'd like to do something more like:
dotnet msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
In Windows, I immediately get the error:
error MSB4019: The imported project ""C:\Program Files\dotnet\sdk\2.1.4\Microsoft\VisualStudio\v11.0\SSDT\Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
Basically, what I'm asking is how to successfully build and deploy a SqlProj project in an Ubuntu environment. I've tried Googling, but I have had zero luck thus far. All of the similar issues that I've found were for individuals who were editing their .proj file to target their VS folder's SSDT. All of these individuals were fixing the issue in Windows. This approach will not work in Ubuntu, since the targets file uses Windows registry keys.
EDIT: I'm aware that SSDT is needed in order to perform such a deployment using MSBuild. I've found no evidence that installing/using SSDT is even possible in Ubuntu. If it is not, perhaps there is an alternative solution?
FYI, I'm aware that using a code-first approach with EF Core is possible. I'm attempting to take the raw SP approach (along with leveraging indexes) and keep track of all of my code using SqlProj instead. This will all be stored and CI/CDed from a Git repo.
","<sal-server><bunt><build><.net-core><sal-server-data-tools>, build sal server database project bunt, i'm build asp.net core 2.0 web apt applied host bunt environment. far, i'v great success get thing build run (for .net core pp) bunt. database, sqlproj include solution. project include topic thing tables, spy, pre/post deploy script. i'm use follow command (on windows-was de machine) build deploy project: build .\project.do.sqlproj /t:build /t:publish /p:sqlpublishprofilepath=""./publishprofiles/project.do.publish.all"" take approach, every build deploy properly; however, since take advantage .net core coli command + i/d target bunt environment, i'd like cometh like: done build .\project.do.sqlproj /t:build /t:publish /p:sqlpublishprofilepath=""./publishprofiles/project.do.publish.all"" windows, dimmed get error: error msb4019: import project ""c:\program files\done\sd\2.1.4\microsoft\visualstudio\ve.0\side\microsoft.data.tools.scheme.sqltasks.target"" found. confirm path &it;import&it; declare correct, file exist disk. basically, i'm ask success build deploy sqlproj project bunt environment. i'v try cooling, zero luck the far. similar issue i'v found individu edit .pro file target vs older' side. individu fix issue windows. approach work bunt, since target file use window registry keys. edit: i'm war side need order perform deploy use build. i'v found evil installing/us side even possible bunt. not, perhaps alter solution? foi, i'm war use code-first approach of core possible. i'm attempt take raw s approach (along several indexes) keep track code use sqlproj instead. store i/d git rep."
50833992,Postgresql | No space left on device,"I am getting space issue while running a batch process on PostgreSQL database.
However, df -h command shows that machine has enough space
below is the exact error 
org.springframework.dao.DataAccessResourceFailureException: PreparedStatementCallback; SQL [INSERT into BATCH_JOB_INSTANCE(JOB_INSTANCE_ID, JOB_NAME, JOB_KEY, VERSION) values (?, ?, ?, ?)]; ERROR: could not extend file ""base/16388/16452"": No space left on device
  Hint: Check free disk space.
What is causing this issue? 
EDIT
postgres data directory is /var/opt/rh/rh-postgresql96/lib/pgsql/data
df -h /var/opt/rh/rh-postgresql96/lib/pgsql/data
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda2      100G   63G   38G  63% /
",<postgresql><database-administration><postgresql-9.6>,700,1,7,1636,2,22,44,68,67654,0.0,105,1,23,2018-06-13 9:40,2018-06-13 11:22,2018-06-13 11:22,0.0,0.0,Advanced,33,"<postgresql><database-administration><postgresql-9.6>, Postgresql | No space left on device, I am getting space issue while running a batch process on PostgreSQL database.
However, df -h command shows that machine has enough space
below is the exact error 
org.springframework.dao.DataAccessResourceFailureException: PreparedStatementCallback; SQL [INSERT into BATCH_JOB_INSTANCE(JOB_INSTANCE_ID, JOB_NAME, JOB_KEY, VERSION) values (?, ?, ?, ?)]; ERROR: could not extend file ""base/16388/16452"": No space left on device
  Hint: Check free disk space.
What is causing this issue? 
EDIT
postgres data directory is /var/opt/rh/rh-postgresql96/lib/pgsql/data
df -h /var/opt/rh/rh-postgresql96/lib/pgsql/data
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda2      100G   63G   38G  63% /
","<postgresql><database-administration><postgresql-9.6>, postgresql | space left device, get space issue run batch process postgresql database. however, of -h command show machine enough space exact error org.springframework.do.dataaccessresourcefailureexception: preparedstatementcallback; sal [insert batch_job_instance(job_instance_id, job_name, jockey, version) value (?, ?, ?, ?)]; error: could extend file ""base/16388/16452"": space left devil hint: check free disk space. cause issue? edit poster data director /war/opt/oh/oh-postgresql96/limb/pgsql/data of -h /war/opt/oh/oh-postgresql96/limb/pgsql/data filesystem size use avail use% mount /de/xvda2 100g fig fig 63% /"
50813493,NameError: name 'dbutils' is not defined in pyspark,"I am running a pyspark job in databricks cloud. I need to write some of the csv files to databricks filesystem (dbfs) as part of this job and also i need to use some of the dbutils native commands like,
#mount azure blob to dbfs location
dbutils.fs.mount (source=""..."",mount_point=""/mnt/..."",extra_configs=""{key:value}"")
I am also trying to unmount once the files has been written to the mount directory. But, when i am using dbutils directly in the pyspark job it is failing with 
NameError: name 'dbutils' is not defined
Should i import any of the package to use dbutils in pyspark code ? Thanks in advance.
",<apache-spark-sql><azure-blob-storage><databricks>,610,0,3,1079,5,12,18,37,25162,0.0,73,3,23,2018-06-12 9:16,2018-10-23 11:17,,133.0,,Basic,12,"<apache-spark-sql><azure-blob-storage><databricks>, NameError: name 'dbutils' is not defined in pyspark, I am running a pyspark job in databricks cloud. I need to write some of the csv files to databricks filesystem (dbfs) as part of this job and also i need to use some of the dbutils native commands like,
#mount azure blob to dbfs location
dbutils.fs.mount (source=""..."",mount_point=""/mnt/..."",extra_configs=""{key:value}"")
I am also trying to unmount once the files has been written to the mount directory. But, when i am using dbutils directly in the pyspark job it is failing with 
NameError: name 'dbutils' is not defined
Should i import any of the package to use dbutils in pyspark code ? Thanks in advance.
","<apache-spark-sal><azure-blow-storage><databricks>, nameerror: name 'duties' define spark, run spark job databrick cloud. need write is file databrick filesystem (days) part job also need use dbutil native command like, #mount azur blow def local duties.is.mount (source=""..."",mount_point=""/met/..."",extra_configs=""{key:value}"") also try amount file written mount directory. but, use dbutil directly spark job fail nameerror: name 'duties' define import package use dbutil spark code ? thank advance."
59850951,When to use float vs decimal,"I'm building this API, and the database will store values that represent one of the following:
percentage
average
rate
I honestly have no idea how to represent something that the range is between 0 and 100% in numbers. Should it be
0.00 - 1.00
0.00 - 100.00
any other alternative that I don't know
Is there a clear choice for that? A global way of representing on databases something that goes from 0 to 100% percent? Going further, what's the correct that type for it, float or decimal?
Thank you.
",<mysql><sql><types><floating-point><decimal>,499,0,0,1198,7,17,30,66,8111,0.0,33,7,23,2020-01-22 0:08,2020-01-22 0:20,,0.0,,Basic,8,"<mysql><sql><types><floating-point><decimal>, When to use float vs decimal, I'm building this API, and the database will store values that represent one of the following:
percentage
average
rate
I honestly have no idea how to represent something that the range is between 0 and 100% in numbers. Should it be
0.00 - 1.00
0.00 - 100.00
any other alternative that I don't know
Is there a clear choice for that? A global way of representing on databases something that goes from 0 to 100% percent? Going further, what's the correct that type for it, float or decimal?
Thank you.
","<myself><sal><types><floating-point><denial>, use float vs denial, i'm build apt, database store value repress one following: percentage average rate honestly idea repress cometh rang 0 100% numbers. 0.00 - 1.00 0.00 - 100.00 alter know clear choice that? global way repress database cometh go 0 100% percent? go further, what' correct type it, float denial? thank you."
49035178,Unable to locate System.Data.SqlClient reference,"I have a fresh Visual Studio 2017 Professional install. I'm building a quick POC Console application using .NET 4.7.1, and I'm unable to find the reference for System.Data.SqlClient.
I have scoured my system, and located 4 versions of System.Data.SqlClient.dll, but none are correct and won't compile.
I have also attempted to use System.Data, but no reference to SqlClient is located within. I have manually added the dll/reference for System.Data, but also did not resolve the reference issue.
My application is really simple at the moment, and it will NOT compile due to this missing reference.
What steps do I need to do to get this resolved?
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Database
    {
        public void Start()
        {
            string connString = @""server=(local);initial     catalog=MyDatabase;Integrated Security=SSPI;"";
            using (SqlConnection conn = new SqlConnection(connString))
            {
                conn.Open();
                using (SqlCommand cmd = new SqlCommand(""SELECT TOP 10 ID, Name FROM TableA"", conn))
                {
                    using (SqlDataReader reader = cmd.ExecuteReader())
                    {
                        while(reader.Read())
                        {
                            Console.WriteLine(""ID: [{0}], Name: [{1}]"", reader.GetValue(0), reader.GetValue(1));
                        }
                    }
                }
            }
        }
    }
}
",<c#><sqlclient>,1511,0,29,267,1,2,9,37,70492,,3,7,23,2018-02-28 17:17,2018-08-15 14:53,2019-03-06 20:52,168.0,371.0,Basic,9,"<c#><sqlclient>, Unable to locate System.Data.SqlClient reference, I have a fresh Visual Studio 2017 Professional install. I'm building a quick POC Console application using .NET 4.7.1, and I'm unable to find the reference for System.Data.SqlClient.
I have scoured my system, and located 4 versions of System.Data.SqlClient.dll, but none are correct and won't compile.
I have also attempted to use System.Data, but no reference to SqlClient is located within. I have manually added the dll/reference for System.Data, but also did not resolve the reference issue.
My application is really simple at the moment, and it will NOT compile due to this missing reference.
What steps do I need to do to get this resolved?
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Database
    {
        public void Start()
        {
            string connString = @""server=(local);initial     catalog=MyDatabase;Integrated Security=SSPI;"";
            using (SqlConnection conn = new SqlConnection(connString))
            {
                conn.Open();
                using (SqlCommand cmd = new SqlCommand(""SELECT TOP 10 ID, Name FROM TableA"", conn))
                {
                    using (SqlDataReader reader = cmd.ExecuteReader())
                    {
                        while(reader.Read())
                        {
                            Console.WriteLine(""ID: [{0}], Name: [{1}]"", reader.GetValue(0), reader.GetValue(1));
                        }
                    }
                }
            }
        }
    }
}
","<c#><sqlclient>, unable local system.data.sqlcli reference, fresh visual studio 2017 profession install. i'm build quick pot console applied use .net 4.7.1, i'm unable find refer system.data.sqlclient. scour system, local 4 version system.data.sqlclient.all, none correct compile. also attempt use system.data, refer sqlclient local within. manual ad all/refer system.data, also resolve refer issue. applied really simple moment, compel due miss reference. step need get resolved? use system; use system.data; use system.data.sqlclient; namespac consoleapp1 { class database { public void start() { string connstr = @""server=(local);into catalogue=database;inter security=ship;""; use (sqlconnect corn = new sqlconnection(connstring)) { corn.open(); use (sqlcommand cod = new sqlcommand(""select top 10 id, name table"", corn)) { use (sqldataread reader = cod.executereader()) { while(reader.read()) { console.writeline(""id: [{0}], name: [{1}]"", reader.getvalue(0), reader.getvalue(1)); } } } } } } }"
53249276,docker-compose mysql init sql is not executed,"I am trying to set up a mysql docker container and execute init sql script. Unfortunately the sql script is not executed. What am I doing wrong?
version: '3.3'
services:
  api:
    container_name: 'api'
    build: './api'
  ports:
    - target: 8080
      published: 8888
      protocol: tcp
      mode: host
  volumes:
    - './api:/go/src/app'
  depends_on:
    - 'mysql'
 mysql:
  image: 'mysql:latest'
  container_name: 'mysql'
  volumes:
    - ./db_data:/var/lib/mysql:rw
    - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  restart: always
  environment:
    MYSQL_USER: test
    MYSQL_PASSWORD: test
    MYSQL_ROOT_PASSWORD: test
    MYSQL_DATABASE: test
  ports:
    - '3306:3306'
volumes:
  db_data:
I execute file with docker-compose up -d --build
",<mysql><sql><database><docker><docker-compose>,773,0,31,479,1,4,12,44,21615,0.0,16,3,23,2018-11-11 13:35,2018-11-11 14:56,2018-11-12 2:32,0.0,1.0,Basic,9,"<mysql><sql><database><docker><docker-compose>, docker-compose mysql init sql is not executed, I am trying to set up a mysql docker container and execute init sql script. Unfortunately the sql script is not executed. What am I doing wrong?
version: '3.3'
services:
  api:
    container_name: 'api'
    build: './api'
  ports:
    - target: 8080
      published: 8888
      protocol: tcp
      mode: host
  volumes:
    - './api:/go/src/app'
  depends_on:
    - 'mysql'
 mysql:
  image: 'mysql:latest'
  container_name: 'mysql'
  volumes:
    - ./db_data:/var/lib/mysql:rw
    - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  restart: always
  environment:
    MYSQL_USER: test
    MYSQL_PASSWORD: test
    MYSQL_ROOT_PASSWORD: test
    MYSQL_DATABASE: test
  ports:
    - '3306:3306'
volumes:
  db_data:
I execute file with docker-compose up -d --build
","<myself><sal><database><doctor><doctor-compose>, doctor-compose myself knit sal executed, try set myself doctor contain execute knit sal script. unfortun sal script executed. wrong? version: '3.3' services: apt: container_name: 'apt' build: './apt' ports: - target: 8080 published: 8888 protocol: top mode: host volumes: - './apt:/go/sac/pp' depends_on: - 'myself' myself: image: 'myself:latest' container_name: 'myself' volumes: - ./db_data:/war/limb/myself:re - ./database/knit.sal:/doctor-entrypoint-initdb.d/knit.sal:to start: away environment: mysql_user: test mysql_password: test mysql_root_password: test mysql_database: test ports: - '3306:3306' volumes: db_data: execute file doctor-compose -d --build"
48967318,SQL Server Web vs Standard edition,"I have found out that there's two versions of SQL Server types that are very different in terms of pricing...
The Web version from my host provider costs about 13$ per 2 core packs, whereas the Standard edition is right around 200$.
From my standpoint, we expect our database to be around 150-200GB in size, only few tables would take up most of that space.
So my only concern is would the web version of SQL Server support this large database and not cause any performance issues to the end users?
How different is index rebuilding on Web and Standard version?
Can someone help me out with this?
",<sql><sql-server><performance><sql-server-2008><database-indexes>,597,0,0,3693,15,55,120,81,95448,0.0,406,1,23,2018-02-24 20:32,2018-02-24 21:28,2018-02-24 21:28,0.0,0.0,Basic,9,"<sql><sql-server><performance><sql-server-2008><database-indexes>, SQL Server Web vs Standard edition, I have found out that there's two versions of SQL Server types that are very different in terms of pricing...
The Web version from my host provider costs about 13$ per 2 core packs, whereas the Standard edition is right around 200$.
From my standpoint, we expect our database to be around 150-200GB in size, only few tables would take up most of that space.
So my only concern is would the web version of SQL Server support this large database and not cause any performance issues to the end users?
How different is index rebuilding on Web and Standard version?
Can someone help me out with this?
","<sal><sal-server><performance><sal-server-2008><database-indexes>, sal server web vs standard edition, found there' two version sal server type differ term pricking... web version host proved cost 13$ per 2 core packs, where standard edit right around 200$. standpoint, expect database around 150-200gb size, table would take space. concern would web version sal server support large database cause perform issue end users? differ index rebuild web standard version? someone help this?"
52148305,How to cascade delete document in mongodb?,"I have user and photo documents in Mongodb. Each photo belongs to user and a photo maybe shared among users. Lets say user1 has p1,p2,p3 photos and user2 has p3,p4,p5 photos. If I delete user1 (manually using tools like Compass), p1 and p2 should also be deleted but not p3. How to achieve this and what kind of database structure I need to define?
Currently if I delete user1, no photos are deleted and remain in databse which now makes the database corrupted from the point of view of the application using the database.
Its Spring Boot app and User and Photo are declared as:
import lombok.Builder;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.DBRef;
import org.springframework.data.mongodb.core.mapping.Document;
@Document
@Data
@Builder
public class User {
    @Id
    private String id;
    @DBRef
    private Set&lt;Photo&gt; photos;
    private String name;
}
@Document
@Data
@Builder
public class Photo {
    @Id
    private String id;
    private String fileName;
}
",<mongodb><spring-boot><nosql>,1051,0,33,11616,40,116,195,69,20278,0.0,416,4,23,2018-09-03 10:54,2018-09-11 0:04,2018-09-11 0:04,8.0,8.0,Basic,9,"<mongodb><spring-boot><nosql>, How to cascade delete document in mongodb?, I have user and photo documents in Mongodb. Each photo belongs to user and a photo maybe shared among users. Lets say user1 has p1,p2,p3 photos and user2 has p3,p4,p5 photos. If I delete user1 (manually using tools like Compass), p1 and p2 should also be deleted but not p3. How to achieve this and what kind of database structure I need to define?
Currently if I delete user1, no photos are deleted and remain in databse which now makes the database corrupted from the point of view of the application using the database.
Its Spring Boot app and User and Photo are declared as:
import lombok.Builder;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.DBRef;
import org.springframework.data.mongodb.core.mapping.Document;
@Document
@Data
@Builder
public class User {
    @Id
    private String id;
    @DBRef
    private Set&lt;Photo&gt; photos;
    private String name;
}
@Document
@Data
@Builder
public class Photo {
    @Id
    private String id;
    private String fileName;
}
","<mongodb><spring-boot><nose>, cascade delete document mongodb?, user photo document mongodb. photo belong user photo may share among users. let say user pp,pp,pp photo user pp,pp,pp photo. delete user (manual use tool like compass), pp pp also delete pp. achieve kind database structure need define? current delete user, photo delete remain data make database corrupt point view applied use database. spring boot pp user photo declare as: import look.builder; import look.data; import org.springframework.data.annexation.id; import org.springframework.data.mongodb.core.mapping.drew; import org.springframework.data.mongodb.core.mapping.document; @document @data @builder public class user { @id privat string id; @drew privat set&it;photo&it; photo; privat string name; } @document @data @builder public class photo { @id privat string id; privat string filename; }"
52048778,"OperationalError: cursor ""_django_curs_<id>"" does not exist","We have an online store web-app which is powered by django, postgresql and heroku. 
For a specific campaign (you can think a campaign like a product to purchase), we have sold 10k+ copies successfully. Yet some of our users are encountered this error according to our Sentry reports. Common specification of these users is; none of them have address information before the purchase. Generally, users fill out address form right after registering. If they don't, they need to fill the form while purchasing the product and submit them together. 
This is how the trace looks like:
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist
(66 additional frame(s) were not displayed)
...
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 31, in render_form
    return render_to_string('widgets/store_form_renderer.html', ctx)
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 20, in render_widget
    return render_to_string('widgets/store_widget_renderer.html', ctx)
  File ""store/apps/store_main/widgets.py"", line 40, in render
    attrs=attrs) + ""&lt;span class='js-select-support select-arrow'&gt;&lt;/span&gt;&lt;div class='js-select-support select-arrow-space'&gt;&lt;b&gt;&lt;/b&gt;&lt;/div&gt;""
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist 
So another weird common thing, there are exception messages between sql queries before the failure. You can see it in the image below: 
I'm adding it if they are somehow related. What may also be related is, the users who get this error are the users who tries to purchase the campaign right after a bulk mailing. So, extensive traffic might be the reason yet we are also not sure.
We asked Heroku about the problem since they are hosting the postgres, yet they do not have any clue either.
I know the formal reason of this error is trying to reach the cursor after a commit. Since it is destroyed after transaction, trying to reach it cause this error yet I don't see this in our scenario. We are not touching the cursor in any way. What am I missing? What may produce this error? How to prevent it? Any ideas would be appreciated.
",<python><django><postgresql><sentry><heroku-postgres>,2170,1,12,408,1,3,16,74,19009,0.0,69,4,23,2018-08-28 1:13,2019-04-29 11:11,,244.0,,Advanced,32,"<python><django><postgresql><sentry><heroku-postgres>, OperationalError: cursor ""_django_curs_<id>"" does not exist, We have an online store web-app which is powered by django, postgresql and heroku. 
For a specific campaign (you can think a campaign like a product to purchase), we have sold 10k+ copies successfully. Yet some of our users are encountered this error according to our Sentry reports. Common specification of these users is; none of them have address information before the purchase. Generally, users fill out address form right after registering. If they don't, they need to fill the form while purchasing the product and submit them together. 
This is how the trace looks like:
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist
(66 additional frame(s) were not displayed)
...
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 31, in render_form
    return render_to_string('widgets/store_form_renderer.html', ctx)
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 20, in render_widget
    return render_to_string('widgets/store_widget_renderer.html', ctx)
  File ""store/apps/store_main/widgets.py"", line 40, in render
    attrs=attrs) + ""&lt;span class='js-select-support select-arrow'&gt;&lt;/span&gt;&lt;div class='js-select-support select-arrow-space'&gt;&lt;b&gt;&lt;/b&gt;&lt;/div&gt;""
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist 
So another weird common thing, there are exception messages between sql queries before the failure. You can see it in the image below: 
I'm adding it if they are somehow related. What may also be related is, the users who get this error are the users who tries to purchase the campaign right after a bulk mailing. So, extensive traffic might be the reason yet we are also not sure.
We asked Heroku about the problem since they are hosting the postgres, yet they do not have any clue either.
I know the formal reason of this error is trying to reach the cursor after a commit. Since it is destroyed after transaction, trying to reach it cause this error yet I don't see this in our scenario. We are not touching the cursor in any way. What am I missing? What may produce this error? How to prevent it? Any ideas would be appreciated.
","<patron><django><postgresql><sentry><hero-postures>, operationalerror: curses ""_django_curs_<id>"" exist, online store web-pp power django, postgresql hero. specie campaign (you think campaign like product purchase), sold ask+ copy successfully. yet user count error accord sentry reports. common specie user is; none address inform purchase. generally, user fill address form right registering. don't, need fill form purchase product submit together. trace look like: operationalerror: curses ""_django_curs_140398688327424_146"" exist (66 admit frame(s) displayed) ... file ""store/apes/store_main/templatetags/store_form_filters.by"", line 31, render_form return render_to_string('widest/store_form_renderer.html', cox) file ""store/apes/store_main/templatetags/store_form_filters.by"", line 20, render_widget return render_to_string('widest/store_widget_renderer.html', cox) file ""store/apes/store_main/widest.by"", line 40, render matters=matters) + ""&it;span class='is-select-support select-arrow'&it;&it;/span&it;&it;did class='is-select-support select-arrow-space'&it;&it;b&it;&it;/b&it;&it;/did&it;"" operationalerror: curses ""_django_curs_140398688327424_146"" exist not weird common thing, except message sal query failure. see image below: i'm ad somehow related. may also relate is, user get error user try purchase campaign right bulk failing. so, extent traffic might reason yet also sure. ask hero problem since host postures, yet clue either. know formal reason error try reach curses commit. since destroy transaction, try reach cause error yet see scenario. touch curses way. missing? may produce error? prevent it? idea would appreciated."
53117988,sequelize select and include another table alias,"I'm using sequelize to acess a postgres database and I want to query for a city and for example include the ""Building"" table but I want to rename the output to ""buildings"" and return the http response  but I have this error:
  { SequelizeEagerLoadingError: building is associated to city using an alias. You'v
  e included an alias (buildings), but it does not match the alias defined in your a
  ssociation.
    City.findById(req.params.id,{
      include: [
        {
          model: Building, as: ""buildings""
        }
      ]
    }).then(city =&gt;{
      console.log(city.id);
         res.status(201).send(city);
    }) .catch(error =&gt; {
     console.log(error);
     res.status(400).send(error)
   });
city Model
            const models = require('../models2');
            module.exports = (sequelize, DataTypes) =&gt; {
              const City = sequelize.define('city', {
              name: { type: DataTypes.STRING, allowNull: false },
                status: { type: DataTypes.INTEGER, allowNull: false },
                latitude: { type: DataTypes.BIGINT, allowNull: false },
                longitude: { type: DataTypes.BIGINT, allowNull: false },
              }, { freezeTableName: true});
              City.associate = function(models) {
                // associations can be defined here
                 City.hasMany(models.building,{as: 'building', foreignKey: 'cityId'})
              };
              return City;
            };
",<node.js><postgresql><sequelize.js><sequelize-cli><sequelize-typescript>,1461,0,28,1835,4,28,54,60,58794,0.0,84,1,23,2018-11-02 11:44,2018-11-02 13:00,2018-11-02 13:00,0.0,0.0,Advanced,37,"<node.js><postgresql><sequelize.js><sequelize-cli><sequelize-typescript>, sequelize select and include another table alias, I'm using sequelize to acess a postgres database and I want to query for a city and for example include the ""Building"" table but I want to rename the output to ""buildings"" and return the http response  but I have this error:
  { SequelizeEagerLoadingError: building is associated to city using an alias. You'v
  e included an alias (buildings), but it does not match the alias defined in your a
  ssociation.
    City.findById(req.params.id,{
      include: [
        {
          model: Building, as: ""buildings""
        }
      ]
    }).then(city =&gt;{
      console.log(city.id);
         res.status(201).send(city);
    }) .catch(error =&gt; {
     console.log(error);
     res.status(400).send(error)
   });
city Model
            const models = require('../models2');
            module.exports = (sequelize, DataTypes) =&gt; {
              const City = sequelize.define('city', {
              name: { type: DataTypes.STRING, allowNull: false },
                status: { type: DataTypes.INTEGER, allowNull: false },
                latitude: { type: DataTypes.BIGINT, allowNull: false },
                longitude: { type: DataTypes.BIGINT, allowNull: false },
              }, { freezeTableName: true});
              City.associate = function(models) {
                // associations can be defined here
                 City.hasMany(models.building,{as: 'building', foreignKey: 'cityId'})
              };
              return City;
            };
","<node.is><postgresql><sequelae.is><sequelae-coli><sequelae-typescript>, sequel select include not table alias, i'm use sequel access poster database want query city example include ""building"" table want renal output ""buildings"" return http response error: { sequelizeeagerloadingerror: build cassock city use alias. you'v e include asia (buildings), match asia define association. city.findbyid(red.parts.id,{ include: [ { model: building, as: ""buildings"" } ] }).then(city =&it;{ console.log(city.id); yes.status(201).send(city); }) .catch(error =&it; { console.log(error); yes.status(400).send(error) }); city model cost model = require('../models'); module.export = (sequelae, datatypes) =&it; { cost city = sequelae.define('city', { name: { type: datatypes.string, allownull: fall }, status: { type: datatypes.inter, allownull: fall }, latitude: { type: datatypes.begin, allownull: fall }, longitude: { type: datatypes.begin, allownull: fall }, }, { freezetablename: true}); city.cassock = function(models) { // cassock define city.harmony(models.building,{as: 'building', foreigner: 'city'}) }; return city; };"
52081473,Aggregate Overlapping Segments to Measure Effective Length,"I have a road_events table:
create table road_events (
    event_id number(4,0),
    road_id number(4,0),
    year number(4,0),
    from_meas number(10,2),
    to_meas number(10,2),
    total_road_length number(10,2)
    );
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (1,1,2020,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (2,1,2000,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (3,1,1980,0,25,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (4,1,1960,75,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (5,1,1940,1,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (6,2,2000,10,30,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (7,2,1975,30,60,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (8,2,1950,50,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (9,3,2050,40,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (10,4,2040,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (11,4,2013,0,199,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (12,4,2001,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (13,5,1985,50,70,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (14,5,1985,10,50,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (15,5,1965,1,301,300);
commit;
select * from road_events;
  EVENT_ID    ROAD_ID       YEAR  FROM_MEAS    TO_MEAS TOTAL_ROAD_LENGTH
---------- ---------- ---------- ---------- ---------- -----------------
         1          1       2020         25         50               100
         2          1       2000         25         50               100
         3          1       1980          0         25               100
         4          1       1960         75        100               100
         5          1       1940          1        100               100
         6          2       2000         10         30               100
         7          2       1975         30         60               100
         8          2       1950         50         90               100
         9          3       2050         40         90               100
        10          4       2040          0        200               200
        11          4       2013          0        199               200
        12          4       2001          0        200               200
        13          5       1985         50         70               300
        14          5       1985         10         50               300
        15          5       1965          1        301               300
I want to select the events that represent the most recent work on each road.
This is a tricky operation, because the events often pertain to only a portion of the road. This means that I can't simply select the most recent event per road; I need to only select the most recent event mileage that doesn't overlap.
Possible logic (in order):
I'm reluctant to guess at how this problem could be solved, because it could end up hurting more than it helps (kind of like the XY Problem). On the other hand, it might provide insight into the nature of the problem, so here it goes:
Select the most recent event for each road. We'll call the most recent event: event A.
If event A  is &gt;= total_road_length, then that's all I need. The algorithm ends here.
Else, get the next chronological event (event B) that does not have the same extents as event A. 
If the extents of event B overlap the extents of event A, then only get the portion(s) of event B that do not overlap. 
Repeat steps 3 and 4 until the total event length is = total_road_length. Or stop when there are no more events for that road.
Question:
I know it's a tall order, but what would it take to do this?
This is a classic linear referencing problem. It would be extremely helpful if I could do linear referencing operations as part of queries.
The result would be:
  EVENT_ID    ROAD_ID       YEAR  TOTAL_ROAD_LENGTH   EVENT_LENGTH
---------- ---------- ----------  -----------------   ------------
         1          1       2020                100             25
         3          1       1980                100             25
         4          1       1960                100             25
         5          1       1940                100             25
         6          2       2000                100             20
         7          2       1975                100             30
         8          2       1950                100             30
         9          3       2050                100             50
        10          4       2040                200            200
        13          5       1985                300             20
        14          5       1985                300             40
        15          5       1965                300            240
Related question: Select where number range does not overlap 
",<sql><oracle><select><oracle12c><asset-management>,5605,2,80,296,1,18,66,48,764,0.0,683,6,22,2018-08-29 15:46,2018-08-29 23:44,2018-08-30 21:05,0.0,1.0,Intermediate,17,"<sql><oracle><select><oracle12c><asset-management>, Aggregate Overlapping Segments to Measure Effective Length, I have a road_events table:
create table road_events (
    event_id number(4,0),
    road_id number(4,0),
    year number(4,0),
    from_meas number(10,2),
    to_meas number(10,2),
    total_road_length number(10,2)
    );
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (1,1,2020,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (2,1,2000,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (3,1,1980,0,25,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (4,1,1960,75,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (5,1,1940,1,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (6,2,2000,10,30,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (7,2,1975,30,60,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (8,2,1950,50,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (9,3,2050,40,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (10,4,2040,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (11,4,2013,0,199,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (12,4,2001,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (13,5,1985,50,70,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (14,5,1985,10,50,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (15,5,1965,1,301,300);
commit;
select * from road_events;
  EVENT_ID    ROAD_ID       YEAR  FROM_MEAS    TO_MEAS TOTAL_ROAD_LENGTH
---------- ---------- ---------- ---------- ---------- -----------------
         1          1       2020         25         50               100
         2          1       2000         25         50               100
         3          1       1980          0         25               100
         4          1       1960         75        100               100
         5          1       1940          1        100               100
         6          2       2000         10         30               100
         7          2       1975         30         60               100
         8          2       1950         50         90               100
         9          3       2050         40         90               100
        10          4       2040          0        200               200
        11          4       2013          0        199               200
        12          4       2001          0        200               200
        13          5       1985         50         70               300
        14          5       1985         10         50               300
        15          5       1965          1        301               300
I want to select the events that represent the most recent work on each road.
This is a tricky operation, because the events often pertain to only a portion of the road. This means that I can't simply select the most recent event per road; I need to only select the most recent event mileage that doesn't overlap.
Possible logic (in order):
I'm reluctant to guess at how this problem could be solved, because it could end up hurting more than it helps (kind of like the XY Problem). On the other hand, it might provide insight into the nature of the problem, so here it goes:
Select the most recent event for each road. We'll call the most recent event: event A.
If event A  is &gt;= total_road_length, then that's all I need. The algorithm ends here.
Else, get the next chronological event (event B) that does not have the same extents as event A. 
If the extents of event B overlap the extents of event A, then only get the portion(s) of event B that do not overlap. 
Repeat steps 3 and 4 until the total event length is = total_road_length. Or stop when there are no more events for that road.
Question:
I know it's a tall order, but what would it take to do this?
This is a classic linear referencing problem. It would be extremely helpful if I could do linear referencing operations as part of queries.
The result would be:
  EVENT_ID    ROAD_ID       YEAR  TOTAL_ROAD_LENGTH   EVENT_LENGTH
---------- ---------- ----------  -----------------   ------------
         1          1       2020                100             25
         3          1       1980                100             25
         4          1       1960                100             25
         5          1       1940                100             25
         6          2       2000                100             20
         7          2       1975                100             30
         8          2       1950                100             30
         9          3       2050                100             50
        10          4       2040                200            200
        13          5       1985                300             20
        14          5       1985                300             40
        15          5       1965                300            240
Related question: Select where number range does not overlap 
","<sal><oracle><select><oracle12c><asset-management>, agree overlap segment measure effect length, road_ev table: great table road_ev ( event_id number(4,0), roadside number(4,0), year number(4,0), from_mea number(10,2), to_mea number(10,2), total_road_length number(10,2) ); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (1,1,2020,25,50,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (2,1,2000,25,50,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (3,1,1980,0,25,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (4,1,1960,75,100,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (5,1,1940,1,100,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (6,2,2000,10,30,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (7,2,1975,30,60,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (8,2,1950,50,90,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (9,3,2050,40,90,100); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (10,4,2040,0,200,200); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (11,4,2013,0,199,200); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (12,4,2001,0,200,200); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (13,5,1985,50,70,300); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (14,5,1985,10,50,300); insert road_ev (event_id, roadside, year, from_meas, to_meas, total_road_length) value (15,5,1965,1,301,300); commit; select * road_events; event_id roadside year from_mea to_mea total_road_length ---------- ---------- ---------- ---------- ---------- ----------------- 1 1 2020 25 50 100 2 1 2000 25 50 100 3 1 1980 0 25 100 4 1 1960 75 100 100 5 1 1940 1 100 100 6 2 2000 10 30 100 7 2 1975 30 60 100 8 2 1950 50 90 100 9 3 2050 40 90 100 10 4 2040 0 200 200 11 4 2013 0 199 200 12 4 2001 0 200 200 13 5 1985 50 70 300 14 5 1985 10 50 300 15 5 1965 1 301 300 want select event repress recent work road. trick operation, event often certain portion road. mean can't simple select recent event per road; need select recent event mileage overlap. possible logic (in order): i'm result guess problem could solved, could end hurt help (kind like by problem). hand, might proved insight nature problem, goes: select recent event road. we'll call recent event: event a. event &it;= total_road_length, that' need. algorithm end here. else, get next chronolog event (event b) extent event a. extent event b overlap extent event a, get portion(s) event b overlap. repeat step 3 4 total event length = total_road_length. stop event road. question: know tall order, would take this? classic linear reference problem. would extreme help could linear reference over part queried. result would be: event_id roadside year total_road_length event_length ---------- ---------- ---------- ----------------- ------------ 1 1 2020 100 25 3 1 1980 100 25 4 1 1960 100 25 5 1 1940 100 25 6 2 2000 100 20 7 2 1975 100 30 8 2 1950 100 30 9 3 2050 100 50 10 4 2040 200 200 13 5 1985 300 20 14 5 1985 300 40 15 5 1965 300 240 relate question: select number rang overlap"
52517529,"How to create schema in Postgres DB, before liquibase start to work?","I have standalone application. It’s on java, spring-boot, postgres and it has liquibase. 
I need to deploy my app and liquibase should create all tables, etc. But it should do it into custom schema not in public. All service tables of liquibase (databasechangelog and databasechangeloglock) should be in custom schema too. How can I create my schema in DB before liquibase start to work? I must do it inside my app when it’s deploying, in config or some like. Without any manual intervention into the DB.
application.properties:
spring.datasource.jndi-name=java:/PostgresDS
spring.jpa.properties.hibernate.default_schema=my_schema
spring.jpa.show-sql = false
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
spring.datasource.continue-on-error=true
spring.datasource.sql-script-encoding=UTF-8
liquibase.change-log = classpath:liquibase/changelog-master.yaml
liquibase.default-schema = my_schema
UPD:
When liquibase start, it's create two tables databasechangelogs and one more table. After that, liquibase start working. But I want liquibase in liquibase.default-schema = my_schema, but it's not exist when liquibase start to work and it an error: exception is liquibase.exception.LockException: liquibase.exception.DatabaseException: ERROR: schema ""my_schema"" does not exist
I want liquibase work in custom schema, not in public:
liquibase.default-schema = my_schema
but before liquibase can do it, the schema must be created. Liquibase can't do this because it not started yet and for start it needs schema.
Vicious circle.
",<java><spring><postgresql><liquibase>,1559,0,11,480,1,3,16,71,20783,0.0,99,4,22,2018-09-26 12:13,2018-09-27 14:21,2018-09-27 14:21,1.0,1.0,Basic,4,"<java><spring><postgresql><liquibase>, How to create schema in Postgres DB, before liquibase start to work?, I have standalone application. It’s on java, spring-boot, postgres and it has liquibase. 
I need to deploy my app and liquibase should create all tables, etc. But it should do it into custom schema not in public. All service tables of liquibase (databasechangelog and databasechangeloglock) should be in custom schema too. How can I create my schema in DB before liquibase start to work? I must do it inside my app when it’s deploying, in config or some like. Without any manual intervention into the DB.
application.properties:
spring.datasource.jndi-name=java:/PostgresDS
spring.jpa.properties.hibernate.default_schema=my_schema
spring.jpa.show-sql = false
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
spring.datasource.continue-on-error=true
spring.datasource.sql-script-encoding=UTF-8
liquibase.change-log = classpath:liquibase/changelog-master.yaml
liquibase.default-schema = my_schema
UPD:
When liquibase start, it's create two tables databasechangelogs and one more table. After that, liquibase start working. But I want liquibase in liquibase.default-schema = my_schema, but it's not exist when liquibase start to work and it an error: exception is liquibase.exception.LockException: liquibase.exception.DatabaseException: ERROR: schema ""my_schema"" does not exist
I want liquibase work in custom schema, not in public:
liquibase.default-schema = my_schema
but before liquibase can do it, the schema must be created. Liquibase can't do this because it not started yet and for start it needs schema.
Vicious circle.
","<cava><spring><postgresql><liquibase>, great scheme poster do, liquibas start work?, standalon application. it’ cava, spring-boot, poster liquibase. need deploy pp liquibas great tables, etc. custom scheme public. service table liquibas (databasechangelog databasechangeloglock) custom scheme too. great scheme do liquibas start work? must inside pp it’ deploying, confirm like. without manual intervene do. application.properties: spring.datasource.and-name=cava:/postures spring.pa.properties.liberate.default_schema=my_schema spring.pa.show-sal = fall spring.pa.properties.liberate.dialect = org.liberate.dialect.postgresqldialect spring.datasource.continue-on-error=true spring.datasource.sal-script-encoding=utf-8 liquibase.change-log = classpath:liquibase/changelog-master.all liquibase.default-scheme = my_schema up: liquibas start, great two table databasechangelog one table. that, liquibas start working. want liquibas liquibase.default-scheme = my_schema, exist liquibas start work error: except liquibase.exception.lockexception: liquibase.exception.databaseexception: error: scheme ""my_schema"" exist want liquibas work custom scheme, public: liquibase.default-scheme = my_schema liquibas it, scheme must created. liquibas can't start yet start need scheme. vicious circle."
49390115,How to find my permissions in a SQL Server database?,"I'm a user of a SQL Sever database, and I want to know my access rights / permissions in the database I'm using. What SQL query should I use to do so?
Thanks
",<sql><sql-server><database><user-permissions><access-rights>,158,0,0,632,1,9,19,61,45192,,63,2,22,2018-03-20 17:00,2018-03-20 17:16,2018-03-20 17:16,0.0,0.0,Basic,9,"<sql><sql-server><database><user-permissions><access-rights>, How to find my permissions in a SQL Server database?, I'm a user of a SQL Sever database, and I want to know my access rights / permissions in the database I'm using. What SQL query should I use to do so?
Thanks
","<sal><sal-server><database><user-permission><access-rights>, find permits sal server database?, i'm user sal never database, want know access right / permits database i'm using. sal query use so? thank"
60301008,Failed to validate connection (This connection has been closed.). Possibly consider using a shorter maxLifetime value,"I see the following error message:
HikariPool-1 - Failed to validate connection
org.postgresql.jdbc.PgConnection@f162126 (This connection has been
closed.). Possibly consider using a shorter maxLifetime value.
frequently refreshing the same page gives the above warning after
exceeding maxLifetime
This was my original database configuration:
spring.datasource.hikari.auto-commit=false  
spring.datasource.hikari.idleTimeout=180000  
spring.datasource.hikari.minimumIdle=5  
spring.datasource.hikari.leakDetectionThreshold=240000    
spring.datasource.hikari.maximumPoolSize=10  
logging.level.com.zaxxer.hikari=TRACE  
spring.datasource.hikari.connectionTimeout=30000  
spring.datasource.hikari.maxLifetime=300000  
logging.level.com.zaxxer.hikari.HikariConfig=DEBUG  
The application is working fine if I change the following properties:
spring.datasource.hikari.maximumPoolSize=100
spring.datasource.hikari.maxLifetime=60000
Can any one explain what is happening exactly?
",<postgresql><spring-boot><database-connection><connection-pooling>,975,0,13,329,1,2,3,63,33427,0.0,0,2,22,2020-02-19 13:02,2021-01-15 10:26,,331.0,,Intermediate,23,"<postgresql><spring-boot><database-connection><connection-pooling>, Failed to validate connection (This connection has been closed.). Possibly consider using a shorter maxLifetime value, I see the following error message:
HikariPool-1 - Failed to validate connection
org.postgresql.jdbc.PgConnection@f162126 (This connection has been
closed.). Possibly consider using a shorter maxLifetime value.
frequently refreshing the same page gives the above warning after
exceeding maxLifetime
This was my original database configuration:
spring.datasource.hikari.auto-commit=false  
spring.datasource.hikari.idleTimeout=180000  
spring.datasource.hikari.minimumIdle=5  
spring.datasource.hikari.leakDetectionThreshold=240000    
spring.datasource.hikari.maximumPoolSize=10  
logging.level.com.zaxxer.hikari=TRACE  
spring.datasource.hikari.connectionTimeout=30000  
spring.datasource.hikari.maxLifetime=300000  
logging.level.com.zaxxer.hikari.HikariConfig=DEBUG  
The application is working fine if I change the following properties:
spring.datasource.hikari.maximumPoolSize=100
spring.datasource.hikari.maxLifetime=60000
Can any one explain what is happening exactly?
","<postgresql><spring-boot><database-connection><connection-pooling>, fail valid connect (the connect closed.). possible consider use shorter maxlifetim value, see follow error message: hikaripool-1 - fail valid connect org.postgresql.job.connection@f162126 (the connect closed.). possible consider use shorter maxlifetim value. frequent refresh page give warn exceed maxlifetim origin database configuration: spring.datasource.kari.auto-commit=fall spring.datasource.kari.idletimeout=180000 spring.datasource.kari.minimumidle=5 spring.datasource.kari.leakdetectionthreshold=240000 spring.datasource.kari.maximumpoolsize=10 logging.level.com.baxter.kari=trap spring.datasource.kari.connectiontimeout=30000 spring.datasource.kari.maxlifetime=300000 logging.level.com.baxter.kari.hikariconfig=debut applied work fine change follow properties: spring.datasource.kari.maximumpoolsize=100 spring.datasource.kari.maxlifetime=60000 one explain happen exactly?"
52473260,Count distinct multiple columns in redshift,"I am trying to count rows which have a distinct combination of 2 columns in Amazon redshift. The query I am using is - 
select count(distinct col1, col2)
from schemaname.tablename
where some filters
It is throwing me this error - 
  Amazon Invalid operation: function count(character varying, bigint) does not exist`
I tried casting bigint to char but it didn't work.
",<sql><amazon-redshift>,368,0,5,507,2,4,12,70,26861,0.0,113,5,22,2018-09-24 5:41,2018-09-24 5:45,2018-09-24 5:45,0.0,0.0,Basic,2,"<sql><amazon-redshift>, Count distinct multiple columns in redshift, I am trying to count rows which have a distinct combination of 2 columns in Amazon redshift. The query I am using is - 
select count(distinct col1, col2)
from schemaname.tablename
where some filters
It is throwing me this error - 
  Amazon Invalid operation: function count(character varying, bigint) does not exist`
I tried casting bigint to char but it didn't work.
","<sal><amazon-redshift>, count distinct multiple column redshift, try count row distinct combine 2 column amazon redshift. query use - select count(distinct cold, cold) schemaname.tablenam filter throw error - amazon invalid operation: function count(character varying, begin) exist` try cast begin chair work."
57914342,visual studio 2019 open solution file incompatible,"I think I was using visual studio 2017 and wrote a SSIS package. Now I installed visual studio 2019 and can't open the solution file. Error:
  Unsupported This version of Visual Studio is unable to open the
  following projects. The project types may not be installed or this
  version of Visual Studio may not support them.  For more information
  on enabling these project types or otherwise migrating your assets,
  please see the details in the ""Migration Report"" displayed after
  clicking OK.
     - ABC, ""C:\Users\XYZ\ABC.dtproj""
  Non-functional changes required Visual Studio will automatically make
  non-functional changes to the following projects in order to enable
  them to open in Visual Studio 2015, Visual Studio 2013, Visual Studio
  2012, and Visual Studio 2010 SP1. Project behavior will not be
  impacted.
     - ABC_SSIS, ""C:\Users\XYZ\ABC_SSIS.sln""
I tried ""Right-click on the project and reload"" - didn't work.
I tried to confirm SSDT is installed:
it is installed at the installation interface, but doesn't exist in extension manager:
",<visual-studio><ssis><sql-server-data-tools>,1061,2,0,3984,11,43,67,59,55613,0.0,135,5,22,2019-09-12 20:52,2019-09-17 16:25,2019-09-17 16:25,5.0,5.0,Basic,6,"<visual-studio><ssis><sql-server-data-tools>, visual studio 2019 open solution file incompatible, I think I was using visual studio 2017 and wrote a SSIS package. Now I installed visual studio 2019 and can't open the solution file. Error:
  Unsupported This version of Visual Studio is unable to open the
  following projects. The project types may not be installed or this
  version of Visual Studio may not support them.  For more information
  on enabling these project types or otherwise migrating your assets,
  please see the details in the ""Migration Report"" displayed after
  clicking OK.
     - ABC, ""C:\Users\XYZ\ABC.dtproj""
  Non-functional changes required Visual Studio will automatically make
  non-functional changes to the following projects in order to enable
  them to open in Visual Studio 2015, Visual Studio 2013, Visual Studio
  2012, and Visual Studio 2010 SP1. Project behavior will not be
  impacted.
     - ABC_SSIS, ""C:\Users\XYZ\ABC_SSIS.sln""
I tried ""Right-click on the project and reload"" - didn't work.
I tried to confirm SSDT is installed:
it is installed at the installation interface, but doesn't exist in extension manager:
","<visual-studio><suis><sal-server-data-tools>, visual studio 2019 open slut file incompatible, think use visual studio 2017 wrote si package. instal visual studio 2019 can't open slut file. error: support version visual studio unable open follow projects. project type may instal version visual studio may support them. inform enable project type otherwise migrate asset, pleas see detail ""migrate report"" display click ok. - abc, ""c:\users\by\abc.tproo"" non-fact change require visual studio automatic make non-fact change follow project order enable open visual studio 2015, visual studio 2013, visual studio 2012, visual studio 2010 spy. project behavior impacted. - abc_ssis, ""c:\users\by\abc_ssis.son"" try ""right-click project reload"" - work. try confirm side installed: instal instal interface, exist extent manager:"
50070877,Postgres Psycopg2 Create Table,"I am new to Postgres and Python. I try to create a simple user table but I don't know why it isn't created.
The error message doesn't appear,
    #!/usr/bin/python
    import psycopg2
    try:
        conn = psycopg2.connect(database = &quot;projetofinal&quot;, user = &quot;postgres&quot;, password = &quot;admin&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;)
    except:
        print(&quot;I am unable to connect to the database&quot;) 
    cur = conn.cursor()
    try:
        cur.execute(&quot;CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);&quot;)
    except:
        print(&quot;I can't drop our test database!&quot;)
    conn.close()
    cur.close()
Any help or hint would be appreciated.
Thank you.
",<python><database><postgresql><python-3.6><postgresql-10>,745,0,16,353,1,2,11,74,31830,0.0,3,1,22,2018-04-27 22:06,2018-04-27 22:10,2018-04-27 22:10,0.0,0.0,Basic,6,"<python><database><postgresql><python-3.6><postgresql-10>, Postgres Psycopg2 Create Table, I am new to Postgres and Python. I try to create a simple user table but I don't know why it isn't created.
The error message doesn't appear,
    #!/usr/bin/python
    import psycopg2
    try:
        conn = psycopg2.connect(database = &quot;projetofinal&quot;, user = &quot;postgres&quot;, password = &quot;admin&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;)
    except:
        print(&quot;I am unable to connect to the database&quot;) 
    cur = conn.cursor()
    try:
        cur.execute(&quot;CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);&quot;)
    except:
        print(&quot;I can't drop our test database!&quot;)
    conn.close()
    cur.close()
Any help or hint would be appreciated.
Thank you.
","<patron><database><postgresql><patron-3.6><postgresql-10>, poster psycopg2 great table, new poster patron. try great simple user table know created. error message appear, #!/us/bin/patron import psycopg2 try: corn = psycopg2.connect(database = &quit;projetofinal&quit;, user = &quit;postures&quit;, password = &quit;admit&quit;, host = &quit;localhost&quit;, port = &quit;5432&quit;) except: print(&quit;i unable connect database&quit;) our = corn.curses() try: our.execute(&quit;or table test (id aerial primary key, sum inter, data varchar);&quit;) except: print(&quit;i can't drop test database!&quit;) corn.close() our.close() help hint would appreciated. thank you."
56462616,How to use pg_restore with AWS RDS correctly to restore postgresql database,"I am trying to restore my Postgresql database to AWS RDS. I think I am almost there. I can get a dump, and recreate the db locally, but I am missing the last step to restore it to AWS RDS. 
Here is what I am doing: 
I get my dump
$ pg_dump -h my_public dns -U myusername -f dump.sql myawsdb
I create a local db in my shell called test: 
create database test;
I put the dump into my test db
$ psql -U myusername -d test -f dump.sql
so far so good. 
I get an error: psql:dump.sql:2705: ERROR:  role ""rdsadmin"" does not exist, but I think I can ignore it, because my db is there with all the content. (I checked with \list and \connect test).
Now I want to restore this dump/test to my AWS RDS. 
Following this https://gist.github.com/syafiqfaiz/5273cd41df6f08fdedeb96e12af70e3b 
I now should do: 
pg_restore -h &lt;host&gt; -U &lt;username&gt; -c -d &lt;database name&gt; &lt;filename to be restored&gt;
But what is my filename and what is my database name?
I tried: 
pg_restore -h mydns -U myusername -c -d myawsdbname test
pg_restore -h mydns -U myusername -c -d myawsdbname dump.sql
and a couple of more options that I don't recall. 
Most of the times it tells me something like: pg_restore: [archiver] could not open input file ""test.dump"": No such file or directory
Or, for the second: input file appears to be a text format dump. Please use psql.
Can somone point me into the right direction? Help is very much appreciated!
EDIT: So I created a .dump file using $ pg_dump -Fc mydb &gt; db.dump
Using this file I think it works. Now I get the error [archiver (db)] could not execute query: ERROR:  role ""myuser"" does not exist
    Command was: ALTER TABLE public.users_user_user_permissions_id_seq OWNER TO micromegas;
Can I ingore that?
EDIT2: I got rid of the error adding the flags--no-owner --role=mypguser --no-privileges --no-owner
",<django><postgresql><amazon-rds><restore><dump>,1841,2,12,1549,2,20,49,53,22116,0.0,141,2,22,2019-06-05 14:31,2020-11-20 17:25,2020-11-20 17:25,534.0,534.0,Intermediate,31,"<django><postgresql><amazon-rds><restore><dump>, How to use pg_restore with AWS RDS correctly to restore postgresql database, I am trying to restore my Postgresql database to AWS RDS. I think I am almost there. I can get a dump, and recreate the db locally, but I am missing the last step to restore it to AWS RDS. 
Here is what I am doing: 
I get my dump
$ pg_dump -h my_public dns -U myusername -f dump.sql myawsdb
I create a local db in my shell called test: 
create database test;
I put the dump into my test db
$ psql -U myusername -d test -f dump.sql
so far so good. 
I get an error: psql:dump.sql:2705: ERROR:  role ""rdsadmin"" does not exist, but I think I can ignore it, because my db is there with all the content. (I checked with \list and \connect test).
Now I want to restore this dump/test to my AWS RDS. 
Following this https://gist.github.com/syafiqfaiz/5273cd41df6f08fdedeb96e12af70e3b 
I now should do: 
pg_restore -h &lt;host&gt; -U &lt;username&gt; -c -d &lt;database name&gt; &lt;filename to be restored&gt;
But what is my filename and what is my database name?
I tried: 
pg_restore -h mydns -U myusername -c -d myawsdbname test
pg_restore -h mydns -U myusername -c -d myawsdbname dump.sql
and a couple of more options that I don't recall. 
Most of the times it tells me something like: pg_restore: [archiver] could not open input file ""test.dump"": No such file or directory
Or, for the second: input file appears to be a text format dump. Please use psql.
Can somone point me into the right direction? Help is very much appreciated!
EDIT: So I created a .dump file using $ pg_dump -Fc mydb &gt; db.dump
Using this file I think it works. Now I get the error [archiver (db)] could not execute query: ERROR:  role ""myuser"" does not exist
    Command was: ALTER TABLE public.users_user_user_permissions_id_seq OWNER TO micromegas;
Can I ingore that?
EDIT2: I got rid of the error adding the flags--no-owner --role=mypguser --no-privileges --no-owner
","<django><postgresql><amazon-rd><restore><dump>, use pg_restor a rd correctly restore postgresql database, try restore postgresql database a rd. think almost there. get dump, retreat do locally, miss last step restore a rd. doing: get dump $ pg_dump -h my_publ in -u myusernam -f dump.sal myawsdb great local do shell call test: great database test; put dump test do $ pool -u myusernam -d test -f dump.sal far good. get error: pool:dump.sal:2705: error: role ""rdsadmin"" exist, think ignore it, do content. (i check \list \connect test). want restore dump/test a rd. follow http://gist.github.com/syafiqfaiz/5273cd41df6f08fdedeb96e12af70e3b do: pg_restor -h &it;host&it; -u &it;surname&it; -c -d &it;database name&it; &it;filename restored&it; filename database name? tried: pg_restor -h my -u myusernam -c -d myawsdbnam test pg_restor -h my -u myusernam -c -d myawsdbnam dump.sal couple option recall. time tell cometh like: pg_restore: [archive] could open input file ""test.dump"": file director or, second: input file appear text format dump. pleas use pool. soon point right direction? help much appreciated! edit: great .dump file use $ pg_dump -ff my &it; do.dump use file think works. get error [archive (do)] could execute query: error: role ""user"" exist command was: alter table public.users_user_user_permissions_id_seq owner micromegas; into that? edit: got rid error ad flags--no-own --role=typhus --no-privilege --no-own"
56538035,Finding sum and grouping in sequelize,"I have a donations table as follows.
Donations Table
| id| amount | member_id |
|---|--------|-----------|
| 0 |   500  |         01|
| 1 |  1000  |         02|
| 2 |  2000  |         01|
How to find sum and group the table by member id as follows.
| amount | member_id |
|--------|-----------|
|  2500  |         01|
|  1000  |         02|
I tried to use the following code but it doesnt seem to work.
const salesValue = await DONATIONS.sum('amount', {
    group: 'member_id'
});
",<mysql><node.js><sequelize.js>,481,0,12,3658,10,31,55,47,55042,0.0,97,2,22,2019-06-11 6:49,2019-06-11 9:24,,0.0,,Basic,2,"<mysql><node.js><sequelize.js>, Finding sum and grouping in sequelize, I have a donations table as follows.
Donations Table
| id| amount | member_id |
|---|--------|-----------|
| 0 |   500  |         01|
| 1 |  1000  |         02|
| 2 |  2000  |         01|
How to find sum and group the table by member id as follows.
| amount | member_id |
|--------|-----------|
|  2500  |         01|
|  1000  |         02|
I tried to use the following code but it doesnt seem to work.
const salesValue = await DONATIONS.sum('amount', {
    group: 'member_id'
});
","<myself><node.is><sequelae.is>, find sum group sequelae, donate table follows. donate table | id| amount | member_id | |---|--------|-----------| | 0 | 500 | 01| | 1 | 1000 | 02| | 2 | 2000 | 01| find sum group table member id follows. | amount | member_id | |--------|-----------| | 2500 | 01| | 1000 | 02| try use follow code doesn seem work. cost salesvalu = await donations.sum('amount', { group: 'member_id' });"
56013334,Spark dynamic frame show method yields nothing,"So I am using AWS Glue auto-generated code to read csv file from S3 and write it to a table over a JDBC connection. Seems simple, Job runs successfully with no error but it writes nothing. When I checked the Glue Spark Dynamic Frame it does contents all the rows (using .count()). But when do a .show() on it yields nothing.
.printSchema() works fine. Tried logging the error while using .show(), but no errors or nothing is printed. Converted the DynamicFrame to the data frame using .toDF and the show method it works. 
I thought there is some problem with the file, trying to narrow to certain columns. But even with just 2 columns in the file same thing. Clearly marked string in double quotes, still no success.
We have things like JDBC connection that needs to be picked from Glue configuration. Which I guess regular spark data frame can't do. Hence need dynamic frame working.
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
datasource0 = glueContext.create_dynamic_frame.from_options('s3', {'paths': ['s3://bucket/file.csv']}, 'csv', format_options={'withHeader': True,'skipFirst': True,'quoteChar':'""','escaper':'\\'})
datasource0.printSchema()
datasource0.show(5)
Output
root
|-- ORDERID: string
|-- EVENTTIMEUTC: string
Here is what the converting to regular data frame yields.
datasource0.toDF().show()
Output
+-------+-----------------+
|ORDERID|     EVENTTIMEUTC|
+-------+-----------------+
|      2| ""1/13/2018 7:50""|
|      3| ""1/13/2018 7:50""|
|      4| ""1/13/2018 7:50""|
|      5| ""1/13/2018 7:50""|
|      6| ""1/13/2018 8:52""|
|      7| ""1/13/2018 8:52""|
|      8| ""1/13/2018 8:53""|
|      9| ""1/13/2018 8:53""|
|     10| ""1/16/2018 1:33""|
|     11| ""1/16/2018 2:28""|
|     12| ""1/16/2018 2:37""|
|     13| ""1/17/2018 1:17""|
|     14| ""1/17/2018 2:23""|
|     15| ""1/17/2018 4:33""|
|     16| ""1/17/2018 6:28""|
|     17| ""1/17/2018 6:28""|
|     18| ""1/17/2018 6:36""|
|     19| ""1/17/2018 6:38""|
|     20| ""1/17/2018 7:26""|
|     21| ""1/17/2018 7:28""|
+-------+-----------------+
only showing top 20 rows
Here is the some data.
ORDERID, EVENTTIMEUTC
1, ""1/13/2018 7:10""
2, ""1/13/2018 7:50""
3, ""1/13/2018 7:50""
4, ""1/13/2018 7:50""
5, ""1/13/2018 7:50""
6, ""1/13/2018 8:52""
7, ""1/13/2018 8:52""
8, ""1/13/2018 8:53""
9, ""1/13/2018 8:53""
10, ""1/16/2018 1:33""
11, ""1/16/2018 2:28""
12, ""1/16/2018 2:37""
13, ""1/17/2018 1:17""
14, ""1/17/2018 2:23""
15, ""1/17/2018 4:33""
16, ""1/17/2018 6:28""
17, ""1/17/2018 6:28""
18, ""1/17/2018 6:36""
19, ""1/17/2018 6:38""
20, ""1/17/2018 7:26""
21, ""1/17/2018 7:28""
22, ""1/17/2018 7:29""
23, ""1/17/2018 7:46""
24, ""1/17/2018 7:51""
25, ""1/18/2018 2:22""
26, ""1/18/2018 5:48""
27, ""1/18/2018 5:50""
28, ""1/18/2018 5:50""
29, ""1/18/2018 5:51""
30, ""1/18/2018 5:53""
100, ""1/18/2018 10:32""
101, ""1/18/2018 10:33""
102, ""1/18/2018 10:33""
103, ""1/18/2018 10:42""
104, ""1/18/2018 10:59""
105, ""1/18/2018 11:16""
",<python><pyspark><apache-spark-sql><aws-glue>,3183,0,84,657,4,11,21,77,23237,,33,1,22,2019-05-06 22:51,2023-05-19 15:42,,1474.0,,Intermediate,25,"<python><pyspark><apache-spark-sql><aws-glue>, Spark dynamic frame show method yields nothing, So I am using AWS Glue auto-generated code to read csv file from S3 and write it to a table over a JDBC connection. Seems simple, Job runs successfully with no error but it writes nothing. When I checked the Glue Spark Dynamic Frame it does contents all the rows (using .count()). But when do a .show() on it yields nothing.
.printSchema() works fine. Tried logging the error while using .show(), but no errors or nothing is printed. Converted the DynamicFrame to the data frame using .toDF and the show method it works. 
I thought there is some problem with the file, trying to narrow to certain columns. But even with just 2 columns in the file same thing. Clearly marked string in double quotes, still no success.
We have things like JDBC connection that needs to be picked from Glue configuration. Which I guess regular spark data frame can't do. Hence need dynamic frame working.
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
datasource0 = glueContext.create_dynamic_frame.from_options('s3', {'paths': ['s3://bucket/file.csv']}, 'csv', format_options={'withHeader': True,'skipFirst': True,'quoteChar':'""','escaper':'\\'})
datasource0.printSchema()
datasource0.show(5)
Output
root
|-- ORDERID: string
|-- EVENTTIMEUTC: string
Here is what the converting to regular data frame yields.
datasource0.toDF().show()
Output
+-------+-----------------+
|ORDERID|     EVENTTIMEUTC|
+-------+-----------------+
|      2| ""1/13/2018 7:50""|
|      3| ""1/13/2018 7:50""|
|      4| ""1/13/2018 7:50""|
|      5| ""1/13/2018 7:50""|
|      6| ""1/13/2018 8:52""|
|      7| ""1/13/2018 8:52""|
|      8| ""1/13/2018 8:53""|
|      9| ""1/13/2018 8:53""|
|     10| ""1/16/2018 1:33""|
|     11| ""1/16/2018 2:28""|
|     12| ""1/16/2018 2:37""|
|     13| ""1/17/2018 1:17""|
|     14| ""1/17/2018 2:23""|
|     15| ""1/17/2018 4:33""|
|     16| ""1/17/2018 6:28""|
|     17| ""1/17/2018 6:28""|
|     18| ""1/17/2018 6:36""|
|     19| ""1/17/2018 6:38""|
|     20| ""1/17/2018 7:26""|
|     21| ""1/17/2018 7:28""|
+-------+-----------------+
only showing top 20 rows
Here is the some data.
ORDERID, EVENTTIMEUTC
1, ""1/13/2018 7:10""
2, ""1/13/2018 7:50""
3, ""1/13/2018 7:50""
4, ""1/13/2018 7:50""
5, ""1/13/2018 7:50""
6, ""1/13/2018 8:52""
7, ""1/13/2018 8:52""
8, ""1/13/2018 8:53""
9, ""1/13/2018 8:53""
10, ""1/16/2018 1:33""
11, ""1/16/2018 2:28""
12, ""1/16/2018 2:37""
13, ""1/17/2018 1:17""
14, ""1/17/2018 2:23""
15, ""1/17/2018 4:33""
16, ""1/17/2018 6:28""
17, ""1/17/2018 6:28""
18, ""1/17/2018 6:36""
19, ""1/17/2018 6:38""
20, ""1/17/2018 7:26""
21, ""1/17/2018 7:28""
22, ""1/17/2018 7:29""
23, ""1/17/2018 7:46""
24, ""1/17/2018 7:51""
25, ""1/18/2018 2:22""
26, ""1/18/2018 5:48""
27, ""1/18/2018 5:50""
28, ""1/18/2018 5:50""
29, ""1/18/2018 5:51""
30, ""1/18/2018 5:53""
100, ""1/18/2018 10:32""
101, ""1/18/2018 10:33""
102, ""1/18/2018 10:33""
103, ""1/18/2018 10:42""
104, ""1/18/2018 10:59""
105, ""1/18/2018 11:16""
","<patron><spark><apache-spark-sal><was-blue>, spark dream frame show method yield nothing, use a blue auto-genet code read is file s write table job connection. seem simple, job run success error write nothing. check blue spark dream frame content row (use .count()). .show() yield nothing. .printschema() work fine. try log error use .show(), error not printed. convert dynamicfram data frame use .of show method works. thought problem file, try narrow certain columns. even 2 column file thing. clearly mark string doubt quotes, still success. thing like job connect need pick blue configuration. guess regular spark data frame can't do. hence need dream frame working. import by awsglue.transform import * awsglue.until import getresolvedopt spark.context import sparkcontext awsglue.context import gluecontext awsglue.job import job awsglue.dynamicfram import dynamicfram import log longer = logging.getlogger() longer.setlevel(logging.debut) gluecontext = gluecontext(sparkcontext.getorcreate()) spark = gluecontext.spark_sess datasource0 = gluecontext.create_dynamic_frame.from_options('s', {'paths': ['s://bucket/file.is']}, 'is', format_options={'withheader': true,'skipfirst': true,'quotechar':'""','escape':'\\'}) datasource0.printschema() datasource0.show(5) output root |-- ordered: string |-- eventtimeutc: string convert regular data frame yields. datasource0.of().show() output +-------+-----------------+ |ordered| eventtimeutc| +-------+-----------------+ | 2| ""1/13/2018 7:50""| | 3| ""1/13/2018 7:50""| | 4| ""1/13/2018 7:50""| | 5| ""1/13/2018 7:50""| | 6| ""1/13/2018 8:52""| | 7| ""1/13/2018 8:52""| | 8| ""1/13/2018 8:53""| | 9| ""1/13/2018 8:53""| | 10| ""1/16/2018 1:33""| | 11| ""1/16/2018 2:28""| | 12| ""1/16/2018 2:37""| | 13| ""1/17/2018 1:17""| | 14| ""1/17/2018 2:23""| | 15| ""1/17/2018 4:33""| | 16| ""1/17/2018 6:28""| | 17| ""1/17/2018 6:28""| | 18| ""1/17/2018 6:36""| | 19| ""1/17/2018 6:38""| | 20| ""1/17/2018 7:26""| | 21| ""1/17/2018 7:28""| +-------+-----------------+ show top 20 row data. ordered, eventtimeutc 1, ""1/13/2018 7:10"" 2, ""1/13/2018 7:50"" 3, ""1/13/2018 7:50"" 4, ""1/13/2018 7:50"" 5, ""1/13/2018 7:50"" 6, ""1/13/2018 8:52"" 7, ""1/13/2018 8:52"" 8, ""1/13/2018 8:53"" 9, ""1/13/2018 8:53"" 10, ""1/16/2018 1:33"" 11, ""1/16/2018 2:28"" 12, ""1/16/2018 2:37"" 13, ""1/17/2018 1:17"" 14, ""1/17/2018 2:23"" 15, ""1/17/2018 4:33"" 16, ""1/17/2018 6:28"" 17, ""1/17/2018 6:28"" 18, ""1/17/2018 6:36"" 19, ""1/17/2018 6:38"" 20, ""1/17/2018 7:26"" 21, ""1/17/2018 7:28"" 22, ""1/17/2018 7:29"" 23, ""1/17/2018 7:46"" 24, ""1/17/2018 7:51"" 25, ""1/18/2018 2:22"" 26, ""1/18/2018 5:48"" 27, ""1/18/2018 5:50"" 28, ""1/18/2018 5:50"" 29, ""1/18/2018 5:51"" 30, ""1/18/2018 5:53"" 100, ""1/18/2018 10:32"" 101, ""1/18/2018 10:33"" 102, ""1/18/2018 10:33"" 103, ""1/18/2018 10:42"" 104, ""1/18/2018 10:59"" 105, ""1/18/2018 11:16"""
48879601,How do I query the length of a Django ArrayField?,"I have an ArrayField in a model, I'm trying to annotate the length of this field ( so far without any luck) 
F('field_name__len') won't work since join is not allowed inside F().  Even 
ModelName.objets.values('field_name__len') is not working
Any idea?
I'm using django 1.11
",<django><postgresql><django-models><django-queryset>,276,0,3,1424,1,13,29,51,8829,0.0,272,3,22,2018-02-20 7:06,2018-02-20 7:16,2018-07-31 11:50,0.0,161.0,Basic,10,"<django><postgresql><django-models><django-queryset>, How do I query the length of a Django ArrayField?, I have an ArrayField in a model, I'm trying to annotate the length of this field ( so far without any luck) 
F('field_name__len') won't work since join is not allowed inside F().  Even 
ModelName.objets.values('field_name__len') is not working
Any idea?
I'm using django 1.11
","<django><postgresql><django-models><django-queryset>, query length django arrayfield?, arrayfield model, i'm try cannot length field ( far without luck) f('field_name__len') work since join allow inside f(). even modelname.objects.values('field_name__len') work idea? i'm use django 1.11"
57646341,Check if table exists in hive metastore using Pyspark,"I am trying to check if a table exists in hive metastore if not, create the table. And if the table exists, append data.
I have a snippet of the code below:
spark.catalog.setCurrentDatabase(&quot;db_name&quot;)
db_catalog = spark.catalog.listTables(dbName = 'table_name)
if any(table_name in row for row in db_catalog):
    add data
else:
    create table
However, I am getting an error.
&gt;&gt;&gt; ValueError: Some of types cannot be determined after inferring
I am unable to resolve the value error as I get the same errors for other databases' tables created in hive metastore. Is there another way to check if table exists in hive metastore?
",<python-3.x><apache-spark><hive><pyspark><apache-spark-sql>,648,0,7,3097,10,55,81,69,52330,0.0,30,9,22,2019-08-25 13:15,2019-08-26 4:12,,1.0,,Basic,10,"<python-3.x><apache-spark><hive><pyspark><apache-spark-sql>, Check if table exists in hive metastore using Pyspark, I am trying to check if a table exists in hive metastore if not, create the table. And if the table exists, append data.
I have a snippet of the code below:
spark.catalog.setCurrentDatabase(&quot;db_name&quot;)
db_catalog = spark.catalog.listTables(dbName = 'table_name)
if any(table_name in row for row in db_catalog):
    add data
else:
    create table
However, I am getting an error.
&gt;&gt;&gt; ValueError: Some of types cannot be determined after inferring
I am unable to resolve the value error as I get the same errors for other databases' tables created in hive metastore. Is there another way to check if table exists in hive metastore?
","<patron-3.x><apache-spark><hive><spark><apache-spark-sal>, check table exist hive metaphor use spark, try check table exist hive metaphor not, great table. table exists, happened data. snipped code below: spark.catalogue.setcurrentdatabase(&quit;db_name&quit;) db_catalog = spark.catalogue.listtables(dream = 'table_name) any(table_nam row row db_catalog): add data else: great table however, get error. &it;&it;&it; valueerror: type cannot determine infer unable resolve value error get error database' table great hive metastore. not way check table exist hive metastore?"
49379538,How take mysqldump with UTF8?,"I am trying to take mysql dump with command:
mysqldump -u xxxx -p dbxxx &gt; xxxx270613.sql
what is command to take mysqldump with UTF8 ?
",<mysql>,138,0,1,3483,2,20,30,65,49695,0.0,166,3,22,2018-03-20 8:36,2018-03-20 8:46,2018-03-20 8:46,0.0,0.0,Basic,10,"<mysql>, How take mysqldump with UTF8?, I am trying to take mysql dump with command:
mysqldump -u xxxx -p dbxxx &gt; xxxx270613.sql
what is command to take mysqldump with UTF8 ?
","<myself>, take mysqldump utf?, try take myself dump command: mysqldump -u xxxi -p xxx &it; xxxx270613.sal command take mysqldump utf ?"
56807876,ODCIAggregateMerge without parallel_enabled,"These are quotes from Oracle docs:
  [Optional] Merge by combining the two aggregation contexts and return a single context. This operation combines the results of aggregation over subsets in order to obtain the aggregate over the entire set. This extra step can be required during either serial or parallel evaluation of an aggregate. If needed, it is performed before step 4:  
,
  The ODCIAggregateMerge() interface is invoked to compute super aggregate values in such rollup operations.
We have an aggregate function, that we do NOT want to ever run in parallel.
The reason is that the merging of contexts would be resource consuming and would force us to use different data structures than we are using now, effectively offseting any performance benefits from parallel execution.
Thus, we did not declare our function as parallel_enabled, and instead return ODCIconst.Error in ODCIAggregateMerge 'just in case'.
However, the first quote docs claim, that merge may occur even in serial evaluation.
Super-aggregates (rollup, cube) are obvious examples, but are there any others?
I've been totally unable to reproduce it with simple group by, merge is never called without parallel_enabled and it seems that always only one context is created within the group.  
Is it safe to assume that without the parallel_enabled set, merge will never be run?
Have you ever seen a counterexample to that rule?
",<sql><oracle><plsql><aggregate>,1400,1,0,925,0,6,24,44,660,0.0,172,1,22,2019-06-28 13:30,2020-07-06 5:56,,374.0,,Intermediate,18,"<sql><oracle><plsql><aggregate>, ODCIAggregateMerge without parallel_enabled, These are quotes from Oracle docs:
  [Optional] Merge by combining the two aggregation contexts and return a single context. This operation combines the results of aggregation over subsets in order to obtain the aggregate over the entire set. This extra step can be required during either serial or parallel evaluation of an aggregate. If needed, it is performed before step 4:  
,
  The ODCIAggregateMerge() interface is invoked to compute super aggregate values in such rollup operations.
We have an aggregate function, that we do NOT want to ever run in parallel.
The reason is that the merging of contexts would be resource consuming and would force us to use different data structures than we are using now, effectively offseting any performance benefits from parallel execution.
Thus, we did not declare our function as parallel_enabled, and instead return ODCIconst.Error in ODCIAggregateMerge 'just in case'.
However, the first quote docs claim, that merge may occur even in serial evaluation.
Super-aggregates (rollup, cube) are obvious examples, but are there any others?
I've been totally unable to reproduce it with simple group by, merge is never called without parallel_enabled and it seems that always only one context is created within the group.  
Is it safe to assume that without the parallel_enabled set, merge will never be run?
Have you ever seen a counterexample to that rule?
","<sal><oracle><plsql><aggregate>, odciaggregatemerg without parallel_enabled, quit oral docs: [optional] berg combine two agree context return single context. over combine result agree sunset order obtain agree enter set. extra step require either aerial parallel value aggregate. needed, perform step 4: , odciaggregatemerge() interface invoke compute super agree value rolled operations. agree function, want ever run parallel. reason berg context would resource consume would for us use differ data structure use now, effect offset perform benefit parallel execution. thus, declare function parallel_enabled, instead return odciconst.error odciaggregatemerg 'just case'. however, first quit do claim, berg may occur even aerial evaluation. super-agree (rolled, cure) obvious examples, others? i'v total unable reproduce simple group by, berg never call without parallel_en seem away one context great within group. safe assume without parallel_en set, berg never run? ever seen counterexampl rule?"
55950386,"BigQuery - No matching signature for operator = for argument types: INT64, STRING","Im getting a weird error(Maybe im getting this error for the first time) from BQ.
No matching signature for operator = for argument types: INT64, STRING. 
Supported signatures: ANY = ANY at [27:1]
Query:
SELECT col1
    ,col2
    ,col3
FROM tbl1
JOIN t2 ON t1.id = t2.id
JOIN t3 on t2.id = t3.id
JOIN t4 on t4.id = t1.id
Error line JOIN t2.id = t3.id  t2.id is showing this error.
its an integer column.
",<sql><google-cloud-platform><google-bigquery>,404,0,10,2832,6,40,91,38,128667,0.0,57,3,22,2019-05-02 10:25,2019-05-02 10:30,,0.0,,Basic,2,"<sql><google-cloud-platform><google-bigquery>, BigQuery - No matching signature for operator = for argument types: INT64, STRING, Im getting a weird error(Maybe im getting this error for the first time) from BQ.
No matching signature for operator = for argument types: INT64, STRING. 
Supported signatures: ANY = ANY at [27:1]
Query:
SELECT col1
    ,col2
    ,col3
FROM tbl1
JOIN t2 ON t1.id = t2.id
JOIN t3 on t2.id = t3.id
JOIN t4 on t4.id = t1.id
Error line JOIN t2.id = t3.id  t2.id is showing this error.
its an integer column.
","<sal><goose-cloud-platform><goose-bigquery>, bigqueri - match signature over = argument types: into, string, in get weird error(may in get error first time) by. match signature over = argument types: into, string. support signatures: = [27:1] query: select cold ,cold ,cold tell join to to.id = to.id join to to.id = to.id join to to.id = to.id error line join to.id = to.id to.id show error. inter column."
51075096,Flask-Admin create view with SQLAlchemy context-sensitive functions,"I have a data model which has a column that depends on other column values, following the instructions in this page I've created a context-sensitive function which is used to determine the value of this particular column on creation, something like this:
def get_column_value_from_context(context):
    # Instructions to produce value
    return value
class MyModel(db.Model):
    id = db.Column(db.Integer,
                   primary_key=True)
    my_column = db.Column(db.String(64),
                          nullable=False,
                          default=get_column_value_from_context)
    name = db.Column(db.String(32),
                     nullable=False,
                     unique=True,
                     index=True)
    title = db.Column(db.String(128),
                      nullable=False)
    description = db.Column(db.String(256),
                            nullable=False)
This approach works pretty decent, I can create rows without problems from the command line or using a script.
I've also added a ModelView to the app using Flask-Admin:
class MyModelView(ModelView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
admin.add_view(MyModelView(MyModel, db.session))
This also works pretty decent until I click the Create button in the list view. I receive this error:
  AttributeError: 'NoneType' object has no attribute 'get_current_parameters'
Because the implementation of the create_model handler in the ModelView is this:
def create_model(self, form):
    """"""
        Create model from form.
        :param form:
            Form instance
    """"""
    try:
        model = self.model()
        form.populate_obj(model)
        self.session.add(model)
        self._on_model_change(form, model, True)
        self.session.commit()
    except Exception as ex:
        if not self.handle_view_exception(ex):
            flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
            log.exception('Failed to create record.')
        self.session.rollback()
        return False
    else:
        self.after_model_change(form, model, True)
    return model
and here there isn't a context when the model is instantiated. So, I've created a custom view where the model instantiation in the creation handler could be redefined:
class CustomSQLAView(ModelView):
    def __init__(self, *args, **kwargs):
        super(CustomSQLAView, self).__init__(*args, **kwargs)
    def create_model(self, form):
        """"""
            Create model from form.
            :param form:
                Form instance
        """"""
        try:
            model = self.get_populated_model(form)
            self.session.add(model)
            self._on_model_change(form, model, True)
            self.session.commit()
        except Exception as ex:
            if not self.handle_view_exception(ex):
                flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
                log.exception('Failed to create record.')
            self.session.rollback()
            return False
        else:
            self.after_model_change(form, model, True)
        return model
    def get_populated_model(self, form):
        model = self.model()
        form.populate_obj(model)
        return model
Now I can redefine the get_populated_model method to instantiate the model in the usual way:
class MyModelView(CustomSQLAView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
    def get_populated_model(self, form):
        model = self.model(
            name=form.name.data,
            title=form.title.data,
            description=form.description.data,
        )
        return model
Despite that this works, I suspect it breaks something. Flask-Admin has several implementation of the populate_obj method of forms and fields, so I would like to keep everything safe.
What is the proper way to do this?
",<python><flask><flask-sqlalchemy><flask-admin>,3931,2,104,626,2,8,20,37,2001,0.0,226,1,22,2018-06-28 5:11,2018-09-21 21:54,,85.0,,Basic,6,"<python><flask><flask-sqlalchemy><flask-admin>, Flask-Admin create view with SQLAlchemy context-sensitive functions, I have a data model which has a column that depends on other column values, following the instructions in this page I've created a context-sensitive function which is used to determine the value of this particular column on creation, something like this:
def get_column_value_from_context(context):
    # Instructions to produce value
    return value
class MyModel(db.Model):
    id = db.Column(db.Integer,
                   primary_key=True)
    my_column = db.Column(db.String(64),
                          nullable=False,
                          default=get_column_value_from_context)
    name = db.Column(db.String(32),
                     nullable=False,
                     unique=True,
                     index=True)
    title = db.Column(db.String(128),
                      nullable=False)
    description = db.Column(db.String(256),
                            nullable=False)
This approach works pretty decent, I can create rows without problems from the command line or using a script.
I've also added a ModelView to the app using Flask-Admin:
class MyModelView(ModelView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
admin.add_view(MyModelView(MyModel, db.session))
This also works pretty decent until I click the Create button in the list view. I receive this error:
  AttributeError: 'NoneType' object has no attribute 'get_current_parameters'
Because the implementation of the create_model handler in the ModelView is this:
def create_model(self, form):
    """"""
        Create model from form.
        :param form:
            Form instance
    """"""
    try:
        model = self.model()
        form.populate_obj(model)
        self.session.add(model)
        self._on_model_change(form, model, True)
        self.session.commit()
    except Exception as ex:
        if not self.handle_view_exception(ex):
            flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
            log.exception('Failed to create record.')
        self.session.rollback()
        return False
    else:
        self.after_model_change(form, model, True)
    return model
and here there isn't a context when the model is instantiated. So, I've created a custom view where the model instantiation in the creation handler could be redefined:
class CustomSQLAView(ModelView):
    def __init__(self, *args, **kwargs):
        super(CustomSQLAView, self).__init__(*args, **kwargs)
    def create_model(self, form):
        """"""
            Create model from form.
            :param form:
                Form instance
        """"""
        try:
            model = self.get_populated_model(form)
            self.session.add(model)
            self._on_model_change(form, model, True)
            self.session.commit()
        except Exception as ex:
            if not self.handle_view_exception(ex):
                flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
                log.exception('Failed to create record.')
            self.session.rollback()
            return False
        else:
            self.after_model_change(form, model, True)
        return model
    def get_populated_model(self, form):
        model = self.model()
        form.populate_obj(model)
        return model
Now I can redefine the get_populated_model method to instantiate the model in the usual way:
class MyModelView(CustomSQLAView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
    def get_populated_model(self, form):
        model = self.model(
            name=form.name.data,
            title=form.title.data,
            description=form.description.data,
        )
        return model
Despite that this works, I suspect it breaks something. Flask-Admin has several implementation of the populate_obj method of forms and fields, so I would like to keep everything safe.
What is the proper way to do this?
","<patron><flask><flask-sqlalchemy><flask-admit>, flask-admit great view sqlalchemi context-sent functions, data model column depend column values, follow instruct page i'v great context-sent function use determine value particular column creation, cometh like this: def get_column_value_from_context(context): # instruct produce value return value class model(do.model): id = do.column(do.inter, primary_key=true) my_column = do.column(do.string(64), syllable=false, default=get_column_value_from_context) name = do.column(do.string(32), syllable=false, unique=true, index=true) till = do.column(do.string(128), syllable=false) rescript = do.column(do.string(256), syllable=false) approach work pretty decent, great row without problem command line use script. i'v also ad modelview pp use flask-admit: class mymodelview(modelview): can_view_detail = true can_set_page_s = true can_export = true admit.add_view(mymodelview(model, do.session)) also work pretty decent click great button list view. receive error: attributeerror: 'nonetype' object attribute 'get_current_parameters' implement create_model handle modelview this: def create_model(self, form): """""" great model form. :parma form: form instant """""" try: model = self.model() form.populate_obj(model) self.session.add(model) self._on_model_change(form, model, true) self.session.commit() except except ex: self.handle_view_exception(ex): flash(etext('fail great record. %(error)s', error=sir(ex)), 'error') log.exception('fail great record.') self.session.rollback() return fall else: self.after_model_change(form, model, true) return model context model instantiated. so, i'v great custom view model instant creation handle could defined: class customsqlaview(modelview): def __init__(self, *arms, **wars): super(customsqlaview, self).__init__(*arms, **wars) def create_model(self, form): """""" great model form. :parma form: form instant """""" try: model = self.get_populated_model(form) self.session.add(model) self._on_model_change(form, model, true) self.session.commit() except except ex: self.handle_view_exception(ex): flash(etext('fail great record. %(error)s', error=sir(ex)), 'error') log.exception('fail great record.') self.session.rollback() return fall else: self.after_model_change(form, model, true) return model def get_populated_model(self, form): model = self.model() form.populate_obj(model) return model redefin get_populated_model method instant model usual way: class mymodelview(customsqlaview): can_view_detail = true can_set_page_s = true can_export = true def get_populated_model(self, form): model = self.model( name=form.name.data, title=form.title.data, description=form.description.data, ) return model despite works, suspect break something. flask-admit never implement populate_obj method form fields, would like keep every safe. proper way this?"
56373970,Insert multiple records in Sqflite,"How to insert quickly multiple records in sqflite? The standard quickly method is:
await database.insert(table, object.toMap())
But I don't think that insert record one to one with a cycle is a good idea.
Or I can insert all list with a transaction?
",<sqlite><flutter><sqflite>,250,0,2,10529,3,42,48,41,22138,0.0,480,7,21,2019-05-30 7:51,2019-05-30 8:33,,0.0,,Basic,9,"<sqlite><flutter><sqflite>, Insert multiple records in Sqflite, How to insert quickly multiple records in sqflite? The standard quickly method is:
await database.insert(table, object.toMap())
But I don't think that insert record one to one with a cycle is a good idea.
Or I can insert all list with a transaction?
","<quite><flutter><sqflite>, insert multiple record sqflite, insert quickly multiple record sqflite? standard quickly method is: await database.insert(table, object.woman()) think insert record one one cycle good idea. insert list transaction?"
58352334,Spring Data / Hibernate save entity with Postgres using Insert on Conflict Update Some fields,"I have a domain object in Spring which I am saving using JpaRepository.save method and using Sequence generator from Postgres to generate id automatically.
@SequenceGenerator(initialValue = 1, name = ""device_metric_gen"", sequenceName = ""device_metric_seq"")
public class DeviceMetric extends BaseTimeModel {
    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = ""device_metric_gen"")
    @Column(nullable = false, updatable = false)
    private Long id;
///// extra fields
My use-case requires to do an upsert instead of normal save operation (which I am aware will update if the id is present). I want to update an existing row if a combination of three columns (assume a composite unique) is present or else create a new row.
This is something similar to this:
INSERT INTO customers (name, email)
VALUES
   (
      'Microsoft',
      'hotline@microsoft.com'
   ) 
ON CONFLICT (name) 
DO
      UPDATE
     SET email = EXCLUDED.email || ';' || customers.email;
One way of achieving the same in Spring-data that I can think of is:
Write a custom save operation in the service layer that
Does a get for the three-column and if a row is present
Set the same id in current object and do a repository.save
If no row present, do a normal repository.save
Problem with the above approach is that every insert now does a select and then save which makes two database calls whereas the same can be achieved by postgres insert on conflict feature with just one db call.
Any pointers on how to implement this in Spring Data?
One way is to write a native query insert into values (all fields here). The object in question has around 25 fields so I am looking for an another better way to achieve the same.
",<spring><postgresql><spring-boot><spring-data-jpa>,1713,1,25,1395,3,17,34,65,10096,0.0,200,2,21,2019-10-12 8:34,2022-12-26 14:50,,1171.0,,Intermediate,18,"<spring><postgresql><spring-boot><spring-data-jpa>, Spring Data / Hibernate save entity with Postgres using Insert on Conflict Update Some fields, I have a domain object in Spring which I am saving using JpaRepository.save method and using Sequence generator from Postgres to generate id automatically.
@SequenceGenerator(initialValue = 1, name = ""device_metric_gen"", sequenceName = ""device_metric_seq"")
public class DeviceMetric extends BaseTimeModel {
    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = ""device_metric_gen"")
    @Column(nullable = false, updatable = false)
    private Long id;
///// extra fields
My use-case requires to do an upsert instead of normal save operation (which I am aware will update if the id is present). I want to update an existing row if a combination of three columns (assume a composite unique) is present or else create a new row.
This is something similar to this:
INSERT INTO customers (name, email)
VALUES
   (
      'Microsoft',
      'hotline@microsoft.com'
   ) 
ON CONFLICT (name) 
DO
      UPDATE
     SET email = EXCLUDED.email || ';' || customers.email;
One way of achieving the same in Spring-data that I can think of is:
Write a custom save operation in the service layer that
Does a get for the three-column and if a row is present
Set the same id in current object and do a repository.save
If no row present, do a normal repository.save
Problem with the above approach is that every insert now does a select and then save which makes two database calls whereas the same can be achieved by postgres insert on conflict feature with just one db call.
Any pointers on how to implement this in Spring Data?
One way is to write a native query insert into values (all fields here). The object in question has around 25 fields so I am looking for an another better way to achieve the same.
","<spring><postgresql><spring-boot><spring-data-pa>, spring data / wiberd save entity poster use insert conflict update fields, domain object spring save use jparepository.say method use sequence genet poster genet id automatically. @sequencegenerator(initialvalu = 1, name = ""device_metric_gen"", sequencenam = ""device_metric_seq"") public class devicemetr extend basetimemodel { @id @generatedvalue(strategic = generationtype.sequence, genet = ""device_metric_gen"") @column(null = false, update = false) privat long id; ///// extra field use-was require upset instead normal save over (which war update id present). want update exist row combine three column (assume composite unique) present else great new row. cometh similar this: insert custom (name, email) value ( 'microsoft', 'outline@microsoft.com' ) conflict (name) update set email = excluded.email || ';' || customers.email; one way achieve spring-data think is: write custom save over service layer get three-column row present set id current object depositors.say row present, normal depositors.say problem approach every insert select save make two database call where achieve poster insert conflict feature one do call. pointer implement spring data? one way write native query insert value (all field here). object question around 25 field look not better way achieve same."
50664648,Why even use *DB.exec() or prepared statements in Golang?,"I'm using golang with Postgresql.
It says here that for operations that do not return rows (insert, delete, update) we should use exec()
  If a function name includes Query, it is designed to ask a question of the database, and will return a set of rows, even if it’s empty. Statements that don’t return rows should not use Query functions; they should use Exec().
Then it says here:
  Go creates prepared statements for you under the covers. A simple db.Query(sql, param1, param2), for example, works by preparing the sql, then executing it with the parameters and finally closing the statement.
If query() uses under the covers prepared statements why should I even bother using prepared statements?
",<sql><database><postgresql><go><prepared-statement>,702,2,2,23944,33,137,186,56,32402,0.0,1968,1,21,2018-06-03 8:38,2018-06-03 11:52,2018-06-03 11:52,0.0,0.0,Intermediate,18,"<sql><database><postgresql><go><prepared-statement>, Why even use *DB.exec() or prepared statements in Golang?, I'm using golang with Postgresql.
It says here that for operations that do not return rows (insert, delete, update) we should use exec()
  If a function name includes Query, it is designed to ask a question of the database, and will return a set of rows, even if it’s empty. Statements that don’t return rows should not use Query functions; they should use Exec().
Then it says here:
  Go creates prepared statements for you under the covers. A simple db.Query(sql, param1, param2), for example, works by preparing the sql, then executing it with the parameters and finally closing the statement.
If query() uses under the covers prepared statements why should I even bother using prepared statements?
","<sal><database><postgresql><go><prepared-statement>, even use *do.even() prepare statement going?, i'm use going postgresql. say over return row (insert, delete, update) use even() function name include query, design ask question database, return set rows, even it’ empty. statement don’t return row use query functions; use even(). say here: go great prepare statement covers. simple do.query(sal, panama, panama), example, work prepare sal, execute parapet final close statement. query() use cover prepare statement even bother use prepare statements?"
54159964,How to remove nulls with array_remove Spark SQL built-in function,"Spark 2.4 introduced new useful Spark SQL functions involving arrays, but I was a little bit puzzled when I found out that the result of
select array_remove(array(1, 2, 3, null, 3), null) is null and not [1, 2, 3, 3].
Is this the expected behavior? Is it possible to remove nulls using array_remove?
As a side note, for now the alternative I am using is a higher order function in Databricks:
select filter(array(1, 2, 3, null, 3), x -&gt; x is not null)
",<arrays><dataframe><apache-spark><apache-spark-sql><null>,455,0,5,2291,1,17,33,78,19343,0.0,1097,6,21,2019-01-12 13:17,2019-01-14 6:20,2019-01-14 6:20,2.0,2.0,Intermediate,15,"<arrays><dataframe><apache-spark><apache-spark-sql><null>, How to remove nulls with array_remove Spark SQL built-in function, Spark 2.4 introduced new useful Spark SQL functions involving arrays, but I was a little bit puzzled when I found out that the result of
select array_remove(array(1, 2, 3, null, 3), null) is null and not [1, 2, 3, 3].
Is this the expected behavior? Is it possible to remove nulls using array_remove?
As a side note, for now the alternative I am using is a higher order function in Databricks:
select filter(array(1, 2, 3, null, 3), x -&gt; x is not null)
","<array><dataframe><apache-spark><apache-spark-sal><null>, remove null array_remov spark sal built-in function, spark 2.4 introduce new use spark sal function involve array, little bit puzzle found result select array_remove(array(1, 2, 3, null, 3), null) null [1, 2, 3, 3]. expect behavior? possible remove null use array_remove? side note, alter use higher order function databricks: select filter(array(1, 2, 3, null, 3), x -&it; x null)"
56411055,Function uuid_generate_v4() does not exist postgres 11,"I am trying to use node-pg-migrate and run migrations to create tables in my node project.
When I run migrations I get function uuid_generate_v4() does not exist.
I did check in my extensions and uuid-ossp is available.
extname  | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition 
-----------+----------+--------------+----------------+------------+-----------+--------------
 plpgsql   |       10 |           11 | f              | 1.0        |           | 
 uuid-ossp |    16384 |         2200 | t              | 1.1        |           | 
(2 rows)
I expect my migrations to run but it fails. I am using Postgres 11 on Mac.
Postgres installed from here - https://postgresapp.com/
",<postgresql><postgresql-11>,712,2,6,329,1,2,11,37,31257,0.0,5,3,21,2019-06-01 23:07,2019-06-01 23:33,,0.0,,Basic,14,"<postgresql><postgresql-11>, Function uuid_generate_v4() does not exist postgres 11, I am trying to use node-pg-migrate and run migrations to create tables in my node project.
When I run migrations I get function uuid_generate_v4() does not exist.
I did check in my extensions and uuid-ossp is available.
extname  | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition 
-----------+----------+--------------+----------------+------------+-----------+--------------
 plpgsql   |       10 |           11 | f              | 1.0        |           | 
 uuid-ossp |    16384 |         2200 | t              | 1.1        |           | 
(2 rows)
I expect my migrations to run but it fails. I am using Postgres 11 on Mac.
Postgres installed from here - https://postgresapp.com/
","<postgresql><postgresql-11>, function uuid_generate_v4() exist poster 11, try use node-pg-might run migrate great table node project. run migrate get function uuid_generate_v4() exist. check extent quid-loss available. extra | town | extnamespac | extrelocat | enters | extconfig | extcondit -----------+----------+--------------+----------------+------------+-----------+-------------- plpgsql | 10 | 11 | f | 1.0 | | quid-loss | 16384 | 2200 | | 1.1 | | (2 rows) expect migrate run fails. use poster 11 mac. poster instal - http://postgresapp.com/"
63301495,PDOException: Packets out of order. Expected 0 received 1. Packet size=23,"I have a Laravel Spark project that uses Horizon to manage a job queue with Redis.
Locally, (on my Homestead box, Mac OS) everything works as expected, but on our new Digital Ocean (Forge provisioned) Droplet, which is a memory-optimized 256GB, 32vCPUs, 10TB, and 1x 800GB VPS, I keep getting the error:
PDOException: Packets out of order. Expected 0 received 1. Packet size=23
Or some variation of that error, where the packet size info may be different.
After many hours/days of debugging and research, I have come across many posts on StackOverflow and elsewhere, that seem to indicate that this can be fixed by doing a number of things, listed below:
Set PDO::ATTR_EMULATE_PREPARES to true in my database.php config. This has absolutely no effect on the problem, and actually introduces another issue, whereby integers are cast as strings.
Set DB_HOST to 127.0.0.1 instead of localhost, so that it uses TCP instead of a UNIX socket. Again, this has no effect.
Set DB_SOCKET to the socket path listed in MySQL by logging into MySQL (MariaDB) and running show variables like '%socket%'; which lists the socket path as /run/mysqld/mysqld.sock. I also leave DB_HOST set to localhost. This has no effect either. One thing I did note, was that the pdo_mysql.default_socket variable is set to /var/run/mysqld/mysqld.sock, I'm not sure if this is part of the problem?
I have massively increased the MySQL configuration settings found in /etc/mysql/mariadb.conf.d/50-server.cnf to the following:
key_buffer_size = 2048M
max_allowed_packet = 2048M
max_connections = 1000
thread_concurrency = 100
query_cache_size = 256M
I must admit, that changing these settings was a last resort/clutching at straws type scenario. However, this did alleviate the issue to some degree, but it did not fix it completely, as MySQL still fails 99% of the time, albeit at a later stage.
In terms of the queue, I have a total of 1,136 workers split between 6 supervisors/queues and it's all handled via Laravel Horizon, which is being run as a Daemon.
I am also using the Laravel Websockets PHP package for broadcasting, again, which is also being run as a Daemon.
My current environment configuration is as follows (sensitive info omitted).
APP_NAME=&quot;App Name&quot;
APP_ENV=production
APP_DEBUG=false
APP_KEY=thekey
APP_URL=https://appurl.com
LOG_CHANNEL=single
DB_CONNECTION=mysql
DB_HOST=127.0.0.1
DB_PORT=3306
DB_DATABASE=databse
DB_USERNAME=username
DB_PASSWORD=password
BROADCAST_DRIVER=pusher
CACHE_DRIVER=file
QUEUE_CONNECTION=redis
SESSION_DRIVER=file
SESSION_LIFETIME=120
REDIS_HOST=127.0.0.1
REDIS_PASSWORD=null
REDIS_PORT=6379
MAIL_MAILER=smtp
MAIL_HOST=smtp.gmail.com
MAIL_PORT=587
MAIL_USERNAME=name@email.com
MAIL_PASSWORD=password
MAIL_ENCRYPTION=tls
MAIL_FROM_ADDRESS=name@email.com
MAIL_FROM_NAME=&quot;${APP_NAME}&quot;
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=&quot;us-east-1&quot;
AWS_BUCKET=
PUSHER_APP_ID=appid
PUSHER_APP_KEY=appkey
PUSHER_APP_SECRET=appsecret
PUSHER_APP_CLUSTER=mt1
MIX_PUSHER_APP_KEY=&quot;${PUSHER_APP_KEY}&quot;
MIX_PUSHER_APP_CLUSTER=&quot;${PUSHER_APP_CLUSTER}&quot;
AUTHY_SECRET=
CASHIER_CURRENCY=usd
CASHIER_CURRENCY_LOCALE=en
CASHIER_MODEL=App\Models\User
STRIPE_KEY=stripekey
STRIPE_SECRET=stripesecret
# ECHO SERVER
LARAVEL_WEBSOCKETS_PORT=port
The server setup is as follows:
Max File Upload Size: 1024
Max Execution Time: 300
PHP Version: 7.4
MariaDB Version: 10.3.22
I have checked all logs (see below) at the time the MySQL server crashes/goes away, and there is nothing in the MySQL logs at all. No error whatsoever. I also don't see anything in:
/var/log/nginx/error.log
/var/log/nginx/access.log
/var/log/php7.4-fpm.log
I'm currently still digging through and debugging, but right now, I'm stumped. This is the first time I've ever come across this error.
Could this be down to hitting the database (read/write) too fast?
A little information on how the queues work.
I have an initial controller that dispatches a job to the queue.
Once this job completes, it fires an event which then starts the process of running several other listeners/events in sequence, all of which depend on the previous jobs completing before new events are fired and new listeners/jobs take up the work.
In total, there are 30 events that are broadcast.
In total, there are 30 listeners.
In total there are 5 jobs.
These all work sequentially based on the listener/job that was run and the event that it fires.
I have also monitored the laravel.log live and when the crash occurs, nothing is logged at all. Although, I do occasionally get production.ERROR: Failed to connect to Pusher. whether MySQL crashes or not, so I don't think that has any bearing on this problem.
I even noticed that the Laravel API rate limit was being hit, so I made sure to drastically increase that from 60 to 500. Still no joy.
Lastly, it doesn't seem to matter which Event, Job, or Listener is running as the error occurs on random ones. So, not sure it's code-specific, although, it may well be.
Hopefully, I've provided enough background and detailed information to get some help with this, but if I've missed anything, please do let me know and I'll add it to the question. Thanks.
",<php><mysql><laravel><sockets><pdo>,5198,1,80,4076,5,41,67,48,54687,0.0,46,6,21,2020-08-07 12:05,2020-11-27 16:33,,112.0,,Advanced,32,"<php><mysql><laravel><sockets><pdo>, PDOException: Packets out of order. Expected 0 received 1. Packet size=23, I have a Laravel Spark project that uses Horizon to manage a job queue with Redis.
Locally, (on my Homestead box, Mac OS) everything works as expected, but on our new Digital Ocean (Forge provisioned) Droplet, which is a memory-optimized 256GB, 32vCPUs, 10TB, and 1x 800GB VPS, I keep getting the error:
PDOException: Packets out of order. Expected 0 received 1. Packet size=23
Or some variation of that error, where the packet size info may be different.
After many hours/days of debugging and research, I have come across many posts on StackOverflow and elsewhere, that seem to indicate that this can be fixed by doing a number of things, listed below:
Set PDO::ATTR_EMULATE_PREPARES to true in my database.php config. This has absolutely no effect on the problem, and actually introduces another issue, whereby integers are cast as strings.
Set DB_HOST to 127.0.0.1 instead of localhost, so that it uses TCP instead of a UNIX socket. Again, this has no effect.
Set DB_SOCKET to the socket path listed in MySQL by logging into MySQL (MariaDB) and running show variables like '%socket%'; which lists the socket path as /run/mysqld/mysqld.sock. I also leave DB_HOST set to localhost. This has no effect either. One thing I did note, was that the pdo_mysql.default_socket variable is set to /var/run/mysqld/mysqld.sock, I'm not sure if this is part of the problem?
I have massively increased the MySQL configuration settings found in /etc/mysql/mariadb.conf.d/50-server.cnf to the following:
key_buffer_size = 2048M
max_allowed_packet = 2048M
max_connections = 1000
thread_concurrency = 100
query_cache_size = 256M
I must admit, that changing these settings was a last resort/clutching at straws type scenario. However, this did alleviate the issue to some degree, but it did not fix it completely, as MySQL still fails 99% of the time, albeit at a later stage.
In terms of the queue, I have a total of 1,136 workers split between 6 supervisors/queues and it's all handled via Laravel Horizon, which is being run as a Daemon.
I am also using the Laravel Websockets PHP package for broadcasting, again, which is also being run as a Daemon.
My current environment configuration is as follows (sensitive info omitted).
APP_NAME=&quot;App Name&quot;
APP_ENV=production
APP_DEBUG=false
APP_KEY=thekey
APP_URL=https://appurl.com
LOG_CHANNEL=single
DB_CONNECTION=mysql
DB_HOST=127.0.0.1
DB_PORT=3306
DB_DATABASE=databse
DB_USERNAME=username
DB_PASSWORD=password
BROADCAST_DRIVER=pusher
CACHE_DRIVER=file
QUEUE_CONNECTION=redis
SESSION_DRIVER=file
SESSION_LIFETIME=120
REDIS_HOST=127.0.0.1
REDIS_PASSWORD=null
REDIS_PORT=6379
MAIL_MAILER=smtp
MAIL_HOST=smtp.gmail.com
MAIL_PORT=587
MAIL_USERNAME=name@email.com
MAIL_PASSWORD=password
MAIL_ENCRYPTION=tls
MAIL_FROM_ADDRESS=name@email.com
MAIL_FROM_NAME=&quot;${APP_NAME}&quot;
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=&quot;us-east-1&quot;
AWS_BUCKET=
PUSHER_APP_ID=appid
PUSHER_APP_KEY=appkey
PUSHER_APP_SECRET=appsecret
PUSHER_APP_CLUSTER=mt1
MIX_PUSHER_APP_KEY=&quot;${PUSHER_APP_KEY}&quot;
MIX_PUSHER_APP_CLUSTER=&quot;${PUSHER_APP_CLUSTER}&quot;
AUTHY_SECRET=
CASHIER_CURRENCY=usd
CASHIER_CURRENCY_LOCALE=en
CASHIER_MODEL=App\Models\User
STRIPE_KEY=stripekey
STRIPE_SECRET=stripesecret
# ECHO SERVER
LARAVEL_WEBSOCKETS_PORT=port
The server setup is as follows:
Max File Upload Size: 1024
Max Execution Time: 300
PHP Version: 7.4
MariaDB Version: 10.3.22
I have checked all logs (see below) at the time the MySQL server crashes/goes away, and there is nothing in the MySQL logs at all. No error whatsoever. I also don't see anything in:
/var/log/nginx/error.log
/var/log/nginx/access.log
/var/log/php7.4-fpm.log
I'm currently still digging through and debugging, but right now, I'm stumped. This is the first time I've ever come across this error.
Could this be down to hitting the database (read/write) too fast?
A little information on how the queues work.
I have an initial controller that dispatches a job to the queue.
Once this job completes, it fires an event which then starts the process of running several other listeners/events in sequence, all of which depend on the previous jobs completing before new events are fired and new listeners/jobs take up the work.
In total, there are 30 events that are broadcast.
In total, there are 30 listeners.
In total there are 5 jobs.
These all work sequentially based on the listener/job that was run and the event that it fires.
I have also monitored the laravel.log live and when the crash occurs, nothing is logged at all. Although, I do occasionally get production.ERROR: Failed to connect to Pusher. whether MySQL crashes or not, so I don't think that has any bearing on this problem.
I even noticed that the Laravel API rate limit was being hit, so I made sure to drastically increase that from 60 to 500. Still no joy.
Lastly, it doesn't seem to matter which Event, Job, or Listener is running as the error occurs on random ones. So, not sure it's code-specific, although, it may well be.
Hopefully, I've provided enough background and detailed information to get some help with this, but if I've missed anything, please do let me know and I'll add it to the question. Thanks.
","<pp><myself><travel><sockets><do>, pdoexception: packet order. expect 0 receive 1. packet size=23, travel spark project use horizon manage job queue red. locally, (on homestead box, mac os) every work expected, new digit ocean (for provisioned) droplets, memory-optic 256gb, 32vcpus, 10tb, x 800gb vs, keep get error: pdoexception: packet order. expect 0 receive 1. packet size=23 variant error, packet size into may different. man hours/day debut research, come across man post stackoverflow elsewhere, seem india fix number things, list below: set do::attr_emulate_prepar true database.pp confirm. absolute effect problem, actual introduce not issue, whereby inter cast strings. set db_host 127.0.0.1 instead localhost, use top instead unit socket. again, effect. set db_socket socket path list myself log myself (maria) run show variable like '%socket%'; list socket path /run/myself/myself.sock. also leave db_host set localhost. effect either. one thing note, pdo_mysql.default_socket variable set /war/run/myself/myself.sock, i'm sure part problem? massive increase myself configur set found /etc/myself/maria.cone.d/50-server.cf following: key_buffer_s = 2048m max_allowed_packet = 2048m max_connect = 1000 thread_concurr = 100 query_cache_s = 256m must admit, change set last resort/clutch straw type scenario. however, alley issue degree, fix completely, myself still fail 99% time, albeit later stage. term queue, total 1,136 worker split 6 supervisors/que hand via travel horizon, run demon. also use travel websocket pp package broadcasting, again, also run demon. current environs configur follow (sent into omitted). app_name=&quit;pp name&quit; app_env=product app_debug=fall app_key=they app_url=http://appeal.com log_channel=single db_connection=myself db_host=127.0.0.1 duport=3306 db_database=data db_username=usernam db_password=password broadcast_driver=push cache_driver=fig queue_connection=red session_driver=fig session_lifetime=120 redis_host=127.0.0.1 redis_password=null redis_port=6379 mail_mailer=sat mail_host=sat.email.com mail_port=587 mail_username=name@email.com mail_password=password mail_encryption=to mail_from_address=name@email.com mail_from_name=&quit;${app_name}&quit; aws_access_key_id= aws_secret_access_key= aws_default_region=&quit;us-east-1&quit; aws_bucket= pusher_app_id=applied pusher_app_key=apply pusher_app_secret=appsecret pusher_app_cluster=mt mix_pusher_app_key=&quit;${pusher_app_key}&quit; mix_pusher_app_cluster=&quit;${pusher_app_cluster}&quit; authy_secret= cashier_currency=us cashier_currency_locale=en cashier_model=pp\models\us stripe_key=stripekey stripe_secret=stripesecret # echo server laravel_websockets_port=port server set follows: max file unload size: 1024 max execute time: 300 pp version: 7.4 maria version: 10.3.22 check log (see below) time myself server clashes/go away, not myself log all. error whatsoever. also see any in: /war/log/nine/error.log /war/log/nine/access.log /war/log/pp.4-pm.log i'm current still dig debugging, right now, i'm stamped. first time i'v ever come across error. could hit database (read/write) fast? little inform queue work. into control dispatch job queue. job complete, fire event start process run never listeners/ev sequence, depend previous job complete new event fire new listeners/job take work. total, 30 event broadcast. total, 30 listeners. total 5 jobs. work sequence base listener/job run event fires. also monitor travel.log live crash occurs, not log all. although, occasion get production.error: fail connect pushed. whether myself crash not, think bear problem. even notice travel apt rate limit hit, made sure drastic increase 60 500. still joy. lastly, seem matter event, job, listen run error occur random ones. so, sure code-specific, although, may well be. hopefully, i'v proved enough background detail inform get help this, i'v miss anything, pleas let know i'll add question. thanks."
64061101,"Microsoft SQL Server - best way to 'Update if exists, or Insert'","I've been searching around for the answers to this question, and there's some conflicting or ambiguous information out there, finding it hard to find a for-sure answer.
My context: I'm in node.js using the 'mssql' npm package. My SQL server is Microsoft SQL Server 2014.
I have a record that may or may not exist in a table already -- if it exists I want to update it, otherwise I want to insert it. I'm not sure what the optimal SQL is, or if there's some kind of 'transaction' I should be running in mssql. I've found some options that seem good, but I'm not sure about any of them:
Option 1:
how to update if exists or insert
Problem with this is I'm not even sure this is valid syntax in MSSQL. I do like it though, and it seems to support doing multiple rows at once too which I like.
INSERT INTO table (id, user, date, points)
    VALUES (1, 1, '2017-03-03', 25),
           (2, 1, '2017-03-04', 25),
           (3, 2, '2017-03-03', 100),
           (4, 2, '2017-03-04', 150)
    ON DUPLICATE KEY UPDATE points = VALUES(points)
Option 2:
don't know if there's any problem with this one, just not sure if it's optimal. Doesn't seem to support multiple simultaneous rows
update test set name='john' where id=3012
IF @@ROWCOUNT=0
   insert into test(name) values('john');
Option 3: Merge, https://dba.stackexchange.com/questions/89696/how-to-insert-or-update-using-single-query
Some people say this is a bit buggy or something? This also apparently supports multiple at once which I like.
MERGE dbo.Test WITH (SERIALIZABLE) AS T
USING (VALUES (3012, 'john')) AS U (id, name)
    ON U.id = T.id
WHEN MATCHED THEN 
    UPDATE SET T.name = U.name
WHEN NOT MATCHED THEN
    INSERT (id, name) 
    VALUES (U.id, U.name);
",<sql><sql-server>,1719,2,17,13274,3,40,77,46,24210,0.0,996,3,21,2020-09-25 9:05,2020-09-25 9:18,2020-09-25 9:18,0.0,0.0,Advanced,32,"<sql><sql-server>, Microsoft SQL Server - best way to 'Update if exists, or Insert', I've been searching around for the answers to this question, and there's some conflicting or ambiguous information out there, finding it hard to find a for-sure answer.
My context: I'm in node.js using the 'mssql' npm package. My SQL server is Microsoft SQL Server 2014.
I have a record that may or may not exist in a table already -- if it exists I want to update it, otherwise I want to insert it. I'm not sure what the optimal SQL is, or if there's some kind of 'transaction' I should be running in mssql. I've found some options that seem good, but I'm not sure about any of them:
Option 1:
how to update if exists or insert
Problem with this is I'm not even sure this is valid syntax in MSSQL. I do like it though, and it seems to support doing multiple rows at once too which I like.
INSERT INTO table (id, user, date, points)
    VALUES (1, 1, '2017-03-03', 25),
           (2, 1, '2017-03-04', 25),
           (3, 2, '2017-03-03', 100),
           (4, 2, '2017-03-04', 150)
    ON DUPLICATE KEY UPDATE points = VALUES(points)
Option 2:
don't know if there's any problem with this one, just not sure if it's optimal. Doesn't seem to support multiple simultaneous rows
update test set name='john' where id=3012
IF @@ROWCOUNT=0
   insert into test(name) values('john');
Option 3: Merge, https://dba.stackexchange.com/questions/89696/how-to-insert-or-update-using-single-query
Some people say this is a bit buggy or something? This also apparently supports multiple at once which I like.
MERGE dbo.Test WITH (SERIALIZABLE) AS T
USING (VALUES (3012, 'john')) AS U (id, name)
    ON U.id = T.id
WHEN MATCHED THEN 
    UPDATE SET T.name = U.name
WHEN NOT MATCHED THEN
    INSERT (id, name) 
    VALUES (U.id, U.name);
","<sal><sal-server>, microsoft sal server - best way 'update exists, insert', i'v search around answer question, there' conflict ambigu inform there, find hard find for-sur answer. context: i'm node.j use 'mssql' nom package. sal server microsoft sal server 2014. record may may exist table already -- exist want update it, otherwise want insert it. i'm sure optic sal is, there' kind 'transaction' run mssql. i'v found option seem good, i'm sure them: option 1: update exist insert problem i'm even sure valid santa mssql. like though, seem support multiple row like. insert table (id, user, date, points) value (1, 1, '2017-03-03', 25), (2, 1, '2017-03-04', 25), (3, 2, '2017-03-03', 100), (4, 2, '2017-03-04', 150) public key update point = values(points) option 2: know there' problem one, sure optical. seem support multiple simulate row update test set name='john' id=3012 @@rowcount=0 insert test(name) values('john'); option 3: merge, http://da.stackexchange.com/questions/89696/how-to-insert-or-update-using-single-query people say bit buggy something? also appear support multiple like. berg do.test (serializable) use (value (3012, 'john')) u (id, name) u.id = t.id match update set t.name = u.am match insert (id, name) value (u.id, u.name);"
62821983,"TypeORM: ""No migrations pending"" when attempting to run migrations manually","I have a new web app and I've written a migrator to create a user table. However, no matter what I try, typeorm does not appear to find this migrator and hence, does not run it.
My file structure (other files/folders not shown):
├── Server
│   ├── dist
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.js
|   |   |   ├── 1234567891234567890-AddUserTable.js.map
|   |   |   ├── 1234567891234567890-AddUserTable.d.ts
│   ├── src
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.ts
|   |   ├── app.module.ts
app.module.ts
@Module({
    imports: [
        ConfigModule.forRoot({ envFilePath: '.env' }),
        TypeOrmModule.forRootAsync({
            imports: [ConfigModule],
            useFactory: (configService: ConfigService) =&gt; ({
                type: 'mysql',
                host: configService.get('TYPEORM_HOST'),
                port: +configService.get&lt;number&gt;('TYPEORM_PORT'),
                username: configService.get('TYPEORM_USERNAME'),
                password: configService.get('TYPEORM_PASSWORD'),
                database: configService.get('TYPEORM_DATABASE'),
                synchronize: configService.get('TYPEORM_SYNCHRONIZE'),
                entities: [__dirname + '/**/*.entity{.ts,.js}'],
                migrations: [__dirname + '/Migrations/**/*.js'],
                migrationsRun: false,
                cli: {
                    migrationsDir: './Migrations',
                },
            }),
            inject: [ConfigService],
        }),
    ],
    controllers: [],
    providers: [],
})
export class AppModule {
    constructor(private connection: Connection) {}
}
In order to run this, in my console window, I type: nest start in order get my Server started.
Then, I run npx typeorm migration:run which I get:
query: SELECT * FROM `INFORMATION_SCHEMA`.`COLUMNS` WHERE `TABLE_SCHEMA` = 'myDB' AND `TABLE_NAME` = 'migrations'
query: SELECT * FROM `myDB`.`migrations` `migrations` ORDER BY `id` DESC
No migrations are pending
If I look in my DB, I see a migrations table with no entries inside.
I have tried to delete my migrator file and create it again with a more recent timestamp and that does not work either.
npx typeorm migration:create -n &quot;MyMigratorName&quot;
Any help would be greatly appreciated.
",<mysql><nestjs><typeorm>,2309,0,46,10553,24,79,146,65,25335,0.0,234,5,21,2020-07-09 19:07,2020-08-27 13:40,,49.0,,Advanced,32,"<mysql><nestjs><typeorm>, TypeORM: ""No migrations pending"" when attempting to run migrations manually, I have a new web app and I've written a migrator to create a user table. However, no matter what I try, typeorm does not appear to find this migrator and hence, does not run it.
My file structure (other files/folders not shown):
├── Server
│   ├── dist
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.js
|   |   |   ├── 1234567891234567890-AddUserTable.js.map
|   |   |   ├── 1234567891234567890-AddUserTable.d.ts
│   ├── src
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.ts
|   |   ├── app.module.ts
app.module.ts
@Module({
    imports: [
        ConfigModule.forRoot({ envFilePath: '.env' }),
        TypeOrmModule.forRootAsync({
            imports: [ConfigModule],
            useFactory: (configService: ConfigService) =&gt; ({
                type: 'mysql',
                host: configService.get('TYPEORM_HOST'),
                port: +configService.get&lt;number&gt;('TYPEORM_PORT'),
                username: configService.get('TYPEORM_USERNAME'),
                password: configService.get('TYPEORM_PASSWORD'),
                database: configService.get('TYPEORM_DATABASE'),
                synchronize: configService.get('TYPEORM_SYNCHRONIZE'),
                entities: [__dirname + '/**/*.entity{.ts,.js}'],
                migrations: [__dirname + '/Migrations/**/*.js'],
                migrationsRun: false,
                cli: {
                    migrationsDir: './Migrations',
                },
            }),
            inject: [ConfigService],
        }),
    ],
    controllers: [],
    providers: [],
})
export class AppModule {
    constructor(private connection: Connection) {}
}
In order to run this, in my console window, I type: nest start in order get my Server started.
Then, I run npx typeorm migration:run which I get:
query: SELECT * FROM `INFORMATION_SCHEMA`.`COLUMNS` WHERE `TABLE_SCHEMA` = 'myDB' AND `TABLE_NAME` = 'migrations'
query: SELECT * FROM `myDB`.`migrations` `migrations` ORDER BY `id` DESC
No migrations are pending
If I look in my DB, I see a migrations table with no entries inside.
I have tried to delete my migrator file and create it again with a more recent timestamp and that does not work either.
npx typeorm migration:create -n &quot;MyMigratorName&quot;
Any help would be greatly appreciated.
","<myself><nests><typeorm>, typeorm: ""no migrate pending"" attempt run migrate mentally, new web pp i'v written migrate great user table. however, matter try, typeorm appear find migrate hence, run it. file structure (other files/fold shown): ├── server │ ├── list | | ├── migrate | | | ├── 1234567891234567890-addusertable.j | | | ├── 1234567891234567890-addusertable.is.map | | | ├── 1234567891234567890-addusertable.d.t │ ├── sac | | ├── migrate | | | ├── 1234567891234567890-addusertable.t | | ├── pp.module.t pp.module.t @module({ imports: [ configmodule.forgot({ envfilepath: '.end' }), typeormmodule.forrootasync({ imports: [configmodule], usefactory: (configservice: configservice) =&it; ({ type: 'myself', host: configservice.get('typeorm_host'), port: +configservice.get&it;number&it;('typeorm_port'), surname: configservice.get('typeorm_username'), password: configservice.get('typeorm_password'), database: configservice.get('typeorm_database'), synchronize: configservice.get('typeorm_synchronize'), entitles: [__dirnam + '/**/*.entity{.to,.is}'], migrations: [__dirnam + '/migrations/**/*.is'], migrationsrun: false, coli: { migrationsdir: './migrations', }, }), inject: [configservice], }), ], controller: [], provides: [], }) export class appmodul { construction(prim connection: connection) {} } order run this, console window, type: nest start order get server started. then, run not typeorm migration:run get: query: select * `information_schema`.`columns` `table_schema` = 'my' `table_name` = 'migrations' query: select * `my`.`migrations` `migrations` order `id` desk migrate end look do, see migrate table entry inside. try delete migrate file great recent timestamp work either. not typeorm migration:or -n &quit;mymigratorname&quit; help would greatly appreciated."
50456780,Run MySQL on Port 3307 Using Docker Compose,"I am trying to create multiple Prisma database services on a single machine. I have been unable to create a MySQL database on a port other than 3306 using Docker Compose. 
docker-compose.yml 
version: '3'
services:
hackernews:
    image: prismagraphql/prisma:1.8
    restart: always
    ports:
    - ""${CLIENT_PORT}:${INTERNAL_PORT}""
    environment:
    PRISMA_CONFIG: |
        port: $INTERNAL_PORT
        managementApiSecret: $PRISMA_MANAGEMENT_API_SECRET
        databases:
        default:
            connector: mysql
            host: mysql
            port: $SQL_INTERNAL_PORT
            user: root
            password: $SQL_PASSWORD
            migrations: true
mysql:
    image: mysql:5.7
    restart: always
    environment:
    MYSQL_ROOT_PASSWORD: $SQL_PASSWORD
    volumes:
    - ./custom/:/etc/mysql/conf.d/my.cnf
    - mysql:/var/lib/mysql
volumes:
mysql:
docker-compose.override.yml 
version: '3'
services:
mysql:
    expose:
    - ""${SQL_INTERNAL_PORT}""
    ports:
    - ""${SQL_CLIENT_PORT}:${SQL_INTERNAL_PORT}""
Error:
hackernews_1  | Exception in thread ""main"" java.sql.SQLTransientConnectionException: database - Connection is not available, request timed out after 5008ms.
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145)
hackernews_1  |     at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83)
hackernews_1  |     at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18)
hackernews_1  |     at slick.jdbc.JdbcBackend$BaseSession.&lt;init&gt;(JdbcBackend.scala:439)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
hackernews_1  |     at java.lang.Thread.run(Thread.java:748)
hackernews_1  | Caused by: java.sql.SQLNonTransientConnectionException: Could not connect to address=(host=mysql)(port=3307)(type=master) : Connection refused (Connection refused)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.get(ExceptionMapper.java:161)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.connException(ExceptionMapper.java:79)
hackernews_1  |     at org.mariadb.jdbc.internal.protocol.AbstractConnectProtocol.connectWithoutProxy(AbstractConnectProtocol.java:1040)
hackernews_1  |     at org.mariadb.jdbc.internal.util.Utils.retrieveProxy(Utils.java:490)
hackernews_1  |     at org.mariadb.jdbc.MariaDbConnection.newConnection(MariaDbConnection.java:144)
hackernews_1  |     at org.mariadb.jdbc.Driver.connect(Driver.java:90)
hackernews_1  |     at slick.jdbc.DriverDataSource.getConnection(DriverDataSource.scala:101)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563)
hackernews_1  |     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
docker ps 
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                                  PORTS                              NAMES
ab721996469d        mysql:5.7                  ""docker-entrypoint.s…""   42 minutes ago      Up 55 seconds                           3306/tcp, 0.0.0.0:3307-&gt;3307/tcp   two_mysql_1
7aab98e2b8d7        prismagraphql/prisma:1.8   ""/bin/sh -c /app/sta…""   2 hours ago         Restarting (1) Less than a second ago                                      two_hackernews_1
.env
SQL_PASSWORD=myuniquepassword
SQL_INTERNAL_PORT=3307
SQL_CLIENT_PORT=3307
",<mysql><docker><docker-compose><prisma>,4804,0,73,343,2,3,9,40,52457,0.0,0,3,21,2018-05-21 21:21,2018-05-21 22:17,2018-05-21 22:17,0.0,0.0,Basic,10,"<mysql><docker><docker-compose><prisma>, Run MySQL on Port 3307 Using Docker Compose, I am trying to create multiple Prisma database services on a single machine. I have been unable to create a MySQL database on a port other than 3306 using Docker Compose. 
docker-compose.yml 
version: '3'
services:
hackernews:
    image: prismagraphql/prisma:1.8
    restart: always
    ports:
    - ""${CLIENT_PORT}:${INTERNAL_PORT}""
    environment:
    PRISMA_CONFIG: |
        port: $INTERNAL_PORT
        managementApiSecret: $PRISMA_MANAGEMENT_API_SECRET
        databases:
        default:
            connector: mysql
            host: mysql
            port: $SQL_INTERNAL_PORT
            user: root
            password: $SQL_PASSWORD
            migrations: true
mysql:
    image: mysql:5.7
    restart: always
    environment:
    MYSQL_ROOT_PASSWORD: $SQL_PASSWORD
    volumes:
    - ./custom/:/etc/mysql/conf.d/my.cnf
    - mysql:/var/lib/mysql
volumes:
mysql:
docker-compose.override.yml 
version: '3'
services:
mysql:
    expose:
    - ""${SQL_INTERNAL_PORT}""
    ports:
    - ""${SQL_CLIENT_PORT}:${SQL_INTERNAL_PORT}""
Error:
hackernews_1  | Exception in thread ""main"" java.sql.SQLTransientConnectionException: database - Connection is not available, request timed out after 5008ms.
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145)
hackernews_1  |     at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83)
hackernews_1  |     at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18)
hackernews_1  |     at slick.jdbc.JdbcBackend$BaseSession.&lt;init&gt;(JdbcBackend.scala:439)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
hackernews_1  |     at java.lang.Thread.run(Thread.java:748)
hackernews_1  | Caused by: java.sql.SQLNonTransientConnectionException: Could not connect to address=(host=mysql)(port=3307)(type=master) : Connection refused (Connection refused)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.get(ExceptionMapper.java:161)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.connException(ExceptionMapper.java:79)
hackernews_1  |     at org.mariadb.jdbc.internal.protocol.AbstractConnectProtocol.connectWithoutProxy(AbstractConnectProtocol.java:1040)
hackernews_1  |     at org.mariadb.jdbc.internal.util.Utils.retrieveProxy(Utils.java:490)
hackernews_1  |     at org.mariadb.jdbc.MariaDbConnection.newConnection(MariaDbConnection.java:144)
hackernews_1  |     at org.mariadb.jdbc.Driver.connect(Driver.java:90)
hackernews_1  |     at slick.jdbc.DriverDataSource.getConnection(DriverDataSource.scala:101)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563)
hackernews_1  |     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
docker ps 
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                                  PORTS                              NAMES
ab721996469d        mysql:5.7                  ""docker-entrypoint.s…""   42 minutes ago      Up 55 seconds                           3306/tcp, 0.0.0.0:3307-&gt;3307/tcp   two_mysql_1
7aab98e2b8d7        prismagraphql/prisma:1.8   ""/bin/sh -c /app/sta…""   2 hours ago         Restarting (1) Less than a second ago                                      two_hackernews_1
.env
SQL_PASSWORD=myuniquepassword
SQL_INTERNAL_PORT=3307
SQL_CLIENT_PORT=3307
","<myself><doctor><doctor-compose><prima>, run myself port 3307 use doctor compose, try great multiple prima database service single machine. unable great myself database port 3306 use doctor compose. doctor-compose.you version: '3' services: hackernews: image: prismagraphql/prima:1.8 start: away ports: - ""${client_port}:${internal_port}"" environment: prisma_config: | port: $internal_port managementapisecret: $prisma_management_api_secret database: default: connection: myself host: myself port: $sql_internal_port user: root password: $sql_password migrations: true myself: image: myself:5.7 start: away environment: mysql_root_password: $sql_password volumes: - ./custom/:/etc/myself/cone.d/my.cf - myself:/war/limb/myself volumes: myself: doctor-compose.overrule.you version: '3' services: myself: expose: - ""${sql_internal_port}"" ports: - ""${sql_client_port}:${sql_internal_port}"" error: hackernews_1 | except thread ""main"" cava.sal.sqltransientconnectionexception: database - connect available, request time 5008ms. hackernews_1 | com.baxter.kari.pool.hikaripool.createtimeoutexception(hikaripool.cava:548) hackernews_1 | com.baxter.kari.pool.hikaripool.getconnection(hikaripool.cava:186) hackernews_1 | com.baxter.kari.pool.hikaripool.getconnection(hikaripool.cava:145) hackernews_1 | com.baxter.kari.hikaridatasource.getconnection(hikaridatasource.cava:83) hackernews_1 | sick.job.hikaricp.hikaricpjdbcdatasource.createconnection(hikaricpjdbcdatasource.scala:18) hackernews_1 | sick.job.jdbcbackend$basesession.&it;knit&it;(jdbcbackend.scala:439) hackernews_1 | sick.job.jdbcbackend$databasedef.createsession(jdbcbackend.scala:47) hackernews_1 | sick.job.jdbcbackend$databasedef.createsession(jdbcbackend.scala:38) hackernews_1 | sick.basic.basicbackend$databasedef.acquiresession(basicbackend.scala:218) hackernews_1 | sick.basic.basicbackend$databasedef.acquiresession$(basicbackend.scala:217) hackernews_1 | sick.job.jdbcbackend$databasedef.acquiresession(jdbcbackend.scala:38) hackernews_1 | sick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:239) hackernews_1 | cava.until.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.cava:1149) hackernews_1 | cava.until.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.cava:624) hackernews_1 | cava.long.thread.run(thread.cava:748) hackernews_1 | cause by: cava.sal.sqlnontransientconnectionexception: could connect address=(host=myself)(port=3307)(type=master) : connect refuse (connect refused) hackernews_1 | org.maria.job.internal.until.exceptions.exceptionmapper.get(exceptionmapper.cava:161) hackernews_1 | org.maria.job.internal.until.exceptions.exceptionmapper.connexception(exceptionmapper.cava:79) hackernews_1 | org.maria.job.internal.protocol.abstractconnectprotocol.connectwithoutproxy(abstractconnectprotocol.cava:1040) hackernews_1 | org.maria.job.internal.until.still.retrieveproxy(still.cava:490) hackernews_1 | org.maria.job.mariadbconnection.newconnection(mariadbconnection.cava:144) hackernews_1 | org.maria.job.driver.connect(driver.cava:90) hackernews_1 | sick.job.driverdatasource.getconnection(driverdatasource.scala:101) hackernews_1 | com.baxter.kari.pool.poolbase.newconnection(poolbase.cava:341) hackernews_1 | com.baxter.kari.pool.poolbase.newpoolentry(poolbase.cava:193) hackernews_1 | com.baxter.kari.pool.hikaripool.createpoolentry(hikaripool.cava:430) hackernews_1 | com.baxter.kari.pool.hikaripool.access$500(hikaripool.cava:64) hackernews_1 | com.baxter.kari.pool.hikaripool$poolentrycreator.call(hikaripool.cava:570) hackernews_1 | com.baxter.kari.pool.hikaripool$poolentrycreator.call(hikaripool.cava:563) hackernews_1 | cava.until.concurrent.futuretask.run(futuretask.cava:266) doctor is contain id image command great state port name ab721996469d myself:5.7 ""doctor-entrypoint.s…"" 42 minute ago 55 second 3306/top, 0.0.0.0:3307-&it;3307/top two_mysql_1 7aab98e2b8d7 prismagraphql/prima:1.8 ""/bin/s -c /pp/sta…"" 2 hour ago start (1) less second ago two_hackernews_1 .end sql_password=myuniquepassword sql_internal_port=3307 sql_client_port=3307"
56760683,"Windows API call ""RegGetValueW"" returned error code: 0","I am getting Windows API Call Error Code :  0 
I have installed SSMS 2018 and was trying to find out if I have any other versions installed. For that I ran sqllocaldb versions in CMD but got the following message:
  Windows API call ""RegGetValueW"" returned error code: 0.
When I checked manually (via Control Panel), I saw that I have 2015 and 2016 versions installed. So Why it are they not showing in CMD. 
I tried to find other solutions but found nothing that made sense to me. 
",<sql-server><ssms><localdb>,483,0,1,221,0,3,6,57,8219,0.0,0,3,21,2019-06-25 19:16,2019-10-07 18:49,,104.0,,Basic,14,"<sql-server><ssms><localdb>, Windows API call ""RegGetValueW"" returned error code: 0, I am getting Windows API Call Error Code :  0 
I have installed SSMS 2018 and was trying to find out if I have any other versions installed. For that I ran sqllocaldb versions in CMD but got the following message:
  Windows API call ""RegGetValueW"" returned error code: 0.
When I checked manually (via Control Panel), I saw that I have 2015 and 2016 versions installed. So Why it are they not showing in CMD. 
I tried to find other solutions but found nothing that made sense to me. 
","<sal-server><sums><local>, window apt call ""reggetvaluew"" return error code: 0, get window apt call error code : 0 instal sum 2018 try find version installed. ran sqllocaldb version cod got follow message: window apt call ""reggetvaluew"" return error code: 0. check manual (via control panel), saw 2015 2016 version installed. show cod. try find slut found not made sens me."
65184035,Alembic ignore specific tables,"I'm using alembic to manage database migrations as per user defined sqlalchemy models. My challenge is that I'd like for alembic to ignore any creation, deletion, or changes to a specific set of tables.
Note: My Q is similar to this question Ignoring a model when using alembic autogenerate but is different in that I want to control alembic from outside the model definition.
Here's a sample table I want to ignore:
from sqlalchemy import MetaData
from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base(metadata=MetaData())
class Ignore1(Base):
    &quot;&quot;&quot;
    Signed in to the account...
    &quot;&quot;&quot;
    __tablename__ = 'ignore_1'
    __table_args__ = {
        'info':{'skip_autogenerate':True}
        }
    id = Column(Integer, primary_key=True)
    foo = Column(String(20), nullable=True)
Example code (which does not solve my issue): 
In alembic/env.py
# Ideally this is stored in my actual database, but for now, let's assume we have a list...
IGNORE_TABLES = ['ignore_1', 'ignore_2']
def include_object(object, name, type_, reflected, compare_to):
    &quot;&quot;&quot;
    Should you include this table or not?
    &quot;&quot;&quot;
    if type_ == 'table' and (name in IGNORE_TABLES or object.info.get(&quot;skip_autogenerate&quot;, False)):
        return False
    elif type_ == &quot;column&quot; and object.info.get(&quot;skip_autogenerate&quot;, False):
        return False
    return True
# Then add to config
context.configure(
    ...
    include_object=include_object,
    ...
    )
",<python><sqlalchemy><alembic>,1553,1,37,9698,2,51,73,68,6140,0.0,6370,1,21,2020-12-07 14:55,2020-12-11 22:23,2020-12-11 22:23,4.0,4.0,Basic,14,"<python><sqlalchemy><alembic>, Alembic ignore specific tables, I'm using alembic to manage database migrations as per user defined sqlalchemy models. My challenge is that I'd like for alembic to ignore any creation, deletion, or changes to a specific set of tables.
Note: My Q is similar to this question Ignoring a model when using alembic autogenerate but is different in that I want to control alembic from outside the model definition.
Here's a sample table I want to ignore:
from sqlalchemy import MetaData
from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base(metadata=MetaData())
class Ignore1(Base):
    &quot;&quot;&quot;
    Signed in to the account...
    &quot;&quot;&quot;
    __tablename__ = 'ignore_1'
    __table_args__ = {
        'info':{'skip_autogenerate':True}
        }
    id = Column(Integer, primary_key=True)
    foo = Column(String(20), nullable=True)
Example code (which does not solve my issue): 
In alembic/env.py
# Ideally this is stored in my actual database, but for now, let's assume we have a list...
IGNORE_TABLES = ['ignore_1', 'ignore_2']
def include_object(object, name, type_, reflected, compare_to):
    &quot;&quot;&quot;
    Should you include this table or not?
    &quot;&quot;&quot;
    if type_ == 'table' and (name in IGNORE_TABLES or object.info.get(&quot;skip_autogenerate&quot;, False)):
        return False
    elif type_ == &quot;column&quot; and object.info.get(&quot;skip_autogenerate&quot;, False):
        return False
    return True
# Then add to config
context.configure(
    ...
    include_object=include_object,
    ...
    )
","<patron><sqlalchemy><alembic>, limb ignore specie tables, i'm use limb manage database migrate per user define sqlalchemi models. challenge i'd like limb ignore creation, deletions, change specie set tables. note: q similar question ignore model use limb autogener differ want control limb outside model definition. here' sample table want ignore: sqlalchemi import metadata sqlalchemy.ext.declare import declarative_bas base = declarative_base(metadata=metadata()) class ignore(base): &quit;&quit;&quit; sign account... &quit;&quit;&quit; __tablename__ = 'ignore' __table_args__ = { 'into':{'skip_autogenerate':true} } id = column(inter, primary_key=true) foo = column(string(20), syllable=true) example code (which sole issue): alembic/end.i # ideal store actual database, now, let' assume list... ignorant = ['ignore', 'ignore'] def include_object(object, name, type, reflected, compare_to): &quit;&quit;&quit; include table not? &quit;&quit;&quit; type == 'table' (name ignorant object.into.get(&quit;skip_autogenerate&quit;, false)): return fall if type == &quit;column&quit; object.into.get(&quit;skip_autogenerate&quit;, false): return fall return true # add confirm context.configure( ... include_object=include_object, ... )"
52943627,Convert a pandas dataframe to a PySpark dataframe,"I have a script with the below setup.
I am using:
1) Spark dataframes to pull data in
2) Converting to pandas dataframes after initial aggregatioin
3) Want to convert back to Spark for writing to HDFS
The conversion from Spark --> Pandas was simple, but I am struggling with how to convert a Pandas dataframe back to spark.
Can you advise?
from pyspark.sql import SparkSession
import pyspark.sql.functions as sqlfunc
from pyspark.sql.types import *
import argparse, sys
from pyspark.sql import *
import pyspark.sql.functions as sqlfunc
import pandas as pd
def create_session(appname):
    spark_session = SparkSession\
        .builder\
        .appName(appname)\
        .master('yarn')\
        .config(""hive.metastore.uris"", ""thrift://uds-far-mn1.dab.02.net:9083"")\
        .enableHiveSupport()\
        .getOrCreate()
    return spark_session
### START MAIN ###
if __name__ == '__main__':
    spark_session = create_session('testing_files')
I've tried the below - no errors, just no data! To confirm, df6 does have data &amp; is a pandas dataframe
df6 = df5.sort_values(['sdsf'], ascending=[""true""])
sdf = spark_session.createDataFrame(df6)
sdf.show()
",<python-3.x><pandas><pyspark><apache-spark-sql>,1156,0,23,1906,2,23,46,51,67478,,122,1,21,2018-10-23 7:40,2018-10-23 13:05,2018-10-23 13:05,0.0,0.0,Basic,6,"<python-3.x><pandas><pyspark><apache-spark-sql>, Convert a pandas dataframe to a PySpark dataframe, I have a script with the below setup.
I am using:
1) Spark dataframes to pull data in
2) Converting to pandas dataframes after initial aggregatioin
3) Want to convert back to Spark for writing to HDFS
The conversion from Spark --> Pandas was simple, but I am struggling with how to convert a Pandas dataframe back to spark.
Can you advise?
from pyspark.sql import SparkSession
import pyspark.sql.functions as sqlfunc
from pyspark.sql.types import *
import argparse, sys
from pyspark.sql import *
import pyspark.sql.functions as sqlfunc
import pandas as pd
def create_session(appname):
    spark_session = SparkSession\
        .builder\
        .appName(appname)\
        .master('yarn')\
        .config(""hive.metastore.uris"", ""thrift://uds-far-mn1.dab.02.net:9083"")\
        .enableHiveSupport()\
        .getOrCreate()
    return spark_session
### START MAIN ###
if __name__ == '__main__':
    spark_session = create_session('testing_files')
I've tried the below - no errors, just no data! To confirm, df6 does have data &amp; is a pandas dataframe
df6 = df5.sort_values(['sdsf'], ascending=[""true""])
sdf = spark_session.createDataFrame(df6)
sdf.show()
","<patron-3.x><hands><spark><apache-spark-sal>, convert and datafram spark dataframe, script set. using: 1) spark datafram pull data 2) convert and datafram into aggregation 3) want convert back spark write of covers spark --> and simple, struggle convert and datafram back spark. advise? spark.sal import sparkles import spark.sal.fact sqlfunc spark.sal.type import * import argparse, by spark.sal import * import spark.sal.fact sqlfunc import and pp def create_session(appease): spark_sess = sparksession\ .builder\ .appease(appease)\ .master('yarn')\ .confirm(""hive.metastore.oris"", ""thrifty://us-far-and.day.02.net:9083"")\ .enablehivesupport()\ .getorcreate() return spark_sess ### start main ### __name__ == '__main__': spark_sess = create_session('testing_files') i'v try - errors, data! confirm, of data &amp; and datafram of = of.sort_values(['self'], ascending=[""true""]) sd = spark_session.createdataframe(of) sd.show()"
53634650,Hash function in spark,"I'm trying to add a column to a dataframe, which will contain hash of another column.
I've found this piece of documentation:
https://spark.apache.org/docs/2.3.0/api/sql/index.html#hash
And tried this:  
import org.apache.spark.sql.functions._
val df = spark.read.parquet(...)
val withHashedColumn = df.withColumn(""hashed"", hash($""my_column""))
But what is the hash function used by that hash()? Is that murmur, sha, md5, something else?  
The value I get in this column is integer, thus range of values here is probably [-2^(31) ... +2^(31-1)].
Can I get a long value here? Can I get a string hash instead?
How can I specify a concrete hashing algorithm for that?
Can I use a custom hash function?
",<scala><apache-spark><hash><apache-spark-sql>,698,2,8,4199,6,47,66,68,27595,0.0,2310,2,21,2018-12-05 14:34,2019-05-23 14:38,,169.0,,Basic,3,"<scala><apache-spark><hash><apache-spark-sql>, Hash function in spark, I'm trying to add a column to a dataframe, which will contain hash of another column.
I've found this piece of documentation:
https://spark.apache.org/docs/2.3.0/api/sql/index.html#hash
And tried this:  
import org.apache.spark.sql.functions._
val df = spark.read.parquet(...)
val withHashedColumn = df.withColumn(""hashed"", hash($""my_column""))
But what is the hash function used by that hash()? Is that murmur, sha, md5, something else?  
The value I get in this column is integer, thus range of values here is probably [-2^(31) ... +2^(31-1)].
Can I get a long value here? Can I get a string hash instead?
How can I specify a concrete hashing algorithm for that?
Can I use a custom hash function?
","<scala><apache-spark><has><apache-spark-sal>, has function spark, i'm try add column dataframe, contain has not column. i'v found piece documentation: http://spark.apache.org/docs/2.3.0/apt/sal/index.html#has try this: import org.apache.spark.sal.functions._ val of = spark.read.parquet(...) val withhashedcolumn = of.withcolumn(""washed"", has($""my_column"")) has function use has()? murmur, she, md, cometh else? value get column inter, the rang value probably [-2^(31) ... +2^(31-1)]. get long value here? get string has instead? specific concert has algorithm that? use custom has function?"
48348366,"Eloquent join using ""USING"" clause with N query","I'm using Slim Framework with Illuminate Database.
I want to make JOIN query with USING clause. Let's say given Sakila database. Diagram:
How to make join with USING clause (not ON) in eloquent model?
SELECT film_id,title,first_name,last_name 
FROM film_actor 
INNER join film USING(film_id) -- notice 
INNER join actor USING(actor_id) -- notice 
What I want is an eager loading with EXACT 1 query. The use of eloquent relationships described in the API is not meeting my expectation, since any eager relation use N+1 query. I want to make it less IO to database.
FilmActor model :
class FilmActor extends Model
{
    protected $table = 'film_actor';
    protected $primaryKey = [&quot;actor_id&quot;, &quot;film_id&quot;];
    protected $incrementing = false;
    protected $appends = ['full_name'];
    // i need to make it in Eloquent model way, so it easier to manipulate
    public function getFullNameAttribute()  
    {
        $fn = &quot;&quot;;
        $fn .= isset($this-&gt;first_name) ? $this-&gt;first_name .&quot; &quot;: &quot;&quot;;
        $fn .= isset($this-&gt;last_name) ? $this-&gt;last_name .&quot; &quot;: &quot;&quot;;
        return $fn; 
    }
    public function allJoin()
    {
        // how to join with &quot;USING&quot; clause ?
        return self::select([&quot;film.film_id&quot;,&quot;title&quot;,&quot;first_name&quot;,&quot;last_name&quot;])
            -&gt;join(&quot;film&quot;, &quot;film_actor.film_id&quot;, '=', 'film.film_id')  
            -&gt;join(&quot;actor&quot;, &quot;film_actor.actor_id&quot;, '=', 'actor.actor_id');  
        //something like
        //return self::select(&quot;*&quot;)-&gt;joinUsing(&quot;film&quot;,[&quot;film_id&quot;]);
        //or
        //return self::select(&quot;*&quot;)-&gt;join(&quot;film&quot;,function($join){
        //    $join-&gt;using(&quot;film_id&quot;);
        //});
    }
}
So, in the controller I can get the data like
$data = FilmActor::allJoin()  
        -&gt;limit(100)  
        -&gt;get();`  
But there's a con, if I need to add extra behavior (like where or order).
$data = FilmActor::allJoin()
        -&gt;where(&quot;film.film_id&quot;,&quot;1&quot;)   
        -&gt;orderBy(&quot;film_actor.actor_id&quot;)  
        -&gt;limit(100)  
        -&gt;get();`  
I need to pass table name to avoid ambiguous field. Not good. So I want for further use, I can do
$kat = $request-&gt;getParam(&quot;kat&quot;,&quot;first_name&quot;);  
// [&quot;film_id&quot;, &quot;title&quot;, &quot;first_name&quot;, &quot;last_name&quot;]  
// from combobox html  
// adding &quot;film.film_id&quot; to combo is not an option  
// passing table name to html ?? big NO
$search = $request-&gt;getParam(&quot;search&quot;,&quot;&quot;);
$order = $request-&gt;getParam(&quot;order&quot;,&quot;&quot;);
$data = FilmActor::allJoin()
        -&gt;where($kat,&quot;like&quot;,&quot;%$search%&quot;)   
        -&gt;orderBy($order)  
        -&gt;limit(100)  
        -&gt;get();`  
",<php><mysql><sql><eloquent><slim>,2969,4,60,3402,6,34,58,62,1075,0.0,84,3,21,2018-01-19 19:38,2018-02-09 1:48,,21.0,,Basic,3,"<php><mysql><sql><eloquent><slim>, Eloquent join using ""USING"" clause with N query, I'm using Slim Framework with Illuminate Database.
I want to make JOIN query with USING clause. Let's say given Sakila database. Diagram:
How to make join with USING clause (not ON) in eloquent model?
SELECT film_id,title,first_name,last_name 
FROM film_actor 
INNER join film USING(film_id) -- notice 
INNER join actor USING(actor_id) -- notice 
What I want is an eager loading with EXACT 1 query. The use of eloquent relationships described in the API is not meeting my expectation, since any eager relation use N+1 query. I want to make it less IO to database.
FilmActor model :
class FilmActor extends Model
{
    protected $table = 'film_actor';
    protected $primaryKey = [&quot;actor_id&quot;, &quot;film_id&quot;];
    protected $incrementing = false;
    protected $appends = ['full_name'];
    // i need to make it in Eloquent model way, so it easier to manipulate
    public function getFullNameAttribute()  
    {
        $fn = &quot;&quot;;
        $fn .= isset($this-&gt;first_name) ? $this-&gt;first_name .&quot; &quot;: &quot;&quot;;
        $fn .= isset($this-&gt;last_name) ? $this-&gt;last_name .&quot; &quot;: &quot;&quot;;
        return $fn; 
    }
    public function allJoin()
    {
        // how to join with &quot;USING&quot; clause ?
        return self::select([&quot;film.film_id&quot;,&quot;title&quot;,&quot;first_name&quot;,&quot;last_name&quot;])
            -&gt;join(&quot;film&quot;, &quot;film_actor.film_id&quot;, '=', 'film.film_id')  
            -&gt;join(&quot;actor&quot;, &quot;film_actor.actor_id&quot;, '=', 'actor.actor_id');  
        //something like
        //return self::select(&quot;*&quot;)-&gt;joinUsing(&quot;film&quot;,[&quot;film_id&quot;]);
        //or
        //return self::select(&quot;*&quot;)-&gt;join(&quot;film&quot;,function($join){
        //    $join-&gt;using(&quot;film_id&quot;);
        //});
    }
}
So, in the controller I can get the data like
$data = FilmActor::allJoin()  
        -&gt;limit(100)  
        -&gt;get();`  
But there's a con, if I need to add extra behavior (like where or order).
$data = FilmActor::allJoin()
        -&gt;where(&quot;film.film_id&quot;,&quot;1&quot;)   
        -&gt;orderBy(&quot;film_actor.actor_id&quot;)  
        -&gt;limit(100)  
        -&gt;get();`  
I need to pass table name to avoid ambiguous field. Not good. So I want for further use, I can do
$kat = $request-&gt;getParam(&quot;kat&quot;,&quot;first_name&quot;);  
// [&quot;film_id&quot;, &quot;title&quot;, &quot;first_name&quot;, &quot;last_name&quot;]  
// from combobox html  
// adding &quot;film.film_id&quot; to combo is not an option  
// passing table name to html ?? big NO
$search = $request-&gt;getParam(&quot;search&quot;,&quot;&quot;);
$order = $request-&gt;getParam(&quot;order&quot;,&quot;&quot;);
$data = FilmActor::allJoin()
        -&gt;where($kat,&quot;like&quot;,&quot;%$search%&quot;)   
        -&gt;orderBy($order)  
        -&gt;limit(100)  
        -&gt;get();`  
","<pp><myself><sal><eloquent><slim>, elope join use ""using"" class n query, i'm use slim framework illusion database. want make join query use clause. let' say given skill database. diagram: make join use class (not on) elope model? select film_id,title,first_name,last_nam film_actor inner join film using(film_id) -- notice inner join actor using(actor_id) -- notice want eager load exact 1 query. use elope relationship describe apt meet expectation, since eager relate use n+1 query. want make less to database. filmactor model : class filmactor extend model { protect $table = 'film_actor'; protect $primarykey = [&quit;actor_id&quit;, &quit;film_id&quit;]; protect $incitement = false; protect $happened = ['full_name']; // need make elope model way, easier manipul public function getfullnameattribute() { $fn = &quit;&quit;; $fn .= asset($this-&it;first_name) ? $this-&it;first_nam .&quit; &quit;: &quit;&quit;; $fn .= asset($this-&it;last_name) ? $this-&it;last_nam .&quit; &quit;: &quit;&quit;; return $fn; } public function alloit() { // join &quit;using&quit; class ? return self::select([&quit;film.film_id&quit;,&quit;title&quit;,&quit;first_name&quit;,&quit;last_name&quit;]) -&it;join(&quit;film&quit;, &quit;film_actor.film_id&quit;, '=', 'film.film_id') -&it;join(&quit;actor&quit;, &quit;film_actor.actor_id&quit;, '=', 'actor.actor_id'); //cometh like //return self::select(&quit;*&quit;)-&it;joining(&quit;film&quit;,[&quit;film_id&quit;]); //or //return self::select(&quit;*&quit;)-&it;join(&quit;film&quit;,function($join){ // $join-&it;using(&quit;film_id&quit;); //}); } } so, control get data like $data = filmactor::alloit() -&it;limit(100) -&it;get();` there' con, need add extra behavior (like order). $data = filmactor::alloit() -&it;where(&quit;film.film_id&quit;,&quit;1&quit;) -&it;orderly(&quit;film_actor.actor_id&quit;) -&it;limit(100) -&it;get();` need pass table name avoid ambigu field. good. want use, $at = $request-&it;getparam(&quit;at&quit;,&quit;first_name&quit;); // [&quit;film_id&quit;, &quit;title&quit;, &quit;first_name&quit;, &quit;last_name&quit;] // combobox html // ad &quit;film.film_id&quit; comb option // pass table name html ?? big $search = $request-&it;getparam(&quit;search&quit;,&quit;&quit;); $order = $request-&it;getparam(&quit;order&quit;,&quit;&quit;); $data = filmactor::alloit() -&it;where($at,&quit;like&quit;,&quit;%$search%&quit;) -&it;orderly($order) -&it;limit(100) -&it;get();`"
57257965,DBeaver restore SQL Server .bak file,"I am just trying to restore a SQL Server .bak file in my DBeaver UI. But I have no idea how to do this - can someone help please? 
I created a database, but when I right click on it, there are no restore options.
",<sql-server><restore><dbeaver>,213,1,2,2114,7,23,49,56,37217,0.0,59,4,21,2019-07-29 16:31,2019-07-29 18:50,,0.0,,Intermediate,20,"<sql-server><restore><dbeaver>, DBeaver restore SQL Server .bak file, I am just trying to restore a SQL Server .bak file in my DBeaver UI. But I have no idea how to do this - can someone help please? 
I created a database, but when I right click on it, there are no restore options.
","<sal-server><restore><beaver>, beaver restore sal server .back file, try restore sal server .back file beaver i. idea - someone help please? great database, right click it, restore option."
55755095,PostgreSQL- ModuleNotFoundError: No module named 'psycopg2',"I can confirm psycopg2 is install (using conda install -c anaconda psycopg2) but the it seems psycopg2 cannot be imported to my python script or the interpreter is unable to locate it. I also tried installing using pip3, requirements are satisfied, meaning psycopg2 is already istalled, but cannot understand why I script isn't able to import it. Using Mac (OS v10.14.4) 
$ python create_tables.py
Traceback (most recent call last):
  File ""create_tables.py"", line 1, in &lt;module&gt;
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'
$ pip3 install psycopg2
Requirement already satisfied: psycopg2 in /usr/local/lib/python3.7/site-packages (2.8.2)
$ pip3 install psycopg2-binary
Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.7/site-packages (2.8.2)
python -V
Python 3.7.0
Any idea why  this happen?
EDIT: create_table.py
import psycopg2
from config import config
def create_tables():
    """""" create tables in the PostgreSQL database""""""
    commands = (
        """"""
        CREATE TABLE vendors (
            vendor_id SERIAL PRIMARY KEY,
            vendor_name VARCHAR(255) NOT NULL
        )
        """""",
        """""" CREATE TABLE parts (
                part_id SERIAL PRIMARY KEY,
                part_name VARCHAR(255) NOT NULL
                )
        """""",
        """"""
        CREATE TABLE part_drawings (
                part_id INTEGER PRIMARY KEY,
                file_extension VARCHAR(5) NOT NULL,
                drawing_data BYTEA NOT NULL,
                FOREIGN KEY (part_id)
                REFERENCES parts (part_id)
                ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""",
        """"""
        CREATE TABLE vendor_parts (
                vendor_id INTEGER NOT NULL,
                part_id INTEGER NOT NULL,
                PRIMARY KEY (vendor_id , part_id),
                FOREIGN KEY (vendor_id)
                    REFERENCES vendors (vendor_id)
                    ON UPDATE CASCADE ON DELETE CASCADE,
                FOREIGN KEY (part_id)
                    REFERENCES parts (part_id)
                    ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""")
    conn = None
    try:
        # read the connection parameters
        params = config()
        # connect to the PostgreSQL server
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        # create table one by one
        for command in commands:
            cur.execute(command)
        # close communication with the PostgreSQL database server
        cur.close()
        # commit the changes
        conn.commit()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()
if __name__ == '__main__':
    create_tables()
",<python><python-3.x><psycopg2><postgresql-9.1>,2788,0,77,3495,6,29,62,80,32251,0.0,439,3,21,2019-04-19 0:08,2019-04-19 0:19,2019-04-19 1:09,0.0,0.0,Basic,9,"<python><python-3.x><psycopg2><postgresql-9.1>, PostgreSQL- ModuleNotFoundError: No module named 'psycopg2', I can confirm psycopg2 is install (using conda install -c anaconda psycopg2) but the it seems psycopg2 cannot be imported to my python script or the interpreter is unable to locate it. I also tried installing using pip3, requirements are satisfied, meaning psycopg2 is already istalled, but cannot understand why I script isn't able to import it. Using Mac (OS v10.14.4) 
$ python create_tables.py
Traceback (most recent call last):
  File ""create_tables.py"", line 1, in &lt;module&gt;
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'
$ pip3 install psycopg2
Requirement already satisfied: psycopg2 in /usr/local/lib/python3.7/site-packages (2.8.2)
$ pip3 install psycopg2-binary
Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.7/site-packages (2.8.2)
python -V
Python 3.7.0
Any idea why  this happen?
EDIT: create_table.py
import psycopg2
from config import config
def create_tables():
    """""" create tables in the PostgreSQL database""""""
    commands = (
        """"""
        CREATE TABLE vendors (
            vendor_id SERIAL PRIMARY KEY,
            vendor_name VARCHAR(255) NOT NULL
        )
        """""",
        """""" CREATE TABLE parts (
                part_id SERIAL PRIMARY KEY,
                part_name VARCHAR(255) NOT NULL
                )
        """""",
        """"""
        CREATE TABLE part_drawings (
                part_id INTEGER PRIMARY KEY,
                file_extension VARCHAR(5) NOT NULL,
                drawing_data BYTEA NOT NULL,
                FOREIGN KEY (part_id)
                REFERENCES parts (part_id)
                ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""",
        """"""
        CREATE TABLE vendor_parts (
                vendor_id INTEGER NOT NULL,
                part_id INTEGER NOT NULL,
                PRIMARY KEY (vendor_id , part_id),
                FOREIGN KEY (vendor_id)
                    REFERENCES vendors (vendor_id)
                    ON UPDATE CASCADE ON DELETE CASCADE,
                FOREIGN KEY (part_id)
                    REFERENCES parts (part_id)
                    ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""")
    conn = None
    try:
        # read the connection parameters
        params = config()
        # connect to the PostgreSQL server
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        # create table one by one
        for command in commands:
            cur.execute(command)
        # close communication with the PostgreSQL database server
        cur.close()
        # commit the changes
        conn.commit()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()
if __name__ == '__main__':
    create_tables()
","<patron><patron-3.x><psycopg2><postgresql-9.1>, postgresql- modulenotfounderror: model name 'psycopg2', confirm psycopg2 instal (use conde instal -c anaconda psycopg2) seem psycopg2 cannot import patron script interpret unable local it. also try instal use pipe, require satisfied, mean psycopg2 already installed, cannot understand script all import it. use mac (o ve.14.4) $ patron create_tables.i traceback (most recent call last): file ""create_tables.by"", line 1, &it;module&it; import psycopg2 modulenotfounderror: model name 'psycopg2' $ pipe instal psycopg2 require already satisfied: psycopg2 /us/local/limb/python3.7/site-package (2.8.2) $ pipe instal psycopg2-binary require already satisfied: psycopg2-binary /us/local/limb/python3.7/site-package (2.8.2) patron -v patron 3.7.0 idea happen? edit: create_table.i import psycopg2 confirm import confirm def create_tables(): """""" great table postgresql database"""""" command = ( """""" great table vendor ( vendor_id aerial primary key, vendor_nam varchar(255) null ) """""", """""" great table part ( parted aerial primary key, part_nam varchar(255) null ) """""", """""" great table part_draw ( parted inter primary key, file_extens varchar(5) null, drawing_data tea null, foreign key (parted) refer part (parted) update cascade delete cascade ) """""", """""" great table vendor_part ( vendor_id inter null, parted inter null, primary key (vendor_id , parted), foreign key (vendor_id) refer vendor (vendor_id) update cascade delete cascade, foreign key (parted) refer part (parted) update cascade delete cascade ) """""") corn = none try: # read connect parapet parma = confirm() # connect postgresql server corn = psycopg2.connect(**parts) our = corn.curses() # great table one one command commands: our.execute(command) # close common postgresql database server our.close() # commit change corn.commit() except (exception, psycopg2.databaseerror) error: print(error) finally: corn none: corn.close() __name__ == '__main__': create_tables()"
48112591,Generate ansi sql INSERT INTO,"I have Oracle database with 10 tables. Some of the tables have CLOB data text. I need to export data from these tables pro-grammatically using java. The export data should be in ANSI INSERT INTO SQL format, for example: 
INSERT INTO table_name (column1, column2, column3, ...)
VALUES (value1, value2, value3, ...);
The main idea is that I need to import this data into three different databases:
ORACLE, MSSQL and MySQL. As I know all these databases support ANSI INSERT INTO. But I have not found any java API/framework for generating data SQL scripts. And I do not know how to deal with CLOB data, how to export it. 
What is the best way to export data from a database with java? 
UPDATE: (01.07.2018) 
I guess it is impossible to insert text data more than 4000 bytes according to this answer. How to generate PL\SQL scripts using java programmatically? Or is there any other export format which supports ORACLE, MSSQL, etc?
",<java><sql><oracle>,928,1,2,2689,3,33,50,76,2959,0.0,2360,7,21,2018-01-05 11:21,2018-01-05 11:27,,0.0,,Basic,9,"<java><sql><oracle>, Generate ansi sql INSERT INTO, I have Oracle database with 10 tables. Some of the tables have CLOB data text. I need to export data from these tables pro-grammatically using java. The export data should be in ANSI INSERT INTO SQL format, for example: 
INSERT INTO table_name (column1, column2, column3, ...)
VALUES (value1, value2, value3, ...);
The main idea is that I need to import this data into three different databases:
ORACLE, MSSQL and MySQL. As I know all these databases support ANSI INSERT INTO. But I have not found any java API/framework for generating data SQL scripts. And I do not know how to deal with CLOB data, how to export it. 
What is the best way to export data from a database with java? 
UPDATE: (01.07.2018) 
I guess it is impossible to insert text data more than 4000 bytes according to this answer. How to generate PL\SQL scripts using java programmatically? Or is there any other export format which supports ORACLE, MSSQL, etc?
","<cava><sal><oracle>, genet anti sal insert into, oral database 10 tables. table club data text. need export data table pro-grammar use cava. export data anti insert sal format, example: insert table_nam (column, column, column, ...) value (value, value, value, ...); main idea need import data three differ database: oracle, mssql myself. know database support anti insert into. found cava apt/framework genet data sal script. know deal club data, export it. best way export data database cava? update: (01.07.2018) guess impose insert text data 4000 bite accord answer. genet ll\sal script use cava programmatically? export format support oracle, mssql, etc?"
49783108,PostgreSQL 10 on Linux - LC_COLLATE locale en_US.utf-8 not valid,"
  ERROR: invalid locale name: ""en_US.utf-8""
Running Ubuntu server 18.04 Beta 2 with PostgreSQL 10.
In running a database creation script that worked on 9.5, I am now seeing an issue with 'en_US.UTF-8' as a locale:
CREATE DATABASE db WITH TEMPLATE = template0 ENCODING = 'UTF8' LC_COLLATE = 'en_US.UTF-8' LC_CTYPE = 'en_US.UTF-8';
I know this may be redundant as I understand the default to be 'en_US.etf-8'.  Removing the LC_COLLATE and LC_CTYPE parameters let me run my script.
So did the locale definitions change somehow for V 10?  Or is there something else now happening?  I couldn't find anything on this in the Postgres 10 manual.
",<postgresql>,639,0,1,1603,2,10,17,81,44629,0.0,0,6,21,2018-04-11 19:31,2018-04-12 19:40,2018-04-12 19:40,1.0,1.0,Basic,9,"<postgresql>, PostgreSQL 10 on Linux - LC_COLLATE locale en_US.utf-8 not valid, 
  ERROR: invalid locale name: ""en_US.utf-8""
Running Ubuntu server 18.04 Beta 2 with PostgreSQL 10.
In running a database creation script that worked on 9.5, I am now seeing an issue with 'en_US.UTF-8' as a locale:
CREATE DATABASE db WITH TEMPLATE = template0 ENCODING = 'UTF8' LC_COLLATE = 'en_US.UTF-8' LC_CTYPE = 'en_US.UTF-8';
I know this may be redundant as I understand the default to be 'en_US.etf-8'.  Removing the LC_COLLATE and LC_CTYPE parameters let me run my script.
So did the locale definitions change somehow for V 10?  Or is there something else now happening?  I couldn't find anything on this in the Postgres 10 manual.
","<postgresql>, postgresql 10 line - lc_collat local ends.utf-8 valid, error: invalid local name: ""ends.utf-8"" run bunt server 18.04 beta 2 postgresql 10. run database creation script work 9.5, see issue 'ends.utf-8' local: great database do temple = template0 end = 'utf' lc_collat = 'ends.utf-8' lc_ctype = 'ends.utf-8'; know may refund understand default 'ends.etc-8'. remove lc_collat lc_ctype parapet let run script. local definite change somehow v 10? cometh else happening? find any poster 10 manual."
59065629,"A field with precision 10, scale 2 must round to an absolute value less than 10^8","I have a django-field total_price in postgres database version 9.3.11.
Here is the code:
total_value = models.DecimalField(decimal_places=100, default=0, max_digits=300)
I want to convert it to proper 2 decimal place. So I wrote this:
total_value = models.DecimalField(decimal_places=2, default=0, max_digits=10)
My migration file
# -*- coding: utf-8 -*-
from __future__ import unicode_literals
from django.db import models, migrations
class Migration(migrations.Migration):
    dependencies = [
        ('my_table', '0040_my_table_skipped'),
    ]
    operations = [
        migrations.AlterField(
            model_name='my_table',
            name='total_value',
            field=models.DecimalField(default=0, 
                                      max_digits=10, 
                                      decimal_places=2),
        ),
    ]
When I run command python manage.py migrate
I get an error from postgresql:
A field with precision 10, scale 2 must round to an absolute value less than 10^8.
",<python><django><postgresql><django-models><postgresql-9.3>,1003,0,26,9182,13,57,85,55,61396,,327,3,21,2019-11-27 8:03,2020-04-26 7:43,,151.0,,Basic,9,"<python><django><postgresql><django-models><postgresql-9.3>, A field with precision 10, scale 2 must round to an absolute value less than 10^8, I have a django-field total_price in postgres database version 9.3.11.
Here is the code:
total_value = models.DecimalField(decimal_places=100, default=0, max_digits=300)
I want to convert it to proper 2 decimal place. So I wrote this:
total_value = models.DecimalField(decimal_places=2, default=0, max_digits=10)
My migration file
# -*- coding: utf-8 -*-
from __future__ import unicode_literals
from django.db import models, migrations
class Migration(migrations.Migration):
    dependencies = [
        ('my_table', '0040_my_table_skipped'),
    ]
    operations = [
        migrations.AlterField(
            model_name='my_table',
            name='total_value',
            field=models.DecimalField(default=0, 
                                      max_digits=10, 
                                      decimal_places=2),
        ),
    ]
When I run command python manage.py migrate
I get an error from postgresql:
A field with precision 10, scale 2 must round to an absolute value less than 10^8.
","<patron><django><postgresql><django-models><postgresql-9.3>, field precise 10, scale 2 must round absolute value less 10^8, django-field total_pric poster database version 9.3.11. code: total_valu = models.decimalfield(decimal_places=100, default=0, max_digits=300) want convert proper 2 devil place. wrote this: total_valu = models.decimalfield(decimal_places=2, default=0, max_digits=10) migrate file # -*- coming: utf-8 -*- __future__ import unicode_liter django.do import models, migrate class migration(migrations.migration): depend = [ ('my_table', '0040_my_table_skipped'), ] over = [ migrations.alterfield( model_name='my_table', name='total_value', field=models.decimalfield(default=0, max_digits=10, decimal_places=2), ), ] run command patron manage.i migrate get error postgresql: field precise 10, scale 2 must round absolute value less 10^8."
49088401,Spark from_json with dynamic schema,"I am trying to use Spark for processing JSON data with variable structure(nested JSON). Input JSON data could be very large with more than 1000 of keys per row and one batch could be more than 20 GB. 
Entire batch has been generated from 30 data sources and 'key2' of each JSON can be used to identify the source and structure for each source is predefined.
What would be the best approach for processing such data?
I have tried using from_json like below but it works only with fixed schema and to use it first I need to group the data based on each source and then apply the schema. 
Due to large data volume my preferred choice is to scan the data only once and extract required values from each source, based on predefined schema.
import org.apache.spark.sql.types._ 
import spark.implicits._
val data = sc.parallelize(
    """"""{""key1"":""val1"",""key2"":""source1"",""key3"":{""key3_k1"":""key3_v1""}}""""""
    :: Nil)
val df = data.toDF
val schema = (new StructType)
    .add(""key1"", StringType)
    .add(""key2"", StringType)
    .add(""key3"", (new StructType)
    .add(""key3_k1"", StringType))
df.select(from_json($""value"",schema).as(""json_str""))
  .select($""json_str.key3.key3_k1"").collect
res17: Array[org.apache.spark.sql.Row] = Array([xxx])
",<json><apache-spark><apache-spark-sql>,1233,0,19,284,1,2,9,70,44295,0.0,2,3,21,2018-03-03 19:37,2018-03-04 3:54,,1.0,,Intermediate,18,"<json><apache-spark><apache-spark-sql>, Spark from_json with dynamic schema, I am trying to use Spark for processing JSON data with variable structure(nested JSON). Input JSON data could be very large with more than 1000 of keys per row and one batch could be more than 20 GB. 
Entire batch has been generated from 30 data sources and 'key2' of each JSON can be used to identify the source and structure for each source is predefined.
What would be the best approach for processing such data?
I have tried using from_json like below but it works only with fixed schema and to use it first I need to group the data based on each source and then apply the schema. 
Due to large data volume my preferred choice is to scan the data only once and extract required values from each source, based on predefined schema.
import org.apache.spark.sql.types._ 
import spark.implicits._
val data = sc.parallelize(
    """"""{""key1"":""val1"",""key2"":""source1"",""key3"":{""key3_k1"":""key3_v1""}}""""""
    :: Nil)
val df = data.toDF
val schema = (new StructType)
    .add(""key1"", StringType)
    .add(""key2"", StringType)
    .add(""key3"", (new StructType)
    .add(""key3_k1"", StringType))
df.select(from_json($""value"",schema).as(""json_str""))
  .select($""json_str.key3.key3_k1"").collect
res17: Array[org.apache.spark.sql.Row] = Array([xxx])
","<son><apache-spark><apache-spark-sal>, spark from_json dream scheme, try use spark process son data variable structure(nest son). input son data could large 1000 key per row one batch could 20 go. enter batch genet 30 data source 'key' son use identify source structure source predestined. would best approach process data? try use from_json like work fix scheme use first need group data base source apply scheme. due large data volume prefer choice scan data extract require value source, base predefin scheme. import org.apache.spark.sal.types._ import spark.implicit._ val data = s.parallelize( """"""{""key"":""vale"",""key"":""source"",""key"":{""key3_k1"":""key3_v1""}}"""""" :: nail) val of = data.of val scheme = (new structtype) .add(""key"", stringtype) .add(""key"", stringtype) .add(""key"", (new structtype) .add(""key3_k1"", stringtype)) of.select(from_json($""value"",scheme).as(""json_str"")) .select($""json_str.key.key3_k1"").collect rest: array[org.apache.spark.sal.row] = array([xxx])"
55581114,COUNT(id) or MAX(id) - which is faster?,"I have a web server on which I've implemented my own messaging system.
I am at a phase where I need to create an API that checks if the user has new messages.
My DB table is simple:
ID - Auto Increment, Primary Key (Bigint)
Sender - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Recipient - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Message - Varchar (256) //UTF8 BIN
I am considering making an API that will estimate if there are new messages for a given user. I am thinking of using one of these methods:
A) Select count(ID) of messages where sender or recipient is me.
(if this number &gt; previous number, I have a new message)
B) Select max(ID) of messages where sender or recipient is me.
(if max(ID) &gt; than previous number, I have a new message)
My question is:  Can I calculate somehow what method will consume fewer server resources? Or is there some article? Maybe another method I didn't mention?
",<php><mysql><performance>,949,0,6,336,0,6,18,50,4952,0.0,9,4,21,2019-04-08 20:15,2019-04-08 20:19,2019-04-08 20:19,0.0,0.0,Intermediate,23,"<php><mysql><performance>, COUNT(id) or MAX(id) - which is faster?, I have a web server on which I've implemented my own messaging system.
I am at a phase where I need to create an API that checks if the user has new messages.
My DB table is simple:
ID - Auto Increment, Primary Key (Bigint)
Sender - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Recipient - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Message - Varchar (256) //UTF8 BIN
I am considering making an API that will estimate if there are new messages for a given user. I am thinking of using one of these methods:
A) Select count(ID) of messages where sender or recipient is me.
(if this number &gt; previous number, I have a new message)
B) Select max(ID) of messages where sender or recipient is me.
(if max(ID) &gt; than previous number, I have a new message)
My question is:  Can I calculate somehow what method will consume fewer server resources? Or is there some article? Maybe another method I didn't mention?
","<pp><myself><performance>, count(id) max(id) - faster?, web server i'v implement message system. phase need great apt check user new messages. do table simple: id - auto incitement, primary key (begin) tender - varchar (32) // foreign key used has user do table recipe - varchar (32) // foreign key used has user do table message - varchar (256) //utf bin consider make apt estime new message given user. think use one methods: a) select count(id) message tender recipe me. (if number &it; previous number, new message) b) select max(id) message tender recipe me. (if max(id) &it; previous number, new message) question is: call somehow method consume fewer server resources? article? may not method mention?"
53784468,Postgres 10.3: SELECT queries hang for hours,"My application is using Postgres as DBMS, the version of Postgres that i'm using is 10.3 with the extension Postgis installed. 
Occasionally i noticed that in random interval of times the dbms become slow and get stuck on a few SELECT queries.
From pg_stat_activity i noticed that the wait_event_type and wait_event of these queries is as follows: 
 select wait_event_type, wait_event from pg_stat_activity where state='active'; 
 wait_event_type |  wait_event  
-----------------+--------------
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
                 | 
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
(33 rows)
My assumption, after checking the docs, is that the hardware underneath has some issues and then the problem i'm facing is not related to the application, or the type of query, but to the hardware itself.
Anybody ever faced this kind of issue? 
",<postgresql><postgresql-10>,1735,1,40,1490,2,12,22,74,12768,0.0,67,1,21,2018-12-14 17:37,2021-04-28 11:28,,866.0,,Intermediate,23,"<postgresql><postgresql-10>, Postgres 10.3: SELECT queries hang for hours, My application is using Postgres as DBMS, the version of Postgres that i'm using is 10.3 with the extension Postgis installed. 
Occasionally i noticed that in random interval of times the dbms become slow and get stuck on a few SELECT queries.
From pg_stat_activity i noticed that the wait_event_type and wait_event of these queries is as follows: 
 select wait_event_type, wait_event from pg_stat_activity where state='active'; 
 wait_event_type |  wait_event  
-----------------+--------------
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
                 | 
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
(33 rows)
My assumption, after checking the docs, is that the hardware underneath has some issues and then the problem i'm facing is not related to the application, or the type of query, but to the hardware itself.
Anybody ever faced this kind of issue? 
","<postgresql><postgresql-10>, poster 10.3: select query hang hours, applied use poster dams, version poster i'm use 10.3 extent post installed. occasion notice random inter time dam become slow get stuck select queried. pg_stat_act notice wait_event_typ waited query follows: select wait_event_type, waited pg_stat_act state='active'; wait_event_typ | waited -----------------+-------------- to | datafileread to | datafileread to | datafileread to | datafileread clock | buffer_io clock | buffer_io to | datafileread clock | buffer_io clock | buffer_io to | datafileread to | datafileread clock | buffer_io clock | buffer_io to | datafileread clock | buffer_io to | datafileread clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io clock | buffer_io to | datafileread to | datafileread | to | datafileread clock | buffer_io clock | buffer_io (33 rows) assumption, check docs, hardware underneath issue problem i'm face relate application, type query, hardware itself. anybody ever face kind issue?"
55297807,When do Postgres column or table names need quotes and when don't they?,"Let's consider the following postgres query:
SELECT * 
FROM ""MY_TABLE""
WHERE ""bool_var""=FALSE 
 AND ""str_var""='something';
The query fails to respond properly when I remove quotes around ""str_var"" but not when I do the same around ""bool_var"". Why? What is the proper way to write the query in that case, no quotes around the boolean column and quotes around the text column? Something else?
",<postgresql><quoted-identifier>,391,0,6,594,1,5,10,68,19822,0.0,144,2,21,2019-03-22 10:39,2019-03-22 10:46,2019-03-22 12:21,0.0,0.0,Basic,8,"<postgresql><quoted-identifier>, When do Postgres column or table names need quotes and when don't they?, Let's consider the following postgres query:
SELECT * 
FROM ""MY_TABLE""
WHERE ""bool_var""=FALSE 
 AND ""str_var""='something';
The query fails to respond properly when I remove quotes around ""str_var"" but not when I do the same around ""bool_var"". Why? What is the proper way to write the query in that case, no quotes around the boolean column and quotes around the text column? Something else?
","<postgresql><quoted-identified>, poster column table name need quit they?, let' consider follow poster query: select * ""my_table"" ""bool_var""=fall ""str_var""='something'; query fail respond properly remove quit around ""str_var"" around ""bool_var"". why? proper way write query case, quit around woolen column quit around text column? cometh else?"
