QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
53554458,SQLAlchemy: Get database name from engine,"After creating an SQLALchemy engine like this
engine = create_engine('mssql+pyodbc://user:pass@dbserver:port/db_name?driver=ODBC+Driver+13+for+SQL+Server)
Is there a way to get db_name from the engine-object? I know I can parse the name from the connection string but is there a better way of doing this? I had a look at the SQLAlchemy-API but couldn't find an answer.
",<python><sql-server>,369,1,3,317,1,2,13,72,13350,0.0,18,1,13,2018-11-30 9:13,2019-04-22 15:35,2019-04-22 15:35,143.0,143.0,Basic,3
48308826,Wamp Server Error [Local Server - 2 of 3 services running],"I am new to wamp servers and trying to install wampServer 3.1.0 on my windows 10 machine . 
Somehow it is not installed properly and is having configuration error .
At present ""Wamp server is still in orange state and is throwing the error"" 
  2 of 3 services running
As of my understanding either of Apache,MySQl orPHP is not working .
On further investigation I found that Apache is ok.
But on running mysql.exe(C:\wamp64\bin\mysql\mysql5.7.19\bin) it is throwing :
  ERROR 2003 (HY000): Can't connect to MySQL server on 'localhost'
  (10061)
which lands me to SO-32519474 ,
I tried following the steps ,but it looks good to me in my case .
On further searching I find that wampmysqld64 is stopped in the services.
when I am trying to restart it I am getting the error 
I am stuck up here and have no further clue how to get it fixed 
Any help is highly appreciated.
",<mysql><windows-services><wamp><wampserver>,869,2,0,553,1,10,29,61,126418,0.0,43,18,13,2018-01-17 19:44,2018-01-18 18:00,2018-01-18 18:00,1.0,1.0,Basic,14
53763417,Number of unique elements in all columns of a pyspark dataframe,"How it is possible to calculate the number of unique elements in each column of a pyspark dataframe:
import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = pd.DataFrame([[1, 100], [1, 200], [2, 300], [3, 100], [4, 100], [4, 300]], columns=['col1', 'col2'])
df_spark = spark.createDataFrame(df)
print(df_spark.show())
# +----+----+
# |col1|col2|
# +----+----+
# |   1| 100|
# |   1| 200|
# |   2| 300|
# |   3| 100|
# |   4| 100|
# |   4| 300|
# +----+----+
# Some transformations on df_spark here
# How to get a number of unique elements (just a number) in each columns?
I know only the following solution which is very slow, both of these lines are calculated in the same amount of time:
col1_num_unique = df_spark.select('col1').distinct().count()
col2_num_unique = df_spark.select('col2').distinct().count()
There are about 10 millions rows in df_spark.
",<python><apache-spark><dataframe><pyspark><apache-spark-sql>,907,0,24,3017,10,41,60,72,19737,,5507,3,13,2018-12-13 13:53,2018-12-13 15:08,2018-12-13 15:08,0.0,0.0,Basic,9
51038591,Error installing mysql,"Beginning configuration step: Initializing Database
Attempting to run MySQL Server with --initialize-insecure option...
Starting process for MySQL Server 8.0.11...
Starting process with command: C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe --defaults-file=""C:\ProgramData\MySQL\MySQL Server 8.0\my.ini"" --console --initialize-insecure=on...
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-011071] [Server] Unknown suffix '.' used for variable 'lower_case_table_names' (value '0.0')
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-011071] [Server] C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe: Error while setting value '0.0' to 'lower_case_table_names'
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-06-26T08:44:42.036600Z 0 [Note] [MY-010120] [Server] Binlog end
Process for mysqld, with ID 3232, was run successfully and exited with code 1.
Failed to start process for MySQL Server 8.0.11.
Database initialization failed.
Ended configuration step: Initializing Database
I am having this error in log during the installing of MySQL on the application configuration part.
",<mysql><installation>,1106,0,12,351,1,2,12,54,36564,0.0,4,12,13,2018-06-26 8:45,2018-07-03 15:15,,7.0,,Basic,13
57943166,Flutter Syncing LocalDB With RemoteDB,"I have data stored locally in sqlite.  I have a remote MySql server.  In android I could setup SyncAdapter to handle the syncing between the localdb and the remotedb.
When a record is saved locally and there is an internet connection it should push the data to the server in the background.  It should also periodically update the data stored in SqLite.
Now I'm trying to find the equivalent of SyncAdapter in flutter to do this but I can't seem to find one.  How would I implement this functionality in flutter without having to use firebase?
",<mysql><flutter><sqlite><android-syncadapter>,544,0,0,7999,17,66,124,77,1345,0.0,520,2,13,2019-09-15 10:19,2021-09-16 0:07,,732.0,,Basic,9
50967722,room error: The columns returned by the query does not have the fields fieldname,"Here is a sample POJO
public class Product{
  private long id;
  private String name;
  private double price;
 ... constructor for all fields
 ... getters and setters
}
Now, in my productDAO if I have a query like this
@Query(select id, name from products)
LiveData&lt;List&lt;Product&gt;&gt; getProducts()
I get an error like:
  The columns returned by the query does not have the fields [price] in
  ... Product even though they are annotated as non-null or primitive.
  Columns returned by the query: [id,name]
a) If I go in my Products and set
@Nullable
private double price;
the error remains.
b) If I go in my Products and set
@Ignore
private double price;
the error goes away but if I have another query for instance
 @Query(select p.id, p.name, p.price, t.someField from products p inner join table t)
    LiveData&lt;List&lt;Product&gt;&gt; getJoinQueryResponse()
because @Ignore is set the price is returned as 0.0. 
So how to solve this? Hopefully I don't need to make a POJO for each different response from room...
",<android><android-sqlite><android-room><android-architecture-components>,1028,0,17,14879,40,131,218,43,14673,,908,1,13,2018-06-21 11:41,2018-06-21 11:54,2018-06-21 11:54,0.0,0.0,Basic,1
52722671,How to make GROUP BY in a cypher query?,"I want to translate a SQL query to cypher. Please, is there any solution to make GROUP BY in cypher?
    SELECT dt.d_year, 
           item.i_brand_id          brand_id, 
           item.i_brand             brand, 
           Sum(ss_ext_discount_amt) sum_agg 
    FROM   date_dim dt, 
   store_sales, 
   item 
    WHERE  dt.d_date_sk = store_sales.ss_sold_date_sk 
    AND store_sales.ss_item_sk = item.i_item_sk 
    AND item.i_manufact_id = 427 
    AND dt.d_moy = 11 
    GROUP  BY dt.d_year, 
      item.i_brand, 
      item.i_brand_id 
   ORDER  BY dt.d_year, 
      sum_agg DESC, 
      brand_id;
",<sql><neo4j><group-by><cypher>,604,0,17,135,1,1,6,50,16169,0.0,0,2,13,2018-10-09 13:47,2018-10-09 16:32,2018-10-09 16:32,0.0,0.0,Basic,10
53378568,SSIS vs. Oracle Data Integrator,"Currently I am a Data Engineer that works mainly with SSIS. While reading about the ETL tools available in the market, i found that Oracle has its own ETL tool called ODI (Oracle Data integrator). I searched for an unbiased comparison between the Oracle Data Integrator and SSIS. I didn't find any article about that. There are some biased article such as :
ETL Tools Comparison of Oracle ODI &amp; Microsoft SSIS Tool -Dec 2014
Competitive Comparison of SQL Server 2008 Integration Services
Based on Stackoverflow questions, there are about 16000 questions about SSIS while ODI has about 200 questions. Which mean that SSIS is more popular.
Is there any unbiased comparison between both technologies? And what are the services that ODI provides and that are not found in SSIS?
Please I am not looking for personal opinions, I need an unbiased answer
",<sql-server><oracle><ssis><etl><oracle-data-integrator>,851,2,0,36491,13,67,124,52,4754,0.0,4879,1,13,2018-11-19 16:07,2018-11-20 6:59,2018-11-20 6:59,1.0,1.0,Intermediate,19
53462775,How to determine if postgis is enabled on a database?,"I wanted to know if there is a way to determine that PostGis was enabled on a database. 
I am trying to replicate my production server with my dev machine and I am not sure if the database on my dev machine had either PostGIS or postgis_topology enabled or both. 
I tried looking around for a solution but could not come up with anything. 
Any suggestions in this regard would be helpful.
",<postgresql><postgis>,389,0,0,16573,40,141,245,63,13297,0.0,503,5,13,2018-11-24 22:09,2018-11-24 23:59,2018-11-24 23:59,0.0,0.0,Basic,10
50973091,Exploding column with index,"I know that I can ""explode"" a column of type array like this:
import org.apache.spark.sql._
import org.apache.spark.sql.functions.explode
val explodedDf = 
    payloadLegsDf.withColumn(""legs"", explode(payloadLegsDf.col(""legs"")))
Now I have multiple rows; one for each item in the array.
Is there a way I can ""explode with index""?  So that there will be a new column that contains the index of the item in the original array?
(I can think of hacks to do this.  First make the array field into an array of tuples of the original value and the index.  Then do the explode.  Then unpack the tuples.  But is there a more elegant way?)
",<scala><apache-spark-sql>,630,0,4,8646,33,119,206,54,6767,0.0,723,1,13,2018-06-21 16:09,2018-06-21 16:40,2018-06-21 16:40,0.0,0.0,Basic,10
63760689,Estimating size of Postgres indexes,"I'm trying to get a better understanding of the tradeoffs involved in creating Postgres indexes. As part of that, I'd love to understand how much space indexes usually use. I've read through the docs, but can't find any information on this. I've been doing my own little experiments creating tables and indexes, but it would be amazing if someone could offer an explanation of why the size is what it is. Assume a common table like this with 1M rows, where each row has a unique id and a unique outstanding.
CREATE TABLE account (
    id integer,
    active boolean NOT NULL,
    outstanding double precision NOT NULL,
);
and the indexes created by
CREATE INDEX id_idx ON account(id)
CREATE INDEX outstanding_idx ON account(outstanding)
CREATE INDEX id_outstanding_idx ON account(id, outstanding)
CREATE INDEX active_idx ON account(active)
CREATE INDEX partial_id_idx ON account(id) WHERE active
What would you estimate the index sizes to be in bytes and more importantly, why?
",<postgresql><indexing><storage>,978,1,12,5483,14,63,104,53,6478,0.0,1781,2,13,2020-09-06 4:24,2020-09-06 12:00,2020-09-11 1:57,0.0,5.0,Basic,10
49991865,Node.js & MySQL - Error: 1251 - Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Right now I have only this code:
const Sequelize = require('sequelize');
const sequelize = new Sequelize('database', 'root', 'passwd', {
  host: 'localhost',
  dialect: 'mysql',
    // http://docs.sequelizejs.com/manual/tutorial/querying.html#operators
  operatorsAliases: false
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
But when I try to run the .js I get this error. I have already tried a lot of solutions out there, including the one I found more often but it didn't work. So right now I don't know what to do. Can somebody help me?
Thank you
",<javascript><mysql><node.js><sequelize.js>,716,3,16,409,1,5,10,80,23565,0.0,23,4,13,2018-04-24 0:48,2018-04-24 11:41,,0.0,,Basic,9
60228723,Cannot connect to local postgresql DB using DBeaver,"I'm trying to connect to a postgresql database which is in localhost:5432 but I keep getting the error: 
FATAL: Ident authentication failed for user """".
I installed Postgres11 on virtual machine running Centos7. Created a database through command line, with the name business_db.
I've checked and postgresql is running in localhost:5432.
My pg_hba.conf file is like this:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             all                                     peer
host    all             all             127.0.0.1/32           ident
host    all             all             ::1/128                 ident
local   replication     all                                     peer
host    replication     all             127.0.0.1/32            ident
host    replication     all             ::1/128                 ident
The pg_ident.conf file doesn't hold any configurations:
# Put your actual configuration here
# ----------------------------------
# MAPNAME       SYSTEM-USERNAME         PG-USERNAME
The database exists as shown by the command:
I'm logged into the system as ""dev"" user but., whatever I try while testing connection whit DBeaver, I allways get the error:
I also tried to set User as postgres and use my system password but get the same error. What am I missing?
",<postgresql><jdbc><dbeaver>,1320,2,11,187,1,1,9,57,32946,,6,4,13,2020-02-14 14:59,2020-02-14 15:19,2020-02-14 15:19,0.0,0.0,Basic,14
60278766,Best way to Insert Python NumPy array into PostgreSQL database,"Our team uses software that is heavily reliant on dumping NumPy data into files, which slows our code quite a lot. If we could store our NumPy arrays directly in PostgreSQL we would get a major performance boost.
Other performant methods of storing NumPy arrays in any database or searchable database-like structure are welcome, but PostgresSQL would be preferred.  
My question is very similar to one asked previously. However, I am looking for a more robust and performant answer and I wish to store any arbitrary NumPy array. 
",<python><sql><postgresql><numpy>,530,1,0,604,0,9,27,51,8284,0.0,198,2,13,2020-02-18 10:29,2020-02-21 16:45,2020-02-22 1:37,3.0,4.0,Intermediate,23
63742915,EF: Passing a table valued parameter to a user-defined function from C#,"I have a user-defined function in SQL Server that accepts a TVP (table valued parameter) as parameter. In EF, how do I call such a function from C# ?
I tried using the method ObjectContext.CreateQuery&lt;&gt; but got the following error:
The parameter 'param' of function 'QueryByParam' is invalid. Parameters can only be of a type that can be converted to an Edm scalar type.
Also tried method ObjectContext.ExecuteStoreQuery&lt;&gt; and got the same error. It doesn't return an IQueryable anyway.
Sample code
[DbFunction(nameof(SampleDbContext), &quot;QueryByParam&quot;)]
public IQueryable&lt;SecurityQueryRow&gt; QueryByParam(IEnumerable&lt;ProfileType&gt; profiles, bool isActive = false)
{
    DataTable dataTable = ....
    ObjectParameter profilesParam = new ObjectParameter(&quot;profileTypeIds&quot;, dataTable);
    ObjectParameter isActiveParam = new ObjectParameter(&quot;isActive &quot;, isActive);
    return ((IObjectContextAdapter)this).ObjectContext.CreateQuery&lt;SecurityQueryRow&gt;(
            string.Format(&quot;[{0}].[{1}](@profileTypeIds, @isActive)&quot;, GetType().Name, &quot;QueryByParam&quot;),
            profilesParam,
            isActiveParam);
}
The requirement is that we need an IQueryable back, not the consumed result.
",<c#><sql-server><entity-framework><entity-framework-6><table-valued-parameters>,1261,0,16,131,0,1,4,65,3967,0.0,0,1,13,2020-09-04 14:16,2020-09-07 17:09,,3.0,,Basic,9
56382010,Why does MS SQL allow you to create an illegal column?,"I recently saw a tweet stating that you could prevent other developers from reading from a table using the SELECT * FROM TableName by building your table in the following way:
CREATE TABLE [TableName]
(
   [ID] INT IDENTITY NOT NULL,
   [Name] VARCHAR(50) NOT NULL,
   [DontUseStar] AS (1 / 0)
);
It's easy to see that using the SELECT * here would try to read the blank column name as 1 divided by 0 (thus causing a divide by zero error), but without a datatype assigned to the column.
Why does SQL allow you to create a column with no assigned data type, with a name it knows will be illegal?
",<sql><sql-server>,595,0,8,1365,0,12,24,43,550,0.0,443,3,13,2019-05-30 16:21,2019-05-30 16:26,2019-05-30 16:26,0.0,0.0,Basic,9
52127228,"Docker, Flask, SQLAlchemy: ValueError: invalid literal for int() with base 10: 'None'","I have a flask app that can be initialized successfully and connects to Postgresql database. However, when i try to dockerize this app, i get the below error message. ""SQLALCHEMY_DATABASE_URI"" is correct and i can connect to it, so i can't figure where I have gone wrong. 
docker-compose logs
app_1       |   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/url.py"", line 60, in __init__
app_1       |     self.port = int(port)
app_1       | ValueError: invalid literal for int() with base 10: 'None'
Postgres database connects successfully in Docker container 
postgres_1  | LOG:  database system is ready to accept connections
config.py
from os import environ
import os
RDS_USERNAME = environ.get('RDS_USERNAME')
RDS_PASSWORD = environ.get('RDS_PASSWORD')
RDS_HOSTNAME = environ.get('RDS_HOSTNAME')
RDS_PORT = environ.get('RDS_PORT')
RDS_DB_NAME = environ.get('RDS_DB_NAME')
SQLALCHEMY_DATABASE_URI = ""postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dbname}""\
                          .format(username = RDS_USERNAME, password = RDS_PASSWORD, \
                           hostname = RDS_HOSTNAME, port = RDS_PORT, dbname = RDS_DB_NAME)
flask_app.py (entry point)
def create_app():
    app = Flask(__name__, static_folder=""./static"", template_folder=""./static"")
    app.config.from_pyfile('./app/config.py', silent=True)
    register_blueprint(app)
    register_extension(app)
    with app.app_context():
        print(db) -&gt; This prints the correct path for SQLALCHEMY_DATABASE_URI
        db.create_all()
        db.session.commit()
    return app
def register_blueprint(app):
    app.register_blueprint(view_blueprint)
    app.register_blueprint(race_blueprint)
def register_extension(app):
    db.init_app(app)
    migrate.init_app(app)
app = create_app()
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
Dockerfile
FROM ubuntu
RUN apt-get update &amp;&amp; apt-get -y upgrade
RUN apt-get install -y python-pip &amp;&amp; pip install --upgrade pip
RUN mkdir /home/ubuntu
WORKDIR /home/ubuntu/celery-scheduler
ADD requirements.txt /home/ubuntu/celery-scheduler/
RUN pip install -r requirements.txt
COPY . /home/ubuntu/celery-scheduler
EXPOSE 5000
CMD [""python"", ""flask_app.py"", ""--host"", ""0.0.0.0""]
docker-compose.yml
version: '2' 
services:
  app:
    restart: always
    build: 
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app
    depends_on:
      - postgres
  postgres:
    restart: always
      image: postgres:9.6
    environment:
      - POSTGRES_USER=${RDS_USERNAME}
      - POSTGRES_PASSWORD=${RDS_PASSWORD}
      - POSTGRES_HOSTNAME=${RDS_HOSTNAME}
      - POSTGRES_DB=${RDS_DB_NAME}
    ports:
      - ""5432:5432""
",<python><postgresql><docker><flask><sqlalchemy>,2717,0,85,896,2,18,43,35,4479,0.0,317,2,13,2018-09-01 10:30,2018-09-01 11:09,2018-09-15 7:22,0.0,14.0,Basic,9
51264240,rake db:migrate error with mysql2 gem - Library not loaded: libssl.1.0.0.dylib,"Getting the following error after running rake db:migrate
rake aborted!
LoadError: dlopen(/Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle, 9): Library not loaded: libssl.1.0.0.dylib
  Referenced from: /Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle
  Reason: image not found - /Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle
/Users/scott/Google Drive/playground/myApp/myApp/config/application.rb:21:in `&lt;top (required)&gt;'
/Users/scott/Google Drive/playground/myApp/myApp/Rakefile:4:in `&lt;top (required)&gt;'
What does the libssl refer to? 
",<ruby-on-rails><ruby><rubygems><rake><mysql2>,700,0,6,1613,3,25,41,52,7385,0.0,50,7,13,2018-07-10 11:39,2018-07-10 15:34,2018-07-10 15:34,0.0,0.0,Basic,14
57319206,What is the meaning of && in PostGIS?,"In PostGIS, what is the result of the &amp;&amp; operation between two geometries? In my mind, &amp;&amp; returns a boolean, but does return geometry this time. In the following example, the operation is between a LineString and a Polygon.
Firstly, I guess this is the relationship between inclusion and being included. Until I do the following example, I think this should be a relationship of type ""intersection"". Am I right?
select ST_geomfromtext('linestring(0.1 0.1,1.9 1.9)', 4326) &amp;&amp; st_geomfromtext('POLYGON((0 0,0 1,1 1,1 0,0 0))', 4326)
The result is t which represents true.
",<postgresql><postgis>,594,0,9,173,1,1,6,73,10650,,8,2,13,2019-08-02 2:32,2019-08-02 2:35,2019-08-02 2:35,0.0,0.0,Basic,2
54816169,How to keep null values when writing to csv,"I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command. The issue is that Python's csv writer automatically converts Nulls into an empty string """" and it fails my job when the column is an int or float datatype and it tries to insert this """" when it should be a None or null value.
  To make it as easy as possible to interface with modules which
  implement the DB API, the value None is written as the empty string.
  https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer
What is the best way to keep the null value? Is there a better way to write csvs in Python? I'm open to all suggestions.
Example:
I have lat and long values:
42.313270000    -71.116240000
42.377010000    -71.064770000
NULL    NULL
When writing to csv it converts nulls to """":
with file_path.open(mode='w', newline='') as outfile:
    csv_writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
    if include_headers:
        csv_writer.writerow(col[0] for col in self.cursor.description)
    for row in self.cursor:
        csv_writer.writerow(row)
.
42.313270000,-71.116240000
42.377010000,-71.064770000
"""",""""
  NULL
  Specifies the string that represents a null value. The default is \N
  (backslash-N) in text format, and an unquoted empty string in CSV
  format. You might prefer an empty string even in text format for cases
  where you don't want to distinguish nulls from empty strings. This
  option is not allowed when using binary format.
  https://www.postgresql.org/docs/9.2/sql-copy.html
ANSWER:
What solved the problem for me was changing the quoting to csv.QUOTE_MINIMAL.
  csv.QUOTE_MINIMAL Instructs writer objects to only quote those fields
  which contain special characters such as delimiter, quotechar or any
  of the characters in lineterminator.
Related questions:
- Postgresql COPY empty string as NULL not work
",<python><python-3.x><postgresql><csv>,1943,5,12,1416,7,35,63,62,45790,0.0,1300,5,13,2019-02-21 21:02,2019-02-21 21:11,2019-03-08 18:23,0.0,15.0,Basic,3
58951334,"aiopg + sqlalchemy: how to ""drop table if exists"" without raw sql?","I am looking at examples of aiopg usage with sqlalchemy and these lines scare me:
async def create_table(conn):
    await conn.execute('DROP TABLE IF EXISTS tbl')
    await conn.execute(CreateTable(tbl))
I do not want to execute raw sql queries when using sqlalchemy. However I can't find any other way to implement the same logic. My attempts were:
1)
await conn.execute(tbl.drop(checkfirst=True))
This raises:
  sqlalchemy.exc.UnboundExecutionError: Table object 'tbl' is not
  bound to an Engine or Connection.  Execution can not proceed without a
  database to execute against.
Also I can't find a way to bind the table to engine because aiopg doesn't support metadata.create_all
2)
await conn.execute(DropTable(tbl))
This raises:
  psycopg2.errors.UndefinedTable: table ""tbl"" does not exist
Seems like DropTable construct doesn't support IF EXISTS part in any way.
So, the question is, is there any way to rewrite await conn.execute('DROP TABLE IF EXISTS tbl') statement into something without raw sql when using aiopg + sqlalchemy?
",<python><postgresql><sqlalchemy><drop-table><aiopg>,1038,2,8,8250,13,37,74,74,3493,0.0,1175,1,13,2019-11-20 9:51,2021-10-16 14:02,2021-10-16 14:02,696.0,696.0,Basic,3
55027706,PhpStorm - How to connect to MySQL via Docker,"I am working with PhpStorm 2018.3.4, Docker, MySQL and Ubuntu.
I tried unsuccessfully to configure MySQL with the Docker container network_mysql.
First, I have tried this configuration :
It gave me this error :
Then, I tried this :
This one gave me this other error.
Am I missing something? Is there another place where I must configure something?
docker ps output : 
Here docker network ls :
For the command docker inspect network_mysql, here is a link to the description :
https://pastebin.com/9LmeAkc8
Here is a docker-compose.yml configuration : 
https://pastebin.com/DB4Eye4y
I tried to put - ""3306:3306"" in addition to the wex_server_proxy section with no avail.
The file to modify was this one :
https://pastebin.com/TPBQNCDZ
I added the ports section, opening the 3306 port :) And then, it works.
",<mysql><docker><phpstorm>,805,12,7,463,1,8,21,45,23650,0.0,39,3,13,2019-03-06 16:20,2019-03-06 23:14,2019-03-06 23:14,0.0,0.0,Basic,3
50972190,Room Database Migration Failed: ALTER TABLE to add multiple columns,"I'm upgrading my Database from version 3 to version 4 by providing migration from 3 to 4.
Here's my code for migration:
private static Migration MIGRATION_3_4 = new Migration(3, 4) {
    @Override
    public void migrate(@NonNull SupportSQLiteDatabase database) {
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN localVideoUrl TEXT;"");
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN postType TEXT"");
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN videoUrl TEXT"");
    }
};
Here's the code which create room database
this.mAppDataBase = Room.databaseBuilder(getApplicationContext(), AppDataBase.class, ""my_db"")
                        .addMigrations(MIGRATION_2_3, MIGRATION_3_4)
                        .build();
Here's the piece of code that I have added on my PostModel
@Expose
private String postType;
@Expose
private String videoUrl;
@Expose
private String localVideoUrl;
public String getPostType() {
    return postType;
}
public void setPostType(String postType) {
    this.postType = postType;
}
public String getVideoUrl() {
    return videoUrl;
}
public void setVideoUrl(String videoUrl) {
    this.videoUrl = videoUrl;
}
public String getLocalVideoUrl() {
    return localVideoUrl;
}
public void setLocalVideoUrl(String localVideoUrl) {
    this.localVideoUrl = localVideoUrl;
}
And below is the error I'm getting. The error is not related to the notNull property of room entity. 
  java.lang.IllegalStateException: Migration didn't properly handle
  posts(com.myapp.Database.PostModel).
  Expected:
      TableInfo{name='posts', columns={imageWidth=Column{name='imageWidth', type='INTEGER',
  affinity='3', notNull=true, primaryKeyPosition=0},
  localVideoUrl=Column{name='localVideoUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  authorImageLocalUrl=Column{name='authorImageLocalUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  videoUrl=Column{name='videoUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  imageLocalUrl=Column{name='imageLocalUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, postType=Column{name='postType',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  authorName=Column{name='authorName', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, imageUrl=Column{name='imageUrl',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  id=Column{name='id', type='INTEGER', affinity='3', notNull=true,
  primaryKeyPosition=1}, title=Column{name='title', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  authorImageUrl=Column{name='authorImageUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageHeight=Column{name='imageHeight', type='INTEGER', affinity='3',
  notNull=true, primaryKeyPosition=0}}, foreignKeys=[], indices=[]}
  Found:
      TableInfo{name='posts', columns={imageWidth=Column{name='imageWidth', type='INTEGER',
  affinity='3', notNull=true, primaryKeyPosition=0},
  authorImageLocalUrl=Column{name='authorImageLocalUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageLocalUrl=Column{name='imageLocalUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  authorName=Column{name='authorName', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, imageUrl=Column{name='imageUrl',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  id=Column{name='id', type='INTEGER', affinity='3', notNull=true,
  primaryKeyPosition=1}, title=Column{name='title', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  authorImageUrl=Column{name='authorImageUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageHeight=Column{name='imageHeight', type='INTEGER', affinity='3',
  notNull=true, primaryKeyPosition=0}}, foreignKeys=[], indices=[]}
",<sqlite><android-sqlite><android-room>,3926,0,43,380,2,4,16,72,4538,0.0,9,1,13,2018-06-21 15:23,2019-09-06 8:35,,442.0,,Intermediate,26
54115837,Column does not allow DBNull.Value - No KeepNulls - Proper Column Mappings,"I am using c# with .NET 4.5.2, pushing to SQL Server 2017 14.0.1000.169
In my database, I have a table with a DateAdded field, of type DateTimeOffset.
I am attempting to BulkCopy with the following code:
private Maybe BulkCopy(SqlSchemaTable table, System.Data.DataTable dt, bool identityInsertOn)
{
    try
    {
        var options = SqlBulkCopyOptions.TableLock | SqlBulkCopyOptions.FireTriggers | SqlBulkCopyOptions.UseInternalTransaction; //  | SqlBulkCopyOptions.CheckConstraints; // Tried CheckConstraints, but it didn't change anything.
        if (identityInsertOn) options |= SqlBulkCopyOptions.KeepIdentity;
        using (var conn = new SqlConnection(_connString))
        using (var bulkCopy = new SqlBulkCopy(conn, options, null))
        {
            bulkCopy.DestinationTableName = table.TableName;
            dt.Columns.Cast&lt;System.Data.DataColumn&gt;().ToList()
                .ForEach(x =&gt; bulkCopy.ColumnMappings.Add(new SqlBulkCopyColumnMapping(x.ColumnName, x.ColumnName)));
            try
            {
                conn.Open();
                bulkCopy.WriteToServer(dt);
            }
            catch (Exception ex)
            {
                return Maybe.Failure(ex);
            }
        }
    }
    catch (Exception ex)
    {
        return Maybe.Failure(ex);
    }
    return Maybe.Success();
}
The two possible reasons I know of for the does not allow DBNull error are:
Columns are in the wrong order, which is solved by either putting them in the same order as their Database Ordinal, or by performing a Column Mapping.
KeepNulls is enabled, and DBNull.Value (or null?) are set in the DataTable.
But I am Mapping correctly and NOT ever setting KeepNulls.
Yet I am receiving the error: 
  Column DateAdded does not allow DBNull.Value
EDIT I also tried just NOT SETTING anything, including null, DBNull.Value, and DefaultValue... just literally not setting that column at all.
Also, if I Remove the DateAdded column from the DataTable, it Works.  But I don't want that.  Of the 100,000 records, maybe 20 of them have data.  So in my batches of 500, sometimes None have data in the DateAdded field, sometimes one or two have data.
So I'd like to keep the column in my DataTable but let it use the DefaultValue.
One last note: I have alternated between setting the DataColumn's Value to DBNull.Value versus dt.Columns[x.ColumnName].DefaultValue.  Both ways give the same error.
Edit 2
This is the code I'm using to populate the data in my Data Table:
foreach (var column in table)
{
    System.Data.DataRow newRow = dt.NewRow();
    foreach (var field in column)
    {
        if (!IsNull(field.Value) &amp;&amp; !IsEmptyDateOrNumber(field.ColumnType, field.Value))
        {
            // For DateAdded, this is not hit on the first batch, though there are columns Before and After DateAdded on the same row which do have value.
            // But it WILL be hit once or twice a few batches later.  So I don't want to completely remove the definition from the DataTable.
            newRow[field.ColumnName] = field.Value;
        }
        else
        {
            // newRow[field.ColumnName] = dt.Columns[field.ColumnName].DefaultValue;
            // newRow[field.ColumnName] = DBNull.Value;
            // dt.Columns[field.ColumnName].AllowDBNull = true;
        }
    }
    dt.Rows.Add(newRow);
}
IsNull() returns TRUE if the value is null or the string ""null"", as is required for my business requirements.
IsEmptyDateOrNumber() will return TRUE if the field is a numeric or date type, and the value is null or empty """".  Because while empty is valid for many string-like fields, it is never a valid numeric value.
The condition to assign the field a value is hit exactly 0 percent of the time for this particular column.  Thus nothing is set.
",<c#><sql-server><datatable>,3799,0,67,5878,2,46,58,39,19028,0.0,668,3,13,2019-01-09 17:55,2019-01-09 19:15,2019-01-16 16:59,0.0,7.0,Intermediate,31
64861331,How can I install or upgrade to sqlite 3.33.0 on Ubuntu 18.04?,"I'm currently running Ubuntu 18.04 with SQLite3. SQLite 3 is at version 3.22.0 and I need to upgrade it to version 3.33.0 to take advantage of new functionality that is available. If I remove and reinstall SQLite3 with apt-get, it just re=installs 3.22.0. How can I upgrade to the latest version of SQLite3?
",<sqlite><upgrade><ubuntu-18.04><apt-get>,308,0,0,432,0,3,13,51,9754,0.0,66,1,13,2020-11-16 15:58,2021-02-15 23:57,,91.0,,Intermediate,15
54060265,How to list files in S3 bucket using Spark Session?,"Is it possible to list all of the files in given S3 path (ex: s3://my-bucket/my-folder/*.extension) using a SparkSession object?
",<amazon-web-services><apache-spark><amazon-s3><apache-spark-sql>,129,0,0,5384,16,63,116,49,30042,0.0,1797,4,13,2019-01-06 9:34,2019-01-06 11:52,2019-01-06 20:03,0.0,0.0,Basic,3
52624097,GIN Index has O(N^2) complexity for array overlap operator?,"I ran into an issue with using the &amp;&amp; array operator on a GIN index of mine. Basically I have a query that looks like this:
SELECT *
FROM example
WHERE keys &amp;&amp; ARRAY[1,2,3,...]
This works fine for a small number of array elements (N) in the array literal, but gets really slow as N gets bigger in what appears to be O(N^2) complexity.
However, from studying the GIN data structure as described by the docs, it seems that the performance for this could be O(N). In fact, it's possible to coerce the query planner into an O(N) plan like this:
SELECT DISTINCT ON (example.id) *
FROM unnest(ARRAY[1,2,3,...]) key
JOIN example ON keys &amp;&amp; ARRAY[key]
In order to illustrate this better, I've created a jupyter notebook that populates an example table, show the query plans for both queries, and most importantly benchmarks them and plots a time vs array size (N) graph.
https://github.com/felixge/pg-slow-gin/blob/master/pg-slow-gin.ipynb
Please help me understand what causes the O(N^2) performance for query 1 and if query 2 is the best way to work around this issue.
Thanks
Felix Geisendörfer
PS: I'm using Postgres 10, but also verified that this problem exists with Postgres 11.
I've also posted this question on the postgres performance mailing list, but unfortunately didn't get any answer.
",<sql><postgresql><indexing>,1315,4,6,2932,5,27,36,50,1363,0.0,46,1,13,2018-10-03 9:39,2022-01-25 16:15,,1210.0,,Advanced,33
53989887,How do I configure PyMySQL connect for SSL?,"I'm trying to connect my database using SSL with PyMySQL, but I can't find good documentation on what the syntax is.
These credentials work in Workbench and with the CLI, but I get this error when using PyMySQL.
Can't connect to MySQL server on 'server.domain.com' ([WinError 10061] No connection could be made because the target machine actively refused it)&quot;)
db_conn = pymysql.connect(
    host=db_creds['host'],
    user=db_creds['user'],
    passwd=db_creds['passwd'],
    db=db_creds['db'],
    charset=db_creds['charset'],
    ssl={'ssl':{'ca': 'C:/SSL_CERTS/ca-cert.pem',
                'key' : 'C:/SSL_CERTS/client-key.pem',
                'cert' : 'C:/SSL_CERTS/client-cert.pem'
                }
        }
)
If I shut SSL off and drop the SSL parameter, I can connect unsecured just fine.   What am I doing wrong with the SSL parameter?
Edit: PyMySQL now wants ssl parameters listed like this instead of in a dict.
db_conn = pymysql.connect(
     host=db_creds['host'],
     user=db_creds['user'],
     passwd=db_creds['passwd'],
     db=db_creds['db'],
     charset=db_creds['charset'],
     ssl_ca='C:/SSL_CERTS/ca-cert.pem',
     ssl_key='C:/SSL_CERTS/client-key.pem',
     ssl_cert='C:/SSL_CERTS/client-cert.pem'
                          )
",<python><ssl><pymysql>,1262,0,22,403,1,4,14,56,14190,0.0,9,2,13,2018-12-31 17:20,2019-01-05 2:11,2019-01-05 2:11,5.0,5.0,Basic,14
56717592,Dangerous query method deprecation warning on Rails 5.2.3,"I am in the process of upgrading my Rails app to 5.2.3
I am using the following code in my app.
MyModel.order('LOWER(name) ASC')
It raises the following deprecation warning:
DEPRECATION WARNING: Dangerous query method (method whose arguments are used as raw SQL) called with non-attribute argument(s): ""LOWER(name)"". Non-attribute arguments will be disallowed in Rails 6.0. This method should not be called with user-provided values, such as request parameters or model attributes. Known-safe values can be passed by wrapping them in Arel.sql()
I have changed the above as the deprecation warning recommends and the warning gone away:
MyModel.order(Arel.sql('LOWER(name) ASC'))
I have surfed about related discussion here. It seems this change is introduced to disallow the SQL injections. 
But the order clause LOWER(name) ASC doesn't contains any user input. Why this ordering is considered as insecure? Is this the intended behavior or Am I missing anything here?
",<ruby-on-rails><sql-injection><ruby-on-rails-5.2>,967,1,4,1367,1,6,22,61,3109,0.0,47,1,13,2019-06-22 17:20,2019-06-23 11:26,2019-06-23 11:26,1.0,1.0,Basic,3
49047779,Google Data Studio & AWS MySQL SSL Connection,"I am trying to remotely connect Google Data Studio with our MySQL Database, which is hosted on an AWS instance. To allow for a secure connection, we added SSL access to the AWS's MySQL database user as recommended in the documentation:
GRANT USAGE ON *.* TO 'encrypted_user'@'%' REQUIRE SSL;
The problem here is that AWS, unlike GOOGLE CloudSQL, only generates a Server certificate, and not a Client certificate, nor a Client private key (as far as I can tell). Both the latter is needed to enable SSL for Google Data Studio &amp; MySQL connection. 
Just to add a side-note, we also white-listed Google's recommended IPs as listed here. There are a lot of users in this thread complaining that white-listing specific IPs does not work, they had to add wildcard on the subnets. So we have also added addresses of the /16 subnets for each IP: 
64.18.%.%
64.233.%.%
66.102.%.%
66.249.%.%
72.14.%.%
74.125.%.%
108.177.%.%
173.194.%.%
207.126.%.%
209.85.%.%
216.58.%.%
216.239.%.%
Finally, one does not need to restart the AWS firewall after white-listing new IPs, it is immediately in-effect. 
My Questions:
Is there absolutely no way to create a client certificate and a client private key on MySQL hosted on AWS ?
I would really want to use SSL between Google Data Studio (GDS) and our MySQL-DB, but the GDS-UI does not allow us to connect without filling in the client certificate and client private key. Is there any work around at the moment for me to allow this secure connection ? 
Thanks in advance!
",<mysql><amazon-web-services><ssl><looker-studio>,1504,5,13,1401,2,20,37,50,7823,0.0,433,1,13,2018-03-01 10:35,2018-06-08 13:31,,99.0,,Intermediate,29
52728319,Can't set lower_case_table_names in MySQL 8.x on Windows 10,"In MySQL 8.0.12 running on Windows 10, it seems impossible to set lower_case_table_names to 2, so as to achieve the appearance of mixed case DB and table names in Workbench. I realize that under the hood these objects may remain lower case, which is fine. But I want it to look right in Workbench, and I could always achieve this in previous versions of MySQL. When I attempt to do that and restart the service so it takes effect, the service crashes and stops. In the mysql logs I see this:
  Different lower_case_table_names settings for server ('2') and data
  dictionary ('1'). 
  Data Dictionary initialization failed.
This seems to be a common problem for a lot of people.
I read here that the solution is: 
  So lower_case_table_names needs to be set together with
  --initialize.
But I have no idea what that means, or how to set it at startup. I have googled all over and read several forum articles but I can't find clear instructions on how to resolve this.
",<mysql><windows><mysql-workbench>,969,1,2,6916,24,81,159,77,11801,,1220,2,13,2018-10-09 19:47,2018-10-11 23:56,2018-10-16 23:35,2.0,7.0,Advanced,39
50477946,Detect duplicate items in recursive CTE,"I have a set of dependencies stored in my database. I'm looking to find all the objects that depend on the current one, whether directly or indirectly.  Since objects can depend zero or more other objects, it's perfectly reasonable that object 1 is depended on by object 9 twice (9 depends on 4 and 5, both of which depend on 1).  I'd like to get the list of all the objects that depend on the current object without duplication.
This gets more complex if there are loops.  Without loops, one could use DISTINCT, though going through long chains more than once only to cull them at the end is still a problem.  With loops, however, it becomes important that the RECURSIVE CTE doesn't union with something it has already seen.
So what I have so far looks like this:
WITH RECURSIVE __dependents AS (
  SELECT object, array[object.id] AS seen_objects
  FROM immediate_object_dependents(_objectid) object
  UNION ALL
  SELECT object, d.seen_objects || object.id
  FROM __dependents d
  JOIN immediate_object_dependents((d.object).id) object
    ON object.id &lt;&gt; ALL (d.seen_objects)
) SELECT (object).* FROM __dependents;
(It's in a stored procedure, so I can pass in _objectid)
Unfortunately, this just omits a given object when I've seen it before in the current chain, which would be fine if a recursive CTE was being done depth-first, but when it's breadth-first, it becomes problematic.
Ideally, the solution would be in SQL rather than PLPGSQL, but either one works.
As an example, I set this up in postgres:
create table objectdependencies (
  id int,
  dependson int
);
create index on objectdependencies (dependson);
insert into objectdependencies values (1, 2), (1, 4), (2, 3), (2, 4), (3, 4);
And then I tried running this:
with recursive rdeps as (
  select dep
  from objectdependencies dep
  where dep.dependson = 4 -- starting point
  union all
  select dep
  from objectdependencies dep
  join rdeps r
    on (r.dep).id = dep.dependson
) select (dep).id from rdeps;
I'm expecting ""1, 2, 3"" as output.
However, this somehow goes on forever (which I also don't understand).  If I add in a level check (select dep, 0 as level, ... select dep, level + 1, on ... and level &lt; 3), I see that 2 and 3 are repeating.  Conversely, if I add a seen check:
with recursive rdeps as (
  select dep, array[id] as seen
  from objectdependencies dep
  where dep.dependson = 4 -- starting point
  union all
  select dep, r.seen || dep.id
  from objectdependencies dep
  join rdeps r
    on (r.dep).id = dep.dependson and dep.id &lt;&gt; ALL (r.seen)
) select (dep).id from rdeps;
then I get 1, 2, 3, 2, 3, and it stops. I could use DISTINCT in the outer select, but that only reasonably works on this data because there is no loop. With a larger dataset and more loops, we will continue to grow the CTE's output only to have the DISTINCT pare it back down.  I would like the CTE to simply stop that branch when it's already seen that particular value somewhere else.
Edit: this is not simply about cycle detection (though there can be cycles). It's about uncovering everything referenced by this object, directly and indirectly.  So if we have 1->2->3->5->6->7 and 2->4->5, we can start at 1, go to 2, from there we can go to 3 and 4, both of those branches will go to 5, but I don't need both branches to do so - the first one can go to 5, and the other can simply stop there. Then we go on to 6 and 7. Most cycle detection will find no cycles and return 5, 6, 7 all twice. Given that I expect most of my production data to have 0-3 immediate references, and most of those to be likewise, it will be very common for there to be multiple branches from one object to another, and going down those branches will be not only redundant but a huge waste of time and resource.
",<sql><postgresql><common-table-expression>,3772,0,43,21814,5,42,69,76,3894,0.0,1133,4,13,2018-05-22 23:47,2018-05-25 23:35,2018-05-29 6:00,3.0,7.0,Intermediate,17
53197922,Difference between .query() and .execute() in MySQL,"I'm having difficulty comprehending the implementation of prepared statements. I've done a fair amount of research but most of the information I found is either out of context or contain examples far more complex than what I'm trying to accomplish. Can anyone clarify for me why the execute method in the second example below is throwing a syntax error?
NOTE: I'm using the node-mysql2 package here.
controller.js  (using query mysql method)
  const db = require(""../lib/database"");
  async addNewThing(req, res, next) {
    let data = req.body
    const queryString = 'INSERT INTO table SET ?'
    try {
      await db.query(queryString, data)
      res.status(201).json({
        message: 'Record inserted',
        data
      })
    } catch (error) {
      next(error)
    }
  }
Record is successfully inserted into the database
controller.js    (using execute mysql method)
  const db = require(""../lib/database"");
  async addNewThing(req, res, next) {
    let data = req.body
    const queryString = 'INSERT INTO table SET ?'
    try {
      await db.execute(queryString, [data])
      res.status(201).json({
        message: 'Record inserted',
        data
      })
    } catch (error) {
      next(error)
    }
  }
Results in the following error:
  You have an error in your SQL syntax; check the manual that
  corresponds to your MySQL server version for the right syntax to use
  near '?' at line 1
data
{ thing_id: '987654', thing_name: 'thing' }
",<mysql><node.js><express><syntax-error>,1457,0,37,472,1,4,13,69,13077,0.0,6,1,13,2018-11-07 21:15,2018-11-09 3:08,2018-11-09 3:08,2.0,2.0,Intermediate,19
52791121,Is Java Spring JPA native query SQL injection proof?,"I'm writing this question because I didn't find any useful article about how to prevent SQL Injection in Spring Data JPA. All the tutorials are showing how to use these queries but they don't mentioned anything about these possible attacks.
I'm having the following query:
@Repository
public interface UserRepository extends CrudRepository&lt;User, Integer&gt; {
    @Query(nativeQuery = true, value = ""SELECT * FROM users WHERE email LIKE %:emailAddress%"")
    public ResponseList&lt;User&gt; getUsers(@Param(""emailAddress"") String emailAddress);
}
The rest controller to deliver the request:
@RequestMapping(value = ""/get-users"", method = RequestMethod.POST)
public Response&lt;StringResponse&gt; getUsers(WebRequest request) {
    return userService.getUsers(request.getParameter(""email""));
}
Are JPQL or native query parameters escaped before executing them?
This is the query with SQL injection executed in my MySQL console which drops the users table:
SELECT * FROM users WHERE email LIKE '%'; DROP TABLE users; -- %';
I have tried to execute the SQL attack by sending a POST request to the server:
http://localhost:8080/get-users
POST: key/value: ""email"" : ""'; DROP TABLE users; --""
I have enabled Hibernate's sql logging and this is what the above request produced:
[http-nio-8080-exec-8] DEBUG org.hibernate.SQL - SELECT * FROM users WHERE email LIKE ?
Hibernate: SELECT * FROM users WHERE email LIKE ?
[http-nio-8080-exec-8] DEBUG org.hibernate.loader.Loader - bindNamedParameters() %'; DROP TABLE users; -- % -&gt; emailAddress [1]
[http-nio-8080-exec-8] TRACE o.h.type.descriptor.sql.BasicBinder - binding parameter [1] as [VARCHAR] - [%'; DROP TABLE users; -- %]
The table wasn't dropped (which is good) but why the parameter isn't escaped?
What if I don't annotate the @Param(""emailAddress"") and I use indexed parameters?:
@Query(nativeQuery = true, value = ""SELECT * FROM users WHERE email LIKE ?1"")
public ResponseList&lt;User&gt; getUsers(String email);
",<spring-data-jpa><sql-injection>,1971,1,21,9447,22,84,137,39,12261,0.0,261,1,13,2018-10-13 8:37,2018-10-14 12:42,2018-10-14 12:42,1.0,1.0,Advanced,43
50414906,"Why am I getting an ""Invalid use of Nulls"" error in my query?","I use a userform in Excel to run some internal queries between my spreadsheets.
However I am getting Invalid Use of Null. I am aware of the nz null syntax (SQL MS Access - Invalid Use of Null), however my queries can be quite large, and I am wondering if there is anything I can add to my VBA code to allow nulls. 
",<sql><excel><vba><ms-access>,315,1,2,1584,6,33,69,37,3362,,303,1,13,2018-05-18 15:40,2018-05-23 19:56,2018-05-23 19:56,5.0,5.0,Basic,13
52644981,Spark: Return empty column if column does not exist in dataframe,"As shown in the below code, I am reading a JSON file into a dataframe and then selecting some fields from that dataframe into another one.
df_record = spark.read.json(""path/to/file.JSON"",multiLine=True)
df_basicInfo = df_record.select(col(""key1"").alias(""ID""), \
                                col(""key2"").alias(""Status""), \
                                col(""key3.ResponseType"").alias(""ResponseType""), \
                                col(""key3.someIndicator"").alias(""SomeIndicator"") \
                                )
Issue is that some times, the JSON file does not have some of the keys that I try to fetch - like ResponseType. So it ends up throwing errors like:
org.apache.spark.sql.AnalysisException: No such struct field ResponseType
How can I get around this issue without forcing a schema at the time of read? is it possible to make it return a NULL under that column when it is not available?
how do I detect if a spark dataframe has a column Does mention how to detect if a column is available in a dataframe. This question, however, is about how to use that function. 
",<apache-spark><pyspark><apache-spark-sql>,1086,1,9,833,3,10,26,39,32979,0.0,23,6,13,2018-10-04 10:55,2018-10-04 12:06,2018-10-04 12:06,0.0,0.0,Basic,9
56776974,Connect to a database in cloud,"I have an SQLite database (110kb) in an S3 bucket. I want to connect to that database every time I run my Python application.
An option is to download database everytime I run the Python application and connect it. But I want to know if there exists a way to connect to that SQLite database through memory, using S3FileSystem and open.
I'm using SQLite3 library in Python 3.6.
",<python><python-3.x><sqlite><amazon-s3><in-memory-database>,377,0,2,2063,3,15,26,71,18444,0.0,476,5,13,2019-06-26 16:04,2019-06-26 18:18,2019-12-21 6:48,0.0,178.0,Basic,3
54124262,"How to fix ""Illuminate\Database\QueryException: SQLSTATE[HY000] [1044] Access denied for user""","I tried to run: php artisan migrate
Also to connect to MySQL using Xampp on Windows.
I Got this error:
Illuminate\Database\QueryException  : SQLSTATE[HY000] [1044] Access
denied for user ''@'localhost' to database 'homestead' (SQL: select *
from information_schema.tables where table_schema = homestead and
table_name = migrations)
  at
C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|  Exception trace:
  1   PDOException::(&quot;SQLSTATE[HY000] [1044] Access denied for user
''@'localhost' to database 'homestead'&quot;)
      C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  2  
PDO::__construct(&quot;mysql:host=127.0.0.1;port=3306;dbname=homestead&quot;,
&quot;homestead&quot;, &quot;&quot;, [])
      C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  Please use the argument -v to see more details.
.env file:
DB_CONNECTION=mysql 
DB_HOST=127.0.0.1 
DB_PORT=3306 
DB_DATABASE=homestead 
DB_USERNAME=homestead 
DB_PASSWORD=
",<php><mysql><laravel><laravel-5>,1581,0,34,349,1,5,15,54,144061,0.0,5,10,13,2019-01-10 8:07,2019-01-10 8:38,2019-01-10 8:38,0.0,0.0,Basic,9
53382161,"message -""could not read a hi value - you need to populate the table: hibernate_sequence""","My problem is as follows: When i create POST request in ""Postman"" app. This is what i try to POST 
  {""name"": ""John Doe"", ""email"":""jdoe@test.com"", ""city"": ""London""}
I am getting the following error:
{
""timestamp"": ""2018-11-19T20:16:00.486+0000"",
""status"": 500,
""error"": ""Internal Server Error"",
""message"": ""could not read a hi value - you need to populate the table: hibernate_sequence; nested exception is org.hibernate.id.IdentifierGenerationException: could not read a hi value - you need to populate the table: hibernate_sequence"",
""path"": ""/api/ver01/product""
}
I was looking for answer in search box but none of them helped me. So i think that the problem is in sql code but I am not sure. Whole project is written in intelliJ IDE.
This is my Product class.
package com.hubertkulas.webstore.store.archetype;
import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;
import java.math.BigDecimal;
import java.sql.Date;
@Entity
@JsonIgnoreProperties({""hibernateLazyInitializer"", ""handler""})
public class Product {
@Id
@GeneratedValue(strategy = GenerationType.AUTO)
private Long id;
private boolean contact;
private String email;
private String category;
private String name;
private String city;
private String model;
private BigDecimal price;
@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = ""MM-dd-yyyy"")
private Date date;
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getEmail() {
    return email;
}
public void setEmail(String email) {
    this.email = email;
}
public String getCity() {
    return city;
}
public void setCity(String city) {
    this.city = city;
}
public String getCategory() {
    return category;
}
public void setCategory(String category) {
    this.category = category;
}
public String getModel() {
    return model;
}
public void setModel(String model) {
    this.model = model;
}
public BigDecimal getPrice() {
    return price;
}
public void setPrice(BigDecimal price) {
    this.price = price;
}
public Date getDate() {
    return date;
}
public void setDate(Date date) {
    this.date = date;
}
public boolean isContact() {
    return contact;
}
public void setContact(boolean contact) {
    this.contact = contact;
}
public Long getId() {
    return id;
}
// setter for id because Jackson will use it
public void setId(Long id) {
    this.id = id;
}
}
This is my ProductController class
package com.hubertkulas.webstore.store.controllers;
import com.hubertkulas.webstore.store.archetype.Product;
import com.hubertkulas.webstore.store.jparepository.ProductRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.*;
import java.util.List;
@RestController
@RequestMapping(""api/ver01/product"")
public class ProductController {
//injecting ProductRepository when ProductController is called
@Autowired
private ProductRepository productRepository;
@GetMapping
public List&lt;Product&gt; list() {
    //finds all of the records and returns it
   return productRepository.findAll();
}
@PostMapping
@ResponseStatus(HttpStatus.OK)
public void create(@RequestBody Product product){
    productRepository.save(product);
}
@GetMapping(""/{id}"")
public Product get(@PathVariable(""id"") long id){
    // return specific record with added id
    return productRepository.getOne(id);
}
}
This is my ProductRepository Interface
package com.hubertkulas.webstore.store.jparepository;
import com.hubertkulas.webstore.store.archetype.Product;
import org.springframework.data.jpa.repository.JpaRepository;
//Using Jpa for CRUD operations
public interface ProductRepository extends JpaRepository&lt;Product, Long&gt; {
}
And this is my database
CREATE TABLE
product
(
    id BIGINT NOT NULL,
    contact BOOLEAN NOT NULL,
    email VARCHAR,
    category VARCHAR,
    name VARCHAR,
    city VARCHAR,
    date DATETIME,
    price NUMERIC,
    model VARCHAR,
    PRIMARY KEY (id)
);
CREATE TABLE
hibernate_sequence
(
    next_val BIGINT
);
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (1, 1, 'abraham@site.com', 'Electronics', 'Abraham Westbrom', 'New 
York', 4419619200000, '3250');
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (2, 1, 'udon@site.com', 'Electronics', 'Udon Hon', 'London', 
4419619200000, '799');
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (3, 0, 'mateuszsinus@site.com', 'Software', 'Mateusz Sinus', 
'Warsaw', 4419619200000, '10000');
INSERT INTO hibernate_sequence (next_val) VALUES (4);
",<java><sql><hibernate><spring-boot><postman>,4821,0,193,168,1,2,10,47,25192,0.0,8,4,13,2018-11-19 20:28,2018-11-19 20:40,2018-11-19 20:40,0.0,0.0,Basic,9
54781017,How to Map Input and Output Columns dynamically in SSIS?,"I Have to Upload Data in SQL Server from .dbf Files through SSIS.
My Output Column is fixed but the input column is not fixed because the files come from the client and the client may have updated data by his own style. there may be some unused columns too or the input column name can be different from the output column.
One idea I had in my mind was to map files input column with output column in SQL Database table and use only those column which is present in the row for file id.
But I am not getting how to do that. Any idea?
Table Example
FileID
InputColumn
OutputColumn
Active
1
CustCd
CustCode
1
1
CName
CustName
1
1
Address
CustAdd
1
2
Cust_Code
CustCode
1
2
Customer Name
CustName
1
2
Location
CustAdd
1
",<sql><sql-server><ssis><etl><ssis-2012>,717,0,0,623,3,11,25,46,14738,0.0,57,1,13,2019-02-20 7:33,2019-02-20 23:59,2019-02-20 23:59,0.0,0.0,Basic,9
59389911,sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine,"I am trying to push the excel xlsx data to mySQl Alchemy by using this simple code...
import pandas as pd
import os
import sqlalchemy
mydir = (os.getcwd()).replace('\\', '/') + '/'
# MySQL Connection
MYSQL_USER = 'xxxxxxx'
MYSQL_PASSWORD = 'xxxxxxxx'
MYSQL_HOST_IP = '127.0.0.1'
MYSQL_PORT = 3306
MYSQL_DATABASE = 'xlsx_test_db'
# connect db
engine = sqlalchemy.create_engine('mysql+mysqlconnector://' + MYSQL_USER + ':' + MYSQL_PASSWORD + '@' + MYSQL_HOST_IP + ':' + str(
    MYSQL_PORT) + '/' + MYSQL_DATABASE, echo=False)
engine.connect()
# reading and insert one file at a time
for file in os.listdir('.'):
    # only process excels files
    file_basename, extension = file.split('.')
    if extension == 'xlsx':
        df = pd.read_excel(r'' + mydir + 'MNM_Rotterdam_5_Daily_Details-20191216081027.xlsx', sheet_name='Report')
        df.to_sql(file_basename, con=engine, if_exists='replace')
But I found this error
Traceback (most recent call last):
  File ""C:/Users/DELL/PycharmProjects/automateDB/myWatchDog.py"", line 28, in &lt;module&gt;
    df.to_sql(file_basename, con=engine, if_exists='replace')
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\core\generic.py"", line 2532, in to_sql
    dtype=dtype, method=method)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 460, in to_sql
    chunksize=chunksize, dtype=dtype, method=method)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 1173, in to_sql
    table.create()
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 577, in create
    self.pd_sql.drop_table(self.name, self.schema)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 1222, in drop_table
    self.meta.reflect(only=[table_name], schema=schema)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\sqlalchemy\sql\schema.py"", line 3956, in reflect
    (bind.engine, s, ', '.join(missing)))
sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine(mysql+mysqlconnector://root:***@127.0.0.1:3306/xlsx_test_db): (MNM_Rotterdam_5_Daily_Details-20191216081027)
So any one could me to solve this...
Thank you...
I hope it would be clear enough....
",<python><pandas><sqlalchemy>,2358,0,45,2279,7,36,80,37,16097,0.0,328,4,13,2019-12-18 10:23,2020-03-28 19:39,,101.0,,Basic,9
57822113,More than 24 hours in a day in postgreSQL,"Assuming I have this schema:
create table rental (
    id           integer,
    rental_date  timestamp,
    customer_id  smallint,
    return_date  timestamp,
);
Running this query returns strange results:
select customer_id, avg(return_date - rental_date) as ""avg""
from rental
group by customer_id
order by ""avg"" DESC
It displays:
customer_id|avg_rent_duration     |
-----------|----------------------|
        315|     6 days 14:13:22.5|
        187|5 days 34:58:38.571428|
        321|5 days 32:56:32.727273|
        539|5 days 31:39:57.272727|
        436|       5 days 31:09:46|
        532|5 days 30:59:34.838709|
        427|       5 days 29:27:05|
        555|5 days 26:48:35.294118|
...
599 rows
Why is there values like 5 days 34:58:38, 5 days 32:56:32 and so on? I thought there where only 24 hours in a day, maybe I'm wrong.
EDIT
Demo here: http://sqlfiddle.com/#!17/caa7a/1/0
Sample data:
insert into rental (rental_date, customer_id, return_date)
values
('2007-01-02 13:10:06', 1, '2007-01-03 01:01:01'),
('2007-01-02 01:01:01', 1, '2007-01-09 15:10:06'),
('2007-01-10 22:10:06', 1, '2007-01-11 01:01:01'),
('2007-01-30 01:01:01', 1, '2007-02-03 22:10:06');
",<sql><postgresql><intervals>,1173,2,31,30632,39,173,267,73,2400,0.0,1001,2,13,2019-09-06 12:31,2019-09-06 14:43,2019-09-06 14:43,0.0,0.0,Basic,1
49408290,Django + Docker + SQLite3 How to mount database to the container?,"I had sqlite database with some sort of tables on my localhost and i wanted to copy and paste that database to my server which runs on docker. I created paths like this:
db_data: there is my sqlite database which i want to run in my django project.
web: there is my whole django project
in my docker-compose.yml i writed this volume:
version: ""3""
services:
  web:
    build: ./web/
    ports:
      - ""8001:8001""
    volumes:
      - ./web:/code
      - /home/cosmo/db_data/db.sqlite3:/code/db.sqlite3
    command: python manage.py runserver 0.0.0.0:8001
So i thik that docker will get database in my db_data and will make volume inside my web folder (in my project. There i had database on my localhost so it wouldnt be problem.) But i will paste here settings.py:
# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# Database
# https://docs.djangoproject.com/en/2.0/ref/settings/#databases
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
So when i will open my db.sqlite3 inside db_data every tables and content are there, but when i will run container the db.sqlite3 in my project folder (web) is empty.
When i will run docker ps command there is no database container maybe this is the problem i dont know. I have there only:
in the red circle is my django container. So when i will run my server and try to login every account from the database is unknown. So I think that container works with that empty db in my project. Someone please has any solution? Thanks.
",<django><sqlite><docker><mount>,1654,3,25,491,0,7,23,49,9856,0.0,25,1,13,2018-03-21 13:56,2021-02-17 4:36,2021-02-17 4:36,1064.0,1064.0,Basic,9
62911496,postgresql-client-13 : Depends: libpq5 (>= 13~beta2) but 12.3-1.pgdg18.04+1 is to be installed,"I want to try new PostgreSQL and follow this instruction. But installation fails:
$ sudo apt install postgresql-client-13
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
 postgresql-client-13 : Depends: libpq5 (&gt;= 13~beta2) but 12.3-1.pgdg18.04+1 is to be installed
E: Unable to correct problems, you have held broken packages.
I also tried this instruction to resolve unmet dependencies
What did I wrong and how to install psql 13?
UPD
Content of my sources.list.d:
kes@kes-X751SA /etc/apt/sources.list.d $ cat pgdg.list 
deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg main
kes@kes-X751SA /etc/apt/sources.list.d $ cat pgdg-testing.list 
deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg-testing main 13
Also:
$ sudo apt-cache policy postgresql-13
postgresql-13:
  Installed: (none)
  Candidate: 13~beta2-1.pgdg18.04+1
  Version table:
     13~beta2-1.pgdg18.04+1 100
        100 http://apt.postgresql.org/pub/repos/apt bionic-pgdg-testing/13 amd64 Packages
",<postgresql><linux-mint-19><postgresql-13>,1367,4,28,22762,17,111,163,43,12564,0.0,4593,3,13,2020-07-15 9:15,2020-08-07 9:51,2020-08-07 9:51,23.0,23.0,Basic,9
58562024,"Docker password authentication failed for user ""postgres""","I'm writing a docker-compose file to launch some services. But the db service is a trouble maker, I always get this error:
FATAL:  password authentication failed for user ""postgres""
DETAIL:  Password does not match for user ""postgres"".
Connection matched pg_hba.conf line 95: ""host all all all md5""
I've read a lot of threads, and I've correctly set the POSTGRES_USER and POSTGRES_PASSWORD. I have also remove the previous volumes and container to force postgresql to re-init the password. But I can't figure out why it's still not working.
So what is the correct way to force the re-initialization of the postgresql image. So I would be able to connect to my database.
I've seen that this error: Connection matched pg_hba.conf line 95: ""host all all all md5"", and I've heard about the postgres conf file. But it's an official container it's supposed to work, isn't it ?
version: '3'
services:
  poll:
    build: poll
    container_name: ""poll""
    ports:
      - ""5000:80""
    networks:
      - poll-tier
    environment:
      - REDIS_HOST=redis
    depends_on:
      - redis
  worker:
    build: worker
    container_name: ""worker""
    networks:
      - back-tier
    environment:
      - REDIS_HOST=redis
      - POSTGRES_HOST=db
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
    depends_on:
      - redis
      - db
  redis:
    image: ""redis:alpine""
    container_name: ""redis""
    networks:
      - poll-tier
      - back-tier
  result:
    build: result
    container_name: ""result""
    ports:
      - ""5001:80""
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
      - POSTGRES_HOST=db
      - RESULT_PORT=80
    networks:
      - result-tier
    depends_on:
      - db
  db:
    image: ""postgres:alpine""
    container_name: ""db""
    restart: always
    networks:
      - back-tier
      - result-tier
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=postgres
volumes:
  db-data:
    driver: local
networks:
  poll-tier: {}
  back-tier: {}
  result-tier: {}
I'm expected to get the db connected, and not password authentication failed for user ""postgres"".
",<postgresql><docker><docker-compose>,2189,0,76,991,1,8,22,68,26693,0.0,173,2,13,2019-10-25 16:00,2019-10-25 19:41,2019-10-25 19:41,0.0,0.0,Basic,9
48633900,Presto: Cast timestamp w/TZ to plain timestamp WITHOUT converting to UTC,"This query in Presto:
select *, 
  cast(ts_w_tz as timestamp) as ts, 
  cast(substr(cast(ts_w_tz as varchar), 1, 23) as timestamp) as local_ts_workaround 
from (select timestamp '2018-02-06 23:00:00.000 Australia/Melbourne' as ts_w_tz);
Returns:
                   ts_w_tz                   |           ts            |   local_ts_workaround   
---------------------------------------------+-------------------------+-------------------------
 2018-02-06 23:00:00.000 Australia/Melbourne | 2018-02-06 12:00:00.000 | 2018-02-06 23:00:00.000
As you can see, the act of casting the timestamp with timezone to a timestamp has resulted in the timestamp being converted back to UTC time (eg ts). IMO the correct behaviour should be to return the 'wall reading' of the timestamp, as per local_ts_workaround.
I realise there are many posts about how Presto's handling of this is wrong and doesn't conform to the SQL standard, and that there is a fix in the works. But in the meantime this is a major pain since the effect is that there appears to be no built in way to get a localised timestamp withOUT timezone (as per local_ts_workaround).
Obviously, I have the string conversion workaround for now, but this seems horrible. I am wondering if anyone has a better workaround or can point out something that I have missed?
Thanks.
",<sql><presto>,1322,0,10,8162,10,35,32,45,12762,0.0,15,2,13,2018-02-06 0:59,2020-05-07 6:28,2021-02-15 18:07,821.0,1105.0,Advanced,33
53513963,How to restore a database from bak file from azure data studio on Mac,"Previously on Mac I use mysql operation studio and I click on database and click restore then browse to my bak file, but now they change to azure data studio and when I repeat the same steps I got this error:
""You must enable preview features in order to use restore"" 
but I cannot figure out hot to enable that. I have googled and tried few things even open my azure, microsoft account on website but I do not see that option. 
Can some one help please !
",<sql><azure-sql-database>,456,1,0,1259,3,22,52,53,13507,0.0,84,4,13,2018-11-28 7:10,2018-12-11 15:49,2018-12-11 15:49,13.0,13.0,Basic,9
48396535,How can I modify the time zone in Azure SQL Database?,"Can you please tell me how to change the time zone in Azure SQL Database?
",<azure-sql-database>,74,0,0,141,1,1,4,63,24596,0.0,0,1,13,2018-01-23 7:44,2018-01-23 12:14,,0.0,,Basic,9
59488379,AWS Athena partition fetch all paths,"Recently, I've experienced an issue with AWS Athena when there is quite high number of partitions.
The old version had a database and tables with only 1 partition level, say id=x. Let's take one table; for example, where we store payment parameters per id (product), and there are not plenty of IDs. Assume its around 1000-5000. Now while querying that table with passing id number on where clause like "".. where id = 10"". The queries were returned pretty fast actually. Assume we update the data twice a day.
Lately, we've been thinking to add another partition level for day like, ""../id=x/dt=yyyy-mm-dd/.."". This means that partition number grows  xID times per day if a month passes and if we have 3000 IDs, we'd approximately get 3000x30=90000 partitions a month. Thus, a rapid grow in number of partitions.
On, say 3 months old data (~270k partitions), we'd like to see a query like the following would return in at most 20 seconds or so.
select count(*) from db.table where id = x and dt = 'yyyy-mm-dd'
This takes like a minute.
The Real Case
It turns out Athena first fetches the all partitions (metadata) and s3 paths (regardless the usage of where clause) and then filter those s3 paths that you would like to see on where condition. The first part (fetching all s3 paths by partitions lasts long proportionally to the number of partitions)
The more partitions you have, the slower the query executed.
Intuitively, I expected that Athena fetches only s3 paths stated on where clause, I mean this would be the one way of magic of the partitioning. Maybe it fetches all paths 
Does anybody know a work around, or do we use Athena in a wrong way ?
Should Athena be used only with small number of partitions ?
Edit
In order to clarify the statement above, I add a piece from support mail.
from Support
  ...
  You mentioned that your new system has 360000 which is a huge number.
  So when you are doing select * from &lt;partitioned table&gt;, Athena first download all partition metadata and searched S3 path mapped with
  those partitions. This process of fetching data for each partition
  lead to longer time in query execution.
  ...
Update
An issue opened on AWS forums. The linked issue raised on aws forums is here.
Thanks.
",<amazon-web-services><nosql><aws-glue><presto><amazon-athena>,2239,1,2,2037,1,16,25,61,1429,0.0,110,1,13,2019-12-26 12:18,2020-03-28 16:34,2020-03-28 16:34,93.0,93.0,Advanced,33
