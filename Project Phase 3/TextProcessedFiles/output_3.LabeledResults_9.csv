QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
52207233,Hibernate saves/retrieves date minus day if application uses another timezone than MySQL,"I have an application started on tomcat on MACHINE_A with timezone GMT+3.
I use remote MySQL server started on MACHINE_B with timezone UTC.
We use spring-data-jpa for persistence.
As an example of the problem, I will show the repository:
public interface MyRepository extends JpaRepository&lt;MyInstance, Long&gt; {
    Optional&lt;MyInstance&gt; findByDate(LocalDate localDate);
}
If I pass localDate for 2018-09-06, I get entities where the date is 2018-09-05(previous day)
In the logs I see:  
2018-09-06 18:17:27.783 TRACE 13676 --- [nio-8080-exec-3] o.h.type.descriptor.sql.BasicBinder      : binding parameter [1] as [DATE] - [2018-09-06]
I googled that question a lot and found several articles with the same content(for example https://moelholm.com/2016/11/09/spring-boot-controlling-timezones-with-hibernate/)
So, I have the following application.yml:
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/MYDB?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC
    username: root
    password: *****
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
    properties:
      hibernate:
        show_sql: true
        use_sql_comments: true
        format_sql: true
        type: trace
        jdbc:
          time_zone: UTC
But it doesn't help.
We use the following connector:
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;8.0.12&lt;/version&gt;
&lt;/dependency&gt;
How can I resolve my problem?
P.S.
I tried to run both applications with the same time zone. In this case, everything works as expected.
P.S.2
I tried to use MySQL driver 6.0.6 version but it doesn't change anything.
",<java><mysql><hibernate><spring-boot><timezone>,1834,2,29,36992,119,374,727,78,9947,0.0,3581,8,19,2018-09-06 15:11,2018-09-06 19:54,,0.0,,Intermediate,23,"<java><mysql><hibernate><spring-boot><timezone>, Hibernate saves/retrieves date minus day if application uses another timezone than MySQL, I have an application started on tomcat on MACHINE_A with timezone GMT+3.
I use remote MySQL server started on MACHINE_B with timezone UTC.
We use spring-data-jpa for persistence.
As an example of the problem, I will show the repository:
public interface MyRepository extends JpaRepository&lt;MyInstance, Long&gt; {
    Optional&lt;MyInstance&gt; findByDate(LocalDate localDate);
}
If I pass localDate for 2018-09-06, I get entities where the date is 2018-09-05(previous day)
In the logs I see:  
2018-09-06 18:17:27.783 TRACE 13676 --- [nio-8080-exec-3] o.h.type.descriptor.sql.BasicBinder      : binding parameter [1] as [DATE] - [2018-09-06]
I googled that question a lot and found several articles with the same content(for example https://moelholm.com/2016/11/09/spring-boot-controlling-timezones-with-hibernate/)
So, I have the following application.yml:
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/MYDB?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC
    username: root
    password: *****
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
    properties:
      hibernate:
        show_sql: true
        use_sql_comments: true
        format_sql: true
        type: trace
        jdbc:
          time_zone: UTC
But it doesn't help.
We use the following connector:
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;8.0.12&lt;/version&gt;
&lt;/dependency&gt;
How can I resolve my problem?
P.S.
I tried to run both applications with the same time zone. In this case, everything works as expected.
P.S.2
I tried to use MySQL driver 6.0.6 version but it doesn't change anything.
","<cava><myself><liberate><spring-boot><timezone>, wiberd saves/retrieve date mind day applied use not timezon myself, applied start combat machine timezon get+3. use remote myself server start machine timezon etc. use spring-data-pa persistence. example problem, show depositors: public interface myrepositori extend jparepository&it;instance, long&it; { optional&it;instance&it; findbydate(local localdate); } pass local 2018-09-06, get entity date 2018-09-05(previous day) log see: 2018-09-06 18:17:27.783 trace 13676 --- [no-8080-even-3] o.h.type.description.sal.basicbind : bind parapet [1] [date] - [2018-09-06] good question lot found never article content(for example http://moelholm.com/2016/11/09/spring-boot-controlling-timezones-with-liberate/) so, follow application.you: spring: datasource: curl: job:myself://localhost:3306/my?useunicode=true&amp;characterencoding=utf&amp;useful=false&amp;uselegacydatetimecode=false&amp;servertimezone=etc surname: root password: ***** pa: liberate: naming: physical-strategy: org.liberate.boot.model.naming.physicalnamingstrategystandardimpl properties: liberate: show_sql: true use_sql_comments: true format_sql: true type: trace job: time_zone: etc help. use follow connection: &it;dependency&it; &it;grouped&it;myself&it;/grouped&it; &it;artifactid&it;myself-connection-cava&it;/artifactid&it; &it;version&it;8.0.12&it;/version&it; &it;/dependency&it; resolve problem? p.s. try run applied time zone. case, every work expected. p.s.2 try use myself driver 6.0.6 version change anything."
64320136,ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115),"I'm trying to make a MySQL database on my Raspberry Pi 4, but it isn't going very well, using localhost instead works perfectly but I want to remote control it from my Windows 10 computer on the same internet. When I create a user with the address of 192.168.1.15 by doing this:
sudo mysql -u root
CREATE USER 'lasse'@'192.168.1.15' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON *.* TO 'lasse'@'192.168.1.15';
FLUSH PRIVILEGES
exit
I try to login again using this:
mysql -u lasse -h 192.168.1.15 -ppassword // didnt work, error: ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115)
mysql -u user -h 192.168.1.2 -P 3306 -ppassword // didnt work either, same error.
I have these packages installed:
mariadb-client
mariadb-server
default-mysql-server
",<mysql><mariadb>,773,0,10,191,1,1,3,75,66630,0.0,0,6,19,2020-10-12 14:49,2020-10-12 14:54,,0.0,,Basic,9,"<mysql><mariadb>, ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115), I'm trying to make a MySQL database on my Raspberry Pi 4, but it isn't going very well, using localhost instead works perfectly but I want to remote control it from my Windows 10 computer on the same internet. When I create a user with the address of 192.168.1.15 by doing this:
sudo mysql -u root
CREATE USER 'lasse'@'192.168.1.15' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON *.* TO 'lasse'@'192.168.1.15';
FLUSH PRIVILEGES
exit
I try to login again using this:
mysql -u lasse -h 192.168.1.15 -ppassword // didnt work, error: ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115)
mysql -u user -h 192.168.1.2 -P 3306 -ppassword // didnt work either, same error.
I have these packages installed:
mariadb-client
mariadb-server
default-mysql-server
","<myself><maria>, error 2002 (hy000): can't connect myself server '192.168.1.15' (115), i'm try make myself database raspberri i 4, go well, use localhost instead work perfectly want remote control window 10 compute internet. great user address 192.168.1.15 this: so myself -u root great user 'lapse'@'192.168.1.15' identify 'password'; grant privilege *.* 'lapse'@'192.168.1.15'; flush privilege exit try login use this: myself -u last -h 192.168.1.15 -password // didn work, error: error 2002 (hy000): can't connect myself server '192.168.1.15' (115) myself -u user -h 192.168.1.2 -p 3306 -password // didn work either, error. package installed: maria-coli maria-serve default-myself-serve"
48261997,room migration using existing boolean column types,"What i found out so far 
All the @entity annotated classes are processed during compiletime and an Implementation for Database class is generated. Then before accessing the db, validateMigration method of this generated class is called. This validateMigration method verifies with the existing db schema via raw query
PRAGMA table_info mytable name 
(see L208 of android.arch.persistence.room.util.TableInfo.java)
Now the problem 
My sqlite3 db has some columns with column type as BOOLEAN. (which slqite internally handles to int). Now when i create room entities say 
public someEntity {
     @columnInfo(name=""someName"")
     public Boolean myValue;
}
The room's create table query will be
Create Table someEntity ( myValue INTEGER)
Where as when we query the existing db with PRAGMA table_info someEntity we get
1|myValue|BOOLEAN|0||0
As explained above room verifies the ( sqlite to room ) migration by comparing field name, column type etc. And since the column types dont match (BOOLEAN and INTEGER) it throws an error saying migration failed.
Can anyone suggest a workaround to this ? Can we make room create BOOLEAN column type in sqlite ? (Also afaik we can't change/alter column types of existing tables.)
PS: I also see a similar issue with VARCHAR - Using an existing VARCHAR column with Room
",<java><android><sqlite><android-room>,1306,1,8,491,1,4,12,43,13150,0.0,45,2,19,2018-01-15 11:23,2019-03-22 17:29,2019-03-22 17:29,431.0,431.0,Basic,9,"<java><android><sqlite><android-room>, room migration using existing boolean column types, What i found out so far 
All the @entity annotated classes are processed during compiletime and an Implementation for Database class is generated. Then before accessing the db, validateMigration method of this generated class is called. This validateMigration method verifies with the existing db schema via raw query
PRAGMA table_info mytable name 
(see L208 of android.arch.persistence.room.util.TableInfo.java)
Now the problem 
My sqlite3 db has some columns with column type as BOOLEAN. (which slqite internally handles to int). Now when i create room entities say 
public someEntity {
     @columnInfo(name=""someName"")
     public Boolean myValue;
}
The room's create table query will be
Create Table someEntity ( myValue INTEGER)
Where as when we query the existing db with PRAGMA table_info someEntity we get
1|myValue|BOOLEAN|0||0
As explained above room verifies the ( sqlite to room ) migration by comparing field name, column type etc. And since the column types dont match (BOOLEAN and INTEGER) it throws an error saying migration failed.
Can anyone suggest a workaround to this ? Can we make room create BOOLEAN column type in sqlite ? (Also afaik we can't change/alter column types of existing tables.)
PS: I also see a similar issue with VARCHAR - Using an existing VARCHAR column with Room
","<cava><andros><quite><andros-room>, room migrate use exist woolen column types, found far @entity cannot class process compiletim implement database class generate. access do, validatemigr method genet class called. validatemigr method verify exist do scheme via raw query trauma table_info metal name (see l208 andros.arch.persistence.room.until.tableinfo.cava) problem sqlite3 do column column type woolen. (which spite inter hand in). great room entity say public moment { @columninfo(name=""sometime"") public woolen value; } room' great table query great table moment ( myvalu inter) query exist do trauma table_info moment get 1|value|woolen|0||0 explain room verify ( quite room ) migrate compare field name, column type etc. since column type dont match (woolen inter) throw error say migrate failed. anyone suggest workaround ? make room great woolen column type quite ? (also again can't change/at column type exist tables.) is: also see similar issue varchar - use exist varchar column room"
62708607,How to fix diesel_cli link libpq.lib error with Postgres tools installed in Docker?,"I'm trying (for hours now) to install the cargo crate diesel_cli for postgres. However, every time I run the recommended cargo command:
cargo install diesel_cli --no-default-features --features postgres
I wait a few minutes just to see the same build fail with this message:
note: LINK : fatal error LNK1181: cannot open input file 'libpq.lib'
error: aborting due to previous error
error: failed to compile `diesel_cli v1.4.1`, intermediate artifacts can be found at `C:\Users\&lt;user name here&gt;\AppData\Local\Temp\cargo-installUU2DtT`
Caused by:
  could not compile `diesel_cli`.
I'm running postgres in a docker container and have the binaries on my C:\pgsql with the lib and bin directories both on the PATH so I can't figure out why it's not linking. What else could be required they didn't mention in the docs?
",<postgresql><rust><rust-cargo><libpq><rust-diesel>,820,1,13,3137,4,26,57,56,11733,0.0,342,8,19,2020-07-03 4:38,2020-08-18 8:36,2020-08-18 8:36,46.0,46.0,Basic,9,"<postgresql><rust><rust-cargo><libpq><rust-diesel>, How to fix diesel_cli link libpq.lib error with Postgres tools installed in Docker?, I'm trying (for hours now) to install the cargo crate diesel_cli for postgres. However, every time I run the recommended cargo command:
cargo install diesel_cli --no-default-features --features postgres
I wait a few minutes just to see the same build fail with this message:
note: LINK : fatal error LNK1181: cannot open input file 'libpq.lib'
error: aborting due to previous error
error: failed to compile `diesel_cli v1.4.1`, intermediate artifacts can be found at `C:\Users\&lt;user name here&gt;\AppData\Local\Temp\cargo-installUU2DtT`
Caused by:
  could not compile `diesel_cli`.
I'm running postgres in a docker container and have the binaries on my C:\pgsql with the lib and bin directories both on the PATH so I can't figure out why it's not linking. What else could be required they didn't mention in the docs?
","<postgresql><rust><rust-cargo><lips><rust-diese>, fix diesel_cli link lips.limb error poster tool instal doctor?, i'm try (for hour now) instal cargo crate diesel_cli postures. however, every time run recommend cargo command: cargo instal diesel_cli --no-default-feature --feature poster wait minute see build fail message: note: link : fatal error lnk1181: cannot open input file 'lips.limb' error: abort due previous error error: fail compel `diesel_cli ve.4.1`, intercede artifice found `c:\users\&it;us name here&it;\appdata\local\hemp\cargo-installuu2dtt` cause by: could compel `diesel_cli`. i'm run poster doctor contain binary c:\pgsql limb bin director path can't figure linking. else could require mention docs?"
55976471,How can I use AWS's Dynamo Db with Django?,"I am developing web applications, APIs, and backends using the Django MVC framework. A major aspect of Django is its implementation of an ORM for models. It is an exceptionally good ORM. Typically when using Django, one utilizes an existing interface that maps one's Django model to a specific DBMS like Postgres, MySQL, or Oracle for example.
I have some specific needs, requirements regarding performance and scalability, so I really want to use AWS's Dynamo DB because it is highly cost efficient, very performant, and scales really well.
While I think Django allows one to implement their own interface for a DBMS if one wishes to do so, it is clearly advantageous to be able to use an existing DBMS interface when constructing one's Django models if one exists.
Can someone recommend a Django model interface to use so I can construct a model in Django that uses AWS's Dynamo DB?
How about one using MongoDB?
",<python><django><orm><nosql><amazon-dynamodb>,914,0,0,773,3,7,15,65,26399,0.0,139,5,19,2019-05-03 20:08,2019-11-22 6:35,,203.0,,Intermediate,20,"<python><django><orm><nosql><amazon-dynamodb>, How can I use AWS's Dynamo Db with Django?, I am developing web applications, APIs, and backends using the Django MVC framework. A major aspect of Django is its implementation of an ORM for models. It is an exceptionally good ORM. Typically when using Django, one utilizes an existing interface that maps one's Django model to a specific DBMS like Postgres, MySQL, or Oracle for example.
I have some specific needs, requirements regarding performance and scalability, so I really want to use AWS's Dynamo DB because it is highly cost efficient, very performant, and scales really well.
While I think Django allows one to implement their own interface for a DBMS if one wishes to do so, it is clearly advantageous to be able to use an existing DBMS interface when constructing one's Django models if one exists.
Can someone recommend a Django model interface to use so I can construct a model in Django that uses AWS's Dynamo DB?
How about one using MongoDB?
","<patron><django><or><nose><amazon-dynamodb>, use was' dynamic do django?, develop web applications, axis, backed use django mac framework. major aspect django implement or models. except good or. topic use django, one until exist interface map one' django model specie dam like postures, myself, oral example. specie needs, require regard perform capability, really want use was' dynamic do highly cost efficient, performance, scale really well. think django allow one implement interface dam one wish so, clearly advantage all use exist dam interface construct one' django model one exists. someone recommend django model interface use construct model django use was' dynamic do? one use mongodb?"
56239790,Update SQL Query with populated variables from AJAX functions over multiple PHP Pages,"i try to get help with that question. 
All in all Q: It doesnt Update my DB entry like this Step by Step Order how i think it could be done.
its a bit difficult to explain, but i try to explain it step by step with minimal and readable Code. I use the original code, its hard to convert it in reproducible Examples.
A.1 Page ma_aktuelle_ReadOut.php
 There is a php Part
 &lt;?php echo ""&lt;a href='ma_Testende.php?TestergebnisID=&amp;TestaufstellungID="". $row['TestaufstellungID'].""&amp;TesterID="".$row['TesterID'].""' title='Test stoppen' data-toggle='tooltip' class='stoppen'&gt;   &lt;span class='glyphicon glyphicon-stop'&gt;&lt;/span&gt;&lt;/a&gt;"";
?&gt;
When i click this link the following javascript function is called and ask me ""really stop?""
&lt;script language=""JavaScript"" type=""text/javascript""&gt;
$(document).ready(function(){
  $(""a.stoppen"").click(function(e){
   if(!confirm('Wirklich stoppen?')){
    e.preventDefault();
    $('.alert').show()
    return false;
    }
    return true;
            });
        });
&lt;/script&gt;
&lt;style&gt;
 .alert {
  display: none;
    }
&lt;/style&gt;
When i cklick ""yes"" it opens the second Page.
A 2 Page ma_Testende.php
On this Page are 2 AJAX JS Functions.
The first Ajax is asking for ""Datum"" via type:get from the following next page and wait till success (see Page B 3):
&lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js""&gt;
/* function to get Stoptime for Cycle from DB.TesterCycleCount zu erhalten  */ 
$(document).ready(async function(){
var Datum;
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value from TesterID */ 
await $.ajax({ /* First Ajax function */
            url: 'ma_get-TesterID_Testende.php',
            type: 'get', 
            data: {TesterID:TesterID}, 
            dataType: 'json',
            success:function(response){ 
                var CID = response['CID'];
                Datum = response['Datum'];
                console.log(response);
            },
             error: function(jqxhtt, status, exception) {
                 console.log(exception);
         alert('Exception:', exception)
            }
        });
console.log();
        var TestaufstellungID = ""&lt;?php echo $_GET['TestaufstellungID']; ?&gt;"";
         $.ajax({ /* Second Ajax function */
            url: 'ma_TestendeSQL.php',
            type: 'get', 
            data: {TestaufstellungID:TestaufstellungID, Datum: Datum}, 
            dataType: 'json',
            success:function(data){ 
            alert('Successfully called');
     },
     error: function(jqxhr, status, exception) {
         console.log(exception);
         alert('Exception:', exception)
            }
        });
        });
&lt;/script&gt;
B 3 Page ma_get-TesterID_Testende.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
                $sqlreadZeit = ""Select TOP 1 CID,Datum from DB.dbo.TesterCycleCount where TesterID = '"".$_GET['TesterID'].""' order by Datum DESC"";
                $result1 = sqlsrv_query($conn, $sqlreadZeit);
                $zeiten_arr = array();
                while ($row = sqlsrv_fetch_array($result1, SQLSRV_FETCH_ASSOC)) {
                $CID = $row['CID'];
                $Datum = $row['Datum']-&gt;format('d.m.Y h:m:s');
                $zeiten_arr[] = array(""CID"" =&gt; $CID, ""Datum"" =&gt; $Datum);
                                }
    header('Content-type: application/json');
  echo json_encode($zeiten_arr); 
?&gt;
Back with the ""Datum"" the second AJAX is called (see Page A 2) 
With the ""Datum"" and ""TestaufstellungID"" as variable it should be call the next Page and Update the DB entry with the populated variablles.
B. 4 Page ma_TestendeSQL.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
$TestaufstellungID = $_GET['TestaufstellungID'];
$Testende= $_GET['Datum'];
$Testdatum = date('Y-d-m');
$stop = $connection-&gt;prepare(""WITH UpdateTestende AS (
  SELECT TOP 1  * from DB.dbo.Testergebnisse 
  WHERE TestaufstellungID = :TestaufstellungID
  ORDER BY TestergebnisID DESC 
)
update UpdateTestende 
set Testende = :Testende,
Datum = :Testdatum"");
$stop-&gt;execute(array(':TestaufstellungID' =&gt; $TestaufstellungID, ':Testdatum' =&gt; $Testdatum, ':Testende' =&gt; $Testende));
    header('Content-type: application/json');
?&gt;
The php variable $Testende get the populated ""Datum"" from the Ajax functions. All in all at the end it should be Update, when i click on the link the ( Page A 1) my DB entry with the populated ""Datum"" which i get from the first Ajax call ( Page A 2 ) from the SQL Query ( Page B 3) back to the second AJAX Call ( Page A 2 )  than with the data: {TestaufstellungID:TestaufstellungID, Datum: Datum}  to the last page ( Page B 4) 
But it doesnt Update my DB entry like this Step by Step Order how i think it could be done. 
Encapsulated is the SQL-Code working fine. With the Code header('Content-type: application/json'); the browser tell me the following when i click on the link from ( Page A 1 )
SyntaxError: JSON.parse: unexpected character at line 1 column 1 of the JSON data
Thats why i posted all the Step i think on one point the variables are not passed right to the next page or they are empty becasue the code is not executed in the right order Server/Client PHP/JS or Asynchronous problem... 
The console.log tell me nothing. At the moment i have no idea where to start with the debugging?
Hope someone can help me. thx
Edit: iam pretty sure the ajax call is empty, but i dont see it in which step the values getting empty
Edit2:
AJAX Call is empty or is not starting.
Further invstigation:  The Ajax alert me the error part with empty exception and dont alert me the success part. So it doesnt  go to the page ma_get-TesterID_Testende.php or it doesnt  return back the Datum .
Could be not enabled Cross-Site-Scripting be the Problem? 
But in another Page is a similiar Ajax Call working fine.
$(document).ready(function(){
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    $(""#TesterID"").change(function(){ /* Wenn du änderst und vom Select Feld auswählst */
        var TesterID = $(this).val(); /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    });
});
In this example the Ajax Call starts when i change the value from a Dropdown selection Form.    Is there a difference? 
How this Ajax should work i try to explain in my other question step by step, how it my application should be execute.
Update SQL Query with populated variables from AJAX functions over multiple PHP Pages
Edit 3:
JQuery Version:
https://code.jquery.com/jquery-3.4.1.js
",<javascript><php><jquery><sql-server><ajax>,8300,5,155,668,0,4,17,42,777,,18,1,19,2019-05-21 13:48,2019-05-28 23:31,,7.0,,Intermediate,31,"<javascript><php><jquery><sql-server><ajax>, Update SQL Query with populated variables from AJAX functions over multiple PHP Pages, i try to get help with that question. 
All in all Q: It doesnt Update my DB entry like this Step by Step Order how i think it could be done.
its a bit difficult to explain, but i try to explain it step by step with minimal and readable Code. I use the original code, its hard to convert it in reproducible Examples.
A.1 Page ma_aktuelle_ReadOut.php
 There is a php Part
 &lt;?php echo ""&lt;a href='ma_Testende.php?TestergebnisID=&amp;TestaufstellungID="". $row['TestaufstellungID'].""&amp;TesterID="".$row['TesterID'].""' title='Test stoppen' data-toggle='tooltip' class='stoppen'&gt;   &lt;span class='glyphicon glyphicon-stop'&gt;&lt;/span&gt;&lt;/a&gt;"";
?&gt;
When i click this link the following javascript function is called and ask me ""really stop?""
&lt;script language=""JavaScript"" type=""text/javascript""&gt;
$(document).ready(function(){
  $(""a.stoppen"").click(function(e){
   if(!confirm('Wirklich stoppen?')){
    e.preventDefault();
    $('.alert').show()
    return false;
    }
    return true;
            });
        });
&lt;/script&gt;
&lt;style&gt;
 .alert {
  display: none;
    }
&lt;/style&gt;
When i cklick ""yes"" it opens the second Page.
A 2 Page ma_Testende.php
On this Page are 2 AJAX JS Functions.
The first Ajax is asking for ""Datum"" via type:get from the following next page and wait till success (see Page B 3):
&lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js""&gt;
/* function to get Stoptime for Cycle from DB.TesterCycleCount zu erhalten  */ 
$(document).ready(async function(){
var Datum;
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value from TesterID */ 
await $.ajax({ /* First Ajax function */
            url: 'ma_get-TesterID_Testende.php',
            type: 'get', 
            data: {TesterID:TesterID}, 
            dataType: 'json',
            success:function(response){ 
                var CID = response['CID'];
                Datum = response['Datum'];
                console.log(response);
            },
             error: function(jqxhtt, status, exception) {
                 console.log(exception);
         alert('Exception:', exception)
            }
        });
console.log();
        var TestaufstellungID = ""&lt;?php echo $_GET['TestaufstellungID']; ?&gt;"";
         $.ajax({ /* Second Ajax function */
            url: 'ma_TestendeSQL.php',
            type: 'get', 
            data: {TestaufstellungID:TestaufstellungID, Datum: Datum}, 
            dataType: 'json',
            success:function(data){ 
            alert('Successfully called');
     },
     error: function(jqxhr, status, exception) {
         console.log(exception);
         alert('Exception:', exception)
            }
        });
        });
&lt;/script&gt;
B 3 Page ma_get-TesterID_Testende.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
                $sqlreadZeit = ""Select TOP 1 CID,Datum from DB.dbo.TesterCycleCount where TesterID = '"".$_GET['TesterID'].""' order by Datum DESC"";
                $result1 = sqlsrv_query($conn, $sqlreadZeit);
                $zeiten_arr = array();
                while ($row = sqlsrv_fetch_array($result1, SQLSRV_FETCH_ASSOC)) {
                $CID = $row['CID'];
                $Datum = $row['Datum']-&gt;format('d.m.Y h:m:s');
                $zeiten_arr[] = array(""CID"" =&gt; $CID, ""Datum"" =&gt; $Datum);
                                }
    header('Content-type: application/json');
  echo json_encode($zeiten_arr); 
?&gt;
Back with the ""Datum"" the second AJAX is called (see Page A 2) 
With the ""Datum"" and ""TestaufstellungID"" as variable it should be call the next Page and Update the DB entry with the populated variablles.
B. 4 Page ma_TestendeSQL.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
$TestaufstellungID = $_GET['TestaufstellungID'];
$Testende= $_GET['Datum'];
$Testdatum = date('Y-d-m');
$stop = $connection-&gt;prepare(""WITH UpdateTestende AS (
  SELECT TOP 1  * from DB.dbo.Testergebnisse 
  WHERE TestaufstellungID = :TestaufstellungID
  ORDER BY TestergebnisID DESC 
)
update UpdateTestende 
set Testende = :Testende,
Datum = :Testdatum"");
$stop-&gt;execute(array(':TestaufstellungID' =&gt; $TestaufstellungID, ':Testdatum' =&gt; $Testdatum, ':Testende' =&gt; $Testende));
    header('Content-type: application/json');
?&gt;
The php variable $Testende get the populated ""Datum"" from the Ajax functions. All in all at the end it should be Update, when i click on the link the ( Page A 1) my DB entry with the populated ""Datum"" which i get from the first Ajax call ( Page A 2 ) from the SQL Query ( Page B 3) back to the second AJAX Call ( Page A 2 )  than with the data: {TestaufstellungID:TestaufstellungID, Datum: Datum}  to the last page ( Page B 4) 
But it doesnt Update my DB entry like this Step by Step Order how i think it could be done. 
Encapsulated is the SQL-Code working fine. With the Code header('Content-type: application/json'); the browser tell me the following when i click on the link from ( Page A 1 )
SyntaxError: JSON.parse: unexpected character at line 1 column 1 of the JSON data
Thats why i posted all the Step i think on one point the variables are not passed right to the next page or they are empty becasue the code is not executed in the right order Server/Client PHP/JS or Asynchronous problem... 
The console.log tell me nothing. At the moment i have no idea where to start with the debugging?
Hope someone can help me. thx
Edit: iam pretty sure the ajax call is empty, but i dont see it in which step the values getting empty
Edit2:
AJAX Call is empty or is not starting.
Further invstigation:  The Ajax alert me the error part with empty exception and dont alert me the success part. So it doesnt  go to the page ma_get-TesterID_Testende.php or it doesnt  return back the Datum .
Could be not enabled Cross-Site-Scripting be the Problem? 
But in another Page is a similiar Ajax Call working fine.
$(document).ready(function(){
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    $(""#TesterID"").change(function(){ /* Wenn du änderst und vom Select Feld auswählst */
        var TesterID = $(this).val(); /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    });
});
In this example the Ajax Call starts when i change the value from a Dropdown selection Form.    Is there a difference? 
How this Ajax should work i try to explain in my other question step by step, how it my application should be execute.
Update SQL Query with populated variables from AJAX functions over multiple PHP Pages
Edit 3:
JQuery Version:
https://code.jquery.com/jquery-3.4.1.js
","<javascript><pp><query><sal-server><ajar>, update sal query soul variable ajar function multiple pp pages, try get help question. q: doesn update do entry like step step order think could done. bit difficult explain, try explain step step minims readable code. use origin code, hard convert reproduce examples. a.1 page ma_aktuelle_readout.pp pp part &it;?pp echo ""&it;a he='ma_testende.pp?testergebnisid=&amp;testaufstellungid="". $row['testaufstellungid'].""&amp;tested="".$row['tested'].""' title='test stopped' data-tongue='tooltip' class='stopped'&it; &it;span class='glyphicon glyphicon-stop'&it;&it;/span&it;&it;/a&it;""; ?&it; click link follow javascript function call ask ""really stop?"" &it;script language=""javascript"" type=""text/javascript""&it; $(document).ready(function(){ $(""a.stopped"").click(function(e){ if(!confirm('wirklich stopped?')){ e.preventdefault(); $('.alert').show() return false; } return true; }); }); &it;/script&it; &it;style&it; .alert { display: none; } &it;/style&it; click ""yes"" open second page. 2 page ma_testende.pp page 2 ajar is functions. first ajar ask ""date"" via type:get follow next page wait till success (see page b 3): &it;script sac=""http://ajar.googleapis.com/ajar/lips/query/1.12.4/query.min.is""&it; /* function get stoptim cycle do.testercyclecount zu erhalten */ $(document).ready(async function(){ war date; war tested = ""&it;?pp echo $get['tested']; ?&it;""; /* value tested */ await $.ajar({ /* first ajar function */ curl: 'market-testerid_testende.pp', type: 'get', data: {tested:tested}, datatype: 'son', success:function(response){ war did = response['did']; date = response['date']; console.log(response); }, error: function(jqxhtt, status, exception) { console.log(exception); alert('exception:', exception) } }); console.log(); war testaufstellungid = ""&it;?pp echo $get['testaufstellungid']; ?&it;""; $.ajar({ /* second ajar function */ curl: 'ma_testendesql.pp', type: 'get', data: {testaufstellungid:testaufstellungid, date: date}, datatype: 'son', success:function(data){ alert('success called'); }, error: function(jqxhr, status, exception) { console.log(exception); alert('exception:', exception) } }); }); &it;/script&it; b 3 page market-testerid_testende.pp &it;?pp $into = array( ""database"" =&it; $database, ""did"" =&it; $surname, ""pad"" =&it; $password ); $corn = sqlsrv_connect($server, $into); $sqlreadzeit = ""select top 1 did,date do.do.testercyclecount tested = '"".$get['tested'].""' order date desk""; $result = sqlsrv_query($corn, $sqlreadzeit); $zeiten_arr = array(); ($row = sqlsrv_fetch_array($result, sqlsrv_fetch_assoc)) { $did = $row['did']; $date = $row['date']-&it;format('d.m.i h:m:s'); $zeiten_arr[] = array(""did"" =&it; $did, ""date"" =&it; $date); } header('content-type: application/son'); echo json_encode($zeiten_arr); ?&it; back ""date"" second ajar call (see page 2) ""date"" ""testaufstellungid"" variable call next page update do entry soul variable. b. 4 page ma_testendesql.pp &it;?pp $into = array( ""database"" =&it; $database, ""did"" =&it; $surname, ""pad"" =&it; $password ); $corn = sqlsrv_connect($server, $into); $testaufstellungid = $get['testaufstellungid']; $tested= $get['date']; $testdatum = date('y-d-m'); $stop = $connection-&it;prepare(""with updatetestend ( select top 1 * do.do.testergebniss testaufstellungid = :testaufstellungid order testergebnisid desk ) update updatetestend set tested = :tested, date = :testdatum""); $stop-&it;execute(array(':testaufstellungid' =&it; $testaufstellungid, ':testdatum' =&it; $testdatum, ':tested' =&it; $tested)); header('content-type: application/son'); ?&it; pp variable $tested get soul ""date"" ajar functions. end update, click link ( page 1) do entry soul ""date"" get first ajar call ( page 2 ) sal query ( page b 3) back second ajar call ( page 2 ) data: {testaufstellungid:testaufstellungid, date: date} last page ( page b 4) doesn update do entry like step step order think could done. encapsuled sal-code work fine. code header('content-type: application/son'); brother tell follow click link ( page 1 ) syntaxerror: son.pause: expect character line 1 column 1 son data that post step think one point variable pass right next page empty because code execute right order server/coli pp/j asynchron problem... console.log tell nothing. moment idea start debugging? hope someone help me. the edit: am pretty sure ajar call empty, dont see step value get empty edit: ajar call empty starting. investigation: ajar alert error part empty except dont alert success part. doesn go page market-testerid_testende.pp doesn return back date . could enable cross-site-script problem? not page similar ajar call work fine. $(document).ready(function(){ war tested = ""&it;?pp echo $get['tested']; ?&it;""; /* value der tested erhalten */ $.ajar({ /* ajar aufrufen */ curl: 'market-tested.pp', type: 'get', /* method zum übertragen der date */ data: {tested:tested}, /* date zu übermitteln */ datatype: 'son', success:function(response){ /* die zurückgegebenen date erhalten */ war len = response.length; $(""#teststart"").empty(); /* die erhaltenden date werden be der id angezeigt */ for( war = 0; i&it;len; i++){ war did = response[i]['did']; war date = response[i]['date']; $(""#teststart"").happened(""&it;opt value='""+date+""'&it;""+date+""&it;/option&it;""); } } }); $(""#tested"").change(function(){ /* went du änderst und vol select felt auswählst */ war tested = $(this).val(); /* value der tested erhalten */ $.ajar({ /* ajar aufrufen */ curl: 'market-tested.pp', type: 'get', /* method zum übertragen der date */ data: {tested:tested}, /* date zu übermitteln */ datatype: 'son', success:function(response){ /* die zurückgegebenen date erhalten */ war len = response.length; $(""#teststart"").empty(); /* die erhaltenden date werden be der id angezeigt */ for( war = 0; i&it;len; i++){ war did = response[i]['did']; war date = response[i]['date']; $(""#teststart"").happened(""&it;opt value='""+date+""'&it;""+date+""&it;/option&it;""); } } }); }); }); example ajar call start change value dropdown select form. difference? ajar work try explain question step step, applied execute. update sal query soul variable ajar function multiple pp page edit 3: query version: http://code.query.com/query-3.4.1.j"
63684133,prisma can't connect to postgresql,"I've tried to connect Prisma with postgreSQL several times.
prisma show this error message : &quot;Error: undefined: invalid port number in &quot;postgresql://postgres:password@localhost:5432/linker&quot;)&quot;.
-error
-prisma/.env
DATABASE_URL=postgresql://postgres:password@localhost:5432/linker
-schema.prisma
datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}
So, first, I checked the port number to see if it was right and 5432 is right because I use the default port number. I also checked the postgresql.conf file, which is set to &quot;listen_address=&quot;*&quot;&quot; , &quot;port=5432&quot;.
And I went into pgAdmin4 and saw server's properties. the port number was 5432 as shown below image, and the username was set &quot;postgres&quot;.
I don't know why prisma can't connect
Did i something missed?
",<postgresql><prisma>,861,3,5,345,1,3,10,66,15046,0.0,28,5,19,2020-09-01 8:36,2021-06-02 8:09,2021-07-01 16:23,274.0,303.0,Basic,6,"<postgresql><prisma>, prisma can't connect to postgresql, I've tried to connect Prisma with postgreSQL several times.
prisma show this error message : &quot;Error: undefined: invalid port number in &quot;postgresql://postgres:password@localhost:5432/linker&quot;)&quot;.
-error
-prisma/.env
DATABASE_URL=postgresql://postgres:password@localhost:5432/linker
-schema.prisma
datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}
So, first, I checked the port number to see if it was right and 5432 is right because I use the default port number. I also checked the postgresql.conf file, which is set to &quot;listen_address=&quot;*&quot;&quot; , &quot;port=5432&quot;.
And I went into pgAdmin4 and saw server's properties. the port number was 5432 as shown below image, and the username was set &quot;postgres&quot;.
I don't know why prisma can't connect
Did i something missed?
","<postgresql><prima>, prima can't connect postgresql, i'v try connect prima postgresql never times. prima show error message : &quit;error: undefined: invalid port number &quit;postgresql://postures:password@localhost:5432/linked&quit;)&quit;. -error -prima/.end database_url=postgresql://postures:password@localhost:5432/link -scheme.prima datasourc do { proved = &quit;postgresql&quit; curl = end(&quit;database_url&quit;) } so, first, check port number see right 5432 right use default port number. also check postgresql.cone file, set &quit;listen_address=&quit;*&quit;&quit; , &quit;port=5432&quit;. went pgadmin4 saw server' properties. port number 5432 shown image, usernam set &quit;postures&quit;. know prima can't connect cometh missed?"
65409104,What does Room return if a query had no results,"Having a difficult time finding this answer and the documentation doesn't seem to answer the question.
If I have a basic ROOM query,
@Query(&quot;SELECT * FROM geotable WHERE geohash = :geohash&quot;)
abstract suspend fun getGeoTable(geohash: String) : GeoTable
and there is no such item that uses this primary key, what happens?  Android studio says that the DAO object will never return a null.  It seems that EmptyResultSetException only gets thrown when you have Single using RxJava which I am not using.  So what does plain old ROOM throw when it finds nothing?
",<android><sqlite><android-room>,567,0,2,1073,2,12,21,73,5379,0.0,16,4,19,2020-12-22 12:53,2021-04-21 9:24,,120.0,,Basic,13,"<android><sqlite><android-room>, What does Room return if a query had no results, Having a difficult time finding this answer and the documentation doesn't seem to answer the question.
If I have a basic ROOM query,
@Query(&quot;SELECT * FROM geotable WHERE geohash = :geohash&quot;)
abstract suspend fun getGeoTable(geohash: String) : GeoTable
and there is no such item that uses this primary key, what happens?  Android studio says that the DAO object will never return a null.  It seems that EmptyResultSetException only gets thrown when you have Single using RxJava which I am not using.  So what does plain old ROOM throw when it finds nothing?
","<andros><quite><andros-room>, room return query results, difficult time find answer document seem answer question. basic room query, @query(&quit;select * geotabl geohash = :geohash&quit;) abstract suspend fun getgeotable(geohash: string) : geotabl item use primary key, happens? andros studio say do object never return null. seem emptyresultsetexcept get thrown single use rxjava using. plain old room throw find nothing?"
64474420,PostgreSQL authentication method 10 not supported,"I'm trying to follow the diesel.rs tutorial using PostgreSQL. When I get to the Diesel setup step, I get an &quot;authentication method 10 not supported&quot; error. How do I resolve it?
",<postgresql><rust><rust-diesel>,187,1,0,393,1,4,11,41,100878,0.0,0,3,19,2020-10-22 2:34,2020-10-22 6:04,,0.0,,Basic,14,"<postgresql><rust><rust-diesel>, PostgreSQL authentication method 10 not supported, I'm trying to follow the diesel.rs tutorial using PostgreSQL. When I get to the Diesel setup step, I get an &quot;authentication method 10 not supported&quot; error. How do I resolve it?
","<postgresql><rust><rust-diese>, postgresql authentic method 10 supported, i'm try follow diese.r tutor use postgresql. get diese set step, get &quit;authentic method 10 supported&quit; error. resolve it?"
58033899,Why is Spark broadcast exchange data size bigger than raw size on join?,"I am doing a broadcast join of two tables A and B.
B is a cached table created with the following Spark SQL:
create table B as select segment_ids_hash from  stb_ranker.c3po_segments
      where
        from_unixtime(unix_timestamp(string(dayid), 'yyyyMMdd')) &gt;= CAST('2019-07-31 00:00:00.000000000' AS TIMESTAMP)
      and
        segmentid_check('(6|8|10|12|14|371|372|373|374|375|376|582|583|585|586|587|589|591|592|594|596|597|599|601|602|604|606|607|609|610|611|613|615|616)', seg_ids) = true
cache table B
The column 'segment_ids_hash' is of integer type and the result contains 36.4 million records.
The cached table size is about 140 MB, as shown below
Then I did the join as follows:
select count(*) from A broadcast join B on A.segment_ids_hash = B.segment_ids_hash
Here broadcast exchange data size is about 3.2 GB.
My question is why the broadcast exchange data size (3.2GB) is so much bigger than the raw data size (~140 MB). What are the overheads? Is there any way to reduce the broadcast exchange data size?
Thanks
",<apache-spark><apache-spark-sql>,1033,2,7,1497,3,17,26,37,5192,0.0,39,2,19,2019-09-20 19:02,2020-02-24 11:20,2020-02-24 11:20,157.0,157.0,Intermediate,23,"<apache-spark><apache-spark-sql>, Why is Spark broadcast exchange data size bigger than raw size on join?, I am doing a broadcast join of two tables A and B.
B is a cached table created with the following Spark SQL:
create table B as select segment_ids_hash from  stb_ranker.c3po_segments
      where
        from_unixtime(unix_timestamp(string(dayid), 'yyyyMMdd')) &gt;= CAST('2019-07-31 00:00:00.000000000' AS TIMESTAMP)
      and
        segmentid_check('(6|8|10|12|14|371|372|373|374|375|376|582|583|585|586|587|589|591|592|594|596|597|599|601|602|604|606|607|609|610|611|613|615|616)', seg_ids) = true
cache table B
The column 'segment_ids_hash' is of integer type and the result contains 36.4 million records.
The cached table size is about 140 MB, as shown below
Then I did the join as follows:
select count(*) from A broadcast join B on A.segment_ids_hash = B.segment_ids_hash
Here broadcast exchange data size is about 3.2 GB.
My question is why the broadcast exchange data size (3.2GB) is so much bigger than the raw data size (~140 MB). What are the overheads? Is there any way to reduce the broadcast exchange data size?
Thanks
","<apache-spark><apache-spark-sal>, spark broadcast exchange data size bigger raw size join?, broadcast join two table b. b each table great follow spark sal: great table b select segment_ids_hash stb_ranker.c3po_seg from_unixtime(unix_timestamp(string(david), 'yyyymmdd')) &it;= cast('2019-07-31 00:00:00.000000000' timestamp) segmentid_check('(6|8|10|12|14|371|372|373|374|375|376|582|583|585|586|587|589|591|592|594|596|597|599|601|602|604|606|607|609|610|611|613|615|616)', seg_ids) = true each table b column 'segment_ids_hash' inter type result contain 36.4 million records. each table size 140 mb, shown join follows: select count(*) broadcast join b a.segment_ids_hash = b.segment_ids_hash broadcast exchange data size 3.2 go. question broadcast exchange data size (3.go) much bigger raw data size (~140 mb). overhead? way reduce broadcast exchange data size? thank"
60457719,Query without WHILE Loop,"We have appointment table as shown below. Each appointment need to be categorized as ""New"" or ""Followup"". Any appointment (for a patient) within 30 days of first appointment (of that patient) is Followup.  After 30 days, appointment is again ""New"". Any appointment within 30 days become ""Followup"".
I am currently doing this by typing while loop.
How to achieve this without WHILE loop?
Table
CREATE TABLE #Appt1 (ApptID INT, PatientID INT, ApptDate DATE)
INSERT INTO #Appt1
SELECT  1,101,'2020-01-05' UNION
SELECT  2,505,'2020-01-06' UNION
SELECT  3,505,'2020-01-10' UNION
SELECT  4,505,'2020-01-20' UNION
SELECT  5,101,'2020-01-25' UNION
SELECT  6,101,'2020-02-12'  UNION
SELECT  7,101,'2020-02-20'  UNION
SELECT  8,101,'2020-03-30'  UNION
SELECT  9,303,'2020-01-28' UNION
SELECT  10,303,'2020-02-02' 
",<sql><sql-server><t-sql><sql-server-2016>,804,1,12,22285,67,261,420,56,1312,0.0,5076,10,19,2020-02-28 18:58,2020-02-28 19:13,2020-03-02 16:58,0.0,3.0,Basic,10,"<sql><sql-server><t-sql><sql-server-2016>, Query without WHILE Loop, We have appointment table as shown below. Each appointment need to be categorized as ""New"" or ""Followup"". Any appointment (for a patient) within 30 days of first appointment (of that patient) is Followup.  After 30 days, appointment is again ""New"". Any appointment within 30 days become ""Followup"".
I am currently doing this by typing while loop.
How to achieve this without WHILE loop?
Table
CREATE TABLE #Appt1 (ApptID INT, PatientID INT, ApptDate DATE)
INSERT INTO #Appt1
SELECT  1,101,'2020-01-05' UNION
SELECT  2,505,'2020-01-06' UNION
SELECT  3,505,'2020-01-10' UNION
SELECT  4,505,'2020-01-20' UNION
SELECT  5,101,'2020-01-25' UNION
SELECT  6,101,'2020-02-12'  UNION
SELECT  7,101,'2020-02-20'  UNION
SELECT  8,101,'2020-03-30'  UNION
SELECT  9,303,'2020-01-28' UNION
SELECT  10,303,'2020-02-02' 
","<sal><sal-server><t-sal><sal-server-2016>, query without loop, appoint table shown below. appoint need category ""new"" ""followed"". appoint (for patient) within 30 day first appoint (of patient) followed. 30 days, appoint ""new"". appoint within 30 day become ""followed"". current type loop. achieve without loop? table great table #apply (applied in, patient in, apptdat date) insert #apply select 1,101,'2020-01-05' union select 2,505,'2020-01-06' union select 3,505,'2020-01-10' union select 4,505,'2020-01-20' union select 5,101,'2020-01-25' union select 6,101,'2020-02-12' union select 7,101,'2020-02-20' union select 8,101,'2020-03-30' union select 9,303,'2020-01-28' union select 10,303,'2020-02-02'"
64414030,How to use nested pydantic models for sqlalchemy in a flexible way,"from fastapi import Depends, FastAPI, HTTPException, Body, Request
from sqlalchemy import create_engine, Boolean, Column, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Session, sessionmaker, relationship
from sqlalchemy.inspection import inspect
from typing import List, Optional
from pydantic import BaseModel
import json
SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
app = FastAPI()
# sqlalchemy models
class RootModel(Base):
    __tablename__ = &quot;root_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someRootText = Column(String)
    subData = relationship(&quot;SubModel&quot;, back_populates=&quot;rootData&quot;)
class SubModel(Base):
    __tablename__ = &quot;sub_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someSubText = Column(String)
    root_id = Column(Integer, ForeignKey(&quot;root_table.id&quot;))
    rootData = relationship(&quot;RootModel&quot;, back_populates=&quot;subData&quot;)
# pydantic models/schemas
class SchemaSubBase(BaseModel):
    someSubText: str
    class Config:
        orm_mode = True
class SchemaSub(SchemaSubBase):
    id: int
    root_id: int
    class Config:
        orm_mode = True
class SchemaRootBase(BaseModel):
    someRootText: str
    subData: List[SchemaSubBase] = []
    class Config:
        orm_mode = True
class SchemaRoot(SchemaRootBase):
    id: int
    class Config:
        orm_mode = True
class SchemaSimpleBase(BaseModel):
    someRootText: str
    class Config:
        orm_mode = True
class SchemaSimple(SchemaSimpleBase):
    id: int
    class Config:
        orm_mode = True
Base.metadata.create_all(bind=engine)
# database functions (CRUD)
def db_add_simple_data_pydantic(db: Session, root: SchemaRootBase):
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic_generic(db: Session, root: SchemaRootBase):
    # this fails:
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic(db: Session, root: SchemaRootBase):
    # start: hack: i have to manually generate the sqlalchemy model from the pydantic model
    root_dict = root.dict()
    sub_dicts = []
    # i have to remove the list form root dict in order to fix the error from above
    for key in list(root_dict):
        if isinstance(root_dict[key], list):
            sub_dicts = root_dict[key]
            del root_dict[key]
    # now i can do it
    db_root = RootModel(**root_dict)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    # end: hack
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_nopydantic(db: Session, root):
    print(root)
    sub_dicts = root.pop(&quot;subData&quot;)
    print(sub_dicts)
    db_root = RootModel(**root)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    # problem
    &quot;&quot;&quot;
    if I would now &quot;return db_root&quot;, the answer would be of this:
    {
        &quot;someRootText&quot;: &quot;string&quot;,
        &quot;id&quot;: 24
    }
    and not containing &quot;subData&quot;
    therefore I have to do the following.
    Why?
    &quot;&quot;&quot;
    from sqlalchemy.orm import joinedload
    db_root = (
        db.query(RootModel)
            .options(joinedload(RootModel.subData))
            .filter(RootModel.id == db_root.id)
            .all()
    )[0]
    return db_root
# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
@app.post(&quot;/addNestedModel_pydantic_generic&quot;, response_model=SchemaRootBase)
def addSipleModel_pydantic_generic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addSimpleModel_pydantic&quot;, response_model=SchemaSimpleBase)
def add_simple_data_pydantic(root: SchemaSimpleBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_nopydantic&quot;)
def add_nested_data_nopydantic(root=Body(...), db: Session = Depends(get_db)):
    data = db_add_nested_data_nopydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_pydantic&quot;, response_model=SchemaRootBase)
def add_nested_data_pydantic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_nested_data_pydantic(db=db, root=root)
    return data
Description
My Question is:
How to make nested sqlalchemy models from nested pydantic models (or python dicts)  in a generic way and write them to the database in &quot;one shot&quot;.
My example model is called RootModel and has a list of submodels called &quot;sub models&quot; in subData key.
Please see above for pydantic and sqlalchemy definitions.
Example:
The user provides a nested json string:
{
  &quot;someRootText&quot;: &quot;string&quot;,
  &quot;subData&quot;: [
    {
      &quot;someSubText&quot;: &quot;string&quot;
    }
  ]
}
Open the browser and call the endpoint /docs.
You can play around with all endpoints and POST the json string from above.
/addNestedModel_pydantic_generic
When you call the endpoint /addNestedModel_pydantic_generic it will fail, because sqlalchemy cannot create the nested model from pydantic nested model directly:
AttributeError: 'dict' object has no attribute '_sa_instance_state' 
​/addSimpleModel_pydantic
With a non-nested model  it works.
The remaining endpoints are showing &quot;hacks&quot; to solve the problem of nested models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with pydantic models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with python dicts.
My solutions are only hacks, I want a generic way to create nested sqlalchemy models either from pydantic (preferred) or from a python dict.
Environment
OS: Windows,
FastAPI Version : 0.61.1
Python version: Python 3.8.5
sqlalchemy: 1.3.19
pydantic : 1.6.1
",<sqlalchemy><fastapi><pydantic>,6555,0,207,191,1,1,4,81,18993,0.0,1,4,19,2020-10-18 13:41,2021-04-30 11:29,,194.0,,Intermediate,17,"<sqlalchemy><fastapi><pydantic>, How to use nested pydantic models for sqlalchemy in a flexible way, from fastapi import Depends, FastAPI, HTTPException, Body, Request
from sqlalchemy import create_engine, Boolean, Column, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Session, sessionmaker, relationship
from sqlalchemy.inspection import inspect
from typing import List, Optional
from pydantic import BaseModel
import json
SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
app = FastAPI()
# sqlalchemy models
class RootModel(Base):
    __tablename__ = &quot;root_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someRootText = Column(String)
    subData = relationship(&quot;SubModel&quot;, back_populates=&quot;rootData&quot;)
class SubModel(Base):
    __tablename__ = &quot;sub_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someSubText = Column(String)
    root_id = Column(Integer, ForeignKey(&quot;root_table.id&quot;))
    rootData = relationship(&quot;RootModel&quot;, back_populates=&quot;subData&quot;)
# pydantic models/schemas
class SchemaSubBase(BaseModel):
    someSubText: str
    class Config:
        orm_mode = True
class SchemaSub(SchemaSubBase):
    id: int
    root_id: int
    class Config:
        orm_mode = True
class SchemaRootBase(BaseModel):
    someRootText: str
    subData: List[SchemaSubBase] = []
    class Config:
        orm_mode = True
class SchemaRoot(SchemaRootBase):
    id: int
    class Config:
        orm_mode = True
class SchemaSimpleBase(BaseModel):
    someRootText: str
    class Config:
        orm_mode = True
class SchemaSimple(SchemaSimpleBase):
    id: int
    class Config:
        orm_mode = True
Base.metadata.create_all(bind=engine)
# database functions (CRUD)
def db_add_simple_data_pydantic(db: Session, root: SchemaRootBase):
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic_generic(db: Session, root: SchemaRootBase):
    # this fails:
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic(db: Session, root: SchemaRootBase):
    # start: hack: i have to manually generate the sqlalchemy model from the pydantic model
    root_dict = root.dict()
    sub_dicts = []
    # i have to remove the list form root dict in order to fix the error from above
    for key in list(root_dict):
        if isinstance(root_dict[key], list):
            sub_dicts = root_dict[key]
            del root_dict[key]
    # now i can do it
    db_root = RootModel(**root_dict)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    # end: hack
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_nopydantic(db: Session, root):
    print(root)
    sub_dicts = root.pop(&quot;subData&quot;)
    print(sub_dicts)
    db_root = RootModel(**root)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    # problem
    &quot;&quot;&quot;
    if I would now &quot;return db_root&quot;, the answer would be of this:
    {
        &quot;someRootText&quot;: &quot;string&quot;,
        &quot;id&quot;: 24
    }
    and not containing &quot;subData&quot;
    therefore I have to do the following.
    Why?
    &quot;&quot;&quot;
    from sqlalchemy.orm import joinedload
    db_root = (
        db.query(RootModel)
            .options(joinedload(RootModel.subData))
            .filter(RootModel.id == db_root.id)
            .all()
    )[0]
    return db_root
# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
@app.post(&quot;/addNestedModel_pydantic_generic&quot;, response_model=SchemaRootBase)
def addSipleModel_pydantic_generic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addSimpleModel_pydantic&quot;, response_model=SchemaSimpleBase)
def add_simple_data_pydantic(root: SchemaSimpleBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_nopydantic&quot;)
def add_nested_data_nopydantic(root=Body(...), db: Session = Depends(get_db)):
    data = db_add_nested_data_nopydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_pydantic&quot;, response_model=SchemaRootBase)
def add_nested_data_pydantic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_nested_data_pydantic(db=db, root=root)
    return data
Description
My Question is:
How to make nested sqlalchemy models from nested pydantic models (or python dicts)  in a generic way and write them to the database in &quot;one shot&quot;.
My example model is called RootModel and has a list of submodels called &quot;sub models&quot; in subData key.
Please see above for pydantic and sqlalchemy definitions.
Example:
The user provides a nested json string:
{
  &quot;someRootText&quot;: &quot;string&quot;,
  &quot;subData&quot;: [
    {
      &quot;someSubText&quot;: &quot;string&quot;
    }
  ]
}
Open the browser and call the endpoint /docs.
You can play around with all endpoints and POST the json string from above.
/addNestedModel_pydantic_generic
When you call the endpoint /addNestedModel_pydantic_generic it will fail, because sqlalchemy cannot create the nested model from pydantic nested model directly:
AttributeError: 'dict' object has no attribute '_sa_instance_state' 
​/addSimpleModel_pydantic
With a non-nested model  it works.
The remaining endpoints are showing &quot;hacks&quot; to solve the problem of nested models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with pydantic models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with python dicts.
My solutions are only hacks, I want a generic way to create nested sqlalchemy models either from pydantic (preferred) or from a python dict.
Environment
OS: Windows,
FastAPI Version : 0.61.1
Python version: Python 3.8.5
sqlalchemy: 1.3.19
pydantic : 1.6.1
","<sqlalchemy><fastapi><pedantic>, use nest plant model sqlalchemi flexible way, fastapi import depends, fastapi, httpexception, body, request sqlalchemi import create_engine, woolen, column, foreigner, inter, string sqlalchemy.ext.declare import declarative_bas sqlalchemy.or import session, sessionmaker, relationship sqlalchemy.inspect import inspect type import list, option plant import basemodel import son sqlalchemy_database_url = &quit;quite:///./test.do&quit; engine = create_engine( sqlalchemy_database_url, connect_args={&quit;check_same_thread&quit;: false} ) sessionloc = sessionmaker(autocommit=false, autoflush=false, bind=engine) base = declarative_base() pp = fastapi() # sqlalchemi model class rootmodel(base): __tablename__ = &quit;root_table&quit; id = column(inter, primary_key=true, index=true) someroottext = column(string) subdata = relationship(&quit;submodel&quit;, back_populates=&quit;rootdata&quit;) class submodel(base): __tablename__ = &quit;suitable&quit; id = column(inter, primary_key=true, index=true) somesubtext = column(string) rooted = column(inter, foreigner(&quit;root_table.id&quit;)) rootdata = relationship(&quit;rootmodel&quit;, back_populates=&quit;subdata&quit;) # plant models/scheme class schemasubbase(basemodel): somesubtext: sir class confirm: orm_mod = true class schemasub(schemasubbase): id: in rooted: in class confirm: orm_mod = true class schemarootbase(basemodel): someroottext: sir subdata: list[schemasubbase] = [] class confirm: orm_mod = true class schemaroot(schemarootbase): id: in class confirm: orm_mod = true class schemasimplebase(basemodel): someroottext: sir class confirm: orm_mod = true class schemasimple(schemasimplebase): id: in class confirm: orm_mod = true base.metadata.create_all(bind=engine) # database function (crude) def db_add_simple_data_pydantic(do: session, root: schemarootbase): db_root = rootmodel(**root.duct()) do.add(db_root) do.commit() do.refresh(db_root) return db_root def db_add_nested_data_pydantic_generic(do: session, root: schemarootbase): # fails: db_root = rootmodel(**root.duct()) do.add(db_root) do.commit() do.refresh(db_root) return db_root def db_add_nested_data_pydantic(do: session, root: schemarootbase): # start: hack: manual genet sqlalchemi model plant model root_dict = root.duct() sub_dict = [] # remove list form root duct order fix error key list(root_dict): instance(root_dict[key], list): sub_dict = root_dict[key] del root_dict[key] # db_root = rootmodel(**root_dict) sub_dict sub_dicts: db_root.subdata.happened(submodel(**sub_dict)) # end: hack do.add(db_root) do.commit() do.refresh(db_root) return db_root def db_add_nested_data_nopydantic(do: session, root): print(root) sub_dict = root.pop(&quit;subdata&quit;) print(sub_dicts) db_root = rootmodel(**root) sub_dict sub_dicts: db_root.subdata.happened(submodel(**sub_dict)) do.add(db_root) do.commit() do.refresh(db_root) # problem &quit;&quit;&quit; would &quit;return db_root&quit;, answer would this: { &quit;someroottext&quit;: &quit;string&quit;, &quit;id&quit;: 24 } contain &quit;subdata&quit; therefore following. why? &quit;&quit;&quit; sqlalchemy.or import joinedload db_root = ( do.query(rootmodel) .option(joinedload(rootmodel.subdata)) .filter(rootmodel.id == db_root.id) .all() )[0] return db_root # depend def get_db(): do = sessionlocal() try: yield do finally: do.close() @pp.post(&quit;/addnestedmodel_pydantic_generic&quit;, response_model=schemarootbase) def addsiplemodel_pydantic_generic(root: schemarootbase, do: session = depends(get_db)): data = db_add_simple_data_pydantic(do=do, root=root) return data @pp.post(&quit;/addsimplemodel_pydantic&quit;, response_model=schemasimplebase) def add_simple_data_pydantic(root: schemasimplebase, do: session = depends(get_db)): data = db_add_simple_data_pydantic(do=do, root=root) return data @pp.post(&quit;/addnestedmodel_nopydantic&quit;) def add_nested_data_nopydantic(root=body(...), do: session = depends(get_db)): data = db_add_nested_data_nopydantic(do=do, root=root) return data @pp.post(&quit;/addnestedmodel_pydantic&quit;, response_model=schemarootbase) def add_nested_data_pydantic(root: schemarootbase, do: session = depends(get_db)): data = db_add_nested_data_pydantic(do=do, root=root) return data rescript question is: make nest sqlalchemi model nest plant model (or patron ducts) genet way write database &quit;on shot&quit;. example model call rootmodel list submodel call &quit;sub models&quit; subdata key. pleas see plant sqlalchemi definitions. example: user proved nest son string: { &quit;someroottext&quit;: &quit;string&quit;, &quit;subdata&quit;: [ { &quit;somesubtext&quit;: &quit;string&quit; } ] } open brother call endpoint /docs. play around endpoint post son string above. /addnestedmodel_pydantic_gener call endpoint /addnestedmodel_pydantic_gener fail, sqlalchemi cannot great nest model plant nest model directly: attributeerror: 'duct' object attribute '_sa_instance_state' ​/addsimplemodel_pydant non-nest model works. remain endpoint show &quit;backs&quit; sole problem nest models. /addnestedmodel_pydant endpoint genet root model and submodel loop non-genet way plant models. /addnestedmodel_pydant endpoint genet root model and submodel loop non-genet way patron ducts. slut backs, want genet way great nest sqlalchemi model either plant (preferred) patron duct. environs os: windows, fastapi version : 0.61.1 patron version: patron 3.8.5 sqlalchemy: 1.3.19 plant : 1.6.1"
52177610,How to create auto incrementing / SERIAL id columns using DBeaver with PostgreSQL?,"I am a new user for both PostgreSQL and DBeaver (Community edition ver. 5.1.6) and was looking for a way to create an auto incrementing ID column in a table through the DBeaver GUI.
From my research I can see that:
You can set this up easily using SQL eg. id SERIAL NOT_NULL
The underlying problem is that there is no such thing as a 'Serial data type', and that SERIAL equates to nextval('table_name_id_seq').
When I create a table using the SERIAL command in SQL the resulting id column has a nextval('exampletable_id_seq'::regclass') value in the 'Default' attribute.
I have attempted to manually input the nextval() command within the 'Default' attribute for the column in DBeaver in a new table, for example. nextval('mytable_id_seq') with and without the '::regclass;. However this is not working.
I appreciate that doing this in SQL would be easier, and that there is a previously asked question at: Problems de Serial data type in DBeaver &amp; PostgreSQL.
However, I could not find a satisfactory answer and the option of being able to do this through the GUI would be useful, especially if other setup is being done through the DBeaver GUI.
Specifically, my question is:
Is there a functionality for DBeaver to add auto incrementing id's through the GUI?
If so, what would be the steps to do this.
",<postgresql><dbeaver>,1308,1,0,703,2,7,13,77,65316,0.0,58,1,19,2018-09-05 5:24,2018-09-05 22:15,2018-09-05 22:15,0.0,0.0,Basic,3,"<postgresql><dbeaver>, How to create auto incrementing / SERIAL id columns using DBeaver with PostgreSQL?, I am a new user for both PostgreSQL and DBeaver (Community edition ver. 5.1.6) and was looking for a way to create an auto incrementing ID column in a table through the DBeaver GUI.
From my research I can see that:
You can set this up easily using SQL eg. id SERIAL NOT_NULL
The underlying problem is that there is no such thing as a 'Serial data type', and that SERIAL equates to nextval('table_name_id_seq').
When I create a table using the SERIAL command in SQL the resulting id column has a nextval('exampletable_id_seq'::regclass') value in the 'Default' attribute.
I have attempted to manually input the nextval() command within the 'Default' attribute for the column in DBeaver in a new table, for example. nextval('mytable_id_seq') with and without the '::regclass;. However this is not working.
I appreciate that doing this in SQL would be easier, and that there is a previously asked question at: Problems de Serial data type in DBeaver &amp; PostgreSQL.
However, I could not find a satisfactory answer and the option of being able to do this through the GUI would be useful, especially if other setup is being done through the DBeaver GUI.
Specifically, my question is:
Is there a functionality for DBeaver to add auto incrementing id's through the GUI?
If so, what would be the steps to do this.
","<postgresql><beaver>, great auto incitement / aerial id column use beaver postgresql?, new user postgresql beaver (common edit ver. 5.1.6) look way great auto incitement id column table beaver gun. research see that: set vasili use sal eg. id aerial not_nul underlip problem thing 'aerial data type', aerial equal neutral('table_name_id_seq'). great table use aerial command sal result id column neutral('exampletable_id_seq'::regulars') value 'default' attribute. attempt manual input neutral() command within 'default' attribute column beaver new table, example. neutral('mytable_id_seq') without '::regulars;. howe working. appreci sal would easier, previous ask question at: problem de aerial data type beaver &amp; postgresql. however, could find satisfactory answer option all gun would useful, respect set done beaver gun. specifically, question is: function beaver add auto incitement id' gun? so, would step this."
48804649,how to perform a SELECT on a JSON column in mysql/mariaDB,"how to apply WHERE clause on JSON column to perform a SELECT query on a table which is having two columns (id Integer, attr JSON). The JSON is nested and in the filter condition there is only one key value pair of json is allowed. This key value pair can be anywhere in the Josn.
+----+-----------------------------------------------------------------
| id | attr                                                                                          
|
+----+-----------------------------------------------------------------
|  1 | {""id"":""0001"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55}                                         
|
|  2 | {""id"":""0002"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55,""batters"":
       {""batter1"":100,""batter2"":200}} 
+----+-----------------------------------------------------------------
",<mysql><mariadb><mysql-python>,814,0,9,471,1,6,14,76,41449,0.0,18,3,19,2018-02-15 10:13,2018-02-15 12:12,2018-02-15 12:12,0.0,0.0,Basic,10,"<mysql><mariadb><mysql-python>, how to perform a SELECT on a JSON column in mysql/mariaDB, how to apply WHERE clause on JSON column to perform a SELECT query on a table which is having two columns (id Integer, attr JSON). The JSON is nested and in the filter condition there is only one key value pair of json is allowed. This key value pair can be anywhere in the Josn.
+----+-----------------------------------------------------------------
| id | attr                                                                                          
|
+----+-----------------------------------------------------------------
|  1 | {""id"":""0001"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55}                                         
|
|  2 | {""id"":""0002"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55,""batters"":
       {""batter1"":100,""batter2"":200}} 
+----+-----------------------------------------------------------------
","<myself><maria><myself-patron>, perform select son column myself/maria, apply class son column perform select query table two column (id inter, att son). son nest filter conduct one key value pair son allowed. key value pair anywhere john. +----+----------------------------------------------------------------- | id | att | +----+----------------------------------------------------------------- | 1 | {""id"":""0001"",""type"":""dont"",""name"":""cake"",""pp"":0.55} | | 2 | {""id"":""0002"",""type"":""dont"",""name"":""cake"",""pp"":0.55,""matters"": {""battery"":100,""battery"":200}} +----+-----------------------------------------------------------------"
52657760,Android Room: How to Migrate Column Renaming?,"Issue
My app is crashing because I am not handling migration properly. I'm looking for a solution to migrate the name of 1 column in my table. 
In my project I a have a room table named 'content' with a Double attribute 'archivedCount'. In the latest version of the app the attribute archivedCount attribute is re-named to dismissCount, still as type Double.
Original Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var archiveCount: Double) : Parcelable {...}
New Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var dismissCount: Double) : Parcelable {...}
Attempted Solution
After reading a Google Developer Advocate's explanation Understanding migrations with Room, I attempted her solution outlined in the post's section Migrations with complex schema changes which entails making a copy of the original table, deleting the old table, then renaming the newly created table.
With the following approach below there is a runtime error on this line: database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users""); because I already cleared my app's cache so the old table no longer exists. 
Can I update a single column without re-creating the entire table?
static final Migration MIGRATION_1_2 = new Migration(1, 2) {
@Override
public void migrate(SupportSQLiteDatabase database) {
    // Create the new table
    database.execSQL(
            ""CREATE TABLE content_new (id TEXT, dismissCount REAL, PRIMARY KEY(id))"");
    // Copy the data
    database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users"");
    // Remove the old table
    database.execSQL(""DROP TABLE content"");
    // Change the table name to the correct one
    database.execSQL(""ALTER TABLE content_new RENAME TO content"");
  }
};
",<android><sql><android-sqlite><android-room>,1862,1,19,9836,12,72,137,58,12753,0.0,1581,2,19,2018-10-05 2:55,2018-10-05 7:01,2018-10-07 3:56,0.0,2.0,Basic,10,"<android><sql><android-sqlite><android-room>, Android Room: How to Migrate Column Renaming?, Issue
My app is crashing because I am not handling migration properly. I'm looking for a solution to migrate the name of 1 column in my table. 
In my project I a have a room table named 'content' with a Double attribute 'archivedCount'. In the latest version of the app the attribute archivedCount attribute is re-named to dismissCount, still as type Double.
Original Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var archiveCount: Double) : Parcelable {...}
New Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var dismissCount: Double) : Parcelable {...}
Attempted Solution
After reading a Google Developer Advocate's explanation Understanding migrations with Room, I attempted her solution outlined in the post's section Migrations with complex schema changes which entails making a copy of the original table, deleting the old table, then renaming the newly created table.
With the following approach below there is a runtime error on this line: database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users""); because I already cleared my app's cache so the old table no longer exists. 
Can I update a single column without re-creating the entire table?
static final Migration MIGRATION_1_2 = new Migration(1, 2) {
@Override
public void migrate(SupportSQLiteDatabase database) {
    // Create the new table
    database.execSQL(
            ""CREATE TABLE content_new (id TEXT, dismissCount REAL, PRIMARY KEY(id))"");
    // Copy the data
    database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users"");
    // Remove the old table
    database.execSQL(""DROP TABLE content"");
    // Change the table name to the correct one
    database.execSQL(""ALTER TABLE content_new RENAME TO content"");
  }
};
","<andros><sal><andros-quite><andros-room>, andros room: migrate column reading?, issue pp crash hand migrate properly. i'm look slut migrate name 1 column table. project room table name 'content' doubt attribute 'archivedcount'. latest version pp attribute archivedcount attribute re-am dismisscount, still type double. origin content model @entity(tablenam = ""content"") data class content(@primarykey war id: string, war archivecount: double) : parcel {...} new content model @entity(tablenam = ""content"") data class content(@primarykey war id: string, war dismisscount: double) : parcel {...} attempt slut read good develop advocate' explain understand migrate room, attempt slut outline post' section migrate complex scheme change entail make copy origin table, delete old table, renal newly great table. follow approach until error line: database.execsql(""insert content_new (id, dismisscount) select id, archivecount users""); already clear pp' each old table longer exists. update single column without re-great enter table? static final migrate migration_1_2 = new migration(1, 2) { @overdid public void migrate(supportsqlitedatabas database) { // great new table database.execsql( ""great table content_new (id text, dismisscount real, primary key(id))""); // copy data database.execsql(""insert content_new (id, dismisscount) select id, archivecount users""); // remove old table database.execsql(""drop table content""); // change table name correct one database.execsql(""at table content_new renal content""); } };"
56355516,"How to resolve the ""psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist"" when running django unittests on Travis","I'm using Travis for CI/CD as part of my Django app, with a postgresql database. (Django 2.1.4)
The build consistently fails on Travis as soon as the tests run. I receive this error:
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
I have tried: makemigrations, migrate auth, migrate myapp, migrate --run-syncdb.  All of which fail with the same error.
The tests run locally with a sqlite3 database, and on a prod-like heroku environment with a postgresql database.
.travis.yml
...
before script:
-psql -c 'create database travis_ci_test;' -U postgres
services:
-postgresql
script:
-yes | python3 manage.py makemigrations
-python3 manage.py migrate auth
-python3 manage.py migrate --run-syncdb
-python3 manage.py tests test/unit_tests
settings.py
...
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'travis_ci_test',
        'USER': 'postgres',
        'PASSWORD': '',
        'HOST': 'localhost',
        }
    }
...
INSTALLED_APPS = [...
        'django.contrib.auth',
        ]
Here is the output from the failing build on Travis.  'migrate auth' is successful (I think this is the crucial line for auth_user : Applying auth.0001_initial... OK)
0.22s$ psql -c 'create database travis_ci_test;' -U postgres
CREATE DATABASE
1.50s$ yes | python3 manage.py makemigrations
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
No changes detected
The command ""yes | python3 manage.py makemigrations"" exited with 0.
1.68s$ python3 manage.py migrate auth
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
Operations to perform:
  Apply all migrations: auth
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length... OK
  Applying auth.0009_alter_user_last_name_max_length... OK
The command ""python3 manage.py migrate auth"" exited with 0.
1.57s$ python3 manage.py migrate --run-syncdb
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp.apps.MyAppConfig', 'storages']
Operations to perform:
  Synchronize unmigrated apps: messages, myapp, staticfiles, storages
  Apply all migrations: admin, auth, contenttypes, sessions
Synchronizing apps without migrations:
  Creating tables...
    Creating table myapp_model1
    Creating table myapp_model2
    Creating table myapp_model3
    Creating table myapp_model4
    Creating table myapp_model5
    Running deferred SQL...
Running migrations:
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying sessions.0001_initial... OK
The command ""python3 manage.py migrate --run-syncdb"" exited with 0.
1.40s$ python3 manage.py test tests/unit_tests
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'app.apps.MyAppConfig', 'storages']
Creating test database for alias 'default'...
Traceback (most recent call last):
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""manage.py"", line 10, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 381, in execute_from_command_line
    utility.execute()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 375, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 26, in run_from_argv
    super().run_from_argv(argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 316, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 56, in handle
    failures = test_runner.run_tests(test_labels)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 604, in run_tests
    old_config = self.setup_databases()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 551, in setup_databases
    self.parallel, **kwargs
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/utils.py"", line 174, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/creation.py"", line 68, in create_test_db
    run_syncdb=True,
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 148, in call_command
    return command.execute(*args, **defaults)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 83, in wrapped
    res = handle_func(*args, **kwargs)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 172, in handle
    self.sync_apps(connection, executor.loader.unmigrated_apps)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 310, in sync_apps
    self.stdout.write(""    Running deferred SQL...\n"")
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 106, in __exit__
    self.execute(sql)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 133, in execute
    cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 68, in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 77, in _execute_with_wrappers
    return executor(sql, params, many, context)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
django.db.utils.ProgrammingError: relation ""auth_user"" does not exist
The command ""python3 manage.py test tests/unit_tests"" exited with 1.
",<django><python-3.x><postgresql><travis-ci><psycopg2>,9076,0,142,519,1,5,18,81,36847,0.0,28,3,19,2019-05-29 7:49,2019-12-03 18:47,2019-12-03 18:47,188.0,188.0,Advanced,32,"<django><python-3.x><postgresql><travis-ci><psycopg2>, How to resolve the ""psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist"" when running django unittests on Travis, I'm using Travis for CI/CD as part of my Django app, with a postgresql database. (Django 2.1.4)
The build consistently fails on Travis as soon as the tests run. I receive this error:
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
I have tried: makemigrations, migrate auth, migrate myapp, migrate --run-syncdb.  All of which fail with the same error.
The tests run locally with a sqlite3 database, and on a prod-like heroku environment with a postgresql database.
.travis.yml
...
before script:
-psql -c 'create database travis_ci_test;' -U postgres
services:
-postgresql
script:
-yes | python3 manage.py makemigrations
-python3 manage.py migrate auth
-python3 manage.py migrate --run-syncdb
-python3 manage.py tests test/unit_tests
settings.py
...
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'travis_ci_test',
        'USER': 'postgres',
        'PASSWORD': '',
        'HOST': 'localhost',
        }
    }
...
INSTALLED_APPS = [...
        'django.contrib.auth',
        ]
Here is the output from the failing build on Travis.  'migrate auth' is successful (I think this is the crucial line for auth_user : Applying auth.0001_initial... OK)
0.22s$ psql -c 'create database travis_ci_test;' -U postgres
CREATE DATABASE
1.50s$ yes | python3 manage.py makemigrations
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
No changes detected
The command ""yes | python3 manage.py makemigrations"" exited with 0.
1.68s$ python3 manage.py migrate auth
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
Operations to perform:
  Apply all migrations: auth
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length... OK
  Applying auth.0009_alter_user_last_name_max_length... OK
The command ""python3 manage.py migrate auth"" exited with 0.
1.57s$ python3 manage.py migrate --run-syncdb
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp.apps.MyAppConfig', 'storages']
Operations to perform:
  Synchronize unmigrated apps: messages, myapp, staticfiles, storages
  Apply all migrations: admin, auth, contenttypes, sessions
Synchronizing apps without migrations:
  Creating tables...
    Creating table myapp_model1
    Creating table myapp_model2
    Creating table myapp_model3
    Creating table myapp_model4
    Creating table myapp_model5
    Running deferred SQL...
Running migrations:
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying sessions.0001_initial... OK
The command ""python3 manage.py migrate --run-syncdb"" exited with 0.
1.40s$ python3 manage.py test tests/unit_tests
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'app.apps.MyAppConfig', 'storages']
Creating test database for alias 'default'...
Traceback (most recent call last):
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""manage.py"", line 10, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 381, in execute_from_command_line
    utility.execute()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 375, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 26, in run_from_argv
    super().run_from_argv(argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 316, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 56, in handle
    failures = test_runner.run_tests(test_labels)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 604, in run_tests
    old_config = self.setup_databases()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 551, in setup_databases
    self.parallel, **kwargs
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/utils.py"", line 174, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/creation.py"", line 68, in create_test_db
    run_syncdb=True,
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 148, in call_command
    return command.execute(*args, **defaults)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 83, in wrapped
    res = handle_func(*args, **kwargs)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 172, in handle
    self.sync_apps(connection, executor.loader.unmigrated_apps)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 310, in sync_apps
    self.stdout.write(""    Running deferred SQL...\n"")
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 106, in __exit__
    self.execute(sql)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 133, in execute
    cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 68, in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 77, in _execute_with_wrappers
    return executor(sql, params, many, context)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
django.db.utils.ProgrammingError: relation ""auth_user"" does not exist
The command ""python3 manage.py test tests/unit_tests"" exited with 1.
","<django><patron-3.x><postgresql><train-i><psycopg2>, resolve ""psycopg2.errors.undefinedtable: relate ""auth_user"" exist"" run django unites train, i'm use trade i/d part django pp, postgresql database. (django 2.1.4) build consist fail trade soon test run. receive error: psycopg2.errors.undefinedtable: relate ""auth_user"" exist tried: makemigrations, migrate auto, migrate map, migrate --run-synod. fail error. test run local sqlite3 database, proud-like hero environs postgresql database. .train.you ... script: -pool -c 'great database travis_ci_test;' -u poster services: -postgresql script: -ye | python3 manage.i makemigr -python3 manage.i migrate auto -python3 manage.i migrate --run-synod -python3 manage.i test test/unit_test settings.i ... database = { 'default': { 'engine': 'django.do.backed.postgresql_psycopg2', 'name': 'travis_ci_test', 'user': 'postures', 'password': '', 'host': 'localhost', } } ... installed_app = [... 'django.control.auto', ] output fail build train. 'migrate auto' success (i think crucial line authors : apply auto.0001_initial... ok) 0.was$ pool -c 'great database travis_ci_test;' -u poster great database 1.was$ ye | python3 manage.i makemigr test_env... aws_integration... database ... {'engine': 'django.do.backed.postgresql_psycopg2', 'name': 'travis_ci_test', 'user': 'postures', 'password': '', 'host': 'localhost'} instal pp ... ['django.control.admit', 'django.control.auto', 'django.control.contenttypes', 'django.control.sessions', 'django.control.messages', 'django.control.staticfiles', 'races.apes.racesconfig', 'storage'] change detect command ""ye | python3 manage.i makemigrations"" exit 0. 1.was$ python3 manage.i migrate auto test_env... aws_integration... database ... {'engine': 'django.do.backed.postgresql_psycopg2', 'name': 'travis_ci_test', 'user': 'postures', 'password': '', 'host': 'localhost'} instal pp ... ['django.control.admit', 'django.control.auto', 'django.control.contenttypes', 'django.control.sessions', 'django.control.messages', 'django.control.staticfiles', 'races.apes.racesconfig', 'storage'] over perform: apply migrations: auto run migrations: apply contenttypes.0001_initial... ok apply auto.0001_initial... ok apply contenttypes.0002_remove_content_type_name... ok apply auto.0002_alter_permission_name_max_length... ok apply auto.0003_alter_user_email_max_length... ok apply auto.0004_alter_user_username_opts... ok apply auto.0005_alter_user_last_login_null... ok apply auto.0006_require_contenttypes_0002... ok apply auto.0007_alter_validators_add_error_messages... ok apply auto.0008_alter_user_username_max_length... ok apply auto.0009_alter_user_last_name_max_length... ok command ""python3 manage.i migrate auto"" exit 0. 1.was$ python3 manage.i migrate --run-synod test_env... aws_integration... database ... {'engine': 'django.do.backed.postgresql_psycopg2', 'name': 'travis_ci_test', 'user': 'postures', 'password': '', 'host': 'localhost'} instal pp ... ['django.control.admit', 'django.control.auto', 'django.control.contenttypes', 'django.control.sessions', 'django.control.messages', 'django.control.staticfiles', 'map.apes.myappconfig', 'storage'] over perform: synchron unmigr apes: messages, map, staticfiles, storage apply migrations: admit, auto, contenttypes, session synchron pp without migrations: great tables... great table myapp_model1 great table myapp_model2 great table myapp_model3 great table myapp_model4 great table myapp_model5 run defer sal... run migrations: apply admit.0001_initial... ok apply admit.0002_logentry_remove_auto_add... ok apply admit.0003_logentry_add_action_flag_choices... ok apply sessions.0001_initial... ok command ""python3 manage.i migrate --run-synod"" exit 0. 1.was$ python3 manage.i test tests/unit_test test_env... aws_integration... database ... {'engine': 'django.do.backed.postgresql_psycopg2', 'name': 'travis_ci_test', 'user': 'postures', 'password': '', 'host': 'localhost'} instal pp ... ['django.control.admit', 'django.control.auto', 'django.control.contenttypes', 'django.control.sessions', 'django.control.messages', 'django.control.staticfiles', 'pp.apes.myappconfig', 'storage'] great test database asia 'default'... traceback (most recent call last): file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/still.by"", line 85, execute return self.curses.execute(sal, parts) psycopg2.errors.undefinedtable: relate ""auth_user"" exist except direct cause follow exception: traceback (most recent call last): file ""manage.by"", line 10, &it;module&it; execute_from_command_line(says.are) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/__init__.by"", line 381, execute_from_command_lin utility.execute() file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/__init__.by"", line 375, execute self.fetch_command(subcommand).run_from_argv(self.are) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/commands/test.by"", line 26, run_from_argv super().run_from_argv(are) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/base.by"", line 316, run_from_argv self.execute(*arms, **cmd_options) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/base.by"", line 353, execute output = self.handle(*arms, **option) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/commands/test.by"", line 56, hand failure = test_runner.run_tests(test_labels) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/test/runner.by"", line 604, run_test old_config = self.setup_databases() file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/test/runner.by"", line 551, setup_databas self.parallel, **war file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/test/still.by"", line 174, setup_databas serialize=connection.settings_dict.get('test', {}).get('serialize', true), file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/base/creation.by"", line 68, create_test_db run_syncdb=true, file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/__init__.by"", line 148, call_command return command.execute(*arms, **default) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/base.by"", line 353, execute output = self.handle(*arms, **option) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/base.by"", line 83, wrap re = handle_func(*arms, **wars) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/commands/migrate.by"", line 172, hand self.sync_apps(connection, executor.leader.unmigrated_apps) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/core/management/commands/migrate.by"", line 310, sync_app self.stout.write("" run defer sal...\n"") file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/base/scheme.by"", line 106, __exit__ self.execute(sal) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/base/scheme.by"", line 133, execute curses.execute(sal, parts) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/still.by"", line 68, execute return self._execute_with_wrappers(sal, parts, many=false, executor=self.execute) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/still.by"", line 77, _execute_with_wrapp return executor(sal, parts, many, context) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/still.by"", line 85, execute return self.curses.execute(sal, parts) file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/still.by"", line 89, __exit__ rays dj_exc_value.with_traceback(traceback) exc_valu file ""/home/train/virtualenv/python3.6.7/limb/python3.6/site-packages/django/do/backed/still.by"", line 85, execute return self.curses.execute(sal, parts) django.do.still.programmingerror: relate ""auth_user"" exist command ""python3 manage.i test tests/unit_tests"" exit 1."
58085851,Access denied for user 'root'@'localhost' with mariadb 10.4.8 docker container using docker compose and issue while attaching external volume,"I am new to Docker, I was trying to crate docker container of mariadb for my application but when I start running mariadb container it shows Access denied for user 'root'@'localhost' (using password: YES) dockerfile 
Following is the docker compose I am using.
version: '3'
services:
  mysql:
    image: mariadb
    container_name: mariadb
    volumes:
      - dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_ROOT_USER: root
      MYSQL_USER: root
      MYSQL_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  dbvolume:
After trying for multiple times by referring few links I was able to connect my application to docker container but it failed to import AppDatabase.sql script at the time of creating docker container.
But now by using same docker compose file I am not able to connect mariadb to my application and I think even it's not importing SQL script to the database (based on logs I have observed).
Following is the docker log generated while running docker compose:
$ docker logs 3fde358ff015
2019-09-24 17:40:37 0 [Note] mysqld (mysqld 10.4.8-MariaDB-1:10.4.8+maria~bionic) starting as process 1 ...
2019-09-24 17:40:37 0 [Note] InnoDB: Using Linux native AIO
2019-09-24 17:40:37 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2019-09-24 17:40:37 0 [Note] InnoDB: Uses event mutexes
2019-09-24 17:40:37 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-09-24 17:40:37 0 [Note] InnoDB: Number of pools: 1
2019-09-24 17:40:37 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-09-24 17:40:37 0 [Note] mysqld: O_TMPFILE is not supported on /tmp (disabling future attempts)
2019-09-24 17:40:37 0 [Note] InnoDB: Initializing buffer pool, total size = 256M, instances = 1, chunk size = 128M
2019-09-24 17:40:37 0 [Note] InnoDB: Completed initialization of buffer pool
2019-09-24 17:40:37 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2019-09-24 17:40:37 0 [Note] InnoDB: Upgrading redo log: 2*50331648 bytes; LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: Starting to delete and rewrite log files.
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile101 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile1 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0
2019-09-24 17:40:38 0 [Note] InnoDB: New log files created, LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-09-24 17:40:38 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-09-24 17:40:38 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-09-24 17:40:38 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2019-09-24 17:40:38 0 [Note] InnoDB: Waiting for purge to start
2019-09-24 17:40:38 0 [Note] InnoDB: 10.4.8 started; log sequence number 21810033; transaction id 14620
2019-09-24 17:40:38 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2019-09-24 17:40:38 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-09-24 17:40:38 0 [Note] Server socket created on IP: '::'.
2019-09-24 17:40:38 0 [Warning] 'proxies_priv' entry '@% root@c980daa43351' ignored in --skip-name-resolve mode.
2019-09-24 17:40:38 0 [Note] InnoDB: Buffer pool(s) load completed at 190924 17:40:38
2019-09-24 17:40:38 0 [Note] Reading of all Master_info entries succeeded
2019-09-24 17:40:38 0 [Note] Added new Master_info '' to hash table
2019-09-24 17:40:38 0 [Note] mysqld: ready for connections.
Version: '10.4.8-MariaDB-1:10.4.8+maria~bionic'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  mariadb.org binary distribution
SQL Script I am trying to import:
create database appdata;
use appdata;
CREATE TABLE `appdatadetails` (
  `Name` varchar(8) NOT NULL,
  `appIndex` int(11) NOT NULL,
  `connector` varchar(16) DEFAULT NULL,
  `intName` varchar(12) DEFAULT NULL,
  `intIndex` int(11) DEFAULT NULL,
  PRIMARY KEY (`Name`,`appIndex`)
) 
Please help me to understand what I am doing wrong, I have tried all possible solution posted on different blogs. 
Update:
Latest Update:
I was able to up and running mariadb docker image with 10.1. But if I attach volume then still I am facing issue.
Docker Compose:
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - container-volume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  container-volume:
And the log error message, If I attach container-volume volume.
Creating mariadb ... done
Attaching to mariadb
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] mysqld (mysqld 10.1.41-MariaDB-1~bionic) starting as process 1 ...
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using mutexes to ref count buffer pool pages
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: The InnoDB memory heap is disabled
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Compressed tables use zlib 1.2.11
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using Linux native AIO
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using SSE crc32 instructions
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Initializing buffer pool, size = 256.0M
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Completed initialization of buffer pool
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Highest supported file format is Barracuda.
mariadb  | InnoDB: No valid checkpoint found.
mariadb  | InnoDB: A downgrade from MariaDB 10.2.2 or later is not supported.
mariadb  | InnoDB: If this error appears when you are creating an InnoDB database,
mariadb  | InnoDB: the problem may be that during an earlier attempt you managed
mariadb  | InnoDB: to create the InnoDB data files, but log file creation failed.
mariadb  | InnoDB: If that is the case, please refer to
mariadb  | InnoDB: http://dev.mysql.com/doc/refman/5.6/en/error-creating-innodb.html
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' init function returned error.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] Plugin 'FEEDBACK' is disabled.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Unknown/unsupported storage engine: InnoDB
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Aborting
mariadb  | 
mariadb exited with code 1
If I remove container-volume then It is importing .sql script and running well and good.
Updated with working script: Before I was using mariadb 10.4.8 or latest and facing issue(s) to access DB and attaching external volume.
Now I have downgraded (As suggested by @Adiii) and tried. It runs perfectlly and we no need to specify external: true in volumes service
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - ./dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
",<mysql><docker><docker-compose><mariadb>,7774,2,122,1863,6,41,58,80,40726,0.0,101,1,19,2019-09-24 17:58,2019-09-24 18:13,2019-09-24 18:13,0.0,0.0,Advanced,32,"<mysql><docker><docker-compose><mariadb>, Access denied for user 'root'@'localhost' with mariadb 10.4.8 docker container using docker compose and issue while attaching external volume, I am new to Docker, I was trying to crate docker container of mariadb for my application but when I start running mariadb container it shows Access denied for user 'root'@'localhost' (using password: YES) dockerfile 
Following is the docker compose I am using.
version: '3'
services:
  mysql:
    image: mariadb
    container_name: mariadb
    volumes:
      - dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_ROOT_USER: root
      MYSQL_USER: root
      MYSQL_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  dbvolume:
After trying for multiple times by referring few links I was able to connect my application to docker container but it failed to import AppDatabase.sql script at the time of creating docker container.
But now by using same docker compose file I am not able to connect mariadb to my application and I think even it's not importing SQL script to the database (based on logs I have observed).
Following is the docker log generated while running docker compose:
$ docker logs 3fde358ff015
2019-09-24 17:40:37 0 [Note] mysqld (mysqld 10.4.8-MariaDB-1:10.4.8+maria~bionic) starting as process 1 ...
2019-09-24 17:40:37 0 [Note] InnoDB: Using Linux native AIO
2019-09-24 17:40:37 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2019-09-24 17:40:37 0 [Note] InnoDB: Uses event mutexes
2019-09-24 17:40:37 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-09-24 17:40:37 0 [Note] InnoDB: Number of pools: 1
2019-09-24 17:40:37 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-09-24 17:40:37 0 [Note] mysqld: O_TMPFILE is not supported on /tmp (disabling future attempts)
2019-09-24 17:40:37 0 [Note] InnoDB: Initializing buffer pool, total size = 256M, instances = 1, chunk size = 128M
2019-09-24 17:40:37 0 [Note] InnoDB: Completed initialization of buffer pool
2019-09-24 17:40:37 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2019-09-24 17:40:37 0 [Note] InnoDB: Upgrading redo log: 2*50331648 bytes; LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: Starting to delete and rewrite log files.
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile101 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile1 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0
2019-09-24 17:40:38 0 [Note] InnoDB: New log files created, LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-09-24 17:40:38 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-09-24 17:40:38 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-09-24 17:40:38 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2019-09-24 17:40:38 0 [Note] InnoDB: Waiting for purge to start
2019-09-24 17:40:38 0 [Note] InnoDB: 10.4.8 started; log sequence number 21810033; transaction id 14620
2019-09-24 17:40:38 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2019-09-24 17:40:38 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-09-24 17:40:38 0 [Note] Server socket created on IP: '::'.
2019-09-24 17:40:38 0 [Warning] 'proxies_priv' entry '@% root@c980daa43351' ignored in --skip-name-resolve mode.
2019-09-24 17:40:38 0 [Note] InnoDB: Buffer pool(s) load completed at 190924 17:40:38
2019-09-24 17:40:38 0 [Note] Reading of all Master_info entries succeeded
2019-09-24 17:40:38 0 [Note] Added new Master_info '' to hash table
2019-09-24 17:40:38 0 [Note] mysqld: ready for connections.
Version: '10.4.8-MariaDB-1:10.4.8+maria~bionic'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  mariadb.org binary distribution
SQL Script I am trying to import:
create database appdata;
use appdata;
CREATE TABLE `appdatadetails` (
  `Name` varchar(8) NOT NULL,
  `appIndex` int(11) NOT NULL,
  `connector` varchar(16) DEFAULT NULL,
  `intName` varchar(12) DEFAULT NULL,
  `intIndex` int(11) DEFAULT NULL,
  PRIMARY KEY (`Name`,`appIndex`)
) 
Please help me to understand what I am doing wrong, I have tried all possible solution posted on different blogs. 
Update:
Latest Update:
I was able to up and running mariadb docker image with 10.1. But if I attach volume then still I am facing issue.
Docker Compose:
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - container-volume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  container-volume:
And the log error message, If I attach container-volume volume.
Creating mariadb ... done
Attaching to mariadb
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] mysqld (mysqld 10.1.41-MariaDB-1~bionic) starting as process 1 ...
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using mutexes to ref count buffer pool pages
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: The InnoDB memory heap is disabled
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Compressed tables use zlib 1.2.11
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using Linux native AIO
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using SSE crc32 instructions
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Initializing buffer pool, size = 256.0M
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Completed initialization of buffer pool
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Highest supported file format is Barracuda.
mariadb  | InnoDB: No valid checkpoint found.
mariadb  | InnoDB: A downgrade from MariaDB 10.2.2 or later is not supported.
mariadb  | InnoDB: If this error appears when you are creating an InnoDB database,
mariadb  | InnoDB: the problem may be that during an earlier attempt you managed
mariadb  | InnoDB: to create the InnoDB data files, but log file creation failed.
mariadb  | InnoDB: If that is the case, please refer to
mariadb  | InnoDB: http://dev.mysql.com/doc/refman/5.6/en/error-creating-innodb.html
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' init function returned error.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] Plugin 'FEEDBACK' is disabled.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Unknown/unsupported storage engine: InnoDB
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Aborting
mariadb  | 
mariadb exited with code 1
If I remove container-volume then It is importing .sql script and running well and good.
Updated with working script: Before I was using mariadb 10.4.8 or latest and facing issue(s) to access DB and attaching external volume.
Now I have downgraded (As suggested by @Adiii) and tried. It runs perfectlly and we no need to specify external: true in volumes service
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - ./dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
","<myself><doctor><doctor-compose><maria>, access den user 'root'@'localhost' maria 10.4.8 doctor contain use doctor compose issue attach externa volume, new doctor, try crate doctor contain maria applied start run maria contain show access den user 'root'@'localhost' (use password: yes) dockerfil follow doctor compose using. version: '3' services: myself: image: maria container_name: maria volumes: - volume:/war/limb/myself - ./appdatabase.sal:/doctor-entrypoint-initdb.d/appdatabase.sal environment: mysql_root_password: root123 mysql_root_user: root mysql_user: root mysql_password: root123 mysql_database: appdata ports: - ""3333:3306"" volumes: volume: try multiple time refer link all connect applied doctor contain fail import appdatabase.sal script time great doctor container. use doctor compose file all connect maria applied think even import sal script database (base log observed). follow doctor log genet run doctor compose: $ doctor log 3fde358ff015 2019-09-24 17:40:37 0 [note] myself (myself 10.4.8-maria-1:10.4.8+maria~ironic) start process 1 ... 2019-09-24 17:40:37 0 [note] innodb: use line native air 2019-09-24 17:40:37 0 [note] innodb: mute rw_lock use go atom built 2019-09-24 17:40:37 0 [note] innodb: use event mute 2019-09-24 17:40:37 0 [note] innodb: compress table use limb 1.2.11 2019-09-24 17:40:37 0 [note] innodb: number pools: 1 2019-09-24 17:40:37 0 [note] innodb: use she crc32 instruct 2019-09-24 17:40:37 0 [note] myself: o_tmpfil support /tm (distal future attempts) 2019-09-24 17:40:37 0 [note] innodb: into suffer pool, total size = 256m, instant = 1, chink size = 128m 2019-09-24 17:40:37 0 [note] innodb: complete into suffer pool 2019-09-24 17:40:37 0 [note] innodb: myself execute user authorized, page cleaner thread priority changed. see man page setpriority(). 2019-09-24 17:40:37 0 [note] innodb: upgrade red log: 2*50331648 bites; isn=21810033 2019-09-24 17:40:38 0 [note] innodb: start delete regret log files. 2019-09-24 17:40:38 0 [note] innodb: set log file ./ib_logfile101 size 50331648 bite 2019-09-24 17:40:38 0 [note] innodb: set log file ./ib_logfile1 size 50331648 bite 2019-09-24 17:40:38 0 [note] innodb: renal log file ./ib_logfile101 ./ib_logfile0 2019-09-24 17:40:38 0 [note] innodb: new log file created, isn=21810033 2019-09-24 17:40:38 0 [note] innodb: 128 128 rollback segment active. 2019-09-24 17:40:38 0 [note] innodb: great share tablespac temporary table 2019-09-24 17:40:38 0 [note] innodb: set file './ibtmp1' size 12 mb. physics write file full; pleas wait ... 2019-09-24 17:40:38 0 [note] innodb: file './ibtmp1' size 12 mb. 2019-09-24 17:40:38 0 [note] innodb: wait pure start 2019-09-24 17:40:38 0 [note] innodb: 10.4.8 started; log sequence number 21810033; transact id 14620 2019-09-24 17:40:38 0 [note] innodb: load suffer pool(s) /war/limb/myself/ib_buffer_pool 2019-09-24 17:40:38 0 [note] plain 'feedback' disabled. 2019-09-24 17:40:38 0 [note] server socket great in: '::'. 2019-09-24 17:40:38 0 [warning] 'proxies_priv' entry '@% root@c980daa43351' ignore --skin-name-resolve mode. 2019-09-24 17:40:38 0 [note] innodb: suffer pool(s) load complete 190924 17:40:38 2019-09-24 17:40:38 0 [note] read master_info entry succeed 2019-09-24 17:40:38 0 [note] ad new master_info '' has table 2019-09-24 17:40:38 0 [note] myself: ready connections. version: '10.4.8-maria-1:10.4.8+maria~ironic' socket: '/war/run/myself/myself.sock' port: 3306 maria.org binary distribute sal script try import: great database appdata; use appdata; great table `appdatadetails` ( `name` varchar(8) null, `appendix` in(11) null, `connection` varchar(16) default null, `innate` varchar(12) default null, `intended` in(11) default null, primary key (`name`,`appendix`) ) pleas help understand wrong, try possible slut post differ blows. update: latest update: all run maria doctor image 10.1. attach volume still face issue. doctor compose: version: '3' services: myself: image: maria:10.1 container_name: maria volumes: - container-volume:/war/limb/myself - ./appdatabase.sal:/doctor-entrypoint-initdb.d/appdatabase.sal environment: mysql_root_password: root123 mysql_database: appdata ports: - ""3333:3306"" volumes: container-volume: log error message, attach container-volume volume. great maria ... done attach maria maria | 2019-09-25 6:56:26 140542855440384 [note] myself (myself 10.1.41-maria-1~ironic) start process 1 ... maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: use mute red count suffer pool page maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: innodb memory heap distal maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: mute rw_lock use go atom built maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: go built __atomic_thread_fence() use memory barrier maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: compress table use limb 1.2.11 maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: use line native air maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: use she crc32 instruct maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: into suffer pool, size = 256.am maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: complete into suffer pool maria | 2019-09-25 6:56:26 140542855440384 [note] innodb: highest support file format barracuda. maria | innodb: valid checkpoint found. maria | innodb: downward maria 10.2.2 later supported. maria | innodb: error appear great innodb database, maria | innodb: problem may earlier attempt manage maria | innodb: great innodb data files, log file creation failed. maria | innodb: case, pleas refer maria | innodb: http://de.myself.com/do/german/5.6/en/error-creating-innodb.html maria | 2019-09-25 6:56:26 140542855440384 [error] plain 'innodb' knit function return error. maria | 2019-09-25 6:56:26 140542855440384 [error] plain 'innodb' register storage engine failed. maria | 2019-09-25 6:56:26 140542855440384 [note] plain 'feedback' disabled. maria | 2019-09-25 6:56:26 140542855440384 [error] unknown/support storage engine: innodb maria | 2019-09-25 6:56:26 140542855440384 [error] abort maria | maria exit code 1 remove container-volume import .sal script run well good. update work script: use maria 10.4.8 latest face issue(s) access do attach externa volume. downward (a suggest @iii) tried. run perfectly need specific external: true volume service version: '3' services: myself: image: maria:10.1 container_name: maria volumes: - ./volume:/war/limb/myself - ./appdatabase.sal:/doctor-entrypoint-initdb.d/appdatabase.sal environment: mysql_root_password: root123 mysql_database: appdata ports: - ""3333:3306"""
53885511,"pq: sorry, too many clients already","I am getting pq: sorry, too many clients already error when I am calling the GetMessages() multiple times. 
Please find the updated code:
main() code
func main() {
  dbConn, err := InitDB()
  if err != nil {
    Log.Error(""Connection Error: "", err.Error())
    return
  }
  defer dbConn.Close()
  go run()
  var input string
  fmt.Scanln(&amp;input)
}
Database connection code is:
func InitDB()(*sql.DB, error) {
  connectionString := fmt.Sprintf(""user=%v password='%v' dbname=%v sslmode=disable"", USER, PASSWORD, DATABASE)
  db, err = sql.Open(DRIVER, connectionString)
  return db, err
}
run goroutine:
func run() {
  for {
    messages, err := GetMessages()
    if err != nil {
      Log.Error(""Connection Error: "", err.Error())
      return
    }
    log.Info(messages)
  }
}
GetMessages() function code: 
func GetMessages() (messages []string, err error) {
    rows, err := db.Query(`SELECT message1, message2, message3, message4, message5,
            message6, message7, message8, message9, message10, message11, message12, message13, 
            message14, message15, message16, message17, message18, message19, message20, message21,
            message22, message23, message24, message25, message26, message27, message28, message29,
            message30, message31, message32, message33, message34, message35, message36, message37,
            message38, message39, message40, message41, message42, message43, message44, message45,
            message46, message47, message48 FROM table1 WHERE id=1`)
    if err != nil {
        Log.Error(""Query error"", err)
        return messages, err
    }
    var pointers []interface{}
    defer rows.Close()
    for rows.Next() {
        pointers = make([]interface{}, 48)
        messages = make([]string, 48)
        for i, _ := range pointers {
            pointers[i] = &amp;messages[i]
        }
        err = rows.Scan(pointers...)
        if err != nil {
            Log.Error(""Failed to scan row"", err)
            return messages, err
        }
    }
    return messages, nil
}
I checked this answer and I have used scan but still it isn't working
UPDATE
Issue was in another function. I was using db.Query without closing the returned rows object and was repeatedly calling that function. I've updated my code; used db.Exec instead of db.Query and it's working now. Thank you so much @mkopriva for this answer. :)
",<postgresql><go>,2375,3,68,1034,4,17,27,80,18829,0.0,828,2,19,2018-12-21 13:21,2021-08-24 18:13,,977.0,,Advanced,32,"<postgresql><go>, pq: sorry, too many clients already, I am getting pq: sorry, too many clients already error when I am calling the GetMessages() multiple times. 
Please find the updated code:
main() code
func main() {
  dbConn, err := InitDB()
  if err != nil {
    Log.Error(""Connection Error: "", err.Error())
    return
  }
  defer dbConn.Close()
  go run()
  var input string
  fmt.Scanln(&amp;input)
}
Database connection code is:
func InitDB()(*sql.DB, error) {
  connectionString := fmt.Sprintf(""user=%v password='%v' dbname=%v sslmode=disable"", USER, PASSWORD, DATABASE)
  db, err = sql.Open(DRIVER, connectionString)
  return db, err
}
run goroutine:
func run() {
  for {
    messages, err := GetMessages()
    if err != nil {
      Log.Error(""Connection Error: "", err.Error())
      return
    }
    log.Info(messages)
  }
}
GetMessages() function code: 
func GetMessages() (messages []string, err error) {
    rows, err := db.Query(`SELECT message1, message2, message3, message4, message5,
            message6, message7, message8, message9, message10, message11, message12, message13, 
            message14, message15, message16, message17, message18, message19, message20, message21,
            message22, message23, message24, message25, message26, message27, message28, message29,
            message30, message31, message32, message33, message34, message35, message36, message37,
            message38, message39, message40, message41, message42, message43, message44, message45,
            message46, message47, message48 FROM table1 WHERE id=1`)
    if err != nil {
        Log.Error(""Query error"", err)
        return messages, err
    }
    var pointers []interface{}
    defer rows.Close()
    for rows.Next() {
        pointers = make([]interface{}, 48)
        messages = make([]string, 48)
        for i, _ := range pointers {
            pointers[i] = &amp;messages[i]
        }
        err = rows.Scan(pointers...)
        if err != nil {
            Log.Error(""Failed to scan row"", err)
            return messages, err
        }
    }
    return messages, nil
}
I checked this answer and I have used scan but still it isn't working
UPDATE
Issue was in another function. I was using db.Query without closing the returned rows object and was repeatedly calling that function. I've updated my code; used db.Exec instead of db.Query and it's working now. Thank you so much @mkopriva for this answer. :)
","<postgresql><go>, pp: sorry, man client already, get pp: sorry, man client already error call getmessages() multiple times. pleas find update code: main() code fun main() { dbconn, err := initdb() err != nail { log.error(""connect error: "", err.error()) return } defer dbconn.close() go run() war input string fat.scale(&amp;input) } database connect code is: fun initdb()(*sal.do, error) { connections := fat.spring(""user=%v password='%v' name=%v sslmode=disabled"", user, password, database) do, err = sal.open(driver, connectionstring) return do, err } run routine: fun run() { { messages, err := getmessages() err != nail { log.error(""connect error: "", err.error()) return } log.into(messages) } } getmessages() function code: fun getmessages() (message []string, err error) { rows, err := do.query(`select message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message, message table id=1`) err != nail { log.error(""query error"", err) return messages, err } war pointer []interface{} defer rows.close() rows.next() { pointer = make([]interface{}, 48) message = make([]string, 48) i, _ := rang pointer { printers[i] = &amp;messages[i] } err = rows.scan(printers...) err != nail { log.error(""fail scan row"", err) return messages, err } } return messages, nail } check answer use scan still work update issue not function. use do.query without close return row object repeatedly call function. i'v update code; use do.even instead do.query work now. thank much @mkopriva answer. :)"
51109440,Parameter must be an array or an object that implements Countable in phpmyadmin,"When I try and view the wp_posts table in phpmyadmin, I see this error message, but have no idea what it means and have never seen it before.
Can someone help me try and get rid of this somehow?
Warning in ./libraries/sql.lib.php#613
count(): Parameter must be an array or an object that implements Countable
Backtrace
./libraries/sql.lib.php#2128: PMA_isRememberSortingOrder(array)
./libraries/sql.lib.php#2079: PMA_executeQueryAndGetQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
./sql.php#221: PMA_executeQueryAndSendQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
",<sql><wordpress>,919,0,46,4196,7,25,71,59,80218,0.0,142,2,19,2018-06-29 21:10,2018-06-29 22:16,,0.0,,Advanced,32,"<sql><wordpress>, Parameter must be an array or an object that implements Countable in phpmyadmin, When I try and view the wp_posts table in phpmyadmin, I see this error message, but have no idea what it means and have never seen it before.
Can someone help me try and get rid of this somehow?
Warning in ./libraries/sql.lib.php#613
count(): Parameter must be an array or an object that implements Countable
Backtrace
./libraries/sql.lib.php#2128: PMA_isRememberSortingOrder(array)
./libraries/sql.lib.php#2079: PMA_executeQueryAndGetQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
./sql.php#221: PMA_executeQueryAndSendQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
","<sal><wordpress>, parapet must array object implement countabl phpmyadmin, try view wp_post table phpmyadmin, see error message, idea mean never seen before. someone help try get rid somehow? warn ./libraries/sal.limb.pp#613 count(): parapet must array object implement countabl backtrac ./libraries/sal.limb.pp#2128: pma_isremembersortingorder(array) ./libraries/sal.limb.pp#2079: pma_executequeryandgetqueryresponse( array, woolen true, string 'afterhours', string 'wp_posts', null, null, null, null, null, null, string '', string './themes/original/ing/', null, null, null, string 'select * `wp_posts`', null, null, ) ./sal.pp#221: pma_executequeryandsendqueryresponse( array, woolen true, string 'afterhours', string 'wp_posts', null, null, null, null, null, null, string '', string './themes/original/ing/', null, null, null, string 'select * `wp_posts`', null, null, )"
53885749,Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type,"I have a DB which has a custom data type FollowEntityType as a column.
@Entity(primaryKeys = arrayOf(&quot;id&quot;, &quot;type&quot;), tableName = &quot;follow_info&quot;)
data class FollowInfoEntity(
        @ColumnInfo(name = &quot;id&quot;) var id: String,
        @ColumnInfo(name = &quot;type&quot;) var type: FollowEntityType,
)
Since it is a custom data type, I have defined a type converter.
class FollowDatabaseTypeConverter {
    @TypeConverter
    fun toFollowEntity(entityType: String?): FollowEntityType? {
        return FollowEntityType.from(entityType ?: Constants.EMPTY_STRING)
    }
    @TypeConverter
    fun toString(entityType: FollowEntityType?): String? {
        return entityType?.name
    }
}
This works fine and I am able to store/retrieve values in the DB. However, in one of the queries, the build fails.
This is the query.
@Query(&quot;select * from follow_info where type in (:entityTypeList)&quot;)
fun getFollowedEntitiesByType(entityTypeList: List&lt;FollowEntityType&gt;) : List&lt;FollowInfoEntity&gt;
The build fails with the following error.
Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type. You can consider adding a Type Adapter for this.
    java.util.List&lt;? extends FollowEntityType&gt; entityTypeList, @org.jetbrains.annotations.NotNull()
The error is for entityTypeList field, and according to the error, this type should be one of the columns in the DB. I already have FollowEntityType as one of the column types. I don't understand why it is failing. Please help me out as I am not finding any solution to solve this problem.
",<android><sqlite><kotlin><android-room>,1662,0,22,7198,3,38,62,75,15646,0.0,359,7,19,2018-12-21 13:39,2021-06-20 5:31,,912.0,,Basic,7,"<android><sqlite><kotlin><android-room>, Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type, I have a DB which has a custom data type FollowEntityType as a column.
@Entity(primaryKeys = arrayOf(&quot;id&quot;, &quot;type&quot;), tableName = &quot;follow_info&quot;)
data class FollowInfoEntity(
        @ColumnInfo(name = &quot;id&quot;) var id: String,
        @ColumnInfo(name = &quot;type&quot;) var type: FollowEntityType,
)
Since it is a custom data type, I have defined a type converter.
class FollowDatabaseTypeConverter {
    @TypeConverter
    fun toFollowEntity(entityType: String?): FollowEntityType? {
        return FollowEntityType.from(entityType ?: Constants.EMPTY_STRING)
    }
    @TypeConverter
    fun toString(entityType: FollowEntityType?): String? {
        return entityType?.name
    }
}
This works fine and I am able to store/retrieve values in the DB. However, in one of the queries, the build fails.
This is the query.
@Query(&quot;select * from follow_info where type in (:entityTypeList)&quot;)
fun getFollowedEntitiesByType(entityTypeList: List&lt;FollowEntityType&gt;) : List&lt;FollowInfoEntity&gt;
The build fails with the following error.
Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type. You can consider adding a Type Adapter for this.
    java.util.List&lt;? extends FollowEntityType&gt; entityTypeList, @org.jetbrains.annotations.NotNull()
The error is for entityTypeList field, and according to the error, this type should be one of the columns in the DB. I already have FollowEntityType as one of the column types. I don't understand why it is failing. Please help me out as I am not finding any solution to solve this problem.
","<andros><quite><olin><andros-room>, query method parapet either type convert database column list / array contain type, do custom data type followentitytyp column. @entity(primarykey = array(&quit;id&quit;, &quit;type&quit;), tablenam = &quit;follow_info&quit;) data class followinfoentity( @columninfo(am = &quit;id&quit;) war id: string, @columninfo(am = &quit;type&quit;) war type: followentitytype, ) since custom data type, define type converted. class followdatabasetypeconvert { @typeconvert fun tofollowentity(entitytype: string?): followentitytype? { return followentitytype.from(entitytyp ?: constant.empty_string) } @typeconvert fun string(entitytype: followentitytype?): string? { return entitytype?.am } } work fine all store/retrieve value do. however, one queried, build fails. query. @query(&quit;select * follow_info type (:entitytypelist)&quit;) fun getfollowedentitiesbytype(entitytypelist: list&it;followentitytype&it;) : list&it;followinfoentity&it; build fail follow error. query method parapet either type convert database column list / array contain type. consider ad type adapt this. cava.until.list&it;? extend followentitytype&it; entitytypelist, @org.jetbrains.innovations.notnull() error entitytypelist field, accord error, type one column do. already followentitytyp one column types. understand failing. pleas help find slut sole problem."
57008781,How to execute large amount of sql queries asynchronous and in threads,"Problem: I have huge amount of sql queries (around 10k-20k) and I want to run them asynchronous in 50 (or more) threads. 
I wrote a powershell script for this job, but it is very slow (It took about 20 hours to execute all). Desired result is 3-4 hours max.
Question: How can I optimize this powershell script? Should I reconsider and use another technology like python or c#?
I think it's powershell issue, because when I check with whoisactive the queries are executing fast. Creating, exiting and unloading jobs takes a lot of time, because for each thread is created separate PS instances.
My code:
$NumberOfParallerThreads = 50;
$Arr_AllQueries = @('Exec [mystoredproc] @param1=1, @param2=2',
                    'Exec [mystoredproc] @param1=11, @param2=22',
                    'Exec [mystoredproc] @param1=111, @param2=222')
#Creating the batches
$counter = [pscustomobject] @{ Value = 0 };
$Batches_AllQueries = $Arr_AllQueries | Group-Object -Property { 
    [math]::Floor($counter.Value++ / $NumberOfParallerThreads) 
};
forEach ($item in $Batches_AllQueries) {
    $tmpBatch = $item.Group;
    $tmpBatch | % {
        $ScriptBlock = {
            # accept the loop variable across the job-context barrier
            param($query) 
            # Execute a command
            Try 
            {
                Write-Host ""[processing '$query']""
                $objConnection = New-Object System.Data.SqlClient.SqlConnection;
                $objConnection.ConnectionString = 'Data Source=...';
                $ObjCmd = New-Object System.Data.SqlClient.SqlCommand;
                $ObjCmd.CommandText = $query;
                $ObjCmd.Connection = $objConnection;
                $ObjCmd.CommandTimeout = 0;
                $objAdapter = New-Object System.Data.SqlClient.SqlDataAdapter;
                $objAdapter.SelectCommand = $ObjCmd;
                $objDataTable = New-Object System.Data.DataTable;
                $objAdapter.Fill($objDataTable)  | Out-Null;
                $objConnection.Close();
                $objConnection = $null;
            } 
            Catch 
            { 
                $ErrorMessage = $_.Exception.Message
                $FailedItem = $_.Exception.ItemName
                Write-Host ""[Error processing: $($query)]"" -BackgroundColor Red;
                Write-Host $ErrorMessage 
            }
        }
        # pass the loop variable across the job-context barrier
        Start-Job $ScriptBlock -ArgumentList $_ | Out-Null
    }
    # Wait for all to complete
    While (Get-Job -State ""Running"") { Start-Sleep 2 }
    # Display output from all jobs
    Get-Job | Receive-Job | Out-Null
    # Cleanup
    Remove-Job *
}
UPDATE:
Resources: The DB server is on a remote machine with: 
24GB RAM, 
8 cores, 
500GB Storage, 
SQL Server 2016
We want to use the maximum cpu power.
Framework limitation: The only limitation is not to use SQL Server to execute the queries. The requests should come from outside source like: Powershell, C#, Python, etc. 
",<sql><multithreading><powershell><asynchronous><parallel-processing>,3007,0,69,1731,2,27,42,46,5273,0.0,1616,6,19,2019-07-12 14:14,2019-07-13 7:37,2019-07-25 13:37,1.0,13.0,Intermediate,23,"<sql><multithreading><powershell><asynchronous><parallel-processing>, How to execute large amount of sql queries asynchronous and in threads, Problem: I have huge amount of sql queries (around 10k-20k) and I want to run them asynchronous in 50 (or more) threads. 
I wrote a powershell script for this job, but it is very slow (It took about 20 hours to execute all). Desired result is 3-4 hours max.
Question: How can I optimize this powershell script? Should I reconsider and use another technology like python or c#?
I think it's powershell issue, because when I check with whoisactive the queries are executing fast. Creating, exiting and unloading jobs takes a lot of time, because for each thread is created separate PS instances.
My code:
$NumberOfParallerThreads = 50;
$Arr_AllQueries = @('Exec [mystoredproc] @param1=1, @param2=2',
                    'Exec [mystoredproc] @param1=11, @param2=22',
                    'Exec [mystoredproc] @param1=111, @param2=222')
#Creating the batches
$counter = [pscustomobject] @{ Value = 0 };
$Batches_AllQueries = $Arr_AllQueries | Group-Object -Property { 
    [math]::Floor($counter.Value++ / $NumberOfParallerThreads) 
};
forEach ($item in $Batches_AllQueries) {
    $tmpBatch = $item.Group;
    $tmpBatch | % {
        $ScriptBlock = {
            # accept the loop variable across the job-context barrier
            param($query) 
            # Execute a command
            Try 
            {
                Write-Host ""[processing '$query']""
                $objConnection = New-Object System.Data.SqlClient.SqlConnection;
                $objConnection.ConnectionString = 'Data Source=...';
                $ObjCmd = New-Object System.Data.SqlClient.SqlCommand;
                $ObjCmd.CommandText = $query;
                $ObjCmd.Connection = $objConnection;
                $ObjCmd.CommandTimeout = 0;
                $objAdapter = New-Object System.Data.SqlClient.SqlDataAdapter;
                $objAdapter.SelectCommand = $ObjCmd;
                $objDataTable = New-Object System.Data.DataTable;
                $objAdapter.Fill($objDataTable)  | Out-Null;
                $objConnection.Close();
                $objConnection = $null;
            } 
            Catch 
            { 
                $ErrorMessage = $_.Exception.Message
                $FailedItem = $_.Exception.ItemName
                Write-Host ""[Error processing: $($query)]"" -BackgroundColor Red;
                Write-Host $ErrorMessage 
            }
        }
        # pass the loop variable across the job-context barrier
        Start-Job $ScriptBlock -ArgumentList $_ | Out-Null
    }
    # Wait for all to complete
    While (Get-Job -State ""Running"") { Start-Sleep 2 }
    # Display output from all jobs
    Get-Job | Receive-Job | Out-Null
    # Cleanup
    Remove-Job *
}
UPDATE:
Resources: The DB server is on a remote machine with: 
24GB RAM, 
8 cores, 
500GB Storage, 
SQL Server 2016
We want to use the maximum cpu power.
Framework limitation: The only limitation is not to use SQL Server to execute the queries. The requests should come from outside source like: Powershell, C#, Python, etc. 
","<sal><multithreading><powershell><asynchronous><parallel-processing>, execute large amount sal query asynchron threads, problem: huge amount sal query (around ask-ask) want run asynchron 50 (or more) threads. wrote powershel script job, slow (it took 20 hour execute all). desire result 3-4 hour max. question: optic powershel script? reconsider use not technology like patron c#? think powershel issue, check whoisact query execute fast. creating, exit unload job take lot time, thread great spear is instances. code: $numberofparallerthread = 50; $arr_allqueri = @('even [mystoredproc] @panama=1, @panama=2', 'even [mystoredproc] @panama=11, @panama=22', 'even [mystoredproc] @panama=111, @panama=222') #great batch $counter = [pscustomobject] @{ value = 0 }; $batches_allqueri = $arr_allqueri | group-object -property { [path]::floor($counter.value++ / $numberofparallerthreads) }; french ($item $batches_allqueries) { $tmpbatch = $item.group; $tmpbatch | % { $scriptblock = { # accept loop variable across job-context barrier parma($query) # execute command try { write-host ""[process '$query']"" $objconnect = new-object system.data.sqlclient.sqlconnection; $objconnection.connections = 'data source=...'; $objcmd = new-object system.data.sqlclient.sqlcommand; $objcmd.commandtext = $query; $objcmd.connect = $objconnection; $objcmd.commandtimeout = 0; $objadapt = new-object system.data.sqlclient.sqldataadapter; $objadapter.selectcommand = $objcmd; $objdatat = new-object system.data.datatable; $objadapter.fill($objdatatable) | out-null; $objconnection.close(); $objconnect = $null; } catch { $errormessag = $_.exception.message $faileditem = $_.exception.itemnam write-host ""[error processing: $($query)]"" -backgroundcolor red; write-host $errormessag } } # pass loop variable across job-context barrier start-job $scriptblock -argumentlist $_ | out-null } # wait complete (get-job -state ""running"") { start-sleep 2 } # display output job get-job | receive-job | out-null # clean remove-job * } update: resources: do server remote machine with: 24gb ram, 8 comes, 500gb storage, sal server 2016 want use maximum cup power. framework limitation: limit use sal server execute queried. request come outside source like: powershell, c#, patron, etc."
57193597,Mocking a Sqlalchemy session for pytest,"I don't know if this can be done but I'm trying to mock my db.session.save.
I'm using flask and flask-alchemy.
db.py
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
The unit test
def test_post(self):
    with app.app_context():
        with app.test_client() as client:
            with mock.patch('models.db.session.save') as mock_save:
                with mock.patch('models.db.session.commit') as mock_commit:
                    data = self.gen_legend_data()
                    response = client.post('/legends', data=json.dumps([data]), headers=access_header)
                    assert response.status_code == 200
                    mock_save.assert_called()
                    mock_commit.assert_called_once()
And the method:
def post(cls):
    legends = schemas.Legends(many=True).load(request.get_json())
    for legend in legends:
        db.session.add(legend)
    db.session.commit()
    return {'message': 'legends saved'}, 200
I'm trying to mock the db.session.add and db.session.commit. I've tried db.session.save and legends.models.db.session.save and models.db.session.save. They all came back with the save error:
ModuleNotFoundError: No module named 'models.db.session'; 'models.db' is not a package
I don't get the error and I'm not sure how to solve it.
Or am I doing something that is totally wrong in wanting to mock a db.session?
Thanks.
Desmond
",<python><sqlalchemy><mocking><flask-sqlalchemy><pytest>,1384,0,28,755,2,7,26,76,32620,0.0,8,1,19,2019-07-25 2:44,2019-07-25 14:19,2019-07-25 14:19,0.0,0.0,Intermediate,30,"<python><sqlalchemy><mocking><flask-sqlalchemy><pytest>, Mocking a Sqlalchemy session for pytest, I don't know if this can be done but I'm trying to mock my db.session.save.
I'm using flask and flask-alchemy.
db.py
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
The unit test
def test_post(self):
    with app.app_context():
        with app.test_client() as client:
            with mock.patch('models.db.session.save') as mock_save:
                with mock.patch('models.db.session.commit') as mock_commit:
                    data = self.gen_legend_data()
                    response = client.post('/legends', data=json.dumps([data]), headers=access_header)
                    assert response.status_code == 200
                    mock_save.assert_called()
                    mock_commit.assert_called_once()
And the method:
def post(cls):
    legends = schemas.Legends(many=True).load(request.get_json())
    for legend in legends:
        db.session.add(legend)
    db.session.commit()
    return {'message': 'legends saved'}, 200
I'm trying to mock the db.session.add and db.session.commit. I've tried db.session.save and legends.models.db.session.save and models.db.session.save. They all came back with the save error:
ModuleNotFoundError: No module named 'models.db.session'; 'models.db' is not a package
I don't get the error and I'm not sure how to solve it.
Or am I doing something that is totally wrong in wanting to mock a db.session?
Thanks.
Desmond
","<patron><sqlalchemy><mocking><flask-sqlalchemy><test>, mock sqlalchemi session test, know done i'm try mock do.session.save. i'm use flask flask-archery. do.i flask_sqlalchemi import sqlalchemi do = sqlalchemy() unit test def test_post(self): pp.app_context(): pp.test_client() client: mock.patch('models.do.session.save') mock_save: mock.patch('models.do.session.commit') mock_commit: data = self.gen_legend_data() response = client.post('/legends', data=son.dumps([data]), leaders=access_header) assert response.status_cod == 200 mock_save.assert_called() mock_commit.assert_called_once() method: def post(cos): legend = schemes.legends(many=true).load(request.get_json()) legend legends: do.session.add(legend) do.session.commit() return {'message': 'legend saved'}, 200 i'm try mock do.session.add do.session.commit. i'v try do.session.say legends.models.do.session.say models.do.session.save. came back save error: modulenotfounderror: model name 'models.do.session'; 'models.do' package get error i'm sure sole it. cometh total wrong want mock do.session? thanks. demand"
49890142,Is it possible to change the background color of the Object Explorer and the Result Menu in SQL Server Management Studio 2017,"Pretty much what the title says. Just started learning SQL server. I found how to 'unlock' the dark theme and how to change the fonts and sizes but i still can't change the color in the Object Explorer and the Result Menu in SQL Server Management Studio 2017. Is it possible that this is all we got till now... Cause it is hard for me to believe that there is no working full dark theme for SQL Server Management Studio 2017 or the possibility to change it manualy. Any help would be much appreciated. Also is there another IDE or editor for SQL server 2017 other than SQL Server Management Studio 2017, like there are a LOT diffirent editors for programing languages like Sublime, NotePad++, VS Code etc...
",<sql-server>,708,0,0,973,4,15,33,44,20862,,65,3,19,2018-04-18 2:16,2020-10-12 8:56,2020-10-12 8:56,908.0,908.0,Basic,14,"<sql-server>, Is it possible to change the background color of the Object Explorer and the Result Menu in SQL Server Management Studio 2017, Pretty much what the title says. Just started learning SQL server. I found how to 'unlock' the dark theme and how to change the fonts and sizes but i still can't change the color in the Object Explorer and the Result Menu in SQL Server Management Studio 2017. Is it possible that this is all we got till now... Cause it is hard for me to believe that there is no working full dark theme for SQL Server Management Studio 2017 or the possibility to change it manualy. Any help would be much appreciated. Also is there another IDE or editor for SQL server 2017 other than SQL Server Management Studio 2017, like there are a LOT diffirent editors for programing languages like Sublime, NotePad++, VS Code etc...
","<sal-server>, possible change background color object explore result menu sal server manage studio 2017, pretty much till says. start learn sal server. found 'clock' dark theme change font size still can't change color object explore result menu sal server manage studio 2017. possible got till now... cause hard believe work full dark theme sal server manage studio 2017 possible change manual. help would much appreciated. also not side editor sal server 2017 sal server manage studio 2017, like lot differ editor program language like sublime, noted++, vs code etc..."
50643007,What is the difference between fetch Next and fetch First in the Order By [...] OFFSET [..] FETCH [...] clause?,"I executed two sample queries on AdventureWorks2016 and gave me the same result.
When should I use NEXT or FIRST keyword then?
select LastName + ' ' + FirstName 
from person.person
order by LastName asc OFFSET 10 rows **Fetch next** 10 rows only
select LastName + ' ' + FirstName
from person.person
order by LastName asc OFFSET 10 rows **Fetch first** 10 rows only
",<sql><sql-server>,365,0,7,973,3,12,26,59,10751,,163,2,19,2018-06-01 12:02,2018-06-01 12:12,2018-06-01 12:12,0.0,0.0,Basic,10,"<sql><sql-server>, What is the difference between fetch Next and fetch First in the Order By [...] OFFSET [..] FETCH [...] clause?, I executed two sample queries on AdventureWorks2016 and gave me the same result.
When should I use NEXT or FIRST keyword then?
select LastName + ' ' + FirstName 
from person.person
order by LastName asc OFFSET 10 rows **Fetch next** 10 rows only
select LastName + ' ' + FirstName
from person.person
order by LastName asc OFFSET 10 rows **Fetch first** 10 rows only
","<sal><sal-server>, differ fetch next fetch first order [...] offset [..] fetch [...] clause?, execute two sample query adventureworks2016 gave result. use next first eyford then? select lastnam + ' ' + firstnam person.person order lastnam as offset 10 row **fetch next** 10 row select lastnam + ' ' + firstnam person.person order lastnam as offset 10 row **fetch first** 10 row"
49198831,sqlite3.dylib: illegal multi-threaded access to database connection,"I have an iOS app that uses sqlite3 and I'm facing issues with multi-threading crashing the app with the illegal multi-threaded access to database connection message. Of course, it's because I'm using multi-threading; the problem is, my sqlite3 instance is configured to use multi-thread:
sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
Even though I'm using multi-threading (sqlite3 build was also compiled with the multi-threading flag), it causes my app to crash when multiple threads write or read the database simultaneously.
Crash report
Application Specific Information:
BUG IN CLIENT OF sqlite3.dylib: illegal multi-threaded access to database connection
Exception Type:  EXC_BREAKPOINT (SIGTRAP)
Exception Codes: 0x0000000000000001, 0x00000001823ed2fc
Termination Signal: Trace/BPT trap: 5
Termination Reason: Namespace SIGNAL, Code 0x5
Terminating Process: exc handler [0]
Triggered by Thread:  12
Thread 12 Crashed:
0   libsqlite3.dylib                0x00000001823ed2fc sqlite3MutexMisuseAssert + 144 (sqlite3.c:23788)
1   libsqlite3.dylib                0x00000001823ed2ec sqlite3MutexMisuseAssert + 128 (once.h:84)
2   libsqlite3.dylib                0x000000018235248c sqlite3LockAndPrepare + 320 (sqlite3.c:23801)
3   MyCodeCall.m ...........
I've been struggling with this issue for a while and I couldn't find any reference to this on google unfortunately.
UPDATE
+(sqlite3*) getInstance {
  if (instance == NULL) {
    sqlite3_shutdown();
    sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
    sqlite3_initialize();
    NSLog(@&quot;isThreadSafe %d&quot;, sqlite3_threadsafe());
    const char *path = [@&quot;./path/to/db/db.sqlite&quot; cStringUsingEncoding:NSUTF8StringEncoding];
    if (sqlite3_open_v2(path, &amp;database, SQLITE_OPEN_READWRITE|SQLITE_OPEN_CREATE, NULL) != SQLITE_OK) {
      NSLog(@&quot;Database opening failed!&quot;);
    }
  }
  return instance;
}
",<ios><sqlite>,1885,0,36,9650,3,25,40,79,10254,0.0,79,4,19,2018-03-09 17:11,2018-03-12 12:08,2018-03-12 12:08,3.0,3.0,Intermediate,23,"<ios><sqlite>, sqlite3.dylib: illegal multi-threaded access to database connection, I have an iOS app that uses sqlite3 and I'm facing issues with multi-threading crashing the app with the illegal multi-threaded access to database connection message. Of course, it's because I'm using multi-threading; the problem is, my sqlite3 instance is configured to use multi-thread:
sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
Even though I'm using multi-threading (sqlite3 build was also compiled with the multi-threading flag), it causes my app to crash when multiple threads write or read the database simultaneously.
Crash report
Application Specific Information:
BUG IN CLIENT OF sqlite3.dylib: illegal multi-threaded access to database connection
Exception Type:  EXC_BREAKPOINT (SIGTRAP)
Exception Codes: 0x0000000000000001, 0x00000001823ed2fc
Termination Signal: Trace/BPT trap: 5
Termination Reason: Namespace SIGNAL, Code 0x5
Terminating Process: exc handler [0]
Triggered by Thread:  12
Thread 12 Crashed:
0   libsqlite3.dylib                0x00000001823ed2fc sqlite3MutexMisuseAssert + 144 (sqlite3.c:23788)
1   libsqlite3.dylib                0x00000001823ed2ec sqlite3MutexMisuseAssert + 128 (once.h:84)
2   libsqlite3.dylib                0x000000018235248c sqlite3LockAndPrepare + 320 (sqlite3.c:23801)
3   MyCodeCall.m ...........
I've been struggling with this issue for a while and I couldn't find any reference to this on google unfortunately.
UPDATE
+(sqlite3*) getInstance {
  if (instance == NULL) {
    sqlite3_shutdown();
    sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
    sqlite3_initialize();
    NSLog(@&quot;isThreadSafe %d&quot;, sqlite3_threadsafe());
    const char *path = [@&quot;./path/to/db/db.sqlite&quot; cStringUsingEncoding:NSUTF8StringEncoding];
    if (sqlite3_open_v2(path, &amp;database, SQLITE_OPEN_READWRITE|SQLITE_OPEN_CREATE, NULL) != SQLITE_OK) {
      NSLog(@&quot;Database opening failed!&quot;);
    }
  }
  return instance;
}
","<is><quite>, sqlite3.dynia: killed multi-thread access database connection, to pp use sqlite3 i'm face issue multi-thread crash pp killed multi-thread access database connect message. course, i'm use multi-treading; problem is, sqlite3 instant configur use multi-thread: sqlite3_config(sqlite_config_multithread); even though i'm use multi-thread (sqlite3 build also compel multi-thread flag), cause pp crash multiple thread write read database simultaneously. crash report applied specie information: bug client sqlite3.dynia: killed multi-thread access database connect except type: exc_breakpoint (strap) except codes: 0x0000000000000001, 0x00000001823ed2fc german signal: trace/but trap: 5 german reason: namespac signal, code x german process: etc handle [0] trigger thread: 12 thread 12 crashed: 0 libsqlite3.dynia 0x00000001823ed2fc sqlite3mutexmisuseassert + 144 (sqlite3.c:23788) 1 libsqlite3.dynia 0x00000001823ed2ec sqlite3mutexmisuseassert + 128 (once.h:84) 2 libsqlite3.dynia 0x000000018235248c sqlite3lockandprepar + 320 (sqlite3.c:23801) 3 mycodecall.m ........... i'v struggle issue find refer good unfortunately. update +(sqlite3*) getinst { (instant == null) { sqlite3_shutdown(); sqlite3_config(sqlite_config_multithread); sqlite3_initialize(); slow(@&quit;isthreadsaf %d&quit;, sqlite3_threadsafe()); cost chair *path = [@&quit;./path/to/do/do.quite&quit; cstringusingencoding:nsutf8stringencoding]; (sqlite3_open_v2(path, &amp;database, sqlite_open_readwrite|sqlite_open_create, null) != sqlite_ok) { slow(@&quit;database open failed!&quit;); } } return instance; }"
48112363,Rails migration - add unique index allowing skipping null values,"I wanted like to add unique constraint to column name in existing table psql
created table (PostgreSQL) :
class CreateRequests &lt; ActiveRecord::Migration
  def change
    create_table :requests do |t|
      t.integer :user_id
      t.string :name
      t.string :address
      t.timestamps null: true
    end
    add_index :requests, :user_id
  end
end
So I added validation uniqueness in model
class Request &lt; ModelBase
  belongs_to :user
  validates :user, presence: true
  validates :name, uniqueness: true, allow_blank: true
  ...
and migration like this:
def change
    add_index :user_requests, :name, unique: true
end
but I noticed that in some cases name can be empty can I add_index with condition were :name is null / not null?
Edit: yes, I want those empty values to stay in the database (I don't need to modify past request). I think that I need to edit migration to be more accurate to the actual state of db.
",<ruby-on-rails><psql>,928,0,21,1456,1,15,29,57,9766,0.0,70,5,19,2018-01-05 11:09,2018-01-05 11:15,2018-01-08 9:23,0.0,3.0,Intermediate,23,"<ruby-on-rails><psql>, Rails migration - add unique index allowing skipping null values, I wanted like to add unique constraint to column name in existing table psql
created table (PostgreSQL) :
class CreateRequests &lt; ActiveRecord::Migration
  def change
    create_table :requests do |t|
      t.integer :user_id
      t.string :name
      t.string :address
      t.timestamps null: true
    end
    add_index :requests, :user_id
  end
end
So I added validation uniqueness in model
class Request &lt; ModelBase
  belongs_to :user
  validates :user, presence: true
  validates :name, uniqueness: true, allow_blank: true
  ...
and migration like this:
def change
    add_index :user_requests, :name, unique: true
end
but I noticed that in some cases name can be empty can I add_index with condition were :name is null / not null?
Edit: yes, I want those empty values to stay in the database (I don't need to modify past request). I think that I need to edit migration to be more accurate to the actual state of db.
","<ruby-on-rails><pool>, rail migrate - add unique index allow skin null values, want like add unique constraint column name exist table pool great table (postgresql) : class createrequest &it; activerecord::might def change greatest :request |t| t.inter :user_id t.string :name t.string :address t.timestamp null: true end add_index :requests, :user_id end end ad valid unique model class request &it; models belongs_to :user valid :user, presence: true valid :name, uniqueness: true, allow_blank: true ... migrate like this: def change add_index :user_requests, :name, unique: true end notice case name empty add_index conduct :name null / null? edit: yes, want empty value stay database (i need modify past request). think need edit migrate occur actual state do."
54300263,Connect to AWS RDS Postgres database with python,"I have an existing postgres table in RDS with a database name my-rds-table-name
I've connected to it using pgAdmin4 with the following configs of a read-only user:
host_name = ""my-rds-table-name.123456.us-east-1.rds.amazonaws.com""
user_name = ""my_user_name""
password = ""abc123def345""
I have verified that I can query against the table.
However, I cannot connect to it using python:
SQLAlchemy==1.2.16
psycopg2-binary==2.7.6.1
mysqlclient==1.4.1
With:
import psycopg2
engine = psycopg2.connect(
    database=""my-rds-table-name"",
    user=""my_user_name"",
    password=""abc123def345"",
    host=""my-rds-table-name.123456.us-east-1.rds.amazonaws.com"",
    port='5432'
)
It fails with 
psycopg2.OperationalError: FATAL:  database ""my-rds-table-name"" does not exist
Similarly, if I try to connect to it with sqlalchemy:
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL:  database ""my-rds-table-name"" does not exist
What am I missing?
",<python-3.x><postgresql><amazon-web-services><amazon-rds>,945,0,17,8955,10,64,103,38,36512,0.0,2868,2,19,2019-01-22 2:06,2019-01-22 17:54,2019-01-22 17:54,0.0,0.0,Basic,3,"<python-3.x><postgresql><amazon-web-services><amazon-rds>, Connect to AWS RDS Postgres database with python, I have an existing postgres table in RDS with a database name my-rds-table-name
I've connected to it using pgAdmin4 with the following configs of a read-only user:
host_name = ""my-rds-table-name.123456.us-east-1.rds.amazonaws.com""
user_name = ""my_user_name""
password = ""abc123def345""
I have verified that I can query against the table.
However, I cannot connect to it using python:
SQLAlchemy==1.2.16
psycopg2-binary==2.7.6.1
mysqlclient==1.4.1
With:
import psycopg2
engine = psycopg2.connect(
    database=""my-rds-table-name"",
    user=""my_user_name"",
    password=""abc123def345"",
    host=""my-rds-table-name.123456.us-east-1.rds.amazonaws.com"",
    port='5432'
)
It fails with 
psycopg2.OperationalError: FATAL:  database ""my-rds-table-name"" does not exist
Similarly, if I try to connect to it with sqlalchemy:
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL:  database ""my-rds-table-name"" does not exist
What am I missing?
","<patron-3.x><postgresql><amazon-web-services><amazon-rd>, connect a rd poster database patron, exist poster table rd database name my-rd-table-am i'v connect use pgadmin4 follow confirm read-only user: host_nam = ""my-rd-table-name.123456.us-east-1.rd.amazonaws.com"" user_nam = ""my_user_name"" password = ""abc123def345"" verify query table. however, cannot connect use patron: sqlalchemy==1.2.16 psycopg2-binary==2.7.6.1 mysqlclient==1.4.1 with: import psycopg2 engine = psycopg2.connect( database=""my-rd-table-name"", user=""my_user_name"", password=""abc123def345"", host=""my-rd-table-name.123456.us-east-1.rd.amazonaws.com"", port='5432' ) fail psycopg2.operationalerror: fatal: database ""my-rd-table-name"" exist similarly, try connect sqlalchemy: sqlalchemy.etc.operationalerror: (psycopg2.operationalerror) fatal: database ""my-rd-table-name"" exist missing?"
49361286,Unittesting with Pyspark: unclosed socket warnings,"I want to do unittesting with PySpark. The tests itself work, however for each test I get
ResourceWarning: unclosed &lt;socket.socket [...]&gt; and
ResourceWarning: unclosed file &lt;_io.BufferedWriter [...]&gt; warnings and
DeprecationWarnings regarding invalid escape sequences. 
I'd like to understand why / how to solve this to not clutter my unittest output with these warnings.
Here is a MWE:
# filename: pyspark_unittesting.py
# -*- coding: utf-8 -*-
import unittest
def insert_and_collect(val_in):
    from pyspark.sql import SparkSession
    with SparkSession.builder.getOrCreate() as spark:
        col = 'column_x'
        df = spark.createDataFrame([(val_in,)], [col])
        print('one')
        print(df.count())
        print('two')
        collected = df.collect()
        print('three')
        return collected[0][col]
class MyTest(unittest.TestCase):
    def test(self):
        val = 1
        self.assertEqual(insert_and_collect(val), val)
        print('four')
if __name__ == '__main__':
    val = 1
    print('inserted and collected is equal to original: {}'
          .format(insert_and_collect(val) == val))
    print('five')
If I call this with python pyspark_unittesting.py the output is:
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
one
1  
two
three
inserted and collected is equal to original: True
five
If I call this with python -m unittest pyspark_unittesting however, the output is:
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
/usr/lib/python3.6/subprocess.py:766: ResourceWarning: subprocess 10219 is still running
  ResourceWarning, source=self)
/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
one
1                                                                               
two
/usr/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49330), raddr=('127.0.0.1', 44169)&gt;
  self._sock = None
three
four
.
----------------------------------------------------------------------
Ran 1 test in 7.394s
OK
sys:1: ResourceWarning: unclosed file &lt;_io.BufferedWriter name=5&gt;
Edit 2018-03-29
Regarding @acue's answer, I tried out calling the script using subprocess.Popen - very much like it is done within the unittest module:
In [1]: import pathlib
      : import subprocess
      : import sys
      : 
      : here = pathlib.Path('.').absolute()
      : args = [sys.executable, str(here / 'pyspark_unittesting.py')]
      : opts = dict(stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd='/tmp')
      : p = subprocess.Popen(args, **opts)
      : out, err = [b.splitlines() for b in p.communicate()]
      : print(out)
      : print(err)
      : 
      : 
[b'one',
 b'1',
 b'two',
 b'three',
 b'inserted and collected is equal to original: True',
 b'five']
[b""Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties"",
 b'Setting default log level to ""WARN"".',
 b'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',
 b'',
 b'[Stage 0:&gt;                                                          (0 + 0) / 8]',
 b'[Stage 0:&gt;                                                          (0 + 8) / 8]',
 b'                                                                                ']
The Resource Warnings do not appear...
",<python><python-3.x><pyspark><python-unittest><apache-spark-sql>,5017,0,106,1671,3,19,34,80,3559,0.0,100,1,19,2018-03-19 10:57,2020-12-13 11:38,,1000.0,,Intermediate,30,"<python><python-3.x><pyspark><python-unittest><apache-spark-sql>, Unittesting with Pyspark: unclosed socket warnings, I want to do unittesting with PySpark. The tests itself work, however for each test I get
ResourceWarning: unclosed &lt;socket.socket [...]&gt; and
ResourceWarning: unclosed file &lt;_io.BufferedWriter [...]&gt; warnings and
DeprecationWarnings regarding invalid escape sequences. 
I'd like to understand why / how to solve this to not clutter my unittest output with these warnings.
Here is a MWE:
# filename: pyspark_unittesting.py
# -*- coding: utf-8 -*-
import unittest
def insert_and_collect(val_in):
    from pyspark.sql import SparkSession
    with SparkSession.builder.getOrCreate() as spark:
        col = 'column_x'
        df = spark.createDataFrame([(val_in,)], [col])
        print('one')
        print(df.count())
        print('two')
        collected = df.collect()
        print('three')
        return collected[0][col]
class MyTest(unittest.TestCase):
    def test(self):
        val = 1
        self.assertEqual(insert_and_collect(val), val)
        print('four')
if __name__ == '__main__':
    val = 1
    print('inserted and collected is equal to original: {}'
          .format(insert_and_collect(val) == val))
    print('five')
If I call this with python pyspark_unittesting.py the output is:
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
one
1  
two
three
inserted and collected is equal to original: True
five
If I call this with python -m unittest pyspark_unittesting however, the output is:
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
/usr/lib/python3.6/subprocess.py:766: ResourceWarning: subprocess 10219 is still running
  ResourceWarning, source=self)
/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
one
1                                                                               
two
/usr/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49330), raddr=('127.0.0.1', 44169)&gt;
  self._sock = None
three
four
.
----------------------------------------------------------------------
Ran 1 test in 7.394s
OK
sys:1: ResourceWarning: unclosed file &lt;_io.BufferedWriter name=5&gt;
Edit 2018-03-29
Regarding @acue's answer, I tried out calling the script using subprocess.Popen - very much like it is done within the unittest module:
In [1]: import pathlib
      : import subprocess
      : import sys
      : 
      : here = pathlib.Path('.').absolute()
      : args = [sys.executable, str(here / 'pyspark_unittesting.py')]
      : opts = dict(stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd='/tmp')
      : p = subprocess.Popen(args, **opts)
      : out, err = [b.splitlines() for b in p.communicate()]
      : print(out)
      : print(err)
      : 
      : 
[b'one',
 b'1',
 b'two',
 b'three',
 b'inserted and collected is equal to original: True',
 b'five']
[b""Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties"",
 b'Setting default log level to ""WARN"".',
 b'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',
 b'',
 b'[Stage 0:&gt;                                                          (0 + 0) / 8]',
 b'[Stage 0:&gt;                                                          (0 + 8) / 8]',
 b'                                                                                ']
The Resource Warnings do not appear...
","<patron><patron-3.x><spark><patron-unites><apache-spark-sal>, unites spark: uncle socket warnings, want unites spark. test work, howe test get resourcewarning: uncle &it;socket.socket [...]&it; resourcewarning: uncle file &it;rio.bufferedwrit [...]&it; warn deprecationwarn regard invalid escape sequence. i'd like understand / sole clatter unites output warnings. me: # filename: pyspark_unittesting.i # -*- coming: utf-8 -*- import unites def insert_and_collect(vain): spark.sal import sparkles sparksession.builder.getorcreate() spark: col = 'column' of = spark.createdataframe([(vain,)], [col]) print('one') print(of.count()) print('two') collect = of.collect() print('three') return collected[0][col] class test(unites.testcase): def test(self): val = 1 self.assertequal(insert_and_collect(val), val) print('four') __name__ == '__main__': val = 1 print('insert collect equal original: {}' .format(insert_and_collect(val) == val)) print('five') call patron pyspark_unittesting.i output is: use spark' default log profile: org/apache/spark/log-default.property set default log level ""warn"". adjust log level use s.setloglevel(newlevel). sparks, use setloglevel(newlevel). one 1 two three insert collect equal original: true five call patron -m unites pyspark_unittest however, output is: /opt/spark/current/patron/limb/py4j-0.10.4-sac.zip/py4j/java_gateway.by:1890: deprecationwarning: invalid escape sequence \* /opt/spark/current/patron/limb/py4j-0.10.4-sac.zip/py4j/java_gateway.by:1890: deprecationwarning: invalid escape sequence \* /opt/spark/current/patron/limb/spark.zip/spark/sal/readwriter.by:398: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/readwriter.by:759: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/readwriter.by:398: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/readwriter.by:759: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/streaming.by:618: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/streaming.by:618: deprecationwarning: invalid escape sequence \` /opt/spark/current/patron/limb/spark.zip/spark/sal/functions.by:1519: deprecationwarning: invalid escape sequence \d /opt/spark/current/patron/limb/spark.zip/spark/sal/functions.by:1519: deprecationwarning: invalid escape sequence \d use spark' default log profile: org/apache/spark/log-default.property set default log level ""warn"". adjust log level use s.setloglevel(newlevel). sparks, use setloglevel(newlevel). /us/limb/python3.6/subprocess.by:766: resourcewarning: subprocess 10219 still run resourcewarning, source=self) /us/limb/python3.6/importlib/_bootstrap.by:219: importwarning: can't resolve package __spec__ __package__, fall back __name__ __path__ return f(*arms, **was) one 1 two /us/limb/python3.6/socket.by:657: resourcewarning: uncle &it;socket.socket ff=7, family=addressfamily.af_inet, type=socketkind.sock_stream, proto=6, ladder=('127.0.0.1', 49330), add=('127.0.0.1', 44169)&it; self.sock = none three four . ---------------------------------------------------------------------- ran 1 test 7.394 ok says:1: resourcewarning: uncle file &it;rio.bufferedwrit name=5&it; edit 2018-03-29 regard @acute' answer, try call script use subprocess.open - much like done within unites module: [1]: import pathlib : import subprocess : import by : : = pathlib.path('.').absolute() : are = [says.excitable, sir(here / 'pyspark_unittesting.by')] : opt = duct(stout=subprocess.pipe, stern=subprocess.pipe, cod='/tm') : p = subprocess.open(arms, **oats) : out, err = [b.splitlines() b p.communicate()] : print(out) : print(err) : : [b'one', b'1', b'two', b'three', b'insert collect equal original: true', b'five'] [b""use spark' default log profile: org/apache/spark/log-default.properties"", b'set default log level ""warn"".', b'to adjust log level use s.setloglevel(newlevel). sparks, use setloglevel(newlevel).', b'', b'[stage 0:&it; (0 + 0) / 8]', b'[stage 0:&it; (0 + 8) / 8]', b' '] resource warn appear..."
55685341,"Django testing: Got an error creating the test database: database ""database_name"" already exists","I have a problem with testing. It's my first time writing tests and I have a problem.
I just created a test folder inside my app users, and test_urls.py for testing the urls.
When I type:
python manage.py test users
It says:
  Creating test database for alias 'default'... Got an error creating
  the test database: database ""database_name"" already exists
  Type 'yes' if you would like to try deleting the test database
  'database_name', or 'no' to cancel:
What does it mean? What happens if I type yes? Do I lose all my data in database?
",<python><django><database><postgresql>,541,0,3,400,1,8,17,37,19713,0.0,42,2,19,2019-04-15 8:37,2019-04-15 8:41,2019-04-15 8:41,0.0,0.0,Intermediate,30,"<python><django><database><postgresql>, Django testing: Got an error creating the test database: database ""database_name"" already exists, I have a problem with testing. It's my first time writing tests and I have a problem.
I just created a test folder inside my app users, and test_urls.py for testing the urls.
When I type:
python manage.py test users
It says:
  Creating test database for alias 'default'... Got an error creating
  the test database: database ""database_name"" already exists
  Type 'yes' if you would like to try deleting the test database
  'database_name', or 'no' to cancel:
What does it mean? What happens if I type yes? Do I lose all my data in database?
","<patron><django><database><postgresql>, django testing: got error great test database: database ""database_name"" already exists, problem testing. first time write test problem. great test older inside pp users, test_urls.i test curls. type: patron manage.i test user says: great test database asia 'default'... got error great test database: database ""database_name"" already exist type 'yes' would like try delete test database 'database_name', 'no' cancel: mean? happen type yes? lose data database?"
50450162,"PostgreSQL ""tuple already updated by self""","Our database seems to be broken, normally it uses about 1-2% of cpu, but if we run some additional backend services making UPDATE and INSERT queries for 10M rows table (about 1 query per 3 second) everything is going to hell (including CPU increase from 2% to 98% usage).
We have decided to debug what's going on, run VACUUM and ANALYZE to learn what's wrong with db but...
production=# ANALYZE VERBOSE users_user;
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
ERROR:  tuple already updated by self
we are not able to finish ANALYZE on ANY of the tables and could not find any information about this issue. Any suggestions what can be wrong?
 PostgreSQL 9.6.8 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16), 64-bit
Additional info as requested in comments:
  Maybe you have a corrupted pg_class
SELECT * FROM pg_class WHERE relname = 'users_user';
Output: https://pastebin.com/WhmkH34U
  So the first thing to do would be to kick out all other sessions and
  try again
There are no additional sessions, we have dumped the whole DB on the new testing server, issue still occur, there are no clients connected to this DB
",<postgresql><postgresql-9.6>,1462,2,8,925,1,9,12,55,3858,,12,1,19,2018-05-21 13:37,2018-05-29 17:16,,8.0,,Intermediate,23,"<postgresql><postgresql-9.6>, PostgreSQL ""tuple already updated by self"", Our database seems to be broken, normally it uses about 1-2% of cpu, but if we run some additional backend services making UPDATE and INSERT queries for 10M rows table (about 1 query per 3 second) everything is going to hell (including CPU increase from 2% to 98% usage).
We have decided to debug what's going on, run VACUUM and ANALYZE to learn what's wrong with db but...
production=# ANALYZE VERBOSE users_user;
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
ERROR:  tuple already updated by self
we are not able to finish ANALYZE on ANY of the tables and could not find any information about this issue. Any suggestions what can be wrong?
 PostgreSQL 9.6.8 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16), 64-bit
Additional info as requested in comments:
  Maybe you have a corrupted pg_class
SELECT * FROM pg_class WHERE relname = 'users_user';
Output: https://pastebin.com/WhmkH34U
  So the first thing to do would be to kick out all other sessions and
  try again
There are no additional sessions, we have dumped the whole DB on the new testing server, issue still occur, there are no clients connected to this DB
","<postgresql><postgresql-9.6>, postgresql ""up already update self"", database seem broken, normal use 1-2% cup, run admit backed service make update insert query him row table (about 1 query per 3 second) every go hell (include cup increase 2% 98% usage). decide debut what' go on, run vacuum analyze learn what' wrong do but... production=# analyze verbs users_user; into: analyze ""public.users_user"" into: ""users_user"": scan 280 280 pages, contain 23889 live row 57 dead rows; 23889 row sample, 23889 estime total row into: analyze ""public.users_user"" into: ""users_user"": scan 280 280 pages, contain 23889 live row 57 dead rows; 23889 row sample, 23889 estime total row error: up already update self all finish analyze table could find inform issue. suggest wrong? postgresql 9.6.8 x86_64-pp-line-gun, compel go (go) 4.8.5 20150623 (red hat 4.8.5-16), 64-bit admit into request comments: may corrupt pg_class select * pg_class realm = 'users_user'; output: http://gastein.com/whmkh34u first thing would kick session try admit sessions, dump whole do new test server, issue still occur, client connect do"
52344453,"""Loading class com.mysql.jdbc.Driver ... is deprecated"" message","I got an error
Loading class com.mysql.jdbc.Driver. This is deprecated. The new
driver class is com.mysql.cj.jdbc.Driver. The driver is
automatically registered via the SPI and manual loading of the driver
class is generally unnecessary.
Could someone explain why?
",<java><mysql><jdbc>,265,0,0,383,2,5,14,58,26754,0.0,15,2,19,2018-09-15 11:52,2018-09-15 12:43,2018-09-15 12:43,0.0,0.0,Advanced,38,"<java><mysql><jdbc>, ""Loading class com.mysql.jdbc.Driver ... is deprecated"" message, I got an error
Loading class com.mysql.jdbc.Driver. This is deprecated. The new
driver class is com.mysql.cj.jdbc.Driver. The driver is
automatically registered via the SPI and manual loading of the driver
class is generally unnecessary.
Could someone explain why?
","<cava><myself><job>, ""load class com.myself.job.drive ... deprecated"" message, got error load class com.myself.job.driver. deprecated. new driver class com.myself.c.job.driver. driver automatic resist via spy manual load driver class genet unnecessary. could someone explain why?"
50421670,Mysql ERROR : not connected,"I am trying to connect to MySQL database from MySQL shell on windows. 
No matter what I type in MySQL shell, it keeps giving me error : 'Not connected'. 
Query eg 1 : mysql --host=localhost --port=3306 --user=root -p;
Query eg 2 : mysql -u root -p
O/P : ERROR: Not connected
I have MySQL server installed on my machine. Also MySQL service is running in the background.
Also, I was able to connect from MySQL workbench. 
ERROR MESSAGE
MySQL Workbench Connection
",<mysql><sql><database><command-line><mysql-workbench>,461,2,3,827,1,6,11,63,72960,0.0,18,5,19,2018-05-19 4:07,2018-05-19 4:24,2018-05-19 6:31,0.0,0.0,Intermediate,30,"<mysql><sql><database><command-line><mysql-workbench>, Mysql ERROR : not connected, I am trying to connect to MySQL database from MySQL shell on windows. 
No matter what I type in MySQL shell, it keeps giving me error : 'Not connected'. 
Query eg 1 : mysql --host=localhost --port=3306 --user=root -p;
Query eg 2 : mysql -u root -p
O/P : ERROR: Not connected
I have MySQL server installed on my machine. Also MySQL service is running in the background.
Also, I was able to connect from MySQL workbench. 
ERROR MESSAGE
MySQL Workbench Connection
","<myself><sal><database><command-line><myself-workbench>, myself error : connected, try connect myself database myself shell windows. matter type myself shell, keep give error : 'not connected'. query eg 1 : myself --host=localhost --port=3306 --user=root -p; query eg 2 : myself -u root -p o/p : error: connect myself server instal machine. also myself service run background. also, all connect myself workbench. error message myself workbench connect"
48558005,PostgreSQL 9.6. Issue dropping index,"In a legacy PostgreSQL DB I tried to drop an existing index issuing the command:
DROP INDEX testing.idx_testing_data_model_output_data_id;
and see the error:
ERROR:  index ""&lt;index name&gt;"" does not exist
But I can see the index using the \d &lt;table name&gt; command:
leg=# \d testing.data_model
                                           Table ""testing.data_model""
     Column     |            Type             |                                 Modifiers
----------------+-----------------------------+---------------------------------------------------------------------------
 id             | bigint                      | not null default nextval('testing.data_model_id_seq'::regclass) 
 input_data     | text                        | 
 output_data_id | bigint                      | 
Indexes:
    ""pk_testing_data_model"" PRIMARY KEY, btree (id)
    ""idx_testing_data_model_output_data_id"" btree (output_data_id)
Ok, when I try to create the index I receive the following error:
ERROR:  relation ""&lt;index name&gt;"" already exists
It seems that somehow the index creation or index dropping was not successfully complete. How can I resolve this issue?
",<postgresql><ddl>,1162,0,14,737,1,11,31,59,9138,,339,1,19,2018-02-01 8:07,2022-03-17 12:06,,1505.0,,Advanced,32,"<postgresql><ddl>, PostgreSQL 9.6. Issue dropping index, In a legacy PostgreSQL DB I tried to drop an existing index issuing the command:
DROP INDEX testing.idx_testing_data_model_output_data_id;
and see the error:
ERROR:  index ""&lt;index name&gt;"" does not exist
But I can see the index using the \d &lt;table name&gt; command:
leg=# \d testing.data_model
                                           Table ""testing.data_model""
     Column     |            Type             |                                 Modifiers
----------------+-----------------------------+---------------------------------------------------------------------------
 id             | bigint                      | not null default nextval('testing.data_model_id_seq'::regclass) 
 input_data     | text                        | 
 output_data_id | bigint                      | 
Indexes:
    ""pk_testing_data_model"" PRIMARY KEY, btree (id)
    ""idx_testing_data_model_output_data_id"" btree (output_data_id)
Ok, when I try to create the index I receive the following error:
ERROR:  relation ""&lt;index name&gt;"" already exists
It seems that somehow the index creation or index dropping was not successfully complete. How can I resolve this issue?
","<postgresql><del>, postgresql 9.6. issue drop index, legacy postgresql do try drop exist index issue command: drop index testing.idx_testing_data_model_output_data_id; see error: error: index ""&it;index name&it;"" exist see index use \d &it;table name&it; command: leg=# \d testing.data_model table ""testing.data_model"" column | type | modify ----------------+-----------------------------+--------------------------------------------------------------------------- id | begin | null default neutral('testing.data_model_id_seq'::regulars) input_data | text | output_data_id | begin | indexes: ""pk_testing_data_model"" primary key, tree (id) ""idx_testing_data_model_output_data_id"" tree (output_data_id) ok, try great index receive follow error: error: relate ""&it;index name&it;"" already exist seem somehow index creation index drop success complete. resolve issue?"
51149902,Sql Table data type for email address?,"What data type should I use for an email? Just started to learn SQL, and I tried to make some columns, here's table for ID, Username, Password, Money, and Email.
Did I make that correctly?
",<sql><sql-server><database>,189,1,0,223,1,2,15,67,85221,,12,4,19,2018-07-03 8:25,2018-07-03 8:38,2018-07-03 9:41,0.0,0.0,Basic,4,"<sql><sql-server><database>, Sql Table data type for email address?, What data type should I use for an email? Just started to learn SQL, and I tried to make some columns, here's table for ID, Username, Password, Money, and Email.
Did I make that correctly?
","<sal><sal-server><database>, sal table data type email address?, data type use email? start learn sal, try make columns, here' table id, surname, password, money, email. make correctly?"
57978671,SSMS crashes when try to modify database diagram (v18.2),"When I try to modify a database diagram created before the application restart and crashes when trying to access.
It happen only when I save the diagram and close the application. When I try to reopen it throws me an error then restart the SSMS.
I'm running SQL Server 14.0.100 Express Edition.
I reviewed the Microsoft Event Viewer and I get this:
  Faulting application name: Ssms.exe, version: 2019.150.18142.0, time stamp: 0x5d3573be
  Faulting module name: DataDesigners.dll, version: 2019.150.18142.0, time stamp: 0x5d3573f0
  Exception code: 0xc0000005
  Fault offset: 0x00004be8
  Faulting process id: 0x5ec8
  Faulting application start time: 0x01d56d761e232f6c
  Faulting application path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Ssms.exe
  Faulting module path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Tools\VDT\DataDesigners.dll
  Report Id: e797c8be-6448-4547-9f6f-146cd92d8178
  Faulting package full name: 
  Faulting package-relative application ID: 
",<sql-server><database><ssms><diagram><designer>,1041,0,0,781,1,5,22,50,12240,0.0,223,2,19,2019-09-17 16:45,2019-09-20 1:02,2019-09-20 1:02,3.0,3.0,Advanced,32,"<sql-server><database><ssms><diagram><designer>, SSMS crashes when try to modify database diagram (v18.2), When I try to modify a database diagram created before the application restart and crashes when trying to access.
It happen only when I save the diagram and close the application. When I try to reopen it throws me an error then restart the SSMS.
I'm running SQL Server 14.0.100 Express Edition.
I reviewed the Microsoft Event Viewer and I get this:
  Faulting application name: Ssms.exe, version: 2019.150.18142.0, time stamp: 0x5d3573be
  Faulting module name: DataDesigners.dll, version: 2019.150.18142.0, time stamp: 0x5d3573f0
  Exception code: 0xc0000005
  Fault offset: 0x00004be8
  Faulting process id: 0x5ec8
  Faulting application start time: 0x01d56d761e232f6c
  Faulting application path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Ssms.exe
  Faulting module path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Tools\VDT\DataDesigners.dll
  Report Id: e797c8be-6448-4547-9f6f-146cd92d8178
  Faulting package full name: 
  Faulting package-relative application ID: 
","<sal-server><database><sums><diagram><designer>, sum crash try modify database diagram (ve.2), try modify database diagram great applied start crash try access. happen save diagram close application. try open throw error start sums. i'm run sal server 14.0.100 express edition. review microsoft event viewer get this: fault applied name: sums.eye, version: 2019.150.18142.0, time stamp: 0x5d3573be fault model name: datadesigners.all, version: 2019.150.18142.0, time stamp: 0x5d3573f0 except code: 0xc0000005 fault offset: 0x00004be8 fault process id: 0x5ec8 fault applied start time: 0x01d56d761e232f6c fault applied path: c:\program file (x)\microsoft sal server manage studio 18\common\side\sums.ex fault model path: c:\program file (x)\microsoft sal server manage studio 18\common\side\tools\vot\datadesigners.do report id: e797c8be-6448-4547-off-146cd92d8178 fault package full name: fault package-red applied id:"
50003906,Storing UUID as string in mysql using JPA,"I came across a blog of using UUID with Hibernate and MySql. Now the problem is, whenever I take a look at the database the ID's will be non-readable format (binary-16). How can I store UUID as a readable format like 7feb24af-fc38-44de-bc38-04defc3804fe instead of ¡7ôáßEN¹º}ÅÑs
I was using this code 
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""BINARY(16)"" )
private UUID id;
And the result is ¡7ôáßEN¹º}ÅÑs. But I want it as readable UUID so I used the following code which didn't help me
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""CHAR(32)"" )
private UUID id;
How to save the UUID as a string instead of binary(16) without changing the java type UUID
",<java><mysql><hibernate><spring-data-jpa><uuid>,851,0,12,810,2,7,23,53,27624,0.0,57,4,19,2018-04-24 14:13,2018-07-19 16:42,2018-07-19 16:42,86.0,86.0,Basic,10,"<java><mysql><hibernate><spring-data-jpa><uuid>, Storing UUID as string in mysql using JPA, I came across a blog of using UUID with Hibernate and MySql. Now the problem is, whenever I take a look at the database the ID's will be non-readable format (binary-16). How can I store UUID as a readable format like 7feb24af-fc38-44de-bc38-04defc3804fe instead of ¡7ôáßEN¹º}ÅÑs
I was using this code 
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""BINARY(16)"" )
private UUID id;
And the result is ¡7ôáßEN¹º}ÅÑs. But I want it as readable UUID so I used the following code which didn't help me
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""CHAR(32)"" )
private UUID id;
How to save the UUID as a string instead of binary(16) without changing the java type UUID
","<cava><myself><liberate><spring-data-pa><quid>, store quid string myself use pa, came across blow use quid wiberd myself. problem is, when take look database id' non-read format (binary-16). store quid readable format like 7feb24af-fc38-made-bc-04defc3804f instead ¡7ôáßen¹º}of use code @id @generatedvalue( genet = ""guide"" ) @genericgenerator( name = ""guide"", strategic = ""guide"" ) @column( name = ""id"", columndefinit = ""binary(16)"" ) privat quid id; result ¡7ôáßen¹º}ofs. want readable quid use follow code help @id @generatedvalue( genet = ""guide"" ) @genericgenerator( name = ""guide"", strategic = ""guide"" ) @column( name = ""id"", columndefinit = ""chair(32)"" ) privat quid id; save quid string instead binary(16) without change cava type quid"
48576847,"How to combine first name, middle name and last name in SQL server","you can refer the below queries to get the same-
1
select FirstName +' '+ MiddleName +' ' + Lastname as Name from TableName.
2
select CONCAT(FirstName , ' ' , MiddleName , ' ' , Lastname) as Name from 
  TableName
3
select Isnull(FirstName,' ') +' '+ Isnull(MiddleName,' ')+' '+ Isnull(Lastname,' ') 
from TableName.
Note: Point 1 query return if all columns have some value if anyone is null or empty then it will return null for all, means Name will return &quot;NULL&quot; value.
To avoid the point number 1, you can use point number 2 or point number 3 -
We can use IsNull or CONCAT keyword to get the same.
If anyone containing null value then ' ' (blank space) will add with next value.
",<sql-server>,693,0,7,301,1,3,8,63,199946,0.0,4,15,19,2018-02-02 6:24,2018-02-02 6:28,,0.0,,Basic,10,"<sql-server>, How to combine first name, middle name and last name in SQL server, you can refer the below queries to get the same-
1
select FirstName +' '+ MiddleName +' ' + Lastname as Name from TableName.
2
select CONCAT(FirstName , ' ' , MiddleName , ' ' , Lastname) as Name from 
  TableName
3
select Isnull(FirstName,' ') +' '+ Isnull(MiddleName,' ')+' '+ Isnull(Lastname,' ') 
from TableName.
Note: Point 1 query return if all columns have some value if anyone is null or empty then it will return null for all, means Name will return &quot;NULL&quot; value.
To avoid the point number 1, you can use point number 2 or point number 3 -
We can use IsNull or CONCAT keyword to get the same.
If anyone containing null value then ' ' (blank space) will add with next value.
","<sal-server>, combine first name, middle name last name sal server, refer query get same- 1 select firstnam +' '+ middlenam +' ' + lastnam name tablename. 2 select coat(firstnam , ' ' , middlenam , ' ' , lastname) name tablenam 3 select skull(firstname,' ') +' '+ skull(middlename,' ')+' '+ skull(lastname,' ') tablename. note: point 1 query return column value anyone null empty return null all, mean name return &quit;null&quit; value. avoid point number 1, use point number 2 point number 3 - use soul coat eyford get same. anyone contain null value ' ' (blank space) add next value."
50346326,"ProgrammingError: relation ""django_session"" does not exist","Got this error after changing my database from sqlite to postgresql. I've made all my settings changes: 
Here's my settings:
DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.postgresql_psycopg2"",
        'NAME': ""postr1"",
        'USER': ""zorgan"",
        'PASSWORD': config('DB_PASSWORD'),
        'HOST': ""localhost"",
        'PORT': '',
    }
}
as well as performing makemigrations and migrations which were all successful. So I'm able to succesfully start my local server:
System check identified no issues (0 silenced).
May 15, 2018 - 08:59:39
Django version 1.11.8, using settings 'draft1.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
however when I go to the site it returns this error:
ProgrammingError at /news/
relation ""django_session"" does not exist
LINE 1: ...ession_data"", ""django_session"".""expire_date"" FROM ""django_se...
Any idea what the problem is?
",<python><django><postgresql>,937,1,20,8357,27,108,215,59,31004,0.0,258,3,19,2018-05-15 9:09,2018-05-15 9:33,2018-05-15 9:33,0.0,0.0,Basic,10,"<python><django><postgresql>, ProgrammingError: relation ""django_session"" does not exist, Got this error after changing my database from sqlite to postgresql. I've made all my settings changes: 
Here's my settings:
DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.postgresql_psycopg2"",
        'NAME': ""postr1"",
        'USER': ""zorgan"",
        'PASSWORD': config('DB_PASSWORD'),
        'HOST': ""localhost"",
        'PORT': '',
    }
}
as well as performing makemigrations and migrations which were all successful. So I'm able to succesfully start my local server:
System check identified no issues (0 silenced).
May 15, 2018 - 08:59:39
Django version 1.11.8, using settings 'draft1.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
however when I go to the site it returns this error:
ProgrammingError at /news/
relation ""django_session"" does not exist
LINE 1: ...ession_data"", ""django_session"".""expire_date"" FROM ""django_se...
Any idea what the problem is?
","<patron><django><postgresql>, programmingerror: relate ""django_session"" exist, got error change database quite postgresql. i'v made set changes: here' settings: database = { 'default': { 'engine': ""django.do.backed.postgresql_psycopg2"", 'name': ""post"", 'user': ""organ"", 'password': confirm('db_password'), 'host': ""localhost"", 'port': '', } } well perform makemigr migrate successful. i'm all success start local server: system check identify issue (0 silenced). may 15, 2018 - 08:59:39 django version 1.11.8, use set 'draft.settings' start develop server http://127.0.0.1:8000/ quit server control-c. howe go site return error: programmingerror /news/ relate ""django_session"" exist line 1: ...ession_data"", ""django_session"".""expire_date"" ""django_se... idea problem is?"
55018986,Postgresql select count query takes long time,"I have a table named events in my Postgresql 9.5 database. And this table has about 6 million records.
I am runnig a select count(event_id) from events query. But this query takes 40seconds. This is very long time for a database. My event_id field of table is primary key and indexed. Why this takes very long time? (Server is ubuntu vm on vmware has 4cpu)
Explain:
""Aggregate  (cost=826305.19..826305.20 rows=1 width=0) (actual time=24739.306..24739.306 rows=1 loops=1)""
""  Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""  -&gt;  Seq Scan on event_source  (cost=0.00..812594.55 rows=5484255 width=0) (actual time=0.014..24087.050 rows=6320689 loops=1)""
""        Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""Planning time: 0.369 ms""
""Execution time: 24739.364 ms""
",<sql><postgresql><postgresql-9.5>,790,0,8,6499,14,83,181,79,17848,0.0,141,2,19,2019-03-06 8:53,2019-03-06 11:24,,0.0,,Intermediate,23,"<sql><postgresql><postgresql-9.5>, Postgresql select count query takes long time, I have a table named events in my Postgresql 9.5 database. And this table has about 6 million records.
I am runnig a select count(event_id) from events query. But this query takes 40seconds. This is very long time for a database. My event_id field of table is primary key and indexed. Why this takes very long time? (Server is ubuntu vm on vmware has 4cpu)
Explain:
""Aggregate  (cost=826305.19..826305.20 rows=1 width=0) (actual time=24739.306..24739.306 rows=1 loops=1)""
""  Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""  -&gt;  Seq Scan on event_source  (cost=0.00..812594.55 rows=5484255 width=0) (actual time=0.014..24087.050 rows=6320689 loops=1)""
""        Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""Planning time: 0.369 ms""
""Execution time: 24739.364 ms""
","<sal><postgresql><postgresql-9.5>, postgresql select count query take long time, table name event postgresql 9.5 database. table 6 million records. running select count(event_id) event query. query take seconds. long time database. event_id field table primary key indeed. take long time? (server bunt am aware cup) explain: ""agree (cost=826305.19..826305.20 rows=1 width=0) (actual time=24739.306..24739.306 rows=1 loops=1)"" "" suffers: share hit=13 read=757739 dried=53 written=48"" "" -&it; see scan event_sourc (cost=0.00..812594.55 rows=5484255 width=0) (actual time=0.014..24087.050 rows=6320689 loops=1)"" "" suffers: share hit=13 read=757739 dried=53 written=48"" ""plan time: 0.369 ms"" ""execute time: 24739.364 ms"""
57048744,Checking Concurrency on an Entity without updating the Row Version,"I have a parent entity that I need to do a concurrency check (as annotated as below) 
[Timestamp]
public byte[] RowVersion { get; set; }
I have a bunch of client processes that access readonly values out of this parent entity and primarily update its child entities.
The constraint
Clients should not interfere with each other's work, (e.g. updating child records should not throw a concurrency exception on the parent entity). 
I have a server process that does update this parent entity, and in this case the client process needs to throw if the parent entity has been changed. 
 Note : The client's concurrency check is sacrificial, the server's workflow is mission critical. 
The problem
I need to check (from the client process) if the parent entity has changed without updating the parents entity's row version. 
It's easy enough to do a concurrency check on the parent entity in EF:
// Update the row version's original value
_db.Entry(dbManifest)
      .Property(b =&gt; b.RowVersion)
      .OriginalValue = dbManifest.RowVersion; // the row version the client originally read
// Mark the row version as modified
_db.Entry(dbManifest)
       .Property(x =&gt; x.RowVersion)
       .IsModified = true;
The IsModified = true is the deal breaker because it forces the row version to change. Or, said in context, this check from the client process will cause a row version change in the parent entity, which interferes needlessly with the other client processes' workflows.
 A work around : I could potentially wrap the SaveChanges from the client process in a Transaction and then a subsequent read of the parent entity's row version, in-turn, rolling back if the row version has changed.
Summary 
Is there an out-of-the-box way with Entity Framework where I can SaveChanges (in the client process for the child entities) yet also check if the parent entity's row version has changed (without updating the parent entities row version).
",<c#><sql-server><entity-framework><entity-framework-6><database-concurrency>,1941,0,14,79780,9,103,142,41,5504,0.0,2181,3,19,2019-07-16 0:48,2019-07-20 20:10,2019-07-23 14:01,4.0,7.0,Advanced,32,"<c#><sql-server><entity-framework><entity-framework-6><database-concurrency>, Checking Concurrency on an Entity without updating the Row Version, I have a parent entity that I need to do a concurrency check (as annotated as below) 
[Timestamp]
public byte[] RowVersion { get; set; }
I have a bunch of client processes that access readonly values out of this parent entity and primarily update its child entities.
The constraint
Clients should not interfere with each other's work, (e.g. updating child records should not throw a concurrency exception on the parent entity). 
I have a server process that does update this parent entity, and in this case the client process needs to throw if the parent entity has been changed. 
 Note : The client's concurrency check is sacrificial, the server's workflow is mission critical. 
The problem
I need to check (from the client process) if the parent entity has changed without updating the parents entity's row version. 
It's easy enough to do a concurrency check on the parent entity in EF:
// Update the row version's original value
_db.Entry(dbManifest)
      .Property(b =&gt; b.RowVersion)
      .OriginalValue = dbManifest.RowVersion; // the row version the client originally read
// Mark the row version as modified
_db.Entry(dbManifest)
       .Property(x =&gt; x.RowVersion)
       .IsModified = true;
The IsModified = true is the deal breaker because it forces the row version to change. Or, said in context, this check from the client process will cause a row version change in the parent entity, which interferes needlessly with the other client processes' workflows.
 A work around : I could potentially wrap the SaveChanges from the client process in a Transaction and then a subsequent read of the parent entity's row version, in-turn, rolling back if the row version has changed.
Summary 
Is there an out-of-the-box way with Entity Framework where I can SaveChanges (in the client process for the child entities) yet also check if the parent entity's row version has changed (without updating the parent entities row version).
","<c#><sal-server><entity-framework><entity-framework-6><database-concurrence>, check concur entity without update row version, parent entity need concur check (a cannot below) [timestamp] public bite[] rovers { get; set; } bunch client process access readonli value parent entity primarily update child entitles. constraint client interfere other' work, (e.g. update child record throw concur except parent entity). server process update parent entity, case client process need throw parent entity changed. note : client' concur check sacrificing, server' workflow mission critical. problem need check (from client process) parent entity change without update parent entity' row version. east enough concur check parent entity of: // update row version' origin value do.entry(manifest) .property(b =&it; b.conversion) .originalvalu = manifest.conversion; // row version client origin read // mark row version modify do.entry(manifest) .property(x =&it; x.conversion) .ismodifi = true; ismodifi = true deal breakers for row version change. or, said context, check client process cause row version change parent entity, interfere needlessly client processes' workflows. work around : could potent wrap savechang client process transact subsequ read parent entity' row version, in-turn, roll back row version changed. summary out-of-the-box way entity framework savechang (in client process child entitles) yet also check parent entity' row version change (without update parent entity row version)."
59710811,NOT LIKE ANY query in Snowflake,"I am trying to run the following query in Snowflake:
SELECT * FROM chapters 
WHERE
title NOT LIKE ANY ('Summary%', 'Appendix%')
but it errors out. I know Snowflake support LIKE ANY query syntax. But I am not sure why my query is not working.
",<sql-like><snowflake-cloud-data-platform>,242,0,3,3973,11,58,101,52,29218,0.0,394,3,19,2020-01-13 5:04,2020-01-13 5:29,2020-01-13 5:29,0.0,0.0,Basic,9,"<sql-like><snowflake-cloud-data-platform>, NOT LIKE ANY query in Snowflake, I am trying to run the following query in Snowflake:
SELECT * FROM chapters 
WHERE
title NOT LIKE ANY ('Summary%', 'Appendix%')
but it errors out. I know Snowflake support LIKE ANY query syntax. But I am not sure why my query is not working.
","<sal-like><snowflakes-cloud-data-platform>, like query snowflakes, try run follow query snowflakes: select * chapter till like ('summary%', 'appendix%') error out. know snowflakes support like query santa. sure query working."
50546500,How to connect to docker mysql container on remote machine,"I have two machines. My machine with IP1(Europe), and other machine with public IP2(USA). On IP2 I have mysql container running with volume /var/lib/mysql set to be replicated in some folder on the host machine ~/mysqldatabase. Firewall rule for port 3306 is added. Also I have ssh connection to the machine. So I'm not sure where to start. Usually when there is not docker I add just 
bind-address = 0.0.0.0
as configuration in mysql and I open the firewall port 3306 (or an other one that points to mysql) and the things work.
So probably I can install mysql-server package (the host is ubuntu16.04) outside of docker on the IP2 machine and to set it to point to the ~/mysqldatabase folder, but is it really necessary to do that? 
Is there way to connect directly from IP1 to IP2:mysql_container:mysql_database
I run the mysql docker container in two ways. One is with docker file. And the other one is with systemctl service.
Part of the docker-compose.yml:
version: ""3""
services:
  mysql:
    image: mysql:5.7
    volumes:
      - /home/username/mysqldatabase:/var/lib/mysql
    ports:
      - '3306:3306'
    environment:
        MYSQL_ROOT_PASSWORD: rootpass
        MYSQL_DATABASE: somedb
        MYSQL_USER: someuser
        MYSQL_PASSWORD: someuserpassword
mysql.service 
[Unit]
Description=Run %p
Requires=docker.service
After=docker.service
[Service]
Restart=always
ExecStartPre=-/usr/bin/docker kill %p
ExecStartPre=-/usr/bin/docker rm -f %p
docker run --rm --name mysql -v /home/username/mysqldatabase:/var/lib/mysql -p 3306:3306 \
-e MYSQL_ROOT_PASSWORD=rootpass -e MYSQL_DATABASE=somedb -e MYSQL_USER=someuser -e MYSQL_PASSWORD=someuserpassword \
mysql:5.7
ExecStop=/usr/bin/docker stop %p
[Install]
WantedBy=multi-user.target
To make the things more simple lets say that I use only the second approach.
I DON""T have :
1.mysql-client, mysql-server or mysql on the IP2 machine
2.and I haven't set anywhere to bind to 0.0.0.0 because I want to connnect directly to the mysql container. Probably I have to set it
to bind to 0.0.0.0 inside this container.
Result for the firewall 
sudo netstat -tunlp | grep 3306
tcp6       0      0 :::3306                 :::*                    LISTEN      24717/docker-proxy
",<mysql><docker><remote-access><ports>,2223,0,42,6598,4,46,61,39,49403,0.0,1797,6,19,2018-05-26 19:52,2018-05-26 23:57,2021-05-09 18:31,0.0,1079.0,Advanced,39,"<mysql><docker><remote-access><ports>, How to connect to docker mysql container on remote machine, I have two machines. My machine with IP1(Europe), and other machine with public IP2(USA). On IP2 I have mysql container running with volume /var/lib/mysql set to be replicated in some folder on the host machine ~/mysqldatabase. Firewall rule for port 3306 is added. Also I have ssh connection to the machine. So I'm not sure where to start. Usually when there is not docker I add just 
bind-address = 0.0.0.0
as configuration in mysql and I open the firewall port 3306 (or an other one that points to mysql) and the things work.
So probably I can install mysql-server package (the host is ubuntu16.04) outside of docker on the IP2 machine and to set it to point to the ~/mysqldatabase folder, but is it really necessary to do that? 
Is there way to connect directly from IP1 to IP2:mysql_container:mysql_database
I run the mysql docker container in two ways. One is with docker file. And the other one is with systemctl service.
Part of the docker-compose.yml:
version: ""3""
services:
  mysql:
    image: mysql:5.7
    volumes:
      - /home/username/mysqldatabase:/var/lib/mysql
    ports:
      - '3306:3306'
    environment:
        MYSQL_ROOT_PASSWORD: rootpass
        MYSQL_DATABASE: somedb
        MYSQL_USER: someuser
        MYSQL_PASSWORD: someuserpassword
mysql.service 
[Unit]
Description=Run %p
Requires=docker.service
After=docker.service
[Service]
Restart=always
ExecStartPre=-/usr/bin/docker kill %p
ExecStartPre=-/usr/bin/docker rm -f %p
docker run --rm --name mysql -v /home/username/mysqldatabase:/var/lib/mysql -p 3306:3306 \
-e MYSQL_ROOT_PASSWORD=rootpass -e MYSQL_DATABASE=somedb -e MYSQL_USER=someuser -e MYSQL_PASSWORD=someuserpassword \
mysql:5.7
ExecStop=/usr/bin/docker stop %p
[Install]
WantedBy=multi-user.target
To make the things more simple lets say that I use only the second approach.
I DON""T have :
1.mysql-client, mysql-server or mysql on the IP2 machine
2.and I haven't set anywhere to bind to 0.0.0.0 because I want to connnect directly to the mysql container. Probably I have to set it
to bind to 0.0.0.0 inside this container.
Result for the firewall 
sudo netstat -tunlp | grep 3306
tcp6       0      0 :::3306                 :::*                    LISTEN      24717/docker-proxy
","<myself><doctor><remote-access><ports>, connect doctor myself contain remote machine, two machines. machine in(europe), machine public in(us). in myself contain run volume /war/limb/myself set relic older host machine ~/mysqldatabase. firewal rule port 3306 added. also ash connect machine. i'm sure start. usual doctor add bind-address = 0.0.0.0 configur myself open firewal port 3306 (or one point myself) thing work. probably instal myself-serve package (the host ubuntu16.04) outside doctor in machine set point ~/mysqldatabas older, really necessary that? way connect directly in in:mysql_container:mysql_databas run myself doctor contain two ways. one doctor file. one systemctl service. part doctor-compose.you: version: ""3"" services: myself: image: myself:5.7 volumes: - /home/surname/mysqldatabase:/war/limb/myself ports: - '3306:3306' environment: mysql_root_password: rootpass mysql_database: some mysql_user: some mysql_password: someuserpassword myself.service [unit] description=run %p requires=doctor.service after=doctor.service [service] start=away execstartpre=-/us/bin/dock kill %p execstartpre=-/us/bin/dock am -f %p doctor run --am --name myself -v /home/surname/mysqldatabase:/war/limb/myself -p 3306:3306 \ -e mysql_root_password=rootpass -e mysql_database=some -e mysql_user=some -e mysql_password=someuserpassword \ myself:5.7 executor=/us/bin/dock stop %p [install] wanted=multi-user.target make thing simple let say use second approach. don""t : 1.myself-client, myself-serve myself in machine 2.and set anywhere bind 0.0.0.0 want connect directly myself container. probably set bind 0.0.0.0 inside container. result firewal so netstat -tune | grew 3306 type 0 0 :::3306 :::* listen 24717/doctor-prove"
52120182,Bigquery - json_extract all elements from an array,"i'm trying to extract two key from every json in an arry of jsons(using sql legacy)
currently i am using json extract function :
json_extract(json_column , '$[1].X') AS X,
json_extract(json_column , '$[1].Y') AS Y,
how can i make it run on every json at the 'json arry column', and not just [1] (for example)?
An example json:
[
{""blabla"":000,""X"":1,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""2""},
{""blabla"":000,""X"":3,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""4""},
]   
thanks in advance!
",<sql><arrays><json><google-bigquery><legacy-sql>,493,0,9,219,1,2,7,52,50180,0.0,0,2,19,2018-08-31 17:23,2018-08-31 21:03,,0.0,,Basic,10,"<sql><arrays><json><google-bigquery><legacy-sql>, Bigquery - json_extract all elements from an array, i'm trying to extract two key from every json in an arry of jsons(using sql legacy)
currently i am using json extract function :
json_extract(json_column , '$[1].X') AS X,
json_extract(json_column , '$[1].Y') AS Y,
how can i make it run on every json at the 'json arry column', and not just [1] (for example)?
An example json:
[
{""blabla"":000,""X"":1,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""2""},
{""blabla"":000,""X"":3,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""4""},
]   
thanks in advance!
","<sal><array><son><goose-bigquery><legacy-sal>, bigqueri - json_extract element array, i'm try extract two key every son are sons(us sal legacy) current use son extract function : json_extract(json_column , '$[1].x') x, json_extract(json_column , '$[1].y') y, make run every son 'son are column', [1] (for example)? example son: [ {""labia"":000,""x"":1,""labia"":000,""labia"":000,""labia"":000,,""y"":""2""}, {""labia"":000,""x"":3,""labia"":000,""labia"":000,""labia"":000,,""y"":""4""}, ] thank advance!"
52494572,"SQL Alchemy Parametrized Query , binding table name as parameter gives error","I am using parametrized query utilizing Text object in SQL alchemy and are getting different result.
Working example:     
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM Cars 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p2': id_cars})
Example that produces sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax)
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM :p1 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p1': db_table, 'p2': id_cars})
Any idea on how I can run the query with a dynamic table name that are also protected from sql injection?
",<python><sql><prepared-statement>,910,0,20,237,0,2,9,66,4842,0.0,8,2,19,2018-09-25 9:15,2019-04-29 2:08,,216.0,,Basic,10,"<python><sql><prepared-statement>, SQL Alchemy Parametrized Query , binding table name as parameter gives error, I am using parametrized query utilizing Text object in SQL alchemy and are getting different result.
Working example:     
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM Cars 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p2': id_cars})
Example that produces sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax)
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM :p1 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p1': db_table, 'p2': id_cars})
Any idea on how I can run the query with a dynamic table name that are also protected from sql injection?
","<patron><sal><prepared-statement>, sal alchemi parameter query , bind table name parapet give error, use parameter query until text object sal alchemi get differ result. work example: import sqlalchemi shall sqlalchemy.sal import text db_tabl = 'cars' id_car = 8 query = text(""""""select * car dear = :pp """""") self.engine.execute(query, {'pp': id_cars}) example produce sqlalchemy.etc.programmingerror: (pymysql.err.programmingerror) (1064, ""you error sal santa) import sqlalchemi shall sqlalchemy.sal import text db_tabl = 'cars' id_car = 8 query = text(""""""select * :pp dear = :pp """""") self.engine.execute(query, {'pp': db_table, 'pp': id_cars}) idea run query dream table name also protect sal injection?"
51844616,High number of live/dead tuples in postgresql/ Vacuum not working,"There is a table , which has 200 rows . But number of live tuples showing there is more than that (around 60K) .
select count(*) from subscriber_offset_manager;
 count 
-------
   200
(1 row)
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      61453 |          5
(1 row)
But as seen from pg_stat_activity and pg_locks , we are not able to track any open connection .
SELECT query, state,locktype,mode
FROM pg_locks
JOIN pg_stat_activity
  USING (pid)
WHERE relation::regclass = 'subscriber_offset_manager'::regclass
  ;
 query | state | locktype | mode 
-------+-------+----------+------
(0 rows)
I also tried  full vacuum on this table  , Below are results : 
All the times no rows are removed 
some times all the live tuples become dead tuples . 
Here is output .
vacuum FULL VERBOSE ANALYZE subscriber_offset_manager;
INFO:  vacuuming ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": found 0 removable, 67920 nonremovable row versions in 714 pages
DETAIL:  67720 dead row versions cannot be removed yet.
CPU 0.01s/0.06u sec elapsed 0.13 sec.
INFO:  analyzing ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": scanned 710 of 710 pages, containing 200 live rows and 67720 dead rows; 200 rows in sample, 200 estimated total rows
VACUUM
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |        200 |      67749
and after 10 sec 
SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      68325 |        132
How Our App query to this table . 
Our application generally select some rows and based on some business calculation, update the row  . 
select  query --  select based on some id 
select * from subscriber_offset_manager where shard_id=1 ;
update query --  update some other column  for this selected shard id
around 20 threads do this in parallel  and One thread works on only one row .
app is writen in java and we are using hibernate to do db operations . 
Postgresql version is 9.3.24
One more interesting observation : 
 -  when i stop my java app and then do full vacuum , it works fine (number of rows and live tuples become equal). So there is something wrong if we select and update continuously from java app . – 
Problem/Issue 
These live tuples some times go to dead tuples and after some times again comes to live . 
Due to above behaviour select from the table taking time and increasing load on server as lots of live/deadtuples are there ..
",<postgresql><performance><hibernate><postgresql-9.3>,3198,0,41,1311,1,12,29,52,18315,0.0,71,3,19,2018-08-14 15:09,2018-08-14 15:47,2018-08-14 20:41,0.0,0.0,Intermediate,23,"<postgresql><performance><hibernate><postgresql-9.3>, High number of live/dead tuples in postgresql/ Vacuum not working, There is a table , which has 200 rows . But number of live tuples showing there is more than that (around 60K) .
select count(*) from subscriber_offset_manager;
 count 
-------
   200
(1 row)
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      61453 |          5
(1 row)
But as seen from pg_stat_activity and pg_locks , we are not able to track any open connection .
SELECT query, state,locktype,mode
FROM pg_locks
JOIN pg_stat_activity
  USING (pid)
WHERE relation::regclass = 'subscriber_offset_manager'::regclass
  ;
 query | state | locktype | mode 
-------+-------+----------+------
(0 rows)
I also tried  full vacuum on this table  , Below are results : 
All the times no rows are removed 
some times all the live tuples become dead tuples . 
Here is output .
vacuum FULL VERBOSE ANALYZE subscriber_offset_manager;
INFO:  vacuuming ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": found 0 removable, 67920 nonremovable row versions in 714 pages
DETAIL:  67720 dead row versions cannot be removed yet.
CPU 0.01s/0.06u sec elapsed 0.13 sec.
INFO:  analyzing ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": scanned 710 of 710 pages, containing 200 live rows and 67720 dead rows; 200 rows in sample, 200 estimated total rows
VACUUM
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |        200 |      67749
and after 10 sec 
SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      68325 |        132
How Our App query to this table . 
Our application generally select some rows and based on some business calculation, update the row  . 
select  query --  select based on some id 
select * from subscriber_offset_manager where shard_id=1 ;
update query --  update some other column  for this selected shard id
around 20 threads do this in parallel  and One thread works on only one row .
app is writen in java and we are using hibernate to do db operations . 
Postgresql version is 9.3.24
One more interesting observation : 
 -  when i stop my java app and then do full vacuum , it works fine (number of rows and live tuples become equal). So there is something wrong if we select and update continuously from java app . – 
Problem/Issue 
These live tuples some times go to dead tuples and after some times again comes to live . 
Due to above behaviour select from the table taking time and increasing load on server as lots of live/deadtuples are there ..
","<postgresql><performance><liberate><postgresql-9.3>, high number live/dead up postgresql/ vacuum working, table , 200 row . number live up show (around ask) . select count(*) subscriber_offset_manager; count ------- 200 (1 row) select schemaname,release,n_live_tup,n_dead_tup pg_stat_user_t release='subscriber_offset_manager' order n_dead_tup ; schemanam | realm | n_live_tup | n_dead_tup ------------+---------------------------+------------+------------ public | subscriber_offset_manag | 61453 | 5 (1 row) seen pg_stat_act pg_lock , all track open connect . select query, state,locktype,god pg_lock join pg_stat_act use (did) relation::regulars = 'subscriber_offset_manager'::regulars ; query | state | lockup | mode -------+-------+----------+------ (0 rows) also try full vacuum table , result : time row remove time live up become dead up . output . vacuum full verbs analyze subscriber_offset_manager; into: vacuum ""public.subscriber_offset_manager"" into: ""subscriber_offset_manager"": found 0 removable, 67920 nonremov row version 714 page detail: 67720 dead row version cannot remove yet. cup 0.was/0.you see leaps 0.13 see. into: analyze ""public.subscriber_offset_manager"" into: ""subscriber_offset_manager"": scan 710 710 pages, contain 200 live row 67720 dead rows; 200 row sample, 200 estime total row vacuum select schemaname,release,n_live_tup,n_dead_tup pg_stat_user_t release='subscriber_offset_manager' order n_dead_tup ; schemanam | realm | n_live_tup | n_dead_tup ------------+---------------------------+------------+------------ public | subscriber_offset_manag | 200 | 67749 10 see select schemaname,release,n_live_tup,n_dead_tup pg_stat_user_t release='subscriber_offset_manager' order n_dead_tup ; schemanam | realm | n_live_tup | n_dead_tup ------------+---------------------------+------------+------------ public | subscriber_offset_manag | 68325 | 132 pp query table . applied genet select row base busy calculation, update row . select query -- select base id select * subscriber_offset_manag shard_id=1 ; update query -- update column select hard id around 20 thread parallel one thread work one row . pp written cava use wiberd do over . postgresql version 9.3.24 one interest observe : - stop cava pp full vacuum , work fine (number row live up become equal). cometh wrong select update continue cava pp . – problem/issue live up time go dead up time come live . due behaviour select table take time increase load server lot live/deadtupl .."
60377396,Postgresql connection marked as broken by Hikari,"I have Spring application that regularly inserts records in a PostgreSQL-DB. Now, after a scheduled set of imports is done it takes a few seconds until I get the following warning:
com.zaxxer.hikari.pool.ProxyConnection   : HikariPool-1 - Connection org.postgresql.jdbc.PgConnection@7d18a7dc marked as broken because of SQLSTATE(08006), ErrorCode(0)
followed by the this exception:
org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:335) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:143) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:132) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java) ~[HikariCP-3.2.0.jar:na]
    at org.jooq.tools.jdbc.DefaultPreparedStatement.execute(DefaultPreparedStatement.java:209) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:432) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:613) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.Tools$10$1.block(Tools.java:4377) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3118) ~[na:na]
    at org.jooq.impl.Tools$10.get(Tools.java:4374) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[na:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1692) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177) ~[na:na]
Caused by: java.io.EOFException: null
    at org.postgresql.core.PGStream.receiveChar(PGStream.java:308) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1952) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308) ~[postgresql-42.2.5.jar:42.2.5]
On most occurrences this also sets my database in recovery mode.
On the next run of the scheduler the hikari pool warns about the connections that could not be validated:
HikariPool-1 - Failed to validate connection org.postgresql.jdbc.PgConnection@389c2816 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
I fiddled around with the validation-timeout and max-lifetime settings of hikari but none of the changes seemed to have any effect.
",<spring><postgresql><kotlin><jooq><hikaricp>,3480,0,30,609,0,5,16,38,8229,0.0,15,0,19,2020-02-24 13:54,,,,,Intermediate,23,"<spring><postgresql><kotlin><jooq><hikaricp>, Postgresql connection marked as broken by Hikari, I have Spring application that regularly inserts records in a PostgreSQL-DB. Now, after a scheduled set of imports is done it takes a few seconds until I get the following warning:
com.zaxxer.hikari.pool.ProxyConnection   : HikariPool-1 - Connection org.postgresql.jdbc.PgConnection@7d18a7dc marked as broken because of SQLSTATE(08006), ErrorCode(0)
followed by the this exception:
org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:335) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:143) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:132) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java) ~[HikariCP-3.2.0.jar:na]
    at org.jooq.tools.jdbc.DefaultPreparedStatement.execute(DefaultPreparedStatement.java:209) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:432) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:613) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.Tools$10$1.block(Tools.java:4377) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3118) ~[na:na]
    at org.jooq.impl.Tools$10.get(Tools.java:4374) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[na:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1692) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177) ~[na:na]
Caused by: java.io.EOFException: null
    at org.postgresql.core.PGStream.receiveChar(PGStream.java:308) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1952) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308) ~[postgresql-42.2.5.jar:42.2.5]
On most occurrences this also sets my database in recovery mode.
On the next run of the scheduler the hikari pool warns about the connections that could not be validated:
HikariPool-1 - Failed to validate connection org.postgresql.jdbc.PgConnection@389c2816 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
I fiddled around with the validation-timeout and max-lifetime settings of hikari but none of the changes seemed to have any effect.
","<spring><postgresql><olin><room><hikaricp>, postgresql connect mark broken kari, spring applied regularly insert record postgresql-do. now, schedule set import done take second get follow warning: com.baxter.kari.pool.proxyconnect : hikaripool-1 - connect org.postgresql.job.connection@7d18a7dc mark broken sqlstate(08006), errorcode(0) follow exception: org.postgresql.until.psqlexception: i/o error occur send backed. org.postgresql.core.ve.queryexecutorimpl.execute(queryexecutorimpl.cava:335) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.job.statement.executeinternal(statement.cava:441) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.job.statement.execute(statement.cava:365) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.job.pgpreparedstatement.executewithflags(pgpreparedstatement.cava:143) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.job.pgpreparedstatement.execute(pgpreparedstatement.cava:132) ~[postgresql-42.2.5.jar:42.2.5] com.baxter.kari.pool.proxypreparedstatement.execute(proxypreparedstatement.cava:44) ~[hikaricp-3.2.0.jar:na] com.baxter.kari.pool.hikariproxypreparedstatement.execute(hikariproxypreparedstatement.cava) ~[hikaricp-3.2.0.jar:na] org.room.tools.job.defaultpreparedstatement.execute(defaultpreparedstatement.cava:209) ~[room-3.11.9.jar:na] org.room.imply.abstractquery.execute(abstractquery.cava:432) ~[room-3.11.9.jar:na] org.room.imply.abstractdmlquery.execute(abstractdmlquery.cava:613) ~[room-3.11.9.jar:na] org.room.imply.abstractquery.execute(abstractquery.cava:350) ~[room-3.11.9.jar:na] org.room.imply.tools$10$1.block(tools.cava:4377) ~[room-3.11.9.jar:na] cava.base/cava.until.concurrent.forkjoinpool.managedblock(forkjoinpool.cava:3118) ~[na:na] org.room.imply.tools$10.get(tools.cava:4374) ~[room-3.11.9.jar:na] cava.base/cava.until.concurrent.completablefuture$asyncsupply.run(completablefuture.cava:1700) ~[na:na] cava.base/cava.until.concurrent.completablefuture$asyncsupply.even(completablefuture.cava:1692) ~[na:na] cava.base/cava.until.concurrent.forkjointask.doexec(forkjointask.cava:290) ~[na:na] cava.base/cava.until.concurrent.forkjoinpool$workqueue.toplevelexec(forkjoinpool.cava:1020) ~[na:na] cava.base/cava.until.concurrent.forkjoinpool.scan(forkjoinpool.cava:1656) ~[na:na] cava.base/cava.until.concurrent.forkjoinpool.runworker(forkjoinpool.cava:1594) ~[na:na] cava.base/cava.until.concurrent.forkjoinworkerthread.run(forkjoinworkerthread.cava:177) ~[na:na] cause by: cava.to.eofexception: null org.postgresql.core.stream.receivechar(stream.cava:308) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.ve.queryexecutorimpl.processresults(queryexecutorimpl.cava:1952) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.ve.queryexecutorimpl.execute(queryexecutorimpl.cava:308) ~[postgresql-42.2.5.jar:42.2.5] occur also set database recovery mode. next run schedule kari pool warn connect could validated: hikaripool-1 - fail valid connect org.postgresql.job.connection@389c2816 (the connect closed.). possible consider use shorter maxlifetim value. fiddle around variation-timeout max-lifetime set kari none change seem effect."
56564410,Tables could not be fetched - Error loading schema content,"I open workbench and connect to a local database on XAMPP and when open the connection the schema show the error message:
  ""tables could not be fetched""
",<mysql-workbench><workbench><kie-workbench><sql-workbench-j>,154,0,0,161,1,2,7,38,54009,0.0,1,13,19,2019-06-12 14:20,2020-02-22 20:37,,255.0,,Intermediate,15,"<mysql-workbench><workbench><kie-workbench><sql-workbench-j>, Tables could not be fetched - Error loading schema content, I open workbench and connect to a local database on XAMPP and when open the connection the schema show the error message:
  ""tables could not be fetched""
","<myself-workbench><workbench><lie-workbench><sal-workbench-j>, table could fetch - error load scheme content, open workbench connect local database camp open connect scheme show error message: ""table could fetched"""
51136693,how to check HikariCP connection pooling is working or not in Java?,"I have written following properties in my configuration files I am using Log4j 
in my application When I am running a project.
I am getting following message.does that mean connection pooling is configured in my project? if not then how it will be?
INFO: internal.ConnectionProviderInitiator - HHH000130: Instantiating explicit connection provider: com.zaxxer.hikari.hibernate.HikariConnectionProvider
I have referred following link also
link here
Datasource settings
hibernate.datasource.driver-class-name=com.mysql.jdbc.Driver
hibernate.datasource.url=jdbc:mysql://localhost:3306/mydb
hibernate.datasource.username=root
hibernate.datasource.password=root
HikariCP Settings
hibernate.hikari.dataSource.url=jdbc:mysql://localhost:3306/mydb
hibernate.hikari.idleTimeout=10
hibernate.hikari.maximumPoolSize=30
hibernate.hikari.minimumIdle=15
hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
hibernate.hikari.dataSourceClassName=com.mysql.jdbc.jdbc2.optional.MysqlDataSource
",<java><mysql><hibernate><connection-pooling><hikaricp>,1012,1,11,774,1,10,40,51,28351,0.0,531,4,19,2018-07-02 12:52,2018-07-02 12:59,,0.0,,Intermediate,15,"<java><mysql><hibernate><connection-pooling><hikaricp>, how to check HikariCP connection pooling is working or not in Java?, I have written following properties in my configuration files I am using Log4j 
in my application When I am running a project.
I am getting following message.does that mean connection pooling is configured in my project? if not then how it will be?
INFO: internal.ConnectionProviderInitiator - HHH000130: Instantiating explicit connection provider: com.zaxxer.hikari.hibernate.HikariConnectionProvider
I have referred following link also
link here
Datasource settings
hibernate.datasource.driver-class-name=com.mysql.jdbc.Driver
hibernate.datasource.url=jdbc:mysql://localhost:3306/mydb
hibernate.datasource.username=root
hibernate.datasource.password=root
HikariCP Settings
hibernate.hikari.dataSource.url=jdbc:mysql://localhost:3306/mydb
hibernate.hikari.idleTimeout=10
hibernate.hikari.maximumPoolSize=30
hibernate.hikari.minimumIdle=15
hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
hibernate.hikari.dataSourceClassName=com.mysql.jdbc.jdbc2.optional.MysqlDataSource
","<cava><myself><liberate><connection-pooling><hikaricp>, check hikaricp connect pool work cava?, written follow property configur file use log applied run project. get follow message.do mean connect pool configur project? be? into: internal.connectionprovideriniti - hhh000130: instant explicit connect provider: com.baxter.kari.liberate.hikariconnectionprovid refer follow link also link datasourc set liberate.datasource.driver-class-name=com.myself.job.drive liberate.datasource.curl=job:myself://localhost:3306/my liberate.datasource.surname=root liberate.datasource.password=root hikaricp set liberate.kari.datasource.curl=job:myself://localhost:3306/my liberate.kari.idletimeout=10 liberate.kari.maximumpoolsize=30 liberate.kari.minimumidle=15 liberate.connection.provider_class=com.baxter.kari.liberate.hikariconnectionprovid liberate.kari.datasourceclassname=com.myself.job.jdbc2.optional.mysqldatasourc"
59675402,Django Full Text SearchVectorField obsolete in PostgreSQL,"I'm using Django's inbuilt full text search with PostgreSQL.
The Django docs say that performance can be improved by using a SearchVectorField. That field keeps a pre-generated ts_vector column with all the relevant lexemes alongside the model, rather than generating it on the fly during every search.
However, with this approach the ts_vector must be updated whenever the model is updated. To keep it synchronised, the Django docs suggest using &quot;triggers&quot;, and refer us to the PostgreSQL documentation for more details.
However, the PostgreSQL docs themselves say that the trigger approach is now obsolete. Instead of manually updating the ts_vector column, it is better to keep the column automatically up-to-date by using a stored generated column.
How can I use PostgreSQL's recommended approach with Django?
",<django><postgresql><full-text-search>,824,4,4,1178,2,14,28,73,2348,0.0,2151,2,18,2020-01-10 4:08,2020-01-10 5:30,,0.0,,Basic,3,"<django><postgresql><full-text-search>, Django Full Text SearchVectorField obsolete in PostgreSQL, I'm using Django's inbuilt full text search with PostgreSQL.
The Django docs say that performance can be improved by using a SearchVectorField. That field keeps a pre-generated ts_vector column with all the relevant lexemes alongside the model, rather than generating it on the fly during every search.
However, with this approach the ts_vector must be updated whenever the model is updated. To keep it synchronised, the Django docs suggest using &quot;triggers&quot;, and refer us to the PostgreSQL documentation for more details.
However, the PostgreSQL docs themselves say that the trigger approach is now obsolete. Instead of manually updating the ts_vector column, it is better to keep the column automatically up-to-date by using a stored generated column.
How can I use PostgreSQL's recommended approach with Django?
","<django><postgresql><full-text-search>, django full text searchvectorfield obsolete postgresql, i'm use django' built full text search postgresql. django do say perform improve use searchvectorfield. field keep pre-genet ts_vector column rule seem alongside model, rather genet fly every search. however, approach ts_vector must update when model updated. keep synchronised, django do suggest use &quit;trigger&quit;, refer us postgresql document details. however, postgresql do say trigger approach obsolete. instead manual update ts_vector column, better keep column automatic up-to-d use store genet column. use postgresql' recommend approach django?"
