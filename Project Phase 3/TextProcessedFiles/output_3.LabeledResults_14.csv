QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
54944677,Sql Server LDF file taking too large space,"Why is my database log file taking to high space? Its almost taking up 30GB of my HDD. Even after deleting 1,000,000 records, its not freeing up any space.
So, 
1.Why is the log file taking this much space (30gb)?2.how can I free up the space?
",<sql-server>,244,0,5,612,2,6,32,75,43163,0.0,244,5,15,2019-03-01 12:27,2019-03-01 13:00,2019-03-01 13:00,0.0,0.0,Intermediate,23,"<sql-server>, Sql Server LDF file taking too large space, Why is my database log file taking to high space? Its almost taking up 30GB of my HDD. Even after deleting 1,000,000 records, its not freeing up any space.
So, 
1.Why is the log file taking this much space (30gb)?2.how can I free up the space?
","<sal-server>, sal server of file take large space, database log file take high space? almost take 30gb had. even delete 1,000,000 records, free space. so, 1.who log file take much space (30gb)?2.how free space?"
50793970,Select shortest and longest string,"Is it possible to select the shortest and longest strings by characters in a table?
I have a CITY column of type VARCHAR(20) and I want to select the shortest and longest city names in alphabetical order by length.
I did like this
SELECT CITY,LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) IN ( SELECT MAX(LENGTH(CITY)) FROM STATION UNION SELECT MIN(LENGTH(CITY)) FROM STATION ) ORDER BY CITY ASC;
When ordered alphabetically, Let the CITY names be listed as ABC, DEF, PQRS, and WXY, with the respective lengths 3,3,4, and 3. The longest-named city is obviously PQRS, but there are options for the shortest-named city; I have to select ABC because it comes first alphabetically.
My query ended up with all three CITY having length 3.
ABC 3 DEF 3 PQRS 4 WXY 3 
The result of SELECT must be
ABC 3 PQRS 4 
",<mysql><sql>,801,0,5,513,1,6,13,74,40990,0.0,314,20,15,2018-06-11 9:03,2018-06-11 9:15,2018-06-12 9:08,0.0,1.0,Basic,9,"<mysql><sql>, Select shortest and longest string, Is it possible to select the shortest and longest strings by characters in a table?
I have a CITY column of type VARCHAR(20) and I want to select the shortest and longest city names in alphabetical order by length.
I did like this
SELECT CITY,LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) IN ( SELECT MAX(LENGTH(CITY)) FROM STATION UNION SELECT MIN(LENGTH(CITY)) FROM STATION ) ORDER BY CITY ASC;
When ordered alphabetically, Let the CITY names be listed as ABC, DEF, PQRS, and WXY, with the respective lengths 3,3,4, and 3. The longest-named city is obviously PQRS, but there are options for the shortest-named city; I have to select ABC because it comes first alphabetically.
My query ended up with all three CITY having length 3.
ABC 3 DEF 3 PQRS 4 WXY 3 
The result of SELECT must be
ABC 3 PQRS 4 
","<myself><sal>, select shortest longest string, possible select shortest longest string character table? city column type varchar(20) want select shortest longest city name alphabet order length. like select city,length(city) station length(city) ( select max(length(city)) station union select min(length(city)) station ) order city as; order alphabetically, let city name list abc, def, part, way, respect length 3,3,4, 3. longest-am city obvious part, option shortest-am city; select abc come first alphabetically. query end three city length 3. abc 3 def 3 per 4 xi 3 result select must abc 3 per 4"
49432167,How to convert rows into a list of dictionaries in pyspark?,"I have a DataFrame(df) in pyspark, by reading from a hive table:
df=spark.sql('select * from &lt;table_name&gt;')
+++++++++++++++++++++++++++++++++++++++++++
|  Name    |    URL visited               |
+++++++++++++++++++++++++++++++++++++++++++
|  person1 | [google,msn,yahoo]           |
|  person2 | [fb.com,airbnb,wired.com]    |
|  person3 | [fb.com,google.com]          |
+++++++++++++++++++++++++++++++++++++++++++
When i tried the following, got an error
df_dict = dict(zip(df['name'],df['url']))
""TypeError: zip argument #1 must support iteration.""
type(df.name) is of 'pyspark.sql.column.Column'
How do i create a dictionary like the following, which can be iterated later on
{'person1':'google','msn','yahoo'}
{'person2':'fb.com','airbnb','wired.com'}
{'person3':'fb.com','google.com'}
Appreciate your thoughts and help. 
",<apache-spark><pyspark><apache-spark-sql>,833,0,16,169,1,1,3,55,52106,0.0,0,4,15,2018-03-22 15:10,2018-03-22 15:28,,0.0,,Basic,10,"<apache-spark><pyspark><apache-spark-sql>, How to convert rows into a list of dictionaries in pyspark?, I have a DataFrame(df) in pyspark, by reading from a hive table:
df=spark.sql('select * from &lt;table_name&gt;')
+++++++++++++++++++++++++++++++++++++++++++
|  Name    |    URL visited               |
+++++++++++++++++++++++++++++++++++++++++++
|  person1 | [google,msn,yahoo]           |
|  person2 | [fb.com,airbnb,wired.com]    |
|  person3 | [fb.com,google.com]          |
+++++++++++++++++++++++++++++++++++++++++++
When i tried the following, got an error
df_dict = dict(zip(df['name'],df['url']))
""TypeError: zip argument #1 must support iteration.""
type(df.name) is of 'pyspark.sql.column.Column'
How do i create a dictionary like the following, which can be iterated later on
{'person1':'google','msn','yahoo'}
{'person2':'fb.com','airbnb','wired.com'}
{'person3':'fb.com','google.com'}
Appreciate your thoughts and help. 
","<apache-spark><spark><apache-spark-sal>, convert row list dictionary spark?, dataframe(of) spark, read hive table: of=spark.sal('select * &it;table_name&it;') +++++++++++++++++++++++++++++++++++++++++++ | name | curl visit | +++++++++++++++++++++++++++++++++++++++++++ | person | [goose,man,yakov] | | person | [ff.com,airbnb,wired.com] | | person | [ff.com,goose.com] | +++++++++++++++++++++++++++++++++++++++++++ try following, got error df_dict = duct(zip(of['name'],of['curl'])) ""typeerror: zip argument #1 must support operation."" type(of.name) 'spark.sal.column.column' great dictionary like following, inter later {'person':'goose','man','yakov'} {'person':'ff.com','airbnb','wired.com'} {'person':'ff.com','goose.com'} appreci thought help."
48847660,Spark + Parquet + Snappy: Overall compression ratio loses after spark shuffles data,"Commmunity!
Please help me understand how to get better compression ratio with Spark?
Let me describe case:
I have dataset, let's call it product on HDFS which was imported using Sqoop ImportTool as-parquet-file using codec snappy. As result of import, I have 100 files with total 46 GB du, files with diffrrent size (min 11MB, max 1.5GB, avg ~ 500MB). Total count of records a little bit more than 8 billions with 84 columns
I'm doing simple read/repartition/write with Spark using snappy as well and as result I'm getting:
~100 GB output size with the same files count, same codec, same count and same columns.
Code snippet:
val productDF = spark.read.parquet(""/ingest/product/20180202/22-43/"")
productDF
.repartition(100)
.write.mode(org.apache.spark.sql.SaveMode.Overwrite)
.option(""compression"", ""snappy"")
.parquet(""/processed/product/20180215/04-37/read_repartition_write/general"")
Using parquet-tools I have looked into random files from both ingest and processed and they looks as below:
ingest:
creator:                        parquet-mr version 1.5.0-cdh5.11.1 (build ${buildNumber}) 
extra:                          parquet.avro.schema = {""type"":""record"",""name"":""AutoGeneratedSchema"",""doc"":""Sqoop import of QueryResult"",""fields""
and almost all columns looks like
AVAILABLE: OPTIONAL INT64 R:0 D:1
row group 1:                    RC:3640100 TS:36454739 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:172743 SZ:370515/466690/1.26 VC:3640100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 126518400000, max: 1577692800000, num_nulls: 2541633]
processed:
creator:                        parquet-mr version 1.5.0-cdh5.12.0 (build ${buildNumber}) 
extra:                          org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields""
AVAILABLE:                      OPTIONAL INT64 R:0 D:1
...
row group 1:                    RC:6660100 TS:243047789 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:4122795 SZ:4283114/4690840/1.10 VC:6660100 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE ST:[min: -2209136400000, max: 10413820800000, num_nulls: 4444993]
In other hand, without repartition or using coalesce - size remains close to ingest data size.
Going forward, I did following:
read dataset and write it back with   
productDF
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithoutshuffle"")
read dataset, repartition and write it back with 
productDF
  .repartition(500)
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithshuffle"")
As result: 80 GB without and  283 GB with repartition with same # of output files
80GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:456753 SZ:1452623/1452623/1.00 VC:11000100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -1735747200000, max: 2524550400000, num_nulls: 7929352]
283 GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:2800387 SZ:2593838/2593838/1.00 VC:3510100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -2209136400000, max: 10413820800000, num_nulls: 2244255]
It seems, that parquet itself (with encoding?) much reduce size of data even without uncompressed data. How ? :)
I tried to read  uncompressed 80GB, repartition and write back - I've got my 283 GB
The first question for me is why I'm getting bigger size after spark repartitioning/shuffle?
The second is how to efficiently shuffle data in spark to benefit parquet encoding/compression if there any?
In general, I don't want that my data size growing after spark processing, even if I didn't change anything.
Also, I failed to find, is there any configurable compression rate for snappy, e.g. -1 ... -9? As I know, gzip has this, but what is the way to control this rate in Spark/Parquet writer?
Appreciate for any help!
Thanks!
",<apache-spark><apache-spark-sql><parquet><snappy>,3982,0,36,1223,1,12,16,53,15878,0.0,15,2,15,2018-02-18 1:43,2019-06-04 20:54,,471.0,,Intermediate,23,"<apache-spark><apache-spark-sql><parquet><snappy>, Spark + Parquet + Snappy: Overall compression ratio loses after spark shuffles data, Commmunity!
Please help me understand how to get better compression ratio with Spark?
Let me describe case:
I have dataset, let's call it product on HDFS which was imported using Sqoop ImportTool as-parquet-file using codec snappy. As result of import, I have 100 files with total 46 GB du, files with diffrrent size (min 11MB, max 1.5GB, avg ~ 500MB). Total count of records a little bit more than 8 billions with 84 columns
I'm doing simple read/repartition/write with Spark using snappy as well and as result I'm getting:
~100 GB output size with the same files count, same codec, same count and same columns.
Code snippet:
val productDF = spark.read.parquet(""/ingest/product/20180202/22-43/"")
productDF
.repartition(100)
.write.mode(org.apache.spark.sql.SaveMode.Overwrite)
.option(""compression"", ""snappy"")
.parquet(""/processed/product/20180215/04-37/read_repartition_write/general"")
Using parquet-tools I have looked into random files from both ingest and processed and they looks as below:
ingest:
creator:                        parquet-mr version 1.5.0-cdh5.11.1 (build ${buildNumber}) 
extra:                          parquet.avro.schema = {""type"":""record"",""name"":""AutoGeneratedSchema"",""doc"":""Sqoop import of QueryResult"",""fields""
and almost all columns looks like
AVAILABLE: OPTIONAL INT64 R:0 D:1
row group 1:                    RC:3640100 TS:36454739 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:172743 SZ:370515/466690/1.26 VC:3640100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 126518400000, max: 1577692800000, num_nulls: 2541633]
processed:
creator:                        parquet-mr version 1.5.0-cdh5.12.0 (build ${buildNumber}) 
extra:                          org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields""
AVAILABLE:                      OPTIONAL INT64 R:0 D:1
...
row group 1:                    RC:6660100 TS:243047789 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:4122795 SZ:4283114/4690840/1.10 VC:6660100 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE ST:[min: -2209136400000, max: 10413820800000, num_nulls: 4444993]
In other hand, without repartition or using coalesce - size remains close to ingest data size.
Going forward, I did following:
read dataset and write it back with   
productDF
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithoutshuffle"")
read dataset, repartition and write it back with 
productDF
  .repartition(500)
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithshuffle"")
As result: 80 GB without and  283 GB with repartition with same # of output files
80GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:456753 SZ:1452623/1452623/1.00 VC:11000100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -1735747200000, max: 2524550400000, num_nulls: 7929352]
283 GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:2800387 SZ:2593838/2593838/1.00 VC:3510100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -2209136400000, max: 10413820800000, num_nulls: 2244255]
It seems, that parquet itself (with encoding?) much reduce size of data even without uncompressed data. How ? :)
I tried to read  uncompressed 80GB, repartition and write back - I've got my 283 GB
The first question for me is why I'm getting bigger size after spark repartitioning/shuffle?
The second is how to efficiently shuffle data in spark to benefit parquet encoding/compression if there any?
In general, I don't want that my data size growing after spark processing, even if I didn't change anything.
Also, I failed to find, is there any configurable compression rate for snappy, e.g. -1 ... -9? As I know, gzip has this, but what is the way to control this rate in Spark/Parquet writer?
Appreciate for any help!
Thanks!
","<apache-spark><apache-spark-sal><parquet><sappy>, spark + parquet + sappy: overall compress ratio lose spark stuff data, community! pleas help understand get better compress ratio spark? let describe case: dataset, let' call product of import use stoop importtool as-parquet-fig use code sappy. result import, 100 file total 46 go du, file different size (min limb, max 1.go, ave ~ 500mb). total count record little bit 8 billion 84 column i'm simple read/repetition/writ spark use snap well result i'm getting: ~100 go output size file count, code, count columns. code snipped: val produced = spark.read.parquet(""/ingest/product/20180202/22-43/"") produced .repetition(100) .write.mode(org.apache.spark.sal.savemode.overwrite) .option(""compression"", ""sappy"") .parquet(""/processes/product/20180215/04-37/read_repartition_write/general"") use parquet-tool look random file ingest process look below: ingest: creator: parquet-mr version 1.5.0-ch.11.1 (build ${buildnumber}) extra: parquet.are.scheme = {""type"":""record"",""name"":""autogeneratedschema"",""do"":""stoop import queryresult"",""fields"" almost column look like available: option into r:0 d:1 row group 1: re:3640100 to:36454739 offset:4 available: into snap do:0 fro:172743 s:370515/466690/1.26 ve:3640100 end:re,plain_dictionary,bit_pack st:[min: 126518400000, max: 1577692800000, num_nulls: 2541633] processes: creator: parquet-mr version 1.5.0-ch.12.0 (build ${buildnumber}) extra: org.apache.spark.sal.parquet.row.metadata = {""type"":""struck"",""fields"" available: option into r:0 d:1 ... row group 1: re:6660100 to:243047789 offset:4 available: into snap do:0 fro:4122795 s:4283114/4690840/1.10 ve:6660100 end:bit_packed,plain_dictionary,ll st:[min: -2209136400000, max: 10413820800000, num_nulls: 4444993] hand, without repartee use coalesce - size remain close ingest data size. go forward, following: read dataset write back produced .write.mode(org.apache.spark.sal.savemode.overwrite) .option(""compression"", ""none"") .parquet(""/processes/product/20180215/04-37/read_repartition_write/nonewithoutshuffle"") read dataset, repartee write back produced .repetition(500) .write.mode(org.apache.spark.sal.savemode.overwrite) .option(""compression"", ""none"") .parquet(""/processes/product/20180215/04-37/read_repartition_write/nonewithshuffle"") result: 80 go without 283 go repartee # output file 80gb parquet met example: available: into compress do:0 fro:456753 s:1452623/1452623/1.00 ve:11000100 end:re,plain_dictionary,bit_pack st:[min: -1735747200000, max: 2524550400000, num_nulls: 7929352] 283 go parquet met example: available: into compress do:0 fro:2800387 s:2593838/2593838/1.00 ve:3510100 end:re,plain_dictionary,bit_pack st:[min: -2209136400000, max: 10413820800000, num_nulls: 2244255] seems, parquet (with encoding?) much reduce size data even without compress data. ? :) try read compress 80gb, repartee write back - i'v got 283 go first question i'm get bigger size spark repartitioning/scuffle? second effect stuff data spark benefit parquet encoding/compress any? general, want data size grow spark processing, even change anything. also, fail find, configur compress rate sappy, e.g. -1 ... -9? know, grip this, way control rate spark/parquet writer? appreci help! thanks!"
60636473,How to read/write Timestamp in Doobie (Postgres),"How to read/write Timestamp in Doobie?
I have a record class that contains a timestamp field. When I am trying to write it to the database or read it using doobie I get an error Cannot find or construct a Read instance for type.
case class ExampleRecord(data: String, created_at: Timestamp)
val create = sql""create table if not exists example_ts (data TEXT NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP)"".update.run
val insert = Update[ExampleRecord](""insert into example_ts (data, created_at) values (?, ?)"")
  .updateMany(List(
    ExampleRecord(""one"", Timestamp.valueOf(LocalDateTime.now())),
    ExampleRecord(""two"", Timestamp.valueOf(LocalDateTime.now()))
  ))
val select = sql""select data, created_at from example_ts"".query[ExampleRecord].stream
val app = for {
  _ &lt;- create.transact(xa).compile.drain
  _ &lt;- insert.transact(xa).compile.drain
  _ &lt;- select.transact(xa).compile.drain
} yield ()
app.unsafeRunSync()
",<postgresql><scala><doobie>,957,0,18,2021,0,16,26,75,5582,0.0,50,1,15,2020-03-11 12:51,2020-03-11 12:51,2020-03-11 12:51,0.0,0.0,Basic,3,"<postgresql><scala><doobie>, How to read/write Timestamp in Doobie (Postgres), How to read/write Timestamp in Doobie?
I have a record class that contains a timestamp field. When I am trying to write it to the database or read it using doobie I get an error Cannot find or construct a Read instance for type.
case class ExampleRecord(data: String, created_at: Timestamp)
val create = sql""create table if not exists example_ts (data TEXT NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP)"".update.run
val insert = Update[ExampleRecord](""insert into example_ts (data, created_at) values (?, ?)"")
  .updateMany(List(
    ExampleRecord(""one"", Timestamp.valueOf(LocalDateTime.now())),
    ExampleRecord(""two"", Timestamp.valueOf(LocalDateTime.now()))
  ))
val select = sql""select data, created_at from example_ts"".query[ExampleRecord].stream
val app = for {
  _ &lt;- create.transact(xa).compile.drain
  _ &lt;- insert.transact(xa).compile.drain
  _ &lt;- select.transact(xa).compile.drain
} yield ()
app.unsafeRunSync()
","<postgresql><scala><double>, read/writ timestamp door (postures), read/writ timestamp double? record class contain timestamp field. try write database read use door get error cannot find construct read instant type. case class examplerecord(data: string, created_at: timestamp) val great = sal""great table exist example (data text null, created_at timestamp time zone default current_timestamp)"".update.run val insert = update[examplerecord](""insert example (data, created_at) value (?, ?)"") .updatemany(list( examplerecord(""one"", timestamp.value(localdatetime.now())), examplerecord(""two"", timestamp.value(localdatetime.now())) )) val select = sal""select data, created_at examples"".query[examplerecord].stream val pp = { _ &it;- create.transact(a).compile.drain _ &it;- insert.transact(a).compile.drain _ &it;- select.transact(a).compile.drain } yield () pp.unsaferunsync()"
55347251,Cannot select where ip=inet_pton($ip),"I have a unique column in database which is named ip
IP addresses are stored in this column as BINARY(16) (with no collation) after converting them using the PHP function 
$store_ip = inet_pton($ip);
When I try to insert the same IP twice it works fine and fails because it is unique,
But when I try to select the IP it doesn't work and always returns FALSE (not found)
&lt;?php
try {
    $ip = inet_pton($_SERVER['REMOTE_ADDR']);
    $stmt = $db-&gt;prepare(""SELECT * FROM `votes` WHERE ip=?"");
    $stmt-&gt;execute([$ip]);
    $get = $stmt-&gt;fetch();
    if( ! $get){
        echo 'Not found';
    }else{
        echo 'Found';
    }
    // close connection
    $get = null;
    $stmt = null;
} catch (PDOException $e) {
    error_log($e-&gt;getMessage());
}
The part where I insert the IP:
&lt;?php
if( ! filter_var($ip, FILTER_VALIDATE_IP)){
        return FALSE;
}
$ip = inet_pton($_SERVER['REMOTE_ADDR']);
try {
    $stmt = $db-&gt;prepare(""INSERT INTO votes(ip, answer) VALUES(?,?)"");
    $stmt-&gt;execute([$ip, $answer]);
    $stmt = null;
} catch (PDOException $e) {
    return FALSE;
}
",<php><mysql><pdo>,1099,0,39,832,1,15,34,57,1700,0.0,198,3,15,2019-03-25 22:21,2019-03-30 20:50,2019-03-30 20:50,5.0,5.0,Basic,1,"<php><mysql><pdo>, Cannot select where ip=inet_pton($ip), I have a unique column in database which is named ip
IP addresses are stored in this column as BINARY(16) (with no collation) after converting them using the PHP function 
$store_ip = inet_pton($ip);
When I try to insert the same IP twice it works fine and fails because it is unique,
But when I try to select the IP it doesn't work and always returns FALSE (not found)
&lt;?php
try {
    $ip = inet_pton($_SERVER['REMOTE_ADDR']);
    $stmt = $db-&gt;prepare(""SELECT * FROM `votes` WHERE ip=?"");
    $stmt-&gt;execute([$ip]);
    $get = $stmt-&gt;fetch();
    if( ! $get){
        echo 'Not found';
    }else{
        echo 'Found';
    }
    // close connection
    $get = null;
    $stmt = null;
} catch (PDOException $e) {
    error_log($e-&gt;getMessage());
}
The part where I insert the IP:
&lt;?php
if( ! filter_var($ip, FILTER_VALIDATE_IP)){
        return FALSE;
}
$ip = inet_pton($_SERVER['REMOTE_ADDR']);
try {
    $stmt = $db-&gt;prepare(""INSERT INTO votes(ip, answer) VALUES(?,?)"");
    $stmt-&gt;execute([$ip, $answer]);
    $stmt = null;
} catch (PDOException $e) {
    return FALSE;
}
","<pp><myself><do>, cannot select in=inet_pton($in), unique column database name in in address store column binary(16) (with collection) convert use pp function $store_ip = inet_pton($in); try insert in twice work fine fail unique, try select in work away return fall (not found) &it;?pp try { $in = inet_pton($server['remote_addr']); $some = $do-&it;prepare(""select * `votes` in=?""); $some-&it;execute([$in]); $get = $some-&it;fetch(); if( ! $get){ echo 'not found'; }else{ echo 'found'; } // close connect $get = null; $some = null; } catch (pdoexcept $e) { error_log($e-&it;getmessage()); } part insert in: &it;?pp if( ! filter_var($in, filter_validate_ip)){ return false; } $in = inet_pton($server['remote_addr']); try { $some = $do-&it;prepare(""insert votes(in, answer) values(?,?)""); $some-&it;execute([$in, $answer]); $some = null; } catch (pdoexcept $e) { return false; }"
54351783,duplicate key value violates unique constraint - postgres error when trying to create sql table from dask dataframe,"Following on from this question, when I try to create a postgresql table from a dask.dataframe with more than one partition I get the following error:
IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""pg_type_typname_nsp_index""
DETAIL:  Key (typname, typnamespace)=(test1, 2200) already exists.
 [SQL: '\nCREATE TABLE test1 (\n\t""A"" BIGINT, \n\t""B"" BIGINT, \n\t""C"" BIGINT, \n\t""D"" BIGINT, \n\t""E"" BIGINT, \n\t""F"" BIGINT, \n\t""G"" BIGINT, \n\t""H"" BIGINT, \n\t""I"" BIGINT, \n\t""J"" BIGINT, \n\tidx BIGINT\n)\n\n']
You can recreate the error with the following code:
import numpy as np
import dask.dataframe as dd
import dask
import pandas as pd
import sqlalchemy_utils as sqla_utils
import sqlalchemy as sqla
DATABASE_CONFIG = {
    'driver': '',
    'host': '',
    'user': '',
    'password': '',
    'port': 5432,
}
DBNAME = 'dask'
url = '{driver}://{user}:{password}@{host}:{port}/'.format(
        **DATABASE_CONFIG)
db_url = url.rstrip('/') + '/' + DBNAME
# create db if non-existent
if not sqla_utils.database_exists(db_url):
    print('Creating database \'{}\''.format(DBNAME))
    sqla_utils.create_database(db_url)
conn = sqla.create_engine(db_url)
# create pandas df with random numbers
df = pd.DataFrame(np.random.randint(0,40,size=(100, 10)), columns=list('ABCDEFGHIJ'))
# add index so that it can be used as primary key later on
df['idx'] = df.index
# create dask df
ddf = dd.from_pandas(df, npartitions=4)
# Write to psql
dto_sql = dask.delayed(pd.DataFrame.to_sql)
out = [dto_sql(d, 'test', db_url, if_exists='append', index=False, index_label='idx')
       for d in ddf.to_delayed()]
dask.compute(*out)
The code doesn't produce an error if npartitions is set to 1. So I'm guessing it has to do with postgres not being able to handle parallel requests to write to a same sql table...? How can I fix this?
",<python><postgresql><pandas><dask><pandas-to-sql>,1856,1,36,2367,2,30,59,52,27798,0.0,175,3,15,2019-01-24 16:56,2019-03-21 17:39,2019-03-21 17:39,56.0,56.0,Basic,9,"<python><postgresql><pandas><dask><pandas-to-sql>, duplicate key value violates unique constraint - postgres error when trying to create sql table from dask dataframe, Following on from this question, when I try to create a postgresql table from a dask.dataframe with more than one partition I get the following error:
IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""pg_type_typname_nsp_index""
DETAIL:  Key (typname, typnamespace)=(test1, 2200) already exists.
 [SQL: '\nCREATE TABLE test1 (\n\t""A"" BIGINT, \n\t""B"" BIGINT, \n\t""C"" BIGINT, \n\t""D"" BIGINT, \n\t""E"" BIGINT, \n\t""F"" BIGINT, \n\t""G"" BIGINT, \n\t""H"" BIGINT, \n\t""I"" BIGINT, \n\t""J"" BIGINT, \n\tidx BIGINT\n)\n\n']
You can recreate the error with the following code:
import numpy as np
import dask.dataframe as dd
import dask
import pandas as pd
import sqlalchemy_utils as sqla_utils
import sqlalchemy as sqla
DATABASE_CONFIG = {
    'driver': '',
    'host': '',
    'user': '',
    'password': '',
    'port': 5432,
}
DBNAME = 'dask'
url = '{driver}://{user}:{password}@{host}:{port}/'.format(
        **DATABASE_CONFIG)
db_url = url.rstrip('/') + '/' + DBNAME
# create db if non-existent
if not sqla_utils.database_exists(db_url):
    print('Creating database \'{}\''.format(DBNAME))
    sqla_utils.create_database(db_url)
conn = sqla.create_engine(db_url)
# create pandas df with random numbers
df = pd.DataFrame(np.random.randint(0,40,size=(100, 10)), columns=list('ABCDEFGHIJ'))
# add index so that it can be used as primary key later on
df['idx'] = df.index
# create dask df
ddf = dd.from_pandas(df, npartitions=4)
# Write to psql
dto_sql = dask.delayed(pd.DataFrame.to_sql)
out = [dto_sql(d, 'test', db_url, if_exists='append', index=False, index_label='idx')
       for d in ddf.to_delayed()]
dask.compute(*out)
The code doesn't produce an error if npartitions is set to 1. So I'm guessing it has to do with postgres not being able to handle parallel requests to write to a same sql table...? How can I fix this?
","<patron><postgresql><hands><ask><hands-to-sal>, public key value violet unique constraint - poster error try great sal table ask dataframe, follow question, try great postgresql table ask.datafram one partite get follow error: integrityerror: (psycopg2.integrityerror) public key value violet unique constraint ""pg_type_typname_nsp_index"" detail: key (typname, typnamespace)=(test, 2200) already exists. [sal: '\great table test (\n\t""a"" begin, \n\t""b"" begin, \n\t""c"" begin, \n\t""d"" begin, \n\t""e"" begin, \n\t""f"" begin, \n\t""g"" begin, \n\t""h"" begin, \n\t""i"" begin, \n\t""j"" begin, \n\tide begin\n)\n\n'] retreat error follow code: import jump no import ask.datafram did import ask import and pp import sqlalchemy_util sqla_util import sqlalchemi sila database_config = { 'driver': '', 'host': '', 'user': '', 'password': '', 'port': 5432, } name = 'ask' curl = '{driver}://{user}:{password}@{host}:{port}/'.format( **database_config) db_url = curl.strip('/') + '/' + name # great do non-exist sqla_utils.database_exists(db_url): print('great database \'{}\''.format(name)) sqla_utils.create_database(db_url) corn = sila.create_engine(db_url) # great and of random number of = pp.dataframe(no.random.radiant(0,40,size=(100, 10)), columns=list('abcdefghij')) # add index use primary key later of['ix'] = of.index # great ask of def = did.from_pandas(of, partitions=4) # write pool dto_sql = ask.delayed(pp.dataframe.tonsil) = [dto_sql(d, 'test', db_url, if_exists='happened', index=false, index_label='ix') def.to_delayed()] ask.compute(*out) code produce error partite set 1. i'm guess poster all hand parallel request write sal table...? fix this?"
55066509,Error in phpmyadmin Warning in ./libraries/plugin_interface.lib.php#551,"Error:
  Warning in ./libraries/plugin_interface.lib.php#551 count(): Parameter
  must be an array or an object that implements Countable
Backtrace:
./libraries/display_export.lib.php#381: PMA_pluginGetOptions(
string 'Export',
array,
)
./libraries/display_export.lib.php#883: PMA_getHtmlForExportOptionsFormat(array)
./libraries/display_export.lib.php#1099: PMA_getHtmlForExportOptions(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
array,
integer 0,
)
./tbl_export.php#143: PMA_getExportDisplay(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
integer 0,
string '',
)
How can I fix it?
",<mysql><sql><phpmyadmin>,664,0,23,153,1,1,7,55,20468,0.0,1,3,15,2019-03-08 15:40,2019-03-08 15:50,2019-12-08 20:41,0.0,275.0,Basic,13,"<mysql><sql><phpmyadmin>, Error in phpmyadmin Warning in ./libraries/plugin_interface.lib.php#551, Error:
  Warning in ./libraries/plugin_interface.lib.php#551 count(): Parameter
  must be an array or an object that implements Countable
Backtrace:
./libraries/display_export.lib.php#381: PMA_pluginGetOptions(
string 'Export',
array,
)
./libraries/display_export.lib.php#883: PMA_getHtmlForExportOptionsFormat(array)
./libraries/display_export.lib.php#1099: PMA_getHtmlForExportOptions(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
array,
integer 0,
)
./tbl_export.php#143: PMA_getExportDisplay(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
integer 0,
string '',
)
How can I fix it?
","<myself><sal><phpmyadmin>, error phpmyadmin warn ./libraries/plugin_interface.limb.pp#551, error: warn ./libraries/plugin_interface.limb.pp#551 count(): parapet must array object implement countabl backtrace: ./libraries/display_export.limb.pp#381: pma_plugingetoptions( string 'export', array, ) ./libraries/display_export.limb.pp#883: pma_gethtmlforexportoptionsformat(array) ./libraries/display_export.limb.pp#1099: pma_gethtmlforexportoptions( string 'table', string 'bpapluswpdb', string 'wp_commentmeta', string '', inter 0, array, inter 0, ) ./tbl_export.pp#143: pma_getexportdisplay( string 'table', string 'bpapluswpdb', string 'wp_commentmeta', string '', inter 0, inter 0, string '', ) fix it?"
48999379,"psycopg2.OperationalError: FATAL: password authentication failed for user ""<my UNIX user>""","I am a fairly new to web developement.
First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it.
It works well locally with PostgreSQL but I can't make it work on my server.
It seems like it tries to connect to Postgres with my Unix user. 
Why would my server try to do that?
I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations:
Settings.py + python-decouple:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
This is the error message I get each time I try to ./manage.py migrate
'myportfolio' is my Unix user name, the database username is different:
Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
I tried to: 
delete my django code, re install 
delete/purge postgres and reinstall
modify pg_hba.conf local to trust
At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?
",<python><django><postgresql><ubuntu-16.04>,5269,0,67,1560,4,18,27,42,48293,0.0,137,8,15,2018-02-27 0:41,2018-02-27 1:47,2018-02-27 12:36,0.0,0.0,Basic,13,"<python><django><postgresql><ubuntu-16.04>, psycopg2.OperationalError: FATAL: password authentication failed for user ""<my UNIX user>"", I am a fairly new to web developement.
First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it.
It works well locally with PostgreSQL but I can't make it work on my server.
It seems like it tries to connect to Postgres with my Unix user. 
Why would my server try to do that?
I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations:
Settings.py + python-decouple:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
This is the error message I get each time I try to ./manage.py migrate
'myportfolio' is my Unix user name, the database username is different:
Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
I tried to: 
delete my django code, re install 
delete/purge postgres and reinstall
modify pg_hba.conf local to trust
At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?
","<patron><django><postgresql><bunt-16.04>, psycopg2.operationalerror: fatal: password authentic fail user ""<mi unit user>"", fairly new web development. first deploy static west up (bunt 16.04) without problem try add blow pp it. work well local postgresql can't make work server. seem like try connect poster unit user. would server try that? great database owner via poster user, match login inform settings.by, expect psycopg2 try connect database use login information: settings.i + patron-couple: database = { 'default': { 'engine': 'django.do.backed.postgresql_psycopg2', 'name': confirm ('name'), 'user': confirm ('user'), 'password': confirm ('password'), 'host': 'localhost', 'port': '', } } error message get time try ./manage.i migrate 'portfolio' unit user name, database usernam different: traceback (most recent call last): file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 216, ensure_connect self.connect() file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 194, connect self.connect = self.get_new_connection(conn_params) file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/postgresql/base.by"", line 168, get_new_connect connect = database.connect(**conn_params) file ""/home/portfolio/limb/python3.5/site-packages/psycopg2/__init__.by"", line 130, connect corn = connect(don, connection_factory=connection_factory, **kwasync) psycopg2.operationalerror: fatal: password authentic fail user ""portfolio"" fatal: password authentic fail user ""portfolio"" except direct cause follow exception: traceback (most recent call last): file ""./manage.by"", line 15, &it;module&it; execute_from_command_line(says.are) file ""/home/portfolio/limb/python3.5/site-packages/django/core/management/__init__.by"", line 371, execute_from_command_lin utility.execute() file ""/home/portfolio/limb/python3.5/site-packages/django/core/management/__init__.by"", line 365, execute self.fetch_command(subcommand).run_from_argv(self.are) file ""/home/portfolio/limb/python3.5/site-packages/django/core/management/base.by"", line 288, run_from_argv self.execute(*arms, **cmd_options) file ""/home/portfolio/limb/python3.5/site-packages/django/core/management/base.by"", line 335, execute output = self.handle(*arms, **option) file ""/home/portfolio/limb/python3.5/site-packages/django/core/management/commands/migrate.by"", line 79, hand executor = migrationexecutor(connection, self.migration_progress_callback) file ""/home/portfolio/limb/python3.5/site-packages/django/do/migrations/executor.by"", line 18, __init__ self.load = migrationloader(self.connection) file ""/home/portfolio/limb/python3.5/site-packages/django/do/migrations/leader.by"", line 49, __init__ self.build_graph() file ""/home/portfolio/limb/python3.5/site-packages/django/do/migrations/leader.by"", line 206, build_graph self.applied_migr = recorder.applied_migrations() file ""/home/portfolio/limb/python3.5/site-packages/django/do/migrations/recorder.by"", line 61, applied_migr self.has_table(): file ""/home/portfolio/limb/python3.5/site-packages/django/do/migrations/recorder.by"", line 44, has_tabl return self.migration.met.but self.connection.introspective.table_names(self.connection.curses()) file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 255, curses return self._cursor() file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 232, _cursor self.ensure_connection() file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 216, ensure_connect self.connect() file ""/home/portfolio/limb/python3.5/site-packages/django/do/still.by"", line 89, __exit__ rays dj_exc_value.with_traceback(traceback) exc_valu file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 216, ensure_connect self.connect() file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/base/base.by"", line 194, connect self.connect = self.get_new_connection(conn_params) file ""/home/portfolio/limb/python3.5/site-packages/django/do/backed/postgresql/base.by"", line 168, get_new_connect connect = database.connect(**conn_params) file ""/home/portfolio/limb/python3.5/site-packages/psycopg2/__init__.by"", line 130, connect corn = connect(don, connection_factory=connection_factory, **kwasync) django.do.still.operationalerror: fatal: password authentic fail user ""portfolio"" fatal: password authentic fail user ""portfolio"" try to: delete django code, instal delete/pure poster rental modify pg_hba.cone local trust one point great django humerus call 'portfolio' unit user: could great problem ?"
64905397,Sequelize missing FROM-clause entry for table Postgres,"I am trying to filter my DataRow objects by where the included DataPoints data is loc and sortOrder is 2. Below is the query I am trying. I keep getting the following error.
SequelizeDatabaseError: missing FROM-clause entry for table &quot;dataPoints&quot;
I have tried setting required: true and duplicating: false with no luck. I tried these in both the DataRows include as well as DataPoints.
    attributes: ['id', 'name', 'labId'],
    include: [{
      as: 'dataColumns',
      model: DataColumn,
      attributes: ['id', 'name', 'sortOrder', 'dataType', 'isDate', 'isLocation'],
    }, {
      as: 'dataSets',
      model: Study,
      attributes: ['id', 'name'],
    }, {
      as: 'dataRows',
      model: DataRow,
      attributes: ['id'],
      where: {
        [Op.and]: [
          {
            '$dataPoints.data$': 'loc',
          },
          {
            '$dataPoints.sortOrder$': 2,
          },
        ],
      },
      include: [{
        as: 'dataPoints',
        model: DataPoint,
        attributes: ['id', 'sortOrder', 'data'],
      }],
    },],
    order: [
      [{ model: DataColumn, as: 'dataColumns' }, 'sortOrder', 'ASC'],
      [{ model: DataRow, as: 'dataRows' }, { model: DataPoint, as: 'dataPoints' }, 'sortOrder', 'ASC'],
    ],
  }
",<node.js><database><postgresql><sequelize.js>,1272,0,39,183,1,1,5,53,14105,,0,5,15,2020-11-19 4:23,2021-06-24 9:00,,217.0,,Basic,10,"<node.js><database><postgresql><sequelize.js>, Sequelize missing FROM-clause entry for table Postgres, I am trying to filter my DataRow objects by where the included DataPoints data is loc and sortOrder is 2. Below is the query I am trying. I keep getting the following error.
SequelizeDatabaseError: missing FROM-clause entry for table &quot;dataPoints&quot;
I have tried setting required: true and duplicating: false with no luck. I tried these in both the DataRows include as well as DataPoints.
    attributes: ['id', 'name', 'labId'],
    include: [{
      as: 'dataColumns',
      model: DataColumn,
      attributes: ['id', 'name', 'sortOrder', 'dataType', 'isDate', 'isLocation'],
    }, {
      as: 'dataSets',
      model: Study,
      attributes: ['id', 'name'],
    }, {
      as: 'dataRows',
      model: DataRow,
      attributes: ['id'],
      where: {
        [Op.and]: [
          {
            '$dataPoints.data$': 'loc',
          },
          {
            '$dataPoints.sortOrder$': 2,
          },
        ],
      },
      include: [{
        as: 'dataPoints',
        model: DataPoint,
        attributes: ['id', 'sortOrder', 'data'],
      }],
    },],
    order: [
      [{ model: DataColumn, as: 'dataColumns' }, 'sortOrder', 'ASC'],
      [{ model: DataRow, as: 'dataRows' }, { model: DataPoint, as: 'dataPoints' }, 'sortOrder', 'ASC'],
    ],
  }
","<node.is><database><postgresql><sequelae.is>, sequel miss from-class entry table postures, try filter datarow object include datapoint data low sorter 2. query trying. keep get follow error. sequelizedatabaseerror: miss from-class entry table &quit;datapoints&quit; try set required: true implicating: fall luck. try datarow include well datapoints. attributes: ['id', 'name', 'laid'], include: [{ as: 'datacolumns', model: datacolumn, attributes: ['id', 'name', 'sortorder', 'datatype', 'state', 'dislocation'], }, { as: 'datasets', model: study, attributes: ['id', 'name'], }, { as: 'datarows', model: datarow, attributes: ['id'], where: { [op.and]: [ { '$datapoints.data$': 'low', }, { '$datapoints.sortorder$': 2, }, ], }, include: [{ as: 'datapoints', model: datapoint, attributes: ['id', 'sortorder', 'data'], }], },], order: [ [{ model: datacolumn, as: 'datacolumns' }, 'sortorder', 'as'], [{ model: datarow, as: 'datarows' }, { model: datapoint, as: 'datapoints' }, 'sortorder', 'as'], ], }"
50388490,Analyze SQL query in DBeaver,"I would like to get some info about a running query to analyze it. In PgAdmin 3 I could at least use 'Explain Query', while in DBeaver, after clicking 'Explain Execution Plan', nothing happens.
How to obtain any information about query in DBeaver?
@Edit
Sorry if this question seems too broad. I don't expect explanation of how to analyze the query, I just would like to know if it is possible to open an analyzer in DBeaver and how to do it.
",<sql><postgresql><dbeaver>,443,0,0,5196,7,50,85,41,46892,,454,2,15,2018-05-17 9:53,2021-01-26 7:10,,985.0,,Basic,3,"<sql><postgresql><dbeaver>, Analyze SQL query in DBeaver, I would like to get some info about a running query to analyze it. In PgAdmin 3 I could at least use 'Explain Query', while in DBeaver, after clicking 'Explain Execution Plan', nothing happens.
How to obtain any information about query in DBeaver?
@Edit
Sorry if this question seems too broad. I don't expect explanation of how to analyze the query, I just would like to know if it is possible to open an analyzer in DBeaver and how to do it.
","<sal><postgresql><beaver>, analyze sal query beaver, would like get into run query analyze it. pgadmin 3 could least use 'explain query', beaver, click 'explain execute plan', not happens. obtain inform query beaver? @edit sorry question seem broad. expect explain analyze query, would like know possible open analyze beaver it."
52589849,"""Create database if not exists"" in postgres","I am new to postgreSQL and I am trying to create a schema file which will contain all the scripts required to create the database and required tables. The way I used to do this for SQl server was to check if the database exists and then run the necessary scripts. 
The following script in postgreSQL throws an error saying, ""CREATE DATABASE cannot be executed from a function or multi-command string""
do $$
begin
If not exists (select 1 from pg_database where datname = 'TestDB')
Then
 CREATE DATABASE ""TestDB"";
end if;
end
$$
I created a postgres database dump file by exporting a backup of the database but that contains,
Drop Database ""TestDB""
Create Database ""TestDB""
which means everytime I run the schema file, the database would be dropped and recreated and it would be a problem if there is data present in the database.
How do I check if the database exists in postgreSQL without having to drop the database and recreate it everytime I run the file please? 
Thanks in Advance 
",<postgresql><postgresql-9.3><pgadmin-4>,986,0,11,313,1,4,21,79,23684,,15,1,15,2018-10-01 11:11,2020-12-16 11:15,,807.0,,Basic,9,"<postgresql><postgresql-9.3><pgadmin-4>, ""Create database if not exists"" in postgres, I am new to postgreSQL and I am trying to create a schema file which will contain all the scripts required to create the database and required tables. The way I used to do this for SQl server was to check if the database exists and then run the necessary scripts. 
The following script in postgreSQL throws an error saying, ""CREATE DATABASE cannot be executed from a function or multi-command string""
do $$
begin
If not exists (select 1 from pg_database where datname = 'TestDB')
Then
 CREATE DATABASE ""TestDB"";
end if;
end
$$
I created a postgres database dump file by exporting a backup of the database but that contains,
Drop Database ""TestDB""
Create Database ""TestDB""
which means everytime I run the schema file, the database would be dropped and recreated and it would be a problem if there is data present in the database.
How do I check if the database exists in postgreSQL without having to drop the database and recreate it everytime I run the file please? 
Thanks in Advance 
","<postgresql><postgresql-9.3><pgadmin-4>, ""great database exists"" postures, new postgresql try great scheme file contain script require great database require tables. way use sal server check database exist run necessary script. follow script postgresql throw error saying, ""great database cannot execute function multi-command string"" $$ begin exist (select 1 pg_databas data = 'test') great database ""test""; end if; end $$ great poster database dump file export back database contains, drop database ""test"" great database ""test"" mean everytim run scheme file, database would drop retreat would problem data present database. check database exist postgresql without drop database retreat everytim run file please? thank advance"
57059458,SQL: Most Overdue pair of numbers?,"We have a this table and random data load: 
CREATE TABLE [dbo].[webscrape](
    [id] [int] IDENTITY(1,1) NOT NULL,
    [date] [date] NULL,
    [value1] [int] NULL,
    [value2] [int] NULL,
    [value3] [int] NULL,
    [value4] [int] NULL,
    [value5] [int] NULL,
    [sumnumbers] AS ([value1]+[value2]+[value3]+[value4]+[value5])
) ON [PRIMARY]
declare @date date = '1990-01-01',
@endDate date = Getdate()
while @date&lt;=@enddate
begin
insert into [dbo].[webscrape](date,value1,value2,value3,value4,value5)
SELECT @date date,FLOOR(RAND()*(36-1)+1) value1,
FLOOR(RAND()*(36-1)+1) value2,
FLOOR(RAND()*(36-1)+1) value3,
FLOOR(RAND()*(36-1)+1) value4,
FLOOR(RAND()*(36-1)+1) value5
set @date = DATEADD(day,1,@date)
end
select * from [dbo].[webscrape] 
In SQL how can we return pair of values that have gone the longest without occurring on a given date?
And (if you happen to know) in Power BI Q&amp;A NLP, how do we map so that so we can ask in natural language ""when have the most overdue pairs occurred?""
Overdue being the pair of numbers with the longest stretch of time since occurring as of the given date. 
UPDATE:  I am trying this very ugly code.  Any ideas: 
  select *
    from (
      select date,value1 number1,value2 number2 from webscrape union all  
      select date,value1,value3 from webscrape union all
      select date,value1,value4 from webscrape union all
      select date,value1,value5 from webscrape union all
      select date,value2,value3 from webscrape union all
      select date,value2,value4 from webscrape union all
      select date,value2,value5 from webscrape union all
      select date,value3,value4 from webscrape union all
      select date,value3,value5 from webscrape union all
      select date,value4,value5 from webscrape 
    ) t order by date
    ----------------------------------
    select t.number1,t.number2, count(*)
     as counter
    from (
      select value1 number1,value2 number2 from webscrape union all  
      select value1,value3 from webscrape union all
      select value1,value4  from webscrape union all
      select value1,value5 from webscrape union all
      select value2,value3 from webscrape union all
      select value2,value4  from webscrape union all
      select value2,value5 from webscrape union all
      select value3,value4  from webscrape union all
      select value3,value5 from webscrape union all
      select value4,value5 from webscrape 
    ) t
group by t.number1,number2
order by counter
Thanks for any help.
",<sql><sql-server><t-sql><nlp><powerbi>,2503,0,63,1667,9,38,73,44,425,0.0,82,1,15,2019-07-16 14:20,2019-07-19 18:09,2019-07-19 18:09,3.0,3.0,Basic,9,"<sql><sql-server><t-sql><nlp><powerbi>, SQL: Most Overdue pair of numbers?, We have a this table and random data load: 
CREATE TABLE [dbo].[webscrape](
    [id] [int] IDENTITY(1,1) NOT NULL,
    [date] [date] NULL,
    [value1] [int] NULL,
    [value2] [int] NULL,
    [value3] [int] NULL,
    [value4] [int] NULL,
    [value5] [int] NULL,
    [sumnumbers] AS ([value1]+[value2]+[value3]+[value4]+[value5])
) ON [PRIMARY]
declare @date date = '1990-01-01',
@endDate date = Getdate()
while @date&lt;=@enddate
begin
insert into [dbo].[webscrape](date,value1,value2,value3,value4,value5)
SELECT @date date,FLOOR(RAND()*(36-1)+1) value1,
FLOOR(RAND()*(36-1)+1) value2,
FLOOR(RAND()*(36-1)+1) value3,
FLOOR(RAND()*(36-1)+1) value4,
FLOOR(RAND()*(36-1)+1) value5
set @date = DATEADD(day,1,@date)
end
select * from [dbo].[webscrape] 
In SQL how can we return pair of values that have gone the longest without occurring on a given date?
And (if you happen to know) in Power BI Q&amp;A NLP, how do we map so that so we can ask in natural language ""when have the most overdue pairs occurred?""
Overdue being the pair of numbers with the longest stretch of time since occurring as of the given date. 
UPDATE:  I am trying this very ugly code.  Any ideas: 
  select *
    from (
      select date,value1 number1,value2 number2 from webscrape union all  
      select date,value1,value3 from webscrape union all
      select date,value1,value4 from webscrape union all
      select date,value1,value5 from webscrape union all
      select date,value2,value3 from webscrape union all
      select date,value2,value4 from webscrape union all
      select date,value2,value5 from webscrape union all
      select date,value3,value4 from webscrape union all
      select date,value3,value5 from webscrape union all
      select date,value4,value5 from webscrape 
    ) t order by date
    ----------------------------------
    select t.number1,t.number2, count(*)
     as counter
    from (
      select value1 number1,value2 number2 from webscrape union all  
      select value1,value3 from webscrape union all
      select value1,value4  from webscrape union all
      select value1,value5 from webscrape union all
      select value2,value3 from webscrape union all
      select value2,value4  from webscrape union all
      select value2,value5 from webscrape union all
      select value3,value4  from webscrape union all
      select value3,value5 from webscrape union all
      select value4,value5 from webscrape 
    ) t
group by t.number1,number2
order by counter
Thanks for any help.
","<sal><sal-server><t-sal><nap><power>, sal: overdue pair numbers?, table random data load: great table [do].[webscrape]( [id] [in] identity(1,1) null, [date] [date] null, [value] [in] null, [value] [in] null, [value] [in] null, [value] [in] null, [value] [in] null, [sumnumbers] ([value]+[value]+[value]+[value]+[value]) ) [primary] declare @date date = '1990-01-01', @enddat date = sedate() @date&it;=@end begin insert [do].[webscrape](date,value,value,value,value,value) select @date date,floor(and()*(36-1)+1) value, floor(and()*(36-1)+1) value, floor(and()*(36-1)+1) value, floor(and()*(36-1)+1) value, floor(and()*(36-1)+1) value set @date = dated(day,1,@date) end select * [do].[webscrape] sal return pair value gone longest without occur given date? (if happen know) power i q&amp;a nap, map ask nature language ""when overdue pair occurred?"" overdue pair number longest stretch time since occur given date. update: try ugly code. ideas: select * ( select date,value number,value number webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap union select date,value,value webscrap ) order date ---------------------------------- select t.number,t.number, count(*) counter ( select value number,value number webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap union select value,value webscrap ) group t.number,number order counter thank help."
54515875,How can I update a table to insert decimal points at a fixed position in numbers?,"I am using Microsoft SQL Server 2014 and have a table with three columns and the field data type is Decimal(38,0).
I want to update each row of my table to insert a decimal point after the first two digits. For example, I want 123456 to become 12.3456. The numbers are different lengths; some are five digits, some are seven digits, etc.
My table is:
+-------------+-------+-------+
| ID          |   X   |   Y   |
+-------------+-------+-------+
| 1200        | 321121| 345000|
| 1201        | 564777| 4145  |
| 1202        | 4567  | 121444|
| 1203        | 12747 | 789887|
| 1204        | 489899| 124778|
+-------------+-------+-------+
And I want to change this to:
+-------------+--------+--------+
| ID          |   X    |   Y    |
+-------------+--------+--------+
| 1200        | 32.1121| 34.5000|
| 1201        | 56.4777| 41.45  |
| 1202        | 45.67  | 12.1444|
| 1203        | 12.747 | 78.9887|
| 1204        | 48.9899| 12.4778|
+-------------+--------+--------+
My code is:
Update [dbo].[UTM]
     SET [X] = STUFF([X],3,0,'.')
         [Y] = STUFF([X],3,0,'.')
And I tried this:
BEGIN
DECLARE @COUNT1 int;
DECLARE @COUNT2 int;
DECLARE @TEMP_X VARCHAR(255);
DECLARE @TEMP_Y VARCHAR(255);
DECLARE @TEMP_main VARCHAR(255);
SELECT @COUNT1 = COUNT(*) FROM [UTM];
SET @COUNT2 = 0;
    WHILE(@COUNT2&lt;@COUNT1)
    BEGIN
        SET @TEMP_main = (SELECT [id] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_X = (SELECT [X] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_Y = (SELECT [Y] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        UPDATE [dbo].[UTM]
           SET [X] = CONVERT(decimal(38,0),STUFF(@TEMP_X,3,0,'.'))
              ,[Y] = CONVERT(decimal(38,0),STUFF(@TEMP_Y,3,0,'.'))
           WHERE [id] = @TEMP_main;
        SET @COUNT2 = @COUNT2  +  1
    END
END
",<sql><sql-server><numbers><decimal><sql-server-2014>,1904,0,49,492,1,6,21,74,4952,0.0,126,4,15,2019-02-04 12:13,2019-02-04 12:15,2019-02-04 12:32,0.0,0.0,Basic,9,"<sql><sql-server><numbers><decimal><sql-server-2014>, How can I update a table to insert decimal points at a fixed position in numbers?, I am using Microsoft SQL Server 2014 and have a table with three columns and the field data type is Decimal(38,0).
I want to update each row of my table to insert a decimal point after the first two digits. For example, I want 123456 to become 12.3456. The numbers are different lengths; some are five digits, some are seven digits, etc.
My table is:
+-------------+-------+-------+
| ID          |   X   |   Y   |
+-------------+-------+-------+
| 1200        | 321121| 345000|
| 1201        | 564777| 4145  |
| 1202        | 4567  | 121444|
| 1203        | 12747 | 789887|
| 1204        | 489899| 124778|
+-------------+-------+-------+
And I want to change this to:
+-------------+--------+--------+
| ID          |   X    |   Y    |
+-------------+--------+--------+
| 1200        | 32.1121| 34.5000|
| 1201        | 56.4777| 41.45  |
| 1202        | 45.67  | 12.1444|
| 1203        | 12.747 | 78.9887|
| 1204        | 48.9899| 12.4778|
+-------------+--------+--------+
My code is:
Update [dbo].[UTM]
     SET [X] = STUFF([X],3,0,'.')
         [Y] = STUFF([X],3,0,'.')
And I tried this:
BEGIN
DECLARE @COUNT1 int;
DECLARE @COUNT2 int;
DECLARE @TEMP_X VARCHAR(255);
DECLARE @TEMP_Y VARCHAR(255);
DECLARE @TEMP_main VARCHAR(255);
SELECT @COUNT1 = COUNT(*) FROM [UTM];
SET @COUNT2 = 0;
    WHILE(@COUNT2&lt;@COUNT1)
    BEGIN
        SET @TEMP_main = (SELECT [id] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_X = (SELECT [X] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_Y = (SELECT [Y] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        UPDATE [dbo].[UTM]
           SET [X] = CONVERT(decimal(38,0),STUFF(@TEMP_X,3,0,'.'))
              ,[Y] = CONVERT(decimal(38,0),STUFF(@TEMP_Y,3,0,'.'))
           WHERE [id] = @TEMP_main;
        SET @COUNT2 = @COUNT2  +  1
    END
END
","<sal><sal-server><numbers><denial><sal-server-2014>, update table insert devil point fix post numbers?, use microsoft sal server 2014 table three column field data type denial(38,0). want update row table insert devil point first two digits. example, want 123456 become 12.3456. number differ lengths; five digits, seven digits, etc. table is: +-------------+-------+-------+ | id | x | | +-------------+-------+-------+ | 1200 | 321121| 345000| | 1201 | 564777| 4145 | | 1202 | 4567 | 121444| | 1203 | 12747 | 789887| | 1204 | 489899| 124778| +-------------+-------+-------+ want change to: +-------------+--------+--------+ | id | x | | +-------------+--------+--------+ | 1200 | 32.1121| 34.5000| | 1201 | 56.4777| 41.45 | | 1202 | 45.67 | 12.1444| | 1203 | 12.747 | 78.9887| | 1204 | 48.9899| 12.4778| +-------------+--------+--------+ code is: update [do].[tm] set [x] = stuff([x],3,0,'.') [y] = stuff([x],3,0,'.') try this: begin declare @count in; declare @count in; declare @temper varchar(255); declare @temper varchar(255); declare @temp_main varchar(255); select @count = count(*) [tm]; set @count = 0; while(@count&it;@count) begin set @temp_main = (select [id] [tm] order [id] desk offset @count row fetch next 1 row only); set @temper = (select [x] [tm] order [id] desk offset @count row fetch next 1 row only); set @temper = (select [y] [tm] order [id] desk offset @count row fetch next 1 row only); update [do].[tm] set [x] = convert(denial(38,0),stuff(@temper,3,0,'.')) ,[y] = convert(denial(38,0),stuff(@empty,3,0,'.')) [id] = @temp_main; set @count = @count + 1 end end"
55760416,In OLAP cube wrong Grand Total when attribute is filtered,"A user trying to check the Sales Amount per Salesperson. Sample data:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750
Grand Total:          2250
It looks fine, but we have the following hierarchy Company &gt; Class &gt; Group &gt; Subgroup in the cube and if a user tries to use this hierarchy in filters - Grand Total fails (if any attribute is unchecked in this hierarchy). Sample:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
I've noticed the same problem before when we tried to filter Date attribute, if not every day of the month was selected it shown wrong Grand Total too.
Have you an idea why it happens and how to fix it?
Sales Amount is physical measure (not calculated measure), it is selected from SQL view (the same happens with every fact). 
I've asked the same question here, but nobody could answer it.
I've tried to delete all MDX calculations (scopes), but still Grand Total was incorrect. 
EDIT
I've noticed that the problem occurs when filtering like that:
1 element selected from the first level of the hierarchy, 1 element from 2nd level and 1 element from the 3rd level of hierarchy as in the image above.
If the 3rd level isn't filtered it shows good Grand Total.
EDIT 2
I've tried to trace on SSAS, it returns exactly the same output as in Excel. It generated the following MDX when using Salesperson dimension on the rows:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS, 
NON EMPTY { ([Salesperson].[Salesperson].[Salesperson].ALLMEMBERS ) } 
DIMENSION PROPERTIES MEMBER_CAPTION, 
MEMBER_UNIQUE_NAME ON ROWS FROM ( 
SELECT ( {  [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
            [Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
            [Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } 
) ON COLUMNS FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) 
WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
This MDX generated without Salesperson dimension:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS 
FROM ( SELECT ( { [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
[Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
[Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) ON COLUMNS 
FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
I've noticed even if I'm not using any dimension on the rows (in samples above I've used Salesperson dimension) it shows wrong Grand Total.
For example it shows:
Sales Amount 
350
And when using Salesperson dimension on the rows:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
",<sql-server><excel><sql-server-2008-r2><pivot-table><ssas>,3329,5,34,848,4,15,42,74,2853,0.0,68,1,15,2019-04-19 10:27,2019-05-03 11:13,,14.0,,Advanced,32,"<sql-server><excel><sql-server-2008-r2><pivot-table><ssas>, In OLAP cube wrong Grand Total when attribute is filtered, A user trying to check the Sales Amount per Salesperson. Sample data:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750
Grand Total:          2250
It looks fine, but we have the following hierarchy Company &gt; Class &gt; Group &gt; Subgroup in the cube and if a user tries to use this hierarchy in filters - Grand Total fails (if any attribute is unchecked in this hierarchy). Sample:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
I've noticed the same problem before when we tried to filter Date attribute, if not every day of the month was selected it shown wrong Grand Total too.
Have you an idea why it happens and how to fix it?
Sales Amount is physical measure (not calculated measure), it is selected from SQL view (the same happens with every fact). 
I've asked the same question here, but nobody could answer it.
I've tried to delete all MDX calculations (scopes), but still Grand Total was incorrect. 
EDIT
I've noticed that the problem occurs when filtering like that:
1 element selected from the first level of the hierarchy, 1 element from 2nd level and 1 element from the 3rd level of hierarchy as in the image above.
If the 3rd level isn't filtered it shows good Grand Total.
EDIT 2
I've tried to trace on SSAS, it returns exactly the same output as in Excel. It generated the following MDX when using Salesperson dimension on the rows:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS, 
NON EMPTY { ([Salesperson].[Salesperson].[Salesperson].ALLMEMBERS ) } 
DIMENSION PROPERTIES MEMBER_CAPTION, 
MEMBER_UNIQUE_NAME ON ROWS FROM ( 
SELECT ( {  [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
            [Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
            [Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } 
) ON COLUMNS FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) 
WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
This MDX generated without Salesperson dimension:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS 
FROM ( SELECT ( { [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
[Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
[Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) ON COLUMNS 
FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
I've noticed even if I'm not using any dimension on the rows (in samples above I've used Salesperson dimension) it shows wrong Grand Total.
For example it shows:
Sales Amount 
350
And when using Salesperson dimension on the rows:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
","<sal-server><expel><sal-server-2008-re><pilot-table><seas>, flap cure wrong grand total attribute filtered, user try check sale amount per salesperson. sample data: salesperson sale amount 001 1000 002 500 003 750 grand total: 2250 look fine, follow hierarchy company &it; class &it; group &it; subgroup cure user try use hierarchy filter - grand total fail (if attribute check hierarchy). sample: salesperson sale amount 001 1000 002 500 003 750 grand total: 350 i'v notice problem try filter date attribute, every day month select shown wrong grand total too. idea happen fix it? sale amount physics measure (not call measure), select sal view (the happen every fact). i'v ask question here, nobody could answer it. i'v try delete mix call (scope), still grand total incorrect. edit i'v notice problem occur filter like that: 1 element select first level hierarchy, 1 element and level 1 element rd level hierarchy image above. rd level filter show good grand total. edit 2 i'v try trace seas, return exactly output expel. genet follow mix use salesperson times rows: select non empty { [measures].[sal amount] } columns, non empty { ([salesperson].[salesperson].[salesperson].allmemb ) } times property member_caption, member_unique_nam row ( select ( { [item].[class - group - subgroup].[class].&amp;[xxx]&amp;[1.], [item].[class - group - subgroup].[group].&amp;[xxx]&amp;[2.]&amp;[2.2.], [item].[class - group - subgroup].[subgroup].&amp;[xxx]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) column ( select ( { [company].[company].&amp;[xxx] } ) column [sales])) ( [company].[company].&amp;[xxx], [item].[class - group - subgroup].currentmemb ) cell property value, back_color, fore_color, formatted_value, format_string, font_name, font_size, font_flag mix genet without salesperson dimension: select non empty { [measures].[sal amount] } column ( select ( { [item].[class - group - subgroup].[class].&amp;[xxx]&amp;[1.], [item].[class - group - subgroup].[group].&amp;[xxx]&amp;[2.]&amp;[2.2.], [item].[class - group - subgroup].[subgroup].&amp;[xxx]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) column ( select ( { [company].[company].&amp;[xxx] } ) column [sales])) ( [company].[company].&amp;[xxx], [item].[class - group - subgroup].currentmemb ) cell property value, back_color, fore_color, formatted_value, format_string, font_name, font_size, font_flag i'v notice even i'm use times row (in sample i'v use salesperson dimension) show wrong grand total. example shows: sale amount 350 use salesperson times rows: salesperson sale amount 001 1000 002 500 003 750 grand total: 350"
64065118,What extra one gets by selecting Azure SQL Managed Instance vis-a-vis Azure SQL DB PaaS,"I would like to know what extra benefits one get by choosing Azure SQL Managed Instance compared to Azure SQL DB PaaS. I know SQL Managed Instance is offered as a vCore based purchasing model only. Apart from this what is the extra add on and benefits that one gets over the other. Any reply would be appreciated.
",<azure><azure-sql-database><azure-sql-managed-instance>,314,0,0,1967,2,24,46,45,7678,,0,3,15,2020-09-25 13:29,2020-09-26 17:43,,1.0,,Basic,3,"<azure><azure-sql-database><azure-sql-managed-instance>, What extra one gets by selecting Azure SQL Managed Instance vis-a-vis Azure SQL DB PaaS, I would like to know what extra benefits one get by choosing Azure SQL Managed Instance compared to Azure SQL DB PaaS. I know SQL Managed Instance is offered as a vCore based purchasing model only. Apart from this what is the extra add on and benefits that one gets over the other. Any reply would be appreciated.
","<azure><azure-sal-database><azure-sal-managed-instance>, extra one get select azur sal manage instant vis-a-vi azur sal do pass, would like know extra benefit one get choose azur sal manage instant compare azur sal do pass. know sal manage instant offer score base purchase model only. apart extra add benefit one get other. reply would appreciated."
60989709,How to convert a class instance to JsonDocument?,"Let's say we have an entity class that looks like this:
public class SerializedEntity
{
    public JsonDocument Payload { get; set; }
    public SerializedEntity(JsonDocument payload)
    {
        Payload = payload;
    }
}
According to npsql this generates a table with column payload of type jsonb for this class which is correct. 
Now what I would like to do is take any class instance and store it as payload in this table e.g.: 
public class Pizza {
    public string Name { get; set; }
    public int Size { get; set; }
}
should then be possible to be retrieved as an object with following structure:
{Name: ""name"", Size: 10}
So I need something like this:
var pizza = new Pizza(""Margharita"", 10);
var se = new SerializedEntity(someConverter.method(pizza))
",<c#><entity-framework><asp.net-core><npgsql>,764,1,19,2506,2,26,39,66,11557,0.0,257,5,15,2020-04-02 10:43,2020-04-02 11:24,2020-04-02 11:24,0.0,0.0,Basic,6,"<c#><entity-framework><asp.net-core><npgsql>, How to convert a class instance to JsonDocument?, Let's say we have an entity class that looks like this:
public class SerializedEntity
{
    public JsonDocument Payload { get; set; }
    public SerializedEntity(JsonDocument payload)
    {
        Payload = payload;
    }
}
According to npsql this generates a table with column payload of type jsonb for this class which is correct. 
Now what I would like to do is take any class instance and store it as payload in this table e.g.: 
public class Pizza {
    public string Name { get; set; }
    public int Size { get; set; }
}
should then be possible to be retrieved as an object with following structure:
{Name: ""name"", Size: 10}
So I need something like this:
var pizza = new Pizza(""Margharita"", 10);
var se = new SerializedEntity(someConverter.method(pizza))
","<c#><entity-framework><asp.net-core><npgsql>, convert class instant jsondocument?, let' say entity class look like this: public class serializedent { public jsondocu payload { get; set; } public serializedentity(jsondocu payload) { payload = payload; } } accord nasal genet table column payload type son class correct. would like take class instant store payload table e.g.: public class penza { public string name { get; set; } public in size { get; set; } } possible retrieve object follow structure: {name: ""name"", size: 10} need cometh like this: war penza = new penza(""margharita"", 10); war se = new serializedentity(someconverter.method(penza))"
54257933,Calling flush() in @Transactional method in Spring Boot application,"Is it possible that calling Hibernate flush() in the middle of @Transactional method will save incomplete results in the database?
For example, is it possible that this function could save ""John"" to the database?
@Transactional
public void tt() {
    Student s = new Student(""John"");
    em.persist(s);
    em.flush();
    // Perform some calculations that change some attributes of the instance
    s.setName(""Jeff"");
}
I tried it with H2 in-memory database and it didn't save incomplete transaction changes. But is it possible under certain conditions and maybe with another DB engine?
",<java><sql><spring><hibernate><jpa>,588,0,8,501,1,6,29,55,26628,0.0,146,1,15,2019-01-18 16:28,2019-01-18 17:17,2019-01-18 17:17,0.0,0.0,Basic,9,"<java><sql><spring><hibernate><jpa>, Calling flush() in @Transactional method in Spring Boot application, Is it possible that calling Hibernate flush() in the middle of @Transactional method will save incomplete results in the database?
For example, is it possible that this function could save ""John"" to the database?
@Transactional
public void tt() {
    Student s = new Student(""John"");
    em.persist(s);
    em.flush();
    // Perform some calculations that change some attributes of the instance
    s.setName(""Jeff"");
}
I tried it with H2 in-memory database and it didn't save incomplete transaction changes. But is it possible under certain conditions and maybe with another DB engine?
","<cava><sal><spring><liberate><pa>, call flush() @transact method spring boot application, possible call wiberd flush() middle @transact method save incomplete result database? example, possible function could save ""john"" database? @transact public void tt() { student = new student(""john""); em.persist(s); em.flush(); // perform call change attribute instant s.senate(""jeff""); } try he in-memory database save incomplete transact changes. possible certain conduct may not do engine?"
48069425,Converting Between Timezones in Postgres,"I am trying to understand the timestamps and timezones in Postgre. I think I got it, until I red this article.  Focus on the ""Converting Between Timezones"" part. It has two examples. 
(Consider the default timezone configuration to be UTC.)
Example 1
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'); outputs 2015-12-31 16:00:00
According to the article and what I understand, because the '2016-01-01 00:00' part of the timezone function is just a string, it is silently converted to the default UTC. So from '2016-01-01 00:00' UTC it is then converted to US/Pacific as asked by the timezone function, that is 2015-12-31 16:00:00.  
Example 2 
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'::timestamp); outputs 2016-01-01 08:00:00+00
Excuse me, I dont see why and the explanation there does not help. Ok, the '2016-01-01 00:00'::timestamp part of the timezone function is no longer a string, but an actual timestamp. In what timezone? If it is UTC, the output would have to be the same as the Example 1. So it is automatically converted to US/Pacific? Then the output is in UTC? But why? I asked for a US/Pacific in my timezone  not a UTC.
Please explain how the timezone behaves when gets a timestamp and gets asked to transform it. Thank you.
",<postgresql><timezone><timestamp><timezone-offset><timestamp-with-timezone>,1259,1,14,4312,20,69,130,52,33126,0.0,468,2,15,2018-01-02 23:51,2018-01-03 17:46,2018-01-16 8:08,1.0,14.0,Basic,9,"<postgresql><timezone><timestamp><timezone-offset><timestamp-with-timezone>, Converting Between Timezones in Postgres, I am trying to understand the timestamps and timezones in Postgre. I think I got it, until I red this article.  Focus on the ""Converting Between Timezones"" part. It has two examples. 
(Consider the default timezone configuration to be UTC.)
Example 1
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'); outputs 2015-12-31 16:00:00
According to the article and what I understand, because the '2016-01-01 00:00' part of the timezone function is just a string, it is silently converted to the default UTC. So from '2016-01-01 00:00' UTC it is then converted to US/Pacific as asked by the timezone function, that is 2015-12-31 16:00:00.  
Example 2 
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'::timestamp); outputs 2016-01-01 08:00:00+00
Excuse me, I dont see why and the explanation there does not help. Ok, the '2016-01-01 00:00'::timestamp part of the timezone function is no longer a string, but an actual timestamp. In what timezone? If it is UTC, the output would have to be the same as the Example 1. So it is automatically converted to US/Pacific? Then the output is in UTC? But why? I asked for a US/Pacific in my timezone  not a UTC.
Please explain how the timezone behaves when gets a timestamp and gets asked to transform it. Thank you.
","<postgresql><timezone><timestamp><timezone-offset><timestamp-with-timezone>, convert timezon postures, try understand timestamp timezon posture. think got it, red article. focus ""convert timezones"" part. two examples. (consider default timezon configur etc.) example 1 do=# select timezone('us/pacific', '2016-01-01 00:00'); output 2015-12-31 16:00:00 accord article understand, '2016-01-01 00:00' part timezon function string, silent convert default etc. '2016-01-01 00:00' etc convert us/pacify ask timezon function, 2015-12-31 16:00:00. example 2 do=# select timezone('us/pacific', '2016-01-01 00:00'::timestamp); output 2016-01-01 08:00:00+00 excuse me, dont see explain help. ok, '2016-01-01 00:00'::timestamp part timezon function longer string, actual timestamp. timezone? etc, output would example 1. automatic convert us/pacific? output etc? why? ask us/pacify timezon etc. pleas explain timezon behave get timestamp get ask transform it. thank you."
53591359,postgresql: Filter in JSON array,"Let's say we have a table items which has columns name and attributes:
CREATE TABLE students (
  name VARCHAR(100),
  attributes JSON
)
where attributes is an array of (always equally-structured) JSON documents such as
[{""name"":""Attribute 1"",""value"":""Value 1""},{""name"":""Attribute 2"",""value"":""Value 2""}]
I now want to find all students where any attribute value matches something (such as Foo%). Here's a playground example.
I realize that this isn't exactly the most straight-forward design, but for now it's what I have to work with, though performance of such a search being categorically terribly inefficient would of course be a valid concern.
",<sql><postgresql>,648,1,9,19523,7,68,100,73,37808,0.0,1059,3,15,2018-12-03 10:02,2018-12-03 10:43,2018-12-03 10:43,0.0,0.0,Basic,9,"<sql><postgresql>, postgresql: Filter in JSON array, Let's say we have a table items which has columns name and attributes:
CREATE TABLE students (
  name VARCHAR(100),
  attributes JSON
)
where attributes is an array of (always equally-structured) JSON documents such as
[{""name"":""Attribute 1"",""value"":""Value 1""},{""name"":""Attribute 2"",""value"":""Value 2""}]
I now want to find all students where any attribute value matches something (such as Foo%). Here's a playground example.
I realize that this isn't exactly the most straight-forward design, but for now it's what I have to work with, though performance of such a search being categorically terribly inefficient would of course be a valid concern.
","<sal><postgresql>, postgresql: filter son array, let' say table item column name attributes: great table student ( name varchar(100), attribute son ) attribute array (away equally-structures) son document [{""name"":""attribute 1"",""value"":""value 1""},{""name"":""attribute 2"",""value"":""value 2""}] want find student attribute value match cometh (such foo%). here' playground example. realize exactly straight-forward design, work with, though perform search category terrible ineffici would course valid concern."
49971903,Converting epoch to datetime in PySpark data frame using udf,"I have a PySpark dataframe with this schema:
root
 |-- epoch: double (nullable = true)
 |-- var1: double (nullable = true)
 |-- var2: double (nullable = true)
Where epoch is in seconds and should be converted to date time. In order to do so, I define a user defined function (udf) as follows:
from pyspark.sql.functions import udf    
import time
def epoch_to_datetime(x):
    return time.localtime(x)
    # return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x))
    # return x * 0 + 1
epoch_to_datetime_udf = udf(epoch_to_datetime, DoubleType())
df.withColumn(""datetime"", epoch_to_datetime(df2.epoch)).show()
I get this error:
---&gt; 21     return time.localtime(x)
    22     # return x * 0 + 1
    23 
    TypeError: a float is required
If I simply return x + 1 in the function, it works. Trying float(x) or float(str(x)) or numpy.float(x) in time.localtime(x) does not help and I still get an error. Outside of udf, time.localtime(1.514687216E9) or other numbers works fine. Using datetime package to convert epoch to datetim results in similar errors. 
It seems that time and datetime packages do not like to fed with DoubleType from PySpark. Any ideas how I can solve this issue? Thanks.
",<python><apache-spark><pyspark><apache-spark-sql>,1201,0,29,1350,3,17,31,66,38149,0.0,506,3,15,2018-04-23 0:14,2018-04-23 3:07,2018-04-23 3:07,0.0,0.0,Basic,9,"<python><apache-spark><pyspark><apache-spark-sql>, Converting epoch to datetime in PySpark data frame using udf, I have a PySpark dataframe with this schema:
root
 |-- epoch: double (nullable = true)
 |-- var1: double (nullable = true)
 |-- var2: double (nullable = true)
Where epoch is in seconds and should be converted to date time. In order to do so, I define a user defined function (udf) as follows:
from pyspark.sql.functions import udf    
import time
def epoch_to_datetime(x):
    return time.localtime(x)
    # return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x))
    # return x * 0 + 1
epoch_to_datetime_udf = udf(epoch_to_datetime, DoubleType())
df.withColumn(""datetime"", epoch_to_datetime(df2.epoch)).show()
I get this error:
---&gt; 21     return time.localtime(x)
    22     # return x * 0 + 1
    23 
    TypeError: a float is required
If I simply return x + 1 in the function, it works. Trying float(x) or float(str(x)) or numpy.float(x) in time.localtime(x) does not help and I still get an error. Outside of udf, time.localtime(1.514687216E9) or other numbers works fine. Using datetime package to convert epoch to datetim results in similar errors. 
It seems that time and datetime packages do not like to fed with DoubleType from PySpark. Any ideas how I can solve this issue? Thanks.
","<patron><apache-spark><spark><apache-spark-sal>, convert epoch datetim spark data frame use utf, spark datafram scheme: root |-- epoch: doubt (nullabl = true) |-- vary: doubt (nullabl = true) |-- vary: doubt (nullabl = true) epoch second convert date time. order so, define user define function (utf) follows: spark.sal.fact import utf import time def epoch_to_datetime(x): return time.localise(x) # return time.strftime('%y-%m-%d %h:%m:%s', time.localise(x)) # return x * 0 + 1 epoch_to_datetime_udf = utf(epoch_to_datetime, doubletype()) of.withcolumn(""daytime"", epoch_to_datetime(of.epoch)).show() get error: ---&it; 21 return time.localise(x) 22 # return x * 0 + 1 23 typeerror: float require simple return x + 1 function, works. try float(x) float(sir(x)) jump.float(x) time.localise(x) help still get error. outside utf, time.localise(1.514687216e9) number work fine. use datetim package convert epoch datetim result similar errors. seem time datetim package like fed doubletyp spark. idea sole issue? thanks."
57546833,How to take a dump from Mysql 8.0 into 5.7?,"I would like to take a dump from Mysql 8.0.11 and restore it into 5.7.27.
When I tried to restore it I got the error:
  ERROR 1273 (HY000) at line 25: Unknown collation: 'utf8mb4_0900_ai_ci'
Then I tried to use the compatible flag to make it easier on an older MySQL DB.
mysqldump --compatible=mysql4 --add-drop-table -u r00t -h xxx.eu-north-1.rds.amazonaws.com -p radius_db &gt; ~/radius.sql
But that doesn't seem to work either:
  mysqldump: Couldn't execute '/*!40100 SET @@SQL_MODE='MYSQL40' */':
  Variable 'sql_mode' can't be set to the value of 'MYSQL40' (1231)
Any advice would be appreciated.
",<mysql>,602,0,1,64585,90,280,468,69,15424,0.0,2772,2,15,2019-08-18 17:26,2019-08-22 10:35,2019-08-22 10:35,4.0,4.0,Basic,9,"<mysql>, How to take a dump from Mysql 8.0 into 5.7?, I would like to take a dump from Mysql 8.0.11 and restore it into 5.7.27.
When I tried to restore it I got the error:
  ERROR 1273 (HY000) at line 25: Unknown collation: 'utf8mb4_0900_ai_ci'
Then I tried to use the compatible flag to make it easier on an older MySQL DB.
mysqldump --compatible=mysql4 --add-drop-table -u r00t -h xxx.eu-north-1.rds.amazonaws.com -p radius_db &gt; ~/radius.sql
But that doesn't seem to work either:
  mysqldump: Couldn't execute '/*!40100 SET @@SQL_MODE='MYSQL40' */':
  Variable 'sql_mode' can't be set to the value of 'MYSQL40' (1231)
Any advice would be appreciated.
","<myself>, take dump myself 8.0 5.7?, would like take dump myself 8.0.11 restore 5.7.27. try restore got error: error 1273 (hy000) line 25: unknown collection: 'utf8mb4_0900_ai_ci' try use compact flag make easier older myself do. mysqldump --compatible=myself --add-drop-t -u rest -h xxx.e-north-1.rd.amazonaws.com -p radius_db &it; ~/radius.sal seem work either: mysqldump: execute '/*!40100 set @@sql_mode='mysql40' */': variable 'sql_mode' can't set value 'mysql40' (1231) advice would appreciated."
54665992,"Error ""Could not find any index named [IX_MyIndex]"" upon creating it","I came before this very weird error:
  Msg 7999, Level 16, State 9, Line 12
  Could not find any index named 'IX_MyIndex' for table 'dbo.MyTable'.
When running the script to create it!!
CREATE NONCLUSTERED INDEX [IX_MyIndex] ON [dbo].[MyTable] (
    [Field1]
    ,[Field2]
    ) INCLUDE (
    Fields3
    ,Fields4
    ,Fields5
    )
    WITH (
         MAXDOP = 4
         ,DATA_COMPRESSION = PAGE
         ,DROP_EXISTING = ON
        )
What am I missing?
",<sql-server>,456,0,13,3111,4,31,60,53,6255,0.0,363,2,15,2019-02-13 8:52,2019-02-13 8:58,2019-02-13 8:58,0.0,0.0,Basic,9,"<sql-server>, Error ""Could not find any index named [IX_MyIndex]"" upon creating it, I came before this very weird error:
  Msg 7999, Level 16, State 9, Line 12
  Could not find any index named 'IX_MyIndex' for table 'dbo.MyTable'.
When running the script to create it!!
CREATE NONCLUSTERED INDEX [IX_MyIndex] ON [dbo].[MyTable] (
    [Field1]
    ,[Field2]
    ) INCLUDE (
    Fields3
    ,Fields4
    ,Fields5
    )
    WITH (
         MAXDOP = 4
         ,DATA_COMPRESSION = PAGE
         ,DROP_EXISTING = ON
        )
What am I missing?
","<sal-server>, error ""could find index name [ix_myindex]"" upon great it, came weird error: mug 7999, level 16, state 9, line 12 could find index name 'ix_myindex' table 'do.table'. run script great it!! great nonclust index [ix_myindex] [do].[table] ( [field] ,[field] ) include ( fields ,fields ,fields ) ( maxdop = 4 ,data_compress = page ,drop_exist = ) missing?"
58976719,How to convert string column type to integer using Laravel migration?,"I am trying to alter the column datatype using Laravel migration. But I am facing following error. Please help me out.
Schema::table('files', function(Blueprint $table) {
    $table-&gt;integer('app_id')-&gt;change();
    $table-&gt;index(['app_id', 'filename']);
});
  SQLSTATE[42000]: Syntax error or access violation: 1064 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci' at line 1 (SQL: ALTER TABLE files CHANGE app_id app_id INT CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci) 
",<php><mysql><laravel>,647,0,6,183,1,2,6,71,11810,,1,2,15,2019-11-21 14:00,2019-11-25 12:42,2019-11-25 12:42,4.0,4.0,Basic,9,"<php><mysql><laravel>, How to convert string column type to integer using Laravel migration?, I am trying to alter the column datatype using Laravel migration. But I am facing following error. Please help me out.
Schema::table('files', function(Blueprint $table) {
    $table-&gt;integer('app_id')-&gt;change();
    $table-&gt;index(['app_id', 'filename']);
});
  SQLSTATE[42000]: Syntax error or access violation: 1064 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci' at line 1 (SQL: ALTER TABLE files CHANGE app_id app_id INT CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci) 
","<pp><myself><travel>, convert string column type inter use travel migration?, try alter column datatyp use travel migration. face follow error. pleas help out. scheme::table('files', function(blueprint $table) { $table-&it;inter('applied')-&it;change(); $table-&it;index(['applied', 'filename']); }); sqlstate[42000]: santa error access violation: 1064 error sal santa; check manual correspond myself server version right santa use near 'character set utf default 0 null collar utf8_unicode_ci' line 1 (sal: alter table file change applied applied in character set utf default 0 null collar utf8_unicode_ci)"
53537229,MSSQL at Docker exits instantly,"I'm trying to setup MSSQL at Docker at Windows 10, but for some reason it started shutting down my container
I've been using it like that for months, but now I have no idea what's happening
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
    C:\Users\user\
     docker login
    Authenticating with existing credentials...
    Login Succeeded
    C:\Users\user\
     docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=&lt;YourStrong!Passw0rd&gt;123' -p 1433:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest
    337e5efb35f0bf4b465181a0f8be4851b12f353a3a8710ddf817d2f501e5fea
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   3 seconds ago       Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                     PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   6 seconds ago       Exited (1) 2 seconds ago                       sql
    C:\Users\user\
     docker start sql
    sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   14 seconds ago      Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                    PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   16 seconds ago      Exited (1) 1 second ago                       sql
  docker logs sql
shows
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
Anybody has an idea what may be wrong?
",<sql-server>,3024,2,36,1821,6,23,65,80,13734,0.0,141,1,15,2018-11-29 10:47,2018-11-29 13:52,2018-11-29 13:52,0.0,0.0,Basic,9,"<sql-server>, MSSQL at Docker exits instantly, I'm trying to setup MSSQL at Docker at Windows 10, but for some reason it started shutting down my container
I've been using it like that for months, but now I have no idea what's happening
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
    C:\Users\user\
     docker login
    Authenticating with existing credentials...
    Login Succeeded
    C:\Users\user\
     docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=&lt;YourStrong!Passw0rd&gt;123' -p 1433:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest
    337e5efb35f0bf4b465181a0f8be4851b12f353a3a8710ddf817d2f501e5fea
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   3 seconds ago       Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                     PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   6 seconds ago       Exited (1) 2 seconds ago                       sql
    C:\Users\user\
     docker start sql
    sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   14 seconds ago      Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
     docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                    PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls""   16 seconds ago      Exited (1) 1 second ago                       sql
  docker logs sql
shows
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
Anybody has an idea what may be wrong?
","<sal-server>, mssql doctor exit instantly, i'm try set mssql doctor window 10, reason start shut contain i'v use like months, idea what' happen c:\users\user\  doctor is -a contain id image command great state port name c:\users\user\  doctor login authentic exist credentials... login succeed c:\users\user\  doctor run -e 'accept_eula=y' -e 'sa_password=&it;yourstrong!password&it;123' -p 1433:1433 --name sal -d mr.microsoft.com/mssql/server:2017-latest 337e5efb35f0bf4b465181a0f8be4851b12f353a3a8710ddf817d2f501e5fea c:\users\user\  doctor is -a contain id image command great state port name 347q5effb3cf0 mr.microsoft.com/mssql/server:2017-latest ""/opt/mssql/bin/self"" 3 second ago 2 second 0.0.0.0:1433-&it;1433/top sal c:\users\user\  doctor is -a contain id image command great state port name 347q5effb3cf0 mr.microsoft.com/mssql/server:2017-latest ""/opt/mssql/bin/self"" 6 second ago exit (1) 2 second ago sal c:\users\user\  doctor start sal sal c:\users\user\  doctor is -a contain id image command great state port name 347q5effb3cf0 mr.microsoft.com/mssql/server:2017-latest ""/opt/mssql/bin/self"" 14 second ago 2 second 0.0.0.0:1433-&it;1433/top sal c:\users\user\  doctor is -a contain id image command great state port name 347q5effb3cf0 mr.microsoft.com/mssql/server:2017-latest ""/opt/mssql/bin/self"" 16 second ago exit (1) 1 second ago sal doctor log sal show sal server end-us license agreement (tula) must accept sal server start. license term product download http://go.microsoft.com/flank/?linked=746388. accept tula specific --accept-tula command line option, set accept_eula environs variable, use mssql-cone tool. sal server end-us license agreement (tula) must accept sal server start. license term product download http://go.microsoft.com/flank/?linked=746388. accept tula specific --accept-tula command line option, set accept_eula environs variable, use mssql-cone tool. anybody idea may wrong?"
58597639,How to make multiple query in a very short interval / simultaneously,"Hey I'm getting an error message : conn busy from pgx
I don't know how to solve this. Here is my function :
func (r *proverbRepo) SelectPendingProverbs(table string) (proverbs []domain.Proverb, err error) {
    query := fmt.Sprintf(""SELECT id, proverb literal FROM %s"", table)
    rows, err := r.Db.Query(context.Background(), query)
    defer rows.Close()
    if err != nil {
        return
    }
    for rows.Next() {
        var prov domain.Proverb
        if err = rows.Scan(&amp;prov.ID, &amp;prov.Literal); err != nil {
            return
        }
        proverbs = append(proverbs, prov)
    }
    return
}
r.Db is pgx.Connect(context.Background(), os.Getenv(""PSQL_URL""))
I'm fetching two different table in a very short interval from two separate front end requests.
The first request goes through, the other one returns the conn busy error message.
I really don't know what to look for, would somebody help me ?
",<go><psql><pgx>,923,0,22,3691,5,22,38,36,9220,0.0,63,2,15,2019-10-28 20:01,2019-10-28 21:28,2019-10-28 21:28,0.0,0.0,Intermediate,23,"<go><psql><pgx>, How to make multiple query in a very short interval / simultaneously, Hey I'm getting an error message : conn busy from pgx
I don't know how to solve this. Here is my function :
func (r *proverbRepo) SelectPendingProverbs(table string) (proverbs []domain.Proverb, err error) {
    query := fmt.Sprintf(""SELECT id, proverb literal FROM %s"", table)
    rows, err := r.Db.Query(context.Background(), query)
    defer rows.Close()
    if err != nil {
        return
    }
    for rows.Next() {
        var prov domain.Proverb
        if err = rows.Scan(&amp;prov.ID, &amp;prov.Literal); err != nil {
            return
        }
        proverbs = append(proverbs, prov)
    }
    return
}
r.Db is pgx.Connect(context.Background(), os.Getenv(""PSQL_URL""))
I'm fetching two different table in a very short interval from two separate front end requests.
The first request goes through, the other one returns the conn busy error message.
I really don't know what to look for, would somebody help me ?
","<go><pool><pg>, make multiple query short inter / simultaneously, hey i'm get error message : corn busy pg know sole this. function : fun (r *proverbrepo) selectpendingproverbs(t string) (proverb []domain.proverb, err error) { query := fat.spring(""select id, proverb later %s"", table) rows, err := r.do.query(context.background(), query) defer rows.close() err != nail { return } rows.next() { war prove domain.proverb err = rows.scan(&amp;prove.id, &amp;prove.literal); err != nail { return } proverb = happened(proverbs, prove) } return } r.do pg.connect(context.background(), os.geben(""psql_url"")) i'm fetch two differ table short inter two spear front end requests. first request go through, one return corn busy error message. really know look for, would somebody help ?"
50885572,UseSqlite with Entity Framework Core in ASP.NET Core 2.1 not working,"I am starting a Razor pages project in ASP.NET Core 2.1. I am trying to use SQLite but when configuring the database only SQL Server seems to be an option.
Startup.cs
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Application.Models;
using Microsoft.EntityFrameworkCore;
namespace Application
{
    public class Startup
    {
        public Startup(IConfiguration configuration)
        {
            Configuration = configuration;
        }
        public IConfiguration Configuration { get; }
        // This method gets called by the runtime. Use this method to add services to the container.
        public void ConfigureServices(IServiceCollection services)
        {
            services.AddDbContext&lt;ApplicationContext&gt;(options =&gt;
               options.UseSqlite(""Data Source=Database.db""));
            services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
        public void Configure(IApplicationBuilder app, IHostingEnvironment env)
        {
            app.UseStaticFiles();
            app.UseMvc();
        }
    }
}
Intellisense does not recognize options.UseSqlite and builds fail. This was not/ is not an issue with .net core 2.0 projects.
Is this not supported yet? Reading through the documentation makes it seem that it is. I'm not sure what else is going wrong here.
",<c#><sqlite><entity-framework-core>,1593,0,36,467,1,5,20,49,32447,0.0,21,3,15,2018-06-16 6:37,2018-06-16 7:05,2018-06-16 7:05,0.0,0.0,Basic,14,"<c#><sqlite><entity-framework-core>, UseSqlite with Entity Framework Core in ASP.NET Core 2.1 not working, I am starting a Razor pages project in ASP.NET Core 2.1. I am trying to use SQLite but when configuring the database only SQL Server seems to be an option.
Startup.cs
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Application.Models;
using Microsoft.EntityFrameworkCore;
namespace Application
{
    public class Startup
    {
        public Startup(IConfiguration configuration)
        {
            Configuration = configuration;
        }
        public IConfiguration Configuration { get; }
        // This method gets called by the runtime. Use this method to add services to the container.
        public void ConfigureServices(IServiceCollection services)
        {
            services.AddDbContext&lt;ApplicationContext&gt;(options =&gt;
               options.UseSqlite(""Data Source=Database.db""));
            services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
        public void Configure(IApplicationBuilder app, IHostingEnvironment env)
        {
            app.UseStaticFiles();
            app.UseMvc();
        }
    }
}
Intellisense does not recognize options.UseSqlite and builds fail. This was not/ is not an issue with .net core 2.0 projects.
Is this not supported yet? Reading through the documentation makes it seem that it is. I'm not sure what else is going wrong here.
","<c#><quite><entity-framework-core>, usesqlit entity framework core asp.net core 2.1 working, start razor page project asp.net core 2.1. try use quite configur database sal server seem option. started.c use microsoft.aspnetcore.builder; use microsoft.aspnetcore.costing; use microsoft.aspnetcore.mac; use microsoft.extensions.configuration; use microsoft.extensions.dependencyinjection; use application.models; use microsoft.entityframeworkcore; namespac applied { public class started { public started(iconfigur configuration) { configur = configuration; } public iconfigur configur { get; } // method get call auntie. use method add service container. public void configureservices(iservicecollect services) { services.adddbcontext&it;applicationcontext&it;(opt =&it; option.usesqlite(""data source=database.do"")); services.addmvc().setcompatibilityversion(compatibilityversion.version_2_1); } // method get call auntie. use method configur http request pipeline. public void configure(iapplicationbuild pp, ihostingenviron end) { pp.usestaticfiles(); pp.usemvc(); } } } intelligent record option.usesqlit build fail. not/ issue .net core 2.0 projects. support yet? read document make seem is. i'm sure else go wrong here."
56265904,Reading UUID from result set in Postgres JDBC,"I'm using UUID for my id column, I'm looking for a way to retrieve the data inside my Java application. I can't find a  method in ResultSet for getting a UUID. How would I go about getting the UUID?
",<postgresql><jdbc>,199,0,0,8557,27,108,176,64,8613,0.0,554,1,15,2019-05-22 22:44,2019-05-23 3:37,2019-05-23 3:37,1.0,1.0,Basic,3,"<postgresql><jdbc>, Reading UUID from result set in Postgres JDBC, I'm using UUID for my id column, I'm looking for a way to retrieve the data inside my Java application. I can't find a  method in ResultSet for getting a UUID. How would I go about getting the UUID?
","<postgresql><job>, read quid result set poster job, i'm use quid id column, i'm look way retrieve data inside cava application. can't find method results get quid. would go get quid?"
60797825,Why mysql explain analyze is not working?,"Besides having mariadb 10.1.36-MariaDB I get following error.
EXPLAIN ANALYZE select 1
MySQL said: Documentation
1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'ANALYZE select 1' at line 1
What additional I need to do here. My PHP version is 7.2.11.
",<php><mysql><mariadb><query-optimization>,346,0,4,153,0,1,4,48,9362,,0,2,15,2020-03-22 9:44,2020-03-22 9:50,2020-03-22 9:50,0.0,0.0,Basic,14,"<php><mysql><mariadb><query-optimization>, Why mysql explain analyze is not working?, Besides having mariadb 10.1.36-MariaDB I get following error.
EXPLAIN ANALYZE select 1
MySQL said: Documentation
1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'ANALYZE select 1' at line 1
What additional I need to do here. My PHP version is 7.2.11.
","<pp><myself><maria><query-optimization>, myself explain analyze working?, beside maria 10.1.36-maria get follow error. explain analyze select 1 myself said: document 1064 - error sal santa; check manual correspond maria server version right santa use near 'analyze select 1' line 1 admit need here. pp version 7.2.11."
64974214,AWS RDS - Access denied to admin user when using GRANT ALL PRIVILEGES ON the_db.* TO 'the_user'@'%',"When we try to GRANT ALL permissions to a user for a specific database, the admin (superuser) user of database receives the following error.
Access denied for user 'admin'@'%' to database 'the_Db'
After looking other questions in stackoverflow I could not find the solution. I already tried to change * -&gt; % without success, that is the approach suggested in the following source:
http://www.fidian.com/problems-only-tyler-has/using-grant-all-with-amazons-mysql-rds
I think there is an underlying configuration on RDS so I can't grant all permissions for the users, but I don't know how to detect what is happening.
Update
After doing some workarounds I noticed that the &quot;Delete versioning rows&quot; permissions is the one that causes the problem. I can add all permissions but that one.
https://mariadb.com/kb/en/grant/
So the only &quot;way&quot; I could grant other permissions was to specific each one of those with a script like this.
GRANT Alter ON *.* TO 'user_some_app'@'%';
GRANT Create ON *.* TO 'user_some_app'@'%';  
GRANT Create view ON *.* TO 'user_some_app'@'%';
GRANT Delete ON *.* TO 'user_some_app'@'%';
GRANT Drop ON *.* TO 'user_some_app'@'%';
GRANT Grant option ON *.* TO 'user_some_app'@'%';
GRANT Index ON *.* TO 'user_some_app'@'%';
GRANT Insert ON *.* TO 'user_some_app'@'%';
GRANT References ON *.* TO 'user_some_app'@'%';
GRANT Select ON *.* TO 'user_some_app'@'%';
GRANT Show view ON *.* TO 'user_some_app'@'%';
GRANT Trigger ON *.* TO 'user_some_app'@'%';
GRANT Update ON *.* TO 'user_some_app'@'%';
GRANT Alter routine ON *.* TO 'user_some_app'@'%';
GRANT Create routine ON *.* TO 'user_some_app'@'%';
GRANT Create temporary tables ON *.* TO 'user_some_app'@'%';
GRANT Execute ON *.* TO 'user_some_app'@'%';
GRANT Lock tables ON *.* TO 'user_some_app'@'%';
",<mysql><amazon-web-services><mariadb><amazon-rds>,1796,4,19,1394,2,12,16,49,14713,0.0,106,1,15,2020-11-23 18:28,2021-03-02 11:30,2021-03-02 11:30,99.0,99.0,Basic,14,"<mysql><amazon-web-services><mariadb><amazon-rds>, AWS RDS - Access denied to admin user when using GRANT ALL PRIVILEGES ON the_db.* TO 'the_user'@'%', When we try to GRANT ALL permissions to a user for a specific database, the admin (superuser) user of database receives the following error.
Access denied for user 'admin'@'%' to database 'the_Db'
After looking other questions in stackoverflow I could not find the solution. I already tried to change * -&gt; % without success, that is the approach suggested in the following source:
http://www.fidian.com/problems-only-tyler-has/using-grant-all-with-amazons-mysql-rds
I think there is an underlying configuration on RDS so I can't grant all permissions for the users, but I don't know how to detect what is happening.
Update
After doing some workarounds I noticed that the &quot;Delete versioning rows&quot; permissions is the one that causes the problem. I can add all permissions but that one.
https://mariadb.com/kb/en/grant/
So the only &quot;way&quot; I could grant other permissions was to specific each one of those with a script like this.
GRANT Alter ON *.* TO 'user_some_app'@'%';
GRANT Create ON *.* TO 'user_some_app'@'%';  
GRANT Create view ON *.* TO 'user_some_app'@'%';
GRANT Delete ON *.* TO 'user_some_app'@'%';
GRANT Drop ON *.* TO 'user_some_app'@'%';
GRANT Grant option ON *.* TO 'user_some_app'@'%';
GRANT Index ON *.* TO 'user_some_app'@'%';
GRANT Insert ON *.* TO 'user_some_app'@'%';
GRANT References ON *.* TO 'user_some_app'@'%';
GRANT Select ON *.* TO 'user_some_app'@'%';
GRANT Show view ON *.* TO 'user_some_app'@'%';
GRANT Trigger ON *.* TO 'user_some_app'@'%';
GRANT Update ON *.* TO 'user_some_app'@'%';
GRANT Alter routine ON *.* TO 'user_some_app'@'%';
GRANT Create routine ON *.* TO 'user_some_app'@'%';
GRANT Create temporary tables ON *.* TO 'user_some_app'@'%';
GRANT Execute ON *.* TO 'user_some_app'@'%';
GRANT Lock tables ON *.* TO 'user_some_app'@'%';
","<myself><amazon-web-services><maria><amazon-rd>, a rd - access den admit user use grant privilege the_db.* 'the_user'@'%', try grant permits user specie database, admit (superuser) user database receive follow error. access den user 'admit'@'%' database 'the_db' look question stackoverflow could find solution. already try change * -&it; % without success, approach suggest follow source: http://www.midian.com/problems-only-tyler-has/using-grant-all-with-amazon-myself-rd think underlip configur rd can't grant permits users, know detect happening. update workaround notice &quit;delete version rows&quit; permits one cause problem. add permits one. http://maria.com/b/en/grant/ &quit;way&quit; could grant permits specie one script like this. grant alter *.* 'user_some_app'@'%'; grant great *.* 'user_some_app'@'%'; grant great view *.* 'user_some_app'@'%'; grant delete *.* 'user_some_app'@'%'; grant drop *.* 'user_some_app'@'%'; grant grant option *.* 'user_some_app'@'%'; grant index *.* 'user_some_app'@'%'; grant insert *.* 'user_some_app'@'%'; grant refer *.* 'user_some_app'@'%'; grant select *.* 'user_some_app'@'%'; grant show view *.* 'user_some_app'@'%'; grant trigger *.* 'user_some_app'@'%'; grant update *.* 'user_some_app'@'%'; grant alter routine *.* 'user_some_app'@'%'; grant great routine *.* 'user_some_app'@'%'; grant great temporary table *.* 'user_some_app'@'%'; grant execute *.* 'user_some_app'@'%'; grant lock table *.* 'user_some_app'@'%';"
48206047,How to return all the columns with flask-sqlalchemy query join from two tables,"I'm trying to do a join from two tables in flask-sqlalchemy and I want all the columns from both tables but if I execute:
Company.query.join(Buyer, Buyer.buyer_id == Company.id).all()
I have only the columns from Company (It returns, in fact, a Company object). 
I know I can do something like:
Company.query.join(Buyer, Buyer.buyer_id == Company.id) \
             .add_columns(Buyer.id, Buyer.name, etc..).all()
It returns in this case:
(&lt;Company 2&gt;, 1, 'S67FG', etc..)
the problem is that I have a lot of columns and also I don't know how to marshmallow the returned obj with flask-marshmallow (with nested fields does not work).
Is there a way to return a new obj with columns from the two tables?
What is for you the best way to manage these situations?
Any suggestion is highly appreciated. Thanks
",<python><sqlalchemy>,810,0,4,713,3,6,20,66,10857,0.0,78,1,15,2018-01-11 11:31,2018-01-11 11:47,2018-01-11 11:47,0.0,0.0,Basic,9,"<python><sqlalchemy>, How to return all the columns with flask-sqlalchemy query join from two tables, I'm trying to do a join from two tables in flask-sqlalchemy and I want all the columns from both tables but if I execute:
Company.query.join(Buyer, Buyer.buyer_id == Company.id).all()
I have only the columns from Company (It returns, in fact, a Company object). 
I know I can do something like:
Company.query.join(Buyer, Buyer.buyer_id == Company.id) \
             .add_columns(Buyer.id, Buyer.name, etc..).all()
It returns in this case:
(&lt;Company 2&gt;, 1, 'S67FG', etc..)
the problem is that I have a lot of columns and also I don't know how to marshmallow the returned obj with flask-marshmallow (with nested fields does not work).
Is there a way to return a new obj with columns from the two tables?
What is for you the best way to manage these situations?
Any suggestion is highly appreciated. Thanks
","<patron><sqlalchemy>, return column flask-sqlalchemi query join two tables, i'm try join two table flask-sqlalchemi want column table execute: company.query.join(buyer, buyer.buyer_id == company.id).all() column company (it returns, fact, company object). know cometh like: company.query.join(buyer, buyer.buyer_id == company.id) \ .add_columns(buyer.id, buyer.name, etc..).all() return case: (&it;company 2&it;, 1, 's67fg', etc..) problem lot column also know marshmallow return obs flask-marshmallow (with nest field work). way return new obs column two tables? best way manage situations? suggest highly appreciated. thank"
59249332,How to solve 'Cannot authenticate using Kerberos' issue doing EF Core database scaffolding in Linux(Ubuntu 18.04)? Are there any solutions?,"I've been trying to develop a simple AspNetCore application with EntityFrameworkCore to connect and work with the MSSQL server database. And manage all this by Rider IDE, a tool for Database client (DBeaver) and dotnet command line interface(dotnet ef). I'm using the database first approach(create a database on the MSSQL server, fill it with tables, and then build Models based on tables).
My STEP-by-STEP actions:
1)install and set up MSSQL server for my machine working on Ubuntu 18.04. Install the command line tool &quot;SQLCMD&quot;. ///
Link to guide - https://learn.microsoft.com/en-gb/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-ver15
2)locally connected to my MSSQLServer instance.
sqlcmd -S localhost -U SA -P 'MyPasswd'
3)Using Transact-SQL created a Database and installed a DB client (DBeaver) to quickly manage my databases now and in the future.
The next step, as I supposed, was to use tutorials about connecting my Web Application to a database that was found here https://blog.jetbrains.com/dotnet/2017/08/09/running-entity-framework-core-commands-rider/ and here https://www.entityframeworktutorial.net/efcore/create-model-for-existing-database-in-ef-core.aspx
My ASP.NET Core project's package references:
Microsoft.EntityFrameworkCore
Microsoft.EntityFrameworkCore.SqlServer
Microsoft.EntityFrameworkCore.Tools
After typing in the CLI command
dotnet ef dbcontext scaffold &quot;Server=localhost;Database=WebAppDB;Integrated Security=true;&quot; Microsoft.EntityFrameworkCore.SqlServer -c RsvpContext (
to build &quot;RsvpContext&quot; context to connect to my database WebAppDB.)
I see what I see:
Build started...
Build succeeded.
Microsoft.Data.SqlClient.SqlException (0x80131904): **Cannot authenticate using 
Kerberos. Ensure Kerberos has been initialized on the client with 'kinit' and a 
Service Principal Name has been registered for the SQL Server to allow Kerberos 
authentication.**
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
   at Microsoft.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, SqlCredential credential, Object providerInfo, String newPassword, SecureString newSecurePassword, Boolean redirectedUserInstance, SqlConnectionString userConnectionOptions, SessionData reconnectSessionData, Boolean applyTransientFaultHandling, String accessToken, DbConnectionPool pool, SqlAuthenticationProviderManager sqlAuthProviderManager)
   at Microsoft.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, DbConnectionPoolKey poolKey, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnectionPool pool, DbConnection owningObject, DbConnectionOptions options, DbConnectionPoolKey poolKey, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
at Microsoft.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at Microsoft.Data.SqlClient.SqlConnection.Open()
   at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(DbConnection connection, DatabaseModelFactoryOptions options)
at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(String connectionString, DatabaseModelFactoryOptions options)
   at Microsoft.EntityFrameworkCore.Scaffolding.Internal.ReverseEngineerScaffolder.ScaffoldModel(String connectionString, DatabaseModelFactoryOptions databaseOptions, ModelReverseEngineerOptions modelOptions, ModelCodeGenerationOptions codeOptions)
   at Microsoft.EntityFrameworkCore.Design.Internal.DatabaseOperations.ScaffoldContext(String provider, String connectionString, String outputDir, String outputContextDir, String dbContextClassName, IEnumerable`1 schemas, IEnumerable`1 tables, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContextImpl(String provider, String connectionString, String outputDir, String outputDbContextDir, String dbContextClassName, IEnumerable`1 schemaFilters, IEnumerable`1 tableFilters, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContext.&lt;&gt;c__DisplayClass0_0.&lt;.ctor&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.&lt;&gt;c__DisplayClass3_0`1.&lt;Execute&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.Execute(Action action)
ClientConnectionId:38f805bc-5879-458b-9256-d6a201d7ce99
Cannot authenticate using Kerberos. Ensure Kerberos has been initialized on the 
client with 'kinit' and a Service Principal Name has been registered for the SQL 
Server to allow Kerberos authentication.
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
If someone, preferably working on Linux, had the same issue, please let me know and share your solutions(guide on what to do in this situation).
",<asp.net-mvc><ubuntu><kerberos><sql-server-2017><ef-core-3.0>,9027,6,78,155,1,1,8,59,15262,,31,2,15,2019-12-09 12:53,2020-06-19 9:45,2020-06-19 9:45,193.0,193.0,Basic,9,"<asp.net-mvc><ubuntu><kerberos><sql-server-2017><ef-core-3.0>, How to solve 'Cannot authenticate using Kerberos' issue doing EF Core database scaffolding in Linux(Ubuntu 18.04)? Are there any solutions?, I've been trying to develop a simple AspNetCore application with EntityFrameworkCore to connect and work with the MSSQL server database. And manage all this by Rider IDE, a tool for Database client (DBeaver) and dotnet command line interface(dotnet ef). I'm using the database first approach(create a database on the MSSQL server, fill it with tables, and then build Models based on tables).
My STEP-by-STEP actions:
1)install and set up MSSQL server for my machine working on Ubuntu 18.04. Install the command line tool &quot;SQLCMD&quot;. ///
Link to guide - https://learn.microsoft.com/en-gb/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-ver15
2)locally connected to my MSSQLServer instance.
sqlcmd -S localhost -U SA -P 'MyPasswd'
3)Using Transact-SQL created a Database and installed a DB client (DBeaver) to quickly manage my databases now and in the future.
The next step, as I supposed, was to use tutorials about connecting my Web Application to a database that was found here https://blog.jetbrains.com/dotnet/2017/08/09/running-entity-framework-core-commands-rider/ and here https://www.entityframeworktutorial.net/efcore/create-model-for-existing-database-in-ef-core.aspx
My ASP.NET Core project's package references:
Microsoft.EntityFrameworkCore
Microsoft.EntityFrameworkCore.SqlServer
Microsoft.EntityFrameworkCore.Tools
After typing in the CLI command
dotnet ef dbcontext scaffold &quot;Server=localhost;Database=WebAppDB;Integrated Security=true;&quot; Microsoft.EntityFrameworkCore.SqlServer -c RsvpContext (
to build &quot;RsvpContext&quot; context to connect to my database WebAppDB.)
I see what I see:
Build started...
Build succeeded.
Microsoft.Data.SqlClient.SqlException (0x80131904): **Cannot authenticate using 
Kerberos. Ensure Kerberos has been initialized on the client with 'kinit' and a 
Service Principal Name has been registered for the SQL Server to allow Kerberos 
authentication.**
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
   at Microsoft.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, SqlCredential credential, Object providerInfo, String newPassword, SecureString newSecurePassword, Boolean redirectedUserInstance, SqlConnectionString userConnectionOptions, SessionData reconnectSessionData, Boolean applyTransientFaultHandling, String accessToken, DbConnectionPool pool, SqlAuthenticationProviderManager sqlAuthProviderManager)
   at Microsoft.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, DbConnectionPoolKey poolKey, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnectionPool pool, DbConnection owningObject, DbConnectionOptions options, DbConnectionPoolKey poolKey, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
at Microsoft.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at Microsoft.Data.SqlClient.SqlConnection.Open()
   at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(DbConnection connection, DatabaseModelFactoryOptions options)
at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(String connectionString, DatabaseModelFactoryOptions options)
   at Microsoft.EntityFrameworkCore.Scaffolding.Internal.ReverseEngineerScaffolder.ScaffoldModel(String connectionString, DatabaseModelFactoryOptions databaseOptions, ModelReverseEngineerOptions modelOptions, ModelCodeGenerationOptions codeOptions)
   at Microsoft.EntityFrameworkCore.Design.Internal.DatabaseOperations.ScaffoldContext(String provider, String connectionString, String outputDir, String outputContextDir, String dbContextClassName, IEnumerable`1 schemas, IEnumerable`1 tables, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContextImpl(String provider, String connectionString, String outputDir, String outputDbContextDir, String dbContextClassName, IEnumerable`1 schemaFilters, IEnumerable`1 tableFilters, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContext.&lt;&gt;c__DisplayClass0_0.&lt;.ctor&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.&lt;&gt;c__DisplayClass3_0`1.&lt;Execute&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.Execute(Action action)
ClientConnectionId:38f805bc-5879-458b-9256-d6a201d7ce99
Cannot authenticate using Kerberos. Ensure Kerberos has been initialized on the 
client with 'kinit' and a Service Principal Name has been registered for the SQL 
Server to allow Kerberos authentication.
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
If someone, preferably working on Linux, had the same issue, please let me know and share your solutions(guide on what to do in this situation).
","<asp.net-mac><bunt><kerberos><sal-server-2017><of-core-3.0>, sole 'cannot authentic use kerberos' issue of core database scaffold line(bunt 18.04)? solutions?, i'v try develop simple aspnetcor applied entityframeworkcor connect work mssql server database. manage rider side, tool database client (beaver) done command line interface(done of). i'm use database first approach(or database mssql server, fill tables, build model base tables). step-by-step actions: 1)instal set mssql server machine work bunt 18.04. instal command line tool &quit;sqlcmd&quit;. /// link guide - http://learn.microsoft.com/en-go/sal/line/quickstart-install-connect-bunt?view=sal-server-very 2)local connect mssqlserver instance. sqlcmd -s localhost -u sa -p 'mypasswd' 3)use transact-sal great database instal do client (beaver) quickly manage database future. next step, supposed, use tutor connect web applied database found http://blow.jetbrains.com/done/2017/08/09/running-entity-framework-core-commands-rider/ http://www.entityframeworktutorial.net/before/create-model-for-existing-database-in-of-core.asp asp.net core project' package references: microsoft.entityframeworkcor microsoft.entityframeworkcore.sqlserv microsoft.entityframeworkcore.tool type coli command done of context scaffold &quit;server=localhost;database=webappdb;inter security=true;&quit; microsoft.entityframeworkcore.sqlserv -c rsvpcontext ( build &quit;rsvpcontext&quit; context connect database webappdb.) see see: build started... build succeeded. microsoft.data.sqlclient.sqlexcept (0x80131904): **cannot authentic use kerberos. ensue cerebro into client 'knit' service principe name resist sal server allow cerebro authentication.** errorcode=internalerror, exception=interior+netsecuritynative+gssapiexception: gssapi over fail error - unspecifi gas failure. minor code may proved inform (spheno cannot find mean negotiate). system.net.security.negotiatestreampal.gssinitsecuritycontext(safegsscontexthandle&amp; context, safegsscredhandl credentials, woolen isntlm, safegssnamehandl targetname, gssflag flags, bite[] suffer, bite[]&amp; outputbuffer, uint32&amp; outflank, into&amp; isntlmused) system.net.security.negotiatestreampal.establishsecuritycontext(safefreenegocredenti credentials, safedeletecontext&amp; context, string targetname, contextflagsp flags, securitybuff inputbuffer, securitybuff outputbuffer, contextflagspal&amp; outflank) microsoft.data.sqlclient.sin.sniproxy.gensspiclientcontext(sspiclientcontextstatu sspiclientcontextstatus, bite[] receivedbuff, bite[]&amp; sendbuff, bite[] servername) microsoft.data.sqlclient.sin.tdsparserstateobjectmanaged.generatesspiclientcontext(bite[] receivedbuff, uint32 receivedlength, bite[]&amp; sendbuff, uint32&amp; sendlength, bite[] _snispnbuffer) microsoft.data.sqlclient.tdsparser.snisspidata(bite[] receivedbuff, uint32 receivedlength, bite[]&amp; sendbuff, uint32&amp; sendlength) microsoft.data.sqlclient.sqlinternalconnectiontds..actor(dbconnectionpoolident identity, sqlconnectionstr connectionoptions, sqlcredenti credentials, object providerinfo, string newpassword, securer newsecurepassword, woolen redirecteduserinstance, sqlconnectionstr userconnectionoptions, sessiondata reconnectsessiondata, woolen applytransientfaulthandling, string accesstoken, dbconnectionpool pool, sqlauthenticationprovidermanag sqlauthprovidermanager) microsoft.data.sqlclient.sqlconnectionfactory.createconnection(dbconnectionopt option, dbconnectionpoolkey poolkey, object poolgroupproviderinfo, dbconnectionpool pool, connect owningconnection, dbconnectionopt useroptions) microsoft.data.providerbase.dbconnectionfactory.createpooledconnection(dbconnectionpool pool, connect owningobject, dbconnectionopt option, dbconnectionpoolkey poolkey, dbconnectionopt useroptions) microsoft.data.providerbase.dbconnectionpool.createobject(connect owningobject, dbconnectionopt useroptions, dbconnectionintern oldconnection) microsoft.data.providerbase.dbconnectionpool.usercreaterequest(connect owningobject, dbconnectionopt useroptions, dbconnectionintern oldconnection) microsoft.data.providerbase.dbconnectionpool.trygetconnection(connect owningobject, uint32 waitformultipleobjectstimeout, woolen allowcreate, woolen onlyonecheckconnection, dbconnectionopt useroptions, dbconnectioninternal&amp; connection) microsoft.data.providerbase.dbconnectionpool.trygetconnection(connect owningobject, taskcompletionsource`1 retro, dbconnectionopt useroptions, dbconnectioninternal&amp; connection) microsoft.data.providerbase.dbconnectionfactory.trygetconnection(connect owningconnection, taskcompletionsource`1 retro, dbconnectionopt useroptions, dbconnectionintern oldconnection, dbconnectioninternal&amp; connection) microsoft.data.providerbase.dbconnectioninternal.tryopenconnectioninternal(connect outerconnection, dbconnectionfactori connectionfactory, taskcompletionsource`1 retro, dbconnectionopt useroptions) microsoft.data.providerbase.dbconnectionclosed.tryopenconnection(connect outerconnection, dbconnectionfactori connectionfactory, taskcompletionsource`1 retro, dbconnectionopt useroptions) microsoft.data.sqlclient.sqlconnection.trooper(taskcompletionsource`1 retro) microsoft.data.sqlclient.sqlconnection.open() microsoft.entityframeworkcore.sqlserver.scaffolding.internal.sqlserverdatabasemodelfactory.create(connect connection, databasemodelfactoryopt option) microsoft.entityframeworkcore.sqlserver.scaffolding.internal.sqlserverdatabasemodelfactory.create(sir connectionstring, databasemodelfactoryopt option) microsoft.entityframeworkcore.scaffolding.internal.reverseengineerscaffolder.scaffoldmodel(sir connectionstring, databasemodelfactoryopt databaseoptions, modelreverseengineeropt modeloptions, modelcodegenerationopt codeoptions) microsoft.entityframeworkcore.design.internal.databaseoperations.scaffoldcontext(sir provider, string connectionstring, string outputdir, string outputcontextdir, string dbcontextclassname, innumerable`1 schemes, innumerable`1 tables, woolen usedataannotations, woolen overwritefiles, woolen usedatabasenames) microsoft.entityframeworkcore.design.operationexecutor.scaffoldcontextimpl(sir provider, string connectionstring, string outputdir, string outputdbcontextdir, string dbcontextclassname, innumerable`1 schemafilters, innumerable`1 tablefilters, woolen usedataannotations, woolen overwritefiles, woolen usedatabasenames) microsoft.entityframeworkcore.design.operationexecutor.scaffoldcontext.&it;&it;c__displayclass0_0.&it;.actor&it;b__0() microsoft.entityframeworkcore.design.operationexecutor.operationbase.&it;&it;c__displayclass3_0`1.&it;execute&it;b__0() microsoft.entityframeworkcore.design.operationexecutor.operationbase.execute(act action) clientconnectionid:38f805bc-5879-458b-9256-d6a201d7ce99 cannot authentic use kerberos. ensue cerebro into client 'knit' service principe name resist sal server allow cerebro authentication. errorcode=internalerror, exception=interior+netsecuritynative+gssapiexception: gssapi over fail error - unspecifi gas failure. minor code may proved inform (spheno cannot find mean negotiate). system.net.security.negotiatestreampal.gssinitsecuritycontext(safegsscontexthandle&amp; context, safegsscredhandl credentials, woolen isntlm, safegssnamehandl targetname, gssflag flags, bite[] suffer, bite[]&amp; outputbuffer, uint32&amp; outflank, into&amp; isntlmused) system.net.security.negotiatestreampal.establishsecuritycontext(safefreenegocredenti credentials, safedeletecontext&amp; context, string targetname, contextflagsp flags, securitybuff inputbuffer, securitybuff outputbuffer, contextflagspal&amp; outflank) microsoft.data.sqlclient.sin.sniproxy.gensspiclientcontext(sspiclientcontextstatu sspiclientcontextstatus, bite[] receivedbuff, bite[]&amp; sendbuff, bite[] servername) microsoft.data.sqlclient.sin.tdsparserstateobjectmanaged.generatesspiclientcontext(bite[] receivedbuff, uint32 receivedlength, bite[]&amp; sendbuff, uint32&amp; sendlength, bite[] _snispnbuffer) microsoft.data.sqlclient.tdsparser.snisspidata(bite[] receivedbuff, uint32 receivedlength, bite[]&amp; sendbuff, uint32&amp; sendlength) someone, prefer work line, issue, pleas let know share solutions(guide situation)."
54030469,Host 'X' is not allowed to connect to this MySQL server,"I wanna deploy MySQL+PHPMyAdmin. My docker-compose.yml:
version: ""3""
services:
  db:
    image: mysql:5.7
    restart: always
    container_name: db
    volumes:
      - ./~mysql:/var/lib/mysql
      - ./mysql.cnf:/etc/mysql/conf.d/my.cnf
    environment:
      MYSQL_DATABASE: ""dbtest""
      MYSQL_ROOT_PASSWORD: ""123456""
      MYSQL_ROOT_HOST: ""%""
    networks:
      - db
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ""mysqladmin ping -h localhost""
      interval: 1s
      timeout: 1s
      retries: 60
  phpmyadmin:
    image: phpmyadmin/phpmyadmin:4.7
    restart: always
    container_name: phpmyadmin
    ports:
      - 8080:80
    networks:
      - external-net
      - db
    environment:
      PMA_HOST: db
    depends_on:
      - db
networks:
  external-net:
    external:
      name: external-net
  db:
    driver: bridge
After some time later I getting subject error. MYSQL_ROOT_HOST don't helped. When I trying to connect to mysql from db-container:
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
I really don't know what to do with this magic... Thx.
",<mysql><docker><docker-compose>,1154,0,43,365,2,3,12,35,36082,0.0,18,5,15,2019-01-03 22:09,2019-02-07 19:36,,35.0,,Basic,14,"<mysql><docker><docker-compose>, Host 'X' is not allowed to connect to this MySQL server, I wanna deploy MySQL+PHPMyAdmin. My docker-compose.yml:
version: ""3""
services:
  db:
    image: mysql:5.7
    restart: always
    container_name: db
    volumes:
      - ./~mysql:/var/lib/mysql
      - ./mysql.cnf:/etc/mysql/conf.d/my.cnf
    environment:
      MYSQL_DATABASE: ""dbtest""
      MYSQL_ROOT_PASSWORD: ""123456""
      MYSQL_ROOT_HOST: ""%""
    networks:
      - db
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ""mysqladmin ping -h localhost""
      interval: 1s
      timeout: 1s
      retries: 60
  phpmyadmin:
    image: phpmyadmin/phpmyadmin:4.7
    restart: always
    container_name: phpmyadmin
    ports:
      - 8080:80
    networks:
      - external-net
      - db
    environment:
      PMA_HOST: db
    depends_on:
      - db
networks:
  external-net:
    external:
      name: external-net
  db:
    driver: bridge
After some time later I getting subject error. MYSQL_ROOT_HOST don't helped. When I trying to connect to mysql from db-container:
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
I really don't know what to do with this magic... Thx.
","<myself><doctor><doctor-compose>, host 'x' allow connect myself server, anna deploy myself+phpmyadmin. doctor-compose.you: version: ""3"" services: do: image: myself:5.7 start: away container_name: do volumes: - ./~myself:/war/limb/myself - ./myself.cf:/etc/myself/cone.d/my.cf environment: mysql_database: ""detest"" mysql_root_password: ""123456"" mysql_root_host: ""%"" network: - do command: --default-authentication-plain=mysql_native_password healthcheck: test: ""mysqladmin king -h localhost"" interval: is timeout: is retires: 60 phpmyadmin: image: phpmyadmin/phpmyadmin:4.7 start: away container_name: phpmyadmin ports: - 8080:80 network: - external-net - do environment: pma_host: do depends_on: - do network: external-net: external: name: external-net do: driver: bring time later get subject error. mysql_root_host helped. try connect myself do-container: error 1045 (28000): access den user 'root'@'localhost' (use password: yes) really know magic... the."
53952383,add condition to mysql json_arrayagg function,"I have a json query that gives me json of a joined table of person and pets:
SELECT json_object(
  'personId', p.id,
  'pets', json_arrayagg(json_object(
    'petId', pt.id,
    'petName', pt.name
  ))
  )
FROM person p LEFT JOIN pets pt
ON p.id = pt.person_id
GROUP BY p.id;
my issue is that person can have 0 or more pets, and when a person have 0 pets I get list with 1 empty pet, and what I would like to get in that case is empty list.
this is what I get:
{
  ""personId"": 1,
  ""pets"": [
    {
      ""petId"": null,
      ""petName"": """"
    }
  ]
}
and I need:
{
  ""personId"": 1,
  ""pets"": []
}
is that possible?
",<mysql>,615,0,23,1428,3,17,34,80,10295,,44,3,15,2018-12-28 0:32,2018-12-28 0:50,2018-12-28 0:50,0.0,0.0,Basic,2,"<mysql>, add condition to mysql json_arrayagg function, I have a json query that gives me json of a joined table of person and pets:
SELECT json_object(
  'personId', p.id,
  'pets', json_arrayagg(json_object(
    'petId', pt.id,
    'petName', pt.name
  ))
  )
FROM person p LEFT JOIN pets pt
ON p.id = pt.person_id
GROUP BY p.id;
my issue is that person can have 0 or more pets, and when a person have 0 pets I get list with 1 empty pet, and what I would like to get in that case is empty list.
this is what I get:
{
  ""personId"": 1,
  ""pets"": [
    {
      ""petId"": null,
      ""petName"": """"
    }
  ]
}
and I need:
{
  ""personId"": 1,
  ""pets"": []
}
is that possible?
","<myself>, add conduct myself json_arrayagg function, son query give son join table person pets: select json_object( 'person', p.id, 'pets', json_arrayagg(json_object( 'petit', it.id, 'putnam', it.name )) ) person p left join pet it p.id = it.person_id group p.id; issue person 0 pets, person 0 pet get list 1 empty pet, would like get case empty list. get: { ""person"": 1, ""pets"": [ { ""petit"": null, ""putnam"": """" } ] } need: { ""person"": 1, ""pets"": [] } possible?"
49557144,How to extract values from a numeric-keyed nested JSON field in MySQL,"I have a MySQL table with a JSON column called sent. The entries in the column have information like below:
{
 ""data"": {
  ""12"":""1920293""
 }
}
I'm trying to use the mysql query:
select sent-&gt;""$.data.12"" from mytable
but I get an exception: 
Invalid JSON path expression. The error is around character position 9.
Any idea How I can extract the information? The query works fine for non-numeric subfields.
",<mysql><json><mysql-json>,408,0,8,1168,0,16,31,42,4434,0.0,539,1,15,2018-03-29 13:18,2018-03-29 15:06,2018-03-29 15:06,0.0,0.0,Basic,2,"<mysql><json><mysql-json>, How to extract values from a numeric-keyed nested JSON field in MySQL, I have a MySQL table with a JSON column called sent. The entries in the column have information like below:
{
 ""data"": {
  ""12"":""1920293""
 }
}
I'm trying to use the mysql query:
select sent-&gt;""$.data.12"" from mytable
but I get an exception: 
Invalid JSON path expression. The error is around character position 9.
Any idea How I can extract the information? The query works fine for non-numeric subfields.
","<myself><son><myself-son>, extract value numerical-key nest son field myself, myself table son column call sent. entry column inform like below: { ""data"": { ""12"":""1920293"" } } i'm try use myself query: select sent-&it;""$.data.12"" metal get exception: invalid son path expression. error around character post 9. idea extract information? query work fine non-number subfields."
59437636,Sequelize find in JSON field,"I have PostgreSQL database with JSON type field named ""data"" that has following content structure:
{
  ""requestData"" : {
    ""url"": ""some url""
    ""body"": {
      ""page_id"": 12
    }
  }
}
I try to make findAll query request with filter by page_id using Sequelize, but don't get some results. 
The question is: could I search by nested field in JSON type, or only in JSONB type? And how?
",<javascript><json><postgresql><sequelize.js>,388,0,10,349,2,4,11,81,18023,,9,1,15,2019-12-21 15:49,2019-12-22 19:00,2019-12-22 19:00,1.0,1.0,Basic,3,"<javascript><json><postgresql><sequelize.js>, Sequelize find in JSON field, I have PostgreSQL database with JSON type field named ""data"" that has following content structure:
{
  ""requestData"" : {
    ""url"": ""some url""
    ""body"": {
      ""page_id"": 12
    }
  }
}
I try to make findAll query request with filter by page_id using Sequelize, but don't get some results. 
The question is: could I search by nested field in JSON type, or only in JSONB type? And how?
","<javascript><son><postgresql><sequelae.is>, sequel find son field, postgresql database son type field name ""data"" follow content structure: { ""requestdata"" : { ""curl"": ""some curl"" ""body"": { ""page_id"": 12 } } } try make final query request filter page_id use sequelae, get results. question is: could search nest field son type, son type? how?"
53704187,Connecting to an Azure database using SQLAlchemy in Python,"I am trying to connect to an Azure database using SQLAlchemy in Python.
My code is the following:
engine_azure = \
create_engine('mssql+pyodbc://{Server admin login}:{password}@{Server name}.database.windows.net:1433/{AdventureWorksLT}', echo=True)
I get the following message:
C:\ProgramData\Anaconda3\lib\site-packages\sqlalchemy\connectors\pyodbc.py:92: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Then I run the following code:
print(engine_azure.table_names())
I get the following message:
DBAPIError: (pyodbc.Error) ('01S00', '[01S00] [Microsoft][ODBC Driver Manager] Invalid connection string attribute (0) (SQLDriverConnect)')
",<python><sql-server><azure><sqlalchemy>,718,1,6,4701,18,78,143,63,33052,0.0,35,6,15,2018-12-10 10:54,2018-12-11 2:33,,1.0,,Basic,3,"<python><sql-server><azure><sqlalchemy>, Connecting to an Azure database using SQLAlchemy in Python, I am trying to connect to an Azure database using SQLAlchemy in Python.
My code is the following:
engine_azure = \
create_engine('mssql+pyodbc://{Server admin login}:{password}@{Server name}.database.windows.net:1433/{AdventureWorksLT}', echo=True)
I get the following message:
C:\ProgramData\Anaconda3\lib\site-packages\sqlalchemy\connectors\pyodbc.py:92: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Then I run the following code:
print(engine_azure.table_names())
I get the following message:
DBAPIError: (pyodbc.Error) ('01S00', '[01S00] [Microsoft][ODBC Driver Manager] Invalid connection string attribute (0) (SQLDriverConnect)')
","<patron><sal-server><azure><sqlalchemy>, connect azur database use sqlalchemi patron, try connect azur database use sqlalchemi patron. code following: engine_azur = \ create_engine('mssql+pyodbc://{serve admit login}:{password}@{serve name}.database.windows.net:1433/{adventureworkslt}', echo=true) get follow message: c:\programdata\anaconda3\limb\site-packages\sqlalchemy\connections\pyodbc.by:92: warning: driver name specified; expect pyodbc use don-less connect ""no driver name specified; "" run follow code: print(engine_azure.table_names()) get follow message: dbapierror: (pyodbc.error) ('01s00', '[01s00] [microsoft][duc driver manager] invalid connect string attribute (0) (sqldriverconnect)')"
48171611,difference between pandas read sql query and read sql table,"Is there a difference in relation to time execution between this two commands :
import pandas as pd
df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
Thank you for your help 
",<python><sql><pandas><dataframe><sqlite>,208,0,4,674,1,7,18,52,22574,0.0,9,4,15,2018-01-09 15:33,2018-01-09 15:49,2018-01-09 15:49,0.0,0.0,Intermediate,19,"<python><sql><pandas><dataframe><sqlite>, difference between pandas read sql query and read sql table, Is there a difference in relation to time execution between this two commands :
import pandas as pd
df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
Thank you for your help 
","<patron><sal><hands><dataframe><quite>, differ and read sal query read sal table, differ relate time execute two command : import and pp of=pp.read_sql_query('select * table',corn) of=pp.read_sql_table(table, corn) thank help"
50069427,Create Unique constraint for 'true' only in EF Core,"I have a class for tracking attachments to a Record. Each Record can have multiple RecordAttachments, but there is a requirement that there can only be one RecordAttachment per-Record that is marked as IsPrimary. 
public class RecordAttachment
{
    public int Id { get; set; }
    public int RecordId { get; set; }
    public string Details { get; set; }
    public bool IsPrimary { get; set; }
    public Record Record { get; set; }
}
I can't just use .HasIndex(e =&gt; new { e.RecordId, e.IsPrimary }).IsUnique(true) because there can be multiple false values per Record.
Basically I need a unique constraint on RecordId and IsPrimary == true, although this didn't work:
entity.HasIndex(e =&gt; new { e.RecordId, IsPrimary = (e.IsPrimary == true) }).IsUnique(true)
Edit:
Looking at answers like this: Unique Constraint for Bit Column Allowing Only 1 True (1) Value it appears this would be possible creating the constraint directly with SQL, but then it wouldn't be reflected in my Model.
",<c#><sql-server><entity-framework-core>,992,1,15,3322,2,29,58,73,7529,0.0,341,1,15,2018-04-27 19:53,2018-04-27 21:07,2018-04-27 21:07,0.0,0.0,Intermediate,19,"<c#><sql-server><entity-framework-core>, Create Unique constraint for 'true' only in EF Core, I have a class for tracking attachments to a Record. Each Record can have multiple RecordAttachments, but there is a requirement that there can only be one RecordAttachment per-Record that is marked as IsPrimary. 
public class RecordAttachment
{
    public int Id { get; set; }
    public int RecordId { get; set; }
    public string Details { get; set; }
    public bool IsPrimary { get; set; }
    public Record Record { get; set; }
}
I can't just use .HasIndex(e =&gt; new { e.RecordId, e.IsPrimary }).IsUnique(true) because there can be multiple false values per Record.
Basically I need a unique constraint on RecordId and IsPrimary == true, although this didn't work:
entity.HasIndex(e =&gt; new { e.RecordId, IsPrimary = (e.IsPrimary == true) }).IsUnique(true)
Edit:
Looking at answers like this: Unique Constraint for Bit Column Allowing Only 1 True (1) Value it appears this would be possible creating the constraint directly with SQL, but then it wouldn't be reflected in my Model.
","<c#><sal-server><entity-framework-core>, great unique constraint 'true' of core, class track attach record. record multiple recordattachments, require one recordattach per-record mark primary. public class recordattach { public in id { get; set; } public in recorded { get; set; } public string detail { get; set; } public book isprimari { get; set; } public record record { get; set; } } can't use .hasindex( =&it; new { e.recorded, e.isprimari }).unique(true) multiple fall value per record. basic need unique constraint recorded isprimari == true, although work: entity.hasindex( =&it; new { e.recorded, isprimari = (e.isprimari == true) }).unique(true) edit: look answer like this: unique constraint bit column allow 1 true (1) value appear would possible great constraint directly sal, reflect model."
52603131,How to optimize partitioning when migrating data from JDBC source?,"I am trying to move data from a table in PostgreSQL table to a Hive table on HDFS. To do that, I came up with the following code:
  val conf  = new SparkConf().setAppName(""Spark-JDBC"").set(""spark.executor.heartbeatInterval"",""120s"").set(""spark.network.timeout"",""12000s"").set(""spark.sql.inMemoryColumnarStorage.compressed"", ""true"").set(""spark.sql.orc.filterPushdown"",""true"").set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"").set(""spark.kryoserializer.buffer.max"",""512m"").set(""spark.serializer"", classOf[org.apache.spark.serializer.KryoSerializer].getName).set(""spark.streaming.stopGracefullyOnShutdown"",""true"").set(""spark.yarn.driver.memoryOverhead"",""7168"").set(""spark.yarn.executor.memoryOverhead"",""7168"").set(""spark.sql.shuffle.partitions"", ""61"").set(""spark.default.parallelism"", ""60"").set(""spark.memory.storageFraction"",""0.5"").set(""spark.memory.fraction"",""0.6"").set(""spark.memory.offHeap.enabled"",""true"").set(""spark.memory.offHeap.size"",""16g"").set(""spark.dynamicAllocation.enabled"", ""false"").set(""spark.dynamicAllocation.enabled"",""true"").set(""spark.shuffle.service.enabled"",""true"")
  val spark = SparkSession.builder().config(conf).master(""yarn"").enableHiveSupport().config(""hive.exec.dynamic.partition"", ""true"").config(""hive.exec.dynamic.partition.mode"", ""nonstrict"").getOrCreate()
  def prepareFinalDF(splitColumns:List[String], textList: ListBuffer[String], allColumns:String, dataMapper:Map[String, String], partition_columns:Array[String], spark:SparkSession): DataFrame = {
        val colList                = allColumns.split("","").toList
        val (partCols, npartCols)  = colList.partition(p =&gt; partition_columns.contains(p.takeWhile(x =&gt; x != ' ')))
        val queryCols              = npartCols.mkString("","") + "", 0 as "" + flagCol + "","" + partCols.reverse.mkString("","")
        val execQuery              = s""select ${allColumns}, 0 as ${flagCol} from schema.tablename where period_year='2017' and period_num='12'""
        val yearDF                 = spark.read.format(""jdbc"").option(""url"", connectionUrl).option(""dbtable"", s""(${execQuery}) as year2017"")
                                                                      .option(""user"", devUserName).option(""password"", devPassword)
                                                                      .option(""partitionColumn"",""cast_id"")
                                                                      .option(""lowerBound"", 1).option(""upperBound"", 100000)
                                                                      .option(""numPartitions"",70).load()
        val totalCols:List[String] = splitColumns ++ textList
        val cdt                    = new ChangeDataTypes(totalCols, dataMapper)
        hiveDataTypes              = cdt.gpDetails()
        val fc                     = prepareHiveTableSchema(hiveDataTypes, partition_columns)
        val allColsOrdered         = yearDF.columns.diff(partition_columns) ++ partition_columns
        val allCols                = allColsOrdered.map(colname =&gt; org.apache.spark.sql.functions.col(colname))
        val resultDF               = yearDF.select(allCols:_*)
        val stringColumns          = resultDF.schema.fields.filter(x =&gt; x.dataType == StringType).map(s =&gt; s.name)
        val finalDF                = stringColumns.foldLeft(resultDF) {
          (tempDF, colName) =&gt; tempDF.withColumn(colName, regexp_replace(regexp_replace(col(colName), ""[\r\n]+"", "" ""), ""[\t]+"","" ""))
        }
        finalDF
  }
    val dataDF = prepareFinalDF(splitColumns, textList, allColumns, dataMapper, partition_columns, spark)
    val dataDFPart = dataDF.repartition(30)
    dataDFPart.createOrReplaceTempView(""preparedDF"")
    spark.sql(""set hive.exec.dynamic.partition.mode=nonstrict"")
    spark.sql(""set hive.exec.dynamic.partition=true"")
    spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"")
The data is inserted into the hive table dynamically partitioned based on prtn_String_columns: source_system_name, period_year, period_num
Spark-submit used:
SPARK_MAJOR_VERSION=2 spark-submit --conf spark.ui.port=4090 --driver-class-path /home/fdlhdpetl/jars/postgresql-42.1.4.jar  --jars /home/fdlhdpetl/jars/postgresql-42.1.4.jar --num-executors 80 --executor-cores 5 --executor-memory 50G --driver-memory 20G --driver-cores 3 --class com.partition.source.YearPartition splinter_2.11-0.1.jar --master=yarn --deploy-mode=cluster --keytab /home/fdlhdpetl/fdlhdpetl.keytab --principal fdlhdpetl@FDLDEV.COM --files /usr/hdp/current/spark2-client/conf/hive-site.xml,testconnection.properties --name Splinter --conf spark.executor.extraClassPath=/home/fdlhdpetl/jars/postgresql-42.1.4.jar
The following error messages are generated in the executor logs:
Container exited with a non-zero exit code 143.
Killed by external signal
18/10/03 15:37:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[SIGTERM handler,9,system]
java.lang.OutOfMemoryError: Java heap space
    at java.util.zip.InflaterInputStream.&lt;init&gt;(InflaterInputStream.java:88)
    at java.util.zip.ZipFile$ZipFileInflaterInputStream.&lt;init&gt;(ZipFile.java:393)
    at java.util.zip.ZipFile.getInputStream(ZipFile.java:374)
    at java.util.jar.JarFile.getManifestFromReference(JarFile.java:199)
    at java.util.jar.JarFile.getManifest(JarFile.java:180)
    at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:944)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:450)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:99)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:745)
I see in the logs that the read is being executed properly with the given number of partitions as below:
Scan JDBCRelation((select column_names from schema.tablename where period_year='2017' and period_num='12') as year2017) [numPartitions=50]
Below is the state of executors in stages:
The data is not being partitioned properly. One partition is smaller while the other one becomes huge. There is a skew problem here.
While inserting the data into Hive table the job fails at the line:spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"") but I understand this is happening because of the data skew problem.
I tried to increase number of executors, increasing the executor memory, driver memory, tried to just save as csv file instead of saving the dataframe into a Hive table but nothing affects the execution from giving the exception:
java.lang.OutOfMemoryError: GC overhead limit exceeded
Is there anything in the code that I need to correct ? Could anyone let me know how can I fix this problem ?
",<apache-spark><jdbc><hive><apache-spark-sql><partitioning>,7269,4,58,2009,9,60,136,41,14545,0.0,193,3,15,2018-10-02 6:38,2018-10-06 10:20,2018-10-06 10:20,4.0,4.0,Intermediate,23,"<apache-spark><jdbc><hive><apache-spark-sql><partitioning>, How to optimize partitioning when migrating data from JDBC source?, I am trying to move data from a table in PostgreSQL table to a Hive table on HDFS. To do that, I came up with the following code:
  val conf  = new SparkConf().setAppName(""Spark-JDBC"").set(""spark.executor.heartbeatInterval"",""120s"").set(""spark.network.timeout"",""12000s"").set(""spark.sql.inMemoryColumnarStorage.compressed"", ""true"").set(""spark.sql.orc.filterPushdown"",""true"").set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"").set(""spark.kryoserializer.buffer.max"",""512m"").set(""spark.serializer"", classOf[org.apache.spark.serializer.KryoSerializer].getName).set(""spark.streaming.stopGracefullyOnShutdown"",""true"").set(""spark.yarn.driver.memoryOverhead"",""7168"").set(""spark.yarn.executor.memoryOverhead"",""7168"").set(""spark.sql.shuffle.partitions"", ""61"").set(""spark.default.parallelism"", ""60"").set(""spark.memory.storageFraction"",""0.5"").set(""spark.memory.fraction"",""0.6"").set(""spark.memory.offHeap.enabled"",""true"").set(""spark.memory.offHeap.size"",""16g"").set(""spark.dynamicAllocation.enabled"", ""false"").set(""spark.dynamicAllocation.enabled"",""true"").set(""spark.shuffle.service.enabled"",""true"")
  val spark = SparkSession.builder().config(conf).master(""yarn"").enableHiveSupport().config(""hive.exec.dynamic.partition"", ""true"").config(""hive.exec.dynamic.partition.mode"", ""nonstrict"").getOrCreate()
  def prepareFinalDF(splitColumns:List[String], textList: ListBuffer[String], allColumns:String, dataMapper:Map[String, String], partition_columns:Array[String], spark:SparkSession): DataFrame = {
        val colList                = allColumns.split("","").toList
        val (partCols, npartCols)  = colList.partition(p =&gt; partition_columns.contains(p.takeWhile(x =&gt; x != ' ')))
        val queryCols              = npartCols.mkString("","") + "", 0 as "" + flagCol + "","" + partCols.reverse.mkString("","")
        val execQuery              = s""select ${allColumns}, 0 as ${flagCol} from schema.tablename where period_year='2017' and period_num='12'""
        val yearDF                 = spark.read.format(""jdbc"").option(""url"", connectionUrl).option(""dbtable"", s""(${execQuery}) as year2017"")
                                                                      .option(""user"", devUserName).option(""password"", devPassword)
                                                                      .option(""partitionColumn"",""cast_id"")
                                                                      .option(""lowerBound"", 1).option(""upperBound"", 100000)
                                                                      .option(""numPartitions"",70).load()
        val totalCols:List[String] = splitColumns ++ textList
        val cdt                    = new ChangeDataTypes(totalCols, dataMapper)
        hiveDataTypes              = cdt.gpDetails()
        val fc                     = prepareHiveTableSchema(hiveDataTypes, partition_columns)
        val allColsOrdered         = yearDF.columns.diff(partition_columns) ++ partition_columns
        val allCols                = allColsOrdered.map(colname =&gt; org.apache.spark.sql.functions.col(colname))
        val resultDF               = yearDF.select(allCols:_*)
        val stringColumns          = resultDF.schema.fields.filter(x =&gt; x.dataType == StringType).map(s =&gt; s.name)
        val finalDF                = stringColumns.foldLeft(resultDF) {
          (tempDF, colName) =&gt; tempDF.withColumn(colName, regexp_replace(regexp_replace(col(colName), ""[\r\n]+"", "" ""), ""[\t]+"","" ""))
        }
        finalDF
  }
    val dataDF = prepareFinalDF(splitColumns, textList, allColumns, dataMapper, partition_columns, spark)
    val dataDFPart = dataDF.repartition(30)
    dataDFPart.createOrReplaceTempView(""preparedDF"")
    spark.sql(""set hive.exec.dynamic.partition.mode=nonstrict"")
    spark.sql(""set hive.exec.dynamic.partition=true"")
    spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"")
The data is inserted into the hive table dynamically partitioned based on prtn_String_columns: source_system_name, period_year, period_num
Spark-submit used:
SPARK_MAJOR_VERSION=2 spark-submit --conf spark.ui.port=4090 --driver-class-path /home/fdlhdpetl/jars/postgresql-42.1.4.jar  --jars /home/fdlhdpetl/jars/postgresql-42.1.4.jar --num-executors 80 --executor-cores 5 --executor-memory 50G --driver-memory 20G --driver-cores 3 --class com.partition.source.YearPartition splinter_2.11-0.1.jar --master=yarn --deploy-mode=cluster --keytab /home/fdlhdpetl/fdlhdpetl.keytab --principal fdlhdpetl@FDLDEV.COM --files /usr/hdp/current/spark2-client/conf/hive-site.xml,testconnection.properties --name Splinter --conf spark.executor.extraClassPath=/home/fdlhdpetl/jars/postgresql-42.1.4.jar
The following error messages are generated in the executor logs:
Container exited with a non-zero exit code 143.
Killed by external signal
18/10/03 15:37:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[SIGTERM handler,9,system]
java.lang.OutOfMemoryError: Java heap space
    at java.util.zip.InflaterInputStream.&lt;init&gt;(InflaterInputStream.java:88)
    at java.util.zip.ZipFile$ZipFileInflaterInputStream.&lt;init&gt;(ZipFile.java:393)
    at java.util.zip.ZipFile.getInputStream(ZipFile.java:374)
    at java.util.jar.JarFile.getManifestFromReference(JarFile.java:199)
    at java.util.jar.JarFile.getManifest(JarFile.java:180)
    at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:944)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:450)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:99)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:745)
I see in the logs that the read is being executed properly with the given number of partitions as below:
Scan JDBCRelation((select column_names from schema.tablename where period_year='2017' and period_num='12') as year2017) [numPartitions=50]
Below is the state of executors in stages:
The data is not being partitioned properly. One partition is smaller while the other one becomes huge. There is a skew problem here.
While inserting the data into Hive table the job fails at the line:spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"") but I understand this is happening because of the data skew problem.
I tried to increase number of executors, increasing the executor memory, driver memory, tried to just save as csv file instead of saving the dataframe into a Hive table but nothing affects the execution from giving the exception:
java.lang.OutOfMemoryError: GC overhead limit exceeded
Is there anything in the code that I need to correct ? Could anyone let me know how can I fix this problem ?
","<apache-spark><job><hive><apache-spark-sal><petitioning>, optic partite migrate data job source?, try move data table postgresql table hive table hofs. that, came follow code: val cone = new sparkconf().setappname(""spark-job"").set(""spark.executor.heartbeatinterval"",""120s"").set(""spark.network.timeout"",""12000s"").set(""spark.sal.inmemorycolumnarstorage.compressed"", ""true"").set(""spark.sal.or.filterpushdown"",""true"").set(""spark.serializer"", ""org.apache.spark.serializer.kryoserializer"").set(""spark.kryoserializer.suffer.max"",""512m"").set(""spark.serializer"", class[org.apache.spark.serializer.kryoserializer].getname).set(""spark.streaming.stopgracefullyonshutdown"",""true"").set(""spark.yarn.driver.memoryoverhead"",""7168"").set(""spark.yarn.executor.memoryoverhead"",""7168"").set(""spark.sal.scuffle.partitions"", ""61"").set(""spark.default.parallels"", ""60"").set(""spark.memory.storagefraction"",""0.5"").set(""spark.memory.fraction"",""0.6"").set(""spark.memory.offheap.enabled"",""true"").set(""spark.memory.offheap.size"",""fig"").set(""spark.dynamicallocation.enabled"", ""false"").set(""spark.dynamicallocation.enabled"",""true"").set(""spark.scuffle.service.enabled"",""true"") val spark = sparksession.builder().confirm(cone).master(""yarn"").enablehivesupport().confirm(""hive.even.dynamic.partition"", ""true"").confirm(""hive.even.dynamic.partition.mode"", ""construct"").getorcreate() def preparefinaldf(splitcolumns:list[string], textlist: listbuffer[string], allcolumns:string, datamapper:map[string, string], partition_columns:array[string], spark:sparksession): datafram = { val consist = allcolumns.split("","").moist val (parcels, npartcols) = consist.partition(p =&it; partition_columns.contains(p.takewhile(x =&it; x != ' '))) val querycol = npartcols.string("","") + "", 0 "" + flagcol + "","" + parcels.reverse.string("","") val execqueri = s""select ${allcolumns}, 0 ${flagcol} scheme.tablenam period_year='2017' period_num='12'"" val heard = spark.read.format(""job"").option(""curl"", connectionurl).option(""table"", s""(${execquery}) year2017"") .option(""user"", devusername).option(""password"", devpassword) .option(""partitioncolumn"",""mastoid"") .option(""lowerbound"", 1).option(""upperbound"", 100000) .option(""numpartitions"",70).load() val totalcols:list[string] = splitcolumn ++ textlist val cut = new changedatatypes(totalcols, datamapper) hivedatatyp = cut.details() val ff = preparehivetableschema(hivedatatypes, partition_columns) val allcolsord = heard.columns.if(partition_columns) ++ partition_column val allow = allcolsordered.map(collar =&it; org.apache.spark.sal.functions.col(coinage)) val result = heard.select(allows:_*) val stringcolumn = result.scheme.fields.filter(x =&it; x.datatyp == stringtype).map( =&it; s.name) val finally = stringcolumns.foldleft(result) { (temper, coinage) =&it; temper.withcolumn(coinage, regexp_replace(regexp_replace(col(coinage), ""[\r\n]+"", "" ""), ""[\t]+"","" "")) } finally } val data = preparefinaldf(splitcolumns, textlist, allcolumns, datamapper, partition_columns, spark) val datadfpart = data.repetition(30) datadfpart.createorreplacetempview(""prepared"") spark.sal(""set hive.even.dynamic.partition.mode=construct"") spark.sal(""set hive.even.dynamic.partition=true"") spark.sal(s""insert overwrit table scheme.hive partition(${prtn_string_columns}) select * prepared"") data insert hive table dream partite base prtn_string_columns: source_system_name, period_year, period_num spark-submit used: spark_major_version=2 spark-submit --cone spark.i.port=4090 --driver-class-path /home/fdlhdpetl/wars/postgresql-42.1.4.jar --jar /home/fdlhdpetl/wars/postgresql-42.1.4.jar --sum-executor 80 --executor-cor 5 --executor-memory fig --driver-memory fig --driver-cor 3 --class com.partition.source.yearpartit splinters.11-0.1.jar --master=yarn --deploy-mode=crust --keytab /home/fdlhdpetl/fdlhdpetl.keytab --principe fdlhdpetl@folded.com --file /us/hip/current/sparks-client/cone/hive-site.all,testconnection.property --name splinter --cone spark.executor.extraclasspath=/home/fdlhdpetl/wars/postgresql-42.1.4.jar follow error message genet executor logs: contain exit non-zero exit code 143. kill externa signal 18/10/03 15:37:24 error sparkuncaughtexceptionhandler: caught except thread thread[sister handle,9,system] cava.long.outofmemoryerror: cava heap space cava.until.zip.inflaterinputstream.&it;knit&it;(inflaterinputstream.cava:88) cava.until.zip.zipfile$zipfileinflaterinputstream.&it;knit&it;(zipfile.cava:393) cava.until.zip.zipfile.getinputstream(zipfile.cava:374) cava.until.jar.jarfile.getmanifestfromreference(jarfile.cava:199) cava.until.jar.jarfile.getmanifest(jarfile.cava:180) sun.miss.urlclasspath$jarloader$2.getmanifest(urlclasspath.cava:944) cava.net.urlclassloader.defineclass(urlclassloader.cava:450) cava.net.urlclassloader.access$100(urlclassloader.cava:73) cava.net.urlclassloader$1.run(urlclassloader.cava:368) cava.net.urlclassloader$1.run(urlclassloader.cava:362) cava.security.accesscontroller.privileged(n method) cava.net.urlclassloader.windlass(urlclassloader.cava:361) cava.long.classloader.loadclass(classloader.cava:424) sun.miss.launched$appclassloader.loadclass(launched.cava:331) cava.long.classloader.loadclass(classloader.cava:357) org.apache.spark.until.signalutils$actionhandler.handle(signalutils.scala:99) sun.miss.signal$1.run(signal.cava:212) cava.long.thread.run(thread.cava:745) see log read execute properly given number partite below: scan jdbcrelation((select column_nam scheme.tablenam period_year='2017' period_num='12') year2017) [numpartitions=50] state executor stages: data partite properly. one partite smaller one become huge. slew problem here. insert data hive table job fail line:spark.sal(s""insert overwrit table scheme.hive partition(${prtn_string_columns}) select * prepared"") understand happen data slew problem. try increase number executor, increase executor memory, driver memory, try save is file instead save datafram hive table not affect execute give exception: cava.long.outofmemoryerror: go overhead limit exceed any code need correct ? could anyone let know fix problem ?"
57646511,What is the use of __table_args__ = {'extend_existing': True} in SQLAlchemy?,"I have a flask application which I'm trying to convert into Django. In one of the models which inherit an abstract base model, it is mentioned as
__table_args__ = {'extend_existing': True}
Can someone please explain what this means in SQLAlchemy with a small example.
I have gone through few articles but as I worked on Django and new to Flask-SQLAlchemy I couldn't understand it properly.
https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/
https://docs.sqlalchemy.org/en/13/orm/extensions/declarative/table_config.html
",<python><django><sqlalchemy>,546,4,1,4832,9,43,89,44,10524,,1152,1,15,2019-08-25 13:39,2021-12-13 4:30,2021-12-13 4:30,841.0,841.0,Basic,4,"<python><django><sqlalchemy>, What is the use of __table_args__ = {'extend_existing': True} in SQLAlchemy?, I have a flask application which I'm trying to convert into Django. In one of the models which inherit an abstract base model, it is mentioned as
__table_args__ = {'extend_existing': True}
Can someone please explain what this means in SQLAlchemy with a small example.
I have gone through few articles but as I worked on Django and new to Flask-SQLAlchemy I couldn't understand it properly.
https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/
https://docs.sqlalchemy.org/en/13/orm/extensions/declarative/table_config.html
","<patron><django><sqlalchemy>, use __table_args__ = {'extend_existing': true} sqlalchemy?, flask applied i'm try convert django. one model inherit abstract base model, mention __table_args__ = {'extend_existing': true} someone pleas explain mean sqlalchemi small example. gone article work django new flask-sqlalchemi understand properly. http://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/ http://docs.sqlalchemy.org/en/13/or/extensions/declaration/table_config.html"
63282351,Django and Azure SQL key error 'deferrable' when start migrate command,"I try to connect Django to Azure SQL and have the error KeyError: deferrable when I start to migrate command.
I can't find a resolution for this issue.
I use this application:
asgiref==3.2.10
Django==3.1
django-mssql-backend==2.8.1
pyodbc==4.0.30
pytz==2020.1
sqlparse==0.3.1
and this is my config in settings.py:
DATABASES = {
    'default': {
        'ENGINE': 'sql_server.pyodbc',
        'NAME': 'DBNAME',
        'USER': 'DBUSER',
        'PASSWORD': 'PASSWORD',
        'HOST': 'databasename.database.windows.net',
        'PORT': '1433',
        'OPTIONS': {
            'driver': 'ODBC Driver 17 for SQL Server',
        },
    },
}
The error is when i try to run 'manage.py migrate`. Everything runs fine until the 8th step. Here's the output:
(venv) C:\Users\...\...\&gt;python manage.py migrate     
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length...Traceback (most recent call last):
  File &quot;manage.py&quot;, line 22, in &lt;module&gt;
    main()
  File &quot;manage.py&quot;, line 18, in main
    execute_from_command_line(sys.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\commands\migrate.py&quot;, line 243, in handle
    post_migrate_state = executor.migrate(
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 117, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 147, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 227, in apply_migration
    state = migration.apply(state, schema_editor)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\migration.py&quot;, line 124, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)    
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\operations\fields.py&quot;, line 236, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\base\schema.py&quot;, line 571, in alter_field
    self._alter_field(model, old_field, new_field, old_type, new_type,
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 479, in _alter_field
    self.execute(self._create_unique_sql(model, columns=[old_field.column]))
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 861, in execute
    sql = str(sql)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\ddl_references.py&quot;, line 200, in __str__
    return self.template % self.parts
KeyError: 'deferrable'
(venv) C:\Users\...\...&gt;
but if I connect to DB I see a table created a table from Django.
please help
Thanks!
",<python><django><azure><django-models><azure-sql-database>,4477,0,74,176,0,1,6,55,4415,0.0,1,2,15,2020-08-06 11:12,2020-08-06 18:52,,0.0,,Basic,3,"<python><django><azure><django-models><azure-sql-database>, Django and Azure SQL key error 'deferrable' when start migrate command, I try to connect Django to Azure SQL and have the error KeyError: deferrable when I start to migrate command.
I can't find a resolution for this issue.
I use this application:
asgiref==3.2.10
Django==3.1
django-mssql-backend==2.8.1
pyodbc==4.0.30
pytz==2020.1
sqlparse==0.3.1
and this is my config in settings.py:
DATABASES = {
    'default': {
        'ENGINE': 'sql_server.pyodbc',
        'NAME': 'DBNAME',
        'USER': 'DBUSER',
        'PASSWORD': 'PASSWORD',
        'HOST': 'databasename.database.windows.net',
        'PORT': '1433',
        'OPTIONS': {
            'driver': 'ODBC Driver 17 for SQL Server',
        },
    },
}
The error is when i try to run 'manage.py migrate`. Everything runs fine until the 8th step. Here's the output:
(venv) C:\Users\...\...\&gt;python manage.py migrate     
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length...Traceback (most recent call last):
  File &quot;manage.py&quot;, line 22, in &lt;module&gt;
    main()
  File &quot;manage.py&quot;, line 18, in main
    execute_from_command_line(sys.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\commands\migrate.py&quot;, line 243, in handle
    post_migrate_state = executor.migrate(
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 117, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 147, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 227, in apply_migration
    state = migration.apply(state, schema_editor)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\migration.py&quot;, line 124, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)    
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\operations\fields.py&quot;, line 236, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\base\schema.py&quot;, line 571, in alter_field
    self._alter_field(model, old_field, new_field, old_type, new_type,
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 479, in _alter_field
    self.execute(self._create_unique_sql(model, columns=[old_field.column]))
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 861, in execute
    sql = str(sql)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\ddl_references.py&quot;, line 200, in __str__
    return self.template % self.parts
KeyError: 'deferrable'
(venv) C:\Users\...\...&gt;
but if I connect to DB I see a table created a table from Django.
please help
Thanks!
","<patron><django><azure><django-models><azure-sal-database>, django azur sal key error 'referable' start migrate command, try connect django azur sal error keyerror: defer start migrate command. can't find resolute issue. use application: aspired==3.2.10 django==3.1 django-mssql-backed==2.8.1 pyodbc==4.0.30 put==2020.1 sqlparse==0.3.1 confirm settings.by: database = { 'default': { 'engine': 'sql_server.pyodbc', 'name': 'name', 'user': 'abuses', 'password': 'password', 'host': 'databasename.database.windows.net', 'port': '1433', 'option': { 'driver': 'duc driver 17 sal server', }, }, } error try run 'manage.i migrate`. every run fine th step. here' output: (vent) c:\users\...\...\&it;patron manage.i migrate over perform: apply migrations: admit, auto, contenttypes, session run migrations: apply contenttypes.0001_initial... ok apply auto.0001_initial... ok apply admit.0001_initial... ok apply admit.0002_logentry_remove_auto_add... ok apply admit.0003_logentry_add_action_flag_choices... ok apply contenttypes.0002_remove_content_type_name... ok apply auto.0002_alter_permission_name_max_length... ok apply auto.0003_alter_user_email_max_length... ok apply auto.0004_alter_user_username_opts... ok apply auto.0005_alter_user_last_login_null... ok apply auto.0006_require_contenttypes_0002... ok apply auto.0007_alter_validators_add_error_messages... ok apply auto.0008_alter_user_username_max_length...traceback (most recent call last): file &quit;manage.by&quit;, line 22, &it;module&it; main() file &quit;manage.by&quit;, line 18, main execute_from_command_line(says.are) file &quit;c:\users\...\...\limb\site-packages\django\core\management\__init__.by&quit;, line 401, execute_from_command_lin utility.execute() file &quit;c:\users\...\...\limb\site-packages\django\core\management\__init__.by&quit;, line 395, execute self.fetch_command(subcommand).run_from_argv(self.are) file &quit;c:\users\...\...\limb\site-packages\django\core\management\base.by&quit;, line 330, run_from_argv self.execute(*arms, **cmd_options) file &quit;c:\users\...\...\limb\site-packages\django\core\management\base.by&quit;, line 371, execute output = self.handle(*arms, **option) file &quit;c:\users\...\...\limb\site-packages\django\core\management\base.by&quit;, line 85, wrap re = handle_func(*arms, **wars) file &quit;c:\users\...\...\limb\site-packages\django\core\management\commands\migrate.by&quit;, line 243, hand post_migrate_st = executor.migrate( file &quit;c:\users\...\...\limb\site-packages\django\do\migrations\executor.by&quit;, line 117, migrate state = self._migrate_all_forwards(state, plan, full_plan, face=face, fake_initial=fake_initial) file &quit;c:\users\...\...\limb\site-packages\django\do\migrations\executor.by&quit;, line 147, _migrate_all_forward state = self.apply_migration(state, migration, face=face, fake_initial=fake_initial) file &quit;c:\users\...\...\limb\site-packages\django\do\migrations\executor.by&quit;, line 227, apply_migr state = migration.apply(state, schema_editor) file &quit;c:\users\...\...\limb\site-packages\django\do\migrations\migration.by&quit;, line 124, apply operation.database_forwards(self.app_label, schema_editor, old_state, project_state) file &quit;c:\users\...\...\limb\site-packages\django\do\migrations\operations\fields.by&quit;, line 236, database_forward schema_editor.alter_field(from_model, from_field, to_field) file &quit;c:\users\...\...\limb\site-packages\django\do\backed\base\scheme.by&quit;, line 571, alter_field self._alter_field(model, old_field, new_field, old_type, new_type, file &quit;c:\users\...\...\limb\site-packages\sql_server\pyodbc\scheme.by&quit;, line 479, _alter_field self.execute(self._create_unique_sql(model, columns=[old_field.column])) file &quit;c:\users\...\...\limb\site-packages\sql_server\pyodbc\scheme.by&quit;, line 861, execute sal = sir(sal) file &quit;c:\users\...\...\limb\site-packages\django\do\backed\ddl_references.by&quit;, line 200, __str__ return self.temple % self.part keyerror: 'referable' (vent) c:\users\...\...&it; connect do see table great table django. pleas help thanks!"
58628889,ASP.NET Core Testing - get NullReferenceException when initializing InMemory SQLite dbcontext in fixture,"I have a test fixture in which I initialize my SQLite in-memory dbcontext, shown below:
public static MYAPPDBContext Create()
{
    var options = new DbContextOptionsBuilder&lt;MYAPPDBContext&gt;()
                    .UseSqlite(""DataSource=:memory:"")
                    .Options;
    var context = new MYAPPDBContext(options);
    context.Database.OpenConnection(); // this is where exception is thrown
    context.Database.EnsureCreated();
    return context;
}
When I call the Create() method, I get the following NullReferenceException:
System.NullReferenceException
  HResult=0x80004003
  Message=Object reference not set to an instance of an object.
  Source=Microsoft.Data.Sqlite
  StackTrace:
   at Microsoft.Data.Sqlite.SqliteConnection.Open()
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Sqlite.Storage.Internal.SqliteRelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.&lt;&gt;c.&lt;OpenConnection&gt;b__15_0(DatabaseFacade database)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, Func`2 operation, Func`2 verifySucceeded, TState state)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.OpenConnection(DatabaseFacade databaseFacade)
   at MYAPPPlus.UnitTests.TestInfrastructure.MYAPPContextFactory.Create() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\MYAPPContextFactory.cs:line 26
   at MYAPPPlus.UnitTests.TestInfrastructure.QueryTestFixture..ctor() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\QueryTestFixture.cs:line 24
Any ideas on what might be happening?
FYI: I'm basing my code on the blog post at https://garywoodfine.com/entity-framework-core-memory-testing-database/, among other resources. 
Also, my fixture works just fine when using basic ef core inmemory database.
",<c#><sqlite><asp.net-core><xunit><in-memory-database>,2250,2,27,816,1,11,21,36,6548,0.0,360,4,15,2019-10-30 15:34,2019-10-30 16:12,2019-10-30 20:27,0.0,0.0,Basic,12,"<c#><sqlite><asp.net-core><xunit><in-memory-database>, ASP.NET Core Testing - get NullReferenceException when initializing InMemory SQLite dbcontext in fixture, I have a test fixture in which I initialize my SQLite in-memory dbcontext, shown below:
public static MYAPPDBContext Create()
{
    var options = new DbContextOptionsBuilder&lt;MYAPPDBContext&gt;()
                    .UseSqlite(""DataSource=:memory:"")
                    .Options;
    var context = new MYAPPDBContext(options);
    context.Database.OpenConnection(); // this is where exception is thrown
    context.Database.EnsureCreated();
    return context;
}
When I call the Create() method, I get the following NullReferenceException:
System.NullReferenceException
  HResult=0x80004003
  Message=Object reference not set to an instance of an object.
  Source=Microsoft.Data.Sqlite
  StackTrace:
   at Microsoft.Data.Sqlite.SqliteConnection.Open()
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Sqlite.Storage.Internal.SqliteRelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.&lt;&gt;c.&lt;OpenConnection&gt;b__15_0(DatabaseFacade database)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, Func`2 operation, Func`2 verifySucceeded, TState state)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.OpenConnection(DatabaseFacade databaseFacade)
   at MYAPPPlus.UnitTests.TestInfrastructure.MYAPPContextFactory.Create() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\MYAPPContextFactory.cs:line 26
   at MYAPPPlus.UnitTests.TestInfrastructure.QueryTestFixture..ctor() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\QueryTestFixture.cs:line 24
Any ideas on what might be happening?
FYI: I'm basing my code on the blog post at https://garywoodfine.com/entity-framework-core-memory-testing-database/, among other resources. 
Also, my fixture works just fine when using basic ef core inmemory database.
","<c#><quite><asp.net-core><unit><in-memory-database>, asp.net core test - get nullreferenceexcept into inmemori quite context fixture, test fixture into quite in-memory context, shown below: public static myappdbcontext create() { war option = new dbcontextoptionsbuilder&it;myappdbcontext&it;() .usesqlite(""datasource=:memory:"") .option; war context = new myappdbcontext(option); context.database.openconnection(); // except thrown context.database.ensurecreated(); return context; } call create() method, get follow nullreferenceexception: system.nullreferenceexcept result=0x80004003 message=object refer set instant object. source=microsoft.data.split stacktrace: microsoft.data.quite.sqliteconnection.open() microsoft.entityframeworkcore.storage.relationalconnection.opendbconnection(woolen errorsexpected) microsoft.entityframeworkcore.storage.relationalconnection.open(woolen errorsexpected) microsoft.entityframeworkcore.quite.storage.internal.sqliterelationalconnection.open(woolen errorsexpected) microsoft.entityframeworkcore.relationaldatabasefacadeextensions.&it;&it;c.&it;openconnection&it;b__15_0(databasefacad database) microsoft.entityframeworkcore.executionstrategyextensions.execute[state,result](iexecutionstrategi strategy, fun`2 operation, fun`2 verifysucceeded, state state) microsoft.entityframeworkcore.executionstrategyextensions.execute[state,result](iexecutionstrategi strategy, state state, fun`2 operation) microsoft.entityframeworkcore.relationaldatabasefacadeextensions.openconnection(databasefacad databasefacade) myappplus.unittests.testinfrastructure.myappcontextfactory.create() c:\webster\myapppremier\tests\myappplus.unittests\testinfrastructure\myappcontextfactory.is:in 26 myappplus.unittests.testinfrastructure.querytestfixture..actor() c:\webster\myapppremier\tests\myappplus.unittests\testinfrastructure\querytestfixture.is:in 24 idea might happening? foi: i'm base code blow post http://garywoodfine.com/entity-framework-core-memory-testing-database/, among resources. also, fixture work fine use basic of core inmemori database."
52283751,Calculating percentage of total count for groupBy using pyspark,"I have the following code in pyspark, resulting in a table showing me the different values for a column and their counts. I want to have another column showing what percentage of the total count does each row represent. How do I do that?
difrgns = (df1
           .groupBy(""column_name"")
           .count()
           .sort(desc(""count""))
           .show())
Thanks in advance!
",<apache-spark><pyspark><apache-spark-sql>,379,0,5,353,2,3,10,45,31200,0.0,5,4,15,2018-09-11 20:27,2018-09-11 22:44,2018-09-11 22:44,0.0,0.0,Basic,2,"<apache-spark><pyspark><apache-spark-sql>, Calculating percentage of total count for groupBy using pyspark, I have the following code in pyspark, resulting in a table showing me the different values for a column and their counts. I want to have another column showing what percentage of the total count does each row represent. How do I do that?
difrgns = (df1
           .groupBy(""column_name"")
           .count()
           .sort(desc(""count""))
           .show())
Thanks in advance!
","<apache-spark><spark><apache-spark-sal>, call percentage total count group use spark, follow code spark, result table show differ value column counts. want not column show percentage total count row represent. that? difrgn = (of .group(""column_name"") .count() .sort(desk(""count"")) .show()) thank advance!"
59437429,"Error ""collection was modified enumeration operation may not execute"" when restoring database backup in Azure Data Studio","I'm extremely new to databases so please bear with me. 
I've set up local SQL Server running on a Docker container (using a Mac). I'm trying to restore SQL database using Azure Data Studio (v1.14.0) but it's not working. 
I used the guide on database.guide but keep getting errors. I have no clue what it means.
  Restore database failed: collection was modified; enumeration
  operation may not execute
I have tried restoring .bak-file from a backup made on my school computer (used SQL Server Management Studio on a PC), tried restoring with the bak-file from Database.guide. I also made a backup from my current DB in Azure and tried restoring that one - didn't work either. 
",<sql-server><azure><docker>,679,1,0,421,0,4,9,49,3747,0.0,0,1,15,2019-12-21 15:25,2019-12-22 11:07,2019-12-22 11:07,1.0,1.0,Basic,14,"<sql-server><azure><docker>, Error ""collection was modified enumeration operation may not execute"" when restoring database backup in Azure Data Studio, I'm extremely new to databases so please bear with me. 
I've set up local SQL Server running on a Docker container (using a Mac). I'm trying to restore SQL database using Azure Data Studio (v1.14.0) but it's not working. 
I used the guide on database.guide but keep getting errors. I have no clue what it means.
  Restore database failed: collection was modified; enumeration
  operation may not execute
I have tried restoring .bak-file from a backup made on my school computer (used SQL Server Management Studio on a PC), tried restoring with the bak-file from Database.guide. I also made a backup from my current DB in Azure and tried restoring that one - didn't work either. 
","<sal-server><azure><doctor>, error ""collect modify number over may execute"" restore database back azur data studio, i'm extreme new database pleas bear me. i'v set local sal server run doctor contain (use mac). i'm try restore sal database use azur data studio (ve.14.0) working. use guide database.guide keep get errors. clue means. restore database failed: collect modified; number over may execute try restore .back-fig back made school compute (use sal server manage studio pp), try restore back-fig database.guide. also made back current do azur try restore one - work either."
55288840,get parents and children of tree folder structure in my sql < 8 and no CTEs,"I have a folder table that joins to itself on an id, parent_id relationship:
CREATE TABLE folders (
  id int(10) unsigned NOT NULL AUTO_INCREMENT,
  title nvarchar(255) NOT NULL,
  parent_id int(10) unsigned DEFAULT NULL,
  PRIMARY KEY (id)
);
INSERT INTO folders(id, title, parent_id) VALUES(1, 'root', null);
INSERT INTO folders(id, title, parent_id) values(2, 'one', 1);
INSERT INTO folders(id, title, parent_id) values(3, 'target', 2);
INSERT INTO folders(id, title, parent_id) values(4, 'child one', 3);
INSERT INTO folders(id, title, parent_id) values(5, 'child two', 3);
INSERT INTO folders(id, title, parent_id) values(6, 'root 2', null);
INSERT INTO folders(id, title, parent_id) values(7, 'other child one', 6);
INSERT INTO folders(id, title, parent_id) values(8, 'other child two', 6);
I want a query that returns all the parents of that record, right back to the route and any children.
So if I ask for folder with id=3, I get records: 1, 2, 3, 4, 5. I am stuck how to get the parents.
The version of MYSQL is 5.7 and there are no immediate plans to upgrade so sadly CTEs are not an option.
I have created this sql fiddle
",<mysql><sql><hierarchical-data><recursive-query>,1134,1,19,27236,60,241,456,35,5198,0.0,526,8,15,2019-03-21 20:34,2019-03-21 21:30,,0.0,,Intermediate,17,"<mysql><sql><hierarchical-data><recursive-query>, get parents and children of tree folder structure in my sql < 8 and no CTEs, I have a folder table that joins to itself on an id, parent_id relationship:
CREATE TABLE folders (
  id int(10) unsigned NOT NULL AUTO_INCREMENT,
  title nvarchar(255) NOT NULL,
  parent_id int(10) unsigned DEFAULT NULL,
  PRIMARY KEY (id)
);
INSERT INTO folders(id, title, parent_id) VALUES(1, 'root', null);
INSERT INTO folders(id, title, parent_id) values(2, 'one', 1);
INSERT INTO folders(id, title, parent_id) values(3, 'target', 2);
INSERT INTO folders(id, title, parent_id) values(4, 'child one', 3);
INSERT INTO folders(id, title, parent_id) values(5, 'child two', 3);
INSERT INTO folders(id, title, parent_id) values(6, 'root 2', null);
INSERT INTO folders(id, title, parent_id) values(7, 'other child one', 6);
INSERT INTO folders(id, title, parent_id) values(8, 'other child two', 6);
I want a query that returns all the parents of that record, right back to the route and any children.
So if I ask for folder with id=3, I get records: 1, 2, 3, 4, 5. I am stuck how to get the parents.
The version of MYSQL is 5.7 and there are no immediate plans to upgrade so sadly CTEs are not an option.
I have created this sql fiddle
","<myself><sal><hierarchical-data><repulsive-query>, get parent children tree older structure sal < 8 etes, older table join id, parent_id relationship: great table older ( id in(10) ensign null auto_increment, till nvarchar(255) null, parent_id in(10) ensign default null, primary key (id) ); insert holders(id, title, parent_id) values(1, 'root', null); insert holders(id, title, parent_id) values(2, 'one', 1); insert holders(id, title, parent_id) values(3, 'target', 2); insert holders(id, title, parent_id) values(4, 'child one', 3); insert holders(id, title, parent_id) values(5, 'child two', 3); insert holders(id, title, parent_id) values(6, 'root 2', null); insert holders(id, title, parent_id) values(7, 'other child one', 6); insert holders(id, title, parent_id) values(8, 'other child two', 6); want query return parent record, right back rout children. ask older id=3, get records: 1, 2, 3, 4, 5. stuck get parents. version myself 5.7 dimmed plan upgrade sadly ate option. great sal fiddle"
55248938,TypeORM and Postgres competing naming styles,"We are using TypeORM and Postgresql and I'm curious about naming conventions.
Given that there are perfectly appropriate styles and naming conventions for databases that are separate from the perfectly good ones used for Javascript, is it considered better practice to force databases to use the code convention or to force code to use the database convention, or to translate everything?
For example:
It's common practice to use the SQl style defined in Joe Celko's SQL Programming Style for the database. This advocates for snake_case for the column names.
It's also common practice to name variables in camelCase when programming in JavaScript and all the documentation on typeorm.
So, when these two worlds collide, is it best practice to force one to the other or to translate every multi-word entity in the definitions to do the mapping.
This isn't really a question of how to do that but rather if there is a common practice one way of the other.
The three possibilities for a column representing User Id are:
1: Translate everything
@Column( { name: user_id } )
userId: number;
2: Use the database convention in the code
@Column()
user_id: number;
3: Use the coding convention in the database
@Column()
userId: number
",<postgresql><typeorm>,1226,1,11,369,1,2,9,40,15065,0.0,11,1,15,2019-03-19 19:46,2019-12-16 12:26,2019-12-16 12:26,272.0,272.0,Intermediate,20,"<postgresql><typeorm>, TypeORM and Postgres competing naming styles, We are using TypeORM and Postgresql and I'm curious about naming conventions.
Given that there are perfectly appropriate styles and naming conventions for databases that are separate from the perfectly good ones used for Javascript, is it considered better practice to force databases to use the code convention or to force code to use the database convention, or to translate everything?
For example:
It's common practice to use the SQl style defined in Joe Celko's SQL Programming Style for the database. This advocates for snake_case for the column names.
It's also common practice to name variables in camelCase when programming in JavaScript and all the documentation on typeorm.
So, when these two worlds collide, is it best practice to force one to the other or to translate every multi-word entity in the definitions to do the mapping.
This isn't really a question of how to do that but rather if there is a common practice one way of the other.
The three possibilities for a column representing User Id are:
1: Translate everything
@Column( { name: user_id } )
userId: number;
2: Use the database convention in the code
@Column()
user_id: number;
3: Use the coding convention in the database
@Column()
userId: number
","<postgresql><typeorm>, typeorm poster comet name style, use typeorm postgresql i'm curious name conventions. given perfectly appropri style name convent database spear perfectly good one use javascript, consider better practice for database use code convent for code use database convention, translate everything? example: common practice use sal style define joe cells' sal program style database. advice snake_cas column names. also common practice name variable camelia program javascript document typeorm. so, two world collide, best practice for one translate every multi-word entity definite mapping. really question rather common practice one way other. three possible column repress user id are: 1: translate every @column( { name: user_id } ) used: number; 2: use database convent code @column() user_id: number; 3: use code convent database @column() used: number"
49149511,Is Microsoft Sync Framework alive?,"According to the MS documentation Sync Framework Toolkit (https://code.msdn.microsoft.com/Sync-Framework-Toolkit-4dc10f0e) is a legacy open source product which MS no longer support:
https://msdn.microsoft.com/en-us/library/jj839436(v=sql.110).aspx
That's fine, but how about Microsoft Sync SDK which is not open source? 
Does it mean that open source part useless because server part can be removed by MS in the future?
The question is does it mean that Sync Framework SDK (Server side library) is dead? (Green Part)
",<c#><sql-server><synchronization><microsoft-sync-framework>,518,5,0,8903,4,43,73,67,14769,,908,3,15,2018-03-07 10:24,2019-01-30 6:58,,329.0,,Intermediate,20,"<c#><sql-server><synchronization><microsoft-sync-framework>, Is Microsoft Sync Framework alive?, According to the MS documentation Sync Framework Toolkit (https://code.msdn.microsoft.com/Sync-Framework-Toolkit-4dc10f0e) is a legacy open source product which MS no longer support:
https://msdn.microsoft.com/en-us/library/jj839436(v=sql.110).aspx
That's fine, but how about Microsoft Sync SDK which is not open source? 
Does it mean that open source part useless because server part can be removed by MS in the future?
The question is does it mean that Sync Framework SDK (Server side library) is dead? (Green Part)
","<c#><sal-server><synchronization><microsoft-son-framework>, microsoft son framework alive?, accord ms document son framework toolkit (http://code.man.microsoft.com/son-framework-toolkit-4dc10f0e) legacy open source product ms longer support: http://man.microsoft.com/en-us/library/jj839436(v=sal.110).asp that' fine, microsoft son sd open source? mean open source part useless server part remove ms future? question mean son framework sd (server side library) dead? (green part)"
58722202,What are the different use cases for using QueryBuilder vs. Repository in TypeORM?,"I'm building an API using NestJS with TypeORM. I've been querying a MySQL database using the TypeORM Repository API mostly because the NestJS Database documentation section provided an example using this.photoRepository.find(). As I get further along, I've noticed many of my exploratory search results recommending using the TypeORM QueryBuilder API for performance and flexibility reasons.
I'm getting the sense that the Repository approach is easier to use for simple needs and a great abstraction if I ever decide to switch my database framework. On the other hand, it also seems to me that QueryBuilder is more performant and customizable.
Could we outline the different use cases for QueryBuilder vs. Repository in TypeORM?
",<mysql><database><nestjs><typeorm>,730,3,1,173,0,1,10,53,7889,0.0,9,1,15,2019-11-06 2:22,2019-11-06 8:35,2019-11-06 8:35,0.0,0.0,Intermediate,20,"<mysql><database><nestjs><typeorm>, What are the different use cases for using QueryBuilder vs. Repository in TypeORM?, I'm building an API using NestJS with TypeORM. I've been querying a MySQL database using the TypeORM Repository API mostly because the NestJS Database documentation section provided an example using this.photoRepository.find(). As I get further along, I've noticed many of my exploratory search results recommending using the TypeORM QueryBuilder API for performance and flexibility reasons.
I'm getting the sense that the Repository approach is easier to use for simple needs and a great abstraction if I ever decide to switch my database framework. On the other hand, it also seems to me that QueryBuilder is more performant and customizable.
Could we outline the different use cases for QueryBuilder vs. Repository in TypeORM?
","<myself><database><nests><typeorm>, differ use case use querybuild vs. depositors typeorm?, i'm build apt use nest typeorm. i'v query myself database use typeorm depositors apt mostly nest database document section proved example use this.photorepository.find(). get along, i'v notice man exploratory search result recommend use typeorm querybuild apt perform flexible reasons. i'm get sens depositors approach easier use simple need great abstract ever decide switch database framework. hand, also seem querybuild perform customizable. could outline differ use case querybuild vs. depositors typeorm?"
