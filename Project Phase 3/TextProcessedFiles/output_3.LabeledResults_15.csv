QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
59624695,Entity Framework Core 3.1 Return value (int) from stored procedure,"this returns -1, how can i get the actual return value from stored procedure?
here is my stored procedure
ALTER PROCEDURE [Production].[Select_TicketQuantity]
    @Ticket NVARCHAR(25),
    @Reference NVARCHAR(20)
AS
BEGIN
    declare @SQL nvarchar (4000)
    SET @SQL = 'select QARTCOL as Quantidade from D805DATPOR.GCARCCR1 where NCOLGIA = ' + @Ticket + ' AND NARTCOM = ''' + @Reference + ''''
    SET @SQL = N'select CONVERT(int,Quantidade) as Quantidade from OpenQuery(MACPAC, ''' + REPLACE(@SQL, '''', '''''') + ''')'
    PRINT @SQL
    EXEC (@SQL)
END   
C# code
int? quantity= 0;
try
{
    quantity= await _context.Database.ExecuteSqlRawAsync(""EXEC Production.Select_TicketQuantity @p0, @p1"", parameters: new[] { ticket, reference});
}
catch (Exception ex)
{
    _logger.LogError($""{ex}"");
    return RedirectToPage(""Index"");
}
",<c#><sql-server><entity-framework-core-3.1>,834,0,23,3390,4,35,80,75,27905,0.0,366,7,15,2020-01-07 8:30,2020-01-07 9:57,,0.0,,Basic,10,"<c#><sql-server><entity-framework-core-3.1>, Entity Framework Core 3.1 Return value (int) from stored procedure, this returns -1, how can i get the actual return value from stored procedure?
here is my stored procedure
ALTER PROCEDURE [Production].[Select_TicketQuantity]
    @Ticket NVARCHAR(25),
    @Reference NVARCHAR(20)
AS
BEGIN
    declare @SQL nvarchar (4000)
    SET @SQL = 'select QARTCOL as Quantidade from D805DATPOR.GCARCCR1 where NCOLGIA = ' + @Ticket + ' AND NARTCOM = ''' + @Reference + ''''
    SET @SQL = N'select CONVERT(int,Quantidade) as Quantidade from OpenQuery(MACPAC, ''' + REPLACE(@SQL, '''', '''''') + ''')'
    PRINT @SQL
    EXEC (@SQL)
END   
C# code
int? quantity= 0;
try
{
    quantity= await _context.Database.ExecuteSqlRawAsync(""EXEC Production.Select_TicketQuantity @p0, @p1"", parameters: new[] { ticket, reference});
}
catch (Exception ex)
{
    _logger.LogError($""{ex}"");
    return RedirectToPage(""Index"");
}
","<c#><sal-server><entity-framework-core-3.1>, entity framework core 3.1 return value (in) store procedure, return -1, get actual return value store procedure? store procedure alter procedure [production].[select_ticketquantity] @ticket nvarchar(25), @refer nvarchar(20) begin declare @sal nvarchar (4000) set @sal = 'select qartcol quantidad d805datpor.gcarccr1 ncolgia = ' + @ticket + ' nartcom = ''' + @refer + '''' set @sal = n'select convert(in,quantidade) quantidad openquery(maniac, ''' + replace(@sal, '''', '''''') + ''')' print @sal even (@sal) end c# code in? quantity= 0; try { quantity= await context.database.executesqlrawasync(""even production.select_ticketquant @pp, @pp"", parameter: new[] { ticket, reference}); } catch (except ex) { longer.logerror($""{ex}""); return redirecttopage(""index""); }"
62828259,Laravel Slow queries,"public function delete( ReportDetailRequest $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = CustomerInfo::find($id);
        $customerRecord-&gt;delete();
    }
I currently have the above in a laravel application where a DELETE request is sent to this controller. At the moment, as you can see, its very simple, but the query seems super slow. It comes back in postman in 2.23 seconds. What should I try to speed this up? The database layer (mysql) does have an index on ID from what I can tell and the application isn't running in debug. Is this typical?
edit:
Good thinking that the request validation may be doing something (it is validating that this user has auth to delete).
class ReportDetailRequest extends FormRequest
{
    /**
     * Determine if the user is authorized to make this request.
     *
     * @return bool
     */
    public function authorize()
    {
        $id = (int)$this-&gt;route('id');
        $customerInfo = CustomerInfo::find($id)-&gt;first();
        $company = $customerInfo-&gt;company_id;
        return (auth()-&gt;user()-&gt;company-&gt;id  == $company );
    }
    /**
     * Get the validation rules that apply to the request.
     *
     * @return array
     */
    public function rules()
    {
        return [
            //
        ];
    }
}
Show create table:
CREATE TABLE &quot;customer_info&quot; (
  &quot;id&quot; int(11) NOT NULL AUTO_INCREMENT,
  &quot;user_id&quot; int(11) DEFAULT NULL,
  &quot;report_guid&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  &quot;customer_email&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;created_at&quot; timestamp NULL DEFAULT NULL,
  &quot;updated_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_read&quot; tinyint(1) NOT NULL,
  &quot;customer_name&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;customer_support_issue&quot; longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  &quot;company_id&quot; int(11) NOT NULL,
  &quot;archived&quot; tinyint(1) NOT NULL,
  &quot;archived_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_active&quot; tinyint(4) DEFAULT NULL,
  &quot;customer_screenshot&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;video_url&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (&quot;id&quot;),
  KEY &quot;indexReportLookup&quot; (&quot;report_guid&quot;),
  KEY &quot;guid&quot; (&quot;report_guid&quot;),
  KEY &quot;customer_info_id_index&quot; (&quot;id&quot;)
)
Baseline:
 public function delete( Request $request )
    {
        // $id = (int)$request-&gt;id;
        // $customerRecord = CustomerInfo::find($id);
        // $foo_sql = $customerRecord-&gt;delete()-&gt;toSql();
        // echo($foo_sql);
        return 'test';
        //$customerRecord-&gt;delete();
    }
Ok so a brand new table, with a brand new Request. with a single ID in it, looks like this:
The controller looks like:
public function deleteTest( Request $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = NewTable::where('id', '=', $id)-&gt;first();
        $customerRecord-&gt;delete();
        return response(null, 200);
    }
Postman version is :Version 7.27.1 (7.27.1)
1630 ms. WTF. 1.6 seconds for a simple request on a new table.
EXPLAIN DELETE:
1   DELETE  new_tables      range   PRIMARY PRIMARY 8   const   1   100 Using where
EXPLAIN SELECT
1   SIMPLE  new_tables      const   PRIMARY PRIMARY 8   const   1   100 Using index
MYSQL version   8.0.18
innodb_version  8.0.18
So now to add to the fun.
A framework free PHP file. Simple GET request. 100ms.
&lt;?php
echo('tester');
?&gt;
Edit. Just to reiterate.
A Laravel GET method (with authentication) returning test, returns 1.6s.
A no framework &quot;sample.php&quot; file returns in 100ms.
A Laravel GET method (without authentication) returning test, returns in 430ms.
A Laravel GET method (without authentication but with DB access), returns in 1483ms.
It look like there is indeed something holding up requests once the application starts using the database.
Route::middleware('auth:api')-&gt;get('/test1','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest1.api');
Route::middleware('auth:api')-&gt;get('/test2','Api\NewTableController@index')-&gt;name('report.deleteTest2.api');
Route::get('/test3','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest3.api');
Route::get('/test4','Api\NewTableController@index')-&gt;name('report.deleteTest4.api');
 Route::get('/test5','Api\NewTableController@dbTest')-&gt;name('report.deleteTest5.api');
NewTableController:
&lt;?php
namespace App\Http\Controllers\Api;
class NewTableController extends Controller
{
    public function index()
    {
        return &quot;test2&quot;;
    }
}
CustomerInfoController ( with some stuff removed, but method is pretty similar conceptually to NewTableController albeit with some dependency injection going on ).
&lt;?php
namespace App\Http\Controllers\Api;
use App\Http\Controllers\Controller;
use Illuminate\Http\Request;
use App\Http\Requests\ReportDetailRequest;
use App\Services\CustomerInfoService;
use Auth;
use App\LookupParent;
use App\LookupChild;
use App\CustomerInfo;
use App\Http\Resources\CustomerInfoResourceCollection;
use App\Http\Resources\CustomerInfoResource;
use App\Http\Resources\CustomerInfoResourceDetail;
use Carbon\Carbon;
use App\NewTable;
class CustomerInfoController extends Controller
{
    protected $customerInfoService;
    public function __construct(
        CustomerInfoService $customerInfoService
        )
    {
        $this-&gt;customerInfoService = $customerInfoService;
    }
    public function deleteTest()
    {
        return 'deleteTest';
    }
   public function dbTest()
   {   
     tap(NewTable::find(1))-&gt;delete();
   }
}
Results:
/test1 (with authentication 1380ms)
/test2 (with authentication 1320ms)
/test3 (without authentication 112ms)
/test4 (without authentication 124ms)
/test5 (db without authentication 1483ms)
In other words, authentication talks to the database as does a simple delete query without authentication. These take at least a second to complete each. This leads to the roughly two second request mentioned above which has both elements (authentication and database access).
Edit. For those of you reading from Google. Problem was to do with the managed database provided by Digital Ocean. Setup a localised database on MySQL on the same box, and problem resolved itself. think it was either latency from datacenters between web server and database across the world somewhere or a misconfiguration on the part of the db admins at DigitalOcean. Resolved myself, problem wasn't Laravel.
",<mysql><laravel><performance>,6782,4,144,4349,6,51,89,76,9288,0.0,182,5,15,2020-07-10 6:09,2020-07-21 12:53,,11.0,,Intermediate,23,"<mysql><laravel><performance>, Laravel Slow queries, public function delete( ReportDetailRequest $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = CustomerInfo::find($id);
        $customerRecord-&gt;delete();
    }
I currently have the above in a laravel application where a DELETE request is sent to this controller. At the moment, as you can see, its very simple, but the query seems super slow. It comes back in postman in 2.23 seconds. What should I try to speed this up? The database layer (mysql) does have an index on ID from what I can tell and the application isn't running in debug. Is this typical?
edit:
Good thinking that the request validation may be doing something (it is validating that this user has auth to delete).
class ReportDetailRequest extends FormRequest
{
    /**
     * Determine if the user is authorized to make this request.
     *
     * @return bool
     */
    public function authorize()
    {
        $id = (int)$this-&gt;route('id');
        $customerInfo = CustomerInfo::find($id)-&gt;first();
        $company = $customerInfo-&gt;company_id;
        return (auth()-&gt;user()-&gt;company-&gt;id  == $company );
    }
    /**
     * Get the validation rules that apply to the request.
     *
     * @return array
     */
    public function rules()
    {
        return [
            //
        ];
    }
}
Show create table:
CREATE TABLE &quot;customer_info&quot; (
  &quot;id&quot; int(11) NOT NULL AUTO_INCREMENT,
  &quot;user_id&quot; int(11) DEFAULT NULL,
  &quot;report_guid&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  &quot;customer_email&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;created_at&quot; timestamp NULL DEFAULT NULL,
  &quot;updated_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_read&quot; tinyint(1) NOT NULL,
  &quot;customer_name&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;customer_support_issue&quot; longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  &quot;company_id&quot; int(11) NOT NULL,
  &quot;archived&quot; tinyint(1) NOT NULL,
  &quot;archived_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_active&quot; tinyint(4) DEFAULT NULL,
  &quot;customer_screenshot&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;video_url&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (&quot;id&quot;),
  KEY &quot;indexReportLookup&quot; (&quot;report_guid&quot;),
  KEY &quot;guid&quot; (&quot;report_guid&quot;),
  KEY &quot;customer_info_id_index&quot; (&quot;id&quot;)
)
Baseline:
 public function delete( Request $request )
    {
        // $id = (int)$request-&gt;id;
        // $customerRecord = CustomerInfo::find($id);
        // $foo_sql = $customerRecord-&gt;delete()-&gt;toSql();
        // echo($foo_sql);
        return 'test';
        //$customerRecord-&gt;delete();
    }
Ok so a brand new table, with a brand new Request. with a single ID in it, looks like this:
The controller looks like:
public function deleteTest( Request $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = NewTable::where('id', '=', $id)-&gt;first();
        $customerRecord-&gt;delete();
        return response(null, 200);
    }
Postman version is :Version 7.27.1 (7.27.1)
1630 ms. WTF. 1.6 seconds for a simple request on a new table.
EXPLAIN DELETE:
1   DELETE  new_tables      range   PRIMARY PRIMARY 8   const   1   100 Using where
EXPLAIN SELECT
1   SIMPLE  new_tables      const   PRIMARY PRIMARY 8   const   1   100 Using index
MYSQL version   8.0.18
innodb_version  8.0.18
So now to add to the fun.
A framework free PHP file. Simple GET request. 100ms.
&lt;?php
echo('tester');
?&gt;
Edit. Just to reiterate.
A Laravel GET method (with authentication) returning test, returns 1.6s.
A no framework &quot;sample.php&quot; file returns in 100ms.
A Laravel GET method (without authentication) returning test, returns in 430ms.
A Laravel GET method (without authentication but with DB access), returns in 1483ms.
It look like there is indeed something holding up requests once the application starts using the database.
Route::middleware('auth:api')-&gt;get('/test1','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest1.api');
Route::middleware('auth:api')-&gt;get('/test2','Api\NewTableController@index')-&gt;name('report.deleteTest2.api');
Route::get('/test3','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest3.api');
Route::get('/test4','Api\NewTableController@index')-&gt;name('report.deleteTest4.api');
 Route::get('/test5','Api\NewTableController@dbTest')-&gt;name('report.deleteTest5.api');
NewTableController:
&lt;?php
namespace App\Http\Controllers\Api;
class NewTableController extends Controller
{
    public function index()
    {
        return &quot;test2&quot;;
    }
}
CustomerInfoController ( with some stuff removed, but method is pretty similar conceptually to NewTableController albeit with some dependency injection going on ).
&lt;?php
namespace App\Http\Controllers\Api;
use App\Http\Controllers\Controller;
use Illuminate\Http\Request;
use App\Http\Requests\ReportDetailRequest;
use App\Services\CustomerInfoService;
use Auth;
use App\LookupParent;
use App\LookupChild;
use App\CustomerInfo;
use App\Http\Resources\CustomerInfoResourceCollection;
use App\Http\Resources\CustomerInfoResource;
use App\Http\Resources\CustomerInfoResourceDetail;
use Carbon\Carbon;
use App\NewTable;
class CustomerInfoController extends Controller
{
    protected $customerInfoService;
    public function __construct(
        CustomerInfoService $customerInfoService
        )
    {
        $this-&gt;customerInfoService = $customerInfoService;
    }
    public function deleteTest()
    {
        return 'deleteTest';
    }
   public function dbTest()
   {   
     tap(NewTable::find(1))-&gt;delete();
   }
}
Results:
/test1 (with authentication 1380ms)
/test2 (with authentication 1320ms)
/test3 (without authentication 112ms)
/test4 (without authentication 124ms)
/test5 (db without authentication 1483ms)
In other words, authentication talks to the database as does a simple delete query without authentication. These take at least a second to complete each. This leads to the roughly two second request mentioned above which has both elements (authentication and database access).
Edit. For those of you reading from Google. Problem was to do with the managed database provided by Digital Ocean. Setup a localised database on MySQL on the same box, and problem resolved itself. think it was either latency from datacenters between web server and database across the world somewhere or a misconfiguration on the part of the db admins at DigitalOcean. Resolved myself, problem wasn't Laravel.
","<myself><travel><performance>, travel slow queried, public function delete( reportdetailrequest $request ) { $id = (in)$request-&it;id; $customerrecord = customerinfo::find($id); $customerrecord-&it;delete(); } current travel applied delete request sent controller. moment, see, simple, query seem super slow. come back potman 2.23 seconds. try speed up? database layer (myself) index id tell applied run debut. typical? edit: good think request valid may cometh (it valid user auto delete). class reportdetailrequest extend formrequest { /** * determine user author make request. * * @return book */ public function authorize() { $id = (in)$this-&it;route('id'); $customerinfo = customerinfo::find($id)-&it;first(); $company = $customerinfo-&it;company_id; return (auto()-&it;user()-&it;company-&it;id == $company ); } /** * get valid rule apply request. * * @return array */ public function rules() { return [ // ]; } } show great table: great table &quit;customer_info&quit; ( &quit;id&quit; in(11) null auto_increment, &quit;user_id&quit; in(11) default null, &quit;report_guid&quit; varchar(255) character set utf8mb4 collar utf8mb4_unicode_ci null, &quit;customer_email&quit; varchar(255) character set utf8mb4 collar utf8mb4_unicode_ci default null, &quit;created_at&quit; timestamp null default null, &quit;updated_at&quit; timestamp null default null, &quit;report_read&quit; tinyint(1) null, &quit;customer_name&quit; varchar(255) character set utf8mb4 collar utf8mb4_unicode_ci default null, &quit;customer_support_issue&quit; context character set utf8mb4 collar utf8mb4_unicode_ci, &quit;company_id&quit; in(11) null, &quit;archive&quit; tinyint(1) null, &quit;archived_at&quit; timestamp null default null, &quit;report_active&quit; tinyint(4) default null, &quit;customer_screenshot&quit; varchar(255) collar utf8mb4_unicode_ci default null, &quit;video_url&quit; varchar(255) collar utf8mb4_unicode_ci default null, primary key (&quit;id&quit;), key &quit;indexreportlookup&quit; (&quit;report_guid&quit;), key &quit;guide&quit; (&quit;report_guid&quit;), key &quit;customer_info_id_index&quit; (&quit;id&quit;) ) vaseline: public function delete( request $request ) { // $id = (in)$request-&it;id; // $customerrecord = customerinfo::find($id); // $foo_sql = $customerrecord-&it;delete()-&it;total(); // echo($foo_sql); return 'test'; //$customerrecord-&it;delete(); } ok brand new table, brand new request. single id it, look like this: control look like: public function deletetest( request $request ) { $id = (in)$request-&it;id; $customerrecord = notable::where('id', '=', $id)-&it;first(); $customerrecord-&it;delete(); return response(null, 200); } potman version :version 7.27.1 (7.27.1) 1630 ms. utf. 1.6 second simple request new table. explain delete: 1 delete new_tabl rang primary primary 8 cost 1 100 use explain select 1 simple new_tabl cost primary primary 8 cost 1 100 use index myself version 8.0.18 innodb_vers 8.0.18 add fun. framework free pp file. simple get request. 100ms. &it;?pp echo('tested'); ?&it; edit. reiterated. travel get method (with authentication) return test, return 1.is. framework &quit;sample.pp&quit; file return 100ms. travel get method (without authentication) return test, return 430ms. travel get method (without authentic do access), return 1483ms. look like index cometh hold request applied start use database. route::middleware('auto:apt')-&it;get('/test','apt\customerinfocontroller@deletetest')-&it;name('report.deletetest1.apt'); route::middleware('auto:apt')-&it;get('/test','apt\newtablecontroller@index')-&it;name('report.deletetest2.apt'); route::get('/test','apt\customerinfocontroller@deletetest')-&it;name('report.deletetest3.apt'); route::get('/test','apt\newtablecontroller@index')-&it;name('report.deletetest4.apt'); route::get('/test','apt\newtablecontroller@detest')-&it;name('report.deletetest5.apt'); newtablecontroller: &it;?pp namespac pp\http\controller\apt; class newtablecontrol extend control { public function index() { return &quit;test&quit;; } } customerinfocontrol ( stuff removed, method pretty similar concept newtablecontrol albeit depend inject go ). &it;?pp namespac pp\http\controller\apt; use pp\http\controller\controller; use illuminate\http\request; use pp\http\requests\reportdetailrequest; use pp\services\customerinfoservice; use auto; use pp\lookupparent; use pp\lookupchild; use pp\customerinfo; use pp\http\resources\customerinforesourcecollection; use pp\http\resources\customerinforesource; use pp\http\resources\customerinforesourcedetail; use carbon\carbon; use pp\notable; class customerinfocontrol extend control { protect $customerinfoservice; public function construct( customerinfoservic $customerinfoservic ) { $this-&it;customerinfoservic = $customerinfoservice; } public function deletetest() { return 'deletetest'; } public function detest() { tap(notable::find(1))-&it;delete(); } } results: /test (with authentic 1380ms) /test (with authentic 1320ms) /test (without authentic 112ms) /test (without authentic 124ms) /test (do without authentic 1483ms) words, authentic talk database simple delete query without authentication. take least second complete each. lead roughly two second request mention element (authentic database access). edit. read goose. problem manage database proved digit ocean. set localise database myself box, problem resolve itself. think either latent adjacent web server database across world somewhere misconfigur part do admit digitalocean. resolve myself, problem travel."
59654712,Invalid value for key 'authentication',"I have a .NET Core 3.0 app where am trying to connect to a Azure SQL database using EF Core and Active directory integrated authentication.
I have verified that I have access to this database from my machine as I can connect to it just fine using SQL server management studio and 'Azure Active Directory-Integrated' authentication.
However, when I try to read data in my app (using EF Core), I always get a System.Argument exception with the following statement:
Invalid value for key 'authentication'
Exception details point to the Db connection string.
So, here is my connection string from my dev appsettings.json file:
&quot;ConnectionStrings&quot;: {
&quot;MCDB&quot;: &quot;Server=tcp:dev-media-center-sql.database.windows.net,1433;Initial
Catalog=MediaCenter;Persist Security Info=False;User
ID={my User ID};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Authentication=Active Directory Integrated;&quot;   },
I have tried hard coding the connection string directly in my code, thinking that there might be a problem with my JSON but I still get the same exception.
Is &quot;Active Directory Integrated&quot; not a valid value for the 'Authentication' keyword? If not, what is it then? I have already tried &quot;ActiveDirectoryIntegrated&quot; (with no spaces) and /&quot;Active Directory Integrated&quot;/ (escaping double quotes) but to no avail.
",<json><azure><.net-core><azure-active-directory><azure-sql-database>,1383,0,0,735,2,7,16,52,14531,0.0,43,6,15,2020-01-08 22:30,2020-09-25 23:43,,261.0,,Basic,2,"<json><azure><.net-core><azure-active-directory><azure-sql-database>, Invalid value for key 'authentication', I have a .NET Core 3.0 app where am trying to connect to a Azure SQL database using EF Core and Active directory integrated authentication.
I have verified that I have access to this database from my machine as I can connect to it just fine using SQL server management studio and 'Azure Active Directory-Integrated' authentication.
However, when I try to read data in my app (using EF Core), I always get a System.Argument exception with the following statement:
Invalid value for key 'authentication'
Exception details point to the Db connection string.
So, here is my connection string from my dev appsettings.json file:
&quot;ConnectionStrings&quot;: {
&quot;MCDB&quot;: &quot;Server=tcp:dev-media-center-sql.database.windows.net,1433;Initial
Catalog=MediaCenter;Persist Security Info=False;User
ID={my User ID};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Authentication=Active Directory Integrated;&quot;   },
I have tried hard coding the connection string directly in my code, thinking that there might be a problem with my JSON but I still get the same exception.
Is &quot;Active Directory Integrated&quot; not a valid value for the 'Authentication' keyword? If not, what is it then? I have already tried &quot;ActiveDirectoryIntegrated&quot; (with no spaces) and /&quot;Active Directory Integrated&quot;/ (escaping double quotes) but to no avail.
","<son><azure><.net-core><azure-active-directory><azure-sal-database>, invalid value key 'authentication', .net core 3.0 pp try connect azur sal database use of core active director inter authentication. verify access database machine connect fine use sal server manage studio 'azur active directory-integrated' authentication. however, try read data pp (use of core), away get system.argue except follow statement: invalid value key 'authentication' except detail point do connect string. so, connect string de appsettings.son file: &quit;connectionstrings&quit;: { &quit;made&quit;: &quit;server=top:de-media-center-sal.database.windows.net,1433;into catalogue=mediacenter;persist secure into=false;us id={mi user id};multipleactiveresultsets=false;encrypt=true;trustservercertificate=false;authentication=act director integrated;&quit; }, try hard code connect string directly code, think might problem son still get exception. &quit;act director integrated&quit; valid value 'authentication' eyford? not, then? already try &quit;activedirectoryintegrated&quit; (with spaces) /&quit;act director integrated&quit;/ (escape doubt quotes) avail."
59849262,Postgresql full text search with TypeOrm,"There is some way to handle full text search with Postgres and TypeOrm. I've seen some examples but they only work with Mysql. How can I get the equivalent of this but with Postgresql?
@Entity()
export class User {
    @PrimaryGeneratedColumn()
    id: string;
    @Index({ fulltext: true })
    @Column(""varchar"")
    name: string;
}
And use query builder:
const searchTerm = ""John"";
const result = await connection.manager.getRepository(User)
            .createQueryBuilder()
            .select()
            .where(`MATCH(name) AGAINST ('${searchTerm}' IN BOOLEAN MODE)`)
            .getMany();
",<postgresql><typeorm>,601,0,17,795,2,10,30,78,18538,0.0,256,3,15,2020-01-21 21:09,2020-01-23 5:14,2020-01-23 5:14,2.0,2.0,Basic,3,"<postgresql><typeorm>, Postgresql full text search with TypeOrm, There is some way to handle full text search with Postgres and TypeOrm. I've seen some examples but they only work with Mysql. How can I get the equivalent of this but with Postgresql?
@Entity()
export class User {
    @PrimaryGeneratedColumn()
    id: string;
    @Index({ fulltext: true })
    @Column(""varchar"")
    name: string;
}
And use query builder:
const searchTerm = ""John"";
const result = await connection.manager.getRepository(User)
            .createQueryBuilder()
            .select()
            .where(`MATCH(name) AGAINST ('${searchTerm}' IN BOOLEAN MODE)`)
            .getMany();
","<postgresql><typeorm>, postgresql full text search typeorm, way hand full text search poster typeorm. i'v seen example work myself. get equal postgresql? @entity() export class user { @primarygeneratedcolumn() id: string; @index({ fullest: true }) @column(""varchar"") name: string; } use query builder: cost searchterm = ""john""; cost result = await connection.manager.getrepository(user) .createquerybuilder() .select() .where(`match(name) ('${searchterm}' woolen mode)`) .germany();"
48305081,Spark' Dataset unpersist behaviour,"Recently I saw some strange behaviour of Spark.
I have a pipeline in my application in which I'm manipulating one big Dataset - pseudocode:
val data = spark.read (...)
data.join(df1, ""key"") //etc, more transformations
data.cache(); // used to not recalculate data after save
data.write.parquet() // some save
val extension = data.join (..) // more transformations - joins, selects, etc.
extension.cache(); // again, cache to not double calculations
extension.count();
// (1)
extension.write.csv() // some other save
extension.groupBy(""key"").agg(some aggregations) //
extension.write.parquet() // other save, without cache it will trigger recomputation of whole dataset
However when I call data.unpersist() i.e. in place (1), Spark deletes from Storage all datasets, also the extension Dataset which is not the dataset I tried to unpersist.
Is that an expected behaviour? How can I free some memory by unpersist on old Dataset without unpersisting all Dataset that was ""next in chain""?
My setup:
Spark version: current master, RC for 2.3
Scala: 2.11
Java: OpenJDK 1.8
Question looks similar to Understanding Spark&#39;s caching, but here I'm doing some actions before unpersist. At first I'm counting everything and then save into storage - I don't know if caching works the same in RDD like in Datasets
",<apache-spark><apache-spark-sql>,1303,1,17,15806,4,47,62,67,6579,0.0,2301,2,15,2018-01-17 15:54,2018-01-17 17:41,2018-01-17 17:41,0.0,0.0,Intermediate,23,"<apache-spark><apache-spark-sql>, Spark' Dataset unpersist behaviour, Recently I saw some strange behaviour of Spark.
I have a pipeline in my application in which I'm manipulating one big Dataset - pseudocode:
val data = spark.read (...)
data.join(df1, ""key"") //etc, more transformations
data.cache(); // used to not recalculate data after save
data.write.parquet() // some save
val extension = data.join (..) // more transformations - joins, selects, etc.
extension.cache(); // again, cache to not double calculations
extension.count();
// (1)
extension.write.csv() // some other save
extension.groupBy(""key"").agg(some aggregations) //
extension.write.parquet() // other save, without cache it will trigger recomputation of whole dataset
However when I call data.unpersist() i.e. in place (1), Spark deletes from Storage all datasets, also the extension Dataset which is not the dataset I tried to unpersist.
Is that an expected behaviour? How can I free some memory by unpersist on old Dataset without unpersisting all Dataset that was ""next in chain""?
My setup:
Spark version: current master, RC for 2.3
Scala: 2.11
Java: OpenJDK 1.8
Question looks similar to Understanding Spark&#39;s caching, but here I'm doing some actions before unpersist. At first I'm counting everything and then save into storage - I don't know if caching works the same in RDD like in Datasets
","<apache-spark><apache-spark-sal>, spark' dataset persist behaviour, recent saw strange behaviour spark. pipelin applied i'm manipul one big dataset - pseudocode: val data = spark.read (...) data.join(of, ""key"") //etc, transform data.ache(); // use recall data save data.write.parquet() // save val extent = data.join (..) // transform - joins, select, etc. extension.ache(); // again, each doubt call extension.count(); // (1) extension.write.is() // save extension.group(""key"").age(so aggregations) // extension.write.parquet() // save, without each trigger recomput whole dataset howe call data.persist() i.e. place (1), spark delete storage datasets, also extent dataset dataset try persist. expect behaviour? free memory persist old dataset without persist dataset ""next chain""? set: spark version: current master, re 2.3 scala: 2.11 cava: opened 1.8 question look similar understand spark&#39; catching, i'm action persist. first i'm count every save storage - know each work red like dataset"
51089008,"An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue","Any reason why this error popped up when I tried bundling an application:
I have tried installing gem install mysql2 -v '0.3.21' as they recommend but it cant install properly. Also I am running this on macOS High Sierra. Sorry for my bad wording for this question because its my first time working with ruby.
 To see why this extension failed to compile, please check the mkmf.log which can be found here:
  /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/mkmf.log
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:762:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:793:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:794:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:797:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:798:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:799:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:830:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1185:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21 for inspection.
Results logged to /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/gem_make.out
An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue.
Make sure that `gem install mysql2 -v '0.3.21' --source 'http://rubygems.org/'` succeeds before bundling.
",<mysql><ruby-on-rails><bundle-install>,3020,1,59,250,1,2,11,73,14099,0.0,4,5,15,2018-06-28 18:01,2018-06-29 20:41,,1.0,,Intermediate,19,"<mysql><ruby-on-rails><bundle-install>, An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue, Any reason why this error popped up when I tried bundling an application:
I have tried installing gem install mysql2 -v '0.3.21' as they recommend but it cant install properly. Also I am running this on macOS High Sierra. Sorry for my bad wording for this question because its my first time working with ruby.
 To see why this extension failed to compile, please check the mkmf.log which can be found here:
  /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/mkmf.log
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:762:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:793:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:794:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:797:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:798:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:799:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:830:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1185:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21 for inspection.
Results logged to /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/gem_make.out
An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue.
Make sure that `gem install mysql2 -v '0.3.21' --source 'http://rubygems.org/'` succeeds before bundling.
","<myself><ruby-on-rails><bundle-install>, error occur instal myself (0.3.21), bundles cannot continue, reason error pop try bundle application: try instal gem instal myself -v '0.3.21' recommend can instal properly. also run mack high pierre. sorry bad word question first time work ruby. see extent fail compile, pleas check mimi.log found here: /users/yamanshrestha/desktop/dorsal/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/myself-0.3.21/mimi.log current directory: /users/yamanshrestha/desktop/dorsal/vendor/bundle/ruby/2.3.0/gems/myself-0.3.21/ext/myself make ""despair="" clean current directory: /users/yamanshrestha/desktop/dorsal/vendor/bundle/ruby/2.3.0/gems/myself-0.3.21/ext/myself make ""despair="" compel inside.c compel client.c client.c:439:3: error: use undeclar identify 'my_bool' my_bool re = mysql_read_query_result(client); ^ client.c:441:19: error: use undeclar identify 'yes' return (void *)(re == 0 ? true : false); ^ client.c:762:3: error: use undeclar identify 'my_bool' my_bool boolval; ^ client.c:793:7: error: use undeclar identify 'boolval' boolval = (value == fall ? 0 : 1); ^ client.c:794:17: error: use undeclar identify 'boolval' removal = &amp;boolval; ^ client.c:797:10: error: use undeclar identify 'mysql_secure_auth'; mean 'mysql_default_auth'? case mysql_secure_auth: ^~~~~~~~~~~~~~~~~ mysql_default_auth /us/local/cellar/myself/8.0.11/include/myself/myself.h:188:3: note: 'mysql_default_auth' declare mysql_default_auth, ^ client.c:798:7: error: use undeclar identify 'boolval' boolval = (value == fall ? 0 : 1); ^ client.c:799:17: error: use undeclar identify 'boolval' removal = &amp;boolval; ^ client.c:830:38: error: use undeclar identify 'boolval' wrapped-&it;reconnect_en = boolval; ^ client.c:1185:38: error: use undeclar identify 'mysql_secure_auth'; mean 'mysql_default_auth'? return _mysql_client_options(self, mysql_secure_auth, value); ^~~~~~~~~~~~~~~~~ mysql_default_auth /us/local/cellar/myself/8.0.11/include/myself/myself.h:188:3: note: 'mysql_default_auth' declare mysql_default_auth, ^ 10 error generate. make: *** [client.o] error 1 make failed, exit code 2 gem file remain instal /users/yamanshrestha/desktop/dorsal/vendor/bundle/ruby/2.3.0/gems/myself-0.3.21 inspection. result log /users/yamanshrestha/desktop/dorsal/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/myself-0.3.21/gem_make.out error occur instal myself (0.3.21), bundles cannot continue. make sure `gem instal myself -v '0.3.21' --source 'http://rubygems.org/'` such building."
59264410,How to connect to Traefik TCP Services with TLS configuration enabled?,"I am trying to configure Traefik so that I would have access to services via domain names, and that I would not have to set different ports. For example, two MongoDB services, both on the default port, but in different domains, example.localhost and example2.localhost. Only this example works. I mean, other cases probably work, but I can't connect to them, and I don't understand what the problem is. This is probably not even a problem with Traefik.
I have prepared a repository with an example that works. You just need to generate your own certificate with mkcert. The page at example.localhost returns the 403 Forbidden error but you should not worry about it, because the purpose of this configuration is to show that SSL is working (padlock, green status). So don't focus on 403.
Only the SSL connection to the mongo service works. I tested it with the Robo 3T program. After selecting the SSL connection, providing the host on example.localhost and selecting the certificate for a self-signed (or own) connection works. And that's the only thing that works that way. Connections to redis (Redis Desktop Manager) and to pgsql (PhpStorm, DBeaver, DbVisualizer) do not work, regardless of whether I provide certificates or not. I do not forward SSL to services, I only connect to Traefik. I spent long hours on it. I searched the internet. I haven't found the answer yet. Has anyone solved this?
PS. I work on Linux Mint, so my configuration should work in this environment without any problem. I would ask for solutions for Linux.
If you do not want to browse the repository, I attach the most important files:
docker-compose.yml
version: ""3.7""
services:
    traefik:
        image: traefik:v2.0
        ports:
            - 80:80
            - 443:443
            - 8080:8080
            - 6379:6379
            - 5432:5432
            - 27017:27017
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock:ro
            - ./config.toml:/etc/traefik/traefik.config.toml:ro
            - ./certs:/etc/certs:ro
        command:
            - --api.insecure
            - --accesslog
            - --log.level=INFO
            - --entrypoints.http.address=:80
            - --entrypoints.https.address=:443
            - --entrypoints.traefik.address=:8080
            - --entrypoints.mongo.address=:27017
            - --entrypoints.postgres.address=:5432
            - --entrypoints.redis.address=:6379
            - --providers.file.filename=/etc/traefik/traefik.config.toml
            - --providers.docker
            - --providers.docker.exposedByDefault=false
            - --providers.docker.useBindPortIP=false
    apache:
        image: php:7.2-apache
        labels:
            - traefik.enable=true
            - traefik.http.routers.http-dev.entrypoints=http
            - traefik.http.routers.http-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.entrypoints=https
            - traefik.http.routers.https-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.tls=true
            - traefik.http.services.dev.loadbalancer.server.port=80
    pgsql:
        image: postgres:10
        environment:
            POSTGRES_DB: postgres
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: password
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.pgsql.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.pgsql.tls=true
            - traefik.tcp.routers.pgsql.service=pgsql
            - traefik.tcp.routers.pgsql.entrypoints=postgres
            - traefik.tcp.services.pgsql.loadbalancer.server.port=5432
    mongo:
        image: mongo:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.mongo.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.mongo.tls=true
            - traefik.tcp.routers.mongo.service=mongo
            - traefik.tcp.routers.mongo.entrypoints=mongo
            - traefik.tcp.services.mongo.loadbalancer.server.port=27017
    redis:
        image: redis:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.redis.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.redis.tls=true
            - traefik.tcp.routers.redis.service=redis
            - traefik.tcp.routers.redis.entrypoints=redis
            - traefik.tcp.services.redis.loadbalancer.server.port=6379
config.toml
[tls]
[[tls.certificates]]
certFile = ""/etc/certs/example.localhost.pem""
keyFile = ""/etc/certs/example.localhost-key.pem""
Build &amp; Run
mkcert example.localhost # in ./certs/
docker-compose up -d
Prepare step by step
Install mkcert (run also mkcert -install for CA)
Clone my code
In certs folder run mkcert example.localhost
Start container by docker-compose up -d
Open page https://example.localhost/ and check if it is secure connection
If address http://example.localhost/ is not reachable, add 127.0.0.1 example.localhost to /etc/hosts
Certs:
Public: ./certs/example.localhost.pem
Private: ./certs/example.localhost-key.pem
CA: ~/.local/share/mkcert/rootCA.pem
Test MongoDB
Install Robo 3T
Create new connection:
Address: example.localhost
Use SSL protocol
CA Certificate: rootCA.pem (or Self-signed Certificate)
Test tool:
Test Redis
Install RedisDesktopManager
Create new connection:
Address: example.localhost
SSL
Public Key: example.localhost.pem
Private Key: example.localhost-key.pem
Authority: rootCA.pem
Test tool:
So far:
Can connect to Postgres via IP (info from Traefik)
jdbc:postgresql://172.21.0.4:5432/postgres?sslmode=disable
jdbc:postgresql://172.21.0.4:5432/postgres?sslfactory=org.postgresql.ssl.NonValidatingFactory
Try telet (IP changes every docker restart):
&gt; telnet 172.27.0.5 5432
Trying 172.27.0.5...
Connected to 172.27.0.5.
Escape character is '^]'.
^]
Connection closed by foreign host.
&gt; telnet example.localhost 5432
Trying ::1...
Connected to example.localhost.
Escape character is '^]'.
^]
HTTP/1.1 400 Bad Request
Content-Type: text/plain; charset=utf-8
Connection: close
400 Bad RequestConnection closed by foreign host.
If I connect directly to postgres, the data is nice. If I connect to via Traefik then I have Bad Request when closing the connection. I have no idea what this means and whether it must mean something.
",<postgresql><docker><ssl><tcp><traefik>,6320,12,120,1882,1,23,30,60,8244,0.0,377,2,15,2019-12-10 9:51,2020-01-29 1:34,,50.0,,Advanced,37,"<postgresql><docker><ssl><tcp><traefik>, How to connect to Traefik TCP Services with TLS configuration enabled?, I am trying to configure Traefik so that I would have access to services via domain names, and that I would not have to set different ports. For example, two MongoDB services, both on the default port, but in different domains, example.localhost and example2.localhost. Only this example works. I mean, other cases probably work, but I can't connect to them, and I don't understand what the problem is. This is probably not even a problem with Traefik.
I have prepared a repository with an example that works. You just need to generate your own certificate with mkcert. The page at example.localhost returns the 403 Forbidden error but you should not worry about it, because the purpose of this configuration is to show that SSL is working (padlock, green status). So don't focus on 403.
Only the SSL connection to the mongo service works. I tested it with the Robo 3T program. After selecting the SSL connection, providing the host on example.localhost and selecting the certificate for a self-signed (or own) connection works. And that's the only thing that works that way. Connections to redis (Redis Desktop Manager) and to pgsql (PhpStorm, DBeaver, DbVisualizer) do not work, regardless of whether I provide certificates or not. I do not forward SSL to services, I only connect to Traefik. I spent long hours on it. I searched the internet. I haven't found the answer yet. Has anyone solved this?
PS. I work on Linux Mint, so my configuration should work in this environment without any problem. I would ask for solutions for Linux.
If you do not want to browse the repository, I attach the most important files:
docker-compose.yml
version: ""3.7""
services:
    traefik:
        image: traefik:v2.0
        ports:
            - 80:80
            - 443:443
            - 8080:8080
            - 6379:6379
            - 5432:5432
            - 27017:27017
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock:ro
            - ./config.toml:/etc/traefik/traefik.config.toml:ro
            - ./certs:/etc/certs:ro
        command:
            - --api.insecure
            - --accesslog
            - --log.level=INFO
            - --entrypoints.http.address=:80
            - --entrypoints.https.address=:443
            - --entrypoints.traefik.address=:8080
            - --entrypoints.mongo.address=:27017
            - --entrypoints.postgres.address=:5432
            - --entrypoints.redis.address=:6379
            - --providers.file.filename=/etc/traefik/traefik.config.toml
            - --providers.docker
            - --providers.docker.exposedByDefault=false
            - --providers.docker.useBindPortIP=false
    apache:
        image: php:7.2-apache
        labels:
            - traefik.enable=true
            - traefik.http.routers.http-dev.entrypoints=http
            - traefik.http.routers.http-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.entrypoints=https
            - traefik.http.routers.https-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.tls=true
            - traefik.http.services.dev.loadbalancer.server.port=80
    pgsql:
        image: postgres:10
        environment:
            POSTGRES_DB: postgres
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: password
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.pgsql.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.pgsql.tls=true
            - traefik.tcp.routers.pgsql.service=pgsql
            - traefik.tcp.routers.pgsql.entrypoints=postgres
            - traefik.tcp.services.pgsql.loadbalancer.server.port=5432
    mongo:
        image: mongo:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.mongo.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.mongo.tls=true
            - traefik.tcp.routers.mongo.service=mongo
            - traefik.tcp.routers.mongo.entrypoints=mongo
            - traefik.tcp.services.mongo.loadbalancer.server.port=27017
    redis:
        image: redis:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.redis.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.redis.tls=true
            - traefik.tcp.routers.redis.service=redis
            - traefik.tcp.routers.redis.entrypoints=redis
            - traefik.tcp.services.redis.loadbalancer.server.port=6379
config.toml
[tls]
[[tls.certificates]]
certFile = ""/etc/certs/example.localhost.pem""
keyFile = ""/etc/certs/example.localhost-key.pem""
Build &amp; Run
mkcert example.localhost # in ./certs/
docker-compose up -d
Prepare step by step
Install mkcert (run also mkcert -install for CA)
Clone my code
In certs folder run mkcert example.localhost
Start container by docker-compose up -d
Open page https://example.localhost/ and check if it is secure connection
If address http://example.localhost/ is not reachable, add 127.0.0.1 example.localhost to /etc/hosts
Certs:
Public: ./certs/example.localhost.pem
Private: ./certs/example.localhost-key.pem
CA: ~/.local/share/mkcert/rootCA.pem
Test MongoDB
Install Robo 3T
Create new connection:
Address: example.localhost
Use SSL protocol
CA Certificate: rootCA.pem (or Self-signed Certificate)
Test tool:
Test Redis
Install RedisDesktopManager
Create new connection:
Address: example.localhost
SSL
Public Key: example.localhost.pem
Private Key: example.localhost-key.pem
Authority: rootCA.pem
Test tool:
So far:
Can connect to Postgres via IP (info from Traefik)
jdbc:postgresql://172.21.0.4:5432/postgres?sslmode=disable
jdbc:postgresql://172.21.0.4:5432/postgres?sslfactory=org.postgresql.ssl.NonValidatingFactory
Try telet (IP changes every docker restart):
&gt; telnet 172.27.0.5 5432
Trying 172.27.0.5...
Connected to 172.27.0.5.
Escape character is '^]'.
^]
Connection closed by foreign host.
&gt; telnet example.localhost 5432
Trying ::1...
Connected to example.localhost.
Escape character is '^]'.
^]
HTTP/1.1 400 Bad Request
Content-Type: text/plain; charset=utf-8
Connection: close
400 Bad RequestConnection closed by foreign host.
If I connect directly to postgres, the data is nice. If I connect to via Traefik then I have Bad Request when closing the connection. I have no idea what this means and whether it must mean something.
","<postgresql><doctor><sal><top><traffic>, connect traffic top service to configur enabled?, try configur traffic would access service via domain names, would set differ ports. example, two mongodb services, default port, differ domain, example.localhost example.localhost. example works. mean, case probably work, can't connect them, understand problem is. probably even problem traffic. prepare depositors example works. need genet certify mkcert. page example.localhost return 403 forbidden error worry it, purpose configur show sal work (padlocks, green status). focus 403. sal connect monro service works. test rob it program. select sal connection, proved host example.localhost select certify self-sign (or own) connect works. that' thing work way. connect red (red desktop manager) pgsql (phpstorm, beaver, dbvisualizer) work, regardless whether proved certify not. forward sal services, connect traffic. spent long hour it. search internet. found answer yet. anyone sole this? is. work line mint, configur work environs without problem. would ask slut line. want brows depositors, attach import files: doctor-compose.you version: ""3.7"" services: traffic: image: traffic:ve.0 ports: - 80:80 - 443:443 - 8080:8080 - 6379:6379 - 5432:5432 - 27017:27017 volumes: - /war/run/doctor.sock:/war/run/doctor.sock:to - ./confirm.toll:/etc/traffic/traffic.confirm.toll:to - ./carts:/etc/carts:to command: - --apt.insecure - --accession - --log.level=into - --entrypoints.http.address=:80 - --entrypoints.http.address=:443 - --entrypoints.traffic.address=:8080 - --entrypoints.monro.address=:27017 - --entrypoints.postures.address=:5432 - --entrypoints.red.address=:6379 - --provides.file.filename=/etc/traffic/traffic.confirm.toll - --provides.dock - --provides.doctor.exposedbydefault=fall - --provides.doctor.usebindportip=fall apache: image: pp:7.2-apace labels: - traffic.enable=true - traffic.http.routes.http-de.entrypoints=http - traffic.http.routes.http-de.rule=host(`example.localhost`) - traffic.http.routes.http-de.entrypoints=http - traffic.http.routes.http-de.rule=host(`example.localhost`) - traffic.http.routes.http-de.tis=true - traffic.http.services.de.loadbalancer.server.port=80 pgsql: image: postures:10 environment: postgres_db: poster postgres_user: poster postgres_password: password labels: - traffic.enable=true - traffic.top.routes.pgsql.rule=hosts(`example.localhost`) - traffic.top.routes.pgsql.tis=true - traffic.top.routes.pgsql.service=pgsql - traffic.top.routes.pgsql.entrypoints=poster - traffic.top.services.pgsql.loadbalancer.server.port=5432 monro: image: monro:3 labels: - traffic.enable=true - traffic.top.routes.monro.rule=hosts(`example.localhost`) - traffic.top.routes.monro.tis=true - traffic.top.routes.monro.service=monro - traffic.top.routes.monro.entrypoints=monro - traffic.top.services.monro.loadbalancer.server.port=27017 red: image: red:3 labels: - traffic.enable=true - traffic.top.routes.red.rule=hosts(`example.localhost`) - traffic.top.routes.red.tis=true - traffic.top.routes.red.service=red - traffic.top.routes.red.entrypoints=red - traffic.top.services.red.loadbalancer.server.port=6379 confirm.toll [tis] [[tis.certificates]] certain = ""/etc/carts/example.localhost.per"" keyfil = ""/etc/carts/example.localhost-key.per"" build &amp; run mkcert example.localhost # ./carts/ doctor-compose -d prepare step step instal mkcert (run also mkcert -instal ca) alone code cent older run mkcert example.localhost start contain doctor-compose -d open page http://example.localhost/ check secure connect address http://example.localhost/ readable, add 127.0.0.1 example.localhost /etc/host carts: public: ./carts/example.localhost.per private: ./carts/example.localhost-key.per ca: ~/.local/share/mkcert/root.per test mongodb instal rob it great new connection: address: example.localhost use sal protocol ca certificate: root.per (or self-sign certificate) test tool: test red instal redisdesktopmanag great new connection: address: example.localhost sal public key: example.localhost.per privat key: example.localhost-key.per authority: root.per test tool: far: connect poster via in (into traffic) job:postgresql://172.21.0.4:5432/postures?sslmode=dis job:postgresql://172.21.0.4:5432/postures?sslfactory=org.postgresql.sal.nonvalidatingfactori try felt (in change every doctor start): &it; tent 172.27.0.5 5432 try 172.27.0.5... connect 172.27.0.5. escape character '^]'. ^] connect close foreign host. &it; tent example.localhost 5432 try ::1... connect example.localhost. escape character '^]'. ^] http/1.1 400 bad request content-type: text/plain; charge=utf-8 connection: close 400 bad requestconnect close foreign host. connect directly postures, data nice. connect via traffic bad request close connection. idea mean whether must mean something."
48277519,how to use COMMIT and ROLLBACK in a PostgreSQL function,"I am using three insert statements, and if there is an error in the third statement, I want to rollback the first and the second one. If there is no way to do this, please tell me a different approach to handle this in PostgresqQL.
If I use COMMIT or ROLLBACK, I get an error.
CREATE OR REPLACE FUNCTION TEST1 ()
   RETURNS VOID
   LANGUAGE 'plpgsql'
   AS $$
BEGIN 
    INSERT INTO table1 VALUES (1);
    INSERT INTO table1 VALUES (2);
    INSERT INTO table1 VALUES ('A');
    COMMIT;
EXCEPTION
   WHEN OTHERS THEN
   ROLLBACK;
END;$$;
The above code is not working; COMMIT and ROLLBACK are not supported by PostgreSQL functions.
",<postgresql><postgresql-10>,631,0,20,487,3,7,23,60,47949,0.0,10,3,15,2018-01-16 9:05,2018-01-16 9:16,2018-01-16 9:16,0.0,0.0,Basic,10,"<postgresql><postgresql-10>, how to use COMMIT and ROLLBACK in a PostgreSQL function, I am using three insert statements, and if there is an error in the third statement, I want to rollback the first and the second one. If there is no way to do this, please tell me a different approach to handle this in PostgresqQL.
If I use COMMIT or ROLLBACK, I get an error.
CREATE OR REPLACE FUNCTION TEST1 ()
   RETURNS VOID
   LANGUAGE 'plpgsql'
   AS $$
BEGIN 
    INSERT INTO table1 VALUES (1);
    INSERT INTO table1 VALUES (2);
    INSERT INTO table1 VALUES ('A');
    COMMIT;
EXCEPTION
   WHEN OTHERS THEN
   ROLLBACK;
END;$$;
The above code is not working; COMMIT and ROLLBACK are not supported by PostgreSQL functions.
","<postgresql><postgresql-10>, use commit rollback postgresql function, use three insert statements, error third statement, want rollback first second one. way this, pleas tell differ approach hand postgresqql. use commit rollback, get error. great replace function test () return void language 'plpgsql' $$ begin insert table value (1); insert table value (2); insert table value ('a'); commit; except other rollback; end;$$; code working; commit rollback support postgresql functions."
63816057,How do i close database instance in gorm 1.20.0,"As i have not found in Close() function with *gorm instance, any help would be appreciated
dbURI := fmt.Sprintf(&quot;user=%s password=%s dbname=%s port=%s sslmode=%s TimeZone=%s&quot;,
    &quot;username&quot;, &quot;password&quot;, &quot;dbname&quot;, &quot;5432&quot;, &quot;disable&quot;, &quot;Asia/Kolkata&quot;)
fmt.Println(dbURI)
connection, err := gorm.Open(postgres.Open(dbURI), &amp;gorm.Config{})
if err != nil {
    fmt.Println(&quot;Error connecting database&quot;)
    panic(err.Error())
} else {
    fmt.Println(&quot;Connected to database&quot;)
}
Note: connection.Close() is not available for GORM 1.20.0
",<postgresql><go><go-gorm><glide-golang>,623,0,11,497,1,4,16,43,26825,0.0,25,3,15,2020-09-09 16:46,2020-09-09 18:24,,0.0,,Basic,10,"<postgresql><go><go-gorm><glide-golang>, How do i close database instance in gorm 1.20.0, As i have not found in Close() function with *gorm instance, any help would be appreciated
dbURI := fmt.Sprintf(&quot;user=%s password=%s dbname=%s port=%s sslmode=%s TimeZone=%s&quot;,
    &quot;username&quot;, &quot;password&quot;, &quot;dbname&quot;, &quot;5432&quot;, &quot;disable&quot;, &quot;Asia/Kolkata&quot;)
fmt.Println(dbURI)
connection, err := gorm.Open(postgres.Open(dbURI), &amp;gorm.Config{})
if err != nil {
    fmt.Println(&quot;Error connecting database&quot;)
    panic(err.Error())
} else {
    fmt.Println(&quot;Connected to database&quot;)
}
Note: connection.Close() is not available for GORM 1.20.0
","<postgresql><go><go-form><glide-going>, close database instant form 1.20.0, found close() function *form instance, help would appreci burn := fat.spring(&quit;user=% password=% name=% port=% sslmode=% timezone=%s&quit;, &quit;surname&quit;, &quit;password&quit;, &quit;name&quit;, &quit;5432&quit;, &quit;disabled&quit;, &quit;asia/kolkata&quit;) fat.print(burn) connection, err := form.open(postures.open(burn), &amp;form.confirm{}) err != nail { fat.print(&quit;error connect database&quit;) panic(err.error()) } else { fat.print(&quit;connect database&quit;) } note: connection.close() avail form 1.20.0"
50474156,java.sql.SQLException: Unknown initial character set index '255' received from server for connector 8.0.11,"While establishing the connection to a MySQL database, I'm getting the following error
java.sql.SQLException: Unknown initial character set index '255' received from 
server. Initial client character set can be forced via the 'characterEncoding' 
property.
Upon googling, I got to know that we need to modify 2 params in my.ini or my.cnf.
I am using MySQL version 8.0.11 and it does not have this file.
Hence I modified these parameters using the SQL commands:
Please note name and duration are column name in the table.    
ALTER TABLE courses MODIFY name VARCHAR(50) COLLATE utf8_unicode_ci;    
ALTER TABLE courses MODIFY duration VARCHAR(50) COLLATE utf8_unicode_ci;
ALTER TABLE courses MODIFY name VARCHAR(50) CHARACTER SET utf8;
ALTER TABLE courses MODIFY duration VARCHAR(50) CHARACTER SET utf8;
Hence my table looks like this 
After this, I restarted MySQL server, but I'm still getting the above error.
Please note I'm deploying my application in tomcat and running rest API call which will connect to the database. While connecting to the database, I'm getting the above error.
",<java><mysql><jdbc>,1088,1,8,445,2,6,13,76,52338,0.0,34,15,15,2018-05-22 18:20,2018-05-22 18:32,2018-08-14 5:25,0.0,84.0,Basic,10,"<java><mysql><jdbc>, java.sql.SQLException: Unknown initial character set index '255' received from server for connector 8.0.11, While establishing the connection to a MySQL database, I'm getting the following error
java.sql.SQLException: Unknown initial character set index '255' received from 
server. Initial client character set can be forced via the 'characterEncoding' 
property.
Upon googling, I got to know that we need to modify 2 params in my.ini or my.cnf.
I am using MySQL version 8.0.11 and it does not have this file.
Hence I modified these parameters using the SQL commands:
Please note name and duration are column name in the table.    
ALTER TABLE courses MODIFY name VARCHAR(50) COLLATE utf8_unicode_ci;    
ALTER TABLE courses MODIFY duration VARCHAR(50) COLLATE utf8_unicode_ci;
ALTER TABLE courses MODIFY name VARCHAR(50) CHARACTER SET utf8;
ALTER TABLE courses MODIFY duration VARCHAR(50) CHARACTER SET utf8;
Hence my table looks like this 
After this, I restarted MySQL server, but I'm still getting the above error.
Please note I'm deploying my application in tomcat and running rest API call which will connect to the database. While connecting to the database, I'm getting the above error.
","<cava><myself><job>, cava.sal.sqlexception: unknown into character set index '255' receive server connection 8.0.11, establish connect myself database, i'm get follow error cava.sal.sqlexception: unknown into character set index '255' receive server. into client character set for via 'characterencoding' property. upon cooling, got know need modify 2 parma my.in my.cf. use myself version 8.0.11 file. hence modify parapet use sal commands: pleas note name murat column name table. alter table course modify name varchar(50) collar utf8_unicode_ci; alter table course modify murat varchar(50) collar utf8_unicode_ci; alter table course modify name varchar(50) character set utf; alter table course modify murat varchar(50) character set utf; hence table look like this, start myself server, i'm still get error. pleas note i'm deploy applied combat run rest apt call connect database. connect database, i'm get error."
48562745,Select from multiple tables in one call,"In my code I have a page that includes information from 3 different tables. To show this information I make 3 SQL select calls and unite them in one list to pass as Model to my view. Can I do it with one SQL call? Data has no connection with one another. 
My code:
public ActionResult Index()
{
    StorePageData PageData = new StorePageData();
    return View(PageData);
}
public class StorePageData
{
     public List&lt;Table1Data&gt; Table1 { get; set; }
     public List&lt;Table2Data&gt; Table2 { get; set; }
     public List&lt;Table3Data&gt; Table3 { get; set; }
     public StorePageData()
     {
          Table1  = //loading from Database1
          Table2  = //loading from Database2
          Table3  = //loading from Database3
     }
}
public class Table1Data
{
     public int Id { get; set; }
     public double Info1 { get; set; }
     public string Info2 { get; set; }
}
public class Table2Data
{
     public int Id { get; set; }
     public List&lt;int&gt; Info1 { get; set; }
     public List&lt;int&gt; Info2 { get; set; }
}
public class Table3Data
{
     public int Id { get; set; }
     public List&lt;string&gt; Info1 { get; set; }
     public List&lt;string&gt; Info2 { get; set; }
}
If there is a way to load all 3 tables in one SQL request it will improve significantly the load time of this page.
Thank you.
",<c#><asp.net><.net><sql-server><asp.net-mvc>,1336,0,36,2700,2,25,32,52,16257,0.0,403,7,15,2018-02-01 12:25,2018-02-01 12:32,2018-02-05 0:07,0.0,4.0,Basic,10,"<c#><asp.net><.net><sql-server><asp.net-mvc>, Select from multiple tables in one call, In my code I have a page that includes information from 3 different tables. To show this information I make 3 SQL select calls and unite them in one list to pass as Model to my view. Can I do it with one SQL call? Data has no connection with one another. 
My code:
public ActionResult Index()
{
    StorePageData PageData = new StorePageData();
    return View(PageData);
}
public class StorePageData
{
     public List&lt;Table1Data&gt; Table1 { get; set; }
     public List&lt;Table2Data&gt; Table2 { get; set; }
     public List&lt;Table3Data&gt; Table3 { get; set; }
     public StorePageData()
     {
          Table1  = //loading from Database1
          Table2  = //loading from Database2
          Table3  = //loading from Database3
     }
}
public class Table1Data
{
     public int Id { get; set; }
     public double Info1 { get; set; }
     public string Info2 { get; set; }
}
public class Table2Data
{
     public int Id { get; set; }
     public List&lt;int&gt; Info1 { get; set; }
     public List&lt;int&gt; Info2 { get; set; }
}
public class Table3Data
{
     public int Id { get; set; }
     public List&lt;string&gt; Info1 { get; set; }
     public List&lt;string&gt; Info2 { get; set; }
}
If there is a way to load all 3 tables in one SQL request it will improve significantly the load time of this page.
Thank you.
","<c#><asp.net><.net><sal-server><asp.net-mac>, select multiple table one call, code page include inform 3 differ tables. show inform make 3 sal select call unit one list pass model view. one sal call? data connect one another. code: public actionresult index() { storepagedata pagedata = new storepagedata(); return view(pagedata); } public class storepagedata { public list&it;table1data&it; table { get; set; } public list&it;table2data&it; table { get; set; } public list&it;table3data&it; table { get; set; } public storepagedata() { table = //load database table = //load database table = //load database } } public class table1data { public in id { get; set; } public doubt into { get; set; } public string into { get; set; } } public class table2data { public in id { get; set; } public list&it;in&it; into { get; set; } public list&it;in&it; into { get; set; } } public class table3data { public in id { get; set; } public list&it;string&it; into { get; set; } public list&it;string&it; into { get; set; } } way load 3 table one sal request improve significantly load time page. thank you."
58894875,How to delete a database in pgadmin,"When i try to create other database with the name ""eCommerce"" in pgadmin 4 this message appears 
ERROR: source database ""template1"" is being accessed by other users
DETAIL: There are 2 other sessions using the database.
I try to delete the others databases but is not working and appears
ERROR: cannot drop a template database
What should i do?
",<java><sql><database>,345,0,0,155,1,2,6,35,35059,0.0,4,6,15,2019-11-16 20:26,2019-11-16 22:26,2019-11-16 22:26,0.0,0.0,Basic,10,"<java><sql><database>, How to delete a database in pgadmin, When i try to create other database with the name ""eCommerce"" in pgadmin 4 this message appears 
ERROR: source database ""template1"" is being accessed by other users
DETAIL: There are 2 other sessions using the database.
I try to delete the others databases but is not working and appears
ERROR: cannot drop a template database
What should i do?
","<cava><sal><database>, delete database pgadmin, try great database name ""commerce"" pgadmin 4 message appear error: source database ""template1"" access user detail: 2 session use database. try delete other database work appear error: cannot drop temple database do?"
51183321,How to use Paging with SQLite?,"I want to integrate Paging with SQLite in existing application. There are around 90 tables in my database so its time consuming to convert to Room. I also tried LIMIT...OFFSET but that takes time to process data every time. 
Thanks.
",<android><sqlite><paging>,233,2,1,4928,9,34,64,35,24595,0.0,113,0,15,2018-07-05 4:15,,,,,Basic,10,"<android><sqlite><paging>, How to use Paging with SQLite?, I want to integrate Paging with SQLite in existing application. There are around 90 tables in my database so its time consuming to convert to Room. I also tried LIMIT...OFFSET but that takes time to process data every time. 
Thanks.
","<andros><quite><paying>, use page quite?, want inter page quite exist application. around 90 table database time consume convert room. also try limit...offset take time process data every time. thanks."
56542036,"pgloader - Failed to connect to mysql at ""localhost"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled","I am trying to migrate my rails application from mysql to postgres. Since we have already running application so I am moving mysql data to postgres database using pgloader. But when I do 
pgloader mysql://root:root_password@127.0.0.1/mysql_database postgresql://postgres_user:postgres_pass@127.0.0.1/postgres_database
I get error - Failed to connect to mysql at ""127.0.0.1"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled. I can easily log in to mysql from terminal though.
Thanks in advance.
",<mysql><ruby-on-rails><postgresql><pgloader>,536,0,1,279,1,2,12,77,11056,,10,1,15,2019-06-11 10:49,2020-03-21 15:19,,284.0,,Intermediate,18,"<mysql><ruby-on-rails><postgresql><pgloader>, pgloader - Failed to connect to mysql at ""localhost"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled, I am trying to migrate my rails application from mysql to postgres. Since we have already running application so I am moving mysql data to postgres database using pgloader. But when I do 
pgloader mysql://root:root_password@127.0.0.1/mysql_database postgresql://postgres_user:postgres_pass@127.0.0.1/postgres_database
I get error - Failed to connect to mysql at ""127.0.0.1"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled. I can easily log in to mysql from terminal though.
Thanks in advance.
","<myself><ruby-on-rails><postgresql><pgloader>, pgloader - fail connect myself ""localhost"" (port 3306) user ""root"": conduct mind:myself-supported-authentic signalled, try migrate rail applied myself postures. since already run applied move myself data poster database use pgloader. pgloader myself://root:root_password@127.0.0.1/mysql_databas postgresql://postgres_user:postgres_pass@127.0.0.1/postgres_databas get error - fail connect myself ""127.0.0.1"" (port 3306) user ""root"": conduct mind:myself-supported-authentic signalled. vasili log myself german though. thank advance."
51693126,"Why does DynamoDB seem to inconsistently return LastEvaluatedKey when no records remain, depending on the record limit of the query?","Using javascript aws-sdk and querying a dynamodb table, with document client. the table contains 10 elements and the query limit = 5. 
For each request I use the LastEvaluatedKey to build a new query and send a fresh request, here are the results :
first request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {}}
second request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {}}
third request --&gt; {Items: Array(0), Count: 0, ScannedCount: 0}
According to this doc 
  If the result contains a LastEvaluatedKey element, proceed to step 2. 
  If there is not a LastEvaluatedKey in the result, then there are no
  more items to be retrieved
It is supposed to not return LastEvaluatedKey in the second request, cause there is no more elements, but it returns one which send to an empty result in the third request.
When i try with limit = 4, every things works as expected
first request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {}}
second request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {}}
third request --&gt; {Items: Array(2), Count: 2, ScannedCount: 2} &lt;--- there is no LastEvaluatedKey as expected
So what is happening here ?
",<amazon-web-services><pagination><nosql><amazon-dynamodb>,1242,1,6,1779,4,17,38,48,19219,0.0,90,3,15,2018-08-05 9:34,2018-08-05 12:39,2018-08-05 12:44,0.0,0.0,Advanced,33,"<amazon-web-services><pagination><nosql><amazon-dynamodb>, Why does DynamoDB seem to inconsistently return LastEvaluatedKey when no records remain, depending on the record limit of the query?, Using javascript aws-sdk and querying a dynamodb table, with document client. the table contains 10 elements and the query limit = 5. 
For each request I use the LastEvaluatedKey to build a new query and send a fresh request, here are the results :
first request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {}}
second request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {}}
third request --&gt; {Items: Array(0), Count: 0, ScannedCount: 0}
According to this doc 
  If the result contains a LastEvaluatedKey element, proceed to step 2. 
  If there is not a LastEvaluatedKey in the result, then there are no
  more items to be retrieved
It is supposed to not return LastEvaluatedKey in the second request, cause there is no more elements, but it returns one which send to an empty result in the third request.
When i try with limit = 4, every things works as expected
first request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {}}
second request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {}}
third request --&gt; {Items: Array(2), Count: 2, ScannedCount: 2} &lt;--- there is no LastEvaluatedKey as expected
So what is happening here ?
","<amazon-web-services><agitation><nose><amazon-dynamodb>, dynamodb seem consist return lastevaluatedkey record remain, depend record limit query?, use javascript was-sd query dynamodb table, document client. table contain 10 element query limit = 5. request use lastevaluatedkey build new query send fresh request, result : first request --&it; {items: array(5), count: 5, scannedcount: 5, lastevaluatedkey: {}} second request --&it; {items: array(5), count: 5, scannedcount: 5, lastevaluatedkey: {}} third request --&it; {items: array(0), count: 0, scannedcount: 0} accord do result contain lastevaluatedkey element, proceed step 2. lastevaluatedkey result, item retrieve suppose return lastevaluatedkey second request, cause elements, return one send empty result third request. try limit = 4, every thing work expect first request --&it; {items: array(4), count: 4, scannedcount: 4, lastevaluatedkey: {}} second request --&it; {items: array(4), count: 4, scannedcount: 4, lastevaluatedkey: {}} third request --&it; {items: array(2), count: 2, scannedcount: 2} &it;--- lastevaluatedkey expect happen ?"
51947581,Select all columns except for some PostgreSQL,"I have to compare tables but there are some columns which I don't need to compare and I only know them (not the ones I have to compare) so I want to select all columns from table except the ones that I don't need to compare.
I thought of something like:
SELECT 'SELECT ' || array_to_string(ARRAY(SELECT 'o' || '.' || c.column_name
    FROM information_schema.columns As c
        WHERE table_name = 'office' 
        AND  c.column_name NOT IN('id', 'deleted')
), ',') || ' FROM officeAs o' As sqlstmt
however the output was SELECT * FROM office As o 
instead of being select a,b,c from office without id and deleted columns.
Does anyone have any ideas what's wrong with this query?
",<sql><postgresql>,682,0,8,2219,4,35,82,53,32424,,232,4,15,2018-08-21 11:21,2018-08-21 11:29,2018-08-21 11:29,0.0,0.0,Basic,10,"<sql><postgresql>, Select all columns except for some PostgreSQL, I have to compare tables but there are some columns which I don't need to compare and I only know them (not the ones I have to compare) so I want to select all columns from table except the ones that I don't need to compare.
I thought of something like:
SELECT 'SELECT ' || array_to_string(ARRAY(SELECT 'o' || '.' || c.column_name
    FROM information_schema.columns As c
        WHERE table_name = 'office' 
        AND  c.column_name NOT IN('id', 'deleted')
), ',') || ' FROM officeAs o' As sqlstmt
however the output was SELECT * FROM office As o 
instead of being select a,b,c from office without id and deleted columns.
Does anyone have any ideas what's wrong with this query?
","<sal><postgresql>, select column except postgresql, compare table column need compare know (not one compare) want select column table except one need compare. thought cometh like: select 'select ' || array_to_string(array(select 'o' || '.' || c.column_nam information_schema.column c table_nam = 'office' c.column_nam in('id', 'delete') ), ',') || ' officer o' sqlstmt howe output select * office instead select a,b,c office without id delete columns. anyone idea what' wrong query?"
53921336,An error happened while reading data from the provider. The remote certificate is invalid according to the validation procedure,"I'm trying to connect Postgres Database on AWS EC2 instance to Microsoft PowerBI. I tried various method available on internet but its showing the above error. Although I've done this connection on AWS RDS. I installed required dependencies (GAC) and all the certificates required for PowerBI.
",<windows><postgresql><amazon-ec2><powerbi>,294,1,0,333,1,2,13,67,26312,0.0,9,6,15,2018-12-25 10:10,2018-12-25 10:33,2018-12-25 10:33,0.0,0.0,Basic,13,"<windows><postgresql><amazon-ec2><powerbi>, An error happened while reading data from the provider. The remote certificate is invalid according to the validation procedure, I'm trying to connect Postgres Database on AWS EC2 instance to Microsoft PowerBI. I tried various method available on internet but its showing the above error. Although I've done this connection on AWS RDS. I installed required dependencies (GAC) and all the certificates required for PowerBI.
","<windows><postgresql><amazon-end><power>, error happen read data provider. remote certify invalid accord valid procedure, i'm try connect poster database a end instant microsoft power. try various method avail internet show error. although i'v done connect a rd. instal require depend (sac) certify require power."
50129411,Why is predicate pushdown not used in typed Dataset API (vs untyped DataFrame API)?,"I always thought that dataset/dataframe API's are the same.. and the only difference is that dataset API will give you compile time safety. Right ?
So.. I have very simple case:
 case class Player (playerID: String, birthYear: Int)
 val playersDs: Dataset[Player] = session.read
  .option(""header"", ""true"")
  .option(""delimiter"", "","")
  .option(""inferSchema"", ""true"")
  .csv(PeopleCsv)
  .as[Player]
 // Let's try to find players born in 1999. 
 // This will work, you have compile time safety... but it will not use predicate pushdown!!!
 playersDs.filter(_.birthYear == 1999).explain()
 // This will work as expected and use predicate pushdown!!!
 // But you can't have compile time safety with this :(
 playersDs.filter('birthYear === 1999).explain()
Explain from first example will show that it's NOT doing predicate pushdown (Notice empty PushedFilters):
== Physical Plan ==
*(1) Filter &lt;function1&gt;.apply
+- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
While the second sample will do it correctly (Notice PushedFilters):
== Physical Plan ==
*(1) Project [.....]
+- *(1) Filter (isnotnull(birthYear#11) &amp;&amp; (birthYear#11 = 1999))
   +- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [IsNotNull(birthYear), EqualTo(birthYear,1999)], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
So the question is.. how can I use DS Api, and have compile time safety.., and predicate pushdown working as expected ????
Is it possible ? If not.. does this mean that DS api gives you compile time safety.. but at the cost of performance!! ??? (DF will be much faster in this case.. especially when processing large parquet files)
",<apache-spark><dataframe><apache-spark-sql><apache-spark-dataset>,1998,0,23,165,0,1,6,81,3471,0.0,2,1,15,2018-05-02 7:40,2018-05-02 9:57,2018-05-02 9:57,0.0,0.0,Basic,3,"<apache-spark><dataframe><apache-spark-sql><apache-spark-dataset>, Why is predicate pushdown not used in typed Dataset API (vs untyped DataFrame API)?, I always thought that dataset/dataframe API's are the same.. and the only difference is that dataset API will give you compile time safety. Right ?
So.. I have very simple case:
 case class Player (playerID: String, birthYear: Int)
 val playersDs: Dataset[Player] = session.read
  .option(""header"", ""true"")
  .option(""delimiter"", "","")
  .option(""inferSchema"", ""true"")
  .csv(PeopleCsv)
  .as[Player]
 // Let's try to find players born in 1999. 
 // This will work, you have compile time safety... but it will not use predicate pushdown!!!
 playersDs.filter(_.birthYear == 1999).explain()
 // This will work as expected and use predicate pushdown!!!
 // But you can't have compile time safety with this :(
 playersDs.filter('birthYear === 1999).explain()
Explain from first example will show that it's NOT doing predicate pushdown (Notice empty PushedFilters):
== Physical Plan ==
*(1) Filter &lt;function1&gt;.apply
+- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
While the second sample will do it correctly (Notice PushedFilters):
== Physical Plan ==
*(1) Project [.....]
+- *(1) Filter (isnotnull(birthYear#11) &amp;&amp; (birthYear#11 = 1999))
   +- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [IsNotNull(birthYear), EqualTo(birthYear,1999)], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
So the question is.. how can I use DS Api, and have compile time safety.., and predicate pushdown working as expected ????
Is it possible ? If not.. does this mean that DS api gives you compile time safety.. but at the cost of performance!! ??? (DF will be much faster in this case.. especially when processing large parquet files)
","<apache-spark><dataframe><apache-spark-sal><apache-spark-dataset>, predict pushdown use type dataset apt (v until datafram apt)?, away thought dataset/datafram apt' same.. differ dataset apt give compel time safety. right ? so.. simple case: case class player (played: string, birthyear: in) val players: dataset[player] = session.read .option(""header"", ""true"") .option(""delimited"", "","") .option(""inferschema"", ""true"") .is(peoples) .as[player] // let' try find player born 1999. // work, compel time safety... use predict pushdown!!! players.filter(_.birthyear == 1999).explain() // work expect use predict pushdown!!! // can't compel time safety :( players.filter('birthyear === 1999).explain() explain first example show predict pushdown (notice empty pushedfilters): == physics plan == *(1) filter &it;function&it;.apply +- *(1) silesian is [...] watched: false, format: is, location: inmemoryfileindex[file:people.is], partitionfilters: [], pushedfilters: [], readschema: struck&it;played:string,birthyear:in,birthmonth:in,birthday:in,birthcountry:string,birthstate:s... second sample correctly (notice pushedfilters): == physics plan == *(1) project [.....] +- *(1) filter (isnotnull(birthyear#11) &amp;&amp; (birthyear#11 = 1999)) +- *(1) silesian is [...] watched: false, format: is, location: inmemoryfileindex[file:people.is], partitionfilters: [], pushedfilters: [isnotnull(birthyear), equal(birthyear,1999)], readschema: struck&it;played:string,birthyear:in,birthmonth:in,birthday:in,birthcountry:string,birthstate:s... question is.. use is apt, compel time safety.., predict pushdown work expect ???? possible ? not.. mean is apt give compel time safety.. cost performance!! ??? (of much faster case.. respect process large parquet files)"
51800800,Results in Grid is not showing in SQL Server,"I couldn't see the results in grid pane, so I did the following: 
Tools &gt; Options &gt; Query Results &gt; Results to Grid 
      &gt; uncheck the ""Discard results after execution"" check box
But still I couldn't see the results in grid. I even hit the ""Reset to Default"" button in same window yet no luck
Please shed some light on this.
",<sql-server><sql-server-2012><ssms><ssms-2012>,339,0,2,751,1,11,28,72,33458,,108,3,15,2018-08-11 14:46,2018-08-11 14:57,,0.0,,Basic,14,"<sql-server><sql-server-2012><ssms><ssms-2012>, Results in Grid is not showing in SQL Server, I couldn't see the results in grid pane, so I did the following: 
Tools &gt; Options &gt; Query Results &gt; Results to Grid 
      &gt; uncheck the ""Discard results after execution"" check box
But still I couldn't see the results in grid. I even hit the ""Reset to Default"" button in same window yet no luck
Please shed some light on this.
","<sal-server><sal-server-2012><sums><sums-2012>, result grid show sal server, see result grid pane, following: tool &it; option &it; query result &it; result grid &it; check ""discard result execution"" check box still see result grid. even hit ""rest default"" button window yet luck pleas shed light this."
62881953,Redis vs SQL Server performance,"Application performance is one of the main reason of using cache over relational database. Because it stores data in memory in the form of key value pair, we can store frequently accessed data in cache which are not changes very frequently. Reading from cache is much faster than database. Redis is one of the best solution in distributed cache market.
I was doing a performance test between Azure Redis cache and Azure SQL Server. I have created a simple ASP.NET Core application and inside that I have read data from SQL Server database as well as Redis multiple times and compare the read time duration between them. For database reading I have used Entity Framework Core and for Redis reading I have used 'Microsoft.Extensions.Caching.StackExchangeRedis'.
Model
using System;
namespace WebApplication2.Models
{
    [Serializable]
    public class Student
    {
        public int Id { get; set; }
        public string Name { get; set; }
        public int Age { get; set; }
        public string Subject { get; set; }
        public Student()
        {
            Name = string.Empty;
            Subject = string.Empty;
        }
    }
}
Entity Framework Core data context.
using Microsoft.EntityFrameworkCore;
using WebApplication2.Models;
namespace WebApplication2.Data
{
    public class StudentContext : DbContext
    {
        public StudentContext(DbContextOptions&lt;StudentContext&gt; options)
            : base(options)
        {
        }
        public DbSet&lt;Student&gt;? Students { get; set; }
    }
}
Startup class
public void ConfigureServices(IServiceCollection services)
{
    services.AddControllersWithViews();
    string studentDbConnectionString = Configuration.GetConnectionString(&quot;StudentDbConnectionString&quot;);
    services.AddDbContext&lt;StudentContext&gt;(option =&gt; option.UseSqlServer(studentDbConnectionString));
    string redisConnectionString = Configuration.GetConnectionString(&quot;RedisConnectionString&quot;);
    services.AddStackExchangeRedisCache(options =&gt;
    {
        options.Configuration = redisConnectionString;
    });
}
appsettings.json
{
  &quot;Logging&quot;: {
    &quot;LogLevel&quot;: {
      &quot;Default&quot;: &quot;Information&quot;,
      &quot;Microsoft&quot;: &quot;Warning&quot;,
      &quot;Microsoft.Hosting.Lifetime&quot;: &quot;Information&quot;
     }
  },
  &quot;AllowedHosts&quot;: &quot;*&quot;,
  &quot;ConnectionStrings&quot;: {
    &quot;StudentDbConnectionString&quot;: &quot;[Azure SQL Server connection string]&quot;,
    &quot;RedisConnectionString&quot;: &quot;[Azure Redis cache connection string]&quot;
  }
}
Home controller
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Caching.Distributed;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Runtime.Serialization.Formatters.Binary;
using WebApplication2.Data;
using WebApplication2.Models;
namespace WebApplication2.Controllers
{
    public class HomeController : Controller
    {
        private readonly StudentContext _studentContext;
        private readonly IDistributedCache _cache;
        public HomeController(StudentContext studentContext, IDistributedCache cache)
        {
            _studentContext = studentContext;
            _cache = cache;
        }
        public IActionResult Index()
        {
            List&lt;Student&gt;? students = null;
            var counter = 10000;
            var sw = Stopwatch.StartNew();
            for (var i = 0; i &lt; counter; i++)
            {
                students = _studentContext.Students.OrderBy(student =&gt; student.Id).ToList();
            }
            sw.Stop();
            ViewData[&quot;DatabaseDuraion&quot;] = $&quot;Database: {sw.ElapsedMilliseconds}&quot;;
            if (students != null &amp;&amp; students.Count &gt; 0)
            {
                List&lt;Student&gt; studentsFromCache;
                var key = &quot;Students&quot;;
                _cache.Set(key, ObjectToByteArray(students));
                sw.Restart();
                for (var i = 0; i &lt; counter; i++)
                {
                    studentsFromCache = (List&lt;Student&gt;)ByteArrayToObject(_cache.Get(key));
                }
                sw.Stop();
                ViewData[&quot;RedisDuraion&quot;] = $&quot;Redis: {sw.ElapsedMilliseconds}&quot;;
            }
            return View();
        }
        private byte[] ObjectToByteArray(object obj)
        {
            var bf = new BinaryFormatter();
            using var ms = new MemoryStream();
            bf.Serialize(ms, obj);
            return ms.ToArray();
        }
        private object ByteArrayToObject(byte[] arrBytes)
        {
            using var memStream = new MemoryStream();
            var binForm = new BinaryFormatter();
            memStream.Write(arrBytes, 0, arrBytes.Length);
            memStream.Seek(0, SeekOrigin.Begin);
            object obj = binForm.Deserialize(memStream);
            return obj;
        }
    }
}
Home\Index.cshtml view
@{
    ViewData[&quot;Title&quot;] = &quot;Home Page&quot;;
}
&lt;div class=&quot;text-center&quot;&gt;        
    &lt;p&gt;@ViewData[&quot;DatabaseDuraion&quot;]&lt;/p&gt;
    &lt;p&gt;@ViewData[&quot;RedisDuraion&quot;]&lt;/p&gt;
&lt;/div&gt;
I have found SQL Server is faster than Redis.
The ASP.NET Core application is hosted in Azure App Service with the same location with Azure SQL Server and Azure Redis.
Please let me know why Redis is slower than SQL Server?
",<sql-server><azure><redis>,5510,1,142,494,1,4,14,77,15095,0.0,94,3,15,2020-07-13 18:21,2020-07-17 6:37,2020-07-17 6:37,4.0,4.0,Intermediate,23,"<sql-server><azure><redis>, Redis vs SQL Server performance, Application performance is one of the main reason of using cache over relational database. Because it stores data in memory in the form of key value pair, we can store frequently accessed data in cache which are not changes very frequently. Reading from cache is much faster than database. Redis is one of the best solution in distributed cache market.
I was doing a performance test between Azure Redis cache and Azure SQL Server. I have created a simple ASP.NET Core application and inside that I have read data from SQL Server database as well as Redis multiple times and compare the read time duration between them. For database reading I have used Entity Framework Core and for Redis reading I have used 'Microsoft.Extensions.Caching.StackExchangeRedis'.
Model
using System;
namespace WebApplication2.Models
{
    [Serializable]
    public class Student
    {
        public int Id { get; set; }
        public string Name { get; set; }
        public int Age { get; set; }
        public string Subject { get; set; }
        public Student()
        {
            Name = string.Empty;
            Subject = string.Empty;
        }
    }
}
Entity Framework Core data context.
using Microsoft.EntityFrameworkCore;
using WebApplication2.Models;
namespace WebApplication2.Data
{
    public class StudentContext : DbContext
    {
        public StudentContext(DbContextOptions&lt;StudentContext&gt; options)
            : base(options)
        {
        }
        public DbSet&lt;Student&gt;? Students { get; set; }
    }
}
Startup class
public void ConfigureServices(IServiceCollection services)
{
    services.AddControllersWithViews();
    string studentDbConnectionString = Configuration.GetConnectionString(&quot;StudentDbConnectionString&quot;);
    services.AddDbContext&lt;StudentContext&gt;(option =&gt; option.UseSqlServer(studentDbConnectionString));
    string redisConnectionString = Configuration.GetConnectionString(&quot;RedisConnectionString&quot;);
    services.AddStackExchangeRedisCache(options =&gt;
    {
        options.Configuration = redisConnectionString;
    });
}
appsettings.json
{
  &quot;Logging&quot;: {
    &quot;LogLevel&quot;: {
      &quot;Default&quot;: &quot;Information&quot;,
      &quot;Microsoft&quot;: &quot;Warning&quot;,
      &quot;Microsoft.Hosting.Lifetime&quot;: &quot;Information&quot;
     }
  },
  &quot;AllowedHosts&quot;: &quot;*&quot;,
  &quot;ConnectionStrings&quot;: {
    &quot;StudentDbConnectionString&quot;: &quot;[Azure SQL Server connection string]&quot;,
    &quot;RedisConnectionString&quot;: &quot;[Azure Redis cache connection string]&quot;
  }
}
Home controller
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Caching.Distributed;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Runtime.Serialization.Formatters.Binary;
using WebApplication2.Data;
using WebApplication2.Models;
namespace WebApplication2.Controllers
{
    public class HomeController : Controller
    {
        private readonly StudentContext _studentContext;
        private readonly IDistributedCache _cache;
        public HomeController(StudentContext studentContext, IDistributedCache cache)
        {
            _studentContext = studentContext;
            _cache = cache;
        }
        public IActionResult Index()
        {
            List&lt;Student&gt;? students = null;
            var counter = 10000;
            var sw = Stopwatch.StartNew();
            for (var i = 0; i &lt; counter; i++)
            {
                students = _studentContext.Students.OrderBy(student =&gt; student.Id).ToList();
            }
            sw.Stop();
            ViewData[&quot;DatabaseDuraion&quot;] = $&quot;Database: {sw.ElapsedMilliseconds}&quot;;
            if (students != null &amp;&amp; students.Count &gt; 0)
            {
                List&lt;Student&gt; studentsFromCache;
                var key = &quot;Students&quot;;
                _cache.Set(key, ObjectToByteArray(students));
                sw.Restart();
                for (var i = 0; i &lt; counter; i++)
                {
                    studentsFromCache = (List&lt;Student&gt;)ByteArrayToObject(_cache.Get(key));
                }
                sw.Stop();
                ViewData[&quot;RedisDuraion&quot;] = $&quot;Redis: {sw.ElapsedMilliseconds}&quot;;
            }
            return View();
        }
        private byte[] ObjectToByteArray(object obj)
        {
            var bf = new BinaryFormatter();
            using var ms = new MemoryStream();
            bf.Serialize(ms, obj);
            return ms.ToArray();
        }
        private object ByteArrayToObject(byte[] arrBytes)
        {
            using var memStream = new MemoryStream();
            var binForm = new BinaryFormatter();
            memStream.Write(arrBytes, 0, arrBytes.Length);
            memStream.Seek(0, SeekOrigin.Begin);
            object obj = binForm.Deserialize(memStream);
            return obj;
        }
    }
}
Home\Index.cshtml view
@{
    ViewData[&quot;Title&quot;] = &quot;Home Page&quot;;
}
&lt;div class=&quot;text-center&quot;&gt;        
    &lt;p&gt;@ViewData[&quot;DatabaseDuraion&quot;]&lt;/p&gt;
    &lt;p&gt;@ViewData[&quot;RedisDuraion&quot;]&lt;/p&gt;
&lt;/div&gt;
I have found SQL Server is faster than Redis.
The ASP.NET Core application is hosted in Azure App Service with the same location with Azure SQL Server and Azure Redis.
Please let me know why Redis is slower than SQL Server?
","<sal-server><azure><red>, red vs sal server performance, applied perform one main reason use each relate database. store data memory form key value pair, store frequent access data each change frequently. read each much faster database. red one best slut distribute each market. perform test azur red each azur sal server. great simple asp.net core applied inside read data sal server database well red multiple time compare read time murat them. database read use entity framework core red read use 'microsoft.extensions.catching.stackexchangeredis'. model use system; namespac webapplication2.model { [serializable] public class student { public in id { get; set; } public string name { get; set; } public in age { get; set; } public string subject { get; set; } public student() { name = string.empty; subject = string.empty; } } } entity framework core data context. use microsoft.entityframeworkcore; use webapplication2.models; namespac webapplication2.data { public class studentcontext : context { public studentcontext(dbcontextoptions&it;studentcontext&it; option) : base(option) { } public set&it;student&it;? student { get; set; } } } started class public void configureservices(iservicecollect services) { services.addcontrollerswithviews(); string studentdbconnectionstr = configuration.getconnectionstring(&quit;studentdbconnectionstring&quit;); services.adddbcontext&it;studentcontext&it;(opt =&it; option.usesqlserver(studentdbconnectionstring)); string redisconnectionstr = configuration.getconnectionstring(&quit;redisconnectionstring&quit;); services.addstackexchangerediscache(opt =&it; { option.configur = redisconnectionstring; }); } appsettings.son { &quit;logging&quit;: { &quit;loglevel&quit;: { &quit;default&quit;: &quit;information&quit;, &quit;microsoft&quit;: &quit;warning&quit;, &quit;microsoft.costing.lifetime&quit;: &quit;information&quit; } }, &quit;allowedhosts&quit;: &quit;*&quit;, &quit;connectionstrings&quit;: { &quit;studentdbconnectionstring&quit;: &quit;[azur sal server connect string]&quit;, &quit;redisconnectionstring&quit;: &quit;[azur red each connect string]&quit; } } home control use microsoft.aspnetcore.mac; use microsoft.extensions.catching.distributed; use system.collections.genetic; use system.diagnostic; use system.to; use system.line; use system.auntie.serialization.formatters.binary; use webapplication2.data; use webapplication2.models; namespac webapplication2.control { public class homecontrol : control { privat readonli studentcontext _studentcontext; privat readonli idistributedcach ache; public homecontroller(studentcontext studentcontext, idistributedcach ache) { _studentcontext = studentcontext; each = ache; } public iactionresult index() { list&it;student&it;? student = null; war counter = 10000; war s = stopwatch.started(); (war = 0; &it; counter; i++) { student = _studentcontext.students.orderly(stud =&it; student.id).moist(); } s.stop(); viewdata[&quit;databaseduraion&quit;] = $&quit;database: {s.elapsedmilliseconds}&quit;; (student != null &amp;&amp; students.count &it; 0) { list&it;student&it; studentsfromcache; war key = &quit;students&quit;; ache.set(key, objecttobytearray(students)); s.start(); (war = 0; &it; counter; i++) { studentsfromcach = (list&it;student&it;)bytearraytoobject(ache.get(key)); } s.stop(); viewdata[&quit;redisduraion&quit;] = $&quit;red: {s.elapsedmilliseconds}&quit;; } return view(); } privat bite[] objecttobytearray(object obs) { war of = new binaryformatter(); use war ms = new memorystream(); of.serialize(ms, obs); return ms.array(); } privat object bytearraytoobject(bite[] arrbytes) { use war memstream = new memorystream(); war inform = new binaryformatter(); memstream.write(arrbytes, 0, arrbytes.length); memstream.seek(0, seekorigin.begin); object obs = inform.deserialize(memstream); return obs; } } } home\index.html view @{ viewdata[&quit;title&quit;] = &quit;him page&quit;; } &it;did class=&quit;text-center&quit;&it; &it;p&it;@viewdata[&quit;databaseduraion&quit;]&it;/p&it; &it;p&it;@viewdata[&quit;redisduraion&quit;]&it;/p&it; &it;/did&it; found sal server faster red. asp.net core applied host azur pp service local azur sal server azur red. pleas let know red slower sal server?"
55093267,How to resolve writing configuration error while installing mysql?,"I am new to MySQL. I downloaded the MySQL windows installer and selected I guess everything I thought I would need for x64 and x86 products especially utilities, workbench and MySQL Server for MYSQL 8.0.15 
When I get to the Configuration Steps section I select Execute and this is what happens: 
  Writing Configuration file turns red. 
Next I checked the Log File: 
The Log File states: 
  Beginning configuration step: Writing Configuration File 
  Invalid server template 
  Ended configuration step: Writing configuration file. 
",<mysql><windows>,534,0,0,151,0,1,4,64,2806,,14,1,15,2019-03-10 23:01,2022-03-31 2:01,,1117.0,,Basic,14,"<mysql><windows>, How to resolve writing configuration error while installing mysql?, I am new to MySQL. I downloaded the MySQL windows installer and selected I guess everything I thought I would need for x64 and x86 products especially utilities, workbench and MySQL Server for MYSQL 8.0.15 
When I get to the Configuration Steps section I select Execute and this is what happens: 
  Writing Configuration file turns red. 
Next I checked the Log File: 
The Log File states: 
  Beginning configuration step: Writing Configuration File 
  Invalid server template 
  Ended configuration step: Writing configuration file. 
","<myself><windows>, resolve write configur error instal myself?, new myself. download myself window instal select guess every thought would need x x product respect utilities, workbench myself server myself 8.0.15 get configur step section select execute happens: write configur file turn red. next check log file: log file states: begin configur step: write configur file invalid server temple end configur step: write configur file."
55162939,BigQuery: Return First Value from Different Groups in a Group By,"I am currently having a problem with a Standard SQL query. I have a list of emails where every email can have multiple functions. See the example below on how the table looks like.
Email                         Function
peter@gmail.com               engineer
peter@gmail.com               specialist
dave@gmail.com                analyst
dave@gmail.com                tester
dave@gmail.com                manager
michael@gmail.com             intern
What I want is a query that returns every email once with the first function it finds. So the above table should return the following:
Email                         Function
peter@gmail.com               engineer
dave@gmail.com                analyst
michael@gmail.com             intern
How do I do this?
What I have right now is a simplified version of the query.
SELECT Email, Function
FROM database
GROUP BY Email, Function
The issue is here is that I have to put both Email and Function in the GROUP BY. If I only put Email in the Group By the query cannot run even though I only want the query to GROUP BY Email.
Thanks!
",<sql><google-bigquery><standards>,1077,0,14,171,1,1,6,60,22249,,3,6,15,2019-03-14 12:47,2019-03-14 12:49,,0.0,,Basic,10,"<sql><google-bigquery><standards>, BigQuery: Return First Value from Different Groups in a Group By, I am currently having a problem with a Standard SQL query. I have a list of emails where every email can have multiple functions. See the example below on how the table looks like.
Email                         Function
peter@gmail.com               engineer
peter@gmail.com               specialist
dave@gmail.com                analyst
dave@gmail.com                tester
dave@gmail.com                manager
michael@gmail.com             intern
What I want is a query that returns every email once with the first function it finds. So the above table should return the following:
Email                         Function
peter@gmail.com               engineer
dave@gmail.com                analyst
michael@gmail.com             intern
How do I do this?
What I have right now is a simplified version of the query.
SELECT Email, Function
FROM database
GROUP BY Email, Function
The issue is here is that I have to put both Email and Function in the GROUP BY. If I only put Email in the Group By the query cannot run even though I only want the query to GROUP BY Email.
Thanks!
","<sal><goose-bigquery><standards>, bigquery: return first value differ group group by, current problem standard sal query. list email every email multiple functions. see example table look like. email function peter@email.com engine peter@email.com specialist dave@email.com analyst dave@email.com tested dave@email.com manage michael@email.com inter want query return every email first function finds. table return following: email function peter@email.com engine dave@email.com analyst michael@email.com inter this? right simplify version query. select email, function database group email, function issue put email function group by. put email group query cannot run even though want query group email. thanks!"
54095648,Using Dapper to get nvarchar(max) returns a string trimmed to 4000 characters. Can this behaviour be changed?,"I have a SQL Server data table which stores a JSON string in one of its columns.  The JSON string is a serialised .net object and the data typically exceeds 4000 characters.
I have a simple stored procedure which I use to retrieve the data:
    @StageID int,
    @Description varchar(250) = null OUTPUT,
    @Program nvarchar(max) = null OUTPUT
AS
BEGIN
    SET NOCOUNT ON;
    SELECT @Program = StageProgram, @Description = Description 
    FROM StageProgram 
    WHERE StageID = @StageID;
    RETURN 0;
END 
I am using the data type nvarchar(max) for the column. When I serialise the .net object to JSON and write it to the database using Dapper, I find that the full string is correctly stored in the database.
However, when I attempt to retrieve the string I find that it is trimmed to 4000 characters, discarding the rest of the data.
Here is the relevant code:
DynamicParameters p = new DynamicParameters();
p.Add(""@StageID"", Properties.Settings.Default.StageID, DbType.Int32, ParameterDirection.Input);
p.Add(""@Description"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@Program"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@ReturnValue"", DbType.Int32, direction: ParameterDirection.ReturnValue);               
try
{
     int stageID = Properties.Settings.Default.StageID;
     connection.Execute(sql, p, commandType: CommandType.StoredProcedure);                 
     json = p.Get&lt;string&gt;(""@Program"");
     int r = p.Get&lt;int&gt;(""@ReturnValue"");                    
}
When I run this, the string json is trimmed to 4000 characters.
If I use the built in .net SQL Server connection to retrieve it instead (using a query rather than a stored procedure for simplicity), the full data is correctly returned:
SqlCommand getProgram = new SqlCommand(""SELECT StageProgram FROM StageProgram WHERE StageID = 1;"");
getProgram.Connection = connection;
string json = Convert.ToString(getProgram.ExecuteScalar());
Is an experienced Dapper user able to provide an explanation for this behaviour? 
Can it be changed?
",<c#><json><sql-server><dapper>,2062,0,31,409,0,6,15,66,8476,0.0,7,1,15,2019-01-08 16:13,2019-01-08 17:08,2019-01-08 17:08,0.0,0.0,Basic,14,"<c#><json><sql-server><dapper>, Using Dapper to get nvarchar(max) returns a string trimmed to 4000 characters. Can this behaviour be changed?, I have a SQL Server data table which stores a JSON string in one of its columns.  The JSON string is a serialised .net object and the data typically exceeds 4000 characters.
I have a simple stored procedure which I use to retrieve the data:
    @StageID int,
    @Description varchar(250) = null OUTPUT,
    @Program nvarchar(max) = null OUTPUT
AS
BEGIN
    SET NOCOUNT ON;
    SELECT @Program = StageProgram, @Description = Description 
    FROM StageProgram 
    WHERE StageID = @StageID;
    RETURN 0;
END 
I am using the data type nvarchar(max) for the column. When I serialise the .net object to JSON and write it to the database using Dapper, I find that the full string is correctly stored in the database.
However, when I attempt to retrieve the string I find that it is trimmed to 4000 characters, discarding the rest of the data.
Here is the relevant code:
DynamicParameters p = new DynamicParameters();
p.Add(""@StageID"", Properties.Settings.Default.StageID, DbType.Int32, ParameterDirection.Input);
p.Add(""@Description"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@Program"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@ReturnValue"", DbType.Int32, direction: ParameterDirection.ReturnValue);               
try
{
     int stageID = Properties.Settings.Default.StageID;
     connection.Execute(sql, p, commandType: CommandType.StoredProcedure);                 
     json = p.Get&lt;string&gt;(""@Program"");
     int r = p.Get&lt;int&gt;(""@ReturnValue"");                    
}
When I run this, the string json is trimmed to 4000 characters.
If I use the built in .net SQL Server connection to retrieve it instead (using a query rather than a stored procedure for simplicity), the full data is correctly returned:
SqlCommand getProgram = new SqlCommand(""SELECT StageProgram FROM StageProgram WHERE StageID = 1;"");
getProgram.Connection = connection;
string json = Convert.ToString(getProgram.ExecuteScalar());
Is an experienced Dapper user able to provide an explanation for this behaviour? 
Can it be changed?
","<c#><son><sal-server><damper>, use damper get nvarchar(max) return string trim 4000 characters. behaviour changed?, sal server data table store son string one columns. son string serialis .net object data topic face 4000 characters. simple store procedure use retrieve data: @stage in, @rescript varchar(250) = null output, @program nvarchar(max) = null output begin set count on; select @program = stageprogram, @rescript = rescript stageprogram stage = @stage; return 0; end use data type nvarchar(max) column. serialis .net object son write database use damper, find full string correctly store database. however, attempt retrieve string find trim 4000 characters, discard rest data. rule code: dynamicparamet p = new dynamicparameters(); p.add(""@stage"", properties.settings.default.stage, type.into, parameterdirection.input); p.add(""@description"", """", type.string, direction: parameterdirection.output); p.add(""@program"", """", type.string, direction: parameterdirection.output); p.add(""@returnvalue"", type.into, direction: parameterdirection.returnvalue); try { in stage = properties.settings.default.stage; connection.execute(sal, p, commandtype: commandtype.storedprocedure); son = p.get&it;string&it;(""@program""); in r = p.get&it;in&it;(""@returnvalue""); } run this, string son trim 4000 characters. use built .net sal server connect retrieve instead (use query rather store procedure simplicity), full data correctly returned: sqlcommand getprogram = new sqlcommand(""select stageprogram stageprogram stage = 1;""); getprogram.connect = connection; string son = convert.string(getprogram.executescalar()); experience damper user all proved explain behaviour? changed?"
57482120,"In Apache Spark, how to convert a slow RDD/dataset into a stream?","I'm investigating an interesting case that involves wide transformations (e.g. repartition &amp; join) on a slow RDD or dataset, e.g. the dataset defined by the following code:
val ds = sqlContext.createDataset(1 to 100)
  .repartition(1)
  .mapPartitions { itr =&gt;
    itr.map { ii =&gt;
      Thread.sleep(100)
      println(f""skewed - ${ii}"")
      ii
    }
  }
The slow dataset is relevant as it resembles a view of a remote data source, and the partition iterator is derived from a single-threaded network protocol (http, jdbc etc.), in this case, the speed of download > the speed of single-threaded processing, but &lt;&lt; the speed of distributed processing.
Unfortunately the conventional Spark computation model won't be efficient on a slow dataset because we are confined to one of the following options:
Use only narrow transformations (flatMap-ish) to pipe the stream with data processing end-to-end in a single thread, obviously the data processing will be a bottle neck and resource utilisation will be low.
Use a wide operation (repartitioning included) to balance the RDD/dataset, while this is essential for parallel data processing efficiency, the Spark coarse-grained scheduler demands that the download to be fully completed, which becomes another bottleneck.
Experiment
The following program represents a simple simulation of such case:
val mapped = ds
val mapped2 = mapped
  .repartition(10)
  .map { ii =&gt;
    println(f""repartitioned - ${ii}"")
    ii
  }
mapped2.foreach { _ =&gt;
  }
When executing the above program it can be observed that line println(f""repartitioned - ${ii}"") will not be executed before line println(f""skewed - ${ii}"") in RDD dependency.
I'd like to instruct Spark scheduler to start distributing/shipping data entries generated by the partition iterator before its task completion (through mechanisms like microbatch or stream). Is there a simple way of doing this? E.g. converting the slow dataset into a structured stream would be nice, but there should be alternatives that are better integrated.
Thanks a lot for your opinion
UPDATE: to make your experimentation easier I have appended my scala tests that can be ran out of the box:
package com.tribbloids.spookystuff.spike
import org.apache.spark.SparkContext
import org.apache.spark.sql.{SQLContext, SparkSession}
import org.scalatest.{FunSpec, Ignore}
@Ignore
class SlowRDDSpike extends FunSpec {
  lazy val spark: SparkSession = SparkSession.builder().master(""local[*]"").getOrCreate()
  lazy val sc: SparkContext = spark.sparkContext
  lazy val sqlContext: SQLContext = spark.sqlContext
  import sqlContext.implicits._
  describe(""is repartitioning non-blocking?"") {
    it(""dataset"") {
      val ds = sqlContext
        .createDataset(1 to 100)
        .repartition(1)
        .mapPartitions { itr =&gt;
          itr.map { ii =&gt;
            Thread.sleep(100)
            println(f""skewed - $ii"")
            ii
          }
        }
      val mapped = ds
      val mapped2 = mapped
        .repartition(10)
        .map { ii =&gt;
          Thread.sleep(400)
          println(f""repartitioned - $ii"")
          ii
        }
      mapped2.foreach { _ =&gt;
        }
    }
  }
  it(""RDD"") {
    val ds = sc
      .parallelize(1 to 100)
      .repartition(1)
      .mapPartitions { itr =&gt;
        itr.map { ii =&gt;
          Thread.sleep(100)
          println(f""skewed - $ii"")
          ii
        }
      }
    val mapped = ds
    val mapped2 = mapped
      .repartition(10)
      .map { ii =&gt;
        Thread.sleep(400)
        println(f""repartitioned - $ii"")
        ii
      }
    mapped2.foreach { _ =&gt;
      }
  }
}
",<scala><apache-spark><apache-spark-sql><spark-streaming>,3646,0,95,3756,14,65,106,80,971,0.0,490,2,15,2019-08-13 16:47,2020-06-26 10:13,,318.0,,Basic,10,"<scala><apache-spark><apache-spark-sql><spark-streaming>, In Apache Spark, how to convert a slow RDD/dataset into a stream?, I'm investigating an interesting case that involves wide transformations (e.g. repartition &amp; join) on a slow RDD or dataset, e.g. the dataset defined by the following code:
val ds = sqlContext.createDataset(1 to 100)
  .repartition(1)
  .mapPartitions { itr =&gt;
    itr.map { ii =&gt;
      Thread.sleep(100)
      println(f""skewed - ${ii}"")
      ii
    }
  }
The slow dataset is relevant as it resembles a view of a remote data source, and the partition iterator is derived from a single-threaded network protocol (http, jdbc etc.), in this case, the speed of download > the speed of single-threaded processing, but &lt;&lt; the speed of distributed processing.
Unfortunately the conventional Spark computation model won't be efficient on a slow dataset because we are confined to one of the following options:
Use only narrow transformations (flatMap-ish) to pipe the stream with data processing end-to-end in a single thread, obviously the data processing will be a bottle neck and resource utilisation will be low.
Use a wide operation (repartitioning included) to balance the RDD/dataset, while this is essential for parallel data processing efficiency, the Spark coarse-grained scheduler demands that the download to be fully completed, which becomes another bottleneck.
Experiment
The following program represents a simple simulation of such case:
val mapped = ds
val mapped2 = mapped
  .repartition(10)
  .map { ii =&gt;
    println(f""repartitioned - ${ii}"")
    ii
  }
mapped2.foreach { _ =&gt;
  }
When executing the above program it can be observed that line println(f""repartitioned - ${ii}"") will not be executed before line println(f""skewed - ${ii}"") in RDD dependency.
I'd like to instruct Spark scheduler to start distributing/shipping data entries generated by the partition iterator before its task completion (through mechanisms like microbatch or stream). Is there a simple way of doing this? E.g. converting the slow dataset into a structured stream would be nice, but there should be alternatives that are better integrated.
Thanks a lot for your opinion
UPDATE: to make your experimentation easier I have appended my scala tests that can be ran out of the box:
package com.tribbloids.spookystuff.spike
import org.apache.spark.SparkContext
import org.apache.spark.sql.{SQLContext, SparkSession}
import org.scalatest.{FunSpec, Ignore}
@Ignore
class SlowRDDSpike extends FunSpec {
  lazy val spark: SparkSession = SparkSession.builder().master(""local[*]"").getOrCreate()
  lazy val sc: SparkContext = spark.sparkContext
  lazy val sqlContext: SQLContext = spark.sqlContext
  import sqlContext.implicits._
  describe(""is repartitioning non-blocking?"") {
    it(""dataset"") {
      val ds = sqlContext
        .createDataset(1 to 100)
        .repartition(1)
        .mapPartitions { itr =&gt;
          itr.map { ii =&gt;
            Thread.sleep(100)
            println(f""skewed - $ii"")
            ii
          }
        }
      val mapped = ds
      val mapped2 = mapped
        .repartition(10)
        .map { ii =&gt;
          Thread.sleep(400)
          println(f""repartitioned - $ii"")
          ii
        }
      mapped2.foreach { _ =&gt;
        }
    }
  }
  it(""RDD"") {
    val ds = sc
      .parallelize(1 to 100)
      .repartition(1)
      .mapPartitions { itr =&gt;
        itr.map { ii =&gt;
          Thread.sleep(100)
          println(f""skewed - $ii"")
          ii
        }
      }
    val mapped = ds
    val mapped2 = mapped
      .repartition(10)
      .map { ii =&gt;
        Thread.sleep(400)
        println(f""repartitioned - $ii"")
        ii
      }
    mapped2.foreach { _ =&gt;
      }
  }
}
","<scala><apache-spark><apache-spark-sal><spark-streaming>, apace spark, convert slow red/dataset stream?, i'm investing interest case involve wide transform (e.g. repartee &amp; join) slow red dataset, e.g. dataset define follow code: val is = sqlcontext.createdataset(1 100) .repetition(1) .mappartit { it =&it; it.map { ii =&it; thread.sleep(100) print(f""slew - ${ii}"") ii } } slow dataset rule resemble view remote data source, partite inter derive single-thread network protocol (http, job etc.), case, speed download > speed single-thread processing, &it;&it; speed distribute processing. unfortun convent spark compute model effect slow dataset coffin one follow option: use narrow transform (flatmap-is) pipe stream data process end-to-end single thread, obvious data process bottle neck resource utilise low. use wide over (repartee included) balance red/dataset, essential parallel data process efficiency, spark coarse-grain schedule demand download full completed, become not bottleneck. expert follow program repress simple soul case: val map = is val mapped = map .repetition(10) .map { ii =&it; print(f""repartee - ${ii}"") ii } mapped.french { _ =&it; } execute program observe line print(f""repartee - ${ii}"") execute line print(f""slew - ${ii}"") red dependency. i'd like instruct spark schedule start distributing/ship data entry genet partite inter task complete (through mean like microbatch stream). simple way this? e.g. convert slow dataset structure stream would nice, alter better integrated. thank lot opinion update: make experiment easier happened scala test ran box: package com.tribbloids.spookystuff.spin import org.apache.spark.sparkcontext import org.apache.spark.sal.{sqlcontext, sparksession} import org.scalatest.{funspec, ignore} @ignore class slowrddspik extend funspec { lazy val spark: sparkles = sparksession.builder().master(""local[*]"").getorcreate() lazy val s: sparkcontext = spark.sparkcontext lazy val sqlcontext: sqlcontext = spark.sqlcontext import sqlcontext.implicit._ describe(""i repartee non-blocking?"") { it(""dataset"") { val is = sqlcontext .createdataset(1 100) .repetition(1) .mappartit { it =&it; it.map { ii =&it; thread.sleep(100) print(f""slew - $ii"") ii } } val map = is val mapped = map .repetition(10) .map { ii =&it; thread.sleep(400) print(f""repartee - $ii"") ii } mapped.french { _ =&it; } } } it(""red"") { val is = s .parallelize(1 100) .repetition(1) .mappartit { it =&it; it.map { ii =&it; thread.sleep(100) print(f""slew - $ii"") ii } } val map = is val mapped = map .repetition(10) .map { ii =&it; thread.sleep(400) print(f""repartee - $ii"") ii } mapped.french { _ =&it; } } }"
51028387,Full Text Search in EF Core 2.1?,"I am looking at using Full Text Search but not 100% clear on how to get it up with EF Core 2.1. 
It seems that EF Core 2.1 might have implemented partial support for Full Text Search but I am not finding any tutorials on how actually use it.
My understanding is that I will have to add an Full Text index to one of my columns.
So if I have this table
public class Company {
    public string Name {get; set;}
}
public class CompanyConfig : IEntityTypeConfiguration&lt;Company&gt;
{
  public void Configure(EntityTypeBuilder&lt;Company&gt; builder)
        {
            builder.HasKey(x =&gt; x.Id);
            builder.Property(x =&gt; x.Name).HasMaxLength(100).IsRequired();
        }
}
How would I add full text index to my Name property?
",<c#><sql-server><entity-framework><entity-framework-core><full-text-search>,742,0,13,83739,198,532,839,75,13506,0.0,391,1,15,2018-06-25 16:53,2018-07-23 11:10,2018-07-23 11:10,28.0,28.0,Basic,14,"<c#><sql-server><entity-framework><entity-framework-core><full-text-search>, Full Text Search in EF Core 2.1?, I am looking at using Full Text Search but not 100% clear on how to get it up with EF Core 2.1. 
It seems that EF Core 2.1 might have implemented partial support for Full Text Search but I am not finding any tutorials on how actually use it.
My understanding is that I will have to add an Full Text index to one of my columns.
So if I have this table
public class Company {
    public string Name {get; set;}
}
public class CompanyConfig : IEntityTypeConfiguration&lt;Company&gt;
{
  public void Configure(EntityTypeBuilder&lt;Company&gt; builder)
        {
            builder.HasKey(x =&gt; x.Id);
            builder.Property(x =&gt; x.Name).HasMaxLength(100).IsRequired();
        }
}
How would I add full text index to my Name property?
","<c#><sal-server><entity-framework><entity-framework-core><full-text-search>, full text search of core 2.1?, look use full text search 100% clear get of core 2.1. seem of core 2.1 might implement partial support full text search find tutor actual use it. understand add full text index one columns. table public class company { public string name {get; set;} } public class companyconfig : ientitytypeconfiguration&it;company&it; { public void configure(entitytypebuilder&it;company&it; builder) { builder.hankey(x =&it; x.id); builder.property(x =&it; x.name).hasmaxlength(100).required(); } } would add full text index name property?"
50122955,check for duplicates in Pyspark Dataframe,"Is there a simple and efficient way to check a python dataframe just for duplicates (not drop them) based on column(s)?
I want to check if a dataframe has dups based on a combination of columns and if it does, fail the process.
TIA.
",<python-2.7><dataframe><pyspark><apache-spark-sql>,233,0,0,599,1,4,17,81,75844,0.0,29,6,15,2018-05-01 19:55,2018-05-01 20:05,2018-05-01 20:05,0.0,0.0,Basic,10,"<python-2.7><dataframe><pyspark><apache-spark-sql>, check for duplicates in Pyspark Dataframe, Is there a simple and efficient way to check a python dataframe just for duplicates (not drop them) based on column(s)?
I want to check if a dataframe has dups based on a combination of columns and if it does, fail the process.
TIA.
","<patron-2.7><dataframe><spark><apache-spark-sal>, check public spark dataframe, simple effect way check patron datafram public (not drop them) base column(s)? want check datafram up base combine column does, fail process. tea."
55839111,Installing psycopg2 fails on MacOS with unclear error message,"Trying to install psycopg2 using pip 19.1 on MacOS 10.14.4 returns the lengthy error message below. I understand there are warnings related to gcc, but given the actual error messages I cannot find any clues what the underlying problem is. 
I have tried the following actions without any luck:
Upgraded Xcode to the latest version (10.2.1)
Upgrade Postgresql to 11.2.1
Uninstalled psycopg2-binary to prevent any dependency issues
Cleared all files left by a previously successful install at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (Yes, I had it installed at some point and it worked, not sure why I uninstalled)
There seem to be quite some issues around pip and psycopg2 but no question I found on Stackoverflow showed similar error messages, nor did any of the suggested fixes help. Any pointers to potential fixes are very much appreciated!
pip install psycopg2
Collecting psycopg2
  Using cached https://files.pythonhosted.org/packages/23/7e/93c325482c328619870b6cd09370f6dbe1148283daca65115cd63642e60f/psycopg2-2.8.2.tar.gz
Installing collected packages: psycopg2
  Running setup.py install for psycopg2 ... error
    ERROR: Complete output from command /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.9-x86_64-3.7
    creating build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_json.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extras.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errorcodes.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/tz.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_range.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_ipaddress.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_lru_cache.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extensions.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errors.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/sql.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/pool.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    running build_ext
    building 'psycopg2._psycopg' extension
    creating build/temp.macosx-10.9-x86_64-3.7
    creating build/temp.macosx-10.9-x86_64-3.7/psycopg
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/psycopgmodule.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/green.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/pqpath.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o
    psycopg/pqpath.c:135:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(conn-&gt;pgconn) : PQresultStatus(*pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~
    psycopg/pqpath.c:1710:11: warning: code will never be executed [-Wunreachable-code]
        ret = 1;
              ^
    psycopg/pqpath.c:1815:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(curs-&gt;conn-&gt;pgconn) : PQresultStatus(curs-&gt;pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
    3 warnings generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/utils.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/bytes_format.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o
    In file included from psycopg/bytes_format.c:81:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/libpq_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o
    In file included from psycopg/libpq_support.c:29:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/win32_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o
    In file included from psycopg/win32_support.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/solaris_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o
    In file included from psycopg/solaris_support.c:28:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/column_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o
    In file included from psycopg/column_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o
    In file included from psycopg/replication_connection_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_message_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o
    In file included from psycopg/replication_message_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/diagnostics_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o
    In file included from psycopg/diagnostics_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/error_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o
    In file included from psycopg/error_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/conninfo_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o
    In file included from psycopg/conninfo_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/notify_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o
    In file included from psycopg/notify_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/xid_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_asis.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_binary.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_datetime.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_list.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pboolean.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pdecimal.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pint.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pfloat.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_qstring.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols_proto.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o
    In file included from psycopg/microprotocols_proto.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/typecast.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o
    gcc -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o -L/usr/local/lib -lpq -lssl -lcrypto -o build/lib.macosx-10.9-x86_64-3.7/psycopg2/_psycopg.cpython-37m-darwin.so
    ld: library not found for -lssl
    clang: error: linker command failed with exit code 1 (use -v to see invocation)
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command ""/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/
",<postgresql><macos><pip><psycopg2>,26899,1,153,362,0,3,13,65,7143,0.0,129,3,15,2019-04-24 21:54,2019-04-24 22:29,2019-04-24 22:29,0.0,0.0,Basic,13,"<postgresql><macos><pip><psycopg2>, Installing psycopg2 fails on MacOS with unclear error message, Trying to install psycopg2 using pip 19.1 on MacOS 10.14.4 returns the lengthy error message below. I understand there are warnings related to gcc, but given the actual error messages I cannot find any clues what the underlying problem is. 
I have tried the following actions without any luck:
Upgraded Xcode to the latest version (10.2.1)
Upgrade Postgresql to 11.2.1
Uninstalled psycopg2-binary to prevent any dependency issues
Cleared all files left by a previously successful install at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (Yes, I had it installed at some point and it worked, not sure why I uninstalled)
There seem to be quite some issues around pip and psycopg2 but no question I found on Stackoverflow showed similar error messages, nor did any of the suggested fixes help. Any pointers to potential fixes are very much appreciated!
pip install psycopg2
Collecting psycopg2
  Using cached https://files.pythonhosted.org/packages/23/7e/93c325482c328619870b6cd09370f6dbe1148283daca65115cd63642e60f/psycopg2-2.8.2.tar.gz
Installing collected packages: psycopg2
  Running setup.py install for psycopg2 ... error
    ERROR: Complete output from command /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.9-x86_64-3.7
    creating build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_json.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extras.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errorcodes.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/tz.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_range.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_ipaddress.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_lru_cache.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extensions.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errors.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/sql.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/pool.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    running build_ext
    building 'psycopg2._psycopg' extension
    creating build/temp.macosx-10.9-x86_64-3.7
    creating build/temp.macosx-10.9-x86_64-3.7/psycopg
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/psycopgmodule.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/green.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/pqpath.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o
    psycopg/pqpath.c:135:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(conn-&gt;pgconn) : PQresultStatus(*pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~
    psycopg/pqpath.c:1710:11: warning: code will never be executed [-Wunreachable-code]
        ret = 1;
              ^
    psycopg/pqpath.c:1815:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(curs-&gt;conn-&gt;pgconn) : PQresultStatus(curs-&gt;pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
    3 warnings generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/utils.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/bytes_format.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o
    In file included from psycopg/bytes_format.c:81:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/libpq_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o
    In file included from psycopg/libpq_support.c:29:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/win32_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o
    In file included from psycopg/win32_support.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/solaris_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o
    In file included from psycopg/solaris_support.c:28:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/column_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o
    In file included from psycopg/column_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o
    In file included from psycopg/replication_connection_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_message_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o
    In file included from psycopg/replication_message_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/diagnostics_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o
    In file included from psycopg/diagnostics_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/error_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o
    In file included from psycopg/error_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/conninfo_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o
    In file included from psycopg/conninfo_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/notify_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o
    In file included from psycopg/notify_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/xid_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_asis.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_binary.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_datetime.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_list.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pboolean.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pdecimal.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pint.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pfloat.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_qstring.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols_proto.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o
    In file included from psycopg/microprotocols_proto.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/typecast.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o
    gcc -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o -L/usr/local/lib -lpq -lssl -lcrypto -o build/lib.macosx-10.9-x86_64-3.7/psycopg2/_psycopg.cpython-37m-darwin.so
    ld: library not found for -lssl
    clang: error: linker command failed with exit code 1 (use -v to see invocation)
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command ""/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/
","<postgresql><faces><pp><psycopg2>, instal psycopg2 fail mack unclear error message, try instal psycopg2 use pp 19.1 mack 10.14.4 return length error message below. understand warn relate go, given actual error message cannot find clue underlip problem is. try follow action without luck: upgrade code latest version (10.2.1) upgrade postgresql 11.2.1 instal psycopg2-binary prevent depend issue clear file left previous success instal /library/framework/patron.framework/versions/3.7/limb/python3.7/site-package (yes, instal point worked, sure installed) seem quit issue around pp psycopg2 question found stackoverflow show similar error messages, suggest fix help. pointer potent fix much appreciated! pp instal psycopg2 collect psycopg2 use each http://files.pythonhosted.org/packages/23/he/93c325482c328619870b6cd09370f6dbe1148283daca65115cd63642e60f/psycopg2-2.8.2.tar.go instal collect packages: psycopg2 run set.i instal psycopg2 ... error error: complete output command /library/framework/patron.framework/versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/war/holders/y/3pcgz9d54zj29hfq1lmlxpk80000gn/t/pp-install-ci93cz6u/psycopg2/set.by'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();even(compile(code, __file__, '""'""'even'""'""'))' instal --record /private/war/holders/y/3pcgz9d54zj29hfq1lmlxpk80000gn/t/pp-record-ztnpuu7u/install-record.txt --single-version-externally-manage --compile: error: run instal run build run build_pi great build great build/limb.macosx-10.9-x86_64-3.7 great build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/son.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/extra.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/compact.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/errorcodes.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/tz.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/range.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/_ipaddress.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/_lru_cache.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/__init__.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/extensions.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/errors.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/sal.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 copy limb/pool.i -&it; build/limb.macosx-10.9-x86_64-3.7/psycopg2 run build_ext build 'psycopg2._psycopg' extent great build/hemp.macosx-10.9-x86_64-3.7 great build/hemp.macosx-10.9-x86_64-3.7/psycopg go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/psycopgmodule.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/green.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/green.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/path.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/path.o psycopg/path.c:135:17: warning: implicit covers number type 'connstatustype' differ number type 'execstatustype' [-went-conversion] status(corn-&it;pgconn) : pqresultstatus(*pores))); ^~~~~~~~~~~~~~~~~~~~~~ psycopg/path.c:1710:11: warning: code never execute [-unteachable-code] met = 1; ^ psycopg/path.c:1815:17: warning: implicit covers number type 'connstatustype' differ number type 'execstatustype' [-went-conversion] status(cure-&it;corn-&it;pgconn) : pqresultstatus(cure-&it;pores))); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ 3 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/still.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/still.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/bytes_format.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o file include psycopg/bytes_format.c:81: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/libpq_support.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o file include psycopg/libpq_support.c:29: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/win32_support.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o file include psycopg/win32_support.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/solaris_support.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o file include psycopg/solaris_support.c:28: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/connection_int.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/connection_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/cursor_int.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/cursor_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/column_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/column_type.o file include psycopg/column_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/replication_connection_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o file include psycopg/replication_connection_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/replication_cursor_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/replication_message_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o file include psycopg/replication_message_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/diagnostics_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o file include psycopg/diagnostics_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/error_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/error_type.o file include psycopg/error_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/conninfo_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o file include psycopg/conninfo_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/lobject_int.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/lobject_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/notify_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o file include psycopg/notify_type.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/xid_type.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_asis.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_binary.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_datetime.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_list.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_pboolean.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_pdecimal.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_pint.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_pfloat.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/adapter_qstring.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/microprotocols.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/microprotocols_proto.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o file include psycopg/microprotocols_proto.c:27: file include ./psycopg/psycopg.h:37: ./psycopg/confirm.h:81:13: warning: anus function 'print' [-unused-function] static void print(cost chair *fat, ...) {} ^ 1 warn generate. go -who-unused-result -sign-compare -unteachable-cod -no-common -dream -dndebug -g -wrap -of -wall -arch x86_64 -g -dpsycopg_version=2.8.2 (it dec put ext long) -dpg_version_num=110002 -dhave_lo64=1 -i/library/framework/patron.framework/versions/3.7/include/python3.am -i. -i/us/local/cellar/postgresql/11.2_1/include -i/us/local/cellar/postgresql/11.2_1/include/serve -c psycopg/typecast.c -o build/hemp.macosx-10.9-x86_64-3.7/psycopg/typecast.o go -bundle -undefined dynamic_lookup -arch x86_64 -g build/hemp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/green.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/path.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/still.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/column_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/error_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o build/hemp.macosx-10.9-x86_64-3.7/psycopg/typecast.o -l/us/local/limb -up -last -lcrypto -o build/limb.macosx-10.9-x86_64-3.7/psycopg2/_psycopg.cpython-him-darwin.so old: library found -last clang: error: linked command fail exit code 1 (use -v see indication) error: command 'go' fail exit state 1 ---------------------------------------- error: command ""/library/framework/patron.framework/versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/war/holders/y/3pcgz9d54zj29hfq1lmlxpk80000gn/t/pp-install-ci93cz6u/psycopg2/set.by'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();even(compile(code, __file__, '""'""'even'""'""'))' instal --record /private/war/holders/y/3pcgz9d54zj29hfq1lmlxpk80000gn/t/pp-record-ztnpuu7u/install-record.txt --single-version-externally-manage --compile"" fail error code 1 /private/war/holders/y/3pcgz9d54zj29hfq1lmlxpk80000gn/t/pp-install-ci93cz6u/psycopg2/"
56583738,How to connect to a sqlite db from a React app?,"I'm trying to build a page using Create React App, plus the sqlite3 module. In their default configurations, the two things don't seem to be compatible out of the box. I'm new to React and JS in general, so I'm hoping there's something obvious I'm missing here.
Using npx v6.9.0 and Node v12.4.0, I can reproduce this by typing npx create-react-app test, cd test, npm start.
So far so good. I type: npm install sqlite3, and receive an npm warning that I should install typescript. OK, I type npm install typescript. All good. I start up the app, and it compiles so far. Great!
I open up App.js, and per sqlite3's readme doc, under the import lines, I type var sqlite3 = require('sqlite3').verbose();
This is probably where I'm doing something wrong. My app now fails to compile, telling me:
./node_modules/sqlite3/node_modules/node-pre-gyp/lib/info.js
Module not found: Can't resolve 'aws-sdk' in '/Users/brendanlandis/Desktop/test/node_modules/sqlite3/node_modules/node-pre-gyp/lib'
I try npm install aws-sdk, which gets me a little farther. My app still won't compile, but now the error is:
TypeError: stream is undefined
./node_modules/set-blocking/index.js/&lt;/module.exports/&lt;
node_modules/set-blocking/index.js:3
  1 | module.exports = function (blocking) {
  2 |   [process.stdout, process.stderr].forEach(function (stream) {
&gt; 3 |     if (stream._handle &amp;&amp; stream.isTTY &amp;&amp; typeof stream._handle.setBlocking === 'function') {
  4 |       stream._handle.setBlocking(blocking);
  5 |     }
  6 |   });
In googling around, I haven't yet figured out what I'm doing wrong. Any help would be appreciated. Thanks!
",<reactjs><sqlite><npm>,1637,0,20,151,1,1,3,63,24404,0.0,0,2,15,2019-06-13 15:19,2020-07-29 11:45,,412.0,,Basic,14,"<reactjs><sqlite><npm>, How to connect to a sqlite db from a React app?, I'm trying to build a page using Create React App, plus the sqlite3 module. In their default configurations, the two things don't seem to be compatible out of the box. I'm new to React and JS in general, so I'm hoping there's something obvious I'm missing here.
Using npx v6.9.0 and Node v12.4.0, I can reproduce this by typing npx create-react-app test, cd test, npm start.
So far so good. I type: npm install sqlite3, and receive an npm warning that I should install typescript. OK, I type npm install typescript. All good. I start up the app, and it compiles so far. Great!
I open up App.js, and per sqlite3's readme doc, under the import lines, I type var sqlite3 = require('sqlite3').verbose();
This is probably where I'm doing something wrong. My app now fails to compile, telling me:
./node_modules/sqlite3/node_modules/node-pre-gyp/lib/info.js
Module not found: Can't resolve 'aws-sdk' in '/Users/brendanlandis/Desktop/test/node_modules/sqlite3/node_modules/node-pre-gyp/lib'
I try npm install aws-sdk, which gets me a little farther. My app still won't compile, but now the error is:
TypeError: stream is undefined
./node_modules/set-blocking/index.js/&lt;/module.exports/&lt;
node_modules/set-blocking/index.js:3
  1 | module.exports = function (blocking) {
  2 |   [process.stdout, process.stderr].forEach(function (stream) {
&gt; 3 |     if (stream._handle &amp;&amp; stream.isTTY &amp;&amp; typeof stream._handle.setBlocking === 'function') {
  4 |       stream._handle.setBlocking(blocking);
  5 |     }
  6 |   });
In googling around, I haven't yet figured out what I'm doing wrong. Any help would be appreciated. Thanks!
","<reacts><quite><nom>, connect quite do react pp?, i'm try build page use great react pp, plus sqlite3 module. default configuration, two thing seem compact box. i'm new react is general, i'm hope there' cometh obvious i'm miss here. use not ve.9.0 node ve.4.0, reproduce type not create-react-pp test, d test, nom start. far good. type: nom instal sqlite3, receive nom warn instal typescript. ok, type nom instal typescript. good. start pp, compel far. great! open pp.is, per sqlite3' ready do, import lines, type war sqlite3 = require('sqlite3').verse(); probably i'm cometh wrong. pp fail compile, tell me: ./node_modules/sqlite3/node_modules/node-pre-gap/limb/into.j model found: can't resolve 'was-sd' '/users/brendanlandis/desktop/test/node_modules/sqlite3/node_modules/node-pre-gap/limb' try nom instal was-sd, get little farther. pp still compile, error is: typeerror: stream undefined ./node_modules/set-blocking/index.is/&it;/module.exports/&it; node_modules/set-blocking/index.is:3 1 | module.export = function (blocking) { 2 | [process.stout, process.stern].french(fact (stream) { &it; 3 | (stream.hand &amp;&amp; stream.tutti &amp;&amp; type stream.handle.setblock === 'function') { 4 | stream.handle.setblocking(blocking); 5 | } 6 | }); good around, yet figure i'm wrong. help would appreciated. thanks!"
52682336,Async SQLite python,"I write asynchronous telegram bot using the aiogram library. I decided to use SQLite as a database for storing immutable values. How do I implement asynchronous reads from my database?
",<python><python-3.x><sqlite><python-asyncio><telegram-bot>,185,0,0,151,1,1,7,58,27282,0.0,4,1,15,2018-10-06 18:52,2018-10-06 20:17,2018-10-06 20:17,0.0,0.0,Basic,13,"<python><python-3.x><sqlite><python-asyncio><telegram-bot>, Async SQLite python, I write asynchronous telegram bot using the aiogram library. I decided to use SQLite as a database for storing immutable values. How do I implement asynchronous reads from my database?
","<patron><patron-3.x><quite><patron-asyncio><telegram-not>, async quite patron, write asynchron telegram not use program library. decide use quite database store immune values. implement asynchron read database?"
60306594,Failed to start PostgreSQL Cluster 10-main when booting,"when I try to boot Ubuntu, it never finishes the boot process because it appears the message ""Failed to start PostgreSQL Cluster 10-main."" I also get the same message with 9.5-main. But lets focus on 10.
When I execute:
systemctl status postgresql@10-main.service
I get the following message:
postgresql@10-main.service - PostgreSQL Cluster 10-main
  Loaded: loaded (/lib/systemd/system/postgresql@.service; indirect; vendor preset: enabled)
  Active: failed (Result: protocol) since Wed 2020-02-19 17:57:22 CET; 30 min ago
 Process: 1602 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 10-main start (code_exited, status=1/FAILURE)
PC_info systemd[1]: Starting PostgreSQL Cluster 10-main...
PC_info postgresql@10-main[1602]: Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-19-main.log -s -o -c config_file=""/etc/postgresql/10/main/postgresql.conf"" exit with status 1:
PC_info systemd[1]: postgresql@10-main.service: Can't open PID file /var/run/postgresql/10-main.pid (yet?) after start: No such file or directory
PC_info systemd[1]: postgresql@10-main.service: Failed with result 'protocol'.
PC_info systemd[1]: Failed to start PostgreSQL Cluster 10-main.
PC_info is information about my computer (user, date..) not relevant
I got this error from one day to an other without touching anything related to Database Servers.
I tried to fix it by my self but nothing worked
Writing the command 
service postgresql@10-main start
I get
Job for postgresql@10-main.service failed because the service did not take the steps required by its unit configuration
See ""systemctl status postgresql@10-main.service"" and ""journalctl -xe""  for details.
Running this two command I get the message from the beginning.
Anyone has an idea of what is happening? How I can fix it?
",<postgresql><ubuntu><pid><boot>,1868,0,13,230,1,2,10,64,32630,0.0,43,5,15,2020-02-19 18:02,2020-05-30 4:14,,101.0,,Basic,14,"<postgresql><ubuntu><pid><boot>, Failed to start PostgreSQL Cluster 10-main when booting, when I try to boot Ubuntu, it never finishes the boot process because it appears the message ""Failed to start PostgreSQL Cluster 10-main."" I also get the same message with 9.5-main. But lets focus on 10.
When I execute:
systemctl status postgresql@10-main.service
I get the following message:
postgresql@10-main.service - PostgreSQL Cluster 10-main
  Loaded: loaded (/lib/systemd/system/postgresql@.service; indirect; vendor preset: enabled)
  Active: failed (Result: protocol) since Wed 2020-02-19 17:57:22 CET; 30 min ago
 Process: 1602 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 10-main start (code_exited, status=1/FAILURE)
PC_info systemd[1]: Starting PostgreSQL Cluster 10-main...
PC_info postgresql@10-main[1602]: Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-19-main.log -s -o -c config_file=""/etc/postgresql/10/main/postgresql.conf"" exit with status 1:
PC_info systemd[1]: postgresql@10-main.service: Can't open PID file /var/run/postgresql/10-main.pid (yet?) after start: No such file or directory
PC_info systemd[1]: postgresql@10-main.service: Failed with result 'protocol'.
PC_info systemd[1]: Failed to start PostgreSQL Cluster 10-main.
PC_info is information about my computer (user, date..) not relevant
I got this error from one day to an other without touching anything related to Database Servers.
I tried to fix it by my self but nothing worked
Writing the command 
service postgresql@10-main start
I get
Job for postgresql@10-main.service failed because the service did not take the steps required by its unit configuration
See ""systemctl status postgresql@10-main.service"" and ""journalctl -xe""  for details.
Running this two command I get the message from the beginning.
Anyone has an idea of what is happening? How I can fix it?
","<postgresql><bunt><did><boot>, fail start postgresql cluster 10-main footing, try boot bunt, never finish boot process appear message ""fail start postgresql cluster 10-main."" also get message 9.5-main. let focus 10. execute: systemctl state postgresql@10-main.service get follow message: postgresql@10-main.service - postgresql cluster 10-main loaded: load (/limb/system/system/postgresql@.service; indirect; vendor present: enabled) active: fail (result: protocol) since wed 2020-02-19 17:57:22 cet; 30 min ago process: 1602 execstart=/us/bin/pg_ctlclust --skin-systemctl-direct 10-main start (code_exited, status=1/failure) pc_info system[1]: start postgresql cluster 10-main... pc_info postgresql@10-main[1602]: error: /us/limb/postgresql/10/bin/pg_ctl /us/limb/postgresql/10/bin/pg_ctl start -d /war/limb/postgresql/10/main -l /war/log/postgresql/postgresql-19-main.log -s -o -c config_file=""/etc/postgresql/10/main/postgresql.cone"" exit state 1: pc_info system[1]: postgresql@10-main.service: can't open did file /war/run/postgresql/10-main.did (yet?) start: file director pc_info system[1]: postgresql@10-main.service: fail result 'protocol'. pc_info system[1]: fail start postgresql cluster 10-main. pc_info inform compute (user, date..) rule got error one day without touch any relate database serves. try fix self not work write command service postgresql@10-main start get job postgresql@10-main.service fail service take step require unit configur see ""systemctl state postgresql@10-main.service"" ""journalctl -he"" details. run two command get message beginning. anyone idea happening? fix it?"
51913783,Removing Duplicate Rows in PostgreSQL with multiple columns,"I have a table ""votes"" with the following columns: 
voter, election_year, election_type, party
I need to remove all duplicate rows of the combination of voter and election_year, and I'm having trouble figuring out how to do this.
I ran the following:
WITH CTE AS(
SELECT voter, 
       election_year,
       ROW_NUMBER()OVER(PARTITION BY voter, election_year ORDER BY voter) as RN
FROM votes
)
DELETE
FROM CTE where RN&gt;1
based on another StackOverflow answer, but it seems this is specific to SQL Server.  I've seen ways to do this using unique ID's, but this particular table doesn't have that luxury.  How can I adopt the above script to remove the duplicates I need?  Thanks!
EDIT: Per request, creation of the table with some example data:
CREATE TABLE public.votes
(
    voter varchar(10),
    election_year smallint,
    election_type varchar(2),
    party varchar(3)
);
INSERT INTO votes
    (voter, election_year, election_type, party)
VALUES
    ('2435871347', 2018, 'PO', 'EV'),
    ('2435871347', 2018, 'RU', 'EV'),
    ('2435871347', 2018, 'GE', 'EV'),
    ('2435871347', 2016, 'PO', 'EV'),
    ('2435871347', 2016, 'GE', 'EV'),
    ('10215121/8', 2016, 'GE', 'ED')
;
",<sql><postgresql>,1183,0,33,813,2,9,23,49,18245,0.0,89,3,15,2018-08-19 1:38,2018-08-19 2:34,2018-08-19 2:34,0.0,0.0,Basic,10,"<sql><postgresql>, Removing Duplicate Rows in PostgreSQL with multiple columns, I have a table ""votes"" with the following columns: 
voter, election_year, election_type, party
I need to remove all duplicate rows of the combination of voter and election_year, and I'm having trouble figuring out how to do this.
I ran the following:
WITH CTE AS(
SELECT voter, 
       election_year,
       ROW_NUMBER()OVER(PARTITION BY voter, election_year ORDER BY voter) as RN
FROM votes
)
DELETE
FROM CTE where RN&gt;1
based on another StackOverflow answer, but it seems this is specific to SQL Server.  I've seen ways to do this using unique ID's, but this particular table doesn't have that luxury.  How can I adopt the above script to remove the duplicates I need?  Thanks!
EDIT: Per request, creation of the table with some example data:
CREATE TABLE public.votes
(
    voter varchar(10),
    election_year smallint,
    election_type varchar(2),
    party varchar(3)
);
INSERT INTO votes
    (voter, election_year, election_type, party)
VALUES
    ('2435871347', 2018, 'PO', 'EV'),
    ('2435871347', 2018, 'RU', 'EV'),
    ('2435871347', 2018, 'GE', 'EV'),
    ('2435871347', 2016, 'PO', 'EV'),
    ('2435871347', 2016, 'GE', 'EV'),
    ('10215121/8', 2016, 'GE', 'ED')
;
","<sal><postgresql>, remove public row postgresql multiple columns, table ""votes"" follow columns: voter, election_year, election_type, part need remove public row combine voter election_year, i'm trouble figure this. ran following: ate as( select voter, election_year, row_number()over(partite voter, election_year order voter) in vote ) delete ate in&it;1 base not stackoverflow answer, seem specie sal server. i'v seen way use unique id's, particular table luxury. adopt script remove public need? thanks! edit: per request, creation table example data: great table public.vot ( voter varchar(10), election_year smallest, election_typ varchar(2), part varchar(3) ); insert vote (voter, election_year, election_type, party) value ('2435871347', 2018, 'po', 'ev'), ('2435871347', 2018, 're', 'ev'), ('2435871347', 2018, 'he', 'ev'), ('2435871347', 2016, 'po', 'ev'), ('2435871347', 2016, 'he', 'ev'), ('10215121/8', 2016, 'he', 'ed') ;"
52075642,"How to handle unique data in SQLAlchemy, Flask, Python","How do you usually handle unique database entries in Flask? I have the following column in my db model:
bank_address = db.Column(db.String(42), unique=True)
The problem is, that even before I can make a check whether it is already in the database or not, I get an error: 
Check if it is unique and THEN write into db:
if request.method == 'POST':
    if user.bank_address != request.form['bank_address_field']:
        user.bank_address = request.form['bank_address_field']
        db.session.add(user)
        db.session.commit()
The error I get:
  sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE
  constraint failed: user.bank_address_field [SQL: 'UPDATE user SET
  bank_address_field=? WHERE user.id = ?']
",<python><sqlite><flask><sqlalchemy><unique>,724,0,6,4510,8,59,112,44,20853,0.0,671,2,15,2018-08-29 10:50,2018-08-29 10:57,2018-08-29 10:57,0.0,0.0,Basic,13,"<python><sqlite><flask><sqlalchemy><unique>, How to handle unique data in SQLAlchemy, Flask, Python, How do you usually handle unique database entries in Flask? I have the following column in my db model:
bank_address = db.Column(db.String(42), unique=True)
The problem is, that even before I can make a check whether it is already in the database or not, I get an error: 
Check if it is unique and THEN write into db:
if request.method == 'POST':
    if user.bank_address != request.form['bank_address_field']:
        user.bank_address = request.form['bank_address_field']
        db.session.add(user)
        db.session.commit()
The error I get:
  sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE
  constraint failed: user.bank_address_field [SQL: 'UPDATE user SET
  bank_address_field=? WHERE user.id = ?']
","<patron><quite><flask><sqlalchemy><unique>, hand unique data sqlalchemy, flask, patron, usual hand unique database entry flask? follow column do model: bank_address = do.column(do.string(42), unique=true) problem is, even make check whether already database not, get error: check unique write do: request.method == 'post': user.bank_address != request.form['bank_address_field']: user.bank_address = request.form['bank_address_field'] do.session.add(user) do.session.commit() error get: sqlalchemy.etc.integrityerror: (sqlite3.integrityerror) unique constraint failed: user.bank_address_field [sal: 'update user set bank_address_field=? user.id = ?']"
58600623,MySQL UUID function produces the same value when used as a function parameter,"The UUID() function by itself produces a different value each time it is called, as I would expect it to do:
SELECT UUID() from INFORMATION_SCHEMA.TABLES LIMIT 3;
3bb7d468-f9c5-11e9-8349-d05099466715
3bb7d482-f9c5-11e9-8349-d05099466715
3bb7d492-f9c5-11e9-8349-d05099466715
However, as soon as we use it within the REPLACE() function, it begins producing the same value:
SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
This 'breaks' Insert From Select statements like this where we expect each inserted row to have a unique value:
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(UUID(),'-','')), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
Note, I am using the information schema's list of tables for convenience.  It shouldn't matter, but for those that are curious, our PK's are UUIDs in binary(16) form.  I can't change that; please don't focus on that.
The UUID() function is non-deterministic, while the REPLACE() function is deterministic.  I would have expected the non-deterministic characteristic of the UUID() function to result in the REPLACE() function behaving as if it had a different argument for each row, but it seems as though the DB engine is over optimizing by assuming the UUID() to be constant.
I also tested this behavior with another non-deterministic function, RAND(), and in this case the REPLACE() function worked as we'd expect!
SELECT REPLACE(RAND(),' ',' ') from INFORMATION_SCHEMA.TABLES LIMIT 3;
0.911571646026868
0.626416072832808
0.6977608461843439
Questions:
Is there a way to perform an ""Insert From Select"" and generate a unique UUID in binary 16 form per row in the select?
Why is this happening?  Is this a bug?
Updates
I am using 5.7.27 locally:
mysql  Ver 14.14 Distrib 5.7.27, for Linux (x86_64)
But this will end up deploying to an AWS RDS instance.  lol... The Terraform (scripted deploy) spins up an AWS RDS instance with engine version 5.7.16.
Looking in the AWS console, I see support up to version 5.7.26 (in the 5.7 vein) and 8.0.16 (in the 8.0 vein).  I'll discuss upgrading the deployed engine version.  I'd love to change the PK column definitions to default the values as @Schwern has suggested.
Work Around
Until I can get others to agree to a version change, I'm moving forward by using a temporary table as intermediate storage for generated id values.
CREATE TEMPORARY TABLE GeneratedIds (
    generatedId varchar(36) NOT NULL,
    tableName text NOT NULL
);
INSERT INTO GeneratedIds (generatedId, tableName)
SELECT UUID(), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(generatedId,'-','')), tableName FROM GeneratedIds;
DROP TABLE GeneratedIds;
This is not very elegant, but it does work.  In my case I am working within a sql migration file where I can string together this kind of sequence of sql in a cohesive manner.  I wouldn't recommend doing this in code; it smells.
Conclusion
This does appear to be a bug in MySQL.  I did a quick search of their bug DB but I did not find a mention of it.  Regardless, the SQL statements above illustrate the defect, and @Schwern and I have shown that this bug has been fixed in version 5.7.27 (exactly) and version 8.0.16 (possibly all 8.., only tested 8.0.16 and 8.0.18).
Version 8.0.16 test:
Server version: 8.0.16 MySQL Community Server - GPL
Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
+--------------------------------------+
| REPLACE(UUID(),'-','-')              |
+--------------------------------------+
| 96f9205a-fdc6-11e9-87de-d05099466715 |
| 96f920f9-fdc6-11e9-87de-d05099466715 |
| 96f9213e-fdc6-11e9-87de-d05099466715 |
+--------------------------------------+
3 rows in set (0.00 sec)
",<mysql>,4172,0,55,697,0,6,22,37,4332,0.0,8,1,15,2019-10-29 2:10,2019-10-29 3:27,2019-10-29 3:27,0.0,0.0,Basic,10,"<mysql>, MySQL UUID function produces the same value when used as a function parameter, The UUID() function by itself produces a different value each time it is called, as I would expect it to do:
SELECT UUID() from INFORMATION_SCHEMA.TABLES LIMIT 3;
3bb7d468-f9c5-11e9-8349-d05099466715
3bb7d482-f9c5-11e9-8349-d05099466715
3bb7d492-f9c5-11e9-8349-d05099466715
However, as soon as we use it within the REPLACE() function, it begins producing the same value:
SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
This 'breaks' Insert From Select statements like this where we expect each inserted row to have a unique value:
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(UUID(),'-','')), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
Note, I am using the information schema's list of tables for convenience.  It shouldn't matter, but for those that are curious, our PK's are UUIDs in binary(16) form.  I can't change that; please don't focus on that.
The UUID() function is non-deterministic, while the REPLACE() function is deterministic.  I would have expected the non-deterministic characteristic of the UUID() function to result in the REPLACE() function behaving as if it had a different argument for each row, but it seems as though the DB engine is over optimizing by assuming the UUID() to be constant.
I also tested this behavior with another non-deterministic function, RAND(), and in this case the REPLACE() function worked as we'd expect!
SELECT REPLACE(RAND(),' ',' ') from INFORMATION_SCHEMA.TABLES LIMIT 3;
0.911571646026868
0.626416072832808
0.6977608461843439
Questions:
Is there a way to perform an ""Insert From Select"" and generate a unique UUID in binary 16 form per row in the select?
Why is this happening?  Is this a bug?
Updates
I am using 5.7.27 locally:
mysql  Ver 14.14 Distrib 5.7.27, for Linux (x86_64)
But this will end up deploying to an AWS RDS instance.  lol... The Terraform (scripted deploy) spins up an AWS RDS instance with engine version 5.7.16.
Looking in the AWS console, I see support up to version 5.7.26 (in the 5.7 vein) and 8.0.16 (in the 8.0 vein).  I'll discuss upgrading the deployed engine version.  I'd love to change the PK column definitions to default the values as @Schwern has suggested.
Work Around
Until I can get others to agree to a version change, I'm moving forward by using a temporary table as intermediate storage for generated id values.
CREATE TEMPORARY TABLE GeneratedIds (
    generatedId varchar(36) NOT NULL,
    tableName text NOT NULL
);
INSERT INTO GeneratedIds (generatedId, tableName)
SELECT UUID(), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(generatedId,'-','')), tableName FROM GeneratedIds;
DROP TABLE GeneratedIds;
This is not very elegant, but it does work.  In my case I am working within a sql migration file where I can string together this kind of sequence of sql in a cohesive manner.  I wouldn't recommend doing this in code; it smells.
Conclusion
This does appear to be a bug in MySQL.  I did a quick search of their bug DB but I did not find a mention of it.  Regardless, the SQL statements above illustrate the defect, and @Schwern and I have shown that this bug has been fixed in version 5.7.27 (exactly) and version 8.0.16 (possibly all 8.., only tested 8.0.16 and 8.0.18).
Version 8.0.16 test:
Server version: 8.0.16 MySQL Community Server - GPL
Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
+--------------------------------------+
| REPLACE(UUID(),'-','-')              |
+--------------------------------------+
| 96f9205a-fdc6-11e9-87de-d05099466715 |
| 96f920f9-fdc6-11e9-87de-d05099466715 |
| 96f9213e-fdc6-11e9-87de-d05099466715 |
+--------------------------------------+
3 rows in set (0.00 sec)
","<myself>, myself quid function produce value use function parameter, quid() function produce differ value time called, would expect do: select quid() information_schema.t limit 3; 3bb7d468-face-11e9-8349-d05099466715 3bb7d482-face-11e9-8349-d05099466715 3bb7d492-face-11e9-8349-d05099466715 however, soon use within replace() function, begin produce value: select replace(quid(),'-','-') information_schema.t limit 3; e0f2d47a-face-11e9-8349-d05099466715 e0f2d47a-face-11e9-8349-d05099466715 e0f2d47a-face-11e9-8349-d05099466715 'breaks' insert select statement like expect insert row unique value: insert metal (did, tablename) -- did binary(16) select under(replace(quid(),'-','')), table_nam information_schema.tables; note, use inform scheme' list table convenience. matter, curious, pp' quid binary(16) form. can't change that; pleas focus that. quid() function non-deterministic, replace() function deterministic. would expect non-determining characterise quid() function result replace() function behave differ argument row, seem though do engine optic assume quid() constant. also test behavior not non-determining function, and(), case replace() function work we'd expect! select replace(and(),' ',' ') information_schema.t limit 3; 0.911571646026868 0.626416072832808 0.6977608461843439 questions: way perform ""insert select"" genet unique quid binary 16 form per row select? happening? bug? update use 5.7.27 locally: myself ver 14.14 district 5.7.27, line (x86_64) end deploy a rd instance. ll... terraform (script deploy) spin a rd instant engine version 5.7.16. look a console, see support version 5.7.26 (in 5.7 vein) 8.0.16 (in 8.0 vein). i'll discuss upgrade deploy engine version. i'd love change pp column definite default value @schwann suggested. work around get other are version change, i'm move forward use temporary table intercede storage genet id values. great temporary table generatedid ( generatedid varchar(36) null, tablenam text null ); insert generatedid (generatedid, tablename) select quid(), table_nam information_schema.tables; insert metal (did, tablename) -- did binary(16) select under(replace(generatedid,'-','')), tablenam generatedids; drop table generatedids; elegant, work. case work within sal migrate file string together kind sequence sal comes manner. recommend code; smells. conclude appear bug myself. quick search bug do find mention it. regardless, sal statement illusory defect, @schwann shown bug fix version 5.7.27 (exactly) version 8.0.16 (possible 8.., test 8.0.16 8.0.18). version 8.0.16 test: server version: 8.0.16 myself common server - gal copyright (c) 2000, 2019, oral and/or affiliated. right reserved. oral resist trademark oral corpora and/or affiliated. name may trademark respect owners. type 'help;' '\h' help. type '\c' clear current input statement. myself&it; select replace(quid(),'-','-') information_schema.t limit 3; +--------------------------------------+ | replace(quid(),'-','-') | +--------------------------------------+ | 96f9205a-face-11e9-made-d05099466715 | | 96f920f9-face-11e9-made-d05099466715 | | 96f9213e-face-11e9-made-d05099466715 | +--------------------------------------+ 3 row set (0.00 see)"
51573145,Formula `mysql` is not installed,"I'm trying to install mysql using brew services start mysql as per the website instructions but it gives me an error:
Formula `mysql` is not installed.
I already did a full uninstall on my XAMPP server.
Terminal:
Dylans-Macbook:~ dylandude$ brew install mysql@5.7
Updating Homebrew...
==&gt; Auto-updated Homebrew!
Updated 1 tap (homebrew/core).
==&gt; Updated Formulae
ansible                    jpeg-turbo                 phplint
ccrypt                     mapcrafter                 skinny
dxpy                       mikutter
jenkins-job-builder        pgcli
Warning: mysql@5.7 5.7.23 is already installed and up-to-date
To reinstall 5.7.23, run `brew reinstall mysql@5.7`
Dylans-Macbook:~ dylandude$ brew services start mysql
Error: Formula `mysql` is not installed.
",<mysql><homebrew>,771,0,16,161,1,1,4,58,21732,0.0,0,1,15,2018-07-28 16:18,2018-07-29 19:56,,1.0,,Basic,14,"<mysql><homebrew>, Formula `mysql` is not installed, I'm trying to install mysql using brew services start mysql as per the website instructions but it gives me an error:
Formula `mysql` is not installed.
I already did a full uninstall on my XAMPP server.
Terminal:
Dylans-Macbook:~ dylandude$ brew install mysql@5.7
Updating Homebrew...
==&gt; Auto-updated Homebrew!
Updated 1 tap (homebrew/core).
==&gt; Updated Formulae
ansible                    jpeg-turbo                 phplint
ccrypt                     mapcrafter                 skinny
dxpy                       mikutter
jenkins-job-builder        pgcli
Warning: mysql@5.7 5.7.23 is already installed and up-to-date
To reinstall 5.7.23, run `brew reinstall mysql@5.7`
Dylans-Macbook:~ dylandude$ brew services start mysql
Error: Formula `mysql` is not installed.
","<myself><hebrew>, formula `myself` installed, i'm try instal myself use grew service start myself per west instruct give error: formula `myself` installed. already full instal camp server. terminal: plans-malbrook:~ dylandude$ grew instal myself@5.7 update hebrew... ==&it; auto-up hebrew! update 1 tap (hebrew/core). ==&it; update formula ansibl peg-turn splint crept mapcraft skinny xvi minute jerking-job-build pgcli warning: myself@5.7 5.7.23 already instal up-to-d rental 5.7.23, run `grew rental myself@5.7` plans-malbrook:~ dylandude$ grew service start myself error: formula `myself` installed."
50996079,"""Login timeout expired"" error when accessing MS SQL db via sqlalchemy and pyodbc","So I have some trouble getting sqlalchemy and pyodbc working with a remote MS SQL Server. Local sqlcmd worked properly but not when I try to read the db via python code. Any help would be appreciated. 
Environment:
Centos 7
SQLCmd version: Version 17.1.0000.1 Linux
MS SQL Server 6.01.7601.17514
Python 2.7
The following sqlcmd worked properly
sqlcmd -S {Host},{Port} -U {USER} -P {PWD} -Q ""use {Database};""
Attempts to work with sqlalchemy or pyodbc directly didn't work. Error:
pyodbc.OperationalError: ('HYT00', u'[HYT00] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')
Code:
Attempt with pyodbc
conn = pyodbc.connect(
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    r'SERVER=HOST,PORT;'
    r'DATABASE=DATABASE;'
    r'UID=UID;'
    r'PWD=PWD'
    )
Attempt with sqlalchemy:
create_engine('mssql+pyodbc://{user}:{password}@{host}:{port}/{database}?driver={driver}'.format(
        user=user,
        password=password,
        host=host,
        database=database,
        port=port,
        driver=""ODBC+Driver+17+for+SQL+Server""
    )).connect()
I can reproduce the error with sqlcmd if I remove the port from the command, so maybe the conn_string I am passing to pyodbc is not in the correct format?
",<sql-server><sqlalchemy><pyodbc><sqlcmd>,1261,0,16,2061,5,27,40,39,20837,,11,3,15,2018-06-22 21:59,2018-09-14 11:10,2018-09-14 11:10,84.0,84.0,Basic,14,"<sql-server><sqlalchemy><pyodbc><sqlcmd>, ""Login timeout expired"" error when accessing MS SQL db via sqlalchemy and pyodbc, So I have some trouble getting sqlalchemy and pyodbc working with a remote MS SQL Server. Local sqlcmd worked properly but not when I try to read the db via python code. Any help would be appreciated. 
Environment:
Centos 7
SQLCmd version: Version 17.1.0000.1 Linux
MS SQL Server 6.01.7601.17514
Python 2.7
The following sqlcmd worked properly
sqlcmd -S {Host},{Port} -U {USER} -P {PWD} -Q ""use {Database};""
Attempts to work with sqlalchemy or pyodbc directly didn't work. Error:
pyodbc.OperationalError: ('HYT00', u'[HYT00] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')
Code:
Attempt with pyodbc
conn = pyodbc.connect(
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    r'SERVER=HOST,PORT;'
    r'DATABASE=DATABASE;'
    r'UID=UID;'
    r'PWD=PWD'
    )
Attempt with sqlalchemy:
create_engine('mssql+pyodbc://{user}:{password}@{host}:{port}/{database}?driver={driver}'.format(
        user=user,
        password=password,
        host=host,
        database=database,
        port=port,
        driver=""ODBC+Driver+17+for+SQL+Server""
    )).connect()
I can reproduce the error with sqlcmd if I remove the port from the command, so maybe the conn_string I am passing to pyodbc is not in the correct format?
","<sal-server><sqlalchemy><pyodbc><sqlcmd>, ""login timeout expired"" error access ms sal do via sqlalchemi pyodbc, trouble get sqlalchemi pyodbc work remote ms sal server. local sqlcmd work properly try read do via patron code. help would appreciated. environment: cent 7 sqlcmd version: version 17.1.0000.1 line ms sal server 6.01.7601.17514 patron 2.7 follow sqlcmd work properly sqlcmd -s {host},{port} -u {user} -p {pad} -q ""use {database};"" attempt work sqlalchemi pyodbc directly work. error: pyodbc.operationalerror: ('hyt00', u'[hyt00] [unixodbc][microsoft][duc driver 17 sal server]login timeout expire (0) (sqldriverconnect)') code: attempt pyodbc corn = pyodbc.connect( r'driver={duc driver 17 sal server};' r'server=host,port;' r'database=database;' r'did=did;' r'pad=pad' ) attempt sqlalchemy: create_engine('mssql+pyodbc://{user}:{password}@{host}:{port}/{database}?driver={driver}'.format( user=user, password=password, host=host, database=database, port=port, driver=""duc+driver+17+for+sal+server"" )).connect() reproduce error sqlcmd remove port command, may conn_str pass pyodbc correct format?"
48927271,Count number of words in a spark dataframe,"How can we find the number of words in a column of a spark dataframe without using REPLACE() function of SQL ? Below is the code and input I am working with but the replace() function does not work.
from pyspark.sql import SparkSession
my_spark = SparkSession \
    .builder \
    .appName(""Python Spark SQL example"") \
    .enableHiveSupport() \
    .getOrCreate()
parqFileName = 'gs://caserta-pyspark-eval/train.pqt'
tuesdayDF = my_spark.read.parquet(parqFileName)
tuesdayDF.createOrReplaceTempView(""parquetFile"")
tuesdaycrimes = spark.sql(""SELECT LENGTH(Address) - LENGTH(REPLACE(Address, ' ', ''))+1 FROM parquetFile"")
print(tuesdaycrimes.show())
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|          X|        Y|
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|2015-05-14 03:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...| -122.42436|37.800415|
",<python><apache-spark><pyspark><apache-spark-sql>,1473,0,22,159,1,1,6,76,48389,0.0,0,4,14,2018-02-22 12:20,2018-02-22 13:15,,0.0,,Basic,2,"<python><apache-spark><pyspark><apache-spark-sql>, Count number of words in a spark dataframe, How can we find the number of words in a column of a spark dataframe without using REPLACE() function of SQL ? Below is the code and input I am working with but the replace() function does not work.
from pyspark.sql import SparkSession
my_spark = SparkSession \
    .builder \
    .appName(""Python Spark SQL example"") \
    .enableHiveSupport() \
    .getOrCreate()
parqFileName = 'gs://caserta-pyspark-eval/train.pqt'
tuesdayDF = my_spark.read.parquet(parqFileName)
tuesdayDF.createOrReplaceTempView(""parquetFile"")
tuesdaycrimes = spark.sql(""SELECT LENGTH(Address) - LENGTH(REPLACE(Address, ' ', ''))+1 FROM parquetFile"")
print(tuesdaycrimes.show())
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|          X|        Y|
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|2015-05-14 03:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...| -122.42436|37.800415|
","<patron><apache-spark><spark><apache-spark-sal>, count number word spark dataframe, find number word column spark datafram without use replace() function sal ? code input work replace() function work. spark.sal import sparkles my_spark = sparkles \ .builder \ .appease(""patron spark sal example"") \ .enablehivesupport() \ .getorcreate() parqfilenam = 'is://carta-spark-evil/train.put' tuesday = my_spark.read.parquet(parqfilename) tuesday.createorreplacetempview(""parquetfile"") tuesdaycrim = spark.sal(""select length(address) - length(replace(address, ' ', ''))+1 parquetfile"") print(tuesdaycrimes.show()) +-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+ | dates| category| rescript|dayofweek|district| resolution| address| x| y| +-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+ |2015-05-14 03:53:00| warrants| warrant arrest|wednesday| northern|arrest, booked| oak st / laguna st| -122.42589|37.774597| |2015-05-14 03:53:00|other offenses|trade violation...|wednesday| northern|arrest, booked| oak st / laguna st| -122.42589|37.774597| |2015-05-14 03:33:00|other offenses|trade violation...|wednesday| northern|arrest, booked|van a / free...| -122.42436|37.800415|"
57539722,How to clear a Cosmos DB database or delete all items using Azure portal,"If go to https://portal.azure.com, open our Azure Cosmos DB account (1) --&gt; Data Explorer (2) --&gt; Click on users (3) --&gt; Click on New SQL Query:
Azure will open a text box to enter a Query:
I've found that Cosmos DB does not allow the usage of DELETE instead of SELECT: https://stackoverflow.com/a/48339202/1198404, so I should do something like:
SELECT * FROM c DELETE c
SELECT * FROM c DELETE *
But none of my attempts worked.
",<azure><azure-cosmosdb><azureportal><azure-cosmosdb-sqlapi>,438,5,2,3849,6,58,90,42,76294,0.0,216,5,14,2019-08-17 20:28,2019-08-17 22:30,2019-08-18 13:17,0.0,1.0,Basic,1,"<azure><azure-cosmosdb><azureportal><azure-cosmosdb-sqlapi>, How to clear a Cosmos DB database or delete all items using Azure portal, If go to https://portal.azure.com, open our Azure Cosmos DB account (1) --&gt; Data Explorer (2) --&gt; Click on users (3) --&gt; Click on New SQL Query:
Azure will open a text box to enter a Query:
I've found that Cosmos DB does not allow the usage of DELETE instead of SELECT: https://stackoverflow.com/a/48339202/1198404, so I should do something like:
SELECT * FROM c DELETE c
SELECT * FROM c DELETE *
But none of my attempts worked.
","<azure><azure-cosmosdb><azureportal><azure-cosmosdb-slap>, clear costo do database delete item use azur portal, go http://portal.azure.com, open azur costo do account (1) --&it; data explore (2) --&it; click user (3) --&it; click new sal query: azur open text box enter query: i'v found costo do allow usage delete instead select: http://stackoverflow.com/a/48339202/1198404, cometh like: select * c delete c select * c delete * none attempt worked."
58880998,"Communications link failure , Spring Boot + MySql +Docker + Hibernate","I'm working with spring boot, hibernate &amp; MySql. While running the application it is running well as per expectation . But while making the docker-compose file and running the app docker image with mysql docker image it gives this error.
Error
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
java.net.ConnectException: Connection refused.
private Connection createConnection() throws SQLException 
{
        DriverManager.registerDriver(new com.mysql.jdbc.Driver());
        String mysqlUrl = &quot;jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false&quot;;
        Connection connection = DriverManager.getConnection(mysqlUrl, &quot;root&quot;, &quot;root&quot;);
        return connection;
}
Application.properties
spring.datasource.url=jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false
spring.datasource.username=root
spring.datasource.password=root
Please guide me how to tackle this.
docker-compose.yml
version: '3'
services:
  docker-mysql:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=database
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
    ports:
      - 3307:3306
  app:
    image: app:latest
    ports:
       - 8091:8091
    depends_on:
       - docker-mysql
",<mysql><spring><spring-boot><docker><docker-compose>,1303,0,26,627,5,15,33,72,43054,0.0,436,6,14,2019-11-15 16:21,2019-11-15 16:50,2019-11-15 17:17,0.0,0.0,Advanced,37,"<mysql><spring><spring-boot><docker><docker-compose>, Communications link failure , Spring Boot + MySql +Docker + Hibernate, I'm working with spring boot, hibernate &amp; MySql. While running the application it is running well as per expectation . But while making the docker-compose file and running the app docker image with mysql docker image it gives this error.
Error
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
java.net.ConnectException: Connection refused.
private Connection createConnection() throws SQLException 
{
        DriverManager.registerDriver(new com.mysql.jdbc.Driver());
        String mysqlUrl = &quot;jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false&quot;;
        Connection connection = DriverManager.getConnection(mysqlUrl, &quot;root&quot;, &quot;root&quot;);
        return connection;
}
Application.properties
spring.datasource.url=jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false
spring.datasource.username=root
spring.datasource.password=root
Please guide me how to tackle this.
docker-compose.yml
version: '3'
services:
  docker-mysql:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=database
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
    ports:
      - 3307:3306
  app:
    image: app:latest
    ports:
       - 8091:8091
    depends_on:
       - docker-mysql
","<myself><spring><spring-boot><doctor><doctor-compose>, common link failure , spring boot + myself +doctor + liberate, i'm work spring boot, wiberd &amp; myself. run applied run well per expect . make doctor-compose file run pp doctor image myself doctor image give error. error com.myself.job.exceptions.jdbc4.communicationsexception: common link failure cava.net.connectexception: connect refused. privat connect createconnection() throw sqlexcept { drivermanager.registerdriver(new com.myself.job.driver()); string mysqlurl = &quit;job:myself://localhost/database?autoreconnect=true&amp;useful=false&quit;; connect connect = drivermanager.getconnection(mysqlurl, &quit;root&quit;, &quit;root&quit;); return connection; } application.property spring.datasource.curl=job:myself://localhost/database?autoreconnect=true&amp;useful=fall spring.datasource.surname=root spring.datasource.password=root pleas guide tackle this. doctor-compose.you version: '3' services: doctor-myself: image: myself:5.7 environment: - mysql_root_password=root - mysql_database=database - mysql_user=root - mysql_password=root ports: - 3307:3306 pp: image: pp:latest ports: - 8091:8091 depends_on: - doctor-myself"
57803604,Homebrew Mariadb Mysql installation root access denied,"So I basically am installing mariadb with mysql on my mac using homebrew.
These are the steps I made:
brew doctor -> worked 
brew update -> worked 
brew install mariadb -> worked 
mysql_install_db -> Failed
  WARNING: The host 'Toms-MacBook-Pro.local' could not be looked up
  with /usr/local/Cellar/mariadb/10.4.6_1/bin/resolveip. This probably
  means that your libc libraries are not 100 % compatible with this
  binary MariaDB version. The MariaDB daemon, mysqld, should work
  normally with the exception that host name resolving will not work.
  This means that you should use IP addresses instead of hostnames when
  specifying MariaDB privileges ! mysql.user table already exists!
Running mysql_upgrade afterwards gave me following error:
  Version check failed. Got the following error when calling the 'mysql'
  command line client ERROR 1698 (28000): Access denied for user
  'root'@'localhost' FATAL ERROR: Upgrade failed
I can't enter mysql like this:
mysql -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
but like this:
sudo mysql -u root
The user table returns this:
MariaDB [(none)]&gt; USE mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
MariaDB [mysql]&gt; SELECT User, Host, plugin FROM mysql.user;
+---------------+-------------------------+-----------------------+
| User          | Host                    | plugin                |
+---------------+-------------------------+-----------------------+
| root          | localhost               | mysql_native_password |
| toms          | localhost               | mysql_native_password |
|               | localhost               |                       |
|               | toms-macbook-pro.local |                       |
+---------------+-------------------------+-----------------------+
4 rows in set (0.004 sec)
",<mysql><macos><mariadb><homebrew>,1945,0,19,259,1,3,10,77,14310,0.0,1,5,14,2019-09-05 10:38,2019-09-05 12:29,2019-09-05 12:41,0.0,0.0,Advanced,39,"<mysql><macos><mariadb><homebrew>, Homebrew Mariadb Mysql installation root access denied, So I basically am installing mariadb with mysql on my mac using homebrew.
These are the steps I made:
brew doctor -> worked 
brew update -> worked 
brew install mariadb -> worked 
mysql_install_db -> Failed
  WARNING: The host 'Toms-MacBook-Pro.local' could not be looked up
  with /usr/local/Cellar/mariadb/10.4.6_1/bin/resolveip. This probably
  means that your libc libraries are not 100 % compatible with this
  binary MariaDB version. The MariaDB daemon, mysqld, should work
  normally with the exception that host name resolving will not work.
  This means that you should use IP addresses instead of hostnames when
  specifying MariaDB privileges ! mysql.user table already exists!
Running mysql_upgrade afterwards gave me following error:
  Version check failed. Got the following error when calling the 'mysql'
  command line client ERROR 1698 (28000): Access denied for user
  'root'@'localhost' FATAL ERROR: Upgrade failed
I can't enter mysql like this:
mysql -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
but like this:
sudo mysql -u root
The user table returns this:
MariaDB [(none)]&gt; USE mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
MariaDB [mysql]&gt; SELECT User, Host, plugin FROM mysql.user;
+---------------+-------------------------+-----------------------+
| User          | Host                    | plugin                |
+---------------+-------------------------+-----------------------+
| root          | localhost               | mysql_native_password |
| toms          | localhost               | mysql_native_password |
|               | localhost               |                       |
|               | toms-macbook-pro.local |                       |
+---------------+-------------------------+-----------------------+
4 rows in set (0.004 sec)
","<myself><faces><maria><hebrew>, hebrew maria myself instal root access denied, basic instal maria myself mac use hebrew. step made: grew doctor -> work grew update -> work grew instal maria -> work mysql_install_db -> fail warning: host 'toes-malbrook-pro.local' could look /us/local/cellar/maria/10.4.6_1/bin/resolved. probably mean like library 100 % compact binary maria version. maria demon, myself, work normal except host name resolve work. mean use in address instead hostnam specific maria privilege ! myself.us table already exists! run mysql_upgrad afterward gave follow error: version check failed. got follow error call 'myself' command line client error 1698 (28000): access den user 'root'@'localhost' fatal error: upgrade fail can't enter myself like this: myself -root error 2002 (hy000): can't connect local myself server socket '/tm/myself.sock' (2) like this: so myself -u root user table return this: maria [(none)]&it; use myself; read table inform complete table column name turn feature get quicker started -a database change maria [myself]&it; select user, host, plain myself.user; +---------------+-------------------------+-----------------------+ | user | host | plain | +---------------+-------------------------+-----------------------+ | root | localhost | mysql_native_password | | tom | localhost | mysql_native_password | | | localhost | | | | toes-malbrook-pro.low | | +---------------+-------------------------+-----------------------+ 4 row set (0.004 see)"
49400284,Count number of rows in golang,"I want to display the number of rows from database using Go. How do I display number of rows?
count, err := db.Query(""SELECT COUNT(*) FROM main_table"")
",<mysql><go>,152,0,1,205,1,3,9,51,27443,0.0,17,2,14,2018-03-21 7:12,2018-03-21 7:42,2018-03-21 7:42,0.0,0.0,Basic,9,"<mysql><go>, Count number of rows in golang, I want to display the number of rows from database using Go. How do I display number of rows?
count, err := db.Query(""SELECT COUNT(*) FROM main_table"")
","<myself><go>, count number row going, want display number row database use go. display number rows? count, err := do.query(""select count(*) main_table"")"
55304197,Array difference in postgresql,"I have two arrays [1,2,3,4,7,6] and [2,3,7] in PostgreSQL which may have common elements. What I am trying to do is to exclude from the first array all the elements that are present in the second. 
So far I have achieved the following:
SELECT array
  (SELECT unnest(array[1, 2, 3, 4, 7, 6])
   EXCEPT SELECT unnest(array[2, 3, 7]));
However, the ordering is not correct as the result is {4,6,1} instead of the desired {1,4,6}.
How can I fix this ? 
I finally created a custom function with the following definition (taken from here) which resolved my issue:
create or replace function array_diff(array1 anyarray, array2 anyarray)
returns anyarray language sql immutable as $$
    select coalesce(array_agg(elem), '{}')
    from unnest(array1) elem
    where elem &lt;&gt; all(array2)
$$;
",<arrays><postgresql>,788,1,13,1251,2,18,38,49,8540,0.0,111,2,14,2019-03-22 16:41,2019-03-22 17:09,2019-03-22 17:09,0.0,0.0,Intermediate,18,"<arrays><postgresql>, Array difference in postgresql, I have two arrays [1,2,3,4,7,6] and [2,3,7] in PostgreSQL which may have common elements. What I am trying to do is to exclude from the first array all the elements that are present in the second. 
So far I have achieved the following:
SELECT array
  (SELECT unnest(array[1, 2, 3, 4, 7, 6])
   EXCEPT SELECT unnest(array[2, 3, 7]));
However, the ordering is not correct as the result is {4,6,1} instead of the desired {1,4,6}.
How can I fix this ? 
I finally created a custom function with the following definition (taken from here) which resolved my issue:
create or replace function array_diff(array1 anyarray, array2 anyarray)
returns anyarray language sql immutable as $$
    select coalesce(array_agg(elem), '{}')
    from unnest(array1) elem
    where elem &lt;&gt; all(array2)
$$;
","<array><postgresql>, array differ postgresql, two array [1,2,3,4,7,6] [2,3,7] postgresql may common elements. try exclude first array element present second. far achieve following: select array (select unrest(array[1, 2, 3, 4, 7, 6]) except select unrest(array[2, 3, 7])); however, order correct result {4,6,1} instead desire {1,4,6}. fix ? final great custom function follow definite (taken here) resolve issue: great replace function array_diff(array anyarray, array anyarray) return anyarray language sal immune $$ select coalesce(array_agg(elm), '{}') unrest(array) elm elm &it;&it; all(array) $$;"
63066240,Setup postgres in Github Actions for Django,"I'm currently working on a website right now on Django. On my computer, I am running it on Docker with a postgres database. Here's the docker-compose file I have:
version: '3'
services:
    db:
        image: postgres
        environment:
            - POSTGRES_DB=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
    web:
        build: .
        volumes:
            - .:/usr/src/app
        ports:
            - &quot;8000:8000&quot;
And here's the relevant part in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'postgres',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': 'db',
        'PORT': 5432,
    }
}
When I run my tests in the docker container with this setup, it works find and the tests run. However, in github actions, it doesn't work. Here's my workflow file:
name: Django CI
on: push
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [3.7, 3.8]
    services:
      db:
        image: postgres
        env:
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run Tests
      run: |
        python manage.py test
When this runs in github actions, I get the following error:
django.db.utils.OperationalError: could not translate host name &quot;db&quot; to address: Temporary failure in name resolution
Could someone please help me with this, and please let me know if you need anymore code.
",<django><github-actions><django-testing><django-postgresql>,1931,0,66,195,0,2,7,76,4544,0.0,1,2,14,2020-07-24 3:17,2020-08-10 18:49,2020-08-10 18:49,17.0,17.0,Intermediate,23,"<django><github-actions><django-testing><django-postgresql>, Setup postgres in Github Actions for Django, I'm currently working on a website right now on Django. On my computer, I am running it on Docker with a postgres database. Here's the docker-compose file I have:
version: '3'
services:
    db:
        image: postgres
        environment:
            - POSTGRES_DB=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
    web:
        build: .
        volumes:
            - .:/usr/src/app
        ports:
            - &quot;8000:8000&quot;
And here's the relevant part in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'postgres',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': 'db',
        'PORT': 5432,
    }
}
When I run my tests in the docker container with this setup, it works find and the tests run. However, in github actions, it doesn't work. Here's my workflow file:
name: Django CI
on: push
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [3.7, 3.8]
    services:
      db:
        image: postgres
        env:
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run Tests
      run: |
        python manage.py test
When this runs in github actions, I get the following error:
django.db.utils.OperationalError: could not translate host name &quot;db&quot; to address: Temporary failure in name resolution
Could someone please help me with this, and please let me know if you need anymore code.
","<django><github-actions><django-testing><django-postgresql>, set poster github action django, i'm current work west right django. computer, run doctor poster database. here' doctor-compose file have: version: '3' services: do: image: poster environment: - postgres_db=poster - postgres_user=poster - postgres_password=poster web: build: . volumes: - .:/us/sac/pp ports: - &quit;8000:8000&quit; here' rule part settings.i database = { 'default': { 'engine': 'django.do.backed.postgresql', 'name': 'postures', 'user': 'postures', 'password': 'postures', 'host': 'do', 'port': 5432, } } run test doctor contain set, work find test run. however, github actions, work. here' workflow file: name: django i on: push jobs: build: runs-on: bunt-latest strategy: max-parallel: 4 matrix: patron-version: [3.7, 3.8] services: do: image: poster end: postgres_db: poster postgres_user: poster postgres_password: poster ports: - 5432:5432 steps: - uses: actions/checkout@ve - name: set patron ${{ matrix.patron-very }} uses: actions/set-patron@ve with: patron-version: ${{ matrix.patron-very }} - name: instal depend run: | patron -m pp instal --upgrade pp pp instal -r requirements.txt - name: run test run: | patron manage.i test run github actions, get follow error: django.do.still.operationalerror: could translate host name &quit;do&quit; address: temporary failure name resolute could someone pleas help this, pleas let know need anymore code."
49546011,Unable to connect mysql to spring boot project,"i am following this https://spring.io/guides/gs/accessing-data-mysql/ guide to connect mysql db to spring boot project
but getting following error when running the application, i am generating spring starter project and only selecting web, mysql and jpa boxes while creating project via spring tool suite
2018-03-28 16:48:42.125 ERROR 15452 --- [           main] com.zaxxer.hikari.HikariConfig           : Failed to load driver class com.mysql.jdbc.Driver from HikariConfig class classloader jdk.internal.loader.ClassLoaders$AppClassLoader@782830e
2018-03-28 16:48:42.128  WARN 15452 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.context.properties.bind.BindException: Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource
2018-03-28 16:48:42.130  INFO 15452 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 16:48:42.142  INFO 15452 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 16:48:42.143 ERROR 15452 --- [           main] o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource:
    Property: driverclassname
    Value: com.mysql.jdbc.Driver
    Origin: ""driverClassName"" from property source ""source""
    Reason: Unable to set value for property driver-class-name
Action:
Update your application's configuration
following is application.properties 
spring.jpa.hibernate.ddl-auto=create
spring.datasource.url=jdbc:mysql://localhost:3306/world
spring.datasource.username=root
spring.datasource.password=admin
and pom.xml
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;proj.mine&lt;/groupId&gt;
    &lt;artifactId&gt;training-app-2&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;training-app-2&lt;/name&gt;
    &lt;description&gt;traning practive app&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
EDIT: Added spring.datasource.driver-class-name=com.mysql.jdbc.Driver in application.properties, error still persists
2018-03-28 17:55:05.641  WARN 3140 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
2018-03-28 17:55:05.643  INFO 3140 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 17:55:05.656  INFO 3140 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 17:55:05.662 ERROR 3140 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:587) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1250) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1099) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:502) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:312) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:310) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:760) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:388) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:327) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1234) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at proj.mine.TrainingApp2Application.main(TrainingApp2Application.java:10) [classes/:na]
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:579) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 18 common frames omitted
Caused by: java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.util.Assert.state(Assert.java:94) ~[spring-core-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:224) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:81) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Unknown Source) ~[na:na]
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 19 common frames omitted
",<mysql><spring><spring-boot><spring-data-jpa><spring-tool-suite>,11921,6,130,147,1,3,9,39,44053,0.0,2,7,14,2018-03-29 0:00,2018-03-29 12:51,2018-03-29 12:51,0.0,0.0,Basic,12,"<mysql><spring><spring-boot><spring-data-jpa><spring-tool-suite>, Unable to connect mysql to spring boot project, i am following this https://spring.io/guides/gs/accessing-data-mysql/ guide to connect mysql db to spring boot project
but getting following error when running the application, i am generating spring starter project and only selecting web, mysql and jpa boxes while creating project via spring tool suite
2018-03-28 16:48:42.125 ERROR 15452 --- [           main] com.zaxxer.hikari.HikariConfig           : Failed to load driver class com.mysql.jdbc.Driver from HikariConfig class classloader jdk.internal.loader.ClassLoaders$AppClassLoader@782830e
2018-03-28 16:48:42.128  WARN 15452 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.context.properties.bind.BindException: Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource
2018-03-28 16:48:42.130  INFO 15452 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 16:48:42.142  INFO 15452 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 16:48:42.143 ERROR 15452 --- [           main] o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource:
    Property: driverclassname
    Value: com.mysql.jdbc.Driver
    Origin: ""driverClassName"" from property source ""source""
    Reason: Unable to set value for property driver-class-name
Action:
Update your application's configuration
following is application.properties 
spring.jpa.hibernate.ddl-auto=create
spring.datasource.url=jdbc:mysql://localhost:3306/world
spring.datasource.username=root
spring.datasource.password=admin
and pom.xml
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;proj.mine&lt;/groupId&gt;
    &lt;artifactId&gt;training-app-2&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;training-app-2&lt;/name&gt;
    &lt;description&gt;traning practive app&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
EDIT: Added spring.datasource.driver-class-name=com.mysql.jdbc.Driver in application.properties, error still persists
2018-03-28 17:55:05.641  WARN 3140 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
2018-03-28 17:55:05.643  INFO 3140 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 17:55:05.656  INFO 3140 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 17:55:05.662 ERROR 3140 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:587) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1250) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1099) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:502) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:312) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:310) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:760) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:388) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:327) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1234) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at proj.mine.TrainingApp2Application.main(TrainingApp2Application.java:10) [classes/:na]
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:579) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 18 common frames omitted
Caused by: java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.util.Assert.state(Assert.java:94) ~[spring-core-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:224) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:81) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Unknown Source) ~[na:na]
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 19 common frames omitted
","<myself><spring><spring-boot><spring-data-pa><spring-tool-suite>, unable connect myself spring boot project, follow http://spring.to/guides/is/accepting-data-myself/ guide connect myself do spring boot project get follow error run application, genet spring started project select web, myself pa box great project via spring tool suit 2018-03-28 16:48:42.125 error 15452 --- [ main] com.baxter.kari.hikariconfig : fail load driver class com.myself.job.drive hikariconfig class classload do.internal.leader.classloaders$appclassloader@782830 2018-03-28 16:48:42.128 warn 15452 --- [ main] configservletwebserverapplicationcontext : except count context into - cancel refresh attempt: org.springframework.beans.factory.beancreationexception: error great bean name 'datasource' define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]: bean instant via factor method failed; nest except org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except org.springframework.boot.context.properties.bind.bindexception: fail bind property '' com.baxter.kari.hikaridatasourc 2018-03-28 16:48:42.130 into 15452 --- [ main] o.apache.carolina.core.standardservic : stop service [combat] 2018-03-28 16:48:42.142 into 15452 --- [ main] conditionevaluationreportlogginglisten : error start applicationcontext. display conduct report re-run applied 'debut' enabled. 2018-03-28 16:48:42.143 error 15452 --- [ main] o.s.b.d.loggingfailureanalysisreport : *************************** applied fail start *************************** description: fail bind property '' com.baxter.kari.hikaridatasource: property: driverclassnam value: com.myself.job.drive origin: ""driverclassname"" property source ""source"" reason: unable set value property driver-class-am action: update application' configur follow application.property spring.pa.liberate.del-auto=or spring.datasource.curl=job:myself://localhost:3306/world spring.datasource.surname=root spring.datasource.password=admit pot.all &it;?all version=""1.0"" encoding=""utf-8""?&it; &it;project omens=""http://haven.apache.org/pot/4.0.0"" omens:xvi=""http://www.we.org/2001/xmlschema-instance"" xvi:schemalocation=""http://haven.apache.org/pot/4.0.0 http://haven.apache.org/sd/haven-4.0.0.sd""&it; &it;modelversion&it;4.0.0&it;/modelversion&it; &it;grouped&it;pro.mine&it;/grouped&it; &it;artifactid&it;training-pp-2&it;/artifactid&it; &it;version&it;0.0.1-snapshot&it;/version&it; &it;packing&it;jar&it;/packing&it; &it;name&it;training-pp-2&it;/name&it; &it;description&it;than practice pp&it;/description&it; &it;parent&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-parent&it;/artifactid&it; &it;version&it;2.0.0.release&it;/version&it; &it;relativepath/&it; &it;!-- lockup parent depositors --&it; &it;/parent&it; &it;properties&it; &it;project.build.sourceencoding&it;utf-8&it;/project.build.sourceencoding&it; &it;project.reporting.outputencoding&it;utf-8&it;/project.reporting.outputencoding&it; &it;cava.version&it;1.8&it;/cava.version&it; &it;/properties&it; &it;dependencies&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-data-pa&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-web&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;myself&it;/grouped&it; &it;artifactid&it;myself-connection-cava&it;/artifactid&it; &it;scope&it;auntie&it;/scope&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-test&it;/artifactid&it; &it;scope&it;test&it;/scope&it; &it;/dependency&it; &it;/dependencies&it; &it;build&it; &it;plains&it; &it;plain&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-haven-plain&it;/artifactid&it; &it;/plain&it; &it;/plains&it; &it;/build&it; &it;/project&it; edit: ad spring.datasource.driver-class-name=com.myself.job.drive application.properties, error still persist 2018-03-28 17:55:05.641 warn 3140 --- [ main] configservletwebserverapplicationcontext : except count context into - cancel refresh attempt: org.springframework.beans.factory.beancreationexception: error great bean name 'datasource' define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]: bean instant via factor method failed; nest except org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except cava.long.illegalstateexception: cannot load driver class: com.myself.job.drive 2018-03-28 17:55:05.643 into 3140 --- [ main] o.apache.carolina.core.standardservic : stop service [combat] 2018-03-28 17:55:05.656 into 3140 --- [ main] conditionevaluationreportlogginglisten : error start applicationcontext. display conduct report re-run applied 'debut' enabled. 2018-03-28 17:55:05.662 error 3140 --- [ main] o.s.boot.springappl : applied run fail org.springframework.beans.factory.beancreationexception: error great bean name 'datasource' define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]: bean instant via factor method failed; nest except org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except cava.long.illegalstateexception: cannot load driver class: com.myself.job.drive org.springframework.beans.factory.support.constructorresolver.instantiateusingfactorymethod(constructorresolver.cava:587) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.instantiateusingfactorymethod(abstractautowirecapablebeanfactory.cava:1250) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbeaninstance(abstractautowirecapablebeanfactory.cava:1099) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:545) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:502) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:312) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:228) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:310) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:200) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.defaultlistablebeanfactory.preinstantiatesingletons(defaultlistablebeanfactory.cava:760) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:868) ~[spring-context-5.0.4.release.jar:5.0.4.release] org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:549) ~[spring-context-5.0.4.release.jar:5.0.4.release] org.springframework.boot.web.serve.context.servletwebserverapplicationcontext.refresh(servletwebserverapplicationcontext.cava:140) ~[spring-boot-2.0.0.release.jar:2.0.0.release] org.springframework.boot.springapplication.refresh(springapplication.cava:752) [spring-boot-2.0.0.release.jar:2.0.0.release] org.springframework.boot.springapplication.refreshcontext(springapplication.cava:388) [spring-boot-2.0.0.release.jar:2.0.0.release] org.springframework.boot.springapplication.run(springapplication.cava:327) [spring-boot-2.0.0.release.jar:2.0.0.release] org.springframework.boot.springapplication.run(springapplication.cava:1246) [spring-boot-2.0.0.release.jar:2.0.0.release] org.springframework.boot.springapplication.run(springapplication.cava:1234) [spring-boot-2.0.0.release.jar:2.0.0.release] pro.mine.trainingapp2application.main(trainingapp2application.cava:10) [classes/:na] cause by: org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except cava.long.illegalstateexception: cannot load driver class: com.myself.job.drive org.springframework.beans.factory.support.simpleinstantiationstrategy.instantiate(simpleinstantiationstrategy.cava:185) ~[spring-beans-5.0.4.release.jar:5.0.4.release] org.springframework.beans.factory.support.constructorresolver.instantiateusingfactorymethod(constructorresolver.cava:579) ~[spring-beans-5.0.4.release.jar:5.0.4.release] ... 18 common frame omit cause by: cava.long.illegalstateexception: cannot load driver class: com.myself.job.drive org.springframework.until.assert.state(assert.cava:94) ~[spring-core-5.0.4.release.jar:5.0.4.release] org.springframework.boot.autoconfigure.job.datasourceproperties.determinedriverclassname(datasourceproperties.cava:224) ~[spring-boot-autoconfigure-2.0.0.release.jar:2.0.0.release] org.springframework.boot.autoconfigure.job.datasourceproperties.initializedatasourcebuilder(datasourceproperties.cava:176) ~[spring-boot-autoconfigure-2.0.0.release.jar:2.0.0.release] org.springframework.boot.autoconfigure.job.datasourceconfiguration.createdatasource(datasourceconfiguration.cava:43) ~[spring-boot-autoconfigure-2.0.0.release.jar:2.0.0.release] org.springframework.boot.autoconfigure.job.datasourceconfiguration$kari.datasource(datasourceconfiguration.cava:81) ~[spring-boot-autoconfigure-2.0.0.release.jar:2.0.0.release] cava.base/do.internal.reflect.nativemethodaccessorimpl.invoked(n method) ~[na:na] cava.base/do.internal.reflect.nativemethodaccessorimpl.invoke(unknown source) ~[na:na] cava.base/do.internal.reflect.delegatingmethodaccessorimpl.invoke(unknown source) ~[na:na] cava.base/cava.long.reflect.method.invoke(unknown source) ~[na:na] org.springframework.beans.factory.support.simpleinstantiationstrategy.instantiate(simpleinstantiationstrategy.cava:154) ~[spring-beans-5.0.4.release.jar:5.0.4.release] ... 19 common frame omit"
56246710,How to resolve DB connection invalidated warning in Airflow Scheduler?,"I am upgrading our Airflow instance from 1.9 to 1.10.3 and whenever the scheduler runs now I get a warning that the database connection has been invalidated and it's trying to reconnect. A bunch of these errors show up in a row. The console also indicates that tasks are being scheduled but if I check the database nothing is ever being written.
The following warning shows up where it didn't before
[2019-05-21 17:29:26,017] {sqlalchemy.py:81} WARNING - DB connection invalidated. Reconnecting...
Eventually, I'll also get this error
FATAL: remaining connection slots are reserved for non-replication superuser connections
I've tried to increase the SQL Alchemy pool size setting in airflow.cfg but that had no effect
# The SqlAlchemy pool size is the maximum number of database connections in the pool.
sql_alchemy_pool_size = 10
I'm using CeleryExecutor and I'm thinking that maybe the number of workers is overloading the database connections.
I run three commands, airflow webserver, airflow scheduler, and airflow worker, so there should only be one worker and I don't see why that would overload the database.
How do I resolve the database connection errors? Is there a setting to increase the number of database connections, if so where is it? Do I need to handle the workers differently?
Update:
Even with no workers running, starting the webserver and scheduler fresh, when the scheduler fills up the airflow pools the DB connection warning starts to appear.
Update 2:
I found the following issue in the Airflow Jira: https://issues.apache.org/jira/browse/AIRFLOW-4567
There is some activity with others saying they see the same issue. It is unclear whether this directly causes the crashes that some people are seeing or whether this is just an annoying cosmetic log. As of yet there is no resolution to this problem.
",<database><postgresql><sqlalchemy><airflow><airflow-scheduler>,1829,2,8,411,0,3,14,63,3873,0.0,257,1,14,2019-05-21 21:39,2019-08-16 16:55,2019-08-16 16:55,87.0,87.0,Advanced,39,"<database><postgresql><sqlalchemy><airflow><airflow-scheduler>, How to resolve DB connection invalidated warning in Airflow Scheduler?, I am upgrading our Airflow instance from 1.9 to 1.10.3 and whenever the scheduler runs now I get a warning that the database connection has been invalidated and it's trying to reconnect. A bunch of these errors show up in a row. The console also indicates that tasks are being scheduled but if I check the database nothing is ever being written.
The following warning shows up where it didn't before
[2019-05-21 17:29:26,017] {sqlalchemy.py:81} WARNING - DB connection invalidated. Reconnecting...
Eventually, I'll also get this error
FATAL: remaining connection slots are reserved for non-replication superuser connections
I've tried to increase the SQL Alchemy pool size setting in airflow.cfg but that had no effect
# The SqlAlchemy pool size is the maximum number of database connections in the pool.
sql_alchemy_pool_size = 10
I'm using CeleryExecutor and I'm thinking that maybe the number of workers is overloading the database connections.
I run three commands, airflow webserver, airflow scheduler, and airflow worker, so there should only be one worker and I don't see why that would overload the database.
How do I resolve the database connection errors? Is there a setting to increase the number of database connections, if so where is it? Do I need to handle the workers differently?
Update:
Even with no workers running, starting the webserver and scheduler fresh, when the scheduler fills up the airflow pools the DB connection warning starts to appear.
Update 2:
I found the following issue in the Airflow Jira: https://issues.apache.org/jira/browse/AIRFLOW-4567
There is some activity with others saying they see the same issue. It is unclear whether this directly causes the crashes that some people are seeing or whether this is just an annoying cosmetic log. As of yet there is no resolution to this problem.
","<database><postgresql><sqlalchemy><inflow><inflow-schedule>, resolve do connect invalid warn inflow schedule?, upgrade inflow instant 1.9 1.10.3 when schedule run get warn database connect invalid try recollect. bunch error show row. console also india task schedule check database not ever written. follow warn show [2019-05-21 17:29:26,017] {sqlalchemy.by:81} warn - do connect invalidated. connecting... eventually, i'll also get error fatal: remain connect slot reserve non-reply humerus connect i'v try increase sal alchemi pool size set inflow.cf effect # sqlalchemi pool size maximum number database connect pool. sql_alchemy_pool_s = 10 i'm use celeryexecutor i'm think may number worker overcoat database connections. run three commands, inflow observer, inflow schedule, inflow worker, one worker see would overcoat database. resolve database connect errors? set increase number database connections, it? need hand worker differently? update: even worker running, start webster schedule fresh, schedule fill inflow pool do connect warn start appear. update 2: found follow issue inflow fire: http://issues.apache.org/fire/brows/inflow-4567 active other say see issue. unclear whether directly cause crash people see whether annoy comet log. yet resolute problem."
54920939,Parsing FB-Purity's Firefox idb (Indexed Database API) object_data blob from Linux bash,"From a Linux bash script, I want to read the structured data stored by a particular Firefox add-on called FB-Purity.
I have found a folder called .mozilla/firefox/b8eab5j0.default/storage/default/moz-extension+++37a9788c-671d-4cae-ba5c-fbdb8788499a^userContextId=4294967295/ that contains a .metadata file which contains the string moz-extension://37a9788c-671d-4cae-ba5c-fbdb8788499a, an URL which when opened in Firefox shows the add-on's details, so I am pretty sure that this folder belongs to the add-on.
That folder contains an idb directory, which sounds like Indexed Database API, a W3C standard apparently used since last year by Firefox it to store add-ons data.
The idb folder only contains an empty folder and an SQLite file.
The SQLite file, unfortunately, does not contain much application structured data, but the object_data table contains a 95KB blob which probably contains the real structured data:
INSERT INTO `object_data` VALUES (1,'0pmegsjfoetupsf.742612367',NULL,NULL,
X'e08b0d0403000101c0f1ffe5a201000400ffff7b00220032003100380035003000320022003a002
2005300610074006f0072007500200055007205105861006e00690022002c00220036003100350036
[... 95KB ...]
00780022007d00000000000000');
Question: Any clue what this blob's format is? How to extract it (using command line or any library or Linux tool) to JSON or any other readable format?
",<sqlite><firefox-addon><blob><indexeddb><firefox-addon-webextensions>,1355,2,11,58957,59,224,377,52,2601,0.0,2729,1,14,2019-02-28 8:02,2020-01-26 22:23,2020-01-26 22:23,332.0,332.0,Intermediate,26,"<sqlite><firefox-addon><blob><indexeddb><firefox-addon-webextensions>, Parsing FB-Purity's Firefox idb (Indexed Database API) object_data blob from Linux bash, From a Linux bash script, I want to read the structured data stored by a particular Firefox add-on called FB-Purity.
I have found a folder called .mozilla/firefox/b8eab5j0.default/storage/default/moz-extension+++37a9788c-671d-4cae-ba5c-fbdb8788499a^userContextId=4294967295/ that contains a .metadata file which contains the string moz-extension://37a9788c-671d-4cae-ba5c-fbdb8788499a, an URL which when opened in Firefox shows the add-on's details, so I am pretty sure that this folder belongs to the add-on.
That folder contains an idb directory, which sounds like Indexed Database API, a W3C standard apparently used since last year by Firefox it to store add-ons data.
The idb folder only contains an empty folder and an SQLite file.
The SQLite file, unfortunately, does not contain much application structured data, but the object_data table contains a 95KB blob which probably contains the real structured data:
INSERT INTO `object_data` VALUES (1,'0pmegsjfoetupsf.742612367',NULL,NULL,
X'e08b0d0403000101c0f1ffe5a201000400ffff7b00220032003100380035003000320022003a002
2005300610074006f0072007500200055007205105861006e00690022002c00220036003100350036
[... 95KB ...]
00780022007d00000000000000');
Question: Any clue what this blob's format is? How to extract it (using command line or any library or Linux tool) to JSON or any other readable format?
","<quite><firefox-don><blow><indexeddb><firefox-don-webextensions>, part ff-purity' firefox ida (index database apt) object_data blow line base, line base script, want read structure data store particular firefox add-on call ff-purity. found older call .maxilla/firefox/b8eab5j0.default/storage/default/mon-extension+++37a9788c-671d-face-back-fbdb8788499a^usercontextid=4294967295/ contain .metadata file contain string mon-extension://37a9788c-671d-face-back-fbdb8788499a, curl open firefox show add-on' details, pretty sure older belong add-on. older contain ida directory, sound like index database apt, was standard appear use since last year firefox store add-on data. ida older contain empty older quite file. quite file, unfortunately, contain much applied structure data, object_data table contain 95kb blow probably contain real structure data: insert `object_data` value (1,'0pmegsjfoetupsf.742612367',null,null, x'e08b0d0403000101c0f1ffe5a201000400ffff7b00220032003100380035003000320022003a002 2005300610074006f0072007500200055007205105861006e00690022002c00220036003100350036 [... 95kb ...] 00780022007d00000000000000'); question: clue blow' format is? extract (use command line library line tool) son readable format?"
51625671,Routing to Different SQL Server Instances Running through Docker on Default Port,"I can use Traefik for web sites since they use headers when they are connecting.
But I want to have multiple different instances of SQL Server running through docker which will be externally available (outside the docker host, potentially outside the local network)
So, is there anything which allows connecting to different sql server instances running on the same docker instance WITHOUT having to give them different ports or external ip addresses such that someone could access
sql01.docker.local,1433 AND sql02.docker.local,1433 from SQL Tools.
Start Additional Question
Since there has been no replies perhaps there is a way to have different instances like: sql.docker.local\instance1 and sql.docker.local\instance2 though I imagine that may also not be possible
End  Additional Question
This is an example of the docker-compose file I was trying to use (before I realised that queries to sql server don't send through a host header - or am I wrong about that?)
version: '2.1'
services:
  traefik:
    container_name: traefik
    image: stefanscherer/traefik-windows
    command: --docker.endpoint=tcp://172.28.80.1:2375 --logLevel=DEBUG
    ports:
      - ""8080:8080""
      - ""80:80""
      - ""1433:1433""
    volumes:
      - ./runtest:C:/etc/traefik
      - C:/Users/mvukomanovic.admin/.docker:C:/etc/ssl
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.enable=false""
  whoami:
    image: stefanscherer/whoami
    labels:
      - ""traefik.backend=whoami""
      - ""traefik.frontend.entryPoints=http""
      - ""traefik.port=8080""
      - ""traefik.frontend.rule=Host:whoami.docker.local""
    networks:
      - default
    restart: unless-stopped
  sql01:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql01
    domainname: sql01.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql01.docker.local,sql01,sql01.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
  sql02:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql02
    domainname: sql02.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql02.docker.local,sql02,sql02.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
networks:
  default:
    external:
      name: nat
",<sql-server><docker><docker-compose><traefik>,2667,0,74,1402,1,16,24,70,2660,0.0,907,3,14,2018-08-01 5:05,2018-08-07 14:40,,6.0,,Advanced,32,"<sql-server><docker><docker-compose><traefik>, Routing to Different SQL Server Instances Running through Docker on Default Port, I can use Traefik for web sites since they use headers when they are connecting.
But I want to have multiple different instances of SQL Server running through docker which will be externally available (outside the docker host, potentially outside the local network)
So, is there anything which allows connecting to different sql server instances running on the same docker instance WITHOUT having to give them different ports or external ip addresses such that someone could access
sql01.docker.local,1433 AND sql02.docker.local,1433 from SQL Tools.
Start Additional Question
Since there has been no replies perhaps there is a way to have different instances like: sql.docker.local\instance1 and sql.docker.local\instance2 though I imagine that may also not be possible
End  Additional Question
This is an example of the docker-compose file I was trying to use (before I realised that queries to sql server don't send through a host header - or am I wrong about that?)
version: '2.1'
services:
  traefik:
    container_name: traefik
    image: stefanscherer/traefik-windows
    command: --docker.endpoint=tcp://172.28.80.1:2375 --logLevel=DEBUG
    ports:
      - ""8080:8080""
      - ""80:80""
      - ""1433:1433""
    volumes:
      - ./runtest:C:/etc/traefik
      - C:/Users/mvukomanovic.admin/.docker:C:/etc/ssl
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.enable=false""
  whoami:
    image: stefanscherer/whoami
    labels:
      - ""traefik.backend=whoami""
      - ""traefik.frontend.entryPoints=http""
      - ""traefik.port=8080""
      - ""traefik.frontend.rule=Host:whoami.docker.local""
    networks:
      - default
    restart: unless-stopped
  sql01:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql01
    domainname: sql01.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql01.docker.local,sql01,sql01.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
  sql02:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql02
    domainname: sql02.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql02.docker.local,sql02,sql02.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
networks:
  default:
    external:
      name: nat
","<sal-server><doctor><doctor-compose><traffic>, rout differ sal server instant run doctor default port, use traffic web site since use header connecting. want multiple differ instant sal server run doctor externa avail (outside doctor host, potent outside local network) so, any allow connect differ sal server instant run doctor instant without give differ port externa in address someone could access sql01.doctor.local,1433 sql02.doctor.local,1433 sal tools. start admit question since reply perhaps way differ instant like: sal.doctor.local\instance sal.doctor.local\instance though imagine may also possible end admit question example doctor-compose file try use (before realise query sal server send host header - wrong that?) version: '2.1' services: traffic: container_name: traffic image: stefanscherer/traffic-window command: --doctor.endpoint=top://172.28.80.1:2375 --loglevel=debut ports: - ""8080:8080"" - ""80:80"" - ""1433:1433"" volumes: - ./contest:c:/etc/traffic - c:/users/mvukomanovic.admit/.doctor:c:/etc/sal network: - default start: unless-stop labels: - ""traffic.enable=false"" whom: image: stefanscherer/whom labels: - ""traffic.backed=whom"" - ""traffic.fronted.entrypoints=http"" - ""traffic.port=8080"" - ""traffic.fronted.rule=host:whom.doctor.local"" network: - default start: unless-stop sql01: image: microsoft/mssql-server-windows-develop environment: - accept_eula=i hostage: sql01 domainname: sql01.local network: - default start: unless-stop labels: - ""traffic.fronted.rule=host:sql01.doctor.local,sql01,sql01.local"" - ""traffic.fronted.entrypoints=mssql"" - ""traffic.port=1433"" - ""traffic.fronted.port=1433"" network: - default start: unless-stop sql02: image: microsoft/mssql-server-windows-develop environment: - accept_eula=i hostage: sql02 domainname: sql02.local network: - default start: unless-stop labels: - ""traffic.fronted.rule=host:sql02.doctor.local,sql02,sql02.local"" - ""traffic.fronted.entrypoints=mssql"" - ""traffic.port=1433"" - ""traffic.fronted.port=1433"" network: - default start: unless-stop network: default: external: name: at"
60869614,Pyspark: how to extract hour from timestamp,"I have a table like the following
    df
 +------------------------------------+-----------------------+
|identifier                          |timestamp              |
+------------------------------------+-----------------------+
|86311425-0890-40a5-8950-54cbaaa60815|2020-03-18 14:41:55 UTC|
|38e121a8-f21f-4d10-bb69-26eb045175b5|2020-03-13 15:19:21 UTC|
|1a69c9b0-283b-4b6d-89ac-66f987280c66|2020-03-16 12:59:51 UTC|
|c7b5c53f-bf40-498f-8302-4b3329322bc9|2020-03-18 22:05:06 UTC|
|0d3d807b-9b3a-466e-907c-c22402240730|2020-03-17 18:40:03 UTC|
+------------------------------------+-----------------------+
tmp.printSchema()
root
 |-- identifier: string (nullable = true)
 |-- timestamp: string (nullable = true)
I would like to have a column that take only the day and the hours from the timestamp.
I am trying the following:
from pyspark.sql.functions import hour
df = df.withColumn(""hour"", hour(col(""timestamp"")))
but I get the following
+--------------------+--------------------+----+
|          identifier|           timestamp|hour|
+--------------------+--------------------+----+
|321869c3-71e5-41d...|2020-03-19 03:34:...|null|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|null|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|null|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|null|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|null|
+--------------------+--------------------+----+
while I would like to have
+--------------------+--------------------+-------------------+
|          identifier|           timestamp|hour               |
+--------------------+--------------------+-------------------+
|321869c3-71e5-41d...|2020-03-19 03:00:...|2020-03-19 03:00:00|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|2020-03-19 02:00:00|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|2020-03-19 01:00:00|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|2020-03-19 04:00:00|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|2020-03-19 01:00:00|
+--------------------+--------------------+-------------------+
",<python><sql><pyspark>,1987,0,36,7043,20,76,142,58,33584,0.0,107,5,14,2020-03-26 14:32,2020-03-26 15:05,,0.0,,Basic,2,"<python><sql><pyspark>, Pyspark: how to extract hour from timestamp, I have a table like the following
    df
 +------------------------------------+-----------------------+
|identifier                          |timestamp              |
+------------------------------------+-----------------------+
|86311425-0890-40a5-8950-54cbaaa60815|2020-03-18 14:41:55 UTC|
|38e121a8-f21f-4d10-bb69-26eb045175b5|2020-03-13 15:19:21 UTC|
|1a69c9b0-283b-4b6d-89ac-66f987280c66|2020-03-16 12:59:51 UTC|
|c7b5c53f-bf40-498f-8302-4b3329322bc9|2020-03-18 22:05:06 UTC|
|0d3d807b-9b3a-466e-907c-c22402240730|2020-03-17 18:40:03 UTC|
+------------------------------------+-----------------------+
tmp.printSchema()
root
 |-- identifier: string (nullable = true)
 |-- timestamp: string (nullable = true)
I would like to have a column that take only the day and the hours from the timestamp.
I am trying the following:
from pyspark.sql.functions import hour
df = df.withColumn(""hour"", hour(col(""timestamp"")))
but I get the following
+--------------------+--------------------+----+
|          identifier|           timestamp|hour|
+--------------------+--------------------+----+
|321869c3-71e5-41d...|2020-03-19 03:34:...|null|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|null|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|null|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|null|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|null|
+--------------------+--------------------+----+
while I would like to have
+--------------------+--------------------+-------------------+
|          identifier|           timestamp|hour               |
+--------------------+--------------------+-------------------+
|321869c3-71e5-41d...|2020-03-19 03:00:...|2020-03-19 03:00:00|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|2020-03-19 02:00:00|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|2020-03-19 01:00:00|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|2020-03-19 04:00:00|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|2020-03-19 01:00:00|
+--------------------+--------------------+-------------------+
","<patron><sal><spark>, spark: extract hour timestamp, table like follow of +------------------------------------+-----------------------+ |identify |timestamp | +------------------------------------+-----------------------+ |86311425-0890-40a5-8950-54cbaaa60815|2020-03-18 14:41:55 etc| |38e121a8-ff-4d10-bb69-26eb045175b5|2020-03-13 15:19:21 etc| |1a69c9b0-283b-bed-sac-66f987280c66|2020-03-16 12:59:51 etc| |c7b5c53f-bf40-498f-8302-4b3329322bc9|2020-03-18 22:05:06 etc| |0d3d807b-ba-466e-907c-c22402240730|2020-03-17 18:40:03 etc| +------------------------------------+-----------------------+ tm.printschema() root |-- identified: string (nullabl = true) |-- timestamp: string (nullabl = true) would like column take day hour timestamp. try following: spark.sal.fact import hour of = of.withcolumn(""hour"", hour(col(""timestamp""))) get follow +--------------------+--------------------+----+ | identified| timestamp|hour| +--------------------+--------------------+----+ |321869c3-71e5-and...|2020-03-19 03:34:...|null| |226b8d50-sca-471...|2020-03-19 02:59:...|null| |47818b7c-34b5-c...|2020-03-19 01:41:...|null| |f5ca5599-7252-and...|2020-03-19 04:25:...|null| |add2ae24-cab-do...|2020-03-19 01:50:...|null| +--------------------+--------------------+----+ would like +--------------------+--------------------+-------------------+ | identified| timestamp|hour | +--------------------+--------------------+-------------------+ |321869c3-71e5-and...|2020-03-19 03:00:...|2020-03-19 03:00:00| |226b8d50-sca-471...|2020-03-19 02:59:...|2020-03-19 02:00:00| |47818b7c-34b5-c...|2020-03-19 01:41:...|2020-03-19 01:00:00| |f5ca5599-7252-and...|2020-03-19 04:25:...|2020-03-19 04:00:00| |add2ae24-cab-do...|2020-03-19 01:50:...|2020-03-19 01:00:00| +--------------------+--------------------+-------------------+"
56093317,"Google Query - ""NOT LIKE"" Statement Doesn't work","The following line doesn't work:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB NOT LIKE '%Info%'&quot;,1)
Throws:
Unable to parse query string for Function QUERY parameter 2: PARSE_ERROR: Encountered &quot;  &quot;AH &quot;&quot; at line 1, column 26. Was expecting one of: &quot;(&quot; ... &quot;(&quot;
However, this one does:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB LIKE '%Info%'&quot;,1)
",<sql><google-sheets><google-sheets-formula><google-query-language>,401,0,2,482,1,10,18,43,17326,,342,2,14,2019-05-11 18:47,2019-05-11 18:51,2019-05-27 9:07,0.0,16.0,Basic,2,"<sql><google-sheets><google-sheets-formula><google-query-language>, Google Query - ""NOT LIKE"" Statement Doesn't work, The following line doesn't work:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB NOT LIKE '%Info%'&quot;,1)
Throws:
Unable to parse query string for Function QUERY parameter 2: PARSE_ERROR: Encountered &quot;  &quot;AH &quot;&quot; at line 1, column 26. Was expecting one of: &quot;(&quot; ... &quot;(&quot;
However, this one does:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB LIKE '%Info%'&quot;,1)
","<sal><goose-sheets><goose-sheets-formula><goose-query-language>, good query - ""not like"" statement work, follow line work: =query(ab:a,&quit;select ab,ac,ad,a ab like '%into%'&quit;,1) throws: unable part query string function query parapet 2: parse_error: count &quit; &quit;ah &quit;&quit; line 1, column 26. expect one of: &quit;(&quit; ... &quit;(&quit; however, one does: =query(ab:a,&quit;select ab,ac,ad,a ab like '%into%'&quit;,1)"
56329093,Memory leaks when using pandas_udf and Parquet serialization?,"I am currently developing my first whole system using PySpark and I am running into some strange, memory-related issues. In one of the stages, I would like to resemble a Split-Apply-Combine strategy in order to modify a DataFrame. That is, I would like to apply a function to each of the groups defined by a given column and finally combine them all. Problem is, the function I want to apply is a prediction method for a fitted model that ""speaks"" the Pandas idiom, i.e., it is vectorized and takes a Pandas Series as an input. 
I have then designed an iterative strategy, traversing the groups and manually applying a pandas_udf.Scalar in order to solve the problem. The combination part is done using incremental calls to DataFrame.unionByName(). I have decided not to use the GroupedMap type of pandas_udf because the docs state that the memory should be managed by the user, and you should have special care whenever one of the groups might be too large to keep it in memory or be represented by a Pandas DataFrame.
The main problem is that all the processing seems to run fine, but in the end I want to serialize the final DataFrame to a Parquet file. And it is at this point where I receive a lot of Java-like errors about DataFrameWriter, or out-of-memory exceptions.
I have tried the code in both Windows and Linux machines. The only way I have managed to avoid the errors has been to increase the --driver-memory value in the machines. The minimum value is different in every platform, and is dependent on the size of the problem, which somehow makes me suspect on memory leaks.
The problem did not happen until I started using pandas_udf. I think that there is probably a memory leak somewhere in the whole process of pyarrow serialization taking place under the hood when using a pandas_udf.
I have created a minimal reproducible example. If I run this script directly using Python, it produces the error. Using spark-submit and increasing a lot the driver memory, it is possible to make it work. 
import pyspark
import pyspark.sql.functions as F
import pyspark.sql.types as spktyp
# Dummy pandas_udf -------------------------------------------------------------
@F.pandas_udf(spktyp.DoubleType())
def predict(x):
    return x + 100.0
# Initialization ---------------------------------------------------------------
spark = pyspark.sql.SparkSession.builder.appName(
        ""mre"").master(""local[3]"").getOrCreate()
sc = spark.sparkContext
# Generate a dataframe ---------------------------------------------------------
out_path = ""out.parquet""
z = 105
m = 750000
schema = spktyp.StructType(
    [spktyp.StructField(""ID"", spktyp.DoubleType(), True)]
)
df = spark.createDataFrame(
    [(float(i),) for i in range(m)],
    schema
)
for j in range(z):
    df = df.withColumn(
        f""N{j}"",
        F.col(""ID"") + float(j)
    )
df = df.withColumn(
    ""X"",
    F.array(
        F.lit(""A""),
        F.lit(""B""),
        F.lit(""C""),
        F.lit(""D""),
        F.lit(""E"")
    ).getItem(
        (F.rand()*3).cast(""int"")
    )
)
# Set the column names for grouping, input and output --------------------------
group_col = ""X""
in_col = ""N0""
out_col = ""EP""
# Extract different group ids in grouping variable -----------------------------
rows = df.select(group_col).distinct().collect()
groups = [row[group_col] for row in rows]
print(f""Groups: {groups}"")
# Split and treat the first id -------------------------------------------------
first, *others = groups
cur_df = df.filter(F.col(group_col) == first)
result = cur_df.withColumn(
    out_col,
    predict(in_col)
)
# Traverse the remaining group ids ---------------------------------------------
for i, other in enumerate(others):
    cur_df = df.filter(F.col(group_col) == other)
    new_df = cur_df.withColumn(
        out_col,
        predict(in_col)
    )
    # Incremental union --------------------------------------------------------
    result = result.unionByName(new_df)
# Save to disk -----------------------------------------------------------------
result.write.mode(""overwrite"").parquet(out_path)
Shockingly (at least for me), the problem seems to vanish if I put a call to repartition() just before the serialization statement.
result = result.repartition(result.rdd.getNumPartitions())
result.write.mode(""overwrite"").parquet(out_path)
Having put this line into place, I can lower a lot the driver memory configuration, and the script runs fine. I can barely understand the relationship among all those factors, although I suspect lazy evaluation of the code and pyarrow serialization might be related.
This is the current environment I am using for development:
arrow-cpp                 0.13.0           py36hee3af98_1    conda-forge
asn1crypto                0.24.0                py36_1003    conda-forge
astroid                   2.2.5                    py36_0
atomicwrites              1.3.0                      py_0    conda-forge
attrs                     19.1.0                     py_0    conda-forge
blas                      1.0                         mkl
boost-cpp                 1.68.0            h6a4c333_1000    conda-forge
brotli                    1.0.7             he025d50_1000    conda-forge
ca-certificates           2019.3.9             hecc5488_0    conda-forge
certifi                   2019.3.9                 py36_0    conda-forge
cffi                      1.12.3           py36hb32ad35_0    conda-forge
chardet                   3.0.4                 py36_1003    conda-forge
colorama                  0.4.1                    py36_0
cryptography              2.6.1            py36hb32ad35_0    conda-forge
dill                      0.2.9                    py36_0
docopt                    0.6.2                    py36_0
entrypoints               0.3                      py36_0
falcon                    1.4.1.post1     py36hfa6e2cd_1000    conda-forge
fastavro                  0.21.21          py36hfa6e2cd_0    conda-forge
flake8                    3.7.7                    py36_0
future                    0.17.1                py36_1000    conda-forge
gflags                    2.2.2                ha925a31_0
glog                      0.3.5                h6538335_1
hug                       2.5.2            py36hfa6e2cd_0    conda-forge
icc_rt                    2019.0.0             h0cc432a_1
idna                      2.8                   py36_1000    conda-forge
intel-openmp              2019.3                      203
isort                     4.3.17                   py36_0
lazy-object-proxy         1.3.1            py36hfa6e2cd_2
libboost                  1.67.0               hd9e427e_4
libprotobuf               3.7.1                h1a1b453_0    conda-forge
lz4-c                     1.8.1.2              h2fa13f4_0
mccabe                    0.6.1                    py36_1
mkl                       2018.0.3                      1
mkl_fft                   1.0.6            py36hdbbee80_0
mkl_random                1.0.1            py36h77b88f5_1
more-itertools            4.3.0                 py36_1000    conda-forge
ninabrlong                0.1.0                     dev_0    &lt;develop&gt;
nose                      1.3.7                 py36_1002    conda-forge
nose-exclude              0.5.0                      py_0    conda-forge
numpy                     1.15.0           py36h9fa60d3_0
numpy-base                1.15.0           py36h4a99626_0
openssl                   1.1.1b               hfa6e2cd_2    conda-forge
pandas                    0.23.3           py36h830ac7b_0
parquet-cpp               1.5.1                         2    conda-forge
pip                       19.0.3                   py36_0
pluggy                    0.11.0                     py_0    conda-forge
progressbar2              3.38.0                     py_1    conda-forge
py                        1.8.0                      py_0    conda-forge
py4j                      0.10.7                   py36_0
pyarrow                   0.13.0           py36h8c67754_0    conda-forge
pycodestyle               2.5.0                    py36_0
pycparser                 2.19                     py36_1    conda-forge
pyflakes                  2.1.1                    py36_0
pygam                     0.8.0                      py_0    conda-forge
pylint                    2.3.1                    py36_0
pyopenssl                 19.0.0                   py36_0    conda-forge
pyreadline                2.1                      py36_1
pysocks                   1.6.8                 py36_1002    conda-forge
pyspark                   2.4.1                      py_0
pytest                    4.5.0                    py36_0    conda-forge
pytest-runner             4.4                        py_0    conda-forge
python                    3.6.6                hea74fb7_0
python-dateutil           2.8.0                    py36_0
python-hdfs               2.3.1                      py_0    conda-forge
python-mimeparse          1.6.0                      py_1    conda-forge
python-utils              2.3.0                      py_1    conda-forge
pytz                      2019.1                     py_0
re2                       2019.04.01       vc14h6538335_0  [vc14]  conda-forge
requests                  2.21.0                py36_1000    conda-forge
requests-kerberos         0.12.0                   py36_0
scikit-learn              0.20.1           py36hb854c30_0
scipy                     1.1.0            py36hc28095f_0
setuptools                41.0.0                   py36_0
six                       1.12.0                   py36_0
snappy                    1.1.7                h777316e_3
sqlite                    3.28.0               he774522_0
thrift-cpp                0.12.0            h59828bf_1002    conda-forge
typed-ast                 1.3.1            py36he774522_0
urllib3                   1.24.2                   py36_0    conda-forge
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.15.26706          h3a45250_0
wcwidth                   0.1.7                      py_1    conda-forge
wheel                     0.33.1                   py36_0
win_inet_pton             1.1.0                    py36_0    conda-forge
wincertstore              0.2              py36h7fe50ca_0
winkerberos               0.7.0                    py36_1
wrapt                     1.11.1           py36he774522_0
xz                        5.2.4                h2fa13f4_4
zlib                      1.2.11               h62dcd97_3
zstd                      1.3.3                hfe6a214_0
Any hint or help would be much appreciated.
",<python><pandas><pyspark><apache-spark-sql><pyarrow>,10600,0,176,259,0,3,13,43,3318,0.0,48,2,14,2019-05-27 15:45,2019-06-05 15:59,,9.0,,Intermediate,23,"<python><pandas><pyspark><apache-spark-sql><pyarrow>, Memory leaks when using pandas_udf and Parquet serialization?, I am currently developing my first whole system using PySpark and I am running into some strange, memory-related issues. In one of the stages, I would like to resemble a Split-Apply-Combine strategy in order to modify a DataFrame. That is, I would like to apply a function to each of the groups defined by a given column and finally combine them all. Problem is, the function I want to apply is a prediction method for a fitted model that ""speaks"" the Pandas idiom, i.e., it is vectorized and takes a Pandas Series as an input. 
I have then designed an iterative strategy, traversing the groups and manually applying a pandas_udf.Scalar in order to solve the problem. The combination part is done using incremental calls to DataFrame.unionByName(). I have decided not to use the GroupedMap type of pandas_udf because the docs state that the memory should be managed by the user, and you should have special care whenever one of the groups might be too large to keep it in memory or be represented by a Pandas DataFrame.
The main problem is that all the processing seems to run fine, but in the end I want to serialize the final DataFrame to a Parquet file. And it is at this point where I receive a lot of Java-like errors about DataFrameWriter, or out-of-memory exceptions.
I have tried the code in both Windows and Linux machines. The only way I have managed to avoid the errors has been to increase the --driver-memory value in the machines. The minimum value is different in every platform, and is dependent on the size of the problem, which somehow makes me suspect on memory leaks.
The problem did not happen until I started using pandas_udf. I think that there is probably a memory leak somewhere in the whole process of pyarrow serialization taking place under the hood when using a pandas_udf.
I have created a minimal reproducible example. If I run this script directly using Python, it produces the error. Using spark-submit and increasing a lot the driver memory, it is possible to make it work. 
import pyspark
import pyspark.sql.functions as F
import pyspark.sql.types as spktyp
# Dummy pandas_udf -------------------------------------------------------------
@F.pandas_udf(spktyp.DoubleType())
def predict(x):
    return x + 100.0
# Initialization ---------------------------------------------------------------
spark = pyspark.sql.SparkSession.builder.appName(
        ""mre"").master(""local[3]"").getOrCreate()
sc = spark.sparkContext
# Generate a dataframe ---------------------------------------------------------
out_path = ""out.parquet""
z = 105
m = 750000
schema = spktyp.StructType(
    [spktyp.StructField(""ID"", spktyp.DoubleType(), True)]
)
df = spark.createDataFrame(
    [(float(i),) for i in range(m)],
    schema
)
for j in range(z):
    df = df.withColumn(
        f""N{j}"",
        F.col(""ID"") + float(j)
    )
df = df.withColumn(
    ""X"",
    F.array(
        F.lit(""A""),
        F.lit(""B""),
        F.lit(""C""),
        F.lit(""D""),
        F.lit(""E"")
    ).getItem(
        (F.rand()*3).cast(""int"")
    )
)
# Set the column names for grouping, input and output --------------------------
group_col = ""X""
in_col = ""N0""
out_col = ""EP""
# Extract different group ids in grouping variable -----------------------------
rows = df.select(group_col).distinct().collect()
groups = [row[group_col] for row in rows]
print(f""Groups: {groups}"")
# Split and treat the first id -------------------------------------------------
first, *others = groups
cur_df = df.filter(F.col(group_col) == first)
result = cur_df.withColumn(
    out_col,
    predict(in_col)
)
# Traverse the remaining group ids ---------------------------------------------
for i, other in enumerate(others):
    cur_df = df.filter(F.col(group_col) == other)
    new_df = cur_df.withColumn(
        out_col,
        predict(in_col)
    )
    # Incremental union --------------------------------------------------------
    result = result.unionByName(new_df)
# Save to disk -----------------------------------------------------------------
result.write.mode(""overwrite"").parquet(out_path)
Shockingly (at least for me), the problem seems to vanish if I put a call to repartition() just before the serialization statement.
result = result.repartition(result.rdd.getNumPartitions())
result.write.mode(""overwrite"").parquet(out_path)
Having put this line into place, I can lower a lot the driver memory configuration, and the script runs fine. I can barely understand the relationship among all those factors, although I suspect lazy evaluation of the code and pyarrow serialization might be related.
This is the current environment I am using for development:
arrow-cpp                 0.13.0           py36hee3af98_1    conda-forge
asn1crypto                0.24.0                py36_1003    conda-forge
astroid                   2.2.5                    py36_0
atomicwrites              1.3.0                      py_0    conda-forge
attrs                     19.1.0                     py_0    conda-forge
blas                      1.0                         mkl
boost-cpp                 1.68.0            h6a4c333_1000    conda-forge
brotli                    1.0.7             he025d50_1000    conda-forge
ca-certificates           2019.3.9             hecc5488_0    conda-forge
certifi                   2019.3.9                 py36_0    conda-forge
cffi                      1.12.3           py36hb32ad35_0    conda-forge
chardet                   3.0.4                 py36_1003    conda-forge
colorama                  0.4.1                    py36_0
cryptography              2.6.1            py36hb32ad35_0    conda-forge
dill                      0.2.9                    py36_0
docopt                    0.6.2                    py36_0
entrypoints               0.3                      py36_0
falcon                    1.4.1.post1     py36hfa6e2cd_1000    conda-forge
fastavro                  0.21.21          py36hfa6e2cd_0    conda-forge
flake8                    3.7.7                    py36_0
future                    0.17.1                py36_1000    conda-forge
gflags                    2.2.2                ha925a31_0
glog                      0.3.5                h6538335_1
hug                       2.5.2            py36hfa6e2cd_0    conda-forge
icc_rt                    2019.0.0             h0cc432a_1
idna                      2.8                   py36_1000    conda-forge
intel-openmp              2019.3                      203
isort                     4.3.17                   py36_0
lazy-object-proxy         1.3.1            py36hfa6e2cd_2
libboost                  1.67.0               hd9e427e_4
libprotobuf               3.7.1                h1a1b453_0    conda-forge
lz4-c                     1.8.1.2              h2fa13f4_0
mccabe                    0.6.1                    py36_1
mkl                       2018.0.3                      1
mkl_fft                   1.0.6            py36hdbbee80_0
mkl_random                1.0.1            py36h77b88f5_1
more-itertools            4.3.0                 py36_1000    conda-forge
ninabrlong                0.1.0                     dev_0    &lt;develop&gt;
nose                      1.3.7                 py36_1002    conda-forge
nose-exclude              0.5.0                      py_0    conda-forge
numpy                     1.15.0           py36h9fa60d3_0
numpy-base                1.15.0           py36h4a99626_0
openssl                   1.1.1b               hfa6e2cd_2    conda-forge
pandas                    0.23.3           py36h830ac7b_0
parquet-cpp               1.5.1                         2    conda-forge
pip                       19.0.3                   py36_0
pluggy                    0.11.0                     py_0    conda-forge
progressbar2              3.38.0                     py_1    conda-forge
py                        1.8.0                      py_0    conda-forge
py4j                      0.10.7                   py36_0
pyarrow                   0.13.0           py36h8c67754_0    conda-forge
pycodestyle               2.5.0                    py36_0
pycparser                 2.19                     py36_1    conda-forge
pyflakes                  2.1.1                    py36_0
pygam                     0.8.0                      py_0    conda-forge
pylint                    2.3.1                    py36_0
pyopenssl                 19.0.0                   py36_0    conda-forge
pyreadline                2.1                      py36_1
pysocks                   1.6.8                 py36_1002    conda-forge
pyspark                   2.4.1                      py_0
pytest                    4.5.0                    py36_0    conda-forge
pytest-runner             4.4                        py_0    conda-forge
python                    3.6.6                hea74fb7_0
python-dateutil           2.8.0                    py36_0
python-hdfs               2.3.1                      py_0    conda-forge
python-mimeparse          1.6.0                      py_1    conda-forge
python-utils              2.3.0                      py_1    conda-forge
pytz                      2019.1                     py_0
re2                       2019.04.01       vc14h6538335_0  [vc14]  conda-forge
requests                  2.21.0                py36_1000    conda-forge
requests-kerberos         0.12.0                   py36_0
scikit-learn              0.20.1           py36hb854c30_0
scipy                     1.1.0            py36hc28095f_0
setuptools                41.0.0                   py36_0
six                       1.12.0                   py36_0
snappy                    1.1.7                h777316e_3
sqlite                    3.28.0               he774522_0
thrift-cpp                0.12.0            h59828bf_1002    conda-forge
typed-ast                 1.3.1            py36he774522_0
urllib3                   1.24.2                   py36_0    conda-forge
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.15.26706          h3a45250_0
wcwidth                   0.1.7                      py_1    conda-forge
wheel                     0.33.1                   py36_0
win_inet_pton             1.1.0                    py36_0    conda-forge
wincertstore              0.2              py36h7fe50ca_0
winkerberos               0.7.0                    py36_1
wrapt                     1.11.1           py36he774522_0
xz                        5.2.4                h2fa13f4_4
zlib                      1.2.11               h62dcd97_3
zstd                      1.3.3                hfe6a214_0
Any hint or help would be much appreciated.
","<patron><hands><spark><apache-spark-sal><marrow>, memory leak use pandas_udf parquet serialization?, current develop first whole system use spark run strange, memory-red issues. one stages, would like resemble split-apply-combine strategic order modify dataframe. is, would like apply function group define given column final combine all. problem is, function want apply predict method fit model ""speaks"" and idiot, i.e., vector take and peri input. design inter strategy, traders group manual apply pandas_udf.scala order sole problem. combine part done use incitement call dataframe.unionbyname(). decide use groupedmap type pandas_udf do state memory manage user, special care when one group might large keep memory repress and dataframe. main problem process seem run fine, end want aerial final datafram parquet file. point receive lot cava-like error dataframewriter, out-of-memory exceptions. try code window line machines. way manage avoid error increase --driver-memory value machines. minimum value differ every platform, depend size problem, somehow make suspect memory leads. problem happen start use pandas_udf. think probably memory leak somewhere whole process marrow aerial take place hood use pandas_udf. great minims reproduce example. run script directly use patron, produce error. use spark-submit increase lot driver memory, possible make work. import spark import spark.sal.fact f import spark.sal.type spktyp # dummy pandas_udf ------------------------------------------------------------- @f.pandas_udf(spktyp.doubletype()) def predict(x): return x + 100.0 # into --------------------------------------------------------------- spark = spark.sal.sparksession.builder.appease( ""are"").master(""local[3]"").getorcreate() s = spark.sparkcontext # genet datafram --------------------------------------------------------- out_path = ""out.parquet"" z = 105 = 750000 scheme = spktyp.structtype( [spktyp.structfield(""id"", spktyp.doubletype(), true)] ) of = spark.createdataframe( [(float(i),) range(m)], scheme ) j range(z): of = of.withcolumn( f""n{j}"", f.col(""id"") + float(j) ) of = of.withcolumn( ""x"", f.array( f.lit(""a""), f.lit(""b""), f.lit(""c""), f.lit(""d""), f.lit(""e"") ).petite( (f.and()*3).cast(""in"") ) ) # set column name grouping, input output -------------------------- group_col = ""x"" in_col = ""no"" out_col = ""up"" # extract differ group id group variable ----------------------------- row = of.select(group_col).distinct().collect() group = [row[group_col] row rows] print(f""groups: {groups}"") # split treat first id ------------------------------------------------- first, *other = group cured = of.filter(f.col(group_col) == first) result = cured.withcolumn( out_col, predict(in_col) ) # traders remain group id --------------------------------------------- i, enumerate(others): cured = of.filter(f.col(group_col) == other) new_df = cured.withcolumn( out_col, predict(in_col) ) # incitement union -------------------------------------------------------- result = result.unionbyname(new_df) # save disk ----------------------------------------------------------------- result.write.mode(""overwrite"").parquet(out_path) shocking (at least me), problem seem vanish put call repetition() aerial statement. result = result.repetition(result.red.getnumpartitions()) result.write.mode(""overwrite"").parquet(out_path) put line place, lower lot driver memory configuration, script run fine. bare understand relationship among factors, although suspect lazy value code marrow aerial might related. current environs use development: arrow-pp 0.13.0 py36hee3af98_1 conde-for asn1crypto 0.24.0 py36_1003 conde-for mastoid 2.2.5 py36_0 atomicwrit 1.3.0 py_0 conde-for att 19.1.0 py_0 conde-for la 1.0 mal boost-pp 1.68.0 h6a4c333_1000 conde-for brotli 1.0.7 he025d50_1000 conde-for ca-certify 2019.3.9 hecc5488_0 conde-for certify 2019.3.9 py36_0 conde-for off 1.12.3 py36hb32ad35_0 conde-for charge 3.0.4 py36_1003 conde-for colorado 0.4.1 py36_0 cryptographi 2.6.1 py36hb32ad35_0 conde-for will 0.2.9 py36_0 docopt 0.6.2 py36_0 entrypoint 0.3 py36_0 falcon 1.4.1.post py36hfa6e2cd_1000 conde-for fastavro 0.21.21 py36hfa6e2cd_0 conde-for flakes 3.7.7 py36_0 future 0.17.1 py36_1000 conde-for flag 2.2.2 ha925a31_0 glow 0.3.5 h6538335_1 hug 2.5.2 py36hfa6e2cd_0 conde-for icc_rt 2019.0.0 h0cc432a_1 idea 2.8 py36_1000 conde-for inter-open 2019.3 203 sort 4.3.17 py36_0 lazy-object-prove 1.3.1 py36hfa6e2cd_2 libboost 1.67.0 hd9e427e_4 libprotobuf 3.7.1 h1a1b453_0 conde-for let-c 1.8.1.2 h2fa13f4_0 mccabe 0.6.1 py36_1 mal 2018.0.3 1 mkl_fft 1.0.6 py36hdbbee80_0 mkl_random 1.0.1 py36h77b88f5_1 more-itertool 4.3.0 py36_1000 conde-for ninabrlong 0.1.0 devil &it;develop&it; nose 1.3.7 py36_1002 conde-for nose-exclude 0.5.0 py_0 conde-for jump 1.15.0 py36h9fa60d3_0 jump-was 1.15.0 py36h4a99626_0 opens 1.1.b hfa6e2cd_2 conde-for and 0.23.3 py36h830ac7b_0 parquet-pp 1.5.1 2 conde-for pp 19.0.3 py36_0 plunge 0.11.0 py_0 conde-for progressbar2 3.38.0 py_1 conde-for by 1.8.0 py_0 conde-for py4j 0.10.7 py36_0 marrow 0.13.0 py36h8c67754_0 conde-for pycodestyl 2.5.0 py36_0 pycpars 2.19 py36_1 conde-for pyflak 2.1.1 py36_0 pagan 0.8.0 py_0 conde-for point 2.3.1 py36_0 pyopenssl 19.0.0 py36_0 conde-for pyreadlin 2.1 py36_1 sock 1.6.8 py36_1002 conde-for spark 2.4.1 py_0 test 4.5.0 py36_0 conde-for test-run 4.4 py_0 conde-for patron 3.6.6 hea74fb7_0 patron-dateutil 2.8.0 py36_0 patron-of 2.3.1 py_0 conde-for patron-mimepars 1.6.0 py_1 conde-for patron-until 2.3.0 py_1 conde-for put 2019.1 py_0 red 2019.04.01 vc14h6538335_0 [vc14] conde-for request 2.21.0 py36_1000 conde-for requests-cerebro 0.12.0 py36_0 spirit-learn 0.20.1 py36hb854c30_0 ship 1.1.0 py36hc28095f_0 setuptool 41.0.0 py36_0 six 1.12.0 py36_0 snap 1.1.7 h777316e_3 quite 3.28.0 he774522_0 thrifty-pp 0.12.0 h59828bf_1002 conde-for type-as 1.3.1 py36he774522_0 urllib3 1.24.2 py36_0 conde-for ve 14.1 h0510ff6_4 vs2015_runtim 14.15.26706 h3a45250_0 width 0.1.7 py_1 conde-for wheel 0.33.1 py36_0 win_inet_pton 1.1.0 py36_0 conde-for wincertstor 0.2 py36h7fe50ca_0 winkerbero 0.7.0 py36_1 wrap 1.11.1 py36he774522_0 x 5.2.4 h2fa13f4_4 limb 1.2.11 h62dcd97_3 used 1.3.3 hfe6a214_0 hint help would much appreciated."
