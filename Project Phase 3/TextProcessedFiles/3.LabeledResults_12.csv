QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
49508697,"Aggregate for each day over time series, without using non-equijoin logic","Initial Question
Given the following dataset paired with a dates table:
MembershipId | ValidFromDate | ValidToDate
==========================================
0001         | 1997-01-01    | 2006-05-09
0002         | 1997-01-01    | 2017-05-12
0003         | 2005-06-02    | 2009-02-07
How many Memberships were open on any given day or timeseries of days?
Initial Answer
Following this question being asked here, this answer provided the necessary functionality:
select d.[Date]
      ,count(m.MembershipID) as MembershipCount
from DIM.[Date] as d
    left join Memberships as m
        on(d.[Date] between m.ValidFromDateKey and m.ValidToDateKey)
where d.CalendarYear = 2016
group by d.[Date]
order by d.[Date];
though a commenter remarked that There are other approaches when the non-equijoin takes too long.
Followup
As such, what would the equijoin only logic look like to replicate the output of the query above?
Progress So Far
From the answers provided so far I have come up with the below, which outperforms on the hardware I am working with across 3.2 million Membership records:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
)
,e as
(
    select d.[Date] as d
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
),c as
(
    select isnull(s.d,e.d) as d
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
)
select d.[Date]
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
;
Following on from that, to split this aggregate into constituent groups per day I have the following, which is also performing well:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,s.MembershipGrouping as g
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
            ,s.MembershipGrouping
)
,e as
(
    select d.[Date] as d
        ,e..MembershipGrouping as g
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
            ,e.MembershipGrouping
),c as
(
    select isnull(s.d,e.d) as d
            ,isnull(s.g,e.g) as g
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (partition by isnull(s.g,e.g) order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
                and s.g = e.g
)
select d.[Date]
    ,c.g
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
        ,c.g
;
Can anyone improve on the above?
",<sql><sql-server><t-sql><date><join>,3058,1,95,12063,3,25,53,80,2101,0.0,633,6,17,2018-03-27 9:07,2018-04-04 7:23,2018-04-07 9:18,8.0,11.0,Advanced,33
53397668,MySql: set a variable with a list,"I've been researching about MySQL statements and similar issues for some thime now and I don't seem to be having any luck.
Can I create a variable that would store a 1 column data result to be used in multiple queries? Would it be efficient to do so or am I confused about how a DB differs from a PL?
I was thinking something like the following pseudo code
SET list = Select ID from Sales;
Update items set aValue = X where salesID in list
delete SalesMessages where salesId in list
I've got about 8 updates to do in a Store Procedure I could do the select for every case instead of creating a variable, but I doesn't feel like the best approach. Any help?
",<mysql><sql>,657,0,3,769,2,7,31,59,35557,0.0,191,1,17,2018-11-20 16:44,2018-11-20 17:40,2018-11-20 17:40,0.0,0.0,Basic,10
53061182,MySQL connection over SSL with Laravel,"I have a problem connecting to my database over ssl in a Laravel application. My config is as follows:
       'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8mb4',
            'collation' =&gt; 'utf8mb4_unicode_ci',
            'prefix' =&gt; '',
            'strict' =&gt; true,
            'engine' =&gt; null,
            'sslmode' =&gt; 'require',
            'options'   =&gt; array(
                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT =&gt; false,
                PDO::MYSQL_ATTR_SSL_KEY =&gt; '/certs/client-key.pem',
                PDO::MYSQL_ATTR_SSL_CERT =&gt; '/certs/client-cert.pem',
                PDO::MYSQL_ATTR_SSL_CA =&gt; '/certs/ca.pem',
            ),
        ],
DB_CONNECTION=mysql
DB_HOST=xxx.xxx.xxx.xxx
DB_PORT=3306
DB_DATABASE=db_name
DB_USERNAME=db_user
DB_PASSWORD=xxxx
These setting give me the following error on the MySQL server.
2018-10-30T09:18:25.403712Z 71 [Note] Access denied for user 'user'@'xxx.xxx.xxx.xxx' (using password: YES)
Through mysql-client it works perfectly from the client server.
mysql -u user -p -h xxx.xxx.xxx.xxx --ssl-ca=/certs/ca.pem --ssl-cert=/certs/client-cert.pem --ssl-key=/certs/client-key.pem
Output of `\s' command performed on the client while connected with the above command.
mysql  Ver 15.1 Distrib 10.1.26-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2
Connection id:      16
Current database:   database
Current user:       user@xxx.xxx.xxx.xxx
SSL:            Cipher in use is DHE-RSA-AES256-SHA
Current pager:      stdout
Using outfile:      ''
Using delimiter:    ;
Server:         MySQL
Server version:     5.7.24-0ubuntu0.18.10.1 (Ubuntu)
Protocol version:   10
Connection:     167.99.215.179 via TCP/IP
Server characterset:    latin1
Db     characterset:    latin1
Client characterset:    utf8mb4
Conn.  characterset:    utf8mb4
TCP port:       3306
Uptime:         23 min 29 sec
Threads: 2  Questions: 38  Slow queries: 0  Opens: 135  Flush tables: 1  Open tables: 128  Queries per second avg: 0.026
--------------
The application runs inside a docker container based on the php:7.2.11-fpm image. And is configured with the following php extensions.
RUN docker-php-ext-install bcmath zip pdo_mysql json tokenizer
MySQL version:
Server version: 5.7.24-0ubuntu0.18.10.1 (Ubuntu)
PHP version:
PHP 7.2.11 (cli) (built: Oct 16 2018 00:46:29) ( NTS )
PDO version:
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite, mysql
pdo_mysql
PDO Driver for MySQL =&gt; enabled
Client API version =&gt; mysqlnd 5.0.12-dev - 20150407 - $Id: 38fea24f2847fa7519001be390c98ae0acafe387
OpenSSL version:
openssl
OpenSSL support =&gt; enabled
OpenSSL Library Version =&gt; OpenSSL 1.1.0f  25 May 2017
OpenSSL Header Version =&gt; OpenSSL 1.1.0f  25 May 2017
Openssl default config =&gt; /usr/lib/ssl/openssl.cnf
Directive =&gt; Local Value =&gt; Master Value
openssl.cafile =&gt; no value =&gt; no value
openssl.capath =&gt; no value =&gt; no value
Any thoughts on this? This is keeping me busy for hours...
",<mysql><laravel><pdo>,3346,0,75,2423,2,20,38,55,27983,0.0,53,3,17,2018-10-30 9:32,2018-10-30 11:57,2018-10-30 11:57,0.0,0.0,Basic,2
53540056,What causes Spring Boot Fail-safe cleanup (collections) to occur,"I have a Java Spring Boot application with the following entities related to the below exception
SProduct
@Entity
@Table(
        name = &quot;product&quot;,
        indexes = @Index(
                name = &quot;idx_asin&quot;,
                columnList = &quot;asin&quot;,
                unique = true
        )
)
public class SProduct implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @Column(name = &quot;asin&quot;, unique = false, nullable = false, length = 10)
    private String asin;
    @Column(name = &quot;rootcategory&quot;)
    private Long rootcategory;
    @Column(name = &quot;imageCSV&quot;, unique = false, nullable = true, length = 350)
    private String imagesCSV;
    @Column(name = &quot;title&quot;, unique = false, nullable = true, length = 350)
    private String title;
    private Date created;
    @OneToMany(fetch = FetchType.EAGER, mappedBy = &quot;mainProduct&quot;, cascade = CascadeType.ALL)
    private Set&lt;FBT&gt; fbts;
    @OneToOne(fetch = FetchType.EAGER, mappedBy = &quot;downloadProductId&quot;, cascade = CascadeType.ALL)
    private Download download;
FBT
@Entity
@Table(
    name = &quot;fbt&quot;,
    uniqueConstraints={@UniqueConstraint(columnNames = {&quot;main_product_id&quot; , &quot;collection&quot;})},
    indexes = {@Index(
        name = &quot;idx_main_product_id&quot;,
        columnList = &quot;main_product_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt1id&quot;,
        columnList = &quot;product_fbt1_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt2id&quot;,
        columnList = &quot;product_fbt2_id&quot;,
        unique = false)
        }
)
public class FBT implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @ManyToOne
    @JoinColumn(name = &quot;main_product_id&quot;)
    private SProduct mainProduct;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt1_id&quot;)
    private SProduct sproductFbt1;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt2_id&quot;)
    private SProduct sproductFbt2;
    @Column(name = &quot;bsr&quot;, nullable = false)
    private int bsr;
    private Date collection;
I had the following query in my fbt repository
  FBT findByMainProductAndCollection(SProduct mainProduct,Date collection);
which caused the following messages to be output exception when the data exists in the database for the mainProduct and collection but returns null otherwise.
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@69b7fcfc&amp;lt;rs=HikariProxyResultSet@325408381 wrapping com.mysql.jdbc.JDBC42ResultSet@108693fa&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@47c40535&amp;lt;rs=HikariProxyResultSet@2005129089 wrapping com.mysql.jdbc.JDBC42ResultSet@9894f70&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5b0cd175&amp;lt;rs=HikariProxyResultSet@1598144514 wrapping com.mysql.jdbc.JDBC42ResultSet@6a7ff475&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@f67e2cc&amp;lt;rs=HikariProxyResultSet@319200129 wrapping com.mysql.jdbc.JDBC42ResultSet@215b8a6&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5961afc0&amp;lt;rs=HikariProxyResultSet@1772496904 wrapping com.mysql.jdbc.JDBC42ResultSet@5956a59b&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : 
I decided to abandon the above and write a @query to count as I only need to determine if the data exists or not and this has prevented the issue which is making me think I should change all my code to use @query.
 @Query(&quot;select count(*) as count from FBT where main_product_id = :id and collection= :collection&quot;)
    int countByMainProductIdAndCollection(@Param(&quot;id&quot;) long id, @Param(&quot;collection&quot;) Date collection);
Although this similarly also occurs seemingly randomly on updates into the database of one SProduct when the product exists in the database already.
SProductRepo.saveAndFlush(s);
I say randomly as 11 applications running the same code exit at random intervals with the above messages. There are no exceptions generated by the code and 10000's of successful database updates occur with the same code that leads to the failure. The code stops when trying to update the database where it has worked previously.
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5c414639&lt;rs=HikariProxyResultSet@1241510017 wrapping Result set representing update count of 13&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5595c065&lt;rs=HikariProxyResultSet@2140082434 wrapping Result set representing update count of 14&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@2956fe24&lt;rs=HikariProxyResultSe
Additionally the SProduct findByAsin(String asin) query cause the same problem however the query in the database works perfectly and this used to work in spring boot.
mysql&gt; select * from product where asin=&quot;B004FXJOQO&quot;;
| id | asin       | created    | imagecsv                                                                        | rootcategory | title                                                                                                        |  9 | B004FXJOQO | 2018-08-04 | 41T0ZwTvSSL.jpg,61V90AZKbGL.jpg,51AdEGCTZqL.jpg,51LDnCYfR0L.jpg,71bbIw43PjL.jpg |       228013 | Dual Voltage Tester, Non Contact Tester for High and Low Voltage with 3-m Drop Protection Klein Tools NCVT-2 |
1 row in set (0.00 sec)
What I would like to know is what are the general reasons this kind of messages get generated?
Why do they stop my application despite try catch statements around my insertion statements that are the last executed statements in my code?
Are there log debugging settings useful to determine the exact reason for why the messages are generated?
Is there a way to turn off or control this functionality?
Pom
  &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.1.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;maven-dependency-plugin.version&gt;2.10&lt;/maven-dependency-plugin.version&gt;
        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;
    &lt;/properties&gt;
    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;Keepa&lt;/id&gt;
            &lt;name&gt;Keepa Repository&lt;/name&gt;
            &lt;url&gt;https://keepa.com/maven/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;
        &lt;/dependency&gt; 
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;javax.mail&lt;/groupId&gt;
            &lt;artifactId&gt;mail&lt;/artifactId&gt;
            &lt;version&gt;1.4.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
            &lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
            &lt;version&gt;1.18&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-jetty&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-oauth2&lt;/artifactId&gt;
            &lt;version&gt;v1-rev120-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-java6&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-gmail&lt;/artifactId&gt;
            &lt;version&gt;v1-rev48-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.5.0&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                    &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.jcraft&lt;/groupId&gt;
            &lt;artifactId&gt;jsch&lt;/artifactId&gt;
            &lt;version&gt;0.1.54&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.myjeeva.digitalocean&lt;/groupId&gt;
            &lt;artifactId&gt;digitalocean-api-client&lt;/artifactId&gt;
            &lt;version&gt;2.16&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
            &lt;artifactId&gt;gson&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.keepa.api&lt;/groupId&gt;
            &lt;artifactId&gt;backend&lt;/artifactId&gt;
            &lt;version&gt;LATEST&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.jdeferred&lt;/groupId&gt;
            &lt;artifactId&gt;jdeferred-core&lt;/artifactId&gt;
            &lt;version&gt;1.2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build
I increased the memory from 1gb to 2gb however the memory is only 30% of what is available.
Any thoughts as to what the issue is?
",<java><mysql><hibernate><spring-boot>,14871,1,242,1534,2,17,39,71,36405,0.0,148,8,17,2018-11-29 13:23,2018-12-28 10:45,2019-01-03 20:10,29.0,35.0,Advanced,32
49780491,Plotting Histogram for all columns in a Data Frame,"I am trying to draw histograms for all of the columns in my data frame.
I imported pyspark and matplotlib. 
df is my data frame variable.
plt is matplotlib.pyplot variable
I was able to draw/plot histogram for individual column, like this:
bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
But when I try to plot it for all variables I am having issues. Here is the for loop I have so far:
for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
How would I do it? Thanks in advance.
",<python><apache-spark><pyspark><apache-spark-sql>,661,0,8,2692,8,43,69,67,40210,0.0,128,2,17,2018-04-11 16:45,2018-04-11 17:53,2018-04-11 17:53,0.0,0.0,Basic,10
58573628,How do you get the url of a postgresql database you made in pgadmin?,"so i created a postgresql database and i want to link it to my python code using sqlalchemy but since i made the database in pgadmin i dont know what to mention as the url of the database in the code.
thank you in advance.
",<python><postgresql><sqlalchemy><pgadmin>,223,0,0,568,1,5,14,40,39250,0.0,25,3,17,2019-10-26 18:38,2019-10-26 19:56,,0.0,,Basic,10
51823868,Activity Monitor always Paused in SSMS 17.8.1,"Whenever I open the Activity Monitor in SQL Server Management Studio 17.8.1(14.0.17277) the overview always switches to (paused).  Even if I choose resume, it quickly changes back to paused.
This happens on a variety of SQL Servers and SQL Server versions (2005 through 2016) so I don't believe it is a conflict with old vs new SQL Setups.
I can run Activity Monitor in SSMS 2012 (11.0.2100.60) on the same servers with no error which confirms that the service is actually running and functional.
Any help or insights would be appreciated.  I'm not a fan of switching back and forth between two management studios if I can help it. (I uses 17 so I can have context menus when right clicking on items in SSMS which wont work on 2016 servers in older versions of the studio).
",<sql-server><ssms-2012><activity-monitor><ssms-2017>,774,0,0,373,1,4,10,46,31182,0.0,35,4,17,2018-08-13 13:45,2019-03-06 12:20,,205.0,,Basic,14
48868156,How to install the specific version of postgres?,"I need to install a postgres 9.6.6 on Ubuntu 14.04, but the command
apt-get install postgresql-9.6
installes  9.6.7.
How can I install 9.6.6?
An output of command
apt-cache policy postgresql-9.6
is 
postgresql-9.6:
  Installed: 9.6.7-1.pgdg14.04+1
  Candidate: 9.6.7-1.pgdg14.04+1
  Version table:
 *** 9.6.7-1.pgdg14.04+1 0
        500 http://apt.postgresql.org/pub/repos/apt/ trusty-pgdg/main amd64 Packages
        100 /var/lib/dpkg/status
",<linux><postgresql><ubuntu>,443,1,9,957,2,15,30,57,38158,0.0,19,3,17,2018-02-19 14:19,2018-02-19 14:24,,0.0,,Basic,14
54738120,Build new Rails app Error loading the 'sqlite3' without evidently write verion,"When generate new rails app, and start the server ""rails s"", first massage I got:
  Puma caught this error: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile. (LoadError)
after reload a page:
  ActiveRecord::ConnectionNotEstablished
  No connection pool with 'primary' found.
  def retrieve_connection(spec_name) #:nodoc:
    pool = retrieve_connection_pool(spec_name)
    raise ConnectionNotEstablished, ""No connection pool with '#{spec_name}' found."" unless pool
    pool.connection
  end
I reinstall ruby, rails, bundler, all except rvm
and I don't know what to do
P.S.
this error disappears when I evidently write sqlite3 verion, but it should work fine from a without it!!!
Help!What to do? or maybe reinstall all of it?
",<ruby-on-rails><ruby><sqlite><rvm><bundler>,872,0,5,173,0,1,10,68,3939,0.0,2,3,17,2019-02-17 22:03,2019-02-18 3:02,2019-02-18 3:02,1.0,1.0,Basic,14
56526882,PgAdmin Error: Failed to decrypt the saved password,"After updating my pgAdmin version to V4-4.8 I can not login to my database anymore. I'm trying to login on Windows 10.
The error:
Failed to decrypt the saved password.
Error: 'utf-8' codec can't decode byte 0xc3 in position 0: invalid continuation byte
",<postgresql><pgadmin>,253,0,5,654,0,6,10,60,5811,,74,1,17,2019-06-10 12:57,2019-07-23 1:43,2019-07-23 1:43,43.0,43.0,Basic,14
48780903,"'limit' clause in Oracle SQL ""SQL command not properly ended""","I know that questions related to 'limit' have been asked before here,
and I have already referred to them. My question is somewhat different.
Here's my query:
select id,somecol from sometable where someval=2 order by id desc limit 3
I'm getting an error saying 'SQL command not properly ended'.
How do I resolve this? If you need additional information, feel free to tell me so.
",<sql><oracle><limit>,379,0,1,173,1,1,8,50,46578,0.0,0,2,17,2018-02-14 6:24,2018-02-14 6:37,2018-02-14 6:59,0.0,0.0,Basic,2
54809012,knex insert multiple rows,"I have a problem inserting many rows into postgres db with knex.
I have dynamic number of rows needed to be inserted. The result i expect is:
insert row four times (four is for an example. I dont know exact number of inserts as it comes dynamically from frontend):
field_id will be diffrent in every row: (1,2,3,4) - i have array of these ID's
id_of_product will be always the same
value will be always diffrent: (req.body[id] that comes from Frontend) - ID in brackets is same value as the field_id from an
array
How i can achieve that? I tried looping it with forEach, but it's async operation so i can't use .then() as it will be called four times
Here's what i tried. i dont know how to set field_id and req.body to take it dynamically.
fields = [1,2,3,4]
Expected result:
knex creates 4 inserts as follows:
field_id: 1,
product_id: some static id
value: frontValue[1]
ETC
knex('metadata').insert(
 [{ field_id: fields, 
    product_id: product_id, 
    value: req.body[fields] 
 }]
)
",<sql><postgresql><knex.js>,989,0,6,273,2,5,13,67,32728,0.0,3,1,17,2019-02-21 14:09,2019-02-21 14:31,2019-02-21 14:31,0.0,0.0,Basic,9
51977699,Turn Presto columns to rows via rotation,"This is the desired input and desired output. I'm unfamiliar with the terms used in SQL or Presto and the documentation seems to point to using map_agg but I think the problem here is dynamically creating columns but was curious if this is possible where the a, b, ... columns are known and finite.  
I'd be great to know the proper function for this in SQL or Presto and of course if this is possible. Preferably in a way that doesn't involve manually adding a clause per desired row => column. There must be a way to do this automatically or by providing a list of values to filter rows that get converted to headers (As in how 'a' below gets moved to being a column title) 
table_a: 
id | key | value
 0 |   'a'  |   1
 1 |   'b'  |   2
Then becomes desired:
id | 'a' | 'b' 
 0    1    2
The closest I can get is to use map_agg to get a set of key: values which can be pulled one at a time in the output. However the desired solution would be to not have to explicitly list every key I want outputted in the end and instead explode or roll out all keys of kvs:
with subquery_A as (
  select 
    id,   
    map_agg(A.key, A.value) as ""kvs""
  from A as a
  group by 1
)
select 
  sub_a.id,
  sub_a.kvs['a'],
  sub_a.kvs['b']
from subquery_A as sub_a
",<sql><string><pivot><presto>,1252,0,29,3507,5,32,38,40,25637,0.0,156,2,17,2018-08-23 2:51,2018-08-28 11:19,,5.0,,Basic,9
54334202,How to execute a stored procedure against linked server?,"We currently execute a stored procedure against a linked server using:
EXECUTE [LinkedServer].[DatabaseName].[dbo].[MyProcedure]
For example:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
And this works fine; querying through a linked server works fine.
Disabling the deprecated &quot;Remote Access&quot; feature
There is apparently a little known, seldom used, feature known as remote access. Microsoft has very little to say about what this feature is, except to say here:
This configuration option is an obscure SQL Server to SQL Server communication feature that is deprecated, and you probably shouldn't be using it.
ⓘ Important
This feature will be removed in the next version of Microsoft SQL Server. Do not use this feature in new development work, and modify applications that currently use this feature as soon as possible. Use sp_addlinkedserver instead.
The remote access option only applies to servers that are added by using sp_addserver, and is included for backward compatibility.
And from the SQL Server 2000 Books Online:
 Note Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
We only ever added linked servers, and have never used this &quot;remote access&quot; feature, and have never added a server using sp_addserver.
We're all good. Right?
Except turning off remote access breaks everything
An auditor has mentioned that we should turn off the remote access feature:
it's a security checkbox on their clipboard
it's deprecated by Microsoft
it's hardly ever used
and we don't use it
Should be fine, right?
Microsoft documents how to turn off this hardly used feature:
Configure the remote access Server Configuration Option
EXEC sp_configure 'remote access', 0 ;  
GO  
RECONFIGURE ;  
GO
Except when we do: everything goes to hell:
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure? 
Just to be absolutely sure I'm using a linked server, I:
EXECUTE sp_dropserver @server='screwdriver', @dropLogins='droplogins'
EXECUTE sp_addlinkedserver N'screwdriver', N'SQL Server'
and re-run my procedure call:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure!?
I can confirm that the server is a linked server (and is not a &quot;remote&quot; server) using:
SELECT SrvName, IsRemote 
FROM master..sysservers
WHERE SrvName = 'Screwdriver'
Srvname      IsRemote
-----------  --------
screwdriver  0
Or using the modern objects:
SELECT Name, Is_Linked
FROM sys.servers
WHERE Name = 'Screwdriver'
Name         Is_linked
-----------  --------
screwdriver  1
To Sum Up
We're at the point now where:
I've disabled remote access
Remote access only applies to servers added through sp_addserver
it doesn't apply to servers added through sp_addlinkedserver
I'm accessing a server added through sp_addlinkedserver
Why isn't it working?
Which brings me to my question:
How to execute a stored procedure against a linked server?
And the corollary:
How to not execute a stored procedure against an added (i.e. &quot;remote&quot;) server?
Bonus Chatter
The remote access configuration option, that you adjust using sp_configure, is also exposed through the user interface. The SSMS UI describes the feature incorrectly:
It incorrect phrasing is:
Allow remote connections to this server
It should be phrased:
Allow remote connections to from this server.
The Books Online also document the feature incorrectly:
Allow remote connections to this server
Controls the execution of stored procedures from remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from a remote server.
It should be:
Allow remote connections to from this server
Controls the execution of stored procedures from to remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from to a remote server.
It makes sense that the youngsters at Microsoft these days don't remember what a 20 year old deprecated feature that they've never touched does.
Documentation from BOL 2000
SQL Server 2000 was the last time this feature was documented. Reproduced here for posterity and debugging purposes:
Configuring Remote Servers
A remote server configuration allows a client connected to one instance of Microsoft® SQL Server™ to execute a stored procedure on another instance of SQL Server without establishing another connection. The server to which the client is connected accepts the client request and sends the request to the remote server on behalf of the client. The remote server processes the request and returns any results to the original server, which in turn passes those results to the client.
If you want to set up a server configuration in order to execute stored procedures on another server and do not have existing remote server configurations, use linked servers instead of remote servers. Both stored procedures and distributed queries are allowed against linked servers; however, only stored procedures are allowed against remote servers.
 Note  Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
See also
Executing a Stored Procedure on a Linked Server
SQL Linked server - Remote access error when executing SP
SQL Server Error: Could not execute remote procedure
MSDN Blogs: Unable to execute a remote stored procedure over a linked server (archive.is)
Configure the remote access Server Configuration Option
sp_addlinkedserver (Transact-SQL)
sp_configure (Transact-SQL)
Security Audit requires turning Remote Access off on all SQL 2005 and SQL 2008 Servers
Alternate title: Disabling SQL Server Remote Access breaks stored procedures.
",<sql-server><sql-server-2008-r2>,6489,16,29,248457,256,876,1230,36,60670,0.0,5616,4,17,2019-01-23 19:16,2019-01-24 13:43,2019-01-24 13:43,1.0,1.0,Basic,9
51768880,Duplicate columns in Oracle query using row limiting clause,"Since Oracle 12c, we can finally use the SQL standard row limiting clause like this:
SELECT * FROM t FETCH FIRST 10 ROWS ONLY
Now, in Oracle 12.1, there was a limitation that is quite annoying when joining tables. It's not possible to have two columns of the same name in the SELECT clause, when using the row limiting clause. E.g. this raises ORA-00918 in Oracle 12.1
SELECT t.id, u.id FROM t, u FETCH FIRST 10 ROWS ONLY
This is a restriction is documented in the manual for all versions 12.1, 12.2, 18.0:
The workaround is obviously to alias the columns
SELECT t.id AS t_id, u.id AS u_id FROM t, u FETCH FIRST 10 ROWS ONLY
Or to resort to ""classic"" pagination using ROWNUM or window functions.
Curiously, though, the original query with ambiguous ID columns runs just fine from Oracle 12.2 onwards. Is this a documentation bug, or an undocumented feature?
",<sql><oracle><pagination>,858,2,6,213305,130,697,1529,48,583,0.0,6816,2,17,2018-08-09 13:52,2018-09-08 6:27,,30.0,,Advanced,35
52197486,"FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off","It has been working fine for last several months; and suddenly started noticing this error in application,
FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off
pg_hba.conf has,
# IPv4 local connections:
host    all             all             127.0.0.1/32            md5
host    all             all             0.0.0.0/0               md5
postgresql.conf has,
listen_addresses = '*'
Both file have not been touched/changed for many months.
Has anybody faced similar issue in a running environment ?
I have gone through several connection related issues on stoackoverflow; but they all point to one of these two files being misconfigured. Thats not the issue in this case.
The root cause is found and fixed.
This is what happened (for the benefit of those who might encounter such a strange issue)
Three mysterious entires were found in pg_hba.conf, right at the top of the file
These had reject method configured for user postgres, pgsql &amp; pgdbadm
None of our team members added them
Because these were right at the top, even before ""# PostgreSQL Client Authentication Configuration File...."" comment starts, we couldn't notice it.
I am still not sure, how these appeared there
It might be some upgrade issue - but we haven't updated Postgres
It might be a partially successful hacking attempt - still investigating this
But to be on safer side, we have changed server credentials and looking into other hardening methods.
It just might save someone a sleepless night, if such an issue occurs, in a perfectly running environment.
",<postgresql>,1583,0,5,161,1,1,5,35,39425,0.0,3,1,16,2018-09-06 6:06,2018-10-12 10:54,,36.0,,Basic,14
55508830,How to upgrade sqlite 3.8.2 to >= 3.8.3,"In a virtual Env with Python 3.7.2, I am trying to run django's python manage.py startap myapp and I get this error:
raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.8.2).
I'm running Ubuntu Trusty 14.04 Server.
How do I upgrade or update my sqlite version to >=3.8.3?
I ran
$ apt list --installed | grep sqlite
libaprutil1-dbd-sqlite3/trusty,now 1.5.3-1 amd64 [installed,automatic]
libdbd-sqlite3/trusty,now 0.9.0-2ubuntu2 amd64 [installed]
libsqlite3-0/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
libsqlite3-dev/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
python-pysqlite2/trusty,now 2.6.3-3 amd64 [installed]
python-pysqlite2-dbg/trusty,now 2.6.3-3 amd64 [installed]
sqlite3/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
and
sudo apt install --only-upgrade libsqlite3-0
Reading package lists... Done
Building dependency tree      
Reading state information... Done
libsqlite3-0 is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.
EDIT:
the settings.py is stock standard:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
",<python><django><sqlite>,1375,0,24,301,1,2,6,80,31448,0.0,1,5,16,2019-04-04 5:59,2019-04-12 16:50,,8.0,,Basic,14
48327065,Why does conversion from DATETIME to DATETIME2 appear to change value?,"I had a stored procedure comparing two dates. From the logic of my application, I expected them to be equal. However, the comparison failed. The reason for this was the fact that one of the values was stored as a DATETIME and had to be CONVERT-ed to a DATETIME2 before being compared to the other DATETIME2. Apparently, this changed its value. I have run this little test:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2='2018-01-18 16:12:25.1130000'
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
Which gave me the following result:
Why is there the difference of 333333ns between these values? I thought that a DATETIME2, as a more precise type, should be able to accurately represent all the values which can be stored in a DATETIME? The documentation of DATETIME2 only says:
  When the conversion is from datetime, the date and time are copied. The fractional precision is extended to 7 digits.
No warnings about the conversion adding or subtracting 333333ns to or from the value! So why does this happen?
I am using SQL Server 2016. 
edit: Strangely, on a different server I am getting a zero difference. Both are SQL Server 2016 but the one where I have the problem has compatibility level set to 130, the one where I don't has it set to 120. Switching between them changes this behaviour.
edit2: DavidG suggested in the comments that the value I am using can be represented as a DATETIME2 but not a DATETIME. So I have modified my test to make sure that the value I am assigning to @DateTime2 is a valid DATETIME value:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2=CONVERT(DATETIME2, @DateTime)
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
This helps a little because the difference is smaller but still not zero:
",<sql><sql-server><t-sql><datetime>,1848,3,16,3879,2,27,40,60,4195,0.0,421,4,16,2018-01-18 17:25,2018-01-18 17:29,2018-01-18 17:43,0.0,0.0,Intermediate,17
52731361,SQLAlchemy Group By With Full Child Objects,"Imagine the following Media table:
| site       | show_id | time |
| ---------------------|-------|
| CNN        | 1       | 'a'   |
| ABC        | 2       | 'b'   |
| ABC        | 5       | 'c'   |
| CNN        | 3       | 'd'   |
| NBC        | 4       | 'e'   |
| NBC        | 5       | 'f'   |
--------------------------------
I would like to iterate over query results grouped by show_id and have tried this query:
listings = session.query(Media).filter(Media.site == ""CNN"").group_by(Media.show_id).all()
Here's how I would like to iterate over the results:
for showtimes in listings:
    for show in showtimes:
        print(show.time)
But that query doesn't give me all of the grouped child objects.  What am I missing?
",<python><sqlalchemy>,727,0,13,3820,5,43,56,49,9843,0.0,153,2,16,2018-10-10 1:26,2018-10-12 6:30,2018-10-12 6:30,2.0,2.0,Intermediate,17
62442611,InnoDB initialization warnings,"I am using docker on Windows 10 to build full web stack (php, nginx, mysql 8). Using docker compose.
docker-compose.yml 
Here I am using services to build web applcation. But i will show you only mysql service
version: '3.8'
services:
    db:
        build: services/mysql
        container_name: db
        image: projects/laradock_mysql:latest
        env_file: ../.env
        restart: on-failure
        volumes:
            - ./storage/data:/var/lib/mysql
        ports:
            - 3306:3306
        networks:
            - sites
networks:
    sites:
        driver: bridge
        ipam:
            driver: default
            config:
                - subnet: 10.100.36.0/24
MySQL DockerFile
FROM mysql:8
COPY conf/my.cnf /etc/mysql/conf.d
EXPOSE 3306
MySQL config file
[mysqld]
innodb_flush_method=O_DSYNC
innodb_flush_log_at_trx_commit=0
default-authentication-plugin=mysql_native_password
Output log when docker-compose up
2020-06-18T04:40:52.552867Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-06-18T04:40:55.549460Z 1 [Warning] [MY-012579] [InnoDB] fallocate(15, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.568116Z 1 [Warning] [MY-012579] [InnoDB] fallocate(16, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.582720Z 1 [Warning] [MY-012579] [InnoDB] fallocate(17, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.600203Z 1 [Warning] [MY-012579] [InnoDB] fallocate(18, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.614565Z 1 [Warning] [MY-012579] [InnoDB] fallocate(19, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.630446Z 1 [Warning] [MY-012579] [InnoDB] fallocate(20, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.649691Z 1 [Warning] [MY-012579] [InnoDB] fallocate(21, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.666272Z 1 [Warning] [MY-012579] [InnoDB] fallocate(22, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.682022Z 1 [Warning] [MY-012579] [InnoDB] fallocate(23, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.698763Z 1 [Warning] [MY-012579] [InnoDB] fallocate(24, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.379410Z 1 [Warning] [MY-012579] [InnoDB] fallocate(25, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.384741Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
Thanks for a Help!
",<mysql><docker><alpine-linux>,2771,0,57,161,0,1,3,39,3162,0.0,0,1,16,2020-06-18 4:53,2020-10-06 18:23,,110.0,,Advanced,39
53297872,How can I sum multiple columns in a spark dataframe in pyspark?,"I've got a list of column names I want to sum
columns = ['col1','col2','col3']
How can I add the three and put it in a new column ? (in an automatic way, so that I can change the column list and have new results)
Dataframe with result I want:
col1   col2   col3   result
 1      2      3       6
",<python><apache-spark><pyspark><apache-spark-sql>,296,0,3,2093,4,16,38,75,47307,0.0,2055,3,16,2018-11-14 10:21,2018-11-14 10:25,2018-11-14 10:25,0.0,0.0,Basic,10
62761424,How to create a database diagrams in visual studio code?,"I am trying Visual studio code to code the database but I cannot create the database diagram. Is there a way I can create it just like in SSMS.
Thank you.
",<sql><database><visual-studio-code><rdbms><diagram>,155,1,0,161,2,2,4,50,33892,0.0,208,1,16,2020-07-06 17:31,2020-11-08 3:13,,125.0,,Basic,3
50370683,"Room migration: ""no such table: room_table_modification_log""","Room 1.1.0 version.
I am getting this error on the first run after migration.
It runs well if I close the app and start it again. 
ROOM: Cannot run invalidation tracker. Is the db closed?
java.lang.IllegalStateException: Cannot perform this operation because the connection pool has been closed.
    at android.database.sqlite.SQLiteConnectionPool.throwIfClosedLocked(SQLiteConnectionPool.java:1182)
    at android.database.sqlite.SQLiteConnectionPool.waitForConnection(SQLiteConnectionPool.java:792)
    at android.database.sqlite.SQLiteConnectionPool.acquireConnection(SQLiteConnectionPool.java:518)
Caused By : SQL(query) error or missing database.
    (no such table: room_table_modification_log (code 1): , while compiling: DELETE FROM proposals WHERE hotel_id = ?)
#################################################################
    at android.database.sqlite.SQLiteConnection.nativePrepareStatement(Native Method)
    at android.database.sqlite.SQLiteConnection.acquirePreparedStatement(SQLiteConnection.java:1095)
    at android.database.sqlite.SQLiteConnection.prepare(SQLiteConnection.java:660)
    at android.database.sqlite.SQLiteSession.prepare(SQLiteSession.java:588)
    at android.database.sqlite.SQLiteProgram.&lt;init&gt;(SQLiteProgram.java:59)
    at android.database.sqlite.SQLiteStatement.&lt;init&gt;(SQLiteStatement.java:31)
    at android.database.sqlite.SQLiteDatabase.compileStatement(SQLiteDatabase.java:1417)
    at android.arch.persistence.db.framework.FrameworkSQLiteDatabase.compileStatement(FrameworkSQLiteDatabase.java:64)
    at android.arch.persistence.room.RoomDatabase.compileStatement(RoomDatabase.java:244)
    at android.arch.persistence.room.SharedSQLiteStatement.createNewStatement(SharedSQLiteStatement.java:65)
    at android.arch.persistence.room.SharedSQLiteStatement.getStmt(SharedSQLiteStatement.java:77)
    at android.arch.persistence.room.SharedSQLiteStatement.acquire(SharedSQLiteStatement.java:87)
    at com.hotellook.db.dao.FavoritesDao_Impl.removeProposals(FavoritesDao_Impl.java:723)
The DB initialization code looks like this (Dagger AppModule)
@Provides
@Singleton
fun provideDataBase(app: Application): AppDatabase =
  Room.databaseBuilder(app, AppDatabase::class.java, DATABASE_NAME)
      .addMigrations(MIGRATION_1_2)
      .addMigrations(MIGRATION_2_3)
      .build()
private companion object {
  const val DATABASE_NAME = ""app.db""
  val MIGRATION_1_2 = object : Migration(1, 2) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column0 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column0 TEXT;"")
    }
  }
  val MIGRATION_2_3 = object : Migration(2, 3) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""CREATE TABLE table0 (column1 INTEGER NOT NULL, column2 TEXT NOT NULL, PRIMARY KEY(column1))"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column2 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column3 REAL;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column2 REAL;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column1 INTEGER DEFAULT 0;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column2 TEXT;"")
    }
  } 
The app upgrades from version 2 to 3.
How can I fix it?
",<android><sqlite><database-migration><android-room>,3454,0,52,912,1,8,19,39,5597,,16,3,16,2018-05-16 12:14,2018-05-19 0:29,2018-05-19 0:29,3.0,3.0,Intermediate,31
48454336,SSIS: Code page goes back to 65001,"In an SSIS package that I'm writing, I have a CSV file as a source. On the Connection Manager General page, it has 65001 as the Code page (I was testing something). Unicode is not checked.
The columns map to a SQL Server destination table with varchar (among others) columns. 
  There's an error at the destination: The column ""columnname"" cannot be processed because more than one code page (65001 and 1252) are specified for it.
My SQL columns have to be varchar, not nvarchar due to other applications that use it.
On the Connection Manager General page I then change the Code page to 1252 (ANSI - Latin I) and OK out, but when I open it again it's back to 65001. It doesn't make a difference if (just for test) I check Unicode or not.
As a note, all this started happening after the CSV file and the SQL table had columns added and removed (users, you know.) Before that, I had no issues whatsoever. Yes, I refreshed the OLE DB destination in the Advanced Editor.
This is SQL Server 2012 and whichever version of BIDS and SSIS come with it.
",<sql-server><csv><encoding><ssis><etl>,1045,0,6,2367,15,50,68,75,62382,0.0,393,7,16,2018-01-26 0:56,2018-01-27 11:31,2018-01-27 11:31,1.0,1.0,Intermediate,15
56746192,Table Partitions with Entity Framework Core,"I have a large database that will use partitioned column-store tables in a redesign. Is it possible to specify the partition in the generated sql with Entity Framework Core 2.2?
This is for an Azure SQL hyperscale database with a table which currently contains about 3 billion rows. When using stored procedures to execute the requests performance is excellent but if the partition range is not specified in the query performance is less that optimal. I am hoping to move away from the inline sql we are currently using in the application layer and move to entity framework core. Being able to specify the partition for the tenant is our only blocker at the moment. 
This is an example where clause in the stored proceduure
Select @Range = $PARTITION.TenantRange(@InputTenantId)
Select ..... FROM xxx where $PARTITION.TenantRange(TenantId) = @range
The above query would provide excellent performance but I am hoping I can make the same specification of the partition using entity framework.
",<sql-server><entity-framework-core><data-partitioning>,992,0,3,161,0,1,4,66,2529,0.0,0,0,16,2019-06-25 2:49,,,,,Basic,3
58786791,Is there a way to directly insert data from a parquet file into PostgreSQL database?,"I'm trying to restore some historic backup files that saved in parquet format, and I want to read from them once and write the data into a PostgreSQL database.
I know that backup files saved using spark, but there is a strict restriction for me that I cant install spark in the DB machine or read the parquet file using spark in a remote device and write it to the database using spark_df.write.jdbc. Everything needs to happen on the DB machine and in the absence of spark and Hadoop only using Postgres and Bash scripting.
my files structure is something like:
foo/
    foo/part-00000-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00001-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00002-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    ..
    ..
I expect to read data and schema from each parquet folder like foo, create a table using that schema and write the data into the shaped table, only using bash and Postgres CLI.
",<bash><postgresql><hdfs><parquet>,987,0,8,410,1,4,16,65,31361,0.0,243,2,16,2019-11-10 8:05,2019-11-10 14:41,2019-11-10 14:41,0.0,0.0,Basic,10
54026900,Check that a List parameter is null in a Spring data JPA query,"I have a Spring Boot application and use Spring Data JPA to query a MySQL database.
I need to get a list of courses filtered with some parameters.
I usually use the syntax param IS NULL or (/*do something with param*/) so that it ignores the parameter if it is null.
With simple datatypes I have no problems but when it comes to a List of objects I don't know how to check for NULL value. How can I check if the ?3 parameter is NULL in the following query ?
@Query(""SELECT DISTINCT c FROM Course c\n"" +
       ""WHERE c.courseDate &lt; CURRENT_TIME\n"" +
       ""AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\n"" +
       ""AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\n"" +
       ""AND ((?3) IS NULL OR (c.category IN ?3)) "")
List&lt;Course&gt; getCoursesFiltered(Long courseId, String filter, List&lt;Category&gt; categories);
Error is :
  could not extract ResultSet; SQL [n/a]; nested exception is org.hibernate.exception.DataException: could not extract ResultSet[SQL: 1241, 21000]
And in the stack trace I can see :
  Caused by: java.sql.SQLException: Operand should contain 1 column(s)
Indeed generated query would be ... AND ((?3, ?4, ?5) IS NULL OR (c.category IN (?3, ?4, ?5))) if my list contains 3 elements. But IS NULL cannot be applied to multiple elements (query works fine if my list contain only one element).
I have tried size(?3) &lt; 1, length(?3) &lt; 1, (?3) IS EMPTY, etc. but still no luck.
",<sql><jpa><spring-data-jpa><spring-data><hql>,1426,0,15,14705,11,57,84,54,36913,0.0,1335,4,16,2019-01-03 17:17,2019-01-03 17:49,2019-01-04 8:07,0.0,1.0,Basic,10
58623291,How to restore database from dump or sql file in docker using volume?,"I am trying to run my database in docker which already have some data into it, But when I run it in docker it gives empty .. So how to restore my database in docker PostgreSQL image.
",<postgresql><docker><docker-compose><docker-volume>,183,0,0,171,1,2,7,58,21776,,0,1,16,2019-10-30 10:27,2019-10-30 11:11,2019-10-30 11:11,0.0,0.0,Basic,10
49210401,select 30 random rows where sum amount = x,"I have a table 
items
id int unsigned auto_increment primary key,
name varchar(255)
price DECIMAL(6,2)
I want to get at least 30 random items from this table where the total sum of price equals 500, what's the best approach to accomplish this?
I have seen this solution which looks have a similar issue MySQL Select 3 random rows where sum of three rows is less than value
And I am wondering if there are any other solutions that are easier to implement and/or more efficient 
",<php><mysql>,477,1,4,534,1,5,15,52,1463,0.0,42,7,16,2018-03-10 14:53,2018-03-11 0:47,2018-03-18 23:20,1.0,8.0,Basic,10
60138692,sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation <<table>>,"I've read over 20 different questions with the same problem - and the suggested answers didn't solve my problem. I'm still getting sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation &lt;&lt;table&gt;&gt;
Environment: EC2, debian 8, postgresql, flask, sqlalchemy
my table in postgresql:
Create table Members(
id BIGSERIAL PRIMARY KEY,
joinDate TIMESTAMPTZ DEFAULT Now(),
password TEXT NOT NULL
);
directly in postgresql: INSERT INTO members (password) VALUES('123')  RETURNING id; works perfect
I've granted my user all possible grants
grant all privileges on database &lt;my_db&gt; to &lt;my_user&gt;
grant all privileges on all table in schema public to &lt;my_user&gt;
grant all privileges on all relations in schema public to &lt;my_user&gt;
I've also created a .htaccess file under var/www/html with Header set Access-Control-Allow-Origin ""*""
my table mapped with sqlalchemy member.py
from datetime import datetime
from sqlalchemy import create_engine, Column, String, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from marshmallow import Schema, fields
from sqlalchemy.schema import UniqueConstraint
import sqlalchemy as sa
Engine = create_engine('postgresql://&lt;my_user&gt;:&lt;my_psw&gt;@&lt;my_db_url&gt;/&lt;my_dn_name&gt;')
Session = sessionmaker(bind=Engine)
Base = declarative_base()
class MemberSchema(Schema):
    id = fields.Number()    
    password = fields.Str()
class Member(Base):
    __tablename__ = 'members'
    id = Column(Integer, primary_key=True)
    password = Column(String)
    def __init__(self, password):
        self.password = password
the code receiving the POST and doing the insert in test.py:
from flask import Flask, render_template, request,jsonify
from flask_cors import CORS, cross_origin
import logging
def register_member(password):
    session = Session()
    newMember = Member(password)
    session.add(newMember)
    session.commit()
    return newMember
app = Flask(__name__)
cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
@app.route(""/register"", methods=[""GET"", ""POST""])
@cross_origin()    
def register():
    try:
        formData = request.form
        register_member(formData['password'])
        return jsonify({""message"":""Regsitrated successfully!""})
    except Exception as ex:
        logging.debug(ex)
if __name__ == ""__main__"":
    app.run(host='0.0.0.0', port=1234, debug=True)
executing this code results in exception with these errors
DEBUG:root:(psycopg2.errors.InsufficientPrivilege) permission denied for relation members
[SQL: INSERT INTO members (password) VALUES (%(password)s) RETURNING members.id]
[parameters: {'password': '4444'}]
and
TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.
Any help would be appreciated
",<python><postgresql><sqlalchemy><permissions>,2887,0,71,1798,4,20,38,39,30027,0.0,8,4,16,2020-02-09 16:19,2020-02-19 17:51,,10.0,,Basic,6
51627330,SQL to find upper case words from a column,"I have a description column in my table and its values are:
This is a EXAMPLE
This is a TEST
This is a VALUE
I want to display only EXAMPLE, TEST, and VALUE from the description column.
How do I achieve this?
",<sql><oracle>,209,0,3,275,0,2,14,80,6722,0.0,4,7,16,2018-08-01 7:09,2018-08-01 7:43,,0.0,,Basic,10
61859247,Cannot connect with SSMS to SQL Server on Docker,"I have followed the official Microsoft documentation and I have installed SQL Server Docker image 
As result I have a SQL Server image running on Docker at the IP address 172.17.0.2
I also can easily connect to it using sqlcmd with my dummy password
The problem is that I cannot connect to it through SSMS:
Login failed for user 'sa'. (Microsoft SQL Server, Error: 18456)
Of course I read other StackOverflow posts before posting this question and I have tried multiple logins: 
localhost,1433
localhost:1433
172.17.0.2,1433
etc...
How can I connect if localhost doesn't work as well as the IP address of the docker image? 
",<sql-server><docker><containers><ssms><sqlcmd>,624,5,1,10484,15,78,123,45,21064,0.0,705,9,16,2020-05-17 21:45,2020-05-20 5:49,,3.0,,Intermediate,20
50999358,Xampp MySQL not starting - “MYSQL not starting on XAMPP 3.2.1 version…”,"i installed xampp version 3.2.1 on my laptop and mysql was working fine on it before but suddenly mysql stopped working while apache and others were working.
when i click on start mysql it displays this error
i use windows 10
8:52:32 AM  [mysql]     Status change detected: running
8:52:40 AM  [mysql]     Status change detected: stopped
8:52:40 AM  [mysql]     Error: MySQL shutdown unexpectedly.
8:52:40 AM  [mysql]     This may be due to a blocked port, missing dependencies, 
8:52:40 AM  [mysql]     improper privileges, a crash, or a shutdown by another method.
8:52:40 AM  [mysql]     Press the Logs button to view error logs and check
8:52:40 AM  [mysql]     the Windows Event Viewer for more clues
8:52:40 AM  [mysql]     If you need more help, copy and post this
8:52:40 AM  [mysql]     entire log window on the forums
then i checked the log file and this is what i found there
`
2018-06-23 08:52:32 2354 InnoDB: Warning: Using innodb_additional_mem_pool_size is DEPRECATED. This option may be removed in future releases, together with the option innodb_use_sys_malloc and with the InnoDB's internal memory allocator.
180623  8:52:32 [Note] InnoDB: Using mutexes to ref count buffer pool pages
180623  8:52:32 [Note] InnoDB: The InnoDB memory heap is disabled
180623  8:52:32 [Note] InnoDB: Mutexes and rw_locks use Windows interlocked functions
180623  8:52:32 [Note] InnoDB: Memory barrier is not used
180623  8:52:32 [Note] InnoDB: Compressed tables use zlib 1.2.3
180623  8:52:32 [Note] InnoDB: Not using CPU crc32 instructions
180623  8:52:32 [Note] InnoDB: Initializing buffer pool, size = 16.0M
180623  8:52:32 [Note] InnoDB: Completed initialization of buffer pool
180623  8:52:32 [Note] InnoDB: Highest supported file format is Barracuda.
180623  8:52:32 [Note] InnoDB: 128 rollback segment(s) are active.
180623  8:52:32 [Note] InnoDB: Waiting for purge to start
180623  8:52:33 [Note] InnoDB:  Percona XtraDB (http://www.percona.com) 5.6.22-72.0 started; log sequence number 1600924
180623  8:52:33 [Note] Plugin 'FEEDBACK' is disabled.
180623  8:52:33 [Note] Server socket created on IP: '::'.
180623  8:52:33 [Warning] 'db' entry 'size of datafile is: 1907       should be: 1760
180409 15:17:15 [error] mysql.db: got error: 0 when reading datafile at record: 3
180409 15:17:15 [error] got an error from unknown thread, ha _myisam.cc:952
180409 15:17:15 [Warning] Recove@80409 15:17:15 [Warning] Checking table:   '.\mysql\db'
180409 15:17:15 [ERROR] mysql.db: 1 client is using or hasn't closed the table properly
180409 15:17:15 [ERROR] mysql.db:' had database in mixed case that has been forced to lowercase because lower_case_table_names is set. It will not be possible to remove this privilege using REVOKE.
180623  8:52:33 [ERROR] Missing system table mysql.roles_mapping; please run mysql_upgrade to create it
180623  8:52:33 [ERROR] Incorrect definition of table mysql.event: expected column 'sql_mode' at position 14 to have type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','IGNORE_BAD_TABLE_OPTIONS','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH'), found type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_A
180623  8:52:33 [ERROR] Event Scheduler: An error occurred when initializing system tables. Disabling the Event Scheduler.
180623  8:52:33 [Warning] Failed to load slave replication state from table mysql.gtid_slave_pos: 1146: Table 'mysql.gtid_slave_pos' doesn't exist
180623  8:52:33 [Warning] Neither --relay-log nor --relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!! Please use '--log-basename=#' or '--relay-log=mysql-relay-bin' to avoid this problem.
180623  8:52:33 [Note] Started replication for '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
'
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL thread initialized, starting replication in log 'FIRST' at position 0, relay log '.\mysql-relay-bin-180401@002018@003a51@003a15@0020@005bnote@005d@0020c@003a@005cxampp@005cmysql@005cbin@005cmysqld@002eexe@003a@0020ready@0020for@0020connections@002e@000d.000095' position: 4
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Fatal error: Invalid (empty) username when attempting to connect to the master server. Connection attempt terminated. Internal MariaDB error code: 1593
180623  8:52:33 [ERROR] Failed to open the relay log '.\mysql-relay-bin-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.000036' (relay_log_pos 4)
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread killed while connecting to master
180623  8:52:33 [ERROR] Could not find target log during relay log initialization
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread exiting, read up to log 'FIRST', position 4
180623  8:52:33 [ERROR] Initialized Master_info from 'master-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.info' failed
180623  8:52:33 [Warning] Reading of some Master_info entries failed
180623  8:52:33 [ERROR] Failed to initialize multi master structures
180623  8:52:33 [ERROR] Aborting
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Error reading relay log event: slave SQL thread was killed
180623  8:52:33 [Note] InnoDB: FTS optimize thread exiting.
180623  8:52:33 [Note] InnoDB: Starting shutdown...
180623  8:52:35 [Note] InnoDB: Shutdown completed; log sequence number 1600934
180623  8:52:35 [Note] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete`
",<mysql><xampp>,7304,1,67,161,1,1,3,41,11239,0.0,0,5,16,2018-06-23 8:23,2020-04-22 9:07,,669.0,,Basic,14
51089952,Conversion failed when converting the ****** value '******' to data type ******,"I'm getting this error from SQL Server on SSMS 17 while running a giant query:
  Conversion failed when converting the ****** value '******' to data type ******
I have never seen this with ****'s before, and google searching seems to come up with nothing. Is there a known cause for why SQL Server would provide this message with asterisks?
",<sql><sql-server>,341,0,0,32923,40,135,186,72,13685,0.0,260,6,16,2018-06-28 19:19,2018-10-12 20:44,,106.0,,Basic,9
57955765,Upgrading to MYSQL 8 - MAMP,"I have installed MAMP on my MAC machine and I am trying to upgrade it to use MySQL 8. However, I am not having any luck. I have tried following script but database migration fails. 
Also, sequelPro fails to connect to DB but, phpmyadmin has no issue connecting.
#!/bin/sh
curl -OL https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.17-macos10.14-x86_64.tar.gz
tar xfvz mysql-8.0.*
echo ""stopping mamp""
sudo /Applications/MAMP/bin/stop.sh
sudo killall httpd mysqld
echo ""creating backup""
sudo rsync -arv --progress /Applications/MAMP ~/Desktop/MAMP-Backup
echo ""copy bin""
sudo rsync -arv --progress mysql-8.0.*/bin/* /Applications/MAMP/Library/bin/ --exclude=mysqld_multi --exclude=mysqld_safe 
echo ""copy share""
sudo rsync -arv --progress mysql-8.0.*/share/* /Applications/MAMP/Library/share/
echo ""fixing access (workaround)""
sudo chmod -R o+rw  /Applications/MAMP/db/mysql/
sudo chmod -R o+rw  /Applications/MAMP/tmp/mysql/
#sudo chmod -R o+rw  ""/Library/Application Support/appsolute/MAMP PRO/db/mysql/""
echo ""starting mamp""
sudo /Applications/MAMP/bin/start.sh
echo ""migrate to new version""
/Applications/MAMP/Library/bin/mysql -u root --password=root -h 127.0.0.1
",<mysql><macos><mamp><mysql-8.0>,1174,1,28,503,1,3,14,62,12091,0.0,9,0,16,2019-09-16 11:21,,,,,Basic,14
55502546,Find mean of pyspark array<double>,"
In pyspark, I have a variable length array of doubles for which I would like to find the mean. However, the average function requires a single numeric type. 
Is there a way to find the average of an array without exploding the array out? I have several different arrays and I'd like to be able to do something like the following:
df.select(col(""Segment.Points.trajectory_points.longitude""))
DataFrame[longitude: array]
df.select(avg(col(""Segment.Points.trajectory_points.longitude""))).show()
org.apache.spark.sql.AnalysisException: cannot resolve
'avg(Segment.Points.trajectory_points.longitude)' due to data type
mismatch: function average requires numeric types, not
ArrayType(DoubleType,true);;
If I have 3 unique records with the following arrays, I'd like the mean of these values as the output. This would be 3 mean longitude values.  
Input:  
[Row(longitude=[-80.9, -82.9]),
 Row(longitude=[-82.92, -82.93, -82.94, -82.96, -82.92, -82.92]),
 Row(longitude=[-82.93, -82.93])]
Output:  
-81.9,
-82.931,
-82.93
I am using spark version 2.1.3.
Explode Solution:
So I've got this working by exploding, but I was hoping to avoid this step. Here's what I did
from pyspark.sql.functions import col
import pyspark.sql.functions as F
longitude_exp = df.select(
    col(""ID""), 
    F.posexplode(""Segment.Points.trajectory_points.longitude"").alias(""pos"", ""longitude"")
)
longitude_reduced = long_exp.groupBy(""ID"").agg(avg(""longitude""))
This successfully took the mean. However, since I'll be doing this for several columns, I'll have to explode the same DF several different times. I'll keep working through it to find a cleaner way to do this.
",<apache-spark><pyspark><apache-spark-sql>,1641,0,21,329,0,2,11,68,9340,0.0,35,2,16,2019-04-03 19:05,2019-04-03 20:40,2019-04-03 20:40,0.0,0.0,Basic,1
50676867,Executing Named Queries in Athena,"We want to execute a parameterized query in Athena using the javascript sdk by aws.
Seems Athena's named query may be the way to do, but the documentation seems very cryptic to understand how to go about doing this. 
It would be great if someone can help us do the following
What is the recommended way to avoid sql injection in athena?
Create a parameterized query like SELECT c FROM Country c WHERE c.name = :name 
Pass the name parameter's value
Execute this query
",<amazon-web-services><sql-injection><amazon-athena><named-query>,468,1,2,13452,4,61,79,74,7538,0.0,164,1,16,2018-06-04 9:00,2019-02-03 21:02,,244.0,,Basic,3
58351958,psycopg2.ProgrammingError: column cons.consrc does not exist,"I am working on a web app where I needed to update the Postgres(12) database. I am using flask-migrate for the purpose.
I have tried to fix it, but it seems it is a new issue and hence I am not getting much help. I have tried to modify the database from my terminal, that works obviously. This is what I usually type to update my db:
flask db migrate -m ""..""
This is the error message I am receiving at the terminal:
psycopg2.ProgrammingError: column cons.consrc does not exist
LINE 4:                 cons.consrc as src
                        ^
HINT:  Perhaps you meant to reference the column ""cons.conkey"" or the column ""cons.conbin"".
",<python><database><postgresql><flask>,639,0,5,165,0,1,7,40,8014,0.0,23,1,16,2019-10-12 7:36,2019-10-21 22:48,2019-10-21 22:48,9.0,9.0,Basic,9
59584212,How to grant Refresh permissions to the materialized view to user in POSTGRESQL?,"I am executing sql file on linux by running script.
I can see my queries are getting executed fine but I have following query to refresh view in my testData.sql file that is giving me the error
refresh MATERIALIZED VIEW view_test
Error
psql:/home/test/sql/testData.sql:111: ERROR:  must be owner of relation view_test
I have applied following permissions
grant select,update,delete,insert on view_test to &quot;user123&quot;;
How to grant refresh permissions to the View in POSTGRESQL?
",<postgresql><view><refresh>,486,0,3,565,3,9,21,35,14884,0.0,26,3,16,2020-01-03 19:23,2021-11-17 10:13,,684.0,,Basic,9
52225503,Generate C# class from SQL Server table without Store Procedure,"Is there any way to generate a class for table in SQL Server without adding table in project with ADO.net Entity Framework?
public class Approval
{
        public long Id { get; set; }
        public long? FormId { get; set; }
        public long? DesignationId { get; set; }
        public DateTime? CreatedOn { get; set; }
}
",<c#><sql><asp.net><sql-server><oop>,327,1,7,376,1,3,21,70,43628,0.0,63,3,16,2018-09-07 15:19,2018-10-24 17:08,2019-07-09 15:58,47.0,305.0,Basic,3
57994758,Why Parquet over some RDBMS like Postgres,"I'm working to build a data architecture for my company. A simple ETL with internal and external data with the aim to build static dashboard and other to search trend.
I try to think about every step of the ETL process one by one and now I'm questioning about the Load part.
I plan to use Spark (LocalExcecutor on dev and a service on Azure for production) so I started to think about using Parquet into a Blob service. I know all the advantage of Parquet over CSV or other storage format and I really love this piece of technology. Most of the articles I read about Spark finish with a df.write.parquet(...).
But I cannot figure it out why can I just start a Postgres and save everything here. I understand that we are not producing 100Go per day of data but I want to build something future proof in a fast growing company that gonna produce exponentially data by the business and by the logs and metrics we start recording more and more.
Any pros/cons by more experienced dev ?
EDIT : What also make me questioning this is this tweet : https://twitter.com/markmadsen/status/1044360179213651968
",<postgresql><apache-spark><parquet>,1097,2,1,2620,6,38,71,78,9357,0.0,682,3,16,2019-09-18 14:09,2019-09-19 7:55,2019-09-19 7:55,1.0,1.0,Intermediate,23
52360260,SQL Server The column cannot be processed because more than 1 code page (65001 and 1252) are specified for it,"I did a select * on a table. I exported the results and tried to import it and write to another table (I have to do this via SSIS, I cannot do this via SQL Server (i.e. select into) for security permission purposes, but I can do it this way).
How can I fix this error? What exactly does it mean? I tried searching StackOverflow but my search was not very helpful.
I tried to use an OLEDB source step, but my server instance does not appear for some reason, so now I'm trying to export/import out of SSMS.
",<sql-server><t-sql><ssis>,505,0,2,3043,8,48,95,67,46891,0.0,904,3,16,2018-09-17 3:06,2018-09-18 7:16,2018-09-18 7:16,1.0,1.0,Intermediate,18
52749377,"Google Cloud SQL Postgres, when will PG 10 be available?","I am planning on moving our main project to Postgres 10 at some point.  I like to keep the local dev's database version close to what we are running on prod.
Currently our prod database is on Google Cloud SQL PostgreSQL 9.6.  I have not heard anything from Google on when this managed cloud sql product will offer Postgres 10.x in addition to 9.6.
Does anyone know when Postgres 10 will be a supported option on GCP's managed SQL product?  I would like to start planning for it.
",<postgresql><google-cloud-platform><google-cloud-sql><postgresql-10>,479,0,0,161,0,1,3,38,3227,0.0,0,4,16,2018-10-10 22:01,2018-10-11 6:28,,1.0,,Basic,3
50254277,Connection refused when running Laravel artisan command with Docker,"I'm running Laravel 5.4 in Docker. This is my docker-compose.yml file:
version: '2'
services:
  app:
    container_name: laravel_app
    image: webdevops/php-apache-dev:ubuntu-16.04
    links:
      - mysql
    depends_on:
      - mysql
    ports:
      - 8888:80
    volumes:
      - .:/app
    environment:
      docker: 'true'
      WEB_DOCUMENT_ROOT: '/app/public'
      WEB_NO_CACHE_PATTERN: '\.(.*)$$'
      working_dir: '/app'
  mysql:
    image: mariadb:latest
    ports:
      - 8889:80
    environment:
      MYSQL_ROOT_PASSWORD: 'dev'
      MYSQL_DATABASE: 'dev'
      MYSQL_USER: 'dev'
      MYSQL_PASSWORD: 'dev'
This is the relevant part of my .env file:
DB_CONNECTION=mysql
DB_HOST=mysql
DB_PORT=8889
DB_DATABASE=dev
DB_USERNAME=dev
DB_PASSWORD=dev
I am able to see the Laravel welcome page - that side of things works. But when I run php artisan migrate I get this error:
  SQLSTATE[HY000] [2002] Connection refused (SQL: select * from information_schema.tables where table_schema = dev and table_name = migrations)
I have tried fiddling with the host and port parameters in the .env file.
",<php><mysql><laravel><docker><docker-compose>,1106,0,40,7365,20,69,124,35,28061,0.0,197,6,16,2018-05-09 13:11,2018-05-09 13:29,2018-05-09 13:29,0.0,0.0,Basic,14
48651369,What is the best practice of firestore data structure?,"I'm making a blog app using firebase.
I want to know the best practice of data structure.
As far as I know, there are 2 case.
(I'm using react native)
case 1:
posts
  -postID
   -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
    -favoriteList
      -postID(onlyID)
      -postID(onlyID)
In this case, for example, when we need to get favorite posts.
firebase.firestore().collection(`favorites/${userID}/favoriteList`)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((favorite) =&gt; {
        firebase.firestore().collection(`favorites/`).doc(`${favorite.id}`)
          .get()
          .then((post) =&gt; {
          myPostList.push(post.data())
        });
  });
in this case, we can't order the favorite posts by createdDate. So, need to sort client side. Even if so, we don't use limit() function.
case 2:
posts
  -postID
  -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
     -favoriteList
       -postID
         -title,content,author(userID),createdDate,favoriteCount
       -postID
         -title,content,author(userID),createdDate,favoriteCount
firebase.firestore().collection(`favorites/${userID}/favoriteList`).orderBy('createdDate','desc').limit(30)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((post) =&gt; {
        myPostList.push(post.data())
      });
  });
in this case, When the favorite post is modified by the author,
we have to update all of the favorite posts. (e.g. If 100 users save the post as a favorite, we have to update to 100 data.)
(And I'm not sure we can increment favoritecount by a transaction, exactly same.)
I think if we use firebase.batch(), we can manage it. But I think it seems Inefficient.
It seems that both ways are not perfect. Do you know the best practice of this case?
",<javascript><firebase><react-native><nosql><google-cloud-firestore>,1811,0,40,548,3,11,24,70,12934,0.0,32,2,16,2018-02-06 20:24,2018-03-26 2:19,,48.0,,Advanced,33
55336035,Is it possible and how create a trigger with Entity Framework Core,"Is it possible to create a SQL trigger with Entity Framework Core.
Maybe by using instruction in 
protected override void OnModelCreating(DbModelBuilder dbModelBuilder)
{
}
Or simply by executing SQL statements in Migration scripts
public override void Up()
{
}
",<sql-server><entity-framework-core><database-trigger>,262,0,6,17827,31,124,203,58,20650,,522,2,16,2019-03-25 10:46,2020-02-18 9:41,2020-02-18 9:41,330.0,330.0,Basic,10
55956801,Shutting down ExecutorService 'applicationTaskExecutor',"I have a Spring-boot application deployed in docker container within in AWS ECS cluster.
My application stack is => Spring Boot -- JPA -- MySQL RDS
Initially application is deployed and accessible through EC2 public IP.
But after few minutes only application is Shutting down ExecutorService 'applicationTaskExecutor' and restart container again. it is happening constantly in every 3/4 mins.
I am not getting this kind of error in local deployment connecting to same RDS.
Here is my application.properties
server.port=8192
spring.datasource.url=jdbc:mysql://multichannelappdatabase.cvjsdfsdfsdfsfs.us-east-1.rds.amazonaws.com:3306/multichannelappdb
spring.datasource.username=xxxxx
spring.datasource.password=xxxxx
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.jackson.serialization.FAIL_ON_EMPTY_BEANS=false
spring.jpa.database-platform=org.hibernate.dialect.MySQLDialect
spring.jpa.show-sql=true
#spring.jpa.hibernate.ddl-auto=create
spring.datasource.testWhileIdle = true
spring.datasource.timeBetweenEvictionRunsMillis = 10000
spring.datasource.tomcat.testOnBorrow=true 
spring.datasource.tomcat.validationQuery=SELECT 1
and here is my cloudwatch log

16:20:50
2019-05-02 16:20:50.514 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8192 (http) with context path ''

16:20:50
2019-05-02 16:20:50.516 INFO 1 --- [ main] c.api.app.runner.UserManagerApplication : Started UserManagerApplication in 9.338 seconds (JVM running for 10.291)

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.131 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 14 ms

16:23:19
2019-05-02 16:23:19.253 INFO 1 --- [ Thread-4] o.s.s.concurrent.ThreadPoolTaskExecutor : Shutting down ExecutorService 'applicationTaskExecutor'

16:23:19
2019-05-02 16:23:19.254 INFO 1 --- [ Thread-4] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'

16:23:19
2019-05-02 16:23:19.257 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...

16:23:19
2019-05-02 16:23:19.274 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed.
Need suggestion to resolve this issue.
Still getting the same issue .. any solution will be helpful
",<mysql><spring-boot><spring-data-jpa><amazon-rds><amazon-ecs>,2620,0,44,1196,4,14,31,64,26122,,17,4,16,2019-05-02 16:37,2019-06-06 12:58,,35.0,,Intermediate,31
49142684,psql how to drop current database,"Trying to drop the primary database ""foo"", I get the error
  ERROR:  cannot drop the currently open database
Whats the right way to drop the primary psql database? 
",<postgresql>,165,0,0,16955,16,138,148,81,21100,,4078,2,16,2018-03-07 1:24,2018-03-07 1:24,2018-03-07 8:36,0.0,0.0,Basic,10
53720353,"Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""","This happens even when the DB user specified in .env file is different. For the record ""ankitj"" is also the username of my system. I don't understand why this is happening.
Here's the error:
Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""
    at connection.connect.err (/home/ankitj/Desktop/skillbee/node_modules/sequelize/lib/dialects/postgres/connection-manager.js:128:24)
    at Connection.connectingErrorHandler (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/client.js:140:14)
    at Connection.emit (events.js:160:13)
    at Socket.&lt;anonymous&gt; (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/connection.js:124:12)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
",<node.js><postgresql><sequelize.js>,906,0,10,431,2,5,8,77,14356,0.0,27,6,16,2018-12-11 8:45,2019-01-23 21:36,,43.0,,Basic,13
