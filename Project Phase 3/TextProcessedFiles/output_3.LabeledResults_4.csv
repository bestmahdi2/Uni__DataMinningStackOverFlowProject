QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
61749304,Connection between DBeaver & MySQL,"I use DBeaver to watch an SQL database on MySQL 8+.
Everything is working, but sometimes, opening DBeaver, I have the following error message :
Public Key Retrieval is not allowed
And then, DBeaver can't connect to MySQL.
In order to fix this problem, I have to reconfigure MySQL.
Is there any simplest way to fix this problem ?
",<mysql><dbeaver>,329,0,7,1086,3,14,32,41,56032,0.0,313,4,36,2020-05-12 10:24,2020-05-16 6:37,2020-05-16 6:37,4.0,4.0,Basic,14,"<mysql><dbeaver>, Connection between DBeaver & MySQL, I use DBeaver to watch an SQL database on MySQL 8+.
Everything is working, but sometimes, opening DBeaver, I have the following error message :
Public Key Retrieval is not allowed
And then, DBeaver can't connect to MySQL.
In order to fix this problem, I have to reconfigure MySQL.
Is there any simplest way to fix this problem ?
","<myself><beaver>, connect beaver & myself, use beaver watch sal database myself 8+. every working, sometimes, open beaver, follow error message : public key retrieve allow then, beaver can't connect myself. order fix problem, reconfigur myself. simplest way fix problem ?"
49252670,Iterate rows and columns in Spark dataframe,"I have the following Spark dataframe that is created dynamically:
val sf1 = StructField(""name"", StringType, nullable = true)
val sf2 = StructField(""sector"", StringType, nullable = true)
val sf3 = StructField(""age"", IntegerType, nullable = true)
val fields = List(sf1,sf2,sf3)
val schema = StructType(fields)
val row1 = Row(""Andy"",""aaa"",20)
val row2 = Row(""Berta"",""bbb"",30)
val row3 = Row(""Joe"",""ccc"",40)
val data = Seq(row1,row2,row3)
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.createOrReplaceTempView(""people"")
val sqlDF = spark.sql(""SELECT * FROM people"")
Now, I need to iterate each row and column in sqlDF to print each column, this is my attempt:
sqlDF.foreach { row =&gt;
  row.foreach { col =&gt; println(col) }
}
row is type Row, but is not iterable that's why this code throws a compilation error in row.foreach. How to iterate each column in Row?
",<scala><apache-spark><apache-spark-sql>,894,0,25,1329,23,137,340,44,179916,0.0,4482,9,36,2018-03-13 9:35,2018-03-13 9:41,2018-09-02 1:37,0.0,173.0,Basic,10,"<scala><apache-spark><apache-spark-sql>, Iterate rows and columns in Spark dataframe, I have the following Spark dataframe that is created dynamically:
val sf1 = StructField(""name"", StringType, nullable = true)
val sf2 = StructField(""sector"", StringType, nullable = true)
val sf3 = StructField(""age"", IntegerType, nullable = true)
val fields = List(sf1,sf2,sf3)
val schema = StructType(fields)
val row1 = Row(""Andy"",""aaa"",20)
val row2 = Row(""Berta"",""bbb"",30)
val row3 = Row(""Joe"",""ccc"",40)
val data = Seq(row1,row2,row3)
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.createOrReplaceTempView(""people"")
val sqlDF = spark.sql(""SELECT * FROM people"")
Now, I need to iterate each row and column in sqlDF to print each column, this is my attempt:
sqlDF.foreach { row =&gt;
  row.foreach { col =&gt; println(col) }
}
row is type Row, but is not iterable that's why this code throws a compilation error in row.foreach. How to iterate each column in Row?
","<scala><apache-spark><apache-spark-sal>, inter row column spark dataframe, follow spark datafram great dynamically: val of = structfield(""name"", stringtype, nullabl = true) val of = structfield(""sector"", stringtype, nullabl = true) val of = structfield(""age"", integertype, nullabl = true) val field = list(of,of,of) val scheme = structtype(fields) val rows = row(""andy"",""ana"",20) val rows = row(""berth"",""bob"",30) val rows = row(""joe"",""can"",40) val data = see(rows,rows,rows) val of = spark.createdataframe(spark.sparkcontext.parallelize(data), scheme) of.createorreplacetempview(""people"") val self = spark.sal(""select * people"") now, need inter row column self print column, attempt: self.french { row =&it; row.french { col =&it; print(col) } } row type row, inter that' code throw compel error row.french. inter column row?"
50936251,"ERROR: function dblink(unknown, unknown) does not exist","I have defined a foreign server pointing to another database. I then want to execute a function in that database and get back the results.
When I try this:
SELECT * FROM  dblink('mylink','select someschema.somefunction(''test'', ''ABC'')')
or this:
SELECT t.n FROM  dblink('mylink', 'select * from someschema.mytable') as t(n text)
I get the error:
  ERROR: function dblink(unknown, unknown) does not exist
Running as superuser.
",<postgresql><dblink>,429,0,2,521,2,6,9,55,36166,,0,3,35,2018-06-19 20:13,2019-08-05 12:08,,412.0,,Basic,10,"<postgresql><dblink>, ERROR: function dblink(unknown, unknown) does not exist, I have defined a foreign server pointing to another database. I then want to execute a function in that database and get back the results.
When I try this:
SELECT * FROM  dblink('mylink','select someschema.somefunction(''test'', ''ABC'')')
or this:
SELECT t.n FROM  dblink('mylink', 'select * from someschema.mytable') as t(n text)
I get the error:
  ERROR: function dblink(unknown, unknown) does not exist
Running as superuser.
","<postgresql><blink>, error: function blink(unknown, unknown) exist, define foreign server point not database. want execute function database get back results. try this: select * blink('link','select someschema.somefunction(''test'', ''abc'')') this: select t.n blink('link', 'select * someschema.table') t(n text) get error: error: function blink(unknown, unknown) exist run superuser."
53610385,Docker - Postgres and pgAdmin 4 : Connection refused,"Newbie with docker, I am trying to connect throught localhost my pgAdmin container to the postgres one.
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
0b00555238ba        dpage/pgadmin4      ""/entrypoint.sh""         43 minutes ago      Up 43 minutes       0.0.0.0:80-&gt;80/tcp, 443/tcp   pedantic_turing
e79fb6440a95        postgres            ""docker-entrypoint.s…""   About an hour ago   Up About an hour    0.0.0.0:5432-&gt;5432/tcp        pg-docker
I succeed connecting with psql command.
psql -h localhost -U postgres -d postgres
But when I create the server on pgAdmin with the same parameters as psql I got the following error.
  Unable to connect to server:
  could not connect to server: Connection refused Is the server running
  on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on
  port 5432? could not connect to server: Address not available Is the
  server running on host ""localhost"" (::1) and accepting TCP/IP
  connections on port 5432?
I succeed to connect throught the IPAddress given by docker inspect on the container.
By the way, I checked postgresql.conf and assert that listen_addresses = '*' and also that pg_hba.conf contain host all all all md5.
But I don't get it, why shouldn't I be able to use the localhost address ? And why does docker even give me an address that is not local ?
",<postgresql><docker>,1423,0,7,720,1,9,23,74,45493,0.0,26,8,35,2018-12-04 10:05,2019-05-08 7:59,2019-05-28 3:11,155.0,175.0,Basic,9,"<postgresql><docker>, Docker - Postgres and pgAdmin 4 : Connection refused, Newbie with docker, I am trying to connect throught localhost my pgAdmin container to the postgres one.
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
0b00555238ba        dpage/pgadmin4      ""/entrypoint.sh""         43 minutes ago      Up 43 minutes       0.0.0.0:80-&gt;80/tcp, 443/tcp   pedantic_turing
e79fb6440a95        postgres            ""docker-entrypoint.s…""   About an hour ago   Up About an hour    0.0.0.0:5432-&gt;5432/tcp        pg-docker
I succeed connecting with psql command.
psql -h localhost -U postgres -d postgres
But when I create the server on pgAdmin with the same parameters as psql I got the following error.
  Unable to connect to server:
  could not connect to server: Connection refused Is the server running
  on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on
  port 5432? could not connect to server: Address not available Is the
  server running on host ""localhost"" (::1) and accepting TCP/IP
  connections on port 5432?
I succeed to connect throught the IPAddress given by docker inspect on the container.
By the way, I checked postgresql.conf and assert that listen_addresses = '*' and also that pg_hba.conf contain host all all all md5.
But I don't get it, why shouldn't I be able to use the localhost address ? And why does docker even give me an address that is not local ?
","<postgresql><doctor>, doctor - poster pgadmin 4 : connect refused, newby doctor, try connect thought localhost pgadmin contain poster one. contain id image command great state port name 0b00555238ba page/pgadmin4 ""/entrypoint.s"" 43 minute ago 43 minute 0.0.0.0:80-&it;80/top, 443/top pedantic_tur e79fb6440a95 poster ""doctor-entrypoint.s…"" hour ago hour 0.0.0.0:5432-&it;5432/top pg-doctor succeed connect pool command. pool -h localhost -u poster -d poster great server pgadmin parapet pool got follow error. unable connect server: could connect server: connect refuse server run host ""localhost"" (127.0.0.1) accept top/in connect port 5432? could connect server: address avail server run host ""localhost"" (::1) accept top/in connect port 5432? succeed connect thought address given doctor inspect container. way, check postgresql.cone assert listen_address = '*' also pg_hba.cone contain host md. get it, all use localhost address ? doctor even give address local ?"
53434849,Cannot install postgres on Ubuntu (E: Unable to locate package postgresql),"So I'm having this problem where for some reason I can't install any package on my ubuntu system.
I'm currently on Ubuntu 16.10.
terminal install logs
Update:
I've done entered those commands and got this.
after update and apt-cache
What should I do now?
",<postgresql><ubuntu><ubuntu-16.10>,255,2,1,371,1,4,7,35,99536,0.0,6,5,35,2018-11-22 16:15,2018-11-22 16:26,,0.0,,Intermediate,18,"<postgresql><ubuntu><ubuntu-16.10>, Cannot install postgres on Ubuntu (E: Unable to locate package postgresql), So I'm having this problem where for some reason I can't install any package on my ubuntu system.
I'm currently on Ubuntu 16.10.
terminal install logs
Update:
I've done entered those commands and got this.
after update and apt-cache
What should I do now?
","<postgresql><bunt><bunt-16.10>, cannot instal poster bunt (e: unable local package postgresql), i'm problem reason can't instal package bunt system. i'm current bunt 16.10. german instal log update: i'v done enter command got this. update apt-each now?"
49361088,String_agg for SQL Server before 2017,"Can anyone help me make this query work for SQL Server 2014?
This is working on Postgresql and probably on SQL Server 2017. On Oracle it is listagg instead of string_agg.
Here is the SQL:
select 
    string_agg(t.id,',') AS id
from 
    Table t
I checked on the site some xml option should be used but I could not understand it.
",<sql><sql-server><string-aggregation><string-agg>,329,0,6,1648,4,21,38,36,43394,0.0,197,2,35,2018-03-19 10:47,2018-03-19 10:49,2018-03-19 10:49,0.0,0.0,Intermediate,18,"<sql><sql-server><string-aggregation><string-agg>, String_agg for SQL Server before 2017, Can anyone help me make this query work for SQL Server 2014?
This is working on Postgresql and probably on SQL Server 2017. On Oracle it is listagg instead of string_agg.
Here is the SQL:
select 
    string_agg(t.id,',') AS id
from 
    Table t
I checked on the site some xml option should be used but I could not understand it.
","<sal><sal-server><string-aggregation><string-age>, string_agg sal server 2017, anyone help make query work sal server 2014? work postgresql probably sal server 2017. oral listing instead string_agg. sal: select string_agg(t.id,',') id table check site all option use could understand it."
62697071,"Docker-Compose postgres upgrade initdb: error: directory ""/var/lib/postgresql/data"" exists but is not empty","I had postgres 11 installed using docker-compose. I wanted to upgrade it to 12 but even though I have removed the container and its volume but the status of the container says &quot;Restarting&quot;.
Here is my docker-compose file
version: '3.5'
services:
  postgres:
    image: postgres:12
    environment:
      POSTGRES_HOST_AUTH_METHOD: &quot;trust&quot;
    ports:
    - &quot;5432&quot;
    restart: always
    volumes:
    - /etc/postgresql/12/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
    - db_data:/var/lib/postgresql/data
volumes:
  db_data:
However it is not working and the logs has the following issue
2020-07-02T12:54:47.012973448Z The files belonging to this database system will be owned by user &quot;postgres&quot;.
2020-07-02T12:54:47.013030445Z This user must also own the server process.
2020-07-02T12:54:47.013068962Z 
2020-07-02T12:54:47.013222608Z The database cluster will be initialized with locale &quot;en_US.utf8&quot;.
2020-07-02T12:54:47.013261425Z The default database encoding has accordingly been set to &quot;UTF8&quot;.
2020-07-02T12:54:47.013281815Z The default text search configuration will be set to &quot;english&quot;.
2020-07-02T12:54:47.013293326Z 
2020-07-02T12:54:47.013303793Z Data page checksums are disabled.
2020-07-02T12:54:47.013313919Z 
2020-07-02T12:54:47.013450079Z initdb: error: directory &quot;/var/lib/postgresql/data&quot; exists but is not empty
2020-07-02T12:54:47.013487706Z If you want to create a new database system, either remove or empty
2020-07-02T12:54:47.013501126Z the directory &quot;/var/lib/postgresql/data&quot; or run initdb
2020-07-02T12:54:47.013512379Z with an argument other than &quot;/var/lib/postgresql/data&quot;.
How could I remove or empty this /var/lib/postgresql/data when the container is constantly restarting?
Thanks in advance
",<postgresql><docker><docker-compose>,1835,0,27,495,2,6,16,49,60805,0.0,10,6,34,2020-07-02 13:06,2020-09-10 7:21,,70.0,,Intermediate,19,"<postgresql><docker><docker-compose>, Docker-Compose postgres upgrade initdb: error: directory ""/var/lib/postgresql/data"" exists but is not empty, I had postgres 11 installed using docker-compose. I wanted to upgrade it to 12 but even though I have removed the container and its volume but the status of the container says &quot;Restarting&quot;.
Here is my docker-compose file
version: '3.5'
services:
  postgres:
    image: postgres:12
    environment:
      POSTGRES_HOST_AUTH_METHOD: &quot;trust&quot;
    ports:
    - &quot;5432&quot;
    restart: always
    volumes:
    - /etc/postgresql/12/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
    - db_data:/var/lib/postgresql/data
volumes:
  db_data:
However it is not working and the logs has the following issue
2020-07-02T12:54:47.012973448Z The files belonging to this database system will be owned by user &quot;postgres&quot;.
2020-07-02T12:54:47.013030445Z This user must also own the server process.
2020-07-02T12:54:47.013068962Z 
2020-07-02T12:54:47.013222608Z The database cluster will be initialized with locale &quot;en_US.utf8&quot;.
2020-07-02T12:54:47.013261425Z The default database encoding has accordingly been set to &quot;UTF8&quot;.
2020-07-02T12:54:47.013281815Z The default text search configuration will be set to &quot;english&quot;.
2020-07-02T12:54:47.013293326Z 
2020-07-02T12:54:47.013303793Z Data page checksums are disabled.
2020-07-02T12:54:47.013313919Z 
2020-07-02T12:54:47.013450079Z initdb: error: directory &quot;/var/lib/postgresql/data&quot; exists but is not empty
2020-07-02T12:54:47.013487706Z If you want to create a new database system, either remove or empty
2020-07-02T12:54:47.013501126Z the directory &quot;/var/lib/postgresql/data&quot; or run initdb
2020-07-02T12:54:47.013512379Z with an argument other than &quot;/var/lib/postgresql/data&quot;.
How could I remove or empty this /var/lib/postgresql/data when the container is constantly restarting?
Thanks in advance
","<postgresql><doctor><doctor-compose>, doctor-compose poster upgrade initdb: error: director ""/war/limb/postgresql/data"" exist empty, poster 11 instal use doctor-compose. want upgrade 12 even though remove contain volume state contain say &quit;starting&quit;. doctor-compose file version: '3.5' services: postures: image: postures:12 environment: postgres_host_auth_method: &quit;trust&quit; ports: - &quit;5432&quit; start: away volumes: - /etc/postgresql/12/postgresql.cone:/war/limb/postgresql/data/postgresql.cone - db_data:/war/limb/postgresql/data volumes: db_data: howe work log follow issue 2020-07-02t12:54:47.012973448z file belong database system own user &quit;postures&quit;. 2020-07-02t12:54:47.013030445z user must also server process. 2020-07-02t12:54:47.013068962z 2020-07-02t12:54:47.013222608z database cluster into local &quit;ends.utf&quit;. 2020-07-02t12:54:47.013261425z default database end accordingly set &quit;utf&quit;. 2020-07-02t12:54:47.013281815z default text search configur set &quit;english&quit;. 2020-07-02t12:54:47.013293326z 2020-07-02t12:54:47.013303793z data page checks disabled. 2020-07-02t12:54:47.013313919z 2020-07-02t12:54:47.013450079z initdb: error: director &quit;/war/limb/postgresql/data&quit; exist empty 2020-07-02t12:54:47.013487706z want great new database system, either remove empty 2020-07-02t12:54:47.013501126z director &quit;/war/limb/postgresql/data&quit; run initdb 2020-07-02t12:54:47.013512379z argument &quit;/war/limb/postgresql/data&quit;. could remove empty /war/limb/postgresql/data contain constantly starting? thank advance"
54472165,What is wrong with this code. Why can't I use SqlConnection?,"I am 100% newbie to SQl and wanted to make a ConsoleApp with the use of database. I read some about it and tried. When I needed to make SqlConnection, my VS 2019 Preview showed me this 
  Severity   Code    Description Project File    Line    Suppression State
  Error
      CS1069  The type name 'SqlConnection' could not be found in the namespace 'System.Data.SqlClient'.
   This type has been forwarded to assembly 'System.Data.SqlClient, Version=0.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' Consider adding a reference to that assembly.
      ConsoleApp1 C:\Users\User\Desktop\Bald Code\ConsoleApp1\ConsoleApp1\Program.cs  12
      Active
i don't get why it doesn't work
Here's my code
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Program
    {
        static void Main(string[] args)
        {
            string connectionString;
            SqlConnection cnn;
        }
    }
}
",<c#><sql><sqlclient>,950,0,19,479,1,4,6,63,60516,0.0,2,11,34,2019-02-01 2:44,2019-02-01 2:50,,0.0,,Basic,2,"<c#><sql><sqlclient>, What is wrong with this code. Why can't I use SqlConnection?, I am 100% newbie to SQl and wanted to make a ConsoleApp with the use of database. I read some about it and tried. When I needed to make SqlConnection, my VS 2019 Preview showed me this 
  Severity   Code    Description Project File    Line    Suppression State
  Error
      CS1069  The type name 'SqlConnection' could not be found in the namespace 'System.Data.SqlClient'.
   This type has been forwarded to assembly 'System.Data.SqlClient, Version=0.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' Consider adding a reference to that assembly.
      ConsoleApp1 C:\Users\User\Desktop\Bald Code\ConsoleApp1\ConsoleApp1\Program.cs  12
      Active
i don't get why it doesn't work
Here's my code
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Program
    {
        static void Main(string[] args)
        {
            string connectionString;
            SqlConnection cnn;
        }
    }
}
","<c#><sal><sqlclient>, wrong code. can't use sqlconnection?, 100% newby sal want make consoleapp use database. read tried. need make sqlconnection, vs 2019 review show never code rescript project file line suppress state error cs1069 type name 'sqlconnection' could found namespac 'system.data.sqlclient'. type forward assembly 'system.data.sqlclient, version=0.0.0.0, culture=neutral, publickeytoken=b03f5f7f11d50a3a' consider ad refer assembly. consoleapp1 c:\users\user\desktop\bald code\consoleapp1\consoleapp1\program.c 12 active get work here' code use system; use system.data; use system.data.sqlclient; namespac consoleapp1 { class program { static void main(string[] arms) { string connectionstring; sqlconnect can; } } }"
50770862,The certificate chain was issued by an authority that is not trusted,"I studied the help here on upgrading to aspnetcore 2.1.0
My database is SQLExpress 2016SP1
I am able to add a migration but when I issue 
update-database
at the Package Manager Console, I get an error
  A connection was successfully established with the server, but then an error occurred during the login process.
  (provider: SSL Provider, error: 0 - The certificate chain was issued by an authority that is not trusted.)
The connection string is of the form 
   Server=""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true
The DbContext is
public class ApiDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApiDbContext(DbContextOptions&lt;ApiDbContext&gt; options)
        : base(options)
    {
    }
}
The Context Factory is
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var builder = new ConfigurationBuilder();
        builder.AddJsonFile(""appsettings.json"");
        var config = builder.Build();
        var connectionString = config.GetConnectionString(""MyDatabase"");
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
[Update]
If I hard code the connection string inside the implementation of  IDesignTimeDbContextFactory  then I can run the migrations
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var connectionString =    ""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true"";
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
Hard coding the connection string is not desirable so I would like a better answer.
I am unclear as to why the implementation of  IDesignTimeDbContextFactory is needed.  (my migration does fail without it )
I have updated my Startup.cs and Progam.cs to match those of a newly generated program where the implementation is not needed.
 public class Program
 {
   public static void Main(string[] args)
    {
        CreateWebHostBuilder(args).Build().Run();
    }
    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .UseStartup&lt;Startup&gt;();
}
public class Startup
{
    public Startup(IConfiguration configuration)
    {
        Configuration = configuration;
    }
    public IConfiguration Configuration { get; }
    // This method gets called by the runtime. Use this method to add services to the container.
    public void ConfigureServices(IServiceCollection services)
    {
        services.Configure&lt;CookiePolicyOptions&gt;(options =&gt;
        {
            options.CheckConsentNeeded = context =&gt; true;
            options.MinimumSameSitePolicy = SameSiteMode.None;
        });
        services.AddDbContext&lt;ApiDbContext&gt;(options =&gt;
            options.UseSqlServer(
                Configuration.GetConnectionString(""MyDatabase"")));
        services.AddDefaultIdentity&lt;IdentityUser&gt;()
            .AddEntityFrameworkStores&lt;ApiDbContext&gt;();
services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
    // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {
        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
            app.UseDatabaseErrorPage();
        }
        else
        {
            app.UseExceptionHandler(""/Error"");
            app.UseHsts();
        }
        app.UseHttpsRedirection();
        app.UseStaticFiles();
        app.UseCookiePolicy();
        app.UseAuthentication();
        app.UseMvc();
    }
}
[Update]
I updated to VS2017 version 15.7.3.
When I went to repeat the problem and create the first migration, PM displayed the message
The configuration file 'appsettings.json' was not found and is not optional
When I marked this file copy always then both the add-migration and the update-database worked.
",<sql-server><asp.net-core><asp.net-identity><entity-framework-core>,4400,1,91,16220,42,184,322,41,48600,,1471,4,34,2018-06-09 4:15,2018-06-11 16:35,2018-06-11 16:35,2.0,2.0,Basic,14,"<sql-server><asp.net-core><asp.net-identity><entity-framework-core>, The certificate chain was issued by an authority that is not trusted, I studied the help here on upgrading to aspnetcore 2.1.0
My database is SQLExpress 2016SP1
I am able to add a migration but when I issue 
update-database
at the Package Manager Console, I get an error
  A connection was successfully established with the server, but then an error occurred during the login process.
  (provider: SSL Provider, error: 0 - The certificate chain was issued by an authority that is not trusted.)
The connection string is of the form 
   Server=""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true
The DbContext is
public class ApiDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApiDbContext(DbContextOptions&lt;ApiDbContext&gt; options)
        : base(options)
    {
    }
}
The Context Factory is
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var builder = new ConfigurationBuilder();
        builder.AddJsonFile(""appsettings.json"");
        var config = builder.Build();
        var connectionString = config.GetConnectionString(""MyDatabase"");
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
[Update]
If I hard code the connection string inside the implementation of  IDesignTimeDbContextFactory  then I can run the migrations
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var connectionString =    ""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true"";
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
Hard coding the connection string is not desirable so I would like a better answer.
I am unclear as to why the implementation of  IDesignTimeDbContextFactory is needed.  (my migration does fail without it )
I have updated my Startup.cs and Progam.cs to match those of a newly generated program where the implementation is not needed.
 public class Program
 {
   public static void Main(string[] args)
    {
        CreateWebHostBuilder(args).Build().Run();
    }
    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .UseStartup&lt;Startup&gt;();
}
public class Startup
{
    public Startup(IConfiguration configuration)
    {
        Configuration = configuration;
    }
    public IConfiguration Configuration { get; }
    // This method gets called by the runtime. Use this method to add services to the container.
    public void ConfigureServices(IServiceCollection services)
    {
        services.Configure&lt;CookiePolicyOptions&gt;(options =&gt;
        {
            options.CheckConsentNeeded = context =&gt; true;
            options.MinimumSameSitePolicy = SameSiteMode.None;
        });
        services.AddDbContext&lt;ApiDbContext&gt;(options =&gt;
            options.UseSqlServer(
                Configuration.GetConnectionString(""MyDatabase"")));
        services.AddDefaultIdentity&lt;IdentityUser&gt;()
            .AddEntityFrameworkStores&lt;ApiDbContext&gt;();
services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
    // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {
        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
            app.UseDatabaseErrorPage();
        }
        else
        {
            app.UseExceptionHandler(""/Error"");
            app.UseHsts();
        }
        app.UseHttpsRedirection();
        app.UseStaticFiles();
        app.UseCookiePolicy();
        app.UseAuthentication();
        app.UseMvc();
    }
}
[Update]
I updated to VS2017 version 15.7.3.
When I went to repeat the problem and create the first migration, PM displayed the message
The configuration file 'appsettings.json' was not found and is not optional
When I marked this file copy always then both the add-migration and the update-database worked.
","<sal-server><asp.net-core><asp.net-identity><entity-framework-core>, certify chain issue author trusted, study help upgrade aspnetcor 2.1.0 database sqlexpress 2016sp1 all add migrate issue update-database package manage console, get error connect success establish server, error occur login process. (provider: sal provider, error: 0 - certify chain issue author trusted.) connect string form server=""server=observer;into catalogue=database;trusted_connection=true;multipleactiveresultsets=true context public class apidbcontext : identitydbcontext&it;applicationuser&it; { public apidbcontext(dbcontextoptions&it;apidbcontext&it; option) : base(option) { } } context factor public class mycontextfactori : idesigntimedbcontextfactory&it;apidbcontext&it; { public apidbcontext createdbcontext(string[] arms) { war optionsbuild = new dbcontextoptionsbuilder&it;apidbcontext&it;(); war builder = new configurationbuilder(); builder.addjsonfile(""appsettings.son""); war confirm = builder.build(); war connections = confirm.getconnectionstring(""database""); optionsbuilder.usesqlserver(connectionstring); return new apidbcontext(optionsbuilder.option); } } [update] hard code connect string inside implement idesigntimedbcontextfactori run migrate public class mycontextfactori : idesigntimedbcontextfactory&it;apidbcontext&it; { public apidbcontext createdbcontext(string[] arms) { war optionsbuild = new dbcontextoptionsbuilder&it;apidbcontext&it;(); war connections = ""server=observer;into catalogue=database;trusted_connection=true;multipleactiveresultsets=true""; optionsbuilder.usesqlserver(connectionstring); return new apidbcontext(optionsbuilder.option); } } hard code connect string desire would like better answer. unclear implement idesigntimedbcontextfactori needed. (mi migrate fail without ) update started.c program.c match newly genet program implement needed. public class program { public static void main(string[] arms) { createwebhostbuilder(arms).build().run(); } public static iwebhostbuild createwebhostbuilder(string[] arms) =&it; webhost.createdefaultbuilder(arms) .usestartup&it;started&it;(); } public class started { public started(iconfigur configuration) { configur = configuration; } public iconfigur configur { get; } // method get call auntie. use method add service container. public void configureservices(iservicecollect services) { services.configure&it;cookiepolicyoptions&it;(opt =&it; { option.checkconsentneed = context =&it; true; option.minimumsamesitepolici = samesitemode.none; }); services.adddbcontext&it;apidbcontext&it;(opt =&it; option.usesqlserver( configuration.getconnectionstring(""database""))); services.adddefaultidentity&it;identityuser&it;() .addentityframeworkstores&it;apidbcontext&it;(); services.addmvc().setcompatibilityversion(compatibilityversion.version_2_1); } // method get call auntie. use method configur http request pipeline. public void configure(iapplicationbuild pp, ihostingenviron end) { (end.development()) { pp.usedeveloperexceptionpage(); pp.usedatabaseerrorpage(); } else { pp.useexceptionhandler(""/error""); pp.usehsts(); } pp.usehttpsredirection(); pp.usestaticfiles(); pp.usecookiepolicy(); pp.useauthentication(); pp.usemvc(); } } [update] update vs2017 version 15.7.3. went repeat problem great first migration, pm display message configur file 'appsettings.son' found option mark file copy away add-might update-database worked."
48550653,JOIN same table twice with aliases on SQLAlchemy,"I am trying to port the following query to SQLAlchemy:
SELECT u.username, GROUP_CONCAT(DISTINCT userS.name)
FROM Skills AS filterS 
INNER JOIN UserSkills AS ufs ON filterS.id = ufs.skill_id
INNER JOIN Users AS u ON ufs.user_id = u.id
INNER JOIN UserSkills AS us ON u.id = us.user_id
INNER JOIN Skills AS userS ON us.skill_id = userS.id
WHERE filterS.name IN ('C#', 'SQL')
GROUP BY u.id;
I don't understand how to achieve AS statement in SQLAlchemy. Here is what I currently have:
# User class has attribute skills, that points to class UserSkill
# UserSkill class has attribute skill, that points to class Skill
db.session.query(User.id, User.username, func.group_concat(Skill.name).label('skills')).\
   join(User.skills).\
   join(UserSkill.skill).filter(Skill.id.in_(skillIds)).\
   order_by(desc(func.count(Skill.id))).\
   group_by(User.id).all()
Please help.
",<python><sqlalchemy>,865,0,15,2465,2,22,34,40,36687,0.0,636,2,34,2018-01-31 20:10,2018-02-01 16:18,2018-02-01 16:18,1.0,1.0,Basic,2,"<python><sqlalchemy>, JOIN same table twice with aliases on SQLAlchemy, I am trying to port the following query to SQLAlchemy:
SELECT u.username, GROUP_CONCAT(DISTINCT userS.name)
FROM Skills AS filterS 
INNER JOIN UserSkills AS ufs ON filterS.id = ufs.skill_id
INNER JOIN Users AS u ON ufs.user_id = u.id
INNER JOIN UserSkills AS us ON u.id = us.user_id
INNER JOIN Skills AS userS ON us.skill_id = userS.id
WHERE filterS.name IN ('C#', 'SQL')
GROUP BY u.id;
I don't understand how to achieve AS statement in SQLAlchemy. Here is what I currently have:
# User class has attribute skills, that points to class UserSkill
# UserSkill class has attribute skill, that points to class Skill
db.session.query(User.id, User.username, func.group_concat(Skill.name).label('skills')).\
   join(User.skills).\
   join(UserSkill.skill).filter(Skill.id.in_(skillIds)).\
   order_by(desc(func.count(Skill.id))).\
   group_by(User.id).all()
Please help.
","<patron><sqlalchemy>, join table twice alias sqlalchemy, try port follow query sqlalchemy: select u.surname, group_concat(distinct users.name) skill filter inner join userskil of filters.id = us.skilled inner join user u us.user_id = u.id inner join userskil us u.id = us.user_id inner join skill user us.skilled = users.id filters.am ('c#', 'sal') group u.id; understand achieve statement sqlalchemy. current have: # user class attribute skill, point class userskil # userskil class attribute skill, point class skill do.session.query(user.id, user.surname, fun.group_concat(skill.name).label('skill')).\ join(user.skill).\ join(userskill.skill).filter(skill.id.in(skilled)).\ orderly(desk(fun.count(skill.id))).\ group_by(user.id).all() pleas help."
48815341,Why is Apache-Spark - Python so slow locally as compared to pandas?,"A Spark newbie here.
I recently started playing around with Spark on my local machine on two cores by using the command:
pyspark --master local[2]
I have a 393Mb text file which has almost a million rows. I wanted to perform some data manipulation operation. I am using the built-in dataframe functions of PySpark to perform simple operations like groupBy, sum, max, stddev.
However, when I do the exact same operations in pandas on the exact same dataset, pandas seems to defeat pyspark by a huge margin in terms of latency.
I was wondering what could be a possible reason for this. I have a couple of thoughts.
Do built-in functions do the process of serialization/de-serialization inefficiently? If yes, what are the alternatives to them?
Is the data set too small that it cannot outrun the overhead cost of the underlying JVM on which spark runs?
Thanks for looking. Much appreciated.
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,889,0,5,447,0,6,11,81,9139,0.0,3,1,34,2018-02-15 20:01,2018-02-15 20:26,2018-02-15 20:26,0.0,0.0,Intermediate,23,"<python><pandas><apache-spark><pyspark><apache-spark-sql>, Why is Apache-Spark - Python so slow locally as compared to pandas?, A Spark newbie here.
I recently started playing around with Spark on my local machine on two cores by using the command:
pyspark --master local[2]
I have a 393Mb text file which has almost a million rows. I wanted to perform some data manipulation operation. I am using the built-in dataframe functions of PySpark to perform simple operations like groupBy, sum, max, stddev.
However, when I do the exact same operations in pandas on the exact same dataset, pandas seems to defeat pyspark by a huge margin in terms of latency.
I was wondering what could be a possible reason for this. I have a couple of thoughts.
Do built-in functions do the process of serialization/de-serialization inefficiently? If yes, what are the alternatives to them?
Is the data set too small that it cannot outrun the overhead cost of the underlying JVM on which spark runs?
Thanks for looking. Much appreciated.
","<patron><hands><apache-spark><spark><apache-spark-sal>, apache-spark - patron slow local compare hands?, spark newby here. recent start play around spark local machine two core use command: spark --master local[2] 393mb text file almost million rows. want perform data manipul operation. use built-in datafram function spark perform simple over like group, sum, max, sudden. however, exact over and exact dataset, and seem defeat spark huge margin term lately. wonder could possible reason this. couple thoughts. built-in function process serialization/de-peri efficiently? yes, alter them? data set small cannot outran overhead cost underlip jem spark runs? thank looking. much appreciated."
49518683,The server time zone value 'CEST' is unrecognized,"I am using hibernate (Hibernate Maven 5.2.15.Final, Mysql-connector Maven 8.0.9-rc) whith mysql 5.7 on lampp environment on linux so. 
I am in Italy (Central European Summer Time) and once March 25, occurs follow error on connection db: 
  The server time zone value 'CEST' is unrecognized or represents more
  than one time zone. You must configure either the server or JDBC
  driver (via the serverTimezone configuration property) to use a more
  specifc time zone value if you want to utilize time zone support.
On mysql console, I ran: 
SHOW GLOBAL VARIABLES LIKE 'time_zone';
SET GLOBAL time_zone='Europe/Rome'; 
but that did not persist. 
Then I added to my.cnf file (in /etc/mysql):
[mysqld] 
default-time-zone = 'Europe/Rome' 
and also:
default_time_zone = 'Europe/Rome' 
but the db server did not start still... 
Why does the error occur? 
Could someone help me? 
Thank you!
",<mysql><timezone><lamp>,884,0,6,489,1,5,9,61,51370,0.0,1,4,33,2018-03-27 17:06,2018-05-06 15:40,,40.0,,Basic,14,"<mysql><timezone><lamp>, The server time zone value 'CEST' is unrecognized, I am using hibernate (Hibernate Maven 5.2.15.Final, Mysql-connector Maven 8.0.9-rc) whith mysql 5.7 on lampp environment on linux so. 
I am in Italy (Central European Summer Time) and once March 25, occurs follow error on connection db: 
  The server time zone value 'CEST' is unrecognized or represents more
  than one time zone. You must configure either the server or JDBC
  driver (via the serverTimezone configuration property) to use a more
  specifc time zone value if you want to utilize time zone support.
On mysql console, I ran: 
SHOW GLOBAL VARIABLES LIKE 'time_zone';
SET GLOBAL time_zone='Europe/Rome'; 
but that did not persist. 
Then I added to my.cnf file (in /etc/mysql):
[mysqld] 
default-time-zone = 'Europe/Rome' 
and also:
default_time_zone = 'Europe/Rome' 
but the db server did not start still... 
Why does the error occur? 
Could someone help me? 
Thank you!
","<myself><timezone><lamp>, server time zone value 'west' recognized, use wiberd (wiberd haven 5.2.15.final, myself-connection haven 8.0.9-re) with myself 5.7 lamp environs line so. italy (central european summer time) march 25, occur follow error connect do: server time zone value 'west' unrecogn repress one time zone. must configur either server job driver (via servertimezon configur property) use specific time zone value want until time zone support. myself console, ran: show global variable like 'time_zone'; set global time_zone='europe/rome'; persist. ad my.cf file (in /etc/myself): [myself] default-time-on = 'europe/rome' also: default_time_zon = 'europe/rome' do server start still... error occur? could someone help me? thank you!"
55958103,Tests using embedded postgres fail with Illegal State Exception,"I'm running some tests against an embedded postgres database using otj-pg-embedded. While the tests run fine locally they fail when run by Gitlab-CI with an Illegal State Exception. 
Gitlab CI builds it and runs tests that don't include otj-pg-embedded just fine.
I've commented out most of the test class and pinpointed the problem to:  
public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
import com.goldfinger.models.AuditLog;
import com.opentable.db.postgres.embedded.FlywayPreparer;
import com.opentable.db.postgres.junit.EmbeddedPostgresRules;
import com.opentable.db.postgres.junit.PreparedDbRule;
import org.junit.*;
import org.junit.runner.RunWith;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;
@RunWith(SpringJUnit4ClassRunner.class)
public class SQLAuditRepositoryTest {
    private static SQLAuditRepository sqlAuditRepository;
    private static AuditLog auditLog_1;
    private static AuditLog auditLog_2;
    private static AuditLog auditLog_3;
    private static List&lt;AuditLog&gt; auditLogList;
    @ClassRule
        public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
    @Test
    public void simpleTest() {
        assert (2 == 2);
    }
}
This is the stack trace:
java.lang.IllegalStateException: Process [/tmp/embedded-pg/PG-06e3a92a2edb6ddd6dbdf5602d0252ca/bin/initdb, -A, trust, -U, postgres, -D, /tmp/epg6584640257265165384, -E, UTF-8] failed
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.system(EmbeddedPostgres.java:626)
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.initdb(EmbeddedPostgres.java:240)
...
... many lines here
...
    at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
    at java.lang.Thread.run(Thread.java:745)
This is the gitlab-ci.yml
image: java:latest
services:
  - postgres:latest
before_script:
  - export GRADLE_USER_HOME=`pwd`/.gradle
package:
  stage: build
  script:
    - ./gradlew assemble
test:
  stage: test
  script:
  - ./gradlew check
  artifacts:
    reports:
      junit: build/test-results/test/*.xml
Any help will be appreciated.
",<java><postgresql><gitlab-ci>,2249,0,55,331,1,3,4,71,19067,0.0,1,7,33,2019-05-02 18:18,2019-11-07 9:16,,189.0,,Basic,14,"<java><postgresql><gitlab-ci>, Tests using embedded postgres fail with Illegal State Exception, I'm running some tests against an embedded postgres database using otj-pg-embedded. While the tests run fine locally they fail when run by Gitlab-CI with an Illegal State Exception. 
Gitlab CI builds it and runs tests that don't include otj-pg-embedded just fine.
I've commented out most of the test class and pinpointed the problem to:  
public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
import com.goldfinger.models.AuditLog;
import com.opentable.db.postgres.embedded.FlywayPreparer;
import com.opentable.db.postgres.junit.EmbeddedPostgresRules;
import com.opentable.db.postgres.junit.PreparedDbRule;
import org.junit.*;
import org.junit.runner.RunWith;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;
@RunWith(SpringJUnit4ClassRunner.class)
public class SQLAuditRepositoryTest {
    private static SQLAuditRepository sqlAuditRepository;
    private static AuditLog auditLog_1;
    private static AuditLog auditLog_2;
    private static AuditLog auditLog_3;
    private static List&lt;AuditLog&gt; auditLogList;
    @ClassRule
        public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
    @Test
    public void simpleTest() {
        assert (2 == 2);
    }
}
This is the stack trace:
java.lang.IllegalStateException: Process [/tmp/embedded-pg/PG-06e3a92a2edb6ddd6dbdf5602d0252ca/bin/initdb, -A, trust, -U, postgres, -D, /tmp/epg6584640257265165384, -E, UTF-8] failed
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.system(EmbeddedPostgres.java:626)
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.initdb(EmbeddedPostgres.java:240)
...
... many lines here
...
    at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
    at java.lang.Thread.run(Thread.java:745)
This is the gitlab-ci.yml
image: java:latest
services:
  - postgres:latest
before_script:
  - export GRADLE_USER_HOME=`pwd`/.gradle
package:
  stage: build
  script:
    - ./gradlew assemble
test:
  stage: test
  script:
  - ./gradlew check
  artifacts:
    reports:
      junit: build/test-results/test/*.xml
Any help will be appreciated.
","<cava><postgresql><gitlab-i>, test use ebbed poster fail killed state exception, i'm run test ebbed poster database use of-pg-embedded. test run fine local fail run gitlab-i killed state exception. gitlab i build run test include of-pg-ebbed fine. i'v comment test class pinpoint problem to: public static singleinstancepostgresrul pg = embeddedpostgresrules.singleinstance(); import com.goldfinger.models.auditor; import com.operable.do.postures.embedded.flywaypreparer; import com.operable.do.postures.unit.embeddedpostgresrules; import com.operable.do.postures.unit.prepareddbrule; import org.unit.*; import org.unit.runner.runwith; import org.springframework.job.core.namedparam.namedparameterjdbctemplate; import org.springframework.test.context.unite.springjunit4classrunner; @runwith(springjunit4classrunner.class) public class sqlauditrepositorytest { privat static sqlauditrepositori sqlauditrepository; privat static auditor auditlog_1; privat static auditor auditlog_2; privat static auditor auditlog_3; privat static list&it;auditor&it; auditloglist; @classrul public static singleinstancepostgresrul pg = embeddedpostgresrules.singleinstance(); @test public void simplest() { assert (2 == 2); } } stick trace: cava.long.illegalstateexception: process [/tm/embedded-pg/pg-06e3a92a2edb6ddd6dbdf5602d0252ca/bin/initdb, -a, trust, -u, postures, -d, /tm/epg6584640257265165384, -e, utf-8] fail com.operable.do.postures.embedded.embeddedpostgres.system(embeddedpostgres.cava:626) com.operable.do.postures.embedded.embeddedpostgres.initdb(embeddedpostgres.cava:240) ... ... man line ... org.grade.internal.concurrent.threadfactoryimpl$managedthreadrunnable.run(threadfactoryimpl.cava:55) cava.long.thread.run(thread.cava:745) gitlab-i.you image: cava:latest services: - postures:latest before_script: - export gradle_user_home=`pad`/.grade package: stage: build script: - ./grade assembly test: stage: test script: - ./grade check artifacts: reports: unit: build/test-results/test/*.all help appreciated."
50709059,MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis teners added. Use emitter.setMaxListeners() to increase limit,"I know this might flag as a duplicate solution but the solution on stack overflow is not working for me.
Problem
(node:5716) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis
teners added. Use emitter.setMaxListeners() to increase limit.
My codebase is huge and I facing this error sometimes I don't know why it is happening. I tried to increase the listeners limit but unfortunately, it is not working.
const EventEmitter = require('events');
const emitter = new EventEmitter()
emitter.setMaxListeners(50)
UPDATE
After Some browsing, I run this command to trace warning
node --trace-warnings index.babel.js
Turns out be my socket.io code is the problem I am using socket.io with Redis. This is the error
node:14212) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message li
steners added. Use emitter.setMaxListeners() to increase limit
    at _addListener (events.js:281:19)
    at RedisClient.addListener (events.js:298:10)
    at Namespace.&lt;anonymous&gt; (D:/newProject/services/socket.js:21:17)
    at emitOne (events.js:115:13)
    at Namespace.emit (events.js:210:7)
    at Namespace.emit (D:\newProject\node_modules\socket.io\lib\namespace.js:213:10)
    at D:\newProject\node_modules\socket.io\lib\namespace.js:181:14
    at _combinedTickCallback (internal/process/next_tick.js:131:7)
    at process._tickCallback (internal/process/next_tick.js:180:9)
this is the code (But this code is for more specific tasks it will not execute all the time).
const redis = require('redis');
const config = require('../config');
const sub = redis.createClient(config.REDIS.port, config.REDIS.host);
const pub = redis.createClient(config.REDIS.port, config.REDIS.host);
sub.subscribe('spread');
module.exports = io =&gt; {
  io.on('connection', socket =&gt; {
    /* To find the User Login  */
    let passport = socket.handshake.session.passport; 
    if (typeof passport !== 'undefined') {
      socket.on('typing:send', data =&gt; {
        pub.publish('spread', JSON.stringify(data));
      });
      sub.on('message', (ch, msg) =&gt; {
        // This is the Exact line where I am getting this error
        io.emit(`${JSON.parse(msg).commonID}:receive`, { ...JSON.parse(msg) });
      });
    }
  });
};
",<javascript><mysql><node.js><reactjs>,2274,0,41,2420,6,35,77,42,119087,0.0,77,4,33,2018-06-05 21:16,2018-06-05 22:11,2018-07-04 6:03,0.0,29.0,Basic,14,"<javascript><mysql><node.js><reactjs>, MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis teners added. Use emitter.setMaxListeners() to increase limit, I know this might flag as a duplicate solution but the solution on stack overflow is not working for me.
Problem
(node:5716) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis
teners added. Use emitter.setMaxListeners() to increase limit.
My codebase is huge and I facing this error sometimes I don't know why it is happening. I tried to increase the listeners limit but unfortunately, it is not working.
const EventEmitter = require('events');
const emitter = new EventEmitter()
emitter.setMaxListeners(50)
UPDATE
After Some browsing, I run this command to trace warning
node --trace-warnings index.babel.js
Turns out be my socket.io code is the problem I am using socket.io with Redis. This is the error
node:14212) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message li
steners added. Use emitter.setMaxListeners() to increase limit
    at _addListener (events.js:281:19)
    at RedisClient.addListener (events.js:298:10)
    at Namespace.&lt;anonymous&gt; (D:/newProject/services/socket.js:21:17)
    at emitOne (events.js:115:13)
    at Namespace.emit (events.js:210:7)
    at Namespace.emit (D:\newProject\node_modules\socket.io\lib\namespace.js:213:10)
    at D:\newProject\node_modules\socket.io\lib\namespace.js:181:14
    at _combinedTickCallback (internal/process/next_tick.js:131:7)
    at process._tickCallback (internal/process/next_tick.js:180:9)
this is the code (But this code is for more specific tasks it will not execute all the time).
const redis = require('redis');
const config = require('../config');
const sub = redis.createClient(config.REDIS.port, config.REDIS.host);
const pub = redis.createClient(config.REDIS.port, config.REDIS.host);
sub.subscribe('spread');
module.exports = io =&gt; {
  io.on('connection', socket =&gt; {
    /* To find the User Login  */
    let passport = socket.handshake.session.passport; 
    if (typeof passport !== 'undefined') {
      socket.on('typing:send', data =&gt; {
        pub.publish('spread', JSON.stringify(data));
      });
      sub.on('message', (ch, msg) =&gt; {
        // This is the Exact line where I am getting this error
        io.emit(`${JSON.parse(msg).commonID}:receive`, { ...JSON.parse(msg) });
      });
    }
  });
};
","<javascript><myself><node.is><reacts>, maxlistenersexceededwarning: possible eventemitt memory leak detected. 11 message li tender added. use emitted.setmaxlisteners() increase limit, know might flag public slut slut stick overflow work me. problem (node:5716) maxlistenersexceededwarning: possible eventemitt memory leak detected. 11 message li tender added. use emitted.setmaxlisteners() increase limit. codes huge face error sometime know happening. try increase listen limit unfortunately, working. cost eventemitt = require('events'); cost emit = new eventemitter() emitted.setmaxlisteners(50) update browsing, run command trace warn node --trace-warn index.babel.j turn socket.to code problem use socket.to red. error node:14212) maxlistenersexceededwarning: possible eventemitt memory leak detected. 11 message li stoner added. use emitted.setmaxlisteners() increase limit _addlisten (events.is:281:19) redisclient.addlisten (events.is:298:10) namesake.&it;anonymous&it; (d:/newproject/services/socket.is:21:17) emotion (events.is:115:13) namesake.emit (events.is:210:7) namesake.emit (d:\newproject\node_modules\socket.to\limb\namesake.is:213:10) d:\newproject\node_modules\socket.to\limb\namesake.is:181:14 _combinedtickcallback (internal/process/next_tick.is:131:7) process._tickcallback (internal/process/next_tick.is:180:9) code (but code specie task execute time). cost red = require('red'); cost confirm = require('../confirm'); cost sub = red.createclient(confirm.red.port, confirm.red.host); cost pub = red.createclient(confirm.red.port, confirm.red.host); sub.subscribe('spread'); module.export = to =&it; { to.on('connection', socket =&it; { /* find user login */ let passport = socket.handshake.session.passport; (type passport !== 'undefined') { socket.on('tying:send', data =&it; { pub.publish('spread', son.stringify(data)); }); sub.on('message', (ch, mug) =&it; { // exact line get error to.emit(`${son.pause(mug).common}:receive`, { ...son.pause(mug) }); }); } }); };"
50702865,Entity Framework Attach/Update confusion (EF Core),"As I understand, when ""Update"" is called, every property within a specific entity is modified. 
The ""Attach"" method, on the other hand, starts the entity off in the ""Unmodified"" state. Then, when an operation takes place on a particular property, that specific property only is modified. So ""Attach"" is more useful for individual property changes, and ""Update"" is more useful when you want to update every property in the entity (I may be wrong in this understanding).
However, what I don't understand is what happens when neither of these two methods are called during a property change. For instance, consider an example with a table called ""students"":
student.City = ""Calgary"";
student.Name = ""John Smith"";
database.SaveChanges();
As we are not marking any property in the entity as modified, how will the generated query from the above code differ?
",<c#><entity-framework><sql-update>,853,0,3,774,1,5,11,48,35009,0.0,12,2,33,2018-06-05 14:41,2018-06-06 6:43,2018-06-06 6:43,1.0,1.0,Basic,9,"<c#><entity-framework><sql-update>, Entity Framework Attach/Update confusion (EF Core), As I understand, when ""Update"" is called, every property within a specific entity is modified. 
The ""Attach"" method, on the other hand, starts the entity off in the ""Unmodified"" state. Then, when an operation takes place on a particular property, that specific property only is modified. So ""Attach"" is more useful for individual property changes, and ""Update"" is more useful when you want to update every property in the entity (I may be wrong in this understanding).
However, what I don't understand is what happens when neither of these two methods are called during a property change. For instance, consider an example with a table called ""students"":
student.City = ""Calgary"";
student.Name = ""John Smith"";
database.SaveChanges();
As we are not marking any property in the entity as modified, how will the generated query from the above code differ?
","<c#><entity-framework><sal-update>, entity framework attach/up confuse (of core), understand, ""update"" called, every property within specie entity modified. ""attach"" method, hand, start entity ""modified"" state. then, over take place particular property, specie property modified. ""attach"" use individu property changes, ""update"" use want update every property entity (i may wrong understanding). however, understand happen neither two method call property change. instance, consider example table call ""students"": student.c = ""salary""; student.am = ""john smith""; database.savechanges(); mark property entity modified, genet query code differ?"
64198359,"pg Admin 4 - password for ""postgres"" user when trying to connect to PostgreSQL 13 server","I know that this question has been asked other times but I didn't find a solution to this problem!
I downloaded PostgreSQL 13 with pg Admin 4 and when I open it for the first time after installation it asks me for the master password that I was asked to set during installation, after I give the master password and this gets accepted I try to connect to the default server created during the installation: &quot;PostgreSQL 13&quot;.
At this point, it asks me for a password for the user &quot;postgres&quot; that I don't know where to find. Specifically, it says: Please enter the password for the user 'postgres' to connect the server - &quot;PostgreSQL 13&quot;.
I've already tried all the &quot;default&quot; passwords I managed to find on the internet but the error is always the same:
FATAL: password authentication failed for user &quot;postgres&quot;
I've also tried not to insert any password with the resulting error:
fe_sendauth: no password supplied
I don't know what to do. In PostgreSQL 13 the authentication method is encrypted via scram-sha-256. I already tried to set the method to trust, restart the mac, and open pg Admin 4 that keeps asking me for the password to access the server.
I've also tried to use the command line tool but end up encountering the same errors.
Finally, this is how my pg_hba.conf looks like:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
# &quot;local&quot; is for Unix domain socket connections only
local   all             all                                     scram-sha-256
# IPv4 local connections:
host    all             all             127.0.0.1/32            scram-sha-256
# IPv6 local connections:
host    all             all             ::1/128                 scram-sha-256
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   replication     all                                     scram-sha-256
host    replication     all             127.0.0.1/32            scram-sha-256
host    replication     all             ::1/128                 scram-sha-256
PS. I've also tried to uninstall PostgreSQL 13, deleting the postgres user and re-download and re-install everything... nothing changed.
If someone could help me would become my savior, thanks beforehand!
",<postgresql><pgadmin><pgadmin-4><postgresql-13>,2288,0,15,357,1,3,6,79,120510,0.0,4,20,33,2020-10-04 18:25,2020-10-05 14:12,2020-10-10 4:43,1.0,6.0,Basic,14,"<postgresql><pgadmin><pgadmin-4><postgresql-13>, pg Admin 4 - password for ""postgres"" user when trying to connect to PostgreSQL 13 server, I know that this question has been asked other times but I didn't find a solution to this problem!
I downloaded PostgreSQL 13 with pg Admin 4 and when I open it for the first time after installation it asks me for the master password that I was asked to set during installation, after I give the master password and this gets accepted I try to connect to the default server created during the installation: &quot;PostgreSQL 13&quot;.
At this point, it asks me for a password for the user &quot;postgres&quot; that I don't know where to find. Specifically, it says: Please enter the password for the user 'postgres' to connect the server - &quot;PostgreSQL 13&quot;.
I've already tried all the &quot;default&quot; passwords I managed to find on the internet but the error is always the same:
FATAL: password authentication failed for user &quot;postgres&quot;
I've also tried not to insert any password with the resulting error:
fe_sendauth: no password supplied
I don't know what to do. In PostgreSQL 13 the authentication method is encrypted via scram-sha-256. I already tried to set the method to trust, restart the mac, and open pg Admin 4 that keeps asking me for the password to access the server.
I've also tried to use the command line tool but end up encountering the same errors.
Finally, this is how my pg_hba.conf looks like:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
# &quot;local&quot; is for Unix domain socket connections only
local   all             all                                     scram-sha-256
# IPv4 local connections:
host    all             all             127.0.0.1/32            scram-sha-256
# IPv6 local connections:
host    all             all             ::1/128                 scram-sha-256
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   replication     all                                     scram-sha-256
host    replication     all             127.0.0.1/32            scram-sha-256
host    replication     all             ::1/128                 scram-sha-256
PS. I've also tried to uninstall PostgreSQL 13, deleting the postgres user and re-download and re-install everything... nothing changed.
If someone could help me would become my savior, thanks beforehand!
","<postgresql><pgadmin><pgadmin-4><postgresql-13>, pg admit 4 - password ""postures"" user try connect postgresql 13 server, know question ask time find slut problem! download postgresql 13 pg admit 4 open first time instal ask master password ask set installation, give master password get accept try connect default server great installation: &quit;postgresql 13&quit;. point, ask password user &quit;postures&quit; know find. specifically, says: pleas enter password user 'postures' connect server - &quit;postgresql 13&quit;. i'v already try &quit;default&quit; password manage find internet error away same: fatal: password authentic fail user &quit;postures&quit; i'v also try insert password result error: fe_sendauth: password supply know do. postgresql 13 authentic method encrypt via scream-she-256. already try set method trust, start mac, open pg admit 4 keep ask password access server. i'v also try use command line tool end count errors. finally, pg_hba.cone look like: # type database user address method # &quit;local&quit; unit domain socket connect local scream-she-256 # iv local connections: host 127.0.0.1/32 scream-she-256 # iv local connections: host ::1/128 scream-she-256 # allow relic connect localhost, user # relic privilege. local relic scream-she-256 host relic 127.0.0.1/32 scream-she-256 host relic ::1/128 scream-she-256 is. i'v also try instal postgresql 13, delete poster user re-download re-instal everything... not changed. someone could help would become savior, thank beforehand!"
48605130,How to convert List<Object> into comma separated String,"I am getting list of Address objects from the DB call. 
ArrayList&lt;Address&gt; addresses = new ArrayList&lt;&gt;();
Each Address has an int addressId property.
I am writing an update query where in the IN clause I am sending this whole list of Address objects and I am getting ibatis TypeException. How can I convert List&lt;Address&gt; to a comma separated string which can be sent to update query?
My update query looks like:::
Update tablename set postcode = #{postCode} where id in #{addressID}.
",<java><sql><ibatis>,502,0,9,373,1,4,12,56,57038,0.0,0,8,33,2018-02-04 5:43,2018-02-04 5:51,,0.0,,Basic,10,"<java><sql><ibatis>, How to convert List<Object> into comma separated String, I am getting list of Address objects from the DB call. 
ArrayList&lt;Address&gt; addresses = new ArrayList&lt;&gt;();
Each Address has an int addressId property.
I am writing an update query where in the IN clause I am sending this whole list of Address objects and I am getting ibatis TypeException. How can I convert List&lt;Address&gt; to a comma separated string which can be sent to update query?
My update query looks like:::
Update tablename set postcode = #{postCode} where id in #{addressID}.
","<cava><sal><basis>, convert list<object> comma spear string, get list address object do call. arraylist&it;address&it; address = new arraylist&it;&it;(); address in addressed property. write update query class send whole list address object get bath typeexception. convert list&it;address&it; comma spear string sent update query? update query look like::: update tablenam set posted = #{postpone} id #{addressed}."
50373427,Node.js can't authenticate to MySQL 8.0,"I have a Node.js program that connects to a local MySQL database with the root account (this is not a production setup).
This is the code that creates the connection:
const mysql = require('mysql');
const dbConn = mysql.createConnection({
    host: 'localhost',
    port: 3306,
    user: 'root',
    password: 'myRootPassword',
    database: 'decldb'
});
dbConn.connect(err =&gt; {
    if (err) throw err;
    console.log('Connected!');
});
It worked with MySQL 5.7, but since installing MySQL 8.0 I get this error when starting the Node.js app:
&gt; node .\api-server\main.js
[2018-05-16T13:53:53.153Z] Server launched on port 3000!
C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:80
        throw err; // Rethrow non-MySQL errors
        ^
Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at Handshake.Sequence._packetToError (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Sequence.js:52:14)
    at Handshake.ErrorPacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Handshake.js:130:18)
    at Protocol._parsePacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:279:23)
    at Parser.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:76:12)
    at Protocol.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:39:16)
    at Socket.&lt;anonymous&gt; (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:103:28)
    at emitOne (events.js:116:13)
    at Socket.emit (events.js:211:7)
    at addChunk (_stream_readable.js:263:12)
    at readableAddChunk (_stream_readable.js:250:11)
    --------------------
    at Protocol._enqueue (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:145:48)
    at Protocol.handshake (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:52:23)
    at Connection.connect (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:130:18)
    at Object.&lt;anonymous&gt; (C:\Users\me\project\node_server\main.js:27:8)
    at Module._compile (module.js:652:30)
    at Object.Module._extensions..js (module.js:663:10)
    at Module.load (module.js:565:32)
    at tryModuleLoad (module.js:505:12)
    at Function.Module._load (module.js:497:3)
    at Function.Module.runMain (module.js:693:10)
It seems that the root account uses a new password hashing method:
&gt; select User,Host,plugin from user where User=""root"";
+------+-----------+-----------------------+
| User | Host      | plugin                |
+------+-----------+-----------------------+
| root | localhost | caching_sha2_password |
+------+-----------+-----------------------+
...but I don't know why Node.js is unable to connect to it. I have updated all the npm packages and it's still an issue.
I would like to keep the new password hashing method. Can I still make this connection work? Do I have to wait for an update of the MySQL Node.js package, or change a setting on my side?
",<mysql><node.js><authentication><mysql-8.0>,3124,0,48,1721,5,24,44,71,32919,0.0,4265,5,32,2018-05-16 14:19,2018-05-16 18:40,2018-05-16 18:40,0.0,0.0,Basic,13,"<mysql><node.js><authentication><mysql-8.0>, Node.js can't authenticate to MySQL 8.0, I have a Node.js program that connects to a local MySQL database with the root account (this is not a production setup).
This is the code that creates the connection:
const mysql = require('mysql');
const dbConn = mysql.createConnection({
    host: 'localhost',
    port: 3306,
    user: 'root',
    password: 'myRootPassword',
    database: 'decldb'
});
dbConn.connect(err =&gt; {
    if (err) throw err;
    console.log('Connected!');
});
It worked with MySQL 5.7, but since installing MySQL 8.0 I get this error when starting the Node.js app:
&gt; node .\api-server\main.js
[2018-05-16T13:53:53.153Z] Server launched on port 3000!
C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:80
        throw err; // Rethrow non-MySQL errors
        ^
Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at Handshake.Sequence._packetToError (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Sequence.js:52:14)
    at Handshake.ErrorPacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Handshake.js:130:18)
    at Protocol._parsePacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:279:23)
    at Parser.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:76:12)
    at Protocol.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:39:16)
    at Socket.&lt;anonymous&gt; (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:103:28)
    at emitOne (events.js:116:13)
    at Socket.emit (events.js:211:7)
    at addChunk (_stream_readable.js:263:12)
    at readableAddChunk (_stream_readable.js:250:11)
    --------------------
    at Protocol._enqueue (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:145:48)
    at Protocol.handshake (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:52:23)
    at Connection.connect (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:130:18)
    at Object.&lt;anonymous&gt; (C:\Users\me\project\node_server\main.js:27:8)
    at Module._compile (module.js:652:30)
    at Object.Module._extensions..js (module.js:663:10)
    at Module.load (module.js:565:32)
    at tryModuleLoad (module.js:505:12)
    at Function.Module._load (module.js:497:3)
    at Function.Module.runMain (module.js:693:10)
It seems that the root account uses a new password hashing method:
&gt; select User,Host,plugin from user where User=""root"";
+------+-----------+-----------------------+
| User | Host      | plugin                |
+------+-----------+-----------------------+
| root | localhost | caching_sha2_password |
+------+-----------+-----------------------+
...but I don't know why Node.js is unable to connect to it. I have updated all the npm packages and it's still an issue.
I would like to keep the new password hashing method. Can I still make this connection work? Do I have to wait for an update of the MySQL Node.js package, or change a setting on my side?
","<myself><node.is><authentication><myself-8.0>, node.j can't authentic myself 8.0, node.j program connect local myself database root account (the product set). code great connection: cost myself = require('myself'); cost dbconn = myself.createconnection({ host: 'localhost', port: 3306, user: 'root', password: 'myrootpassword', database: 'decide' }); dbconn.connect(err =&it; { (err) throw err; console.log('connected!'); }); work myself 5.7, since instal myself 8.0 get error start node.j pp: &it; node .\apt-server\main.j [2018-05-16t13:53:53.153z] server launch port 3000! c:\users\me\project\node_server\node_modules\myself\limb\protocol\parker.is:80 throw err; // throw non-myself error ^ error: er_not_supported_auth_mode: client support authentic protocol request server; consider upgrade myself client handshake.sequence._packettoerror (c:\users\me\project\node_server\node_modules\myself\limb\protocol\sequence\sequence.is:52:14) handshake.errorpacket (c:\users\me\project\node_server\node_modules\myself\limb\protocol\sequence\handshake.is:130:18) protocol._parsepacket (c:\users\me\project\node_server\node_modules\myself\limb\protocol\protocol.is:279:23) parker.writ (c:\users\me\project\node_server\node_modules\myself\limb\protocol\parker.is:76:12) protocol.writ (c:\users\me\project\node_server\node_modules\myself\limb\protocol\protocol.is:39:16) socket.&it;anonymous&it; (c:\users\me\project\node_server\node_modules\myself\limb\connection.is:103:28) emotion (events.is:116:13) socket.emit (events.is:211:7) addchunk (_stream_readable.is:263:12) readableaddchunk (_stream_readable.is:250:11) -------------------- protocol._enqueu (c:\users\me\project\node_server\node_modules\myself\limb\protocol\protocol.is:145:48) protocol.handshak (c:\users\me\project\node_server\node_modules\myself\limb\protocol\protocol.is:52:23) connection.connect (c:\users\me\project\node_server\node_modules\myself\limb\connection.is:130:18) object.&it;anonymous&it; (c:\users\me\project\node_server\main.is:27:8) module.compel (module.is:652:30) object.module.extensions..j (module.is:663:10) module.load (module.is:565:32) trymoduleload (module.is:505:12) function.module.load (module.is:497:3) function.module.remain (module.is:693:10) seem root account use new password has method: &it; select user,host,plain user user=""root""; +------+-----------+-----------------------+ | user | host | plain | +------+-----------+-----------------------+ | root | localhost | caching_sha2_password | +------+-----------+-----------------------+ ...but know node.j unable connect it. update nom package still issue. would like keep new password has method. still make connect work? wait update myself node.j package, change set side?"
48420438,"""could not identify an equality operator for type json"" when using distinct","I have the following query:
SELECT 
  distinct(date(survey_results.created_at)), 
  json_build_object(
    'high', 
    ROUND( 
      COUNT(*) FILTER (WHERE ( scores#&gt;&gt;'{medic,categories,motivation}' in('high', 'medium'))) OVER(order by date(survey_results.created_at) ) * 1.0 / 
      (
        CASE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        WHEN 0.0 THEN 1.0 
        ELSE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        END)* 100, 2 ) ) AS childcare FROM survey_results GROUP BY date, scores ORDER BY date asc; 
The problem is with using distinct(date(survey_results.created_at)). With that in place query returns error:
could not identify an equality operator for type json
Here is db fiddle that show that problem. How can I fix that?
",<postgresql>,970,1,13,7462,15,68,135,44,59461,0.0,499,4,32,2018-01-24 10:40,2018-01-24 10:48,2018-01-24 10:48,0.0,0.0,Basic,2,"<postgresql>, ""could not identify an equality operator for type json"" when using distinct, I have the following query:
SELECT 
  distinct(date(survey_results.created_at)), 
  json_build_object(
    'high', 
    ROUND( 
      COUNT(*) FILTER (WHERE ( scores#&gt;&gt;'{medic,categories,motivation}' in('high', 'medium'))) OVER(order by date(survey_results.created_at) ) * 1.0 / 
      (
        CASE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        WHEN 0.0 THEN 1.0 
        ELSE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        END)* 100, 2 ) ) AS childcare FROM survey_results GROUP BY date, scores ORDER BY date asc; 
The problem is with using distinct(date(survey_results.created_at)). With that in place query returns error:
could not identify an equality operator for type json
Here is db fiddle that show that problem. How can I fix that?
","<postgresql>, ""could identify equal over type son"" use distinct, follow query: select distinct(date(survey_results.created_at)), json_build_object( 'high', round( count(*) filter (where ( scores#&it;&it;'{media,categories,motivation}' in('high', 'medium'))) over(ord date(survey_results.created_at) ) * 1.0 / ( case (count(*) filter (where (scores#&it;&it;'{media,categories,motivation}' in('high','medium','low'))) over(ord date(survey_results.created_at))) 0.0 1.0 else (count(*) filter (where (scores#&it;&it;'{media,categories,motivation}' in('high','medium','low'))) over(ord date(survey_results.created_at))) end)* 100, 2 ) ) childcar survey_result group date, score order date as; problem use distinct(date(survey_results.created_at)). place query return error: could identify equal over type son do fiddle show problem. fix that?"
49083573,PHP 7.2.2 + mysql 8.0 PDO gives: authentication method unknown to the client [caching_sha2_password],"I'm using php 7.2.2 and mysql 8.0.
When I try to connect with the right credential I get this error:
PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
Need help to troubleshoot the problem.
",<php><mysql><pdo>,267,0,1,1042,2,11,26,41,64856,0.0,141,4,32,2018-03-03 11:20,2018-04-30 5:38,2018-06-11 20:06,58.0,100.0,Basic,3,"<php><mysql><pdo>, PHP 7.2.2 + mysql 8.0 PDO gives: authentication method unknown to the client [caching_sha2_password], I'm using php 7.2.2 and mysql 8.0.
When I try to connect with the right credential I get this error:
PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
Need help to troubleshoot the problem.
","<pp><myself><do>, pp 7.2.2 + myself 8.0 do gives: authentic method unknown client [caching_sha2_password], i'm use pp 7.2.2 myself 8.0. try connect right credenti get error: pdoexception::(""do::construct(): server request authentic method unknown client [caching_sha2_password]"") need help troubleshoot problem."
49274390,PostgreSQL and Hibernate java.io.IOException: Tried to send an out-of-range integer as a 2-byte value,"I have hibernate query:
getSession()                     
        .createQuery(""from Entity where id in :ids"") 
        .setParameterList(""ids"", ids)
        .list();
where ids is Collection ids, that can contain a lot of ids.
Currently I got exception when collection is very large:
 java.io.IOException: Tried to send an out-of-range integer as a 2-byte value
I heard that postgre has some problem with it. But I can't find the solution how to rewrite it on hql.
",<postgresql><hibernate><exception><ioexception>,465,0,4,320,1,3,4,70,27364,0.0,0,4,32,2018-03-14 9:47,2018-03-14 10:20,,0.0,,Basic,6,"<postgresql><hibernate><exception><ioexception>, PostgreSQL and Hibernate java.io.IOException: Tried to send an out-of-range integer as a 2-byte value, I have hibernate query:
getSession()                     
        .createQuery(""from Entity where id in :ids"") 
        .setParameterList(""ids"", ids)
        .list();
where ids is Collection ids, that can contain a lot of ids.
Currently I got exception when collection is very large:
 java.io.IOException: Tried to send an out-of-range integer as a 2-byte value
I heard that postgre has some problem with it. But I can't find the solution how to rewrite it on hql.
","<postgresql><liberate><exception><exception>, postgresql wiberd cava.to.exception: try send out-of-rang inter 2-bite value, wiberd query: getsession() .createquery(""from entity id :is"") .setparameterlist(""is"", is) .list(); id collect is, contain lot is. current got except collect large: cava.to.exception: try send out-of-rang inter 2-bite value heard poster problem it. can't find slut regret he."
50570592,Mysql 8 remote access,"I usualy setup correctly MySQL for having remote access.
And currently I got stuck with MySQL 8.
The first thing is that on the mysql.conf.d/mysqld.cnf , I don't have any bind-address line, so I added it by hand (bind-address 0.0.0.0)
And I granted access to the user on '%' 
When I connected I got the message ""Authentication failed""
But it works well on localhost/command line
",<mysql><authentication><mysql-8.0>,379,0,0,617,2,8,14,67,84475,0.0,7,4,32,2018-05-28 16:44,2018-08-05 11:13,,69.0,,Advanced,38,"<mysql><authentication><mysql-8.0>, Mysql 8 remote access, I usualy setup correctly MySQL for having remote access.
And currently I got stuck with MySQL 8.
The first thing is that on the mysql.conf.d/mysqld.cnf , I don't have any bind-address line, so I added it by hand (bind-address 0.0.0.0)
And I granted access to the user on '%' 
When I connected I got the message ""Authentication failed""
But it works well on localhost/command line
","<myself><authentication><myself-8.0>, myself 8 remote access, usual set correctly myself remote access. current got stuck myself 8. first thing myself.cone.d/myself.cf , bind-address line, ad hand (bind-address 0.0.0.0) grant access user '%' connect got message ""authentic failed"" work well localhost/command line"
64829748,Pgadmin is not loading,"i have recently installed pgadmin4 onto my laptop and when I launch the application, it just gets stuck on the loading. I had a look at the logs and this is what I see:
The logs
2020-11-14 00:22:46: Checking for system tray...
2020-11-14 00:22:46: Starting pgAdmin4 server...
2020-11-14 00:22:46: Creating server object, port:64222, key:2a079549-63da-44d2-8931-efa9de3a847f, logfile:C:/Users/yonis/AppData/Local/pgadmin4.d41d8cd98f00b204e9800998ecf8427e.log
2020-11-14 00:22:46: Python Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib/site-packages;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/DLLs;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib
2020-11-14 00:22:46: Python Home: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv
2020-11-14 00:22:46: Initializing Python...
2020-11-14 00:22:46: Python initialized.
2020-11-14 00:22:46: Adding new additional path elements
2020-11-14 00:22:46: Redirecting stderr...
2020-11-14 00:22:46: stderr redirected successfully.
2020-11-14 00:22:46: Initializing server...
2020-11-14 00:22:46: Webapp Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/web/pgAdmin4.py
2020-11-14 00:22:46: Server initialized, starting server thread...
2020-11-14 00:22:46: Open the application code and run it.
2020-11-14 00:22:46: Set the port number, key and force SERVER_MODE off
2020-11-14 00:22:46: PyRun_SimpleFile launching application server...
2020-11-14 00:22:47: Application Server URL: http://127.0.0.1:64222/?key=2a079549-63da-44d2-8931-efa9de3a847f
2020-11-14 00:22:47: The server should be up. Attempting to connect and get a response.
2020-11-14 00:22:53: Attempt to connect one more time in case of a long network timeout while looping
2020-11-14 00:22:53: Everything works fine, successfully started pgAdmin4.
",<python><postgresql><pgadmin><heroku-postgres><pgadmin-4>,1753,2,20,321,1,3,3,64,51325,0.0,0,10,32,2020-11-14 0:34,2020-11-14 10:50,,0.0,,Basic,14,"<python><postgresql><pgadmin><heroku-postgres><pgadmin-4>, Pgadmin is not loading, i have recently installed pgadmin4 onto my laptop and when I launch the application, it just gets stuck on the loading. I had a look at the logs and this is what I see:
The logs
2020-11-14 00:22:46: Checking for system tray...
2020-11-14 00:22:46: Starting pgAdmin4 server...
2020-11-14 00:22:46: Creating server object, port:64222, key:2a079549-63da-44d2-8931-efa9de3a847f, logfile:C:/Users/yonis/AppData/Local/pgadmin4.d41d8cd98f00b204e9800998ecf8427e.log
2020-11-14 00:22:46: Python Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib/site-packages;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/DLLs;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib
2020-11-14 00:22:46: Python Home: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv
2020-11-14 00:22:46: Initializing Python...
2020-11-14 00:22:46: Python initialized.
2020-11-14 00:22:46: Adding new additional path elements
2020-11-14 00:22:46: Redirecting stderr...
2020-11-14 00:22:46: stderr redirected successfully.
2020-11-14 00:22:46: Initializing server...
2020-11-14 00:22:46: Webapp Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/web/pgAdmin4.py
2020-11-14 00:22:46: Server initialized, starting server thread...
2020-11-14 00:22:46: Open the application code and run it.
2020-11-14 00:22:46: Set the port number, key and force SERVER_MODE off
2020-11-14 00:22:46: PyRun_SimpleFile launching application server...
2020-11-14 00:22:47: Application Server URL: http://127.0.0.1:64222/?key=2a079549-63da-44d2-8931-efa9de3a847f
2020-11-14 00:22:47: The server should be up. Attempting to connect and get a response.
2020-11-14 00:22:53: Attempt to connect one more time in case of a long network timeout while looping
2020-11-14 00:22:53: Everything works fine, successfully started pgAdmin4.
","<patron><postgresql><pgadmin><hero-postures><pgadmin-4>, pgadmin loading, recent instal pgadmin4 onto lawton launch application, get stuck loading. look log see: log 2020-11-14 00:22:46: check system tray... 2020-11-14 00:22:46: start pgadmin4 server... 2020-11-14 00:22:46: great server object, port:64222, key:2a079549-soda-44d2-8931-efa9de3a847f, logfile:c:/users/boris/appdata/local/pgadmin4.d41d8cd98f00b204e9800998ecf8427e.log 2020-11-14 00:22:46: patron path: c:/program files/postgresql/13/pgadmin 4/vent/limb/site-packages;c:/program files/postgresql/13/pgadmin 4/vent/ills;c:/program files/postgresql/13/pgadmin 4/vent/limb 2020-11-14 00:22:46: patron home: c:/program files/postgresql/13/pgadmin 4/vent 2020-11-14 00:22:46: into patron... 2020-11-14 00:22:46: patron initialized. 2020-11-14 00:22:46: ad new admit path element 2020-11-14 00:22:46: direct stern... 2020-11-14 00:22:46: stern direct successfully. 2020-11-14 00:22:46: into server... 2020-11-14 00:22:46: webapp path: c:/program files/postgresql/13/pgadmin 4/web/pgadmin4.i 2020-11-14 00:22:46: server initialized, start server thread... 2020-11-14 00:22:46: open applied code run it. 2020-11-14 00:22:46: set port number, key for server_mod 2020-11-14 00:22:46: pyrun_simplefil launch applied server... 2020-11-14 00:22:47: applied server curl: http://127.0.0.1:64222/?key=2a079549-soda-44d2-8931-efa9de3a847f 2020-11-14 00:22:47: server up. attempt connect get response. 2020-11-14 00:22:53: attempt connect one time case long network timeout loop 2020-11-14 00:22:53: every work fine, success start pgadmin4."
51375439,Postgres on conflict do update on composite primary keys,"I have a table where a user answers to a question. The rules are that the user can answer to many questions or many users can answer one question BUT a user can answer to a particular question only once. If the user answers to the question again, it should simply replace the old one. Generally the on conflict do update works when we are dealing with unique columns. In this scenario the columns person_id and question_id cannot be unique. However the combination of the two is always unique. How do I implement the insert statement that does update on conflict?
CREATE TABLE ""answer"" (
  ""person_id"" integer NOT NULL REFERENCES person(id), 
  ""question_id"" integer NOT NULL REFERENCES question(id) ON DELETE CASCADE, /* INDEXED */
  ""answer"" character varying (1200) NULL,
  PRIMARY KEY (person_id, question_id) 
);
",<sql><postgresql><upsert>,818,0,8,3055,4,32,59,35,33711,0.0,111,2,32,2018-07-17 7:20,2018-07-17 7:39,2018-07-17 7:39,0.0,0.0,Basic,10,"<sql><postgresql><upsert>, Postgres on conflict do update on composite primary keys, I have a table where a user answers to a question. The rules are that the user can answer to many questions or many users can answer one question BUT a user can answer to a particular question only once. If the user answers to the question again, it should simply replace the old one. Generally the on conflict do update works when we are dealing with unique columns. In this scenario the columns person_id and question_id cannot be unique. However the combination of the two is always unique. How do I implement the insert statement that does update on conflict?
CREATE TABLE ""answer"" (
  ""person_id"" integer NOT NULL REFERENCES person(id), 
  ""question_id"" integer NOT NULL REFERENCES question(id) ON DELETE CASCADE, /* INDEXED */
  ""answer"" character varying (1200) NULL,
  PRIMARY KEY (person_id, question_id) 
);
","<sal><postgresql><upset>, poster conflict update composite primary keys, table user answer question. rule user answer man question man user answer one question user answer particular question once. user answer question again, simple replace old one. genet conflict update work deal unique columns. scenario column person_id questioned cannot unique. howe combine two away unique. implement insert statement update conflict? great table ""answer"" ( ""person_id"" inter null refer person(id), ""questioned"" inter null refer question(id) delete cascade, /* index */ ""answer"" character vary (1200) null, primary key (person_id, questioned) );"
52064130,"QueryFailedError: the column ""price"" contain null values - TypeORM - PostgreSQL","I've created a simple table:
import { Column, Entity, PrimaryGeneratedColumn } from &quot;typeorm&quot;
@Entity()
export class Test {
    @PrimaryGeneratedColumn()
    public id!: number
    @Column({ nullable: false })
    public name!: string
    @Column({ nullable: false, type: &quot;float&quot; })
    public price!: number
}
I generate the migration and run it also. When I have no data in the database and I run the server it succeed. But when I add 1 row in the database and I run it again it appears the following error:
QueryFailedError: the column «price» contain null values
The databse definetely has the row with all the data. I tried a lot of cases and none of them was correct.
Has anybody some idea for it?
",<node.js><postgresql><typescript><typeorm>,724,0,14,1468,4,15,27,68,46673,0.0,26,8,32,2018-08-28 18:21,2018-08-31 9:36,,3.0,,Basic,13,"<node.js><postgresql><typescript><typeorm>, QueryFailedError: the column ""price"" contain null values - TypeORM - PostgreSQL, I've created a simple table:
import { Column, Entity, PrimaryGeneratedColumn } from &quot;typeorm&quot;
@Entity()
export class Test {
    @PrimaryGeneratedColumn()
    public id!: number
    @Column({ nullable: false })
    public name!: string
    @Column({ nullable: false, type: &quot;float&quot; })
    public price!: number
}
I generate the migration and run it also. When I have no data in the database and I run the server it succeed. But when I add 1 row in the database and I run it again it appears the following error:
QueryFailedError: the column «price» contain null values
The databse definetely has the row with all the data. I tried a lot of cases and none of them was correct.
Has anybody some idea for it?
","<node.is><postgresql><typescript><typeorm>, queryfailederror: column ""price"" contain null value - typeorm - postgresql, i'v great simple table: import { column, entity, primarygeneratedcolumn } &quit;typeorm&quit; @entity() export class test { @primarygeneratedcolumn() public id!: number @column({ syllable: fall }) public name!: string @column({ syllable: false, type: &quit;float&quit; }) public price!: number } genet migrate run also. data database run server succeed. add 1 row database run appear follow error: queryfailederror: column «price» contain null value data defined row data. try lot case none correct. anybody idea it?"
52372165,mysql ERROR 1064 (42000): You have an error in your SQL syntax;,"i have installed mysql 8.0.12 into one linux node and when i try to give below grant permission to get access from other nodes, i am getting 42000 error
command issued :
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password';
Return results:
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'secure1t'' at line 1
Any help would be appreciated.
",<mysql>,478,0,1,407,1,4,8,79,84805,0.0,0,3,32,2018-09-17 16:29,2018-09-17 16:41,2018-09-17 16:41,0.0,0.0,Basic,8,"<mysql>, mysql ERROR 1064 (42000): You have an error in your SQL syntax;, i have installed mysql 8.0.12 into one linux node and when i try to give below grant permission to get access from other nodes, i am getting 42000 error
command issued :
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password';
Return results:
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'secure1t'' at line 1
Any help would be appreciated.
","<myself>, myself error 1064 (42000): error sal santa;, instal myself 8.0.12 one line node try give grant permits get access nodes, get 42000 error command issue : grant privilege *.* 'root'@'%' identify 'password'; return results: error 1064 (42000): error sal santa; check manual correspond myself server version right santa use near 'identify 'secret'' line 1 help would appreciated."
55286397,How to compare character varying (varcar) to UUID in PostgreSQL?,"Operator does not exist: character varying = uuid
Client id is UUId and should be why it is not working.
Where I am wrong, since I have tried almost everything I imagined.
SELECT * FROM ""cobranca_assinatura""
INNER JOIN ""cadastro_cliente"" ON (""cobranca_assinatura"".""cliente_id"" = ""cadastro_cliente"".""id"")
WHERE ""cadastro_cliente"".""nome"" LIKE marcelo% ESCAPE '\'
  [2019-03-21 14:40:34] [42883] ERROR: operator does not exist:
  character varying = uuid [2019-03-21 14:40:34] 
  Dica: No operator
  matches the given name and argument type(s). You might need to add
  explicit type casts.
",<postgresql>,587,0,3,2832,6,39,73,73,47759,,81,3,31,2019-03-21 17:47,2019-03-21 18:04,2019-03-21 18:04,0.0,0.0,Basic,2,"<postgresql>, How to compare character varying (varcar) to UUID in PostgreSQL?, Operator does not exist: character varying = uuid
Client id is UUId and should be why it is not working.
Where I am wrong, since I have tried almost everything I imagined.
SELECT * FROM ""cobranca_assinatura""
INNER JOIN ""cadastro_cliente"" ON (""cobranca_assinatura"".""cliente_id"" = ""cadastro_cliente"".""id"")
WHERE ""cadastro_cliente"".""nome"" LIKE marcelo% ESCAPE '\'
  [2019-03-21 14:40:34] [42883] ERROR: operator does not exist:
  character varying = uuid [2019-03-21 14:40:34] 
  Dica: No operator
  matches the given name and argument type(s). You might need to add
  explicit type casts.
","<postgresql>, compare character vary (lascar) quid postgresql?, over exist: character vary = quid client id quid working. wrong, since try almost every imagined. select * ""cobranca_assinatura"" inner join ""cadastro_cliente"" (""cobranca_assinatura"".""cliente_id"" = ""cadastro_cliente"".""id"") ""cadastro_cliente"".""some"" like parcel% escape '\' [2019-03-21 14:40:34] [42883] error: over exist: character vary = quid [2019-03-21 14:40:34] dice: over match given name argument type(s). might need add explicit type casts."
50088142,Authentication method 'caching_sha2_password' not supported by any of the available plugins,"When I try to connect MySQL (8.0) database with Visual Studio 2018 I get this error message 
  ""Authentication method 'caching_sha2_password' not supported by any of the available plugins""
Also I am unable to retrieve Database name.
I use mysql-for-visualstudio-1.2.7 and mysql-connector-net-8.0.11 for connection.
Is there any possible way to fix it.
",<c#><mysql><authentication><visual-studio-2017>,352,0,2,347,1,4,11,61,85143,0.0,2,9,31,2018-04-29 15:18,2018-05-01 20:43,2019-11-15 5:06,2.0,565.0,Basic,14,"<c#><mysql><authentication><visual-studio-2017>, Authentication method 'caching_sha2_password' not supported by any of the available plugins, When I try to connect MySQL (8.0) database with Visual Studio 2018 I get this error message 
  ""Authentication method 'caching_sha2_password' not supported by any of the available plugins""
Also I am unable to retrieve Database name.
I use mysql-for-visualstudio-1.2.7 and mysql-connector-net-8.0.11 for connection.
Is there any possible way to fix it.
","<c#><myself><authentication><visual-studio-2017>, authentic method 'caching_sha2_password' support avail plains, try connect myself (8.0) database visual studio 2018 get error message ""authentic method 'caching_sha2_password' support avail plains"" also unable retrieve database name. use myself-for-visualstudio-1.2.7 myself-connection-net-8.0.11 connection. possible way fix it."
48406304,GroupBy and concat array columns pyspark,"I have this data frame
df = sc.parallelize([(1, [1, 2, 3]), (1, [4, 5, 6]) , (2,[2]),(2,[3])]).toDF([&quot;store&quot;, &quot;values&quot;])
+-----+---------+
|store|   values|
+-----+---------+
|    1|[1, 2, 3]|
|    1|[4, 5, 6]|
|    2|      [2]|
|    2|      [3]|
+-----+---------+
and I would like to convert into the follwing df:
+-----+------------------+
|store|      values      |
+-----+------------------+
|    1|[1, 2, 3, 4, 5, 6]|
|    2|            [2, 3]|
+-----+------------------+
I did this:
from  pyspark.sql import functions as F
df.groupBy(&quot;store&quot;).agg(F.collect_list(&quot;values&quot;))
but the solution has this WrappedArrays
+-----+----------------------------------------------+
|store|collect_list(values)                          |
+-----+----------------------------------------------+
|1    |[WrappedArray(1, 2, 3), WrappedArray(4, 5, 6)]|
|2    |[WrappedArray(2), WrappedArray(3)]            |
+-----+----------------------------------------------+
Is there any way to transform the WrappedArrays into concatenated arrays? Or can I do it differently?
",<pyspark><apache-spark-sql>,1091,0,26,1039,2,12,15,63,56011,0.0,10,6,31,2018-01-23 16:17,2018-01-23 17:05,2018-01-23 17:05,0.0,0.0,Basic,10,"<pyspark><apache-spark-sql>, GroupBy and concat array columns pyspark, I have this data frame
df = sc.parallelize([(1, [1, 2, 3]), (1, [4, 5, 6]) , (2,[2]),(2,[3])]).toDF([&quot;store&quot;, &quot;values&quot;])
+-----+---------+
|store|   values|
+-----+---------+
|    1|[1, 2, 3]|
|    1|[4, 5, 6]|
|    2|      [2]|
|    2|      [3]|
+-----+---------+
and I would like to convert into the follwing df:
+-----+------------------+
|store|      values      |
+-----+------------------+
|    1|[1, 2, 3, 4, 5, 6]|
|    2|            [2, 3]|
+-----+------------------+
I did this:
from  pyspark.sql import functions as F
df.groupBy(&quot;store&quot;).agg(F.collect_list(&quot;values&quot;))
but the solution has this WrappedArrays
+-----+----------------------------------------------+
|store|collect_list(values)                          |
+-----+----------------------------------------------+
|1    |[WrappedArray(1, 2, 3), WrappedArray(4, 5, 6)]|
|2    |[WrappedArray(2), WrappedArray(3)]            |
+-----+----------------------------------------------+
Is there any way to transform the WrappedArrays into concatenated arrays? Or can I do it differently?
","<spark><apache-spark-sal>, group coat array column spark, data frame of = s.parallelize([(1, [1, 2, 3]), (1, [4, 5, 6]) , (2,[2]),(2,[3])]).of([&quit;store&quit;, &quit;values&quit;]) +-----+---------+ |store| values| +-----+---------+ | 1|[1, 2, 3]| | 1|[4, 5, 6]| | 2| [2]| | 2| [3]| +-----+---------+ would like convert follow of: +-----+------------------+ |store| value | +-----+------------------+ | 1|[1, 2, 3, 4, 5, 6]| | 2| [2, 3]| +-----+------------------+ this: spark.sal import function f of.group(&quit;store&quit;).age(f.collect_list(&quit;values&quit;)) slut wrappedarray +-----+----------------------------------------------+ |store|collect_list(values) | +-----+----------------------------------------------+ |1 |[wrappedarray(1, 2, 3), wrappedarray(4, 5, 6)]| |2 |[wrappedarray(2), wrappedarray(3)] | +-----+----------------------------------------------+ way transform wrappedarray concave array? differently?"
56118095,Type hints for SQLAlchemy engine and session objects,"I'm trying to add type hints to my SQLAlchemy script:
connection_string: str = ""sqlite:///:memory:""
engine = create_engine(connection_string)
session = Session(bind=engine)
reveal_type(engine)
reveal_type(session)
I've ran this script against mypy but both types comes back as Any. What type should the engine and session variable be?
",<python><sqlalchemy><mypy><type-hinting>,335,0,9,5261,20,85,155,76,17483,,400,2,31,2019-05-13 18:37,2019-05-13 21:07,2019-05-13 21:07,0.0,0.0,Basic,3,"<python><sqlalchemy><mypy><type-hinting>, Type hints for SQLAlchemy engine and session objects, I'm trying to add type hints to my SQLAlchemy script:
connection_string: str = ""sqlite:///:memory:""
engine = create_engine(connection_string)
session = Session(bind=engine)
reveal_type(engine)
reveal_type(session)
I've ran this script against mypy but both types comes back as Any. What type should the engine and session variable be?
","<patron><sqlalchemy><may><type-hinting>, type hint sqlalchemi engine session objects, i'm try add type hint sqlalchemi script: connection_string: sir = ""quite:///:memory:"" engine = create_engine(connection_string) session = session(bind=engine) reveal_type(engine) reveal_type(session) i'v ran script my type come back any. type engine session variable be?"
49148754,Docker container shuts down giving 'data directory has wrong ownership' error when executed in windows 10,"I have my docker installed in Windows. I am trying to install this application. It has given me the following docker-compose.yml file:
version: '2'
services:
  web:
    build:
      context: .
      dockerfile: Dockerfile-nginx
    ports:
    - &quot;8085:80&quot;
    networks:
      - attendizenet
    volumes:
      - .:/usr/share/nginx/html/attendize
    depends_on:
      - php
  php:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    networks: 
      - attendizenet
  php-worker:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    command: php artisan queue:work --daemon
    networks:
      - attendizenet
  db:
    image: postgres
    environment:
      - POSTGRES_USER=attendize
      - POSTGRES_PASSWORD=attendize
      - POSTGRES_DB=attendize
    ports:
      - &quot;5433:5432&quot;
    volumes:
      - ./docker/pgdata:/var/lib/postgresql/data
    networks:
    - attendizenet
  maildev:
    image: djfarrelly/maildev
    ports:
      - &quot;1080:80&quot;
    networks:
      - attendizenet
  redis:
    image: redis
    networks:
      - attendizenet
networks:
  attendizenet:
    driver: bridge
All the installation goes well, but the PostgreSQL container stops after starting for a moment giving following error.
2018-03-07 08:24:47.927 UTC [1] FATAL:  data directory &quot;/var/lib/postgresql/data&quot; has wrong ownership
2018-03-07 08:24:47.927 UTC [1] HINT:  The server must be started by the user that owns the data directory
A simple PostgreSQL container from Docker Hub works smoothly, but the error occurs when we try to attach a volume to the container.
I am new to docker, so please ignore usage of terms wrongly.
",<postgresql><docker><docker-compose><docker-for-windows>,1895,1,68,717,1,7,19,77,27966,0.0,17,7,31,2018-03-07 9:48,2018-03-07 9:58,2019-03-11 20:52,0.0,369.0,Basic,14,"<postgresql><docker><docker-compose><docker-for-windows>, Docker container shuts down giving 'data directory has wrong ownership' error when executed in windows 10, I have my docker installed in Windows. I am trying to install this application. It has given me the following docker-compose.yml file:
version: '2'
services:
  web:
    build:
      context: .
      dockerfile: Dockerfile-nginx
    ports:
    - &quot;8085:80&quot;
    networks:
      - attendizenet
    volumes:
      - .:/usr/share/nginx/html/attendize
    depends_on:
      - php
  php:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    networks: 
      - attendizenet
  php-worker:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    command: php artisan queue:work --daemon
    networks:
      - attendizenet
  db:
    image: postgres
    environment:
      - POSTGRES_USER=attendize
      - POSTGRES_PASSWORD=attendize
      - POSTGRES_DB=attendize
    ports:
      - &quot;5433:5432&quot;
    volumes:
      - ./docker/pgdata:/var/lib/postgresql/data
    networks:
    - attendizenet
  maildev:
    image: djfarrelly/maildev
    ports:
      - &quot;1080:80&quot;
    networks:
      - attendizenet
  redis:
    image: redis
    networks:
      - attendizenet
networks:
  attendizenet:
    driver: bridge
All the installation goes well, but the PostgreSQL container stops after starting for a moment giving following error.
2018-03-07 08:24:47.927 UTC [1] FATAL:  data directory &quot;/var/lib/postgresql/data&quot; has wrong ownership
2018-03-07 08:24:47.927 UTC [1] HINT:  The server must be started by the user that owns the data directory
A simple PostgreSQL container from Docker Hub works smoothly, but the error occurs when we try to attach a volume to the container.
I am new to docker, so please ignore usage of terms wrongly.
","<postgresql><doctor><doctor-compose><doctor-for-windows>, doctor contain shut give 'data director wrong ownership' error execute window 10, doctor instal windows. try instal application. given follow doctor-compose.you file: version: '2' services: web: build: context: . dockerfile: dockerfile-nine ports: - &quit;8085:80&quit; network: - attendizenet volumes: - .:/us/share/nine/html/attend depends_on: - pp pp: build: context: . dockerfile: dockerfile-pp depends_on: - do - maiden - red volumes: - .:/us/share/nine/html/attend network: - attendizenet pp-worker: build: context: . dockerfile: dockerfile-pp depends_on: - do - maiden - red volumes: - .:/us/share/nine/html/attend command: pp artisans queue:work --demon network: - attendizenet do: image: poster environment: - postgres_user=attend - postgres_password=attend - postgres_db=attend ports: - &quit;5433:5432&quit; volumes: - ./doctor/data:/war/limb/postgresql/data network: - attendizenet maiden: image: djfarrelly/maiden ports: - &quit;1080:80&quit; network: - attendizenet red: image: red network: - attendizenet network: attendizenet: driver: bring instal go well, postgresql contain stop start moment give follow error. 2018-03-07 08:24:47.927 etc [1] fatal: data director &quit;/war/limb/postgresql/data&quit; wrong ownership 2018-03-07 08:24:47.927 etc [1] hint: server must start user own data director simple postgresql contain doctor hut work smoothly, error occur try attach volume container. new doctor, pleas ignore usage term wrongly."
54302088,"How to fix ""Error: The server does not support SSL connections"" when trying to access database in localhost?","I am following Heroku's node.js tutorial to provision a Postgres database. 
After creating a simple table and connecting to localhost:5000/db, I get an error saying ""Error: The server does not support SSL connections"".
I've been searching for solutions for hours but can't seem to fix it. Your help will be greatly appreciated. Thank you!
",<node.js><postgresql><heroku>,339,1,0,313,1,3,5,64,58725,0.0,14,8,31,2019-01-22 5:59,2020-04-14 0:24,2020-04-14 0:24,448.0,448.0,Basic,13,"<node.js><postgresql><heroku>, How to fix ""Error: The server does not support SSL connections"" when trying to access database in localhost?, I am following Heroku's node.js tutorial to provision a Postgres database. 
After creating a simple table and connecting to localhost:5000/db, I get an error saying ""Error: The server does not support SSL connections"".
I've been searching for solutions for hours but can't seem to fix it. Your help will be greatly appreciated. Thank you!
","<node.is><postgresql><hero>, fix ""error: server support sal connections"" try access database localhost?, follow hero' node.j tutor proves poster database. great simple table connect localhost:5000/do, get error say ""error: server support sal connections"". i'v search slut hour can't seem fix it. help greatly appreciated. thank you!"
56089400,Postgres Sql `could not determine data type of parameter` by Hibernate,"I have JpaRepository:
@Repository
public interface CardReaderRepository extends JpaRepository&lt;CardReaderEntity, Integer &gt; {
}
when i execute query in this repo same as this:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null or cr.offLoadingDate &gt;= :fromDate ) "" +
            ""AND  (:toDate   is null or cr.offLoadingDate &lt;= :toDate   ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
error is :
org.postgresql.util.PSQLException: ERROR: could not determine data type of parameter $3
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306) ~[postgresql-42.2.2.jar:42.2.2]
but when a change query to:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null ) "" +
            ""AND  (:toDate   is null ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
this work without error but a want run with fromDate and toDate
note:
offLoadingDate in CardReaderEntity:
@Basic
@Temporal( TemporalType.TIMESTAMP )
public Date getOffLoadingDate() {
    return offLoadingDate;
}
public void setOffLoadingDate(Date offLoadingDate) {
    this.offLoadingDate = offLoadingDate;
}
all of Date import in my java files is java.util.Date.
",<sql><postgresql><hibernate><spring-boot><jpa>,3321,0,63,2225,6,21,37,41,60156,0.0,1936,3,31,2019-05-11 10:23,2020-10-06 9:53,,514.0,,Basic,12,"<sql><postgresql><hibernate><spring-boot><jpa>, Postgres Sql `could not determine data type of parameter` by Hibernate, I have JpaRepository:
@Repository
public interface CardReaderRepository extends JpaRepository&lt;CardReaderEntity, Integer &gt; {
}
when i execute query in this repo same as this:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null or cr.offLoadingDate &gt;= :fromDate ) "" +
            ""AND  (:toDate   is null or cr.offLoadingDate &lt;= :toDate   ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
error is :
org.postgresql.util.PSQLException: ERROR: could not determine data type of parameter $3
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306) ~[postgresql-42.2.2.jar:42.2.2]
but when a change query to:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null ) "" +
            ""AND  (:toDate   is null ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
this work without error but a want run with fromDate and toDate
note:
offLoadingDate in CardReaderEntity:
@Basic
@Temporal( TemporalType.TIMESTAMP )
public Date getOffLoadingDate() {
    return offLoadingDate;
}
public void setOffLoadingDate(Date offLoadingDate) {
    this.offLoadingDate = offLoadingDate;
}
all of Date import in my java files is java.util.Date.
","<sal><postgresql><liberate><spring-boot><pa>, poster sal `could determine data type parameter` liberate, jparepository: @depositors public interface cardreaderrepositori extend jparepository&it;cardreaderentity, inter &it; { } execute query rep this: @query( value = ""select new in.server.component.panel.response."" + ""invalidcardreaderdatadigestresponse("" + "" or.drivernumber, or.exploitercode, or.linecode, "" + "" or.transactiondate, or.offloadingdate, "" + "" or.cardreadernumber, "" + "" sum(or.count) , sum(or.remind) "" + "") "" + ""from cardreaderent or "" + ""where or.company.id = :company "" + ""and or.status.id :status "" + ""and or.delete = fall "" + ""and (:format null or.offloadingd &it;= :format ) "" + ""and (:today null or.offloadingd &it;= :today ) "" + ""group or.drivernumber, or.exploitercode, or.linecode, or.transactiondate, or.offloadingdate, or.cardreadernumber"" ) page&it;invalidcardreaderdatadigestresponse&it; findallinvalidcardreaderdatadigestbycompanyidandstatusidin( @parma( ""company"" ) in company, @parma( ""fromdate"" ) date fromdate, @parma( ""today"" ) date today, @parma( ""statusids"" ) list&it;inter&it; statusids, pageabl pageabl ); error : org.postgresql.until.psqlexception: error: could determine data type parapet $3 org.postgresql.core.ve.queryexecutorimpl.receiveerrorresponse(queryexecutorimpl.cava:2433) ~[postgresql-42.2.2.jar:42.2.2] org.postgresql.core.ve.queryexecutorimpl.processresults(queryexecutorimpl.cava:2178) ~[postgresql-42.2.2.jar:42.2.2] org.postgresql.core.ve.queryexecutorimpl.execute(queryexecutorimpl.cava:306) ~[postgresql-42.2.2.jar:42.2.2] change query to: @query( value = ""select new in.server.component.panel.response."" + ""invalidcardreaderdatadigestresponse("" + "" or.drivernumber, or.exploitercode, or.linecode, "" + "" or.transactiondate, or.offloadingdate, "" + "" or.cardreadernumber, "" + "" sum(or.count) , sum(or.remind) "" + "") "" + ""from cardreaderent or "" + ""where or.company.id = :company "" + ""and or.status.id :status "" + ""and or.delete = fall "" + ""and (:format null ) "" + ""and (:today null ) "" + ""group or.drivernumber, or.exploitercode, or.linecode, or.transactiondate, or.offloadingdate, or.cardreadernumber"" ) page&it;invalidcardreaderdatadigestresponse&it; findallinvalidcardreaderdatadigestbycompanyidandstatusidin( @parma( ""company"" ) in company, @parma( ""fromdate"" ) date fromdate, @parma( ""today"" ) date today, @parma( ""statusids"" ) list&it;inter&it; statusids, pageabl pageabl ); work without error want run format today note: offloadingd cardreaderentity: @basic @temporal( temporaltype.timestamp ) public date getoffloadingdate() { return offloadingdate; } public void setoffloadingdate(d offloadingdate) { this.offloadingd = offloadingdate; } date import cava file cava.until.date."
57451719,"Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column","I have a json file:
{
  &quot;a&quot;: {
    &quot;b&quot;: 1
  }
}
I am trying to read it:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path)
df.show()
But getting an error:
Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException:
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed
when the referenced columns only include the internal corrupt record
column (named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($&quot;_corrupt_record&quot;.isNotNull).count()
and
spark.read.schema(schema).json(file).select(&quot;_corrupt_record&quot;).show().
Instead, you can cache or save the parsed results and then send the
same query. For example, val df =
spark.read.schema(schema).json(file).cache() and then
df.filter($&quot;_corrupt_record&quot;.isNotNull).count().;
So I tried to cache it as they suggest:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path).cache()
df.show()
But I keep getting the same error.
",<json><scala><apache-spark><apache-spark-sql>,1038,0,11,10709,24,92,152,80,42093,0.0,886,4,31,2019-08-11 16:27,2019-08-12 6:28,2019-08-12 6:28,1.0,1.0,Basic,14,"<json><scala><apache-spark><apache-spark-sql>, Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column, I have a json file:
{
  &quot;a&quot;: {
    &quot;b&quot;: 1
  }
}
I am trying to read it:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path)
df.show()
But getting an error:
Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException:
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed
when the referenced columns only include the internal corrupt record
column (named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($&quot;_corrupt_record&quot;.isNotNull).count()
and
spark.read.schema(schema).json(file).select(&quot;_corrupt_record&quot;).show().
Instead, you can cache or save the parsed results and then send the
same query. For example, val df =
spark.read.schema(schema).json(file).cache() and then
df.filter($&quot;_corrupt_record&quot;.isNotNull).count().;
So I tried to cache it as they suggest:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path).cache()
df.show()
But I keep getting the same error.
","<son><scala><apache-spark><apache-spark-sal>, since spark 2.3, query raw son/is file sallow reference column include inter corrupt record column, son file: { &quit;a&quit;: { &quit;b&quit;: 1 } } try read it: val path = &quit;d:/playground/input.son&quit; val of = spark.read.son(path) of.show() get error: except thread &quit;main&quit; org.apache.spark.sal.analysisexception: since spark 2.3, query raw son/is file sallow reference column include inter corrupt record column (name _corrupt_record default). example: spark.read.scheme(scheme).son(file).filter($&quit;_corrupt_record&quit;.isnotnull).count() spark.read.scheme(scheme).son(file).select(&quit;_corrupt_record&quit;).show(). instead, each save part result send query. example, val of = spark.read.scheme(scheme).son(file).ache() of.filter($&quit;_corrupt_record&quit;.isnotnull).count().; try each suggest: val path = &quit;d:/playground/input.son&quit; val of = spark.read.son(path).ache() of.show() keep get error."
65398641,Docker connect SQL Server container non-zero code: 1,"I'm trying to create a SQL Server container from a docker-compose.yml but when I run it, it directly stops with some errors. Note: it's running on an Apple M1 chip with docker Preview
docker-compose.yml:
version: &quot;3.7&quot;
services:
  sql-server-db:
    container_name: sql-server-db
    image: mcr.microsoft.com/mssql/server:2019-latest
    ports: 
      - &quot;1433:1433&quot;
    environment: 
      SA_PASSWORD: &quot;ApplePassDockerConnect&quot;
      ACCEPT_EULA: &quot;Y&quot;
The errors I'm getting:
sql-server-db | /opt/mssql/bin/sqlservr: Invalid mapping of address 0x40092b8000 in reserved address space below 0x400000000000. Possible causes:
sql-server-db | 1) the process (itself, or via a wrapper) starts-up its own running environment sets the stack size limit to unlimited via syscall setrlimit(2);
sql-server-db | 2) the process (itself, or via a wrapper) adjusts its own execution domain and flag the system its legacy personality via syscall personality(2);
sql-server-db | 3) sysadmin deliberately sets the system to run on legacy VA layout mode by adjusting a sysctl knob vm.legacy_va_layout.
sql-server-db |
sql-server-db exited with code 1
",<sql-server><docker><apple-m1>,1170,0,11,301,1,3,3,57,16478,0.0,0,6,30,2020-12-21 19:12,2020-12-21 21:58,,0.0,,Basic,13,"<sql-server><docker><apple-m1>, Docker connect SQL Server container non-zero code: 1, I'm trying to create a SQL Server container from a docker-compose.yml but when I run it, it directly stops with some errors. Note: it's running on an Apple M1 chip with docker Preview
docker-compose.yml:
version: &quot;3.7&quot;
services:
  sql-server-db:
    container_name: sql-server-db
    image: mcr.microsoft.com/mssql/server:2019-latest
    ports: 
      - &quot;1433:1433&quot;
    environment: 
      SA_PASSWORD: &quot;ApplePassDockerConnect&quot;
      ACCEPT_EULA: &quot;Y&quot;
The errors I'm getting:
sql-server-db | /opt/mssql/bin/sqlservr: Invalid mapping of address 0x40092b8000 in reserved address space below 0x400000000000. Possible causes:
sql-server-db | 1) the process (itself, or via a wrapper) starts-up its own running environment sets the stack size limit to unlimited via syscall setrlimit(2);
sql-server-db | 2) the process (itself, or via a wrapper) adjusts its own execution domain and flag the system its legacy personality via syscall personality(2);
sql-server-db | 3) sysadmin deliberately sets the system to run on legacy VA layout mode by adjusting a sysctl knob vm.legacy_va_layout.
sql-server-db |
sql-server-db exited with code 1
","<sal-server><doctor><apple-my>, doctor connect sal server contain non-zero code: 1, i'm try great sal server contain doctor-compose.you run it, directly stop errors. note: run apply my chip doctor review doctor-compose.you: version: &quit;3.7&quit; services: sal-server-do: container_name: sal-server-do image: mr.microsoft.com/mssql/server:2019-latest ports: - &quit;1433:1433&quit; environment: sa_password: &quit;applepassdockerconnect&quit; accept_eula: &quit;y&quit; error i'm getting: sal-server-do | /opt/mssql/bin/sqlservr: invalid map address 0x40092b8000 reserve address space 0x400000000000. possible causes: sal-server-do | 1) process (itself, via wrapped) starts-up run environs set stick size limit limit via rascal setrlimit(2); sal-server-do | 2) process (itself, via wrapped) adjust execute domain flag system legacy person via rascal personality(2); sal-server-do | 3) sysadmin deliver set system run legacy va layout mode adjust sysctl knob am.legacy_va_layout. sal-server-do | sal-server-do exit code 1"
53489250,"Price aside, why ever choose Google Cloud Bigtable over Google Cloud Datastore?","If I have a use case for both huge data storage and searchability, why would I ever choose Google Cloud Bigtable over Google Cloud Datastore?
I've seen a few questions on SO and other sides ""comparing"" Bigtable and Datastore, but it seems to boil down to the same non-specific answers.
Here's my current knowledge and my thoughts:
  Datastore is more expensive.
In the context of this question, let's forget entirely about pricing.
  Bigtable is good for huge datasets.
It seems like Datastore is, too? I'm not seeing what specifically makes Bigtable objectively superior here.
  Bigtable is better than Datastore for analytics.
How? Why? It seems like I can do analytics in Datastore as well, no problem. Why is Bigtable seemingly the unanimous decision industry-wide for analytics? What value do GMail, eBay, etc. get from Bigtable that Datastore can't provide?
  Bigtable is integrated with Hadoop, Spark, etc.
Is Datastore not as well, considering it's built on Bigtable?
From this question, this statement was made in an answer: 
  Bigtable and Datastore are extremely different. Yes, the datastore is build on top of Bigtable, but that does not make it anything like it. That is kind of like saying a car is build on top of [car] wheels, and so a car is not much different from wheels.
However, this seems analogy seems nonsensical, since the car (including the wheels) intrinsically provides more value than just the wheels of a car by themselves. 
It seems at first glance that Bigtable is strictly worse than Datastore, only providing a single index and limiting quick searchability.  What am I missing?
",<google-cloud-platform><nosql><google-cloud-datastore><bigtable><google-cloud-bigtable>,1613,1,0,435,1,6,9,41,15053,0.0,2,1,30,2018-11-26 21:21,2018-11-27 19:41,2018-11-27 19:41,1.0,1.0,Intermediate,17,"<google-cloud-platform><nosql><google-cloud-datastore><bigtable><google-cloud-bigtable>, Price aside, why ever choose Google Cloud Bigtable over Google Cloud Datastore?, If I have a use case for both huge data storage and searchability, why would I ever choose Google Cloud Bigtable over Google Cloud Datastore?
I've seen a few questions on SO and other sides ""comparing"" Bigtable and Datastore, but it seems to boil down to the same non-specific answers.
Here's my current knowledge and my thoughts:
  Datastore is more expensive.
In the context of this question, let's forget entirely about pricing.
  Bigtable is good for huge datasets.
It seems like Datastore is, too? I'm not seeing what specifically makes Bigtable objectively superior here.
  Bigtable is better than Datastore for analytics.
How? Why? It seems like I can do analytics in Datastore as well, no problem. Why is Bigtable seemingly the unanimous decision industry-wide for analytics? What value do GMail, eBay, etc. get from Bigtable that Datastore can't provide?
  Bigtable is integrated with Hadoop, Spark, etc.
Is Datastore not as well, considering it's built on Bigtable?
From this question, this statement was made in an answer: 
  Bigtable and Datastore are extremely different. Yes, the datastore is build on top of Bigtable, but that does not make it anything like it. That is kind of like saying a car is build on top of [car] wheels, and so a car is not much different from wheels.
However, this seems analogy seems nonsensical, since the car (including the wheels) intrinsically provides more value than just the wheels of a car by themselves. 
It seems at first glance that Bigtable is strictly worse than Datastore, only providing a single index and limiting quick searchability.  What am I missing?
","<goose-cloud-platform><nose><goose-cloud-datastore><bigtable><goose-cloud-bigtable>, price aside, ever choose good cloud pigtail good cloud datastore?, use case huge data storage searchability, would ever choose good cloud pigtail good cloud datastore? i'v seen question side ""comparing"" pigtail datastore, seem boil non-specie answers. here' current knowledge thoughts: datastor expensive. context question, let' forget enter pricking. pigtail good huge datasets. seem like datastor is, too? i'm see specie make pigtail object superior here. pigtail better datastor analysis. how? why? seem like analyst datastor well, problem. pigtail seemingly znaim denis industry-wid analysis? value email, bay, etc. get pigtail datastor can't provide? pigtail inter hadoop, spark, etc. datastor well, consider built bigtable? question, statement made answer: pigtail datastor extreme different. yes, datastor build top bigtable, make any like it. kind like say car build top [car] wheels, car much differ wheels. however, seem analogy seem nonsensical, since car (include wheels) entries proved value wheel car themselves. seem first glance pigtail strictly words datastore, proved single index limit quick searchability. missing?"
50927740,"SQLAlchemy: ""create schema if not exists""","I want to do the ""CREATE SCHEMA IF NOT EXISTS"" query in SQLAlchemy.
Is there a better way than this:
    engine = sqlalchemy.create_engine(connstr)
    schema_name = config.get_config_value('db', 'schema_name')
    #Create schema; if it already exists, skip this
    try:
        engine.execute(CreateSchema(schema_name))
    except sqlalchemy.exc.ProgrammingError:
        pass
I am using Python 3.5.
",<python><sql><database><python-3.x><sqlalchemy>,402,0,9,1103,3,14,29,40,25948,0.0,987,5,30,2018-06-19 11:51,2019-03-25 19:50,2019-03-25 19:50,279.0,279.0,Basic,10,"<python><sql><database><python-3.x><sqlalchemy>, SQLAlchemy: ""create schema if not exists"", I want to do the ""CREATE SCHEMA IF NOT EXISTS"" query in SQLAlchemy.
Is there a better way than this:
    engine = sqlalchemy.create_engine(connstr)
    schema_name = config.get_config_value('db', 'schema_name')
    #Create schema; if it already exists, skip this
    try:
        engine.execute(CreateSchema(schema_name))
    except sqlalchemy.exc.ProgrammingError:
        pass
I am using Python 3.5.
","<patron><sal><database><patron-3.x><sqlalchemy>, sqlalchemy: ""great scheme exists"", want ""great scheme exists"" query sqlalchemy. better way this: engine = sqlalchemy.create_engine(connstr) schema_nam = confirm.get_config_value('do', 'schema_name') #great scheme; already exists, skin try: engine.execute(createschema(schema_name)) except sqlalchemy.etc.programmingerror: pass use patron 3.5."
50974851,How to optimise this MySQL query? Millions of Rows,"I have the following query:
SELECT 
    analytics.source AS referrer, 
    COUNT(analytics.id) AS frequency, 
    SUM(IF(transactions.status = 'COMPLETED', 1, 0)) AS sales
FROM analytics
LEFT JOIN transactions ON analytics.id = transactions.analytics
WHERE analytics.user_id = 52094 
GROUP BY analytics.source 
ORDER BY frequency DESC 
LIMIT 10 
The analytics table has 60M rows and the transactions table has 3M rows.
When I run an EXPLAIN on this query, I get:
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| # id |  select_type |      table      |  type  |    possible_keys    |        key        |        key_len       |            ref            |   rows   |   Extra   |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| '1'  |  'SIMPLE'    |  'analytics'    |  'ref' |  'analytics_user_id | analytics_source' |  'analytics_user_id' |  '5'                      |  'const' |  '337662' |  'Using where; Using temporary; Using filesort' |
| '1'  |  'SIMPLE'    |  'transactions' |  'ref' |  'tran_analytics'   |  'tran_analytics' |  '5'                 |  'dijishop2.analytics.id' |  '1'     |  NULL     |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
I can't figure out how to optimise this query as it's already very basic. It takes around 70 seconds to run this query.
Here are the indexes that exist:
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|   # Table   |  Non_unique |          Key_name          |  Seq_in_index |    Column_name   |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'analytics' |  '0'        |  'PRIMARY'                 |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_user_id'       |  '1'          |  'user_id'       |  'A'       |  '130583'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_product_id'    |  '1'          |  'product_id'    |  'A'       |  '490812'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_affil_user_id' |  '1'          |  'affil_user_id' |  'A'       |  '55222'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_source'        |  '1'          |  'source'        |  'A'       |  '24604'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_country_name'  |  '1'          |  'country_name'  |  'A'       |  '39510'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '2'          |  'user_id'       |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '3'          |  'source'        |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|    # Table     |  Non_unique |      Key_name     |  Seq_in_index |    Column_name    |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'transactions' |  '0'        |  'PRIMARY'        |  '1'          |  'id'             |  'A'       |  '2436151'   |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_user_id'   |  '1'          |  'user_id'        |  'A'       |  '56654'     |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'transaction_id' |  '1'          |  'transaction_id' |  'A'       |  '2436151'   |  '191'    |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_analytics' |  '1'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_status'    |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '2'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
Simplified schema for the two tables before adding any extra indexes as suggested as it didn't improve the situation.
CREATE TABLE `analytics` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) DEFAULT NULL,
  `affil_user_id` int(11) DEFAULT NULL,
  `product_id` int(11) DEFAULT NULL,
  `medium` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `source` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `terms` varchar(1024) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `is_browser` tinyint(1) DEFAULT NULL,
  `is_mobile` tinyint(1) DEFAULT NULL,
  `is_robot` tinyint(1) DEFAULT NULL,
  `browser` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `mobile` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `robot` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `platform` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `referrer` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `domain` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `ip` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `continent_code` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `country_name` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `city` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `analytics_user_id` (`user_id`),
  KEY `analytics_product_id` (`product_id`),
  KEY `analytics_affil_user_id` (`affil_user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=64821325 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
CREATE TABLE `transactions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `transaction_id` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  `pay_key` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sender_email` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `amount` decimal(10,2) DEFAULT NULL,
  `currency` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `status` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `analytics` int(11) DEFAULT NULL,
  `ip_address` varchar(46) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `session_id` varchar(60) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `eu_vat_applied` int(1) DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `tran_user_id` (`user_id`),
  KEY `transaction_id` (`transaction_id`(191)),
  KEY `tran_analytics` (`analytics`),
  KEY `tran_status` (`status`)
) ENGINE=InnoDB AUTO_INCREMENT=10019356 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
If the above can not be optimised any further. Any implementation advice on summary tables will be great. We are using a LAMP stack on AWS. The above query is running on RDS (m1.large).
",<mysql><sql><query-optimization><amazon-rds><sql-optimization>,9303,0,91,56442,101,275,411,39,4394,0.0,706,13,30,2018-06-21 17:54,2018-06-21 17:55,2018-06-24 13:19,0.0,3.0,Intermediate,23,"<mysql><sql><query-optimization><amazon-rds><sql-optimization>, How to optimise this MySQL query? Millions of Rows, I have the following query:
SELECT 
    analytics.source AS referrer, 
    COUNT(analytics.id) AS frequency, 
    SUM(IF(transactions.status = 'COMPLETED', 1, 0)) AS sales
FROM analytics
LEFT JOIN transactions ON analytics.id = transactions.analytics
WHERE analytics.user_id = 52094 
GROUP BY analytics.source 
ORDER BY frequency DESC 
LIMIT 10 
The analytics table has 60M rows and the transactions table has 3M rows.
When I run an EXPLAIN on this query, I get:
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| # id |  select_type |      table      |  type  |    possible_keys    |        key        |        key_len       |            ref            |   rows   |   Extra   |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| '1'  |  'SIMPLE'    |  'analytics'    |  'ref' |  'analytics_user_id | analytics_source' |  'analytics_user_id' |  '5'                      |  'const' |  '337662' |  'Using where; Using temporary; Using filesort' |
| '1'  |  'SIMPLE'    |  'transactions' |  'ref' |  'tran_analytics'   |  'tran_analytics' |  '5'                 |  'dijishop2.analytics.id' |  '1'     |  NULL     |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
I can't figure out how to optimise this query as it's already very basic. It takes around 70 seconds to run this query.
Here are the indexes that exist:
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|   # Table   |  Non_unique |          Key_name          |  Seq_in_index |    Column_name   |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'analytics' |  '0'        |  'PRIMARY'                 |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_user_id'       |  '1'          |  'user_id'       |  'A'       |  '130583'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_product_id'    |  '1'          |  'product_id'    |  'A'       |  '490812'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_affil_user_id' |  '1'          |  'affil_user_id' |  'A'       |  '55222'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_source'        |  '1'          |  'source'        |  'A'       |  '24604'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_country_name'  |  '1'          |  'country_name'  |  'A'       |  '39510'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '2'          |  'user_id'       |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '3'          |  'source'        |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|    # Table     |  Non_unique |      Key_name     |  Seq_in_index |    Column_name    |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'transactions' |  '0'        |  'PRIMARY'        |  '1'          |  'id'             |  'A'       |  '2436151'   |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_user_id'   |  '1'          |  'user_id'        |  'A'       |  '56654'     |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'transaction_id' |  '1'          |  'transaction_id' |  'A'       |  '2436151'   |  '191'    |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_analytics' |  '1'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_status'    |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '2'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
Simplified schema for the two tables before adding any extra indexes as suggested as it didn't improve the situation.
CREATE TABLE `analytics` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) DEFAULT NULL,
  `affil_user_id` int(11) DEFAULT NULL,
  `product_id` int(11) DEFAULT NULL,
  `medium` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `source` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `terms` varchar(1024) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `is_browser` tinyint(1) DEFAULT NULL,
  `is_mobile` tinyint(1) DEFAULT NULL,
  `is_robot` tinyint(1) DEFAULT NULL,
  `browser` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `mobile` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `robot` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `platform` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `referrer` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `domain` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `ip` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `continent_code` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `country_name` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `city` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `analytics_user_id` (`user_id`),
  KEY `analytics_product_id` (`product_id`),
  KEY `analytics_affil_user_id` (`affil_user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=64821325 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
CREATE TABLE `transactions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `transaction_id` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  `pay_key` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sender_email` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `amount` decimal(10,2) DEFAULT NULL,
  `currency` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `status` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `analytics` int(11) DEFAULT NULL,
  `ip_address` varchar(46) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `session_id` varchar(60) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `eu_vat_applied` int(1) DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `tran_user_id` (`user_id`),
  KEY `transaction_id` (`transaction_id`(191)),
  KEY `tran_analytics` (`analytics`),
  KEY `tran_status` (`status`)
) ENGINE=InnoDB AUTO_INCREMENT=10019356 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
If the above can not be optimised any further. Any implementation advice on summary tables will be great. We are using a LAMP stack on AWS. The above query is running on RDS (m1.large).
","<myself><sal><query-optimization><amazon-rd><sal-optimization>, optimism myself query? million rows, follow query: select analysis.source referred, count(analysis.id) frequency, sum(if(transactions.state = 'completed', 1, 0)) sale analyst left join transact analysis.id = transactions.analyst analysis.user_id = 52094 group analysis.source order frequent desk limit 10 analyst table him row transact table am rows. run explain query, get: +------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+ | # id | select_typ | table | type | possible_key | key | key_len | red | row | extra | | +------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+ | '1' | 'simple' | 'analysis' | 'red' | 'analytics_user_id | analytics_source' | 'analytics_user_id' | '5' | 'cost' | '337662' | 'use where; use temporary; use filesort' | | '1' | 'simple' | 'transactions' | 'red' | 'tran_analytics' | 'tran_analytics' | '5' | 'dijishop2.analysis.id' | '1' | null | | +------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+ can't figure optimism query already basic. take around 70 second run query. index exist: +-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ | # table | non_uniqu | key_nam | seq_in_index | column_nam | collar | carding | sub_part | pack | null | index_typ | comment | index_com | +-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ | 'analysis' | '0' | 'primary' | '1' | 'id' | 'a' | '56934235' | null | null | '' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_user_id' | '1' | 'user_id' | 'a' | '130583' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_product_id' | '1' | 'product_id' | 'a' | '490812' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_affil_user_id' | '1' | 'affil_user_id' | 'a' | '55222' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_source' | '1' | 'source' | 'a' | '24604' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_country_name' | '1' | 'country_name' | 'a' | '39510' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_gordon' | '1' | 'id' | 'a' | '56934235' | null | null | '' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_gordon' | '2' | 'user_id' | 'a' | '56934235' | null | null | 'yes' | 'tree' | '' | '' | | 'analysis' | '1' | 'analytics_gordon' | '3' | 'source' | 'a' | '56934235' | null | null | 'yes' | 'tree' | '' | '' | +-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ +----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ | # table | non_uniqu | key_nam | seq_in_index | column_nam | collar | carding | sub_part | pack | null | index_typ | comment | index_com | +----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ | 'transactions' | '0' | 'primary' | '1' | 'id' | 'a' | '2436151' | null | null | '' | 'tree' | '' | '' | | 'transactions' | '1' | 'tran_user_id' | '1' | 'user_id' | 'a' | '56654' | null | null | '' | 'tree' | '' | '' | | 'transactions' | '1' | 'transaction_id' | '1' | 'transaction_id' | 'a' | '2436151' | '191' | null | 'yes' | 'tree' | '' | '' | | 'transactions' | '1' | 'tran_analytics' | '1' | 'analysis' | 'a' | '2436151' | null | null | 'yes' | 'tree' | '' | '' | | 'transactions' | '1' | 'tran_status' | '1' | 'status' | 'a' | '22' | null | null | 'yes' | 'tree' | '' | '' | | 'transactions' | '1' | 'gordon_trans' | '1' | 'status' | 'a' | '22' | null | null | 'yes' | 'tree' | '' | '' | | 'transactions' | '1' | 'gordon_trans' | '2' | 'analysis' | 'a' | '2436151' | null | null | 'yes' | 'tree' | '' | '' | +----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+ simplify scheme two table ad extra index suggest improve situation. great table `analysis` ( `id` in(11) null auto_increment, `user_id` in(11) default null, `affil_user_id` in(11) default null, `product_id` in(11) default null, `medium` varchar(45) collar utf8mb4_unicode_ci default null, `source` varchar(45) collar utf8mb4_unicode_ci default null, `terms` varchar(1024) collar utf8mb4_unicode_ci default null, `is_browser` tinyint(1) default null, `immobile` tinyint(1) default null, `is_robot` tinyint(1) default null, `brother` varchar(45) collar utf8mb4_unicode_ci default null, `mobile` varchar(45) collar utf8mb4_unicode_ci default null, `root` varchar(45) collar utf8mb4_unicode_ci default null, `platform` varchar(45) collar utf8mb4_unicode_ci default null, `referred` varchar(255) collar utf8mb4_unicode_ci default null, `domain` varchar(45) collar utf8mb4_unicode_ci default null, `in` varchar(255) collar utf8mb4_unicode_ci default null, `continent_code` varchar(10) collar utf8mb4_unicode_ci default null, `country_name` varchar(100) collar utf8mb4_unicode_ci default null, `city` varchar(100) collar utf8mb4_unicode_ci default null, `date` timestamp null default current_timestamp, primary key (`id`), key `analytics_user_id` (`user_id`), key `analytics_product_id` (`product_id`), key `analytics_affil_user_id` (`affil_user_id`) ) engine=innodb auto_increment=64821325 default charge=utf8mb4 collar=utf8mb4_unicode_ci; great table `transactions` ( `id` in(11) null auto_increment, `transaction_id` varchar(255) collar utf8mb4_unicode_ci default null, `user_id` in(11) null, `pay_key` varchar(50) collar utf8mb4_unicode_ci default null, `sender_email` varchar(255) collar utf8mb4_unicode_ci default null, `amount` denial(10,2) default null, `currency` varchar(10) collar utf8mb4_unicode_ci default null, `status` varchar(50) collar utf8mb4_unicode_ci default null, `analysis` in(11) default null, `ip_address` varchar(46) collar utf8mb4_unicode_ci default null, `session_id` varchar(60) collar utf8mb4_unicode_ci default null, `date` timestamp null default current_timestamp, `eu_vat_applied` in(1) default '0', primary key (`id`), key `tran_user_id` (`user_id`), key `transaction_id` (`transaction_id`(191)), key `tran_analytics` (`analysis`), key `tran_status` (`status`) ) engine=innodb auto_increment=10019356 default charge=utf8mb4 collar=utf8mb4_unicode_ci; optimism further. implement advice summary table great. use lamp stick was. query run rd (my.large)."
48703318,MySql.Data.EntityFrameworkCore vs Pomelo.EntityFrameworkCore.MySql,"Which database connector should I use in .Net Core 2 enterprise level web app which is going to handle large amount of data.
I have two choices:
Pomelo.EntityFrameworkCore.MySql
MySql.Data.EntityFrameworkCore
But still confuse which one to choose for the development.
I started with MySql.Data.EntityFrameworkCore, which is MySQL official provider but after facing several issues while code first migration I had to google again for some better alternative.
After some research I found Pomelo.EntityFrameworkCore.MySql bit more helpful for my application and it also covered the issues I was facing before.
But it still left me a bit confuse about which one to choose for long term.
Pomelo is workign fine currently but I am not sure if they (pomelo team) will keep always keep it updating and will keep it in sync with the latest .Net Core version available in the market??
MySql is not working as expected but the only plus point with this is : It is provided by MySQl itself.
Please help me to decide
",<mysql><entity-framework><entity-framework-core><asp.net-core-2.0><pomelo-entityframeworkcore-mysql>,1004,0,0,910,1,7,20,78,21118,0.0,11,0,30,2018-02-09 10:05,,,,,Intermediate,21,"<mysql><entity-framework><entity-framework-core><asp.net-core-2.0><pomelo-entityframeworkcore-mysql>, MySql.Data.EntityFrameworkCore vs Pomelo.EntityFrameworkCore.MySql, Which database connector should I use in .Net Core 2 enterprise level web app which is going to handle large amount of data.
I have two choices:
Pomelo.EntityFrameworkCore.MySql
MySql.Data.EntityFrameworkCore
But still confuse which one to choose for the development.
I started with MySql.Data.EntityFrameworkCore, which is MySQL official provider but after facing several issues while code first migration I had to google again for some better alternative.
After some research I found Pomelo.EntityFrameworkCore.MySql bit more helpful for my application and it also covered the issues I was facing before.
But it still left me a bit confuse about which one to choose for long term.
Pomelo is workign fine currently but I am not sure if they (pomelo team) will keep always keep it updating and will keep it in sync with the latest .Net Core version available in the market??
MySql is not working as expected but the only plus point with this is : It is provided by MySQl itself.
Please help me to decide
","<myself><entity-framework><entity-framework-core><asp.net-core-2.0><homely-entityframeworkcore-myself>, myself.data.entityframeworkcor vs homely.entityframeworkcore.myself, database connection use .net core 2 enterprise level web pp go hand large amount data. two choice: homely.entityframeworkcore.myself myself.data.entityframeworkcor still confuse one choose development. start myself.data.entityframeworkcore, myself office proved face never issue code first migrate good better alternative. research found homely.entityframeworkcore.myself bit help applied also cover issue face before. still left bit confuse one choose long term. homely working fine current sure (homely team) keep away keep update keep son latest .net core version avail market?? myself work expect plus point : proved myself itself. pleas help decide"
48872965,Postgres: \copy syntax,"With PostgreSQL 9.5 on CentOS 7, I have created a database named sample along with several tables.  I have .csv data in /home/MyUser/data for each table. 
 For example, there exists TableName.csv for the table ""TableName"".
How do I load the csv files into each table?
What I've tried doesn't work and I can't figure out what I'm doing wrong.
Load from within the DB
$ psql sample
sample=# COPY ""TableName"" FROM '/home/MyUser/data/TableName.csv' WITH CSV;
ERROR:  could not open file ""/home/MyUser/data/TableName.csv"" for reading: Permission denied
This implies a file permission problem.  All the files in data/ are -rw-r--r-- and the directory itself is drwxr-xr-x.  So file permissions shouldn't be the problem (unless I'm missing something).  The internet says that COPY has problems with permissions and to try \copy.  
Load from CLI
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH CSV
psql: warning: extra command-line argument ""FROM"" ignored
psql: warning: extra command-line argument ""/home/MyUser/data/TableName.csv"" ignored
psql: warning: extra command-line argument ""WITH"" ignored
psql: warning: extra command-line argument ""CSV"" ignored
psql: FATAL:  Peer authentication failed for user ""sample""
This appears to be a syntax error, but I'm not finding the documentation particularly helpful (man psql then /\copy).  I've also tried the following to the same result.
$ psql \copy sample.""TableName"" FROM /home/MyUser/data/TableName.csv WITH CSV
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH DELIMITER ','
There are several other permutations which yield similar errors.
Web Resources Used
https://www.postgresql.org/docs/9.5/static/app-psql.html
https://www.postgresql.org/docs/9.5/static/sql-copy.html
The correct COPY command to load postgreSQL data from csv file that has single-quoted data?
https://soleil4716.wordpress.com/2010/08/19/using-copy-command-in-postgresql/
Can I use \copy command into a function of postgresql?
https://wiki.postgresql.org/wiki/COPY
",<postgresql>,2005,10,26,4120,4,43,69,36,91883,0.0,406,2,30,2018-02-19 19:31,2018-02-20 9:14,2018-02-20 9:14,1.0,1.0,Intermediate,21,"<postgresql>, Postgres: \copy syntax, With PostgreSQL 9.5 on CentOS 7, I have created a database named sample along with several tables.  I have .csv data in /home/MyUser/data for each table. 
 For example, there exists TableName.csv for the table ""TableName"".
How do I load the csv files into each table?
What I've tried doesn't work and I can't figure out what I'm doing wrong.
Load from within the DB
$ psql sample
sample=# COPY ""TableName"" FROM '/home/MyUser/data/TableName.csv' WITH CSV;
ERROR:  could not open file ""/home/MyUser/data/TableName.csv"" for reading: Permission denied
This implies a file permission problem.  All the files in data/ are -rw-r--r-- and the directory itself is drwxr-xr-x.  So file permissions shouldn't be the problem (unless I'm missing something).  The internet says that COPY has problems with permissions and to try \copy.  
Load from CLI
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH CSV
psql: warning: extra command-line argument ""FROM"" ignored
psql: warning: extra command-line argument ""/home/MyUser/data/TableName.csv"" ignored
psql: warning: extra command-line argument ""WITH"" ignored
psql: warning: extra command-line argument ""CSV"" ignored
psql: FATAL:  Peer authentication failed for user ""sample""
This appears to be a syntax error, but I'm not finding the documentation particularly helpful (man psql then /\copy).  I've also tried the following to the same result.
$ psql \copy sample.""TableName"" FROM /home/MyUser/data/TableName.csv WITH CSV
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH DELIMITER ','
There are several other permutations which yield similar errors.
Web Resources Used
https://www.postgresql.org/docs/9.5/static/app-psql.html
https://www.postgresql.org/docs/9.5/static/sql-copy.html
The correct COPY command to load postgreSQL data from csv file that has single-quoted data?
https://soleil4716.wordpress.com/2010/08/19/using-copy-command-in-postgresql/
Can I use \copy command into a function of postgresql?
https://wiki.postgresql.org/wiki/COPY
","<postgresql>, postures: \copy santa, postgresql 9.5 cent 7, great database name sample along never tables. .is data /home/user/data table. example, exist tablename.is table ""tablename"". load is file table? i'v try work can't figure i'm wrong. load within do $ pool sample sample=# copy ""tablename"" '/home/user/data/tablename.is' is; error: could open file ""/home/user/data/tablename.is"" reading: permits den imply file permits problem. file data/ -re-r--r-- director drawer-or-x. file permits problem (unless i'm miss something). internet say copy problem permits try \copy. load coli $ pool \copy sample /home/user/data/tablename.is is pool: warning: extra command-in argument ""from"" ignore pool: warning: extra command-in argument ""/home/user/data/tablename.is"" ignore pool: warning: extra command-in argument ""with"" ignore pool: warning: extra command-in argument ""is"" ignore pool: fatal: peer authentic fail user ""sample"" appear santa error, i'm find document particularly help (man pool /\copy). i'v also try follow result. $ pool \copy sample.""tablename"" /home/user/data/tablename.is is $ pool \copy sample /home/user/data/tablename.is delight ',' never permit yield similar errors. web resource use http://www.postgresql.org/docs/9.5/static/pp-pool.html http://www.postgresql.org/docs/9.5/static/sal-copy.html correct copy command load postgresql data is file single-quit data? http://soleil4716.wordpress.com/2010/08/19/using-copy-command-in-postgresql/ use \copy command function postgresql? http://wiki.postgresql.org/wiki/copy"
48446399,SQL auto increment pgadmin 4,"I am trying to make a simple database with an number generator but why do I get the error below?
  ERROR:  syntax error at or near ""AUTO_INCREMENT""
  LINE 2: IDNumber int NOT NULL AUTO_INCREMENT,
Code:
CREATE TABLE Finance
(
    IDNumber int NOT NULL AUTO_INCREMENT,
    FinName varchar(50) NOT NULL,
    PRIMARY KEY(IDNumber)
);
",<sql>,330,0,6,497,1,5,14,78,80449,0.0,9,6,29,2018-01-25 15:27,2018-01-25 15:30,2018-01-25 15:32,0.0,0.0,Basic,8,"<sql>, SQL auto increment pgadmin 4, I am trying to make a simple database with an number generator but why do I get the error below?
  ERROR:  syntax error at or near ""AUTO_INCREMENT""
  LINE 2: IDNumber int NOT NULL AUTO_INCREMENT,
Code:
CREATE TABLE Finance
(
    IDNumber int NOT NULL AUTO_INCREMENT,
    FinName varchar(50) NOT NULL,
    PRIMARY KEY(IDNumber)
);
","<sal>, sal auto incitement pgadmin 4, try make simple database number genet get error below? error: santa error near ""auto_increment"" line 2: dumb in null auto_increment, code: great table finance ( dumb in null auto_increment, final varchar(50) null, primary key(number) );"
63585525,SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key are incompatible,"I´m doing migrations in Laravel and this error happens when I proceed with the command PHP artisan migrate:
In Connection.php line 664:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible. (SQL: alter table almacen_movimientos add constraint almacen_movimientos_user_id_foreign foreign key (user_id) references users (id
) on delete restrict)
In PDOStatement.php line 129:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible.
My migrations look like this:
almacen_movimientos table
public function up()
{
    Schema::create('almacen_movimientos', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;integer('cliente_proveedor_id');
        $table-&gt;integer('empresa_id');
        $table-&gt;integer('user_id');
        $table-&gt;enum('tipo' , ['ENTRADA' , 'SALIDA' , 'REUBICACION' , 'TRASPASO' , 'DEVOLUCION' , 'MSRO' , 'ENTRADA POR TRASPASO' , 'SALIDA POR TRASPASO'])-&gt;nullable();
        $table-&gt;string('referencia' , 255)-&gt;nullable();
        $table-&gt;string('observaciones' , 255)-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;string('transportista' , 255)-&gt;nullable();
        $table-&gt;string('operador' , 255)-&gt;nullable();
        $table-&gt;string('procedencia' , 255)-&gt;nullable();
        $table-&gt;integer('almacen_id')-&gt;nullable();
        $table-&gt;foreign('cliente_proveedor_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('user_id')-&gt;references('id')-&gt;on('users')-&gt;onDelete('restrict');
        $table-&gt;foreign('almacen_id')-&gt;references('id')-&gt;on('almacenes')-&gt;onDelete('restrict');
    });
}
Users Table
public function up()
{
    Schema::create('users', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;string('name' , 255);
        $table-&gt;string('apellido_paterno' , 115)-&gt;nullable();
        $table-&gt;string('apellido_materno' , 115)-&gt;nullable();
        $table-&gt;dateTime('fecha_nacimiento')-&gt;nullable();
        $table-&gt;string('telefono1' , 10)-&gt;nullable();
        $table-&gt;string('telefono2' , 10)-&gt;nullable();
        $table-&gt;string('calle' , 255)-&gt;nullable();
        $table-&gt;string('numero' , 45)-&gt;nullable();
        $table-&gt;string('colonia' , 255)-&gt;nullable();
        $table-&gt;string('codigo_postal' , 6)-&gt;nullable();
        $table-&gt;string('email' , 255)-&gt;unique();
        $table-&gt;string('user' , 20)-&gt;nullable()-&gt;unique();
        $table-&gt;string('password' , 255);
        $table-&gt;string('palabra_secreta' , 255);
        $table-&gt;string('remember_token' , 100)-&gt;nullable();
        $table-&gt;unsignedInteger('empresa_id')-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
    });
}
Can somebopdy tell me what am I doing wrong? I cannot fix this.
Thank you.
Regards.
",<php><mysql><laravel>,3540,0,57,429,1,6,14,57,50082,0.0,7,6,29,2020-08-25 19:03,2020-08-25 19:09,,0.0,,Basic,10,"<php><mysql><laravel>, SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key are incompatible, I´m doing migrations in Laravel and this error happens when I proceed with the command PHP artisan migrate:
In Connection.php line 664:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible. (SQL: alter table almacen_movimientos add constraint almacen_movimientos_user_id_foreign foreign key (user_id) references users (id
) on delete restrict)
In PDOStatement.php line 129:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible.
My migrations look like this:
almacen_movimientos table
public function up()
{
    Schema::create('almacen_movimientos', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;integer('cliente_proveedor_id');
        $table-&gt;integer('empresa_id');
        $table-&gt;integer('user_id');
        $table-&gt;enum('tipo' , ['ENTRADA' , 'SALIDA' , 'REUBICACION' , 'TRASPASO' , 'DEVOLUCION' , 'MSRO' , 'ENTRADA POR TRASPASO' , 'SALIDA POR TRASPASO'])-&gt;nullable();
        $table-&gt;string('referencia' , 255)-&gt;nullable();
        $table-&gt;string('observaciones' , 255)-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;string('transportista' , 255)-&gt;nullable();
        $table-&gt;string('operador' , 255)-&gt;nullable();
        $table-&gt;string('procedencia' , 255)-&gt;nullable();
        $table-&gt;integer('almacen_id')-&gt;nullable();
        $table-&gt;foreign('cliente_proveedor_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('user_id')-&gt;references('id')-&gt;on('users')-&gt;onDelete('restrict');
        $table-&gt;foreign('almacen_id')-&gt;references('id')-&gt;on('almacenes')-&gt;onDelete('restrict');
    });
}
Users Table
public function up()
{
    Schema::create('users', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;string('name' , 255);
        $table-&gt;string('apellido_paterno' , 115)-&gt;nullable();
        $table-&gt;string('apellido_materno' , 115)-&gt;nullable();
        $table-&gt;dateTime('fecha_nacimiento')-&gt;nullable();
        $table-&gt;string('telefono1' , 10)-&gt;nullable();
        $table-&gt;string('telefono2' , 10)-&gt;nullable();
        $table-&gt;string('calle' , 255)-&gt;nullable();
        $table-&gt;string('numero' , 45)-&gt;nullable();
        $table-&gt;string('colonia' , 255)-&gt;nullable();
        $table-&gt;string('codigo_postal' , 6)-&gt;nullable();
        $table-&gt;string('email' , 255)-&gt;unique();
        $table-&gt;string('user' , 20)-&gt;nullable()-&gt;unique();
        $table-&gt;string('password' , 255);
        $table-&gt;string('palabra_secreta' , 255);
        $table-&gt;string('remember_token' , 100)-&gt;nullable();
        $table-&gt;unsignedInteger('empresa_id')-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
    });
}
Can somebopdy tell me what am I doing wrong? I cannot fix this.
Thank you.
Regards.
","<pp><myself><travel>, sqlstate[hy000]: genet error: 3780 reference column 'user_id' reference column 'id' foreign key incompatible, i´m migrate travel error happen proceed command pp artisans migrate: connection.pp line 664: sqlstate[hy000]: genet error: 3780 reference column 'user_id' reference column 'id' foreign key constraint 'almacen_movimientos_user_id_foreign' incompatible. (sal: alter table almacen_movimiento add constraint almacen_movimientos_user_id_foreign foreign key (user_id) refer user (id ) delete restrict) pdostatement.pp line 129: sqlstate[hy000]: genet error: 3780 reference column 'user_id' reference column 'id' foreign key constraint 'almacen_movimientos_user_id_foreign' incompatible. migrate look like this: almacen_movimiento table public function up() { scheme::create('almacen_movimientos', function (blueprint $table) { $table-&it;unsignedbiginteger('id'); $table-&it;inter('cliente_proveedor_id'); $table-&it;inter('empresa_id'); $table-&it;inter('user_id'); $table-&it;end('tips' , ['entrada' , 'saliva' , 'reubicacion' , 'trespass' , 'revolution' , 'so' , 'entrada for trespass' , 'saliva for trespass'])-&it;syllable(); $table-&it;string('reference' , 255)-&it;syllable(); $table-&it;string('observations' , 255)-&it;syllable(); $table-&it;timestamp('created_at'); $table-&it;timestamp('updated_at'); $table-&it;timestamp('deleted_at'); $table-&it;string('transportista' , 255)-&it;syllable(); $table-&it;string('operator' , 255)-&it;syllable(); $table-&it;string('procedencia' , 255)-&it;syllable(); $table-&it;inter('almacen_id')-&it;syllable(); $table-&it;foreign('cliente_proveedor_id')-&it;references('id')-&it;on('empress')-&it;delete('restrict'); $table-&it;foreign('empresa_id')-&it;references('id')-&it;on('empress')-&it;delete('restrict'); $table-&it;foreign('user_id')-&it;references('id')-&it;on('users')-&it;delete('restrict'); $table-&it;foreign('almacen_id')-&it;references('id')-&it;on('almacenes')-&it;delete('restrict'); }); } user table public function up() { scheme::create('users', function (blueprint $table) { $table-&it;unsignedbiginteger('id'); $table-&it;string('name' , 255); $table-&it;string('apellido_paterno' , 115)-&it;syllable(); $table-&it;string('apellido_materno' , 115)-&it;syllable(); $table-&it;daytime('fecha_nacimiento')-&it;syllable(); $table-&it;string('telefono1' , 10)-&it;syllable(); $table-&it;string('telefono2' , 10)-&it;syllable(); $table-&it;string('called' , 255)-&it;syllable(); $table-&it;string('number' , 45)-&it;syllable(); $table-&it;string('colonial' , 255)-&it;syllable(); $table-&it;string('codigo_postal' , 6)-&it;syllable(); $table-&it;string('email' , 255)-&it;unique(); $table-&it;string('user' , 20)-&it;syllable()-&it;unique(); $table-&it;string('password' , 255); $table-&it;string('palabra_secreta' , 255); $table-&it;string('remember_token' , 100)-&it;syllable(); $table-&it;unsignedinteger('empresa_id')-&it;syllable(); $table-&it;timestamp('created_at'); $table-&it;timestamp('updated_at'); $table-&it;timestamp('deleted_at'); $table-&it;foreign('empresa_id')-&it;references('id')-&it;on('empress')-&it;delete('restrict'); }); } somebody tell wrong? cannot fix this. thank you. regards."
55509638,"Could not load file or assembly 'System.Memory, Version=4.0.1.' in Visual Studio 2015","For a couple of months i had no issue generating the model from DB by deleting it and recreating it . After a pull from git, an issue has been occurred while trying to make the same process . After the second step (connection string creation with DB) there is no further proceed at the 3rd step and no connection string with the data base is being created at the app.config file.I have tried to test the connection with the database credentials and i am getting the following .
When i try to update specific tables from the model diagram as an alternative i get also the below :
System.Data.Entity.Core.EntityException: An error occurred while
closing the provider connection. See the inner exception for details.
---&gt; System.IO.FileNotFoundException: Could not load file or assembly 'System.Memory, Version=4.0.1.0, Culture=neutral,
PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies.
I have reinstalled Entity Framework and npgsql packages and tried to add all (the same) assemblies but with no success . Similar answers at Stack did not solve my issue . (I am allowed to work with the current versions with no further updates at VS or any of its packages.)
!Notice : i get all the appropriate data from my services when i use the API calls with the current model (proper communication with DB), but i cannot generate a new model from DB .
Any solutions ?
I am using
Windows 10
VS 2015
EntityFrameWork 6.2.0
Npgsql 3.1.1
.Net v.4.6.2
Asp.net
Thanks in advance !
",<c#><.net><visual-studio-2015><entity-framework-6><npgsql>,1476,1,0,1039,2,16,28,74,69750,0.0,62,10,29,2019-04-04 6:59,2019-04-04 7:08,,0.0,,Basic,14,"<c#><.net><visual-studio-2015><entity-framework-6><npgsql>, Could not load file or assembly 'System.Memory, Version=4.0.1.' in Visual Studio 2015, For a couple of months i had no issue generating the model from DB by deleting it and recreating it . After a pull from git, an issue has been occurred while trying to make the same process . After the second step (connection string creation with DB) there is no further proceed at the 3rd step and no connection string with the data base is being created at the app.config file.I have tried to test the connection with the database credentials and i am getting the following .
When i try to update specific tables from the model diagram as an alternative i get also the below :
System.Data.Entity.Core.EntityException: An error occurred while
closing the provider connection. See the inner exception for details.
---&gt; System.IO.FileNotFoundException: Could not load file or assembly 'System.Memory, Version=4.0.1.0, Culture=neutral,
PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies.
I have reinstalled Entity Framework and npgsql packages and tried to add all (the same) assemblies but with no success . Similar answers at Stack did not solve my issue . (I am allowed to work with the current versions with no further updates at VS or any of its packages.)
!Notice : i get all the appropriate data from my services when i use the API calls with the current model (proper communication with DB), but i cannot generate a new model from DB .
Any solutions ?
I am using
Windows 10
VS 2015
EntityFrameWork 6.2.0
Npgsql 3.1.1
.Net v.4.6.2
Asp.net
Thanks in advance !
","<c#><.net><visual-studio-2015><entity-framework-6><npgsql>, could load file assembly 'system.memory, version=4.0.1.' visual studio 2015, couple month issue genet model do delete retreat . pull git, issue occur try make process . second step (connect string creation do) proceed rd step connect string data base great pp.confirm file.i try test connect database credenti get follow . try update specie table model diagram alter get also : system.data.entity.core.entityexception: error occur close proved connection. see inner except details. ---&it; system.to.filenotfoundexception: could load file assembly 'system.memory, version=4.0.1.0, culture=neutral, publickeytoken=cc7b13ffcd2ddd51' one dependencies. rental entity framework npgsql package try add (the same) assembly success . similar answer stick sole issue . (i allow work current version update vs packages.) !notice : get appropri data service use apt call current model (proper common do), cannot genet new model do . slut ? use window 10 vs 2015 entityframework 6.2.0 npgsql 3.1.1 .net v.4.6.2 asp.net thank advance !"
51126162,Laravel Eloquent: is SQL injection prevention done automatically?,"Given the example code (Message is an Eloquent model.):
public function submit(Request $request){
    $this-&gt;validate($request, [
        'name' =&gt; ""required"",
        ""email"" =&gt; ""required""
    ]);
    //database connection
    $message = new Message;
    $message-&gt;name = $request-&gt;input(""name"");
    $message-&gt;email = $request-&gt;input(""email"");
    $message-&gt;save();
}
Does Eloquent use parameterized queries (like PDO) or any other mechanisms to prevent SQL injection?
",<php><sql><laravel><eloquent>,495,0,13,459,1,4,9,42,28881,0.0,7,1,29,2018-07-01 19:01,2018-07-01 19:18,2018-07-01 19:18,0.0,0.0,Basic,14,"<php><sql><laravel><eloquent>, Laravel Eloquent: is SQL injection prevention done automatically?, Given the example code (Message is an Eloquent model.):
public function submit(Request $request){
    $this-&gt;validate($request, [
        'name' =&gt; ""required"",
        ""email"" =&gt; ""required""
    ]);
    //database connection
    $message = new Message;
    $message-&gt;name = $request-&gt;input(""name"");
    $message-&gt;email = $request-&gt;input(""email"");
    $message-&gt;save();
}
Does Eloquent use parameterized queries (like PDO) or any other mechanisms to prevent SQL injection?
","<pp><sal><travel><eloquent>, travel eloquent: sal inject prevent done automatically?, given example code (message elope model.): public function submit(request $request){ $this-&it;validity($request, [ 'name' =&it; ""required"", ""email"" =&it; ""required"" ]); //database connect $message = new message; $message-&it;am = $request-&it;input(""name""); $message-&it;email = $request-&it;input(""email""); $message-&it;save(); } elope use parameter query (like do) mean prevent sal injection?"
49695990,Authenticate from Linux to Windows SQL Server with pyodbc,"I am trying to connect from a linux machine to a windows SQL Server with pyodbc.
I do have a couple of constraints:
Need to log on with a windows domain account
Need to use python3
Need to do it from Linux to Windows
Need to connect to a specific instance
I set up the environment as described by microsoft and have it working (I can import pyodbc and use the configured mussel driver).
I am not familiar with Windows domain authentication and what not, so there is where my problem is.
My connection string:
DRIVER={ODBC Driver 17 for SQL Server};SERVER=myserver.mydomain.com;PORT=1433;DATABASE=MyDatabase;Domain=MyCompanyDomain;Instance=MyInstance;UID=myDomainUser;PWD=XXXXXXXX;Trusted_Connection=yes;Integrated_Security=SSPI
Supposedly one should use ""Trusted_Connection"" to use the Windows domain authentication instead of directly authenticating with the SQL server.
The error I get when running pyodbc.connect(connString):
pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]SSPI Provider: No Kerberos credentials available (851968) (SQLDriverConnect)')
From other sources I read this should work on Windows as this code would use the credentials of the currently logged in user.
My question is how can I connect to a Windows SQL Server instance from Linux using Windows Domain credentials.
",<python><sql><linux><windows><pyodbc>,1330,0,2,820,1,9,19,73,48752,0.0,18,7,29,2018-04-06 15:16,2018-04-06 16:09,2018-04-13 16:51,0.0,7.0,Advanced,36,"<python><sql><linux><windows><pyodbc>, Authenticate from Linux to Windows SQL Server with pyodbc, I am trying to connect from a linux machine to a windows SQL Server with pyodbc.
I do have a couple of constraints:
Need to log on with a windows domain account
Need to use python3
Need to do it from Linux to Windows
Need to connect to a specific instance
I set up the environment as described by microsoft and have it working (I can import pyodbc and use the configured mussel driver).
I am not familiar with Windows domain authentication and what not, so there is where my problem is.
My connection string:
DRIVER={ODBC Driver 17 for SQL Server};SERVER=myserver.mydomain.com;PORT=1433;DATABASE=MyDatabase;Domain=MyCompanyDomain;Instance=MyInstance;UID=myDomainUser;PWD=XXXXXXXX;Trusted_Connection=yes;Integrated_Security=SSPI
Supposedly one should use ""Trusted_Connection"" to use the Windows domain authentication instead of directly authenticating with the SQL server.
The error I get when running pyodbc.connect(connString):
pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]SSPI Provider: No Kerberos credentials available (851968) (SQLDriverConnect)')
From other sources I read this should work on Windows as this code would use the credentials of the currently logged in user.
My question is how can I connect to a Windows SQL Server instance from Linux using Windows Domain credentials.
","<patron><sal><line><windows><pyodbc>, authentic line window sal server pyodbc, try connect line machine window sal server pyodbc. couple constraint: need log window domain account need use python3 need line window need connect specie instant set environs describe microsoft work (i import pyodbc use configur muscle driver). familiar window domain authentic not, problem is. connect string: driver={duc driver 17 sal server};server=observer.domain.com;port=1433;database=database;domain=mycompanydomain;instance=instance;did=mydomainuser;pad=xxxxxxxx;trusted_connection=yes;integrated_security=ship supposedly one use ""trusted_connection"" use window domain authentic instead directly authentic sal server. error get run pyodbc.connect(connstring): pyodbc.error: ('hy000', '[hy000] [unixodbc][microsoft][duc driver 17 sal server]ship provider: cerebro credenti avail (851968) (sqldriverconnect)') source read work window code would use credenti current log user. question connect window sal server instant line use window domain credentials."
49959601,Configure time zone to mysql docker container,"I have a mysql 5.7 docker container. When I run the mysql command:
SELECT now();
It shows the time -3 hours to my current time (which is logical). I want to set the time zone in a config file. Following the documentation in https://hub.docker.com/_/mysql/ I create a volume in my docker-compose.yml file like the following:
mysqldb:
    image: mysql:5.7.21
    container_name: mysql_container
    ports:
      - ""3306:3306""
    volumes:
      - ./.docker/etc/mysql/custom.cnf:/etc/mysql/conf.d/custom.cnf
When I browse the files inside the container the file custom.cnf is there.
In that file, I tried some of the ways I found as solutions like:
[mysqld]
default_time_zone='Europe/Sofia'
or a compromise solution which is less elegant as the zone will have to be changed twice a year (summer / winter):
[mysqld]
default_time_zone='+03:00'
but none works. I have the sensation the this file is not loaded by mysql at all because if I try to put invalid configuration in there nothing happens either (the container starts normally).
Any suggestions on that?
",<mysql><docker><docker-compose><my.cnf>,1056,2,14,1442,3,21,29,56,52256,0.0,101,4,29,2018-04-21 19:43,2018-04-24 12:04,2018-04-24 12:31,3.0,3.0,Basic,14,"<mysql><docker><docker-compose><my.cnf>, Configure time zone to mysql docker container, I have a mysql 5.7 docker container. When I run the mysql command:
SELECT now();
It shows the time -3 hours to my current time (which is logical). I want to set the time zone in a config file. Following the documentation in https://hub.docker.com/_/mysql/ I create a volume in my docker-compose.yml file like the following:
mysqldb:
    image: mysql:5.7.21
    container_name: mysql_container
    ports:
      - ""3306:3306""
    volumes:
      - ./.docker/etc/mysql/custom.cnf:/etc/mysql/conf.d/custom.cnf
When I browse the files inside the container the file custom.cnf is there.
In that file, I tried some of the ways I found as solutions like:
[mysqld]
default_time_zone='Europe/Sofia'
or a compromise solution which is less elegant as the zone will have to be changed twice a year (summer / winter):
[mysqld]
default_time_zone='+03:00'
but none works. I have the sensation the this file is not loaded by mysql at all because if I try to put invalid configuration in there nothing happens either (the container starts normally).
Any suggestions on that?
","<myself><doctor><doctor-compose><my.cf>, configur time zone myself doctor container, myself 5.7 doctor container. run myself command: select now(); show time -3 hour current time (which logical). want set time zone confirm file. follow document http://hut.doctor.com/_/myself/ great volume doctor-compose.you file like following: mysqldb: image: myself:5.7.21 container_name: mysql_contain ports: - ""3306:3306"" volumes: - ./.doctor/etc/myself/custom.cf:/etc/myself/cone.d/custom.cf brows file inside contain file custom.cf there. file, try way found slut like: [myself] default_time_zone='europe/sofa' compromise slut less leg zone change twice year (summer / winter): [myself] default_time_zone='+03:00' none works. sent file load myself try put invalid configur not happen either (the contain start normally). suggest that?"
50177907,com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server,"I'm new in Databases just started to learn them. I have MySQL Server 8.0., Workbench 8.0, Java connector 5.1.31, Java 1.8 itself. Followed multiple guides for newbies how to start with. 
So there is the case. I have database on localhost and successfully connect to it with workbench and via windows prompt. But after I execute code in java:
        Driver sqlDriver = new FabricMySQLDriver();
        DriverManager.registerDriver(sqlDriver);
        Connection sqlConnection = DriverManager.getConnection(""jdbc:mysql://localhost:3306/?user=root"", ""root"", ""root"");
I get exception com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server.
Why it could happen? I've tried to reinstall MySQL, Cleaned OS variables, looked everywhere and couldn't find solution. Would be glad to find it here.
UPD:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.Util.getInstance(Util.java:383)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1023)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2407)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2328)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:832)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:417)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
    at java.sql.DriverManager.getConnection(DriverManager.java:664)
    at java.sql.DriverManager.getConnection(DriverManager.java:247)
    at com.company.Main.main(Main.java:11)
Caused by: java.lang.NullPointerException
    at com.mysql.jdbc.ConnectionImpl.getServerCharacterEncoding(ConnectionImpl.java:3309)
    at com.mysql.jdbc.MysqlIO.sendConnectionAttributes(MysqlIO.java:1985)
    at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1911)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1288)
    at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2508)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2346)
    ... 13 more
",<java><mysql><jdbc>,3352,0,37,471,1,4,11,72,88224,0.0,35,2,29,2018-05-04 15:02,2018-05-05 7:21,,1.0,,Basic,12,"<java><mysql><jdbc>, com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server, I'm new in Databases just started to learn them. I have MySQL Server 8.0., Workbench 8.0, Java connector 5.1.31, Java 1.8 itself. Followed multiple guides for newbies how to start with. 
So there is the case. I have database on localhost and successfully connect to it with workbench and via windows prompt. But after I execute code in java:
        Driver sqlDriver = new FabricMySQLDriver();
        DriverManager.registerDriver(sqlDriver);
        Connection sqlConnection = DriverManager.getConnection(""jdbc:mysql://localhost:3306/?user=root"", ""root"", ""root"");
I get exception com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server.
Why it could happen? I've tried to reinstall MySQL, Cleaned OS variables, looked everywhere and couldn't find solution. Would be glad to find it here.
UPD:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.Util.getInstance(Util.java:383)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1023)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2407)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2328)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:832)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:417)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
    at java.sql.DriverManager.getConnection(DriverManager.java:664)
    at java.sql.DriverManager.getConnection(DriverManager.java:247)
    at com.company.Main.main(Main.java:11)
Caused by: java.lang.NullPointerException
    at com.mysql.jdbc.ConnectionImpl.getServerCharacterEncoding(ConnectionImpl.java:3309)
    at com.mysql.jdbc.MysqlIO.sendConnectionAttributes(MysqlIO.java:1985)
    at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1911)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1288)
    at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2508)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2346)
    ... 13 more
","<cava><myself><job>, com.myself.job.exceptions.jdbc4.mysqlnontransientconnectionexception: could great connect database server, i'm new database start learn them. myself server 8.0., workbench 8.0, cava connection 5.1.31, cava 1.8 itself. follow multiple guide newby start with. case. database localhost success connect workbench via window prompt. execute code cava: driver sqldriver = new fabricmysqldriver(); drivermanager.registerdriver(sqldriver); connect sqlconnect = drivermanager.getconnection(""job:myself://localhost:3306/?user=root"", ""root"", ""root""); get except com.myself.job.exceptions.jdbc4.mysqlnontransientconnectionexception: could great connect database server. could happen? i'v try rental myself, clean os variable, look everywhere find solution. would glad find here. up: com.myself.job.exceptions.jdbc4.mysqlnontransientconnectionexception: could great connect database server. attempt recollect 3 times. give up. sun.reflect.nativeconstructoraccessorimpl.newinstance0(n method) sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.cava:62) sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.cava:45) cava.long.reflect.construction.newinstance(construction.cava:423) com.myself.job.until.handlenewinstance(until.cava:408) com.myself.job.until.getinstance(until.cava:383) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:1023) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:997) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:983) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:928) com.myself.job.connectionimpl.connectwithretries(connectionimpl.cava:2407) com.myself.job.connectionimpl.createnewio(connectionimpl.cava:2328) com.myself.job.connectionimpl.&it;knit&it;(connectionimpl.cava:832) com.myself.job.jdbc4connection.&it;knit&it;(jdbc4connection.cava:46) sun.reflect.nativeconstructoraccessorimpl.newinstance0(n method) sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.cava:62) sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.cava:45) cava.long.reflect.construction.newinstance(construction.cava:423) com.myself.job.until.handlenewinstance(until.cava:408) com.myself.job.connectionimpl.getinstance(connectionimpl.cava:417) com.myself.job.nonregisteringdriver.connect(nonregisteringdriver.cava:344) cava.sal.drivermanager.getconnection(drivermanager.cava:664) cava.sal.drivermanager.getconnection(drivermanager.cava:247) com.company.main.main(main.cava:11) cause by: cava.long.nullpointerexcept com.myself.job.connectionimpl.getservercharacterencoding(connectionimpl.cava:3309) com.myself.job.mysqlio.sendconnectionattributes(mysqlio.cava:1985) com.myself.job.mysqlio.proceedhandshakewithpluggableauthentication(mysqlio.cava:1911) com.myself.job.mysqlio.dohandshake(mysqlio.cava:1288) com.myself.job.connectionimpl.coreconnect(connectionimpl.cava:2508) com.myself.job.connectionimpl.connectwithretries(connectionimpl.cava:2346) ... 13"
54824209,Indexing JSONField in Django PostgreSQL,"I am using a simple model with an attribute that stores all the data for that object in a JSONField. Think of it as way to transfer NoSQL data to my PostgreSQL database. Kinda like this:
from django.contrib.postgres.fields import JSONField   
class Document(models.Model):
    content = JSONField()
Each Document object has (more or less) the same keys in its content field, so I am querying and ordering those documents using those keys. For the querying and ordering, I am using Django's annotate() function. I recently came across this:
https://docs.djangoproject.com/en/2.1/ref/contrib/postgres/indexes/
I also know that PostgreSQL using JSONB, which is apparently indexible. So my question is this: Can I index my content field somehow to make my read operations faster for complex queries? And if so, then how do I do it? The documentation page I linked has no examples.
",<django><postgresql>,877,2,8,8342,21,74,153,45,7275,0.0,320,3,29,2019-02-22 9:43,2020-09-22 16:45,2022-11-29 19:50,578.0,1376.0,Basic,1,"<django><postgresql>, Indexing JSONField in Django PostgreSQL, I am using a simple model with an attribute that stores all the data for that object in a JSONField. Think of it as way to transfer NoSQL data to my PostgreSQL database. Kinda like this:
from django.contrib.postgres.fields import JSONField   
class Document(models.Model):
    content = JSONField()
Each Document object has (more or less) the same keys in its content field, so I am querying and ordering those documents using those keys. For the querying and ordering, I am using Django's annotate() function. I recently came across this:
https://docs.djangoproject.com/en/2.1/ref/contrib/postgres/indexes/
I also know that PostgreSQL using JSONB, which is apparently indexible. So my question is this: Can I index my content field somehow to make my read operations faster for complex queries? And if so, then how do I do it? The documentation page I linked has no examples.
","<django><postgresql>, index jsonfield django postgresql, use simple model attribute store data object jsonfield. think way transfer nose data postgresql database. kind like this: django.control.postures.field import jsonfield class document(models.model): content = jsonfield() document object (more less) key content field, query order document use keys. query ordering, use django' annette() function. recent came across this: http://docs.djangoproject.com/en/2.1/red/control/postures/indexes/ also know postgresql use son, appear indexible. question this: index content field somehow make read over faster complex queried? so, it? document page link examples."
49797786,How to get cursor in SQLAlchemy,"I am newbie in Python Flask. In my project we are creating db object using below code.    
    app = Flask(__name__)  
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'  
    db = SQLAlchemy(app)   
I want to get cursor object from db. Can someone please help me on it. 
I know using connection object we can get cursor. But can we get cursor from db object which is created by above way? Thanks. 
",<python><flask><sqlalchemy><flask-sqlalchemy><database-cursor>,417,0,3,1519,3,13,19,48,57414,0.0,101,4,29,2018-04-12 13:25,2018-04-12 20:15,2018-04-13 11:50,0.0,1.0,Basic,3,"<python><flask><sqlalchemy><flask-sqlalchemy><database-cursor>, How to get cursor in SQLAlchemy, I am newbie in Python Flask. In my project we are creating db object using below code.    
    app = Flask(__name__)  
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'  
    db = SQLAlchemy(app)   
I want to get cursor object from db. Can someone please help me on it. 
I know using connection object we can get cursor. But can we get cursor from db object which is created by above way? Thanks. 
","<patron><flask><sqlalchemy><flask-sqlalchemy><database-curses>, get curses sqlalchemy, newby patron flask. project great do object use code. pp = flask(__name__) pp.confirm['sqlalchemy_database_uri'] = 'quite:////tm/test.do' do = sqlalchemy(pp) want get curses object do. someone pleas help it. know use connect object get curses. get curses do object great way? thanks."
