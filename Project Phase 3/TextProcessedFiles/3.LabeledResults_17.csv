QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
49097346,How to speed up a SQLAlchemy Query?,"I have a table of over 10 million rows. There are roughly 50+ columns. The table stores sensors data/parameters. Let's say that I need to query data for the whole day or 86,400 seconds. It would need take roughly 20 or more seconds to complete this query.
I have added individual indices on a few columns such as recordTimestamp(that store when the data is captured), deviceId(the identification of the sensor), positionValid(whether GPS geolocation is valid). Then I added a composite index which includes all three columns. 
Below is my query:
t1 = time.time()
conn = engine.connect()
select_statement = select([Datatable]).where(and_(
    Datatable.recordTimestamp &gt;= start_date,
    Datatable.recordTimestamp &lt;= end_date,
    Datatable.deviceId == device_id,
    Datatable.positionValid != None,
    Datatable.recordTimestamp % query_interval == 0))
lol_data = conn.execute(select_statement).fetchall()    
conn.close() 
t2 = time.time()
time_taken = t2 - t1
print('Select: ' + time_taken)
Below is my EXPLAIN ANALYZE statement:
EXPLAIN ANALYZE SELECT datatable.id, datatable.""createdAt"", datatable.""analogInput01"", datatable.""analogInput02"", datatable.""analogInput03"", datatable.""analogInput04"", datatable.""analogInput05"", datatable.""analogInput06"", datatable.""analogInput07"", datatable.""canEngineRpm"", datatable.""canEngineTemperature"", datatable.""canFuelConsumedLiters"", datatable.""canFuelLevel"", datatable.""canVehicleMileage"", datatable.""deviceId"", datatable.""deviceTemperature"", datatable.""deviceInternalVoltage"", datatable.""deviceExternalVoltage"", datatable.""deviceAntennaCut"", datatable.""deviceEnum"", datatable.""deviceVehicleMileage"", datatable.""deviceSimSignal"", datatable.""deviceSimStatus"", datatable.""iButton01"", datatable.""iButton02"", datatable.""recordSequence"", datatable.""recordTimestamp"", datatable.""accelerationAbsolute"", datatable.""accelerationBrake"", datatable.""accelerationBump"", datatable.""accelerationTurn"", datatable.""accelerationX"", datatable.""accelerationY"", datatable.""accelerationZ"", datatable.""positionAltitude"", datatable.""positionDirection"", datatable.""positionSatellites"", datatable.""positionSpeed"", datatable.""positionLatitude"", datatable.""positionLongitude"", datatable.""positionHdop"", datatable.""positionMovement"", datatable.""positionValid"", datatable.""positionEngine"" FROM datatable WHERE datatable.""recordTimestamp"" &gt;= 1519744521 AND datatable.""recordTimestamp"" &lt;= 1519745181 AND datatable.""deviceId"" = '864495033990901' AND datatable.""positionValid"" IS NOT NULL AND datatable.""recordTimestamp"" % 1 = 0;
Below is the result from EXPLAIN ANALYZE of the SELECT:
Index Scan using ""ix_dataTable_recordTimestamp"" on dataTable (cost=0.44..599.35 rows=5 width=301) (actual time=0.070..10.487 rows=661 loops=1)
Index Cond: ((""recordTimestamp"" &gt;= 1519744521) AND (""recordTimestamp"" &lt;= 1519745181))
Filter: ((""positionValid"" IS NOT NULL) AND ((""deviceId"")::text = '864495033990901'::text) AND ((""recordTimestamp"" % 1) = 0))
Rows Removed by Filter: 6970
Planning time: 0.347 ms
Execution time: 10.658 ms
Whereas below is the result from time taken calculated by Python:
Select:  47.98712515830994 
JSON:  0.19731807708740234
Below is my code profiling:
10302 function calls (10235 primitive calls) in 12.612 seconds
Ordered by: cumulative time
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:882(execute)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/sql/elements.py:267(_execute_on_connection)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1016(_execute_clauseelement)
    1    0.000    0.000   12.592   12.592 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1111(_execute_context)
    1    0.000    0.000   12.590   12.590 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/default.py:506(do_execute)
    1   12.590   12.590   12.590   12.590 {method 'execute' of 'psycopg2.extensions.cursor' objects}
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1113(fetchall)
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1080(_fetchall_impl)
    1    0.008    0.008    0.017    0.017 {method 'fetchall' of 'psycopg2.extensions.cursor' objects}
",<python><postgresql><sqlalchemy><psycopg2>,4692,0,36,1347,2,12,26,45,15193,,37,3,14,2018-03-04 16:04,2018-04-03 7:06,,30.0,,Intermediate,23
49395898,Does MySQL support partial indexes?,"Partial indexes only include a subset of the rows of a table.
I've been able to create partial indexes in Oracle, DB2, PostgreSQL, and SQL Server. For example, in SQL Server I can create the index as:
create index ix1_case on client_case (date) 
  where status = 'pending';
This index is cheap since it does not include all 5 million rows of the table, but only the pending cases, that should not exceed a thousand rows. 
How do I do it in MySQL?
",<mysql><sql><database><indexing><relational-database>,447,0,2,46447,10,41,77,46,12817,0.0,2923,3,14,2018-03-20 23:44,2018-03-21 2:03,2018-04-18 15:58,1.0,29.0,Basic,3
59943384,Datatype for phone numbers in postgresql,"I am new to postgresql so can anyone tell me that is there any specific datatype to store phone numbers in postgresql while creating table in pgadmin or is it just string?
",<postgresql><database-design><sqldatatypes>,172,0,0,393,2,6,14,45,40259,0.0,4,2,14,2020-01-28 6:43,2020-01-28 7:35,2020-01-28 7:52,0.0,0.0,Basic,1
48865416,How to use the latest sqlite3 version in python,"I need to use sqlite version 3.8 or higher with python in Amazon Linux. 
I updated my sqlite installation to the latest version:
$ sqlite3 -version
3.22.0 2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d
I also updated my pysqlite version
pip install --upgrade pysqlite
However, my pysqlite still only seems to support sqlite version 3.7:
$ python
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt;
&gt;&gt;&gt; from pysqlite2 import dbapi2 as sqlite
&gt;&gt;&gt; sqlite.version
'2.8.3'
&gt;&gt;&gt; sqlite.sqlite_version
'3.7.17'
How can I update the sqlite python API to support a newer version of sqlite?
",<python><sqlite><pysqlite>,716,0,15,331,1,2,11,73,13026,0.0,2,4,14,2018-02-19 11:40,2018-07-17 15:37,,148.0,,Basic,3
62602720,String to Date migration from Spark 2.0 to 3.0 gives Fail to recognize 'EEE MMM dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter,"I have a date string from a source in the format 'Fri May 24 00:00:00 BST 2019' that I would convert to a date and store in my dataframe as '2019-05-24' using code like my example which works for me under spark 2.0
from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime
df = spark.createDataFrame([(&quot;Fri May 24 00:00:00 BST 2019&quot;,)], ['date_str'])
df2 = df.select('date_str', to_date(from_unixtime(unix_timestamp('date_str', 'EEE MMM dd HH:mm:ss zzz yyyy'))).alias('date'))
df2.show(1, False)
In my sandbox environment I've updated to spark 3.0 and now get the following error for the above code, is there a new method of doing this in 3.0 to convert my string to a date
: org.apache.spark.SparkUpgradeException: You may get a different
result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM
dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter.
You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the
behavior before Spark 3.0.
You can form a valid datetime pattern with the guide from
https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
",<apache-spark><pyspark><apache-spark-sql>,1113,2,4,191,1,1,5,46,59095,0.0,0,4,14,2020-06-26 20:55,2020-06-27 7:46,,1.0,,Basic,3
53579444,"Efficient text preprocessing using PySpark (clean, tokenize, stopwords, stemming, filter)","Recently, I began to learn the spark on the book ""Learning Spark"". In theory, everything is clear, in practice, I was faced with the fact that I first need to preprocess the text, but there were no actual tips on this topic.
The first thing that I took into account is that it is now preferable to use Dataframe instead of RDD, so my preprocessing attempt was made on dataframes.
Required operations:
Clearing text from punctuation (regexp_replace)
Tokenization (Tokenizer)
Delete stop words (StopWordsRemover)
Stematization (SnowballStemmer)
Filtering short words (udf)
My code is:
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, lower, regexp_replace
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from nltk.stem.snowball import SnowballStemmer
spark = SparkSession.builder \
    .config(""spark.executor.memory"", ""3g"") \
    .config(""spark.driver.cores"", ""4"") \
    .getOrCreate()
df = spark.read.json('datasets/entitiesFull/full').select('id', 'text')
# Clean text
df_clean = df.select('id', (lower(regexp_replace('text', ""[^a-zA-Z\\s]"", """")).alias('text')))
# Tokenize text
tokenizer = Tokenizer(inputCol='text', outputCol='words_token')
df_words_token = tokenizer.transform(df_clean).select('id', 'words_token')
# Remove stop words
remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')
df_words_no_stopw = remover.transform(df_words_token).select('id', 'words_clean')
# Stem text
stemmer = SnowballStemmer(language='english')
stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))
df_stemmed = df_words_no_stopw.withColumn(""words_stemmed"", stemmer_udf(""words_clean"")).select('id', 'words_stemmed')
# Filter length word &gt; 3
filter_length_udf = udf(lambda row: [x for x in row if len(x) &gt;= 3], ArrayType(StringType()))
df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))
Processing takes a very long time, the size of the entire document is 60 GB. Does it make sense to use RDD? Will caching help? How can I optimize preprocessing?
First I tested the implementation on the local computer, then I will try on the cluster. Local computer - Ubuntu RAM 6Gb, 4 CPU. Any alternative solution is also welcome. Thanks!
",<python><apache-spark><pyspark><apache-spark-sql><text-processing>,2270,0,30,169,1,1,7,64,15602,0.0,0,1,14,2018-12-02 10:40,2020-07-08 21:59,,584.0,,Intermediate,23
55455166,Special character (Hawaiian 'Okina) leads to weird string behavior,"The Hawaiian quote has some weird behavior in T-SQL when using it in conjunction with string functions. What's going on here? Am I missing something? Do other characters suffer from this same problem?
SELECT UNICODE(N'ʻ') -- Returns 699 as expected.
SELECT REPLACE(N'""ʻ', '""', '_') -- Returns ""ʻ, I expected _ʻ
SELECT REPLACE(N'aʻ', 'a', '_') -- Returns aʻ, I expected _ʻ
SELECT REPLACE(N'""ʻ', N'ʻ', '_') -- Returns __, I expected ""_
SELECT REPLACE(N'-', N'ʻ', '_') -- Returns -, I expected -
Also, strange when used in a LIKE for example:
DECLARE @table TABLE ([Name] NVARCHAR(MAX))
INSERT INTO
    @table
VALUES
    ('John'),
    ('Jane')
SELECT
    *
FROM
    @table
WHERE
    [Name] LIKE N'%ʻ%' -- This returns both records. I expected none.
",<sql-server><t-sql><unicode><collation>,746,1,23,2601,3,24,41,74,826,0.0,49,2,14,2019-04-01 12:29,2019-04-01 13:44,2019-04-04 20:23,0.0,3.0,Basic,2
63609570,Mysql 'VALUES function' is deprecated,"This is my python code which prints the sql query.
def generate_insert_statement(column_names, values_format, table_name, items, insert_template=INSERT_TEMPLATE, ):
    return insert_template.format(
        column_names=&quot;,&quot;.join(column_names),
        values=&quot;,&quot;.join(
            map(
                lambda x: generate_raw_values(values_format, x),
                items
            )
        ),
        table_name=table_name,
        updates_on=create_updates_on_columns(column_names)
    )
query = generate_insert_statement(table_name=property['table_name'],
        column_names=property['column_names'],
        values_format=property['values_format'], items=batch)
        print(query) #here
        execute_commit(query)
When printing the Mysql query my Django project shows following error in the terminal:
'VALUES function' is deprecated and will be removed in a future release. Please use an alias (INSERT INTO ... VALUES (...) AS alias) and replace VALUES(col) in the ON DUPLICATE KEY UPDATE clause with alias.col instead
Mysql doumentation does not say much about it.What does this mean and how to can i rectify it.
INSERT_TEMPLATE = &quot;INSERT INTO {table_name} ({column_names}) VALUES {values} ON DUPLICATE KEY UPDATE {updates_on};&quot;
",<python><mysql>,1276,0,19,187,0,1,9,80,9766,0.0,10,1,14,2020-08-27 5:09,2020-08-27 5:18,2020-08-27 5:18,0.0,0.0,Basic,2
52915923,Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured. SPRING,"I've already checked all the similar questions and every answer says that I need to specify a driverClassName which I already do. Here is my application.yml:
spring:
  application:
    name: cibus-backend
  datasource:
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/Cibus?useSSL=true
    username: root
    password: 1234567890
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: update
      properties:
      hibernate:
        format_sql: true
        type: trace
    database-platform: org.hibernate.dialect.MySQL5InnoDBDialect
    database: mysql
logging:
  level:
    org:
      hibernate:
        type: trace
Am I missing something? The weird thing is that a classmate of mine who has the same code can start the app perfectly well. This is why I think it has something to do with the path. Maybe spring isn't accessing the yml file. I included it in src.main.resources which is the default place where Spring looks for it.
Here is the stacktrace:
Error starting ApplicationContext. To display the conditions report re-run         
your application with 'debug' enabled.
2018-10-21 10:13:15.657 ERROR 10356 --- [JavaFX-Launcher]             
o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured.
Reason: Failed to determine a suitable driver class
Action:
Consider the following:
    If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.
    If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).
Exception in Application init method
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:389)
    at com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:767)
Caused by: java.lang.RuntimeException: Exception in Application init method
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:912)
    at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$155(LauncherImpl.java:182)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:732)
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:197)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1267)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1124)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:372)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1089)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:859)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:780)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:412)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:333)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1277)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1265)
    at labtic.AppStarter.init(AppStarter.java:25)
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:841)
    ... 2 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:590)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:251)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1135)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1062)
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:818)
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:724)
    ... 30 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:582)
    ... 43 more
Caused by: org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:236)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:83)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)
    ... 44 more
Exception running application labtic.AppStarter
Here is the Gradle Build as I was asked to include it:
buildscript {
    ext {
        springBootVersion = '2.0.5.RELEASE'
    }
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath(""org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}"")
    }
}
apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'org.springframework.boot'
apply plugin: 'io.spring.dependency-management'
group = ''
version = '0.0.1-SNAPSHOT'
sourceCompatibility = 1.8
repositories {
    mavenCentral()
    jcenter()
}
dependencies {
    compile('org.springframework.boot:spring-boot-starter-data-jpa')
    runtime('mysql:mysql-connector-java')
    testCompile('org.springframework.boot:spring-boot-starter-test')
    // https://mvnrepository.com/artifact/mysql/mysql-connector-java
    compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.12'
    compileOnly 'org.projectlombok:lombok:1.18.2'
    annotationProcessor ""org.projectlombok:lombok:1.18.2""
}
Any ideas? Thank you in advance.
",<java><mysql><spring><spring-boot>,12522,1,161,604,1,10,19,59,40228,0.0,135,6,14,2018-10-21 13:42,2018-10-21 15:13,2018-10-21 15:13,0.0,0.0,Advanced,39
53433285,MySQL Update or Rename a Key in JSON,"I'm having this json stored in db
{
    ""endDate"": ""2018-10-10"",
    ""startDate"": ""2017-09-05"", 
    ""oldKeyValue"": {
        ""foo"": 1000, 
        ""bar"": 2000, 
        ""baz"": 3000
    },
    ""anotherValue"": 0
}
How can I rename ""oldKeyValue"" key to ""newKeyValue"" without knowing the index of the key in an UPDATE query? I'm looking for something like this
UPDATE `my_table` SET `my_col` = JSON()
NOTE: only the key needs to change, the values (i.e. {""foo"": 1000, ""bar"": 2000, ""baz"": 3000}) should remain the same
",<mysql><json>,515,0,15,4117,20,72,131,53,13753,0.0,170,3,14,2018-11-22 14:39,2018-11-22 14:56,2018-11-22 14:56,0.0,0.0,Basic,2
51933189,Character encoding (UTF-8) in PowerShell session,"Hei all,
as a console/terminal enthusiast and database administrator (PostgreSQL) it is essential for me to work with the correct charcater encoding.
Therefore, I want my client console/terminal window always set to e.g. UTF-8.
Back with Windows' CMD.EXE this attempt was as easy as typing the command chcp 65001 to set the desired code page identifier.
Now, I am in the process of switching to PowerShell and setting the character encoding seems very odd, IMHO.
I've done some research on how to set the PowerShell session to UTF-8 and I figured out, that I need three steps/commmnds to accomplish that.
PS C:\&gt; $OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; [Console]::OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; chcp 65001
Despite the fact that the first two commands are not intuitive and hard to remember...
Leaving out one of them leads to something not working out properly!
Also, setting just one of them seems to have no effect to the others.
So, I must set all three for working with the PostgreSQL's psql database client.
Otherwise I run into encoding issues while exporting/importing data.
Now my question is: ""Why the heck? Isn't there an easier way to simply set the character encoding in PowerShell?""
Unfortunately, I did not find any plausible documentation myself about setting the character enconding!
Thanks in advance
/EDIT
The second comment by TheIncorrigible1 led me to the best answer fo far: Displaying Unicode in Powershell
- So one can set the whole PowerShell with two separated statements to the desired encoding (UTF-8).
PS C:\&gt; $OutputEncoding = [System.Console]::OutputEncoding = [System.Console]::InputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; $PSDefaultParameterValues['*:Encoding'] = 'utf8'
Explanation:
$OutputEncoding sets the encoding for e.g. | (piping) and/or communication between programs and/or processes.
[System.Console]::OutputEncoding sets the encoding for STDOUT and the console/terminal output.
[System.Console]::InputEncoding sets the encoding for STDIN or keyboard input.
$PSDefaultParameterValues['*:Encoding'] sets the encoding for all cmdlets that support the -Encoding option like e.g. Out-File -Encoding.
",<windows><postgresql><powershell><character-encoding>,2200,3,14,358,1,4,14,70,19962,0.0,922,1,14,2018-08-20 14:40,2020-11-04 12:01,,807.0,,Basic,2
51950129,Execute raw query in migration - Sequelize 3.30,"I want to execute a raw query in my migrations up and down functions. 
When I try to do: Sequelize.query, it says ERROR: Sequelize.query is not a function.
This is my migration skeleton file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize, migration) =&gt; {
     return Sequelize.query(...);   //ERROR: Sequelize.query is not a Function
  },
  down: (queryInterface, Sequelize) =&gt; {
     return Sequelize.query(...);  //ERROR: Sequelize.query is not a Function
  }
};
",<node.js><postgresql><sequelize.js>,491,0,17,10327,20,80,123,50,13394,,1192,1,14,2018-08-21 13:42,2018-08-22 1:56,2018-08-22 1:56,1.0,1.0,Basic,3
55998961,RSConfig generates a Dsn Connection String doesn't work,"TL;DR. 
Repro steps, take a backup of your C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config 
Run this command to update the connection string in SSRS's config:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s &lt;ServerName&gt; -i &lt;instanceNameIfNotDefault&gt; -d ""reportserver$ssrs"" -a SQL -u sa -p ""YourSAPassword"" -t
Now browse to the SSRS website and it doesn't work! To fix it either restore your config file or run through the SSRS GUI tool and it works!
How does the RsConfig utility work?
Background
After I install SSRS on an Windows 2016 Server and restore the 2 databases I need to change the Connection String in SSRS configuration file to point to the new SQL server name/instance.
Problem
When I try to change the encrypted Connection String in C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config file using the RSConfig utility:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!"" -t
It changes the Dsn Connection String in the RsReportServer.config. 
Before:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAC2DBxZFsfVB16r0e3......
*
After:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAO2nOjFDJMo........
*
However after this change, browsing to the SSRS Website results in the error:
  The report server can’t connect to its database. Make sure the database is running and accessible. You can also check the report server trace log for details.
If I run the SQL Reporting Services Configuration Tool (GUI) and change the Dsn Connection String browsing to the SSRS Website works! 
Obviously it changes the Dsn but I can't work out what else it does whilst the GUI tool is running. I've used ProcessMonitor and I've seen that the GUI tool does NOT use   RSConfig.exe utility, it uses itself RsConfigTool.exe! So I can't even capture command-line arguments of what the actual command/connection string should be. Also each time we change the connection string a new random one is generated so not sure how to do comparison's of actual vs expected.
I did a WinDiff of Registry keys and apart from some encrypted hexadecimal diffs, nothing stood out.
I run SQLProfiler and there were a bunch of grants that I have emulated in my PowerShell script, eg: 
$sqls += @""
USE [ReportServer`$SSRSTempDB]
if not exists (select * from sysusers where issqlrole = 1 and name = 'RSExecRole')
BEGIN
 EXEC sp_addrole 'RSExecRole'
END;
GO
My hunch is the $ sign in the SQL Database Name and the @ in the ""made up/simulated"" password are not getting escaped when I run the commands, eg:
$MachineName = ""server0012""
$instanceName = ""SSRS""
$saPassword = ""P@ssw0rd!""
$rsConfigPath = ""C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn\rsconfig.exe""
$setupArgs = -join('-c -s ""', $MachineName,'"" -i ""', $instanceName,'"" -d ','""ReportServer`$SSRS"" -t -a SQL -u ""sa"" -p ""', $saPassword,"""""""")
Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope Process
Write-Host $rsConfigPath $setupArgs
$args = $setupArgs.Split("" "")
&amp; ""$rsConfigPath"" $args
Restart-Service -Force ""SQL Server ($instanceName)""
When I run these vanilla commands in Command Prompt (no need to escape PowerShell characters):
rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!""
It changes the Dsn Connection String but browsing to SSRS Website gives same error (above).
How can I find out what else RsConfigTool.exe does when changing the Current Report Server Database? Or any guesses why the Connection String generated using the RSConfig Utility is out of whack - I've tried many different combinations, seems like only the RSConfigTool can actually do it?
Note 1:
I'm scripting this all up as a DevOps project and we are baking these images with packer, so nothing can be done manually. 
Note 2:
The Machine is joined to the domain and renamed after SQL installed. So using a Configuration.ini file I don't think will work.
",<sql><powershell><encryption><reporting-services><connection-string>,4235,4,25,62781,37,202,325,79,3721,,6536,2,14,2019-05-06 4:53,2019-05-15 17:44,2019-05-30 2:06,9.0,24.0,Advanced,32
50037975,Wordpress cannot connect to mysql server,"I have run a mysql server in my macbook, where I can access via both mysql command mysql -u root and navicat application. However, when I open the install page of a brand new wordpress app in my macbook. During the installation, I had got：
",<mysql><wordpress>,240,1,1,339,1,3,11,40,14936,0.0,10,10,14,2018-04-26 8:11,2018-04-26 8:32,,0.0,,Basic,9
52146191,Why can MySQL not use a partial primary key index?,"The MySQL documentation describing the use of index extensions, gives the following table as an example, followed by the query below:
CREATE TABLE t1 (
    i1 INT NOT NULL DEFAULT 0,
    i2 INT NOT NULL DEFAULT 0,
    d DATE DEFAULT NULL,
    PRIMARY KEY (i1, i2),
    INDEX k_d (d)
) ENGINE = InnoDB;
SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = '2000-01-01';
InnoDB internally will convert the index k_d to include the primary key at the end.  That is, the actual index k_d will be on (d, i1, i2), three columns.
The documentation goes on to explain that (emphasis mine):
  The optimizer cannot use the primary key in this case because that comprises columns (i1, i2) and the query does not refer to i2. Instead, the optimizer can use the secondary index k_d on (d), and the execution plan depends on whether the extended index is used.
I am confused by the above statement.  First it says that i1 is not enough to use the primary key index of two columns (i1, i2).  Then, in the second sentence, it says that the index k_d on (d, i1, i2) can be used, despite that only d and i1 are being used, with i2 absent.
My general understanding of indices in MySQL, and in other flavors of SQL, is that a left portion of an index can be used if a subset of all columns in the index are present, starting from the left.
What is different about a primary key (clustered) index and a non clustered secondary index which allows the latter to use a partial index, but the former cannot?
",<mysql><indexing><clustered-index>,1475,1,19,505816,28,292,366,49,2049,0.0,17194,2,14,2018-09-03 8:47,2018-09-06 15:48,2018-09-06 15:48,3.0,3.0,Basic,5
48460910,Electron App Getting Exception While requiring SQLITE3,"package.json 
""name"": ""billingapp"",
""version"": ""1.0.0"",
""description"": """",
""main"": ""index.js"",
""scripts"": {
""rebuild"": ""electron-rebuild -f -w billingapp""
},
""author"": ""S Kundu"",
""license"": ""ISC"",
""dependencies"": {
""electron"": ""^1.7.11"",
""sqlite3"": ""^3.1.13""
}
""devDependencies"": {
""electron-rebuild"": ""^1.7.3""
}
index.js 
const electron  = require('electron');
const path      = require('path');
const url       = require('url');
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
const {app, BrowserWindow, Menu, ipcMain} = electron;
let mainWindow;
app.on('ready', function(){
// Create the login window
mainWindow = new BrowserWindow({
  resizable: true,
  fullscreen: false
});
// Load html in window
mainWindow.loadURL(url.format({
  pathname: path.join(__dirname, 'login.html'),
  protocol: 'file:',
  slashes: true
}));
});
login.html 
&lt;h1&gt;Welcome to billing system&lt;/h1&gt;
These are the code files.
Steps to install NPM Packages
npm install electron
npm install sqlite3
Its working perfect when I remove bellow code:
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
But  with this code , while running 
npm start
is getting bellow error:
App threw an error during load
  Error: Cannot find module
  'C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64\node_sqlite3.node'
      at Module._resolveFilename (module.js:470:15)
      at Function.Module._resolveFilename (C:\Users\sintu\Desktop\BillingSystem\node_modules\electron\dist\resources\electron.asar\common\reset-search-paths.js:35:12)
      at Function.Module._load (module.js:418:25)
      at Module.require (module.js:498:17)
      at require (internal/module.js:20:19)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:4:15)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:190:3)
      at Module._compile (module.js:571:32)
      at Object.Module._extensions..js (module.js:580:10)
      at Module.load (module.js:488:32)
when I run npm run rebuild , I get bellow error 
× Rebuild Failed
An unhandled error occurred inside electron-rebuild
Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
Error: Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
    at SafeSubscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:277:84)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:198:26)
    at Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at MapSubscriber.Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at MapSubscriber.Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at SafeSubscriber._next (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:251:65)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.next (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:186:22)
npm ERR! code ELIFECYCLE
npm ERR! errno 4294967295
npm ERR! billingapp@1.0.0 rebuild: electron-rebuild -f -w billingapp
npm ERR! Exit status 4294967295
npm ERR!
npm ERR! Failed at the billingapp@1.0.0 rebuild script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\sintu\AppData\Roaming\npm-cache_logs\2018-01-30T15_36_46_678Z-debug.log
",<javascript><sqlite><electron>,6886,4,51,165,0,1,11,66,1369,,9,1,14,2018-01-26 11:56,2018-07-23 15:52,,178.0,,Advanced,37
53560489,"EF Core SQLite in memory exception: SQLite Error 1: 'near ""MAX"": syntax error'","I am creating SQLite In Memory database for unit testing:
        var connection = new SqliteConnection(""DataSource=:memory:"");
        connection.Open();
        try
        {
            var options = new DbContextOptionsBuilder&lt;BloggingContext&gt;()
                .UseSqlite(connection)
                .Options;
            // Create the schema in the database
            using (var context = new BloggingContext(options))
            {
                context.Database.EnsureCreated();
            }
            // Run the test against one instance of the context
            using (var context = new BloggingContext(options))
            {
                var service = new BlogService(context);
                service.Add(""http://sample.com"");
            }
            // Use a separate instance of the context to verify correct data was saved to database
            using (var context = new BloggingContext(options))
            {
                Assert.AreEqual(1, context.Blogs.Count());
                Assert.AreEqual(""http://sample.com"", context.Blogs.Single().Url);
            }
        }
context.Database.EnsureCreated(); fails with with exception:
Message: Microsoft.Data.Sqlite.SqliteException : SQLite Error 1: 'near ""MAX"": syntax error'.
There is github issue saying: 
The issue here is varchar(max) is SqlServer specific type. The scaffolding should not add it as relational type which gets passed to migration in other providers which is prone to generate invalid sql at migration.
But how then can I use SQLite in Memory for unit tests if my database contains many varchar(max) columns?
",<c#><sqlite><entity-framework-core><ef-core-2.0><ef-core-2.1>,1619,3,29,2103,3,20,40,58,7400,,160,4,14,2018-11-30 15:33,2018-12-30 20:05,,30.0,,Intermediate,18
53386872,Is it possible to use StringFormat or Constant Variable in android Room Query,"I want to query the user associations list with the following room query using public constant variable Association.MEMBER_STATUS_APPROVED.
@Query(""SELECT * FROM Association WHERE memberStatus = "" + Association.MEMBER_STATUS_APPROVED)
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations();
But, room gives me [SQLITE_ERROR] when build.
 It is possible to re-write that query by replacing the constant variable with parameter like the following.
@Query(""SELECT * FROM Association WHERE memberStatus = :statusApproved"")
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations(String statusApproved);
I would like to know that does Room support such kind of string concatenation or String Format? (or) May be I missing something?
",<android><database><android-sqlite><android-room>,741,0,4,571,1,5,8,39,3871,,6,3,14,2018-11-20 5:39,2018-11-20 6:12,,0.0,,Intermediate,18
65481470,Connect to remote db with ssh tunneling in DBeaver,"I know this question was already asked before (like here), but still I could not find a solution and those posts are quite old.
So I am able to connect to the remote db with an ssh connection and then use the command line like this:
// Putty SSH Connection
host: ssh.strato.de
port: 22
username: xxxxxxx 
password: xxxxxxx 
// connect to mysql with terminal
mysql -h rdbms -u xxxxxxx -p xxxxxxxx
If I try the same with ssh-tunneling in DBeaver I get an connection error
The ssh-tunneling itself seems to work. If I use the same credentials as above and press &quot;Test tunnel configuration&quot; I get a success message.
I tried several other options for port and host (localhost, rdbms.strato.de, etc), which I found via mysql show variables; show processlist; show user();, but none of them worked.
The Strato Support told me that I can only connect to the db internally with phpmyadmin or remotely wiht putty and mysql, but since the last method is working, shouldn't ssh-tunneling also work?
",<mysql><ssh><ssh-tunnel><dbeaver>,997,3,11,511,1,4,14,45,37688,0.0,14,2,14,2020-12-28 17:23,2021-04-27 18:20,,120.0,,Basic,10
51835172,Sequelize Error: Relation does not exist,"I am able to use sequelize.js to do an INSERT INTO command for a table in my development database, but not in my test database.
Despite researching thoroughly, I have not been able to resolve the issue.
A similar question has been posted here, though I have not been able to answer my question with the answers:
sequelize with postgres database not working after migration from mysql
Here is my relevant migration file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize) =&gt; {
    return queryInterface.createTable('Trees', {
      id: {
        allowNull: false,
        autoIncrement: true,
        primaryKey: true,
        type: Sequelize.INTEGER
      },
      title: {
        type: Sequelize.STRING,
        allowNull: false
      },
      content: {
        type: Sequelize.STRING(10000),
        allowNull: false
      },
      createdAt: {
        allowNull: false,
        type: Sequelize.DATE
      },
      updatedAt: {
        allowNull: false,
        type: Sequelize.DATE
      }
    });
  },
  down: (queryInterface, Sequelize) =&gt; {
    return queryInterface.dropTable('Trees');
  }
};
Here is the model file:
'use strict';
module.exports = (sequelize, DataTypes) =&gt; {
  var Tree = sequelize.define('Tree', {
    title: {
      type: DataTypes.STRING,
      allowNull: false
    },
    content: {
      type: DataTypes.STRING(10000),
      allowNull: false
    }
  }, {});
  Tree.associate = function(models) {
    // associations can be defined here
  };
  return Tree;
};
Here is the code that accesses the database:
const setTree = (treeVal, callback) =&gt; {
  console.log(`this would be the part`);
  Tree.create({
    title: 'Tree',
    content: JSON.stringify(treeVal)
  })
  .then((treeStr) =&gt; {
    let primaryTopics = JSON.parse(treeStr.content);
    callback(null, primaryTopics);
  })
  .catch((err) =&gt; {
    callback(err);
  });
}
This is exported in the module.exports method:
  callTree(callback) {
    return Tree.findOne({
      where: {
        title: 'Tree'
      }
    })
    .then((treeStr) =&gt; {
      if (treeStr === null) {
        return callback(`not defined yet`);
      }
      let primaryTopics = treeStr.content;
      primaryTopics = JSON.parse(primaryTopics);
      callback(null, primaryTopics);
    })
    .catch((err) =&gt; {
      callback(err);
    });
  }
And I'm pulling this method for an integration test here (the PrimaryTopic table is in the same database, and I receive no errors trying to run it):
  beforeEach((done) =&gt; {
    this.primaryTopic;
    sequelize.sync({force: true}).then((res) =&gt; {
      PrimaryTopic.create({
        title: 'Title: Hello World',
        content: '&lt;p&gt;Content: Hello World&lt;/p&gt;'
      })
      .then((primaryTopic) =&gt; {
        this.primaryTopic = primaryTopic;
        treeQueries.buildTree((err, res) =&gt; {
          if (err) {
            console.error(err);
          }
        });
        done();
      })
      .catch((err) =&gt; {
        console.log(err);
        done();
      });
    });
  });
I've searched through all the code for possible errors, but haven't found anything yet. 
I can use psql to access the Trees table in the test database, though it is empty.
I can use the same code to insert a value into the Trees table in the development database with no issues.
Here is the error I receive when I try to run a test (using jasmine.js for testing):
{ SequelizeDatabaseError: relation ""Trees"" does not exist
    at Query.formatError (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:363:16)
    at query.catch.err (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:86:18)
    at tryCatcher (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:512:31)
    at Promise._settlePromise (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:569:18)
    at Promise._settlePromise0 (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:614:10)
    at Promise._settlePromises (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:689:18)
    at Async._drainQueue (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:133:16)
    at Async._drainQueues (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:143:10)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:756:18)
    at tryOnImmediate (timers.js:717:5)
    at processImmediate [as _immediateCallback] (timers.js:697:5)
  name: 'SequelizeDatabaseError',
  parent: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  original: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' }
Here's a link to the full repository.
",<javascript><node.js><postgresql><sequelize.js>,8517,2,185,221,2,3,7,49,36600,,0,3,14,2018-08-14 6:42,2020-10-14 14:19,,792.0,,Basic,1
49592794,Postgres: Are There Downsides to Using a JSON Column vs. an integer[] Column?,"
TLDR: If I want to save arrays of integers in a Postgres table, are there any pros or cons to using an array column (integer[]) vs. using a JSON column (eg. does one perform better than the other)?
Backstory:
I'm using a PostgreSQL database, and Node/Knex to manage it.  Knex doesn't have any way of directly defining a PostgreSQL integer[] column type, so someone filed a Knex bug asking for it ... but one of the Knex devs closed the ticket, essentially saying that there was no need to support PostgreSQL array column types when anyone can instead use the JSON column type.
My question is, what downsides (if any) are there to using a JSON column type to hold a simple array of integers?  Are there any benefits, such as improved performance, to using a true array column, or am I equally well off by just storing my arrays inside a JSON column?
EDIT: Just to be clear, all I'm looking for in an answer is either of the following:
A) an explanation of how JSON columns and integer[] columns in PostgreSQL work, including either how one is better than the other or how the two are (at least roughly) equal.
B) no explanation, but at least a reference to some benchmarks that show that one column type or the other performs better (or that the two are equal)
",<postgresql><knex.js>,1261,0,2,34011,31,163,237,52,4026,0.0,2733,1,14,2018-03-31 21:55,2018-04-01 16:37,2018-04-01 16:37,1.0,1.0,Intermediate,23
63354909,Is it possible to use Traefik to proxy PostgreSQL over SSL?,"Motivations
I am a running into an issue when trying to proxy PostgreSQL with Traefik over SSL using Let's Encrypt.
I did some research but it is not well documented and I would like to confirm my observations and leave a record to everyone who faces this situation.
Configuration
I use latest versions of PostgreSQL v12 and Traefik v2. I want to build a pure TCP flow from tcp://example.com:5432 -&gt; tcp://postgresql:5432 over TLS using Let's Encrypt.
Traefik service is configured as follow:
  version: &quot;3.6&quot;
    services:
      traefik:
        image: traefik:latest
        restart: unless-stopped
        volumes:
          - &quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot;
          - &quot;./configuration/traefik.toml:/etc/traefik/traefik.toml:ro&quot;
          - &quot;./configuration/dynamic_conf.toml:/etc/traefik/dynamic_conf.toml&quot;
          - &quot;./letsencrypt/acme.json:/acme.json&quot;
        networks:
          - backend
        ports:
          - &quot;80:80&quot;
          - &quot;443:443&quot;
          - &quot;5432:5432&quot;
    networks:
      backend:
        external: true
With the static setup:
[entryPoints]
  [entryPoints.web]
    address = &quot;:80&quot;
    [entryPoints.web.http]
      [entryPoints.web.http.redirections.entryPoint]
        to = &quot;websecure&quot;
        scheme = &quot;https&quot;
  [entryPoints.websecure]
    address = &quot;:443&quot;
    [entryPoints.websecure.http]
      [entryPoints.websecure.http.tls]
        certresolver = &quot;lets&quot;
  [entryPoints.postgres]
    address = &quot;:5432&quot;
PostgreSQL service is configured as follow:
version: &quot;3.6&quot;
services:
  postgresql:
    image: postgres:latest
    environment:
      - POSTGRES_PASSWORD=secret
    volumes:
      - ./configuration/trial_config.conf:/etc/postgresql/postgresql.conf:ro
      - ./configuration/trial_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./configuration/initdb:/docker-entrypoint-initdb.d
      - postgresql-data:/var/lib/postgresql/data
    networks:
      - backend
    #ports:
    #  - 5432:5432
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`example.com`)&quot;
      - &quot;traefic.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
networks:
  backend:
    external: true
volumes:
  postgresql-data:
It seems my Traefik configuration is correct. Everything is OK in the logs and all sections in dashboard are flagged as Success (no Warnings, no Errors). So I am confident with the Traefik configuration above. The complete flow is about:
EntryPoint(':5432') -&gt; HostSNI(`example.com`) -&gt; TcpRouter(`postgres`) -&gt; Service(`postgres@docker`)
But, it may have a limitation at PostgreSQL side.
Debug
The problem is that I cannot connect the PostgreSQL database. I always get a Timeout error.
I have checked PostgreSQL is listening properly (main cause of Timeout error):
# - Connection Settings -
listen_addresses = '*'
port = 5432
And I checked that I can connect PostgreSQL on the host (outside the container):
psql --host 172.19.0.4 -U postgres
Password for user postgres:
psql (12.2 (Ubuntu 12.2-4), server 12.3 (Debian 12.3-1.pgdg100+1))
Type &quot;help&quot; for help.
postgres=#
Thus I know PostgreSQL is listening outside its container, so Traefik should be able to bind the flow.
I also have checked external traefik can reach the server:
sudo tcpdump -i ens3 port 5432
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes
09:02:37.878614 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [S], seq 1027429527, win 64240, options [mss 1452,nop,wscale 8,nop,nop,sackOK], length 0
09:02:37.879858 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [S.], seq 3545496818, ack 1027429528, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
09:02:37.922591 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [.], ack 1, win 516, length 0
09:02:37.922718 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [P.], seq 1:9, ack 1, win 516, length 8
09:02:37.922750 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [.], ack 9, win 502, length 0
09:02:47.908808 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [F.], seq 9, ack 1, win 516, length 0
09:02:47.909578 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [P.], seq 1:104, ack 10, win 502, length 103
09:02:47.909754 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [F.], seq 104, ack 10, win 502, length 0
09:02:47.961826 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [R.], seq 10, ack 104, win 0, length 0
So, I am wondering why the connection cannot succeed. Something must be wrong between Traefik and PostgreSQL.
SNI incompatibility?
Even when I remove the TLS configuration, the problem is still there, so I don't expect the TLS to be the origin of this problem.
Then I searched and I found few posts relating similar issue:
Introducing SNI in TLS handshake for SSL connections
Traefik 2.0 TCP routing for multiple DBs;
As far as I understand it, the SSL protocol of PostgreSQL is a custom one and does not support SNI for now and might never support it. If it is correct, it will confirm that Traefik cannot proxy PostgreSQL for now and this is a limitation.
By writing this post I would like to confirm my observations and at the same time leave a visible record on Stack Overflow to anyone who faces the same problem and seek for help. My question is then: Is it possible to use Traefik to proxy PostgreSQL?
Update
Intersting observation, if using HostSNI('*') and Let's Encrypt:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Everything is flagged as success in Dashboard but of course Let's Encrypt cannot perform the DNS Challenge for wildcard *, it complaints in logs:
time=&quot;2020-08-12T10:25:22Z&quot; level=error msg=&quot;Unable to obtain ACME certificate for domains \&quot;*\&quot;: unable to generate a wildcard certificate in ACME provider for domain \&quot;*\&quot; : ACME needs a DNSChallenge&quot; providerName=lets.acme routerName=postgres@docker rule=&quot;HostSNI(`*`)&quot;
When I try the following configuration:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.domains[0].main=example.com&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
The error vanishes from logs and in both setups the dashboard seems ok but traffic is not routed to PostgreSQL (time out). Anyway, removing SSL from the configuration makes the flow complete (and unsecure):
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Then it is possible to connect PostgreSQL database:
time=&quot;2020-08-12T10:30:52Z&quot; level=debug msg=&quot;Handling connection from x.y.z.w:58389&quot;
",<postgresql><tcp><proxy><traefik><sni>,8147,3,122,7550,3,39,59,73,10855,0.0,2039,3,14,2020-08-11 9:00,2020-08-12 7:16,2023-01-13 8:21,1.0,885.0,Basic,9
55485858,Using SQLite3 with Django 2.2 and Python 3.6.7 on Centos7,"I am moving my Django code from 2.1.7 directly to the new Django 2.2.  The only problem I encountered in my Centos7 development environment was that my local development database (sqlite3) version was incompatible using my Python 3.6.7.
The error I was getting from ""manage.py runserver"" was:
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later
I am unable to use another version of Python because this is the maximum supported by AWS elasticbeanstalk.  The Python 3.6.7 seems to come with sqlite module of version:
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt; 
I use a seperate development account on my local Centos7 workstation and issue pipenv shell to begin my code development and IDE.  
The only workaround I've found is to manually download SQLite3 autoconf version 3.27.2 and manually compile into that development account home folder using the following commands: 
wget https://www.sqlite.org/2019/sqlite-autoconf-3270200.tar.gz
gzip -d sqlite-autoconf-3270200.tar.gz
tar -xvf sqlite-autoconf-3270200.tar
cd sqlite-autoconf-3270200/
./configure --prefix=/home/devuser/opt/
make
make install
Following that, I have modified my .bashrc to reflect the following: 
export LD_LIBRARY_PATH=""${HOME}/opt/lib""
This seems to do the trick when I log back into my devuser account.  My app seems to run correctly using my local development database.
Python 3.6.7 (default, Dec  5 2018, 15:02:05)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux
&gt;&gt;&gt;import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.27.2'
My local development database is SQLite, but my settings.py does not load any SQLite3 database backend when it senses it's in production on AWS (uses Mysql production database as backend when environment variable flag PRODUCTION is checked).
Is my understanding of the problem correct and is my approach and implementation acceptable? 
I felt that recompiling python was a huge waste of time, and to be honest it may have been faster to stand up a local mysql version and stop wasting time with sqlite... but it's so nice to just copy or dump a file, migrate, and loaddata for a fresh start.
",<django><sqlite><python-3.6><pipenv><django-2.2>,2237,1,22,153,0,3,12,56,7109,0.0,28,1,14,2019-04-03 1:45,2019-06-14 8:47,,72.0,,Basic,9
51968981,"Merging duplicated records together with ""Merge"" syntax","I am using SQL Server 2014. I am currently trying to combine millions of personnel application records in to a single personnel record.  
The records contain the following columns:
ID, First_Name, Last_Name, DOB, Post_Code, Mobile, Email
A person can enter their details numerous times but due to fat fingers or fraud they can sometimes put in, incorrect details.  
In my example Christopher has filled his details in 5 times, First_Name, Last_Name, DOB are always correct, Post_Code, Mobile and Email contain various connotations.  
What I want to do is take the min(id) associated with this group in this case 84015283 and put it in to a new table, this will be the primary key and then you will see the other id's that are associated with it.
Examples
NID       CID
------------------
84015283  84015283
84015283  84069198
84015283  84070263
84015283  84369603
84015283  85061159
Where it gets a little complicated is, where 2 different people can have the same First_Name, Last_Name and DOB, at least one of the other fields must match ""post_code, mobile or email"" as per my example to another record within the group.
Though first_name, last_name, DoB match between ID's 84015283, 84069198, 84070263. 84015283, 84069198 are identical so they would match without an issue, 84070263 matches on the postcode, 84369603 matches on the mobile to a previous record and 85061159 matches on a previous mobile/email but not post_code.
If putting the NID within the original dataset is easier I can go with this rather than putting it all in a separate table.
After some googling and trying to get my head around this, I believe that using ""Merge"" might be a good way to achieve what I am after but I am concerned it will take a very long time due to the number of records involved.
Also going forward any routine would have to be run on subsequent new records.
I have listed the code for the example if anyone can help
DROP TABLE customer_dist
CREATE TABLE [dbo].customer_dist
(
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL,
)
INSERT INTO customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('84015283', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84069198', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84070263', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559822', 'CHigg@AOL.com'),
       ('84369603', 'Christopher', 'Higg', '1956-01-13', 'CH2 3ZA', '07089559829', 'Higg@emailme.com'),
       ('85061159', 'CHRISTOPHER', 'Higg', '1956-01-13', 'CH2 3RA', '07089559829', 'CH@hotmail.com'),
       ('87065122', 'Matthew', 'Davis', '1978-05-10', 'CH5 1TS', '07077084692', 'Matt@gamil.com')
SELECT * FROM customer_dist
Below is the expected results, sorry I should of made it clearer what I wanted at the end.
Output Table Results
    NID         id          First_Name  Last_Name   DoB         post_code   mobile          Email
    84015283    84015283    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84069198    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84070263    Christopher Higg            1/13/1956   CH2 3AZ         7089559822  CHigg@AOL.com
    84015283    84369603    Christopher Higg            1/13/1956   CH2 3ZA         7089559829  Higg@emailme.com
    84015283    85061159    CHRISTOPHER Higg            1/13/1956   CH2 3RA         7089559829  CH@hotmail.com
    78065122    87065122    Matthew Davis               05/10/1978  CH5 1TS
7077084692  Matt@gamil.com
OR                          
NID         id
84015283    84015283
84015283    84069198
84015283    84070263
84015283    84369603
84015283    85061159
87065122    87065122
Apologies for the slow response.
I have updated my required output, I was asked to include an extra record that was not a match to the other records but did not include this in my required output.
HABO's response was the closest to what was needed unfortunately on further testing with other sample data, duplicates were created and the logic broke down.  Other Sample data would be :-
declare @customer_dist as Table (
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL );
INSERT INTO @customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('32006455', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07706212920',  'nastie220@yahoo.com'),
       ('35963960', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863324',  'nastie@hotmail.com'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com'),
       ('46653041', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie2010@yahoo.com'),
       ('48023677', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie@hotmail.com'),
       ('49560434', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie@hotmail.com'),
       ('49861032', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie2001@yahoo.com'),
       ('53130969', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'Nastie@hotmail.cm'),
       ('33843283', 'Mary', 'Wilson',   '1983-09-20',   'BT148HU',  '07484863478',  'nastie2010@yahoo.co.uk'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com')
SELECT * FROM @customer_dist;
",<sql><sql-server><t-sql><sql-server-2014>,5802,1,84,743,2,13,38,58,976,,8,9,14,2018-08-22 14:20,2018-08-22 15:05,,0.0,,Basic,10
55006231,Query to get nextval from sequence with Spring JPA,"I have a repository with which I am trying to get next value from sequence. The sequence name I need to be parameterized.
The repository query looks a like:
  @Query(value = ""SELECT ?1.NEXTVAL from dual;"", nativeQuery = true)
  String getSeqID(@Param(""seqName"") String seqName);
But, I am getting following exception:
org.hibernate.QueryException: JPA-style positional param was not an integral ordinal
    at org.hibernate.engine.query.spi.ParameterParser.parse(ParameterParser.java:187) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.ParamLocationRecognizer.parseLocations(ParamLocationRecognizer.java:59) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.internal.NativeQueryInterpreterStandardImpl.getParameterMetadata(NativeQueryInterpreterStandardImpl.java:34) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.QueryPlanCache.getSQLParameterMetadata(QueryPlanCache.java:125) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
",<java><sql><spring><jpa><spring-data-jpa>,1036,0,5,486,2,8,23,76,60988,0.0,13,5,14,2019-03-05 15:28,2019-03-05 15:40,2020-04-01 20:04,0.0,393.0,Basic,10
54518722,MySQL Connector could not process parameters,"I'm trying to loop through an array and insert each element into a table.  As far as I can see my syntax is correct and I took this code straight from Microsoft Azure's documentation.  
try:
   conn = mysql.connector.connect(**config)
   print(""Connection established"")
except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print(""Something is wrong with the user name or password"")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print(""Database does not exist"")
  else:
    print(err)
else:
  cursor = conn.cursor()
data = ['1','2','3','4','5']
for x in data:
   cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(x))
   print(""Inserted"",cursor.rowcount,""row(s) of data."")
conn.commit()
cursor.close()
conn.close()
print(""Done."")
When I run this is gets to cursor.execute(...) and then fails.  Here is the stack trace.
  Traceback (most recent call last):
    File ""test.py"", line 29, in 
      cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(""test""))
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\cursor_cext.py"", line 248, in execute
      prepared = self._cnx.prepare_for_mysql(params)
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\connection_cext.py"", line 538, in prepare_for_mysql
      raise ValueError(""Could not process parameters"")
  ValueError: Could not process parameters
",<python><mysql><mysql-connector-python>,1446,1,24,514,2,7,21,43,39576,,29,3,14,2019-02-04 14:52,2019-02-04 15:03,2019-02-04 15:03,0.0,0.0,Basic,14
61081569,No exception being thrown when opening MySqlConnection?,"I'm just starting out with async and Task's and my code has stopped processing. It happens when I have an incoming network packet and I try and communicate with the database inside the packet handler.
public class ClientConnectedPacket : IClientPacket
{
    private readonly EntityFactory _entityFactory;
    public ClientConnectedPacket(EntityFactory entityFactory)
    {
        _entityFactory= entityFactory;
    }
    public async Task Handle(NetworkClient client, ClientPacketReader reader)
    {
        client.Entity = await _entityFactory.CreateInstanceAsync( reader.GetValueByKey(""unique_device_id""));
        // this Console.WriteLine never gets reached
        Console.WriteLine($""Client [{reader.GetValueByKey(""unique_device_id"")}] has connected"");
    }
}
The Handle method gets called from an async task
if (_packetRepository.TryGetPacketByName(packetName, out var packet))
{
    await packet.Handle(this, new ClientPacketReader(packetName, packetData));
}
else
{
    Console.WriteLine(""Unknown packet: "" + packetName);
}
Here is the method which I think is causing the issue
public async Task&lt;Entity&gt; CreateInstanceAsync(string uniqueId)
{
    await using (var dbConnection = _databaseProvider.GetConnection())
    { 
        dbConnection.SetQuery(""SELECT COUNT(NULL) FROM `entities` WHERE `unique_id` = @uniqueId"");
        dbConnection.AddParameter(""uniqueId"", uniqueId);
        var row = await dbConnection.ExecuteRowAsync();
        if (row != null)
        {
            return new Entity(uniqueId, false);
        }
    }
    return new Entity(uniqueId,true);
}
DatabaseProvider's GetConnection method:
public DatabaseConnection GetConnection()
{
    var connection = new MySqlConnection(_connectionString);
    var command = connection.CreateCommand();
    return new DatabaseConnection(_logFactory.GetLogger(), connection, command);
}
DatabaseConnection's constructor:
public DatabaseConnection(ILogger logger, MySqlConnection connection, MySqlCommand command)
{
    _logger = logger;
    _connection = connection;    
    _command = command;
    _connection.Open();
}
When I comment out this line, it reaches the Console.WriteLine
_connection.Open();
",<c#><mysql>,2182,0,60,1,8,31,101,36,340,,81,2,14,2020-04-07 13:49,2020-04-12 18:20,,5.0,,Basic,13
54714594,Google CloudSQLAdmin - The service account does not have the required permissions for the bucket,"I am writing a python function which uses service account credentials to call the Google cloudSQLAdmin api to export a database to a bucket. 
The service account has been given project owner permissions, and the bucket has permissions set for project owners. The sqlAdmin api has been enabled for our project. 
Python code:
from google.oauth2 import service_account
from googleapiclient.discovery import build
import googleapiclient
import json
def main():
    SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin', 'https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/devstorage.full_control']
    SERVICE_ACCOUNT_FILE = './creds/service-account-credentials.json'
    PROJECT = ""[REDACTED]""
    DB_INSTANCE = ""[REDACTED]""
    BUCKET_PATH = ""gs://[REDACTED]/[REDACTED].sql""
    DATABASES = [REDACTED]
    BODY = { # Database instance export request.
    ""exportContext"": { # Database instance export context. # Contains details about the export operation.
      ""kind"": ""sql#exportContext"", # This is always sql#exportContext.
      ""fileType"": ""SQL"", # The file type for the specified uri.
          # SQL: The file contains SQL statements.
          # CSV: The file contains CSV data.
      ""uri"": BUCKET_PATH, # The path to the file in Google Cloud Storage where the export will be stored. The URI is in the form gs://bucketName/fileName. If the file already exists, the requests succeeds, but the operation fails. If fileType is SQL and the filename ends with .gz, the contents are compressed.
      ""databases"": DATABASES,
    },
  }
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    sqladmin = googleapiclient.discovery.build('sqladmin', 'v1beta4', credentials=credentials)
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
    print(json.dumps(response, sort_keys=True, indent=4))
Running this code nets the following error:
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""[REDACTED]/main.py1"", line 47, in hello_pubsub
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://www.googleapis.com/sql/v1beta4/projects/[REDACTED]/instances/[REDACTED]/export?alt=json returned ""The service account does not have the required permissions for the bucket.""&gt;
I have tried this across 2 GCP projects, with multiple service accounts with varying permissions. 
Related questions:
Access denied for service account (permission issue?) when importing a csv from cloud storage to cloud sql - This issue was caused by incorrect permissions, which shouldn't be the case here as the account has project owner permissions
",<python><google-api><google-cloud-platform><google-oauth><google-cloud-sql>,3123,5,36,141,1,1,4,68,12673,0.0,0,3,14,2019-02-15 17:49,2019-03-29 10:31,,42.0,,Basic,13
48239668,Fails to initialize MySQL database on Windows 10,"Using Laradock
System Info:
Docker version: 17.10.0-ce, build f4ffd25
OS: Windows 10 Home
When I run docker-compose up -d mysql I'm getting error. Following is the docker logs
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[ERROR] --initialize specified but the data directory has files in it. Aborting.
[ERROR] Aborting
I have tried deleting mysql folder under ~/.laradock\data and didn't work.
Update 1
MySQL Container under laradock Dockerfile
mysql:
  build:
    context: ./mysql
    args:
      - MYSQL_VERSION=${MYSQL_VERSION}
  environment:
    - MYSQL_DATABASE=${MYSQL_DATABASE}
    - MYSQL_USER=${MYSQL_USER}
    - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    - TZ=${WORKSPACE_TIMEZONE}
  volumes:
    - ${DATA_SAVE_PATH}/mysql:/var/lib/mysql
    - ${MYSQL_ENTRYPOINT_INITDB}:/docker-entrypoint-initdb.d
  ports:
    - &quot;${MYSQL_PORT}:3306&quot;
  networks:
    - backend
MySQL Dockerfile
ARG MYSQL_VERSION=8.0
FROM mysql:${MYSQL_VERSION}
MAINTAINER Mahmoud Zalt &lt;mahmoud@zalt.me&gt;
#####################################
# Set Timezone
#####################################
ARG TZ=UTC
ENV TZ ${TZ}
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone
RUN chown -R mysql:root /var/lib/mysql/
ADD my.cnf /etc/mysql/conf.d/my.cnf
CMD [&quot;mysqld&quot;]
EXPOSE 3306
Update 2
After I delete mysql folder under ~/.laradock/data I'm getting following error. After the command it generates the files in below image. When I rerun giving back the previous error mentioned above.
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[Warning] Setting lower_case_table_names=2 because file system for /var/lib/mysql/ is case insensitive
[Warning] You need to use --log-bin to make --log-slave-updates work.
libnuma: Warning: /sys not mounted or invalid. Assuming one node: No such file or directory
mbind: Operation not permitted
[ERROR]
InnoDB: Operating system error number 22 in a file operation.
[ERROR] InnoDB: Error number 22 means
'Invalid argument'
[ERROR] InnoDB: File
./ib_logfile101: 'aio write' returned OS error 122. Cannot continue
operation
[ERROR] InnoDB: Cannot
continue operation.
** I tried in a windows 7 machine and its working.
",<mysql><docker><laradock>,2741,2,43,8479,4,43,70,49,20058,0.0,527,4,14,2018-01-13 12:02,2018-01-13 12:18,2018-02-05 21:38,0.0,23.0,Basic,14
64068518,Postgres race condition involving subselect and foreign key,"We have 2 tables defined as follows
CREATE TABLE foo (
  id BIGSERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);
CREATE TABLE bar (
  foo_id BIGINT UNIQUE,
  foo_name TEXT NOT NULL UNIQUE REFERENCES foo (name)
);
I've noticed that when executing the following two queries concurrently
INSERT INTO foo (name) VALUES ('BAZ')
INSERT INTO bar (foo_name, foo_id) VALUES ('BAZ', (SELECT id FROM foo WHERE name = 'BAZ'))
it is possible under certain circumstances to end up inserting a row into bar where foo_id is NULL. The two queries are executed in different transactions, by two completely different processes.
How is this possible? I'd expect the second statement to either fail due to a foreign key violation (if the record in foo is not there), or succeed with a non-null value of foo_id (if it is).
What is causing this race condition? Is it due to the subselect, or is it due to the timing of when the foreign key constraint is checked?
We are using isolation level &quot;read committed&quot; and postgres version 10.3.
EDIT
I think the question was not particularly clear on what is confusing me. The question is about how and why 2 different states of the database were being observed during the execution of a single statement. The subselect is observing that the record in foo as being absent, whereas the fk check sees it as present. If it's just that there's no rule preventing this race condition, then this is an interesting question in itself - why would it not be possible to use transaction ids to ensure that the same state of the database is observed for both?
",<sql><postgresql><concurrency><foreign-keys><subquery>,1578,0,16,522,0,6,23,44,935,0.0,136,5,14,2020-09-25 17:08,2020-09-25 19:10,2020-09-25 19:10,0.0,0.0,Advanced,32
51294893,How to connect Ms SQL from a Flutter App?,"Some context: The Db already exists and the app is for internal use of the company, that's why I'm not doing an API.
I need to connect my app to an SQL server to execute a query and retreive data from it.
I've already tried with this plugin but no succes SqlJocky5
Someone have done something similar already with flutter? How you did it? there's another library for connecting the app with a sql server?
So, What I'm looking for is if there's a library to do it like in Xamarin Forms (SqlClient) or in Android Studio Java (JDBC Driver).
",<android><sql><dart><flutter>,538,1,0,1116,1,10,26,48,47670,0.0,21,1,14,2018-07-11 22:14,2018-07-16 19:06,2018-07-16 19:06,5.0,5.0,Intermediate,20
49554728,Jenkins Job - DatabaseError: file is encrypted or is not a database,"When running this code for connecting to a db through cmd - locally and on the actual server it works fine. But I have set it up on Jenkins and receive the error: 
DatabaseError: file is encrypted or is not a database
It seems to be happening on this line:
  self.cursor.execute(*args)
The database class is:
class DatabaseManager(object):
    def __init__(self, db):
        self.conn = sqlite3.connect(db)
        self.cursor = self.conn.cursor()
    def query(self, *args):
        self.cursor.execute(*args)
        self.conn.commit()
        return self.cursor
    def __del__(self):
        self.conn.close()
",<python><jenkins><sqlite>,615,0,14,1207,4,27,50,47,442,0.0,11,3,14,2018-03-29 11:18,2018-03-29 11:23,,0.0,,Basic,14
62324520,How to use SSH Tunnel to connect to an RDS instance via an EC2 instance?,"So this is really new to me, so apologies if this is a dumb question.
I have a RDS instance that is not publicly accessible and is sitting in its own private VPC. I have an EC2 instance that is allowed to connect to RDS, but nothing else is allowed to connect to the instance.
I now want PgAdmin to be able to show data from my RDS instance.
I went through the wizard in PgAdmin, I put in the EC2 Instance's Public IP as Tunnel host, the username is ec2-user and the authentication is by identity file (using the pem file that I use to ssh into the instance).
However, I still can't connect. In the Advanced tab, PGAdmin asks for a Host address, but complains when I put in my RDS instance's endpoint.
How do I get my local pgAdmin to now access my DB which is no longer accessible to the public internet?
--- forgot to add the error message
Unable to connect to server:
Failed to create the SSH tunnel.
Error: Could not establish session to SSH gateway
",<postgresql><amazon-web-services><amazon-rds><pgadmin>,954,0,5,7366,32,91,157,76,24188,0.0,22,2,14,2020-06-11 12:33,2020-06-12 0:32,,1.0,,Intermediate,20
58378708,SQLAlchemy: Can't reconnect until invalid transaction is rolled back,"I have a weird problem.
I have a simple py3 app, which uses sqlalchemy.
But several hours later, there is an error:
  (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back
My init part:
self.db_engine = create_engine(self.db_config, pool_pre_ping=True) # echo=True if needed to see background SQL
Session = sessionmaker(bind=self.db_engine)
self.db_session = Session()
The query (this is the only query that happens):
while True:
    device_id = self.db_session.query(Device).filter(Device.owned_by == msg['user_id']).first()
    sleep(20)
The whole script is in infinite loop, single threaded (SQS reading out). Does anybody cope with this problem?
",<python><python-3.x><sqlalchemy>,690,0,6,423,1,3,13,47,19864,,9,1,13,2019-10-14 14:17,2019-11-22 10:01,2019-11-22 10:01,39.0,39.0,Basic,13
59034719,Spark: Prevent shuffle/exchange when joining two identically partitioned dataframes,"I have two dataframes df1 and df2 and I want to join these tables many times on a high cardinality field called visitor_id.  I would like to perform only one initial shuffle and have all the joins take place without shuffling/exchanging data between spark executors.  
To do so, I have created another column called visitor_partition that consistently assigns each visitor_id a random value between [0, 1000).  I have used a custom partitioner to ensure that the df1 and df2 are exactly partitioned such that each partition contains exclusively rows from one value of visitor_partition.  This initial repartition is the only time I want to shuffle the data.
I have saved each dataframe to parquet in s3, paritioning by visitor partition -- for each data frame, this creates 1000 files organized in df1/visitor_partition=0, df1/visitor_partition=1...df1/visitor_partition=999.
Now I load each dataframe from the parquet and register them as tempviews via df1.createOrReplaceTempView('df1') (and the same thing for df2) and then run the following query
SELECT
   ...
FROM
  df1 FULL JOIN df1 ON
    df1.visitor_partition = df2.visitor_partition AND
    df1.visitor_id = df2.visitor_id
In theory, the query execution planner should realize that no shuffling is necessary here.  E.g., a single executor could load in data from df1/visitor_partition=1 and df2/visitor_partition=2 and join the rows in there.  However, in practice spark 2.4.4's query planner performs a full data shuffle here.
Is there some way I can prevent this shuffle from taking place?
",<apache-spark><join><pyspark><apache-spark-sql>,1552,0,20,13135,18,58,93,39,3962,0.0,729,1,13,2019-11-25 15:05,2019-11-25 17:16,2019-11-25 17:16,0.0,0.0,Intermediate,23
49152718,DISTINCT ON() in jOOQ,"I would like to make a query in PostgreSQL
select 
  distinct on(uuid) 
  (select nr_zew from bo_get_sip_cti_polaczenie_info(uuid)) as nr_zew 
from bo_sip_cti_event_day
where data_ins::date = current_date
and kierunek like 'P'
and (hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') 
in Java as far I have
Result&lt;Record&gt; result = create
    .select()
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
and it works but as I try to add
Result&lt;Record&gt; result = create
    .selectDistinct(""uuid"")
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
then it says that cannot do selectDistinct(String). How can I use distinct in jOOQ?
",<java><postgresql><jooq>,973,0,22,5196,7,50,85,52,9395,0.0,454,1,13,2018-03-07 13:06,2018-03-08 8:27,2018-03-08 8:27,1.0,1.0,Basic,3
55536681,What happens with returning IEnumerable if used with async/await (streaming data from SQL Server with Dapper)?,"I am using Dapper to stream data from a very large set in SQL Server. It works fine with returning IEnumerable and calling Query(), but when I switch to QueryAsync(), it seems that the program tries to read all of the data from SQL Server instead of streaming.
According to this question, it should work fine with buffered: false, which I am doing, but the question says nothing about async/await.
Now according to this question, it's not straightforward to do what I want with QueryAsync().
Do I understand correctly that enumerables are iterated when the context is switched for async/await?
Another question if this is something that will be possible to do when the new C#8 async streaming is available?
",<c#><sql-server><async-await><dapper><c#-8.0>,707,2,7,28437,27,124,212,35,4718,0.0,3887,3,13,2019-04-05 13:38,2019-04-05 14:04,2019-04-05 14:04,0.0,0.0,Advanced,33
55395326,How to debug T-SQL with SQL Server Management Studio 2017?,"The latest changelog (18.0 Preview 7) of SQL Server Management Studio announced, that the T-SQL Debugger is deprecated.
What are the alternatives for the future? Can someone understand this decision? I fear that removing a fundamental development tool like this will effect many developers.
",<sql-server><t-sql><debugging><ssms><ssms-2017>,291,2,0,9394,10,64,93,65,12881,,1122,3,13,2019-03-28 10:28,2019-05-22 1:08,2019-05-22 1:08,55.0,55.0,Intermediate,21
52426656,Track last modification timestamp of a row in Postgres,"In Postgres I want to store table's last update/insert time. Microsoft SQL Server offers a type timestamp which is automatically maintained by the database. 
But timestamp in Postgres works differently, it is not updated automatically and the column is always null.
",<postgresql><timestamp>,266,0,2,135,1,1,7,35,20813,0.0,6,2,13,2018-09-20 13:47,2019-06-06 15:40,2019-06-06 15:40,259.0,259.0,Basic,9
60384146,Capture the user who deleted the row in Temporal table,"I understand that temporal tables are intended to give you a point in time view of the data. I am using temporal tables for auditing purpose. I have the following Temporal table.
Lets assume this is the current state of the Temporal table:
ID  RoleID  UserID      ModifiedBy
------------------------------------------
1   11      1001        foo@example.com
2   22      1001        foo@example.com
3   33      1002        bar@example.com
4   11      1003        foo@example.com
I have a web application using EF Core. My EF code always sets the ModifiedBy to currently logged in user. I logged into the application as bar@example.com and deleted a record with ID 2. SQL Server will automatically insert the deleted record into the history table as expected and keep ModifiedBy as foo@example.com because that was the point in time value of ModifiedBy column.
However now the system does not know who deleted the row. In this scenario bar@example.com is the one who actually deleted the row. How do I capture the user who deleted the record? What are my options here?
",<sql-server><t-sql><ef-core-2.2><temporal-tables>,1067,0,10,31334,54,220,415,77,1833,0.0,249,2,13,2020-02-24 21:27,2021-07-17 13:21,,509.0,,Basic,9
50809120,Postgres INSERT INTO with SELECT ordering,"When inserting into Postgres via a select statement, are the rows guaranteed to be inserted in the same order that the select statement returns them?
That is, given a table bar (where id is SERIAL PRIMARY KEY, and name is TEXT):
id | name
---+-----
 0 | A
 1 | B
 2 | C
And another table, foo (empty and with the same schema), if I INSERT INTO foo (name) SELECT name FROM bar ORDER BY id DESC will foo be guaranteed to have:
id | name
---+-----
 0 | C
 1 | B
 2 | A
This seems to be the case, but I'd like to confirm that it isn't an implementation detail that may not hold with larger selects.
I read through section 13.8 in the SQL-92 standard and general rule #3 claims that ""The query expression is effectively evaluated before inserting any rows into B."", but it doesn't explicitly say anything about ordering. Is the standard purposefully vague (perhaps to allow parallel insertions?) and ordering is an implementation detail?
",<sql><postgresql>,933,1,19,1555,1,10,15,64,6563,,19,2,13,2018-06-12 4:09,2018-06-12 5:11,,0.0,,Basic,9
53129719,Semantics of INSERT SELECT FOR UPDATE ON CONFLICT DO NOTHING RETURNING,"We have encountered a very peculiar issue with our production system. Unfortunately despite a lot of effort, I have not been able to reproduce the issue locally, so I cannot provide a minimal, complete and verifiable example. Also, as this is production code, I have had to change the names of the tables in the following example. However I believe I am presenting all the relevant facts.
We have four tables bucket_holder, bucket, item and bucket_total created as follows:
CREATE TABLE bucket_holder (
  id SERIAL PRIMARY KEY,
  bucket_holder_uid UUID NOT NULL
);
CREATE TABLE bucket ( 
  id SERIAL PRIMARY KEY, 
  bucket_uid UUID NOT NULL, 
  bucket_holder_id INTEGER NOT NULL REFERENCES bucket_holder (id), 
  default_bucket BOOLEAN NOT NULL
);
CREATE TABLE item ( 
  id SERIAL PRIMARY KEY, 
  item_uid UUID NOT NULL, 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
CREATE TABLE bucket_total ( 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
There are also indexes on appropriate columns as follows:
CREATE UNIQUE INDEX idx1 ON bucket_holder (bucket_holder_uid);
CREATE UNIQUE INDEX idx2 ON bucket (bucket_uid);
CREATE UNIQUE INDEX idx3 ON item (item_uid);
CREATE UNIQUE INDEX idx4 ON bucket_total (bucket_id);
The idea is that a bucket_holder holds buckets, one of which is a default_bucket, buckets hold items and each bucket has a unique bucket_total record containing the sum of the amounts of all the items.
We are trying to do bulk inserts into the item table as follows:
WITH
unnested AS ( 
  SELECT * 
  FROM UNNEST(
    ARRAY['00000000-0000-0000-0000-00000000001a', '00000000-0000-0000-0000-00000000002a']::UUID[], 
    ARRAY['00000000-0000-0000-0000-00000000001c', '00000000-0000-0000-0000-00000000002c']::UUID[], 
    ARRAY[1.11, 2.22]::NUMERIC[]
  ) 
  AS T(bucket_holder_uid, item_uid, amount) 
), 
inserted_item AS ( 
  INSERT INTO item (bucket_id, item_uid, amount) 
  SELECT bucket.id, unnested.item_uid, unnested.amount 
  FROM unnested 
  JOIN bucket_holder ON unnested.bucket_holder_uid = bucket_holder.bucket_holder_uid 
  JOIN bucket ON bucket.bucket_holder_id = bucket_holder.id 
  JOIN bucket_total ON bucket_total.bucket_id = bucket.id 
  WHERE bucket.default_bucket 
  FOR UPDATE OF bucket_total 
  ON CONFLICT DO NOTHING 
  RETURNING bucket_id, amount 
), 
total_for_bucket AS ( 
  SELECT bucket_id, SUM(amount) AS total 
  FROM inserted_item 
  GROUP BY bucket_id 
) 
UPDATE bucket_total 
SET amount = amount + total_for_bucket.total 
FROM total_for_bucket 
WHERE bucket_total.bucket_id = total_for_bucket.bucket_id
In reality the arrays passed in are dynamic and have length up to 1000, but all 3 arrays have the same length. The arrays are always sorted so that the bucket_holder_uids are in order in order to ensure that deadlock cannot occur. The point of the ON CONFLICT DO NOTHING is that we should be able to handle the situation where some of the items were already present (the conflict is on item_uid). In this case the bucket_total should of course not be updated.
This query assumes that appropriate bucket_holder, bucket and bucket_total records already exist. It is ok for the query to fail otherwise as in practice this situation will not occur. Here is an example of setting up some sample data:
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000001a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000001b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000001a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000001b'), 0);
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000002a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000002b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000002a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000002b'), 0);
This query appears to have done the correct thing for hundreds of thousands of items, but for a handful of items, the bucket_total has been updated by twice the amount of the item. I don't know if it's been updated twice or if it was updated once by twice the amount of the item. However in these cases, only one item has been inserted (inserting twice would be impossible anyway as there is a uniqueness constraint on item_uid). Our logs suggest that for the affected buckets, two threads were executing the query simultaneously. 
Can anyone see and explain any issue with this query and indicate how it could be rewritten?
We are using version PG9.6.6
UPDATE
We've spoken to a core postgres developer about this, who apparently doesn't see a concurrency issue here. We're now investigating really nasty possibilities such as index corruption, or the (remote) chance of a pg bug.
",<sql><postgresql><concurrency><common-table-expression>,5103,0,94,522,0,6,23,64,819,0.0,136,1,13,2018-11-03 8:40,2018-11-08 4:17,,5.0,,Basic,9
48249783,Search Query not accurate enough,"I have a search query done by me to the best of my knowledge in PHP but there are some improvements required:
When I search say 'what is food' and I have 'what is food' in the database all results containing one of the keywords 'what', 'is', 'food' are shown. The desired behaviour is to display results containing the exact phrase 'what is food' (first)
Only the last word in the query is highlighted and I want to highlight all words
Desired behaviour: The right answer shows at the top, regardless of its position in the database.
My current code is like this:
if (isset($_GET[""mainSearch""]))
{
  $condition = '';
  $mainSearch = SQLite3::escapeString($_GET['mainSearch']);
  $keyword = $_GET['mainSearch'];
  $query = explode("" "", $keyword);
  $perpageview=7;
  if ($_GET[""pageno""])
  {
      $page=$_GET[""pageno""];
  }
  else
  {
      $page=1;
  }
  $frompage = $page*$perpageview-$perpageview;
  foreach ($query as $text)
  {
      $condition .= ""question LIKE '%"".SQLite3::escapeString($text).""%' OR answer LIKE '%"".SQLite3::escapeString($text).""%' OR "";
  }
  foreach ($query as $text_2)
  {
      $condition_2 .= ""bname LIKE '%"".SQLite3::escapeString($text_2).""%' OR bankreq LIKE '%"".SQLite3::escapeString($text_2).""%' OR "";
  }
  $condition = substr($condition, 0, -4);
  $condition_2 = substr($condition_2, 0, -4);
  $order = "" ORDER BY quiz_id DESC "";
  $order_2 = "" ORDER BY id DESC "";
  $sql_query = ""SELECT * FROM questions WHERE "" . $condition . ' '. $order.' LIMIT '.$frompage.','.$perpageview;
  $sql_query_count = ""SELECT COUNT(*) as count FROM questions WHERE "" . $condition .' '. $order;
  //$mainAnswer = ""SELECT * FROM questions WHERE question LIKE '%$mainSearch%' or answer LIKE '%$mainSearch%'"";
  $bank_query = ""SELECT * FROM banks WHERE "" . $condition_2 . ' LIMIT 1';
  $result = $db-&gt;query($sql_query);
  $resultCount = $db-&gt;querySingle($sql_query_count);
  $bankret = $db-&gt;query($bank_query);
  //$mainAnsRet = $db-&gt;query($mainAnswer);
  $pagecount = ceil($resultCount/$perpageview);
  if ($resultCount &gt; 0)
  {
  if ($result &amp;&amp; $bankret)
  {
      while ($row = $result-&gt;fetchArray(SQLITE3_ASSOC))
      {
          $wording = str_replace($text, ""&lt;span style='font-weight: bold; color: #1a0dab;'&gt;"".$text.""&lt;/span&gt;"", $row['answer']);
           echo '&lt;div class=""quesbox_3""&gt;
            &lt;div class=""questitle""&gt;
                &lt;h2&gt;'.$row[""question""].'&lt;/h2&gt;
            &lt;/div&gt;
            &lt;div class=""quesanswer""&gt;'.$wording.'&lt;/div&gt;
        &lt;/div&gt;';
      }
      while ($brow = $bankret-&gt;fetchArray(SQLITE3_ASSOC))
      {
            $bname = $brow['bname'];
            $bankbrief = $brow['bankbrief'];
            $bankreq = $brow['bankreq'];
            $bankaddress = $brow['bankaddress'];
            $banklogo = $brow['banklogo'];
            $founded = $brow['founded'];
            $owner = $brow['owner'];
            $available = $brow['available'];
           echo '&lt;div class=""modulecontent""&gt;
            &lt;div class=""modulename""&gt;
                &lt;div class=""mname""&gt;'.$bname.'&lt;/div&gt;
                &lt;div class=""mlogo""&gt;&lt;img src=""'.$banklogo.'""&gt;&lt;/div&gt;
            &lt;/div&gt;';
            if (strlen($bankreq) &gt; 300)
            {
                $bankcut = substr($bankreq, 0, 300);
                $bankreq = substr($bankcut, 0, strrpos($bankcut, ' ')).'... &lt;a href=""bankprofile.php?bname='.$bname.'""&gt;Read More&lt;/a&gt;';
                echo '&lt;div class=""modulebrief""&gt;'.$bankreq.'&lt;/div&gt;';
            }
            echo '&lt;div class=""modulelinks""&gt;
                &lt;div class=""mfound""&gt;Founded: &lt;span&gt;'.$founded.'&lt;/span&gt;&lt;/div&gt;
                &lt;div class=""mowned""&gt;Ownd By: &lt;span&gt;'.$owner.'&lt;/span&gt;&lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;';
               // &lt;div class=""mavailable""&gt;Available for Export Loan: &lt;span&gt;'.$available.'&lt;/span&gt;&lt;/div&gt;
      }
      ?&gt;
      &lt;div class=""page_num""&gt;
      &lt;?php
      for ($i=1; $i &lt;= $pagecount; $i++) {
         echo '&lt;a href=""searchresult.php?mainSearch='.$mainSearch.'&amp;pageno='.$i.'""&gt;'.$i.'&lt;/a&gt;';
      }
      ?&gt;
      &lt;/div&gt;
      &lt;?php
  }
  }
  else
  {
      $session_n = $_SESSION['log_id'];
      $sesdate = date('d/M/Y');
      echo ""&lt;div class='searchNone'&gt;&lt;p&gt;No results found&lt;/p&gt;&lt;/div&gt;
      &lt;div class='sendSearchQ'&gt;
      &lt;p&gt;Please send us your question.&lt;/p&gt;
      &lt;form action='sendquestion.php' method='post' encytype='multipart/form-data'&gt;
      &lt;div class='searchQinputs'&gt;
          &lt;input type='text' name='searchQuestion' id='searchQuestion'placeholder='Whats your question'&gt;&lt;br&gt;
          &lt;input type='submit' name='sendQuestion' id='sendQuestion' value='Send'&gt;
          &lt;input type='text' name='user' id='user' value='$session_n' style='display: none'&gt;
          &lt;input type='text' name='qDate' id='qDate' value='$sesdate' style='display: none'&gt;
          &lt;input type='text' name='status' id='status' value='0' style='display: none'&gt;
          &lt;/div&gt;
      &lt;/form&gt;
      &lt;/div&gt;"";
  }
}
",<php><search><sqlite>,5282,0,124,493,1,7,29,68,444,0.0,25,5,13,2018-01-14 12:55,2018-01-18 18:59,2018-01-18 18:59,4.0,4.0,Basic,9
51033689,How to fix error on postgres install ubuntu,"I'm struggling to fix an error on install of the postgres client. I'm installing this on a Continuous Integration build, so I need it to install without error. The thing is, the client is installed, and I can even run psql commands, if I ssh into the server, but I need this to run without my touch, which means the install has to happen without error. 
I've done all the google-foo, and none of the suggestions I've seen on Ubuntu forums, or here seem to point in the right direction. This is all on ubuntu 14.04.
Alternatively, maybe I can just silence the errors, as long as the client is usable.
Following is the error I run into:
sudo apt-get install postgresql-client
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following additional packages will be installed:
      libpq5 postgresql-client-9.6 postgresql-client-common
    Suggested packages:
      postgresql-9.6 postgresql-doc-9.6
    The following NEW packages will be installed:
      libpq5 postgresql-client postgresql-client-9.6 postgresql-client-common
    0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.
    Need to get 1494 kB of archives.
    After this operation, 6121 kB of additional disk space will be used.
    Get:1 http://deb.debian.org/debian stretch/main amd64 libpq5 amd64 9.6.7-0+deb9u1 [132 kB]
    Get:2 http://deb.debian.org/debian stretch/main amd64 postgresql-client-common all 181+deb9u1 [79.0 kB]
    Get:3 http://deb.debian.org/debian stretch/main amd64 postgresql-client-9.6 amd64 9.6.7-0+deb9u1 [1228 kB]
    Get:4 http://deb.debian.org/debian stretch/main amd64 postgresql-client all 9.6+181+deb9u1 [55.7 kB]
    Fetched 1494 kB in 0s (55.5 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    Selecting previously unselected package libpq5:amd64.
    (Reading database ... 31433 files and directories currently installed.)
    Preparing to unpack .../libpq5_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking libpq5:amd64 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client-common.
    Preparing to unpack .../postgresql-client-common_181+deb9u1_all.deb ...
    Unpacking postgresql-client-common (181+deb9u1) ...
    Selecting previously unselected package postgresql-client-9.6.
    Preparing to unpack .../postgresql-client-9.6_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client.
    Preparing to unpack .../postgresql-client_9.6+181+deb9u1_all.deb ...
    Unpacking postgresql-client (9.6+181+deb9u1) ...
    Setting up libpq5:amd64 (9.6.7-0+deb9u1) ...
    Processing triggers for libc-bin (2.24-11+deb9u3) ...
    Setting up postgresql-client-common (181+deb9u1) ...
    Setting up postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    update-alternatives: using /usr/share/postgresql/9.6/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode
    update-alternatives: error: error creating symbolic link '/usr/share/man/man7/ABORT.7.gz.dpkg-tmp': No such file or directory
    dpkg: error processing package postgresql-client-9.6 (--configure):
     subprocess installed post-installation script returned error exit status 2
    dpkg: dependency problems prevent configuration of postgresql-client:
     postgresql-client depends on postgresql-client-9.6; however:
      Package postgresql-client-9.6 is not configured yet.
    dpkg: error processing package postgresql-client (--configure):
     dependency problems - leaving unconfigured
    Errors were encountered while processing:
     postgresql-client-9.6
     postgresql-client
    E: Sub-process /usr/bin/dpkg returned an error code (1)
    Exited with code 100
I've tried the following to fix:
    sudo apt-get purge postgr*
    sudo apt-get autoremove
    sudo apt-get install synaptic
    sudo apt-get update
from: https://ubuntuforums.org/showthread.php?t=2277582
    which psql
    /usr/bin/psql
And
    more /etc/apt/sources.list
    deb http://deb.debian.org/debian stretch main
    deb http://deb.debian.org/debian stretch-updates main
    deb http://security.debian.org/debian-security stretch/updates main
I'm stumped on how to move forward. 
",<postgresql><ubuntu><debian>,4276,4,2,1049,1,14,40,68,10181,0.0,51,1,13,2018-06-26 1:20,2018-10-04 20:55,2018-10-04 20:55,100.0,100.0,Basic,9
56017410,Pyspark Error:- dataType <class 'pyspark.sql.types.StringType'> should be an instance of <class 'pyspark.sql.types.DataType'>,"I need to extract some data from a pipelinedRDD but while converting it to Dataframe it is giving the following error:
Traceback (most recent call last):
  File ""/home/karan/Desktop/meds.py"", line 42, in &lt;module&gt;
    relevantToSymEntered(newrdd)
  File ""/home/karan/Desktop/meds.py"", line 26, in relevantToSymEntered
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed 
medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms 
Recorded"",""Severeness""],ArrayType)]))
  File ""/home/karan/Downloads/spark-2.4.2-bin-
hadoop2.7/python/pyspark/sql/types.py"", line 409, in __init__
    ""dataType %s should be an instance of %s"" % (dataType, DataType)
AssertionError: dataType &lt;class 'pyspark.sql.types.StringType'&gt; should be an 
instance of &lt;class 'pyspark.sql.types.DataType'&gt;
1. Thing my error is of different type it is TypeError while I got problems with AssertionError.
My problem has nothing to do with casting of data types.
I've already tried using toDF() but it changes the column names which is undesirable.
import findspark
findspark.init('/home/karan/Downloads/spark-2.4.2-bin-hadoop2.7')
from pyspark.sql import SQLContext
from pyspark.sql.types import StructType, StringType, IntegerType, StructField, ArrayType
from pyspark import SparkConf, SparkContext
import pandas as pd
def reduceColoumns(self):
    try:
        filtered=self.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""Disease""],x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return filtered
def cleanData(self,s):
    try:
        self.zipWithIndex
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return self.filter(lambda x: x[1][0]==s)
def relevantToSymEntered(self):
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms Recorded"",""Severeness""],ArrayType)]))
    #mat = mat.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    print(type(mat))
conf = SparkConf().setMaster(""local[*]"").setAppName(""MovieSimilarities"")
sc = SparkContext(conf = conf)
spark=SQLContext(sc)
rdd = spark.read.csv(""/home/karan/Desktop/ExportExcel2.csv"",header=True,sep="","",multiLine=""True"")
print(rdd)
newrdd=reduceColoumns(rdd)
x=input(""Enter the disease-"")
newrdd=cleanData(newrdd,x)
relevantToSymEntered(newrdd)
",<python><apache-spark><pyspark><apache-spark-sql>,2474,0,61,149,1,2,7,59,24530,0.0,0,1,13,2019-05-07 7:16,2020-05-05 3:53,,364.0,,Basic,9
48189015,How to perform case insensitive ORDER BY in mysql?,"I want to perform case-insensitive ORDER BY in MySQL.
I have the data in my database like 
A, C, b, e, D etc
I'm getting the result as 
A, C, D, b, e 
But, I want the result as
A, b, C, D, e 
How can I get that?
",<mysql>,212,0,0,668,1,9,22,76,10626,0.0,313,4,13,2018-01-10 13:55,2018-01-10 13:57,2018-01-10 13:57,0.0,0.0,Basic,10
53986727,"""Insert Into"" statement causing errors due to ""Parameter 7 (""""): The supplied value is not a valid instance of data type float.""","I'm loading a batch of CSV files into a SQL Server table using Python one row at a time.  The files each contain a number of free text fields and erroneous data which I trim and rename before attempting to insert.  
In general (about 95% of the time), the code seems to work however exceptions appear with the error message described below.
I'm confused as a) I only have four columns in my table, and can't understand why it would be looking for Parameter 7, and b) the text columns are being loaded into nvarchar(max) formatted columns, so I wouldn't expect a data type error.  
I've checked the source files to see which rows threw an error, there seems to be no discernible difference between the problem rows and others that are successfully loaded.
I've trimmed the process right back to only insert the JobID (as a bigint) and it works without issue, but as soon as I bring in the text fields, it causes an error.
I'm using Python 3.7.0 and loading into SQL Server 14.0
import numpy as np
import pyodbc
import os
import glob
import pandas as pd
import csv
import config
import urllib
import shutil
import codecs
path = ""C:\\myFilePath""
allFiles = glob.glob(os.path.join(path, ""*.csv""))
for file_ in allFiles:
    df = pd.concat((pd.read_csv(f, encoding='utf8') for f in allFiles))
cnxn = pyodbc.connect(""Driver={ODBC Driver 13 for SQL Server};""                                             
                  ""Server=myServer;""
                  ""Database=myDatabase;""
                  ""Trusted_Connection=yes;""
                  ""SelectMethod=cursor;""
                  )
df2 = df[['JobID', 'NPS_score', 'Obtuse_Column_Name_1', 'Obtuse_Column_Name_2']].copy()
df2.columns = ['JobID', 'Score','Q1', 'Q2']
cursor = cnxn.cursor()
for index,row in df2.iterrows():
    try:
        counter = counter + 1
        cursor.execute(""""""insert into [myDB].[dbo].[input_test]( [JobID], [Score], [Q1], [Q2]) VALUES (?, ?, ?, ?)"""""", row['JobID'],row['Score'],row['Q1'], row['Q2'])
        cursor.commit()
        print(counter)
    except Exception as e:
        print(e) 
        continue    
cursor.close()  
cnxn.close()
I expect the data to be loaded but on some lines get the following error code: 
  ('42000', '[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL
  Server]The incoming tabular data stream (TDS) remote procedure call
  (RPC) protocol stream is incorrect. Parameter 7 (""""): The supplied
  value is not a valid instance of data type float. Check the source
  data for invalid values. An example of an invalid value is data of
  numeric type with scale greater than precision. (8023)
  (SQLExecDirectW)')
",<python><sql-server><pandas>,2620,0,41,379,1,2,10,71,27292,0.0,0,3,13,2018-12-31 11:06,2018-12-31 13:12,2018-12-31 13:12,0.0,0.0,Basic,9
53529974,What does df.repartition with no column arguments partition on?,"In PySpark the repartition module has an optional columns argument which will of course repartition your dataframe by that key.
My question is - how does Spark repartition when there's no key? I couldn't dig any further into the source code to find where this goes through Spark itself.
def repartition(self, numPartitions, *cols):
    """"""
    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The
    resulting DataFrame is hash partitioned.
    :param numPartitions:
        can be an int to specify the target number of partitions or a Column.
        If it is a Column, it will be used as the first partitioning column. If not specified,
        the default number of partitions is used.
    .. versionchanged:: 1.6
       Added optional arguments to specify the partitioning columns. Also made numPartitions
       optional if partitioning columns are specified.
    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()
    10
    &gt;&gt;&gt; data = df.union(df).repartition(""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  5|  Bob|
    |  5|  Bob|
    |  2|Alice|
    |  2|Alice|
    +---+-----+
    &gt;&gt;&gt; data = data.repartition(7, ""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  2|Alice|
    |  5|  Bob|
    |  2|Alice|
    |  5|  Bob|
    +---+-----+
    &gt;&gt;&gt; data.rdd.getNumPartitions()
    7
    """"""
    if isinstance(numPartitions, int):
        if len(cols) == 0:
            return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)
        else:
            return DataFrame(
                self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)
    elif isinstance(numPartitions, (basestring, Column)):
        cols = (numPartitions, ) + cols
        return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)
    else:
        raise TypeError(""numPartitions should be an int or Column"")
For example: it's totally fine to call these lines but I have no idea what it's actually doing. Is it a hash of the entire line? Perhaps the first column in the dataframe?
df_2 = df_1\
       .where(sf.col('some_column') == 1)\
       .repartition(32)\
       .alias('df_2')
",<python><apache-spark><pyspark><apache-spark-sql>,2260,0,54,177,0,1,11,47,4970,0.0,19,1,13,2018-11-29 0:04,2018-11-29 0:31,,0.0,,Basic,2
53536206,Android Room database query does not return id column,"The problem is that the query returns all columns except  'id'
I use fts4 and in docs it says:
  FTS-enabled tables always use a primary key of type INTEGER and with
  the column name ""rowid"". If your FTS-table-backed entity defines a
  primary key, it must use that type and column name.
here is my entity class:
@Fts4
@Entity(tableName = ""projects"")
public class Project {
    @ColumnInfo(name = ""rowid"")
    @PrimaryKey(autoGenerate = true)
    private int id;
    private String name;
    @ColumnInfo(name = ""start_date"")
    private String startDate;
    @ColumnInfo(name = ""end_date"")
    private String endDate;
    private String description;
    @ColumnInfo(name = ""icon_path"")
    private String iconPath;
    private long budget;
public Project(String name, String startDate, String endDate, String description, String iconPath, long budget) {
    this.name = name;
    this.startDate = startDate;
    this.endDate = endDate;
    this.description = description;
    this.iconPath = iconPath;
    this.budget = budget;
}
public int getId() {
    return id;
}
public void setId(int id) {
    this.id = id;
}
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getStartDate() {
    return startDate;
}
public void setStartDate(String startDate) {
    this.startDate = startDate;
}
public String getEndDate() {
    return endDate;
}
public void setEndDate(String endDate) {
    this.endDate = endDate;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getIconPath() {
    return iconPath;
}
public void setIconPath(String iconPath) {
    this.iconPath = iconPath;
}
public long getBudget() {
    return budget;
}
public void setBudget(long budget) {
    this.budget = budget;
}
and here is my simple query:
@Query(""SELECT * FROM projects"")
public LiveData&lt;List&lt;Project&gt;&gt; getAllProjectsI);
I got a warning :
  app.aarsham.projeno.data.Model.Project has some fields [rowid] which
  are not returned by the query. If they are not supposed to be read
  from the result, you can mark them with @Ignore annotation. You can
  suppress this warning by annotating the method with
  @SuppressWarnings(RoomWarnings.CURSOR_MISMATCH). Columns returned by
  the query: name, start_date, end_date, description, icon_path, budget.
  Fields in app.aarsham.projeno.data.Model.Project: rowid, name,
  start_date, end_date, description, icon_path, budget.
and an error:
  The columns returned by the query does not have the fields [id] in
  app.aarsham.projeno.data.Model.Project even though they are annotated
  as non-null or primitive. Columns returned by the query:
  [name,start_date,end_date,description,icon_path,budget]
can anyone help about this?
",<android><sqlite><android-room><fts4>,2833,0,85,161,0,2,9,76,4006,0.0,67,1,13,2018-11-29 9:55,2018-12-30 3:09,2018-12-30 3:09,31.0,31.0,Basic,10
