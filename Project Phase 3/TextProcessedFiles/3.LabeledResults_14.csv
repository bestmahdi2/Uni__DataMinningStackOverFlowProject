QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
54944677,Sql Server LDF file taking too large space,"Why is my database log file taking to high space? Its almost taking up 30GB of my HDD. Even after deleting 1,000,000 records, its not freeing up any space.
So, 
1.Why is the log file taking this much space (30gb)?2.how can I free up the space?
",<sql-server>,244,0,5,612,2,6,32,75,43163,0.0,244,5,15,2019-03-01 12:27,2019-03-01 13:00,2019-03-01 13:00,0.0,0.0,Intermediate,23
50793970,Select shortest and longest string,"Is it possible to select the shortest and longest strings by characters in a table?
I have a CITY column of type VARCHAR(20) and I want to select the shortest and longest city names in alphabetical order by length.
I did like this
SELECT CITY,LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) IN ( SELECT MAX(LENGTH(CITY)) FROM STATION UNION SELECT MIN(LENGTH(CITY)) FROM STATION ) ORDER BY CITY ASC;
When ordered alphabetically, Let the CITY names be listed as ABC, DEF, PQRS, and WXY, with the respective lengths 3,3,4, and 3. The longest-named city is obviously PQRS, but there are options for the shortest-named city; I have to select ABC because it comes first alphabetically.
My query ended up with all three CITY having length 3.
ABC 3 DEF 3 PQRS 4 WXY 3 
The result of SELECT must be
ABC 3 PQRS 4 
",<mysql><sql>,801,0,5,513,1,6,13,74,40990,0.0,314,20,15,2018-06-11 9:03,2018-06-11 9:15,2018-06-12 9:08,0.0,1.0,Basic,9
49432167,How to convert rows into a list of dictionaries in pyspark?,"I have a DataFrame(df) in pyspark, by reading from a hive table:
df=spark.sql('select * from &lt;table_name&gt;')
+++++++++++++++++++++++++++++++++++++++++++
|  Name    |    URL visited               |
+++++++++++++++++++++++++++++++++++++++++++
|  person1 | [google,msn,yahoo]           |
|  person2 | [fb.com,airbnb,wired.com]    |
|  person3 | [fb.com,google.com]          |
+++++++++++++++++++++++++++++++++++++++++++
When i tried the following, got an error
df_dict = dict(zip(df['name'],df['url']))
""TypeError: zip argument #1 must support iteration.""
type(df.name) is of 'pyspark.sql.column.Column'
How do i create a dictionary like the following, which can be iterated later on
{'person1':'google','msn','yahoo'}
{'person2':'fb.com','airbnb','wired.com'}
{'person3':'fb.com','google.com'}
Appreciate your thoughts and help. 
",<apache-spark><pyspark><apache-spark-sql>,833,0,16,169,1,1,3,55,52106,0.0,0,4,15,2018-03-22 15:10,2018-03-22 15:28,,0.0,,Basic,10
48847660,Spark + Parquet + Snappy: Overall compression ratio loses after spark shuffles data,"Commmunity!
Please help me understand how to get better compression ratio with Spark?
Let me describe case:
I have dataset, let's call it product on HDFS which was imported using Sqoop ImportTool as-parquet-file using codec snappy. As result of import, I have 100 files with total 46 GB du, files with diffrrent size (min 11MB, max 1.5GB, avg ~ 500MB). Total count of records a little bit more than 8 billions with 84 columns
I'm doing simple read/repartition/write with Spark using snappy as well and as result I'm getting:
~100 GB output size with the same files count, same codec, same count and same columns.
Code snippet:
val productDF = spark.read.parquet(""/ingest/product/20180202/22-43/"")
productDF
.repartition(100)
.write.mode(org.apache.spark.sql.SaveMode.Overwrite)
.option(""compression"", ""snappy"")
.parquet(""/processed/product/20180215/04-37/read_repartition_write/general"")
Using parquet-tools I have looked into random files from both ingest and processed and they looks as below:
ingest:
creator:                        parquet-mr version 1.5.0-cdh5.11.1 (build ${buildNumber}) 
extra:                          parquet.avro.schema = {""type"":""record"",""name"":""AutoGeneratedSchema"",""doc"":""Sqoop import of QueryResult"",""fields""
and almost all columns looks like
AVAILABLE: OPTIONAL INT64 R:0 D:1
row group 1:                    RC:3640100 TS:36454739 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:172743 SZ:370515/466690/1.26 VC:3640100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 126518400000, max: 1577692800000, num_nulls: 2541633]
processed:
creator:                        parquet-mr version 1.5.0-cdh5.12.0 (build ${buildNumber}) 
extra:                          org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields""
AVAILABLE:                      OPTIONAL INT64 R:0 D:1
...
row group 1:                    RC:6660100 TS:243047789 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:4122795 SZ:4283114/4690840/1.10 VC:6660100 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE ST:[min: -2209136400000, max: 10413820800000, num_nulls: 4444993]
In other hand, without repartition or using coalesce - size remains close to ingest data size.
Going forward, I did following:
read dataset and write it back with   
productDF
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithoutshuffle"")
read dataset, repartition and write it back with 
productDF
  .repartition(500)
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithshuffle"")
As result: 80 GB without and  283 GB with repartition with same # of output files
80GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:456753 SZ:1452623/1452623/1.00 VC:11000100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -1735747200000, max: 2524550400000, num_nulls: 7929352]
283 GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:2800387 SZ:2593838/2593838/1.00 VC:3510100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -2209136400000, max: 10413820800000, num_nulls: 2244255]
It seems, that parquet itself (with encoding?) much reduce size of data even without uncompressed data. How ? :)
I tried to read  uncompressed 80GB, repartition and write back - I've got my 283 GB
The first question for me is why I'm getting bigger size after spark repartitioning/shuffle?
The second is how to efficiently shuffle data in spark to benefit parquet encoding/compression if there any?
In general, I don't want that my data size growing after spark processing, even if I didn't change anything.
Also, I failed to find, is there any configurable compression rate for snappy, e.g. -1 ... -9? As I know, gzip has this, but what is the way to control this rate in Spark/Parquet writer?
Appreciate for any help!
Thanks!
",<apache-spark><apache-spark-sql><parquet><snappy>,3982,0,36,1223,1,12,16,53,15878,0.0,15,2,15,2018-02-18 1:43,2019-06-04 20:54,,471.0,,Intermediate,23
60636473,How to read/write Timestamp in Doobie (Postgres),"How to read/write Timestamp in Doobie?
I have a record class that contains a timestamp field. When I am trying to write it to the database or read it using doobie I get an error Cannot find or construct a Read instance for type.
case class ExampleRecord(data: String, created_at: Timestamp)
val create = sql""create table if not exists example_ts (data TEXT NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP)"".update.run
val insert = Update[ExampleRecord](""insert into example_ts (data, created_at) values (?, ?)"")
  .updateMany(List(
    ExampleRecord(""one"", Timestamp.valueOf(LocalDateTime.now())),
    ExampleRecord(""two"", Timestamp.valueOf(LocalDateTime.now()))
  ))
val select = sql""select data, created_at from example_ts"".query[ExampleRecord].stream
val app = for {
  _ &lt;- create.transact(xa).compile.drain
  _ &lt;- insert.transact(xa).compile.drain
  _ &lt;- select.transact(xa).compile.drain
} yield ()
app.unsafeRunSync()
",<postgresql><scala><doobie>,957,0,18,2021,0,16,26,75,5582,0.0,50,1,15,2020-03-11 12:51,2020-03-11 12:51,2020-03-11 12:51,0.0,0.0,Basic,3
55347251,Cannot select where ip=inet_pton($ip),"I have a unique column in database which is named ip
IP addresses are stored in this column as BINARY(16) (with no collation) after converting them using the PHP function 
$store_ip = inet_pton($ip);
When I try to insert the same IP twice it works fine and fails because it is unique,
But when I try to select the IP it doesn't work and always returns FALSE (not found)
&lt;?php
try {
    $ip = inet_pton($_SERVER['REMOTE_ADDR']);
    $stmt = $db-&gt;prepare(""SELECT * FROM `votes` WHERE ip=?"");
    $stmt-&gt;execute([$ip]);
    $get = $stmt-&gt;fetch();
    if( ! $get){
        echo 'Not found';
    }else{
        echo 'Found';
    }
    // close connection
    $get = null;
    $stmt = null;
} catch (PDOException $e) {
    error_log($e-&gt;getMessage());
}
The part where I insert the IP:
&lt;?php
if( ! filter_var($ip, FILTER_VALIDATE_IP)){
        return FALSE;
}
$ip = inet_pton($_SERVER['REMOTE_ADDR']);
try {
    $stmt = $db-&gt;prepare(""INSERT INTO votes(ip, answer) VALUES(?,?)"");
    $stmt-&gt;execute([$ip, $answer]);
    $stmt = null;
} catch (PDOException $e) {
    return FALSE;
}
",<php><mysql><pdo>,1099,0,39,832,1,15,34,57,1700,0.0,198,3,15,2019-03-25 22:21,2019-03-30 20:50,2019-03-30 20:50,5.0,5.0,Basic,1
54351783,duplicate key value violates unique constraint - postgres error when trying to create sql table from dask dataframe,"Following on from this question, when I try to create a postgresql table from a dask.dataframe with more than one partition I get the following error:
IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""pg_type_typname_nsp_index""
DETAIL:  Key (typname, typnamespace)=(test1, 2200) already exists.
 [SQL: '\nCREATE TABLE test1 (\n\t""A"" BIGINT, \n\t""B"" BIGINT, \n\t""C"" BIGINT, \n\t""D"" BIGINT, \n\t""E"" BIGINT, \n\t""F"" BIGINT, \n\t""G"" BIGINT, \n\t""H"" BIGINT, \n\t""I"" BIGINT, \n\t""J"" BIGINT, \n\tidx BIGINT\n)\n\n']
You can recreate the error with the following code:
import numpy as np
import dask.dataframe as dd
import dask
import pandas as pd
import sqlalchemy_utils as sqla_utils
import sqlalchemy as sqla
DATABASE_CONFIG = {
    'driver': '',
    'host': '',
    'user': '',
    'password': '',
    'port': 5432,
}
DBNAME = 'dask'
url = '{driver}://{user}:{password}@{host}:{port}/'.format(
        **DATABASE_CONFIG)
db_url = url.rstrip('/') + '/' + DBNAME
# create db if non-existent
if not sqla_utils.database_exists(db_url):
    print('Creating database \'{}\''.format(DBNAME))
    sqla_utils.create_database(db_url)
conn = sqla.create_engine(db_url)
# create pandas df with random numbers
df = pd.DataFrame(np.random.randint(0,40,size=(100, 10)), columns=list('ABCDEFGHIJ'))
# add index so that it can be used as primary key later on
df['idx'] = df.index
# create dask df
ddf = dd.from_pandas(df, npartitions=4)
# Write to psql
dto_sql = dask.delayed(pd.DataFrame.to_sql)
out = [dto_sql(d, 'test', db_url, if_exists='append', index=False, index_label='idx')
       for d in ddf.to_delayed()]
dask.compute(*out)
The code doesn't produce an error if npartitions is set to 1. So I'm guessing it has to do with postgres not being able to handle parallel requests to write to a same sql table...? How can I fix this?
",<python><postgresql><pandas><dask><pandas-to-sql>,1856,1,36,2367,2,30,59,52,27798,0.0,175,3,15,2019-01-24 16:56,2019-03-21 17:39,2019-03-21 17:39,56.0,56.0,Basic,9
55066509,Error in phpmyadmin Warning in ./libraries/plugin_interface.lib.php#551,"Error:
  Warning in ./libraries/plugin_interface.lib.php#551 count(): Parameter
  must be an array or an object that implements Countable
Backtrace:
./libraries/display_export.lib.php#381: PMA_pluginGetOptions(
string 'Export',
array,
)
./libraries/display_export.lib.php#883: PMA_getHtmlForExportOptionsFormat(array)
./libraries/display_export.lib.php#1099: PMA_getHtmlForExportOptions(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
array,
integer 0,
)
./tbl_export.php#143: PMA_getExportDisplay(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
integer 0,
string '',
)
How can I fix it?
",<mysql><sql><phpmyadmin>,664,0,23,153,1,1,7,55,20468,0.0,1,3,15,2019-03-08 15:40,2019-03-08 15:50,2019-12-08 20:41,0.0,275.0,Basic,13
48999379,"psycopg2.OperationalError: FATAL: password authentication failed for user ""<my UNIX user>""","I am a fairly new to web developement.
First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it.
It works well locally with PostgreSQL but I can't make it work on my server.
It seems like it tries to connect to Postgres with my Unix user. 
Why would my server try to do that?
I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations:
Settings.py + python-decouple:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
This is the error message I get each time I try to ./manage.py migrate
'myportfolio' is my Unix user name, the database username is different:
Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
I tried to: 
delete my django code, re install 
delete/purge postgres and reinstall
modify pg_hba.conf local to trust
At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?
",<python><django><postgresql><ubuntu-16.04>,5269,0,67,1560,4,18,27,42,48293,0.0,137,8,15,2018-02-27 0:41,2018-02-27 1:47,2018-02-27 12:36,0.0,0.0,Basic,13
64905397,Sequelize missing FROM-clause entry for table Postgres,"I am trying to filter my DataRow objects by where the included DataPoints data is loc and sortOrder is 2. Below is the query I am trying. I keep getting the following error.
SequelizeDatabaseError: missing FROM-clause entry for table &quot;dataPoints&quot;
I have tried setting required: true and duplicating: false with no luck. I tried these in both the DataRows include as well as DataPoints.
    attributes: ['id', 'name', 'labId'],
    include: [{
      as: 'dataColumns',
      model: DataColumn,
      attributes: ['id', 'name', 'sortOrder', 'dataType', 'isDate', 'isLocation'],
    }, {
      as: 'dataSets',
      model: Study,
      attributes: ['id', 'name'],
    }, {
      as: 'dataRows',
      model: DataRow,
      attributes: ['id'],
      where: {
        [Op.and]: [
          {
            '$dataPoints.data$': 'loc',
          },
          {
            '$dataPoints.sortOrder$': 2,
          },
        ],
      },
      include: [{
        as: 'dataPoints',
        model: DataPoint,
        attributes: ['id', 'sortOrder', 'data'],
      }],
    },],
    order: [
      [{ model: DataColumn, as: 'dataColumns' }, 'sortOrder', 'ASC'],
      [{ model: DataRow, as: 'dataRows' }, { model: DataPoint, as: 'dataPoints' }, 'sortOrder', 'ASC'],
    ],
  }
",<node.js><database><postgresql><sequelize.js>,1272,0,39,183,1,1,5,53,14105,,0,5,15,2020-11-19 4:23,2021-06-24 9:00,,217.0,,Basic,10
50388490,Analyze SQL query in DBeaver,"I would like to get some info about a running query to analyze it. In PgAdmin 3 I could at least use 'Explain Query', while in DBeaver, after clicking 'Explain Execution Plan', nothing happens.
How to obtain any information about query in DBeaver?
@Edit
Sorry if this question seems too broad. I don't expect explanation of how to analyze the query, I just would like to know if it is possible to open an analyzer in DBeaver and how to do it.
",<sql><postgresql><dbeaver>,443,0,0,5196,7,50,85,41,46892,,454,2,15,2018-05-17 9:53,2021-01-26 7:10,,985.0,,Basic,3
52589849,"""Create database if not exists"" in postgres","I am new to postgreSQL and I am trying to create a schema file which will contain all the scripts required to create the database and required tables. The way I used to do this for SQl server was to check if the database exists and then run the necessary scripts. 
The following script in postgreSQL throws an error saying, ""CREATE DATABASE cannot be executed from a function or multi-command string""
do $$
begin
If not exists (select 1 from pg_database where datname = 'TestDB')
Then
 CREATE DATABASE ""TestDB"";
end if;
end
$$
I created a postgres database dump file by exporting a backup of the database but that contains,
Drop Database ""TestDB""
Create Database ""TestDB""
which means everytime I run the schema file, the database would be dropped and recreated and it would be a problem if there is data present in the database.
How do I check if the database exists in postgreSQL without having to drop the database and recreate it everytime I run the file please? 
Thanks in Advance 
",<postgresql><postgresql-9.3><pgadmin-4>,986,0,11,313,1,4,21,79,23684,,15,1,15,2018-10-01 11:11,2020-12-16 11:15,,807.0,,Basic,9
57059458,SQL: Most Overdue pair of numbers?,"We have a this table and random data load: 
CREATE TABLE [dbo].[webscrape](
    [id] [int] IDENTITY(1,1) NOT NULL,
    [date] [date] NULL,
    [value1] [int] NULL,
    [value2] [int] NULL,
    [value3] [int] NULL,
    [value4] [int] NULL,
    [value5] [int] NULL,
    [sumnumbers] AS ([value1]+[value2]+[value3]+[value4]+[value5])
) ON [PRIMARY]
declare @date date = '1990-01-01',
@endDate date = Getdate()
while @date&lt;=@enddate
begin
insert into [dbo].[webscrape](date,value1,value2,value3,value4,value5)
SELECT @date date,FLOOR(RAND()*(36-1)+1) value1,
FLOOR(RAND()*(36-1)+1) value2,
FLOOR(RAND()*(36-1)+1) value3,
FLOOR(RAND()*(36-1)+1) value4,
FLOOR(RAND()*(36-1)+1) value5
set @date = DATEADD(day,1,@date)
end
select * from [dbo].[webscrape] 
In SQL how can we return pair of values that have gone the longest without occurring on a given date?
And (if you happen to know) in Power BI Q&amp;A NLP, how do we map so that so we can ask in natural language ""when have the most overdue pairs occurred?""
Overdue being the pair of numbers with the longest stretch of time since occurring as of the given date. 
UPDATE:  I am trying this very ugly code.  Any ideas: 
  select *
    from (
      select date,value1 number1,value2 number2 from webscrape union all  
      select date,value1,value3 from webscrape union all
      select date,value1,value4 from webscrape union all
      select date,value1,value5 from webscrape union all
      select date,value2,value3 from webscrape union all
      select date,value2,value4 from webscrape union all
      select date,value2,value5 from webscrape union all
      select date,value3,value4 from webscrape union all
      select date,value3,value5 from webscrape union all
      select date,value4,value5 from webscrape 
    ) t order by date
    ----------------------------------
    select t.number1,t.number2, count(*)
     as counter
    from (
      select value1 number1,value2 number2 from webscrape union all  
      select value1,value3 from webscrape union all
      select value1,value4  from webscrape union all
      select value1,value5 from webscrape union all
      select value2,value3 from webscrape union all
      select value2,value4  from webscrape union all
      select value2,value5 from webscrape union all
      select value3,value4  from webscrape union all
      select value3,value5 from webscrape union all
      select value4,value5 from webscrape 
    ) t
group by t.number1,number2
order by counter
Thanks for any help.
",<sql><sql-server><t-sql><nlp><powerbi>,2503,0,63,1667,9,38,73,44,425,0.0,82,1,15,2019-07-16 14:20,2019-07-19 18:09,2019-07-19 18:09,3.0,3.0,Basic,9
54515875,How can I update a table to insert decimal points at a fixed position in numbers?,"I am using Microsoft SQL Server 2014 and have a table with three columns and the field data type is Decimal(38,0).
I want to update each row of my table to insert a decimal point after the first two digits. For example, I want 123456 to become 12.3456. The numbers are different lengths; some are five digits, some are seven digits, etc.
My table is:
+-------------+-------+-------+
| ID          |   X   |   Y   |
+-------------+-------+-------+
| 1200        | 321121| 345000|
| 1201        | 564777| 4145  |
| 1202        | 4567  | 121444|
| 1203        | 12747 | 789887|
| 1204        | 489899| 124778|
+-------------+-------+-------+
And I want to change this to:
+-------------+--------+--------+
| ID          |   X    |   Y    |
+-------------+--------+--------+
| 1200        | 32.1121| 34.5000|
| 1201        | 56.4777| 41.45  |
| 1202        | 45.67  | 12.1444|
| 1203        | 12.747 | 78.9887|
| 1204        | 48.9899| 12.4778|
+-------------+--------+--------+
My code is:
Update [dbo].[UTM]
     SET [X] = STUFF([X],3,0,'.')
         [Y] = STUFF([X],3,0,'.')
And I tried this:
BEGIN
DECLARE @COUNT1 int;
DECLARE @COUNT2 int;
DECLARE @TEMP_X VARCHAR(255);
DECLARE @TEMP_Y VARCHAR(255);
DECLARE @TEMP_main VARCHAR(255);
SELECT @COUNT1 = COUNT(*) FROM [UTM];
SET @COUNT2 = 0;
    WHILE(@COUNT2&lt;@COUNT1)
    BEGIN
        SET @TEMP_main = (SELECT [id] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_X = (SELECT [X] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_Y = (SELECT [Y] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        UPDATE [dbo].[UTM]
           SET [X] = CONVERT(decimal(38,0),STUFF(@TEMP_X,3,0,'.'))
              ,[Y] = CONVERT(decimal(38,0),STUFF(@TEMP_Y,3,0,'.'))
           WHERE [id] = @TEMP_main;
        SET @COUNT2 = @COUNT2  +  1
    END
END
",<sql><sql-server><numbers><decimal><sql-server-2014>,1904,0,49,492,1,6,21,74,4952,0.0,126,4,15,2019-02-04 12:13,2019-02-04 12:15,2019-02-04 12:32,0.0,0.0,Basic,9
55760416,In OLAP cube wrong Grand Total when attribute is filtered,"A user trying to check the Sales Amount per Salesperson. Sample data:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750
Grand Total:          2250
It looks fine, but we have the following hierarchy Company &gt; Class &gt; Group &gt; Subgroup in the cube and if a user tries to use this hierarchy in filters - Grand Total fails (if any attribute is unchecked in this hierarchy). Sample:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
I've noticed the same problem before when we tried to filter Date attribute, if not every day of the month was selected it shown wrong Grand Total too.
Have you an idea why it happens and how to fix it?
Sales Amount is physical measure (not calculated measure), it is selected from SQL view (the same happens with every fact). 
I've asked the same question here, but nobody could answer it.
I've tried to delete all MDX calculations (scopes), but still Grand Total was incorrect. 
EDIT
I've noticed that the problem occurs when filtering like that:
1 element selected from the first level of the hierarchy, 1 element from 2nd level and 1 element from the 3rd level of hierarchy as in the image above.
If the 3rd level isn't filtered it shows good Grand Total.
EDIT 2
I've tried to trace on SSAS, it returns exactly the same output as in Excel. It generated the following MDX when using Salesperson dimension on the rows:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS, 
NON EMPTY { ([Salesperson].[Salesperson].[Salesperson].ALLMEMBERS ) } 
DIMENSION PROPERTIES MEMBER_CAPTION, 
MEMBER_UNIQUE_NAME ON ROWS FROM ( 
SELECT ( {  [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
            [Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
            [Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } 
) ON COLUMNS FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) 
WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
This MDX generated without Salesperson dimension:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS 
FROM ( SELECT ( { [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
[Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
[Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) ON COLUMNS 
FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
I've noticed even if I'm not using any dimension on the rows (in samples above I've used Salesperson dimension) it shows wrong Grand Total.
For example it shows:
Sales Amount 
350
And when using Salesperson dimension on the rows:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
",<sql-server><excel><sql-server-2008-r2><pivot-table><ssas>,3329,5,34,848,4,15,42,74,2853,0.0,68,1,15,2019-04-19 10:27,2019-05-03 11:13,,14.0,,Advanced,32
64065118,What extra one gets by selecting Azure SQL Managed Instance vis-a-vis Azure SQL DB PaaS,"I would like to know what extra benefits one get by choosing Azure SQL Managed Instance compared to Azure SQL DB PaaS. I know SQL Managed Instance is offered as a vCore based purchasing model only. Apart from this what is the extra add on and benefits that one gets over the other. Any reply would be appreciated.
",<azure><azure-sql-database><azure-sql-managed-instance>,314,0,0,1967,2,24,46,45,7678,,0,3,15,2020-09-25 13:29,2020-09-26 17:43,,1.0,,Basic,3
60989709,How to convert a class instance to JsonDocument?,"Let's say we have an entity class that looks like this:
public class SerializedEntity
{
    public JsonDocument Payload { get; set; }
    public SerializedEntity(JsonDocument payload)
    {
        Payload = payload;
    }
}
According to npsql this generates a table with column payload of type jsonb for this class which is correct. 
Now what I would like to do is take any class instance and store it as payload in this table e.g.: 
public class Pizza {
    public string Name { get; set; }
    public int Size { get; set; }
}
should then be possible to be retrieved as an object with following structure:
{Name: ""name"", Size: 10}
So I need something like this:
var pizza = new Pizza(""Margharita"", 10);
var se = new SerializedEntity(someConverter.method(pizza))
",<c#><entity-framework><asp.net-core><npgsql>,764,1,19,2506,2,26,39,66,11557,0.0,257,5,15,2020-04-02 10:43,2020-04-02 11:24,2020-04-02 11:24,0.0,0.0,Basic,6
54257933,Calling flush() in @Transactional method in Spring Boot application,"Is it possible that calling Hibernate flush() in the middle of @Transactional method will save incomplete results in the database?
For example, is it possible that this function could save ""John"" to the database?
@Transactional
public void tt() {
    Student s = new Student(""John"");
    em.persist(s);
    em.flush();
    // Perform some calculations that change some attributes of the instance
    s.setName(""Jeff"");
}
I tried it with H2 in-memory database and it didn't save incomplete transaction changes. But is it possible under certain conditions and maybe with another DB engine?
",<java><sql><spring><hibernate><jpa>,588,0,8,501,1,6,29,55,26628,0.0,146,1,15,2019-01-18 16:28,2019-01-18 17:17,2019-01-18 17:17,0.0,0.0,Basic,9
48069425,Converting Between Timezones in Postgres,"I am trying to understand the timestamps and timezones in Postgre. I think I got it, until I red this article.  Focus on the ""Converting Between Timezones"" part. It has two examples. 
(Consider the default timezone configuration to be UTC.)
Example 1
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'); outputs 2015-12-31 16:00:00
According to the article and what I understand, because the '2016-01-01 00:00' part of the timezone function is just a string, it is silently converted to the default UTC. So from '2016-01-01 00:00' UTC it is then converted to US/Pacific as asked by the timezone function, that is 2015-12-31 16:00:00.  
Example 2 
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'::timestamp); outputs 2016-01-01 08:00:00+00
Excuse me, I dont see why and the explanation there does not help. Ok, the '2016-01-01 00:00'::timestamp part of the timezone function is no longer a string, but an actual timestamp. In what timezone? If it is UTC, the output would have to be the same as the Example 1. So it is automatically converted to US/Pacific? Then the output is in UTC? But why? I asked for a US/Pacific in my timezone  not a UTC.
Please explain how the timezone behaves when gets a timestamp and gets asked to transform it. Thank you.
",<postgresql><timezone><timestamp><timezone-offset><timestamp-with-timezone>,1259,1,14,4312,20,69,130,52,33126,0.0,468,2,15,2018-01-02 23:51,2018-01-03 17:46,2018-01-16 8:08,1.0,14.0,Basic,9
53591359,postgresql: Filter in JSON array,"Let's say we have a table items which has columns name and attributes:
CREATE TABLE students (
  name VARCHAR(100),
  attributes JSON
)
where attributes is an array of (always equally-structured) JSON documents such as
[{""name"":""Attribute 1"",""value"":""Value 1""},{""name"":""Attribute 2"",""value"":""Value 2""}]
I now want to find all students where any attribute value matches something (such as Foo%). Here's a playground example.
I realize that this isn't exactly the most straight-forward design, but for now it's what I have to work with, though performance of such a search being categorically terribly inefficient would of course be a valid concern.
",<sql><postgresql>,648,1,9,19523,7,68,100,73,37808,0.0,1059,3,15,2018-12-03 10:02,2018-12-03 10:43,2018-12-03 10:43,0.0,0.0,Basic,9
49971903,Converting epoch to datetime in PySpark data frame using udf,"I have a PySpark dataframe with this schema:
root
 |-- epoch: double (nullable = true)
 |-- var1: double (nullable = true)
 |-- var2: double (nullable = true)
Where epoch is in seconds and should be converted to date time. In order to do so, I define a user defined function (udf) as follows:
from pyspark.sql.functions import udf    
import time
def epoch_to_datetime(x):
    return time.localtime(x)
    # return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x))
    # return x * 0 + 1
epoch_to_datetime_udf = udf(epoch_to_datetime, DoubleType())
df.withColumn(""datetime"", epoch_to_datetime(df2.epoch)).show()
I get this error:
---&gt; 21     return time.localtime(x)
    22     # return x * 0 + 1
    23 
    TypeError: a float is required
If I simply return x + 1 in the function, it works. Trying float(x) or float(str(x)) or numpy.float(x) in time.localtime(x) does not help and I still get an error. Outside of udf, time.localtime(1.514687216E9) or other numbers works fine. Using datetime package to convert epoch to datetim results in similar errors. 
It seems that time and datetime packages do not like to fed with DoubleType from PySpark. Any ideas how I can solve this issue? Thanks.
",<python><apache-spark><pyspark><apache-spark-sql>,1201,0,29,1350,3,17,31,66,38149,0.0,506,3,15,2018-04-23 0:14,2018-04-23 3:07,2018-04-23 3:07,0.0,0.0,Basic,9
57546833,How to take a dump from Mysql 8.0 into 5.7?,"I would like to take a dump from Mysql 8.0.11 and restore it into 5.7.27.
When I tried to restore it I got the error:
  ERROR 1273 (HY000) at line 25: Unknown collation: 'utf8mb4_0900_ai_ci'
Then I tried to use the compatible flag to make it easier on an older MySQL DB.
mysqldump --compatible=mysql4 --add-drop-table -u r00t -h xxx.eu-north-1.rds.amazonaws.com -p radius_db &gt; ~/radius.sql
But that doesn't seem to work either:
  mysqldump: Couldn't execute '/*!40100 SET @@SQL_MODE='MYSQL40' */':
  Variable 'sql_mode' can't be set to the value of 'MYSQL40' (1231)
Any advice would be appreciated.
",<mysql>,602,0,1,64585,90,280,468,69,15424,0.0,2772,2,15,2019-08-18 17:26,2019-08-22 10:35,2019-08-22 10:35,4.0,4.0,Basic,9
54665992,"Error ""Could not find any index named [IX_MyIndex]"" upon creating it","I came before this very weird error:
  Msg 7999, Level 16, State 9, Line 12
  Could not find any index named 'IX_MyIndex' for table 'dbo.MyTable'.
When running the script to create it!!
CREATE NONCLUSTERED INDEX [IX_MyIndex] ON [dbo].[MyTable] (
    [Field1]
    ,[Field2]
    ) INCLUDE (
    Fields3
    ,Fields4
    ,Fields5
    )
    WITH (
         MAXDOP = 4
         ,DATA_COMPRESSION = PAGE
         ,DROP_EXISTING = ON
        )
What am I missing?
",<sql-server>,456,0,13,3111,4,31,60,53,6255,0.0,363,2,15,2019-02-13 8:52,2019-02-13 8:58,2019-02-13 8:58,0.0,0.0,Basic,9
58976719,How to convert string column type to integer using Laravel migration?,"I am trying to alter the column datatype using Laravel migration. But I am facing following error. Please help me out.
Schema::table('files', function(Blueprint $table) {
    $table-&gt;integer('app_id')-&gt;change();
    $table-&gt;index(['app_id', 'filename']);
});
  SQLSTATE[42000]: Syntax error or access violation: 1064 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci' at line 1 (SQL: ALTER TABLE files CHANGE app_id app_id INT CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci) 
",<php><mysql><laravel>,647,0,6,183,1,2,6,71,11810,,1,2,15,2019-11-21 14:00,2019-11-25 12:42,2019-11-25 12:42,4.0,4.0,Basic,9
53537229,MSSQL at Docker exits instantly,"I'm trying to setup MSSQL at Docker at Windows 10, but for some reason it started shutting down my container
I've been using it like that for months, but now I have no idea what's happening
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
    C:\Users\user\
    λ docker login
    Authenticating with existing credentials...
    Login Succeeded
    C:\Users\user\
    λ docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=&lt;YourStrong!Passw0rd&gt;123' -p 1433:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest
    337e5efb35f0bf4b465181a0f8be4851b12f353a3a8710ddf817d2f501e5fea
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   3 seconds ago       Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                     PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   6 seconds ago       Exited (1) 2 seconds ago                       sql
    C:\Users\user\
    λ docker start sql
    sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   14 seconds ago      Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                    PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   16 seconds ago      Exited (1) 1 second ago                       sql
  docker logs sql
shows
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
Anybody has an idea what may be wrong?
",<sql-server>,3024,2,36,1821,6,23,65,80,13734,0.0,141,1,15,2018-11-29 10:47,2018-11-29 13:52,2018-11-29 13:52,0.0,0.0,Basic,9
58597639,How to make multiple query in a very short interval / simultaneously,"Hey I'm getting an error message : conn busy from pgx
I don't know how to solve this. Here is my function :
func (r *proverbRepo) SelectPendingProverbs(table string) (proverbs []domain.Proverb, err error) {
    query := fmt.Sprintf(""SELECT id, proverb literal FROM %s"", table)
    rows, err := r.Db.Query(context.Background(), query)
    defer rows.Close()
    if err != nil {
        return
    }
    for rows.Next() {
        var prov domain.Proverb
        if err = rows.Scan(&amp;prov.ID, &amp;prov.Literal); err != nil {
            return
        }
        proverbs = append(proverbs, prov)
    }
    return
}
r.Db is pgx.Connect(context.Background(), os.Getenv(""PSQL_URL""))
I'm fetching two different table in a very short interval from two separate front end requests.
The first request goes through, the other one returns the conn busy error message.
I really don't know what to look for, would somebody help me ?
",<go><psql><pgx>,923,0,22,3691,5,22,38,36,9220,0.0,63,2,15,2019-10-28 20:01,2019-10-28 21:28,2019-10-28 21:28,0.0,0.0,Intermediate,23
50885572,UseSqlite with Entity Framework Core in ASP.NET Core 2.1 not working,"I am starting a Razor pages project in ASP.NET Core 2.1. I am trying to use SQLite but when configuring the database only SQL Server seems to be an option.
Startup.cs
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Application.Models;
using Microsoft.EntityFrameworkCore;
namespace Application
{
    public class Startup
    {
        public Startup(IConfiguration configuration)
        {
            Configuration = configuration;
        }
        public IConfiguration Configuration { get; }
        // This method gets called by the runtime. Use this method to add services to the container.
        public void ConfigureServices(IServiceCollection services)
        {
            services.AddDbContext&lt;ApplicationContext&gt;(options =&gt;
               options.UseSqlite(""Data Source=Database.db""));
            services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
        public void Configure(IApplicationBuilder app, IHostingEnvironment env)
        {
            app.UseStaticFiles();
            app.UseMvc();
        }
    }
}
Intellisense does not recognize options.UseSqlite and builds fail. This was not/ is not an issue with .net core 2.0 projects.
Is this not supported yet? Reading through the documentation makes it seem that it is. I'm not sure what else is going wrong here.
",<c#><sqlite><entity-framework-core>,1593,0,36,467,1,5,20,49,32447,0.0,21,3,15,2018-06-16 6:37,2018-06-16 7:05,2018-06-16 7:05,0.0,0.0,Basic,14
56265904,Reading UUID from result set in Postgres JDBC,"I'm using UUID for my id column, I'm looking for a way to retrieve the data inside my Java application. I can't find a  method in ResultSet for getting a UUID. How would I go about getting the UUID?
",<postgresql><jdbc>,199,0,0,8557,27,108,176,64,8613,0.0,554,1,15,2019-05-22 22:44,2019-05-23 3:37,2019-05-23 3:37,1.0,1.0,Basic,3
60797825,Why mysql explain analyze is not working?,"Besides having mariadb 10.1.36-MariaDB I get following error.
EXPLAIN ANALYZE select 1
MySQL said: Documentation
1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'ANALYZE select 1' at line 1
What additional I need to do here. My PHP version is 7.2.11.
",<php><mysql><mariadb><query-optimization>,346,0,4,153,0,1,4,48,9362,,0,2,15,2020-03-22 9:44,2020-03-22 9:50,2020-03-22 9:50,0.0,0.0,Basic,14
64974214,AWS RDS - Access denied to admin user when using GRANT ALL PRIVILEGES ON the_db.* TO 'the_user'@'%',"When we try to GRANT ALL permissions to a user for a specific database, the admin (superuser) user of database receives the following error.
Access denied for user 'admin'@'%' to database 'the_Db'
After looking other questions in stackoverflow I could not find the solution. I already tried to change * -&gt; % without success, that is the approach suggested in the following source:
http://www.fidian.com/problems-only-tyler-has/using-grant-all-with-amazons-mysql-rds
I think there is an underlying configuration on RDS so I can't grant all permissions for the users, but I don't know how to detect what is happening.
Update
After doing some workarounds I noticed that the &quot;Delete versioning rows&quot; permissions is the one that causes the problem. I can add all permissions but that one.
https://mariadb.com/kb/en/grant/
So the only &quot;way&quot; I could grant other permissions was to specific each one of those with a script like this.
GRANT Alter ON *.* TO 'user_some_app'@'%';
GRANT Create ON *.* TO 'user_some_app'@'%';  
GRANT Create view ON *.* TO 'user_some_app'@'%';
GRANT Delete ON *.* TO 'user_some_app'@'%';
GRANT Drop ON *.* TO 'user_some_app'@'%';
GRANT Grant option ON *.* TO 'user_some_app'@'%';
GRANT Index ON *.* TO 'user_some_app'@'%';
GRANT Insert ON *.* TO 'user_some_app'@'%';
GRANT References ON *.* TO 'user_some_app'@'%';
GRANT Select ON *.* TO 'user_some_app'@'%';
GRANT Show view ON *.* TO 'user_some_app'@'%';
GRANT Trigger ON *.* TO 'user_some_app'@'%';
GRANT Update ON *.* TO 'user_some_app'@'%';
GRANT Alter routine ON *.* TO 'user_some_app'@'%';
GRANT Create routine ON *.* TO 'user_some_app'@'%';
GRANT Create temporary tables ON *.* TO 'user_some_app'@'%';
GRANT Execute ON *.* TO 'user_some_app'@'%';
GRANT Lock tables ON *.* TO 'user_some_app'@'%';
",<mysql><amazon-web-services><mariadb><amazon-rds>,1796,4,19,1394,2,12,16,49,14713,0.0,106,1,15,2020-11-23 18:28,2021-03-02 11:30,2021-03-02 11:30,99.0,99.0,Basic,14
48206047,How to return all the columns with flask-sqlalchemy query join from two tables,"I'm trying to do a join from two tables in flask-sqlalchemy and I want all the columns from both tables but if I execute:
Company.query.join(Buyer, Buyer.buyer_id == Company.id).all()
I have only the columns from Company (It returns, in fact, a Company object). 
I know I can do something like:
Company.query.join(Buyer, Buyer.buyer_id == Company.id) \
             .add_columns(Buyer.id, Buyer.name, etc..).all()
It returns in this case:
(&lt;Company 2&gt;, 1, 'S67FG', etc..)
the problem is that I have a lot of columns and also I don't know how to marshmallow the returned obj with flask-marshmallow (with nested fields does not work).
Is there a way to return a new obj with columns from the two tables?
What is for you the best way to manage these situations?
Any suggestion is highly appreciated. Thanks
",<python><sqlalchemy>,810,0,4,713,3,6,20,66,10857,0.0,78,1,15,2018-01-11 11:31,2018-01-11 11:47,2018-01-11 11:47,0.0,0.0,Basic,9
59249332,How to solve 'Cannot authenticate using Kerberos' issue doing EF Core database scaffolding in Linux(Ubuntu 18.04)? Are there any solutions?,"I've been trying to develop a simple AspNetCore application with EntityFrameworkCore to connect and work with the MSSQL server database. And manage all this by Rider IDE, a tool for Database client (DBeaver) and dotnet command line interface(dotnet ef). I'm using the database first approach(create a database on the MSSQL server, fill it with tables, and then build Models based on tables).
My STEP-by-STEP actions:
1)install and set up MSSQL server for my machine working on Ubuntu 18.04. Install the command line tool &quot;SQLCMD&quot;. ///
Link to guide - https://learn.microsoft.com/en-gb/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-ver15
2)locally connected to my MSSQLServer instance.
sqlcmd -S localhost -U SA -P 'MyPasswd'
3)Using Transact-SQL created a Database and installed a DB client (DBeaver) to quickly manage my databases now and in the future.
The next step, as I supposed, was to use tutorials about connecting my Web Application to a database that was found here https://blog.jetbrains.com/dotnet/2017/08/09/running-entity-framework-core-commands-rider/ and here https://www.entityframeworktutorial.net/efcore/create-model-for-existing-database-in-ef-core.aspx
My ASP.NET Core project's package references:
Microsoft.EntityFrameworkCore
Microsoft.EntityFrameworkCore.SqlServer
Microsoft.EntityFrameworkCore.Tools
After typing in the CLI command
dotnet ef dbcontext scaffold &quot;Server=localhost;Database=WebAppDB;Integrated Security=true;&quot; Microsoft.EntityFrameworkCore.SqlServer -c RsvpContext (
to build &quot;RsvpContext&quot; context to connect to my database WebAppDB.)
I see what I see:
Build started...
Build succeeded.
Microsoft.Data.SqlClient.SqlException (0x80131904): **Cannot authenticate using 
Kerberos. Ensure Kerberos has been initialized on the client with 'kinit' and a 
Service Principal Name has been registered for the SQL Server to allow Kerberos 
authentication.**
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
   at Microsoft.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, SqlCredential credential, Object providerInfo, String newPassword, SecureString newSecurePassword, Boolean redirectedUserInstance, SqlConnectionString userConnectionOptions, SessionData reconnectSessionData, Boolean applyTransientFaultHandling, String accessToken, DbConnectionPool pool, SqlAuthenticationProviderManager sqlAuthProviderManager)
   at Microsoft.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, DbConnectionPoolKey poolKey, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnectionPool pool, DbConnection owningObject, DbConnectionOptions options, DbConnectionPoolKey poolKey, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
at Microsoft.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at Microsoft.Data.SqlClient.SqlConnection.Open()
   at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(DbConnection connection, DatabaseModelFactoryOptions options)
at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(String connectionString, DatabaseModelFactoryOptions options)
   at Microsoft.EntityFrameworkCore.Scaffolding.Internal.ReverseEngineerScaffolder.ScaffoldModel(String connectionString, DatabaseModelFactoryOptions databaseOptions, ModelReverseEngineerOptions modelOptions, ModelCodeGenerationOptions codeOptions)
   at Microsoft.EntityFrameworkCore.Design.Internal.DatabaseOperations.ScaffoldContext(String provider, String connectionString, String outputDir, String outputContextDir, String dbContextClassName, IEnumerable`1 schemas, IEnumerable`1 tables, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContextImpl(String provider, String connectionString, String outputDir, String outputDbContextDir, String dbContextClassName, IEnumerable`1 schemaFilters, IEnumerable`1 tableFilters, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContext.&lt;&gt;c__DisplayClass0_0.&lt;.ctor&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.&lt;&gt;c__DisplayClass3_0`1.&lt;Execute&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.Execute(Action action)
ClientConnectionId:38f805bc-5879-458b-9256-d6a201d7ce99
Cannot authenticate using Kerberos. Ensure Kerberos has been initialized on the 
client with 'kinit' and a Service Principal Name has been registered for the SQL 
Server to allow Kerberos authentication.
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
If someone, preferably working on Linux, had the same issue, please let me know and share your solutions(guide on what to do in this situation).
",<asp.net-mvc><ubuntu><kerberos><sql-server-2017><ef-core-3.0>,9027,6,78,155,1,1,8,59,15262,,31,2,15,2019-12-09 12:53,2020-06-19 9:45,2020-06-19 9:45,193.0,193.0,Basic,9
54030469,Host 'X' is not allowed to connect to this MySQL server,"I wanna deploy MySQL+PHPMyAdmin. My docker-compose.yml:
version: ""3""
services:
  db:
    image: mysql:5.7
    restart: always
    container_name: db
    volumes:
      - ./~mysql:/var/lib/mysql
      - ./mysql.cnf:/etc/mysql/conf.d/my.cnf
    environment:
      MYSQL_DATABASE: ""dbtest""
      MYSQL_ROOT_PASSWORD: ""123456""
      MYSQL_ROOT_HOST: ""%""
    networks:
      - db
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ""mysqladmin ping -h localhost""
      interval: 1s
      timeout: 1s
      retries: 60
  phpmyadmin:
    image: phpmyadmin/phpmyadmin:4.7
    restart: always
    container_name: phpmyadmin
    ports:
      - 8080:80
    networks:
      - external-net
      - db
    environment:
      PMA_HOST: db
    depends_on:
      - db
networks:
  external-net:
    external:
      name: external-net
  db:
    driver: bridge
After some time later I getting subject error. MYSQL_ROOT_HOST don't helped. When I trying to connect to mysql from db-container:
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
I really don't know what to do with this magic... Thx.
",<mysql><docker><docker-compose>,1154,0,43,365,2,3,12,35,36082,0.0,18,5,15,2019-01-03 22:09,2019-02-07 19:36,,35.0,,Basic,14
53952383,add condition to mysql json_arrayagg function,"I have a json query that gives me json of a joined table of person and pets:
SELECT json_object(
  'personId', p.id,
  'pets', json_arrayagg(json_object(
    'petId', pt.id,
    'petName', pt.name
  ))
  )
FROM person p LEFT JOIN pets pt
ON p.id = pt.person_id
GROUP BY p.id;
my issue is that person can have 0 or more pets, and when a person have 0 pets I get list with 1 empty pet, and what I would like to get in that case is empty list.
this is what I get:
{
  ""personId"": 1,
  ""pets"": [
    {
      ""petId"": null,
      ""petName"": """"
    }
  ]
}
and I need:
{
  ""personId"": 1,
  ""pets"": []
}
is that possible?
",<mysql>,615,0,23,1428,3,17,34,80,10295,,44,3,15,2018-12-28 0:32,2018-12-28 0:50,2018-12-28 0:50,0.0,0.0,Basic,2
49557144,How to extract values from a numeric-keyed nested JSON field in MySQL,"I have a MySQL table with a JSON column called sent. The entries in the column have information like below:
{
 ""data"": {
  ""12"":""1920293""
 }
}
I'm trying to use the mysql query:
select sent-&gt;""$.data.12"" from mytable
but I get an exception: 
Invalid JSON path expression. The error is around character position 9.
Any idea How I can extract the information? The query works fine for non-numeric subfields.
",<mysql><json><mysql-json>,408,0,8,1168,0,16,31,42,4434,0.0,539,1,15,2018-03-29 13:18,2018-03-29 15:06,2018-03-29 15:06,0.0,0.0,Basic,2
59437636,Sequelize find in JSON field,"I have PostgreSQL database with JSON type field named ""data"" that has following content structure:
{
  ""requestData"" : {
    ""url"": ""some url""
    ""body"": {
      ""page_id"": 12
    }
  }
}
I try to make findAll query request with filter by page_id using Sequelize, but don't get some results. 
The question is: could I search by nested field in JSON type, or only in JSONB type? And how?
",<javascript><json><postgresql><sequelize.js>,388,0,10,349,2,4,11,81,18023,,9,1,15,2019-12-21 15:49,2019-12-22 19:00,2019-12-22 19:00,1.0,1.0,Basic,3
53704187,Connecting to an Azure database using SQLAlchemy in Python,"I am trying to connect to an Azure database using SQLAlchemy in Python.
My code is the following:
engine_azure = \
create_engine('mssql+pyodbc://{Server admin login}:{password}@{Server name}.database.windows.net:1433/{AdventureWorksLT}', echo=True)
I get the following message:
C:\ProgramData\Anaconda3\lib\site-packages\sqlalchemy\connectors\pyodbc.py:92: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Then I run the following code:
print(engine_azure.table_names())
I get the following message:
DBAPIError: (pyodbc.Error) ('01S00', '[01S00] [Microsoft][ODBC Driver Manager] Invalid connection string attribute (0) (SQLDriverConnect)')
",<python><sql-server><azure><sqlalchemy>,718,1,6,4701,18,78,143,63,33052,0.0,35,6,15,2018-12-10 10:54,2018-12-11 2:33,,1.0,,Basic,3
48171611,difference between pandas read sql query and read sql table,"Is there a difference in relation to time execution between this two commands :
import pandas as pd
df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
Thank you for your help 
",<python><sql><pandas><dataframe><sqlite>,208,0,4,674,1,7,18,52,22574,0.0,9,4,15,2018-01-09 15:33,2018-01-09 15:49,2018-01-09 15:49,0.0,0.0,Intermediate,19
50069427,Create Unique constraint for 'true' only in EF Core,"I have a class for tracking attachments to a Record. Each Record can have multiple RecordAttachments, but there is a requirement that there can only be one RecordAttachment per-Record that is marked as IsPrimary. 
public class RecordAttachment
{
    public int Id { get; set; }
    public int RecordId { get; set; }
    public string Details { get; set; }
    public bool IsPrimary { get; set; }
    public Record Record { get; set; }
}
I can't just use .HasIndex(e =&gt; new { e.RecordId, e.IsPrimary }).IsUnique(true) because there can be multiple false values per Record.
Basically I need a unique constraint on RecordId and IsPrimary == true, although this didn't work:
entity.HasIndex(e =&gt; new { e.RecordId, IsPrimary = (e.IsPrimary == true) }).IsUnique(true)
Edit:
Looking at answers like this: Unique Constraint for Bit Column Allowing Only 1 True (1) Value it appears this would be possible creating the constraint directly with SQL, but then it wouldn't be reflected in my Model.
",<c#><sql-server><entity-framework-core>,992,1,15,3322,2,29,58,73,7529,0.0,341,1,15,2018-04-27 19:53,2018-04-27 21:07,2018-04-27 21:07,0.0,0.0,Intermediate,19
52603131,How to optimize partitioning when migrating data from JDBC source?,"I am trying to move data from a table in PostgreSQL table to a Hive table on HDFS. To do that, I came up with the following code:
  val conf  = new SparkConf().setAppName(""Spark-JDBC"").set(""spark.executor.heartbeatInterval"",""120s"").set(""spark.network.timeout"",""12000s"").set(""spark.sql.inMemoryColumnarStorage.compressed"", ""true"").set(""spark.sql.orc.filterPushdown"",""true"").set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"").set(""spark.kryoserializer.buffer.max"",""512m"").set(""spark.serializer"", classOf[org.apache.spark.serializer.KryoSerializer].getName).set(""spark.streaming.stopGracefullyOnShutdown"",""true"").set(""spark.yarn.driver.memoryOverhead"",""7168"").set(""spark.yarn.executor.memoryOverhead"",""7168"").set(""spark.sql.shuffle.partitions"", ""61"").set(""spark.default.parallelism"", ""60"").set(""spark.memory.storageFraction"",""0.5"").set(""spark.memory.fraction"",""0.6"").set(""spark.memory.offHeap.enabled"",""true"").set(""spark.memory.offHeap.size"",""16g"").set(""spark.dynamicAllocation.enabled"", ""false"").set(""spark.dynamicAllocation.enabled"",""true"").set(""spark.shuffle.service.enabled"",""true"")
  val spark = SparkSession.builder().config(conf).master(""yarn"").enableHiveSupport().config(""hive.exec.dynamic.partition"", ""true"").config(""hive.exec.dynamic.partition.mode"", ""nonstrict"").getOrCreate()
  def prepareFinalDF(splitColumns:List[String], textList: ListBuffer[String], allColumns:String, dataMapper:Map[String, String], partition_columns:Array[String], spark:SparkSession): DataFrame = {
        val colList                = allColumns.split("","").toList
        val (partCols, npartCols)  = colList.partition(p =&gt; partition_columns.contains(p.takeWhile(x =&gt; x != ' ')))
        val queryCols              = npartCols.mkString("","") + "", 0 as "" + flagCol + "","" + partCols.reverse.mkString("","")
        val execQuery              = s""select ${allColumns}, 0 as ${flagCol} from schema.tablename where period_year='2017' and period_num='12'""
        val yearDF                 = spark.read.format(""jdbc"").option(""url"", connectionUrl).option(""dbtable"", s""(${execQuery}) as year2017"")
                                                                      .option(""user"", devUserName).option(""password"", devPassword)
                                                                      .option(""partitionColumn"",""cast_id"")
                                                                      .option(""lowerBound"", 1).option(""upperBound"", 100000)
                                                                      .option(""numPartitions"",70).load()
        val totalCols:List[String] = splitColumns ++ textList
        val cdt                    = new ChangeDataTypes(totalCols, dataMapper)
        hiveDataTypes              = cdt.gpDetails()
        val fc                     = prepareHiveTableSchema(hiveDataTypes, partition_columns)
        val allColsOrdered         = yearDF.columns.diff(partition_columns) ++ partition_columns
        val allCols                = allColsOrdered.map(colname =&gt; org.apache.spark.sql.functions.col(colname))
        val resultDF               = yearDF.select(allCols:_*)
        val stringColumns          = resultDF.schema.fields.filter(x =&gt; x.dataType == StringType).map(s =&gt; s.name)
        val finalDF                = stringColumns.foldLeft(resultDF) {
          (tempDF, colName) =&gt; tempDF.withColumn(colName, regexp_replace(regexp_replace(col(colName), ""[\r\n]+"", "" ""), ""[\t]+"","" ""))
        }
        finalDF
  }
    val dataDF = prepareFinalDF(splitColumns, textList, allColumns, dataMapper, partition_columns, spark)
    val dataDFPart = dataDF.repartition(30)
    dataDFPart.createOrReplaceTempView(""preparedDF"")
    spark.sql(""set hive.exec.dynamic.partition.mode=nonstrict"")
    spark.sql(""set hive.exec.dynamic.partition=true"")
    spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"")
The data is inserted into the hive table dynamically partitioned based on prtn_String_columns: source_system_name, period_year, period_num
Spark-submit used:
SPARK_MAJOR_VERSION=2 spark-submit --conf spark.ui.port=4090 --driver-class-path /home/fdlhdpetl/jars/postgresql-42.1.4.jar  --jars /home/fdlhdpetl/jars/postgresql-42.1.4.jar --num-executors 80 --executor-cores 5 --executor-memory 50G --driver-memory 20G --driver-cores 3 --class com.partition.source.YearPartition splinter_2.11-0.1.jar --master=yarn --deploy-mode=cluster --keytab /home/fdlhdpetl/fdlhdpetl.keytab --principal fdlhdpetl@FDLDEV.COM --files /usr/hdp/current/spark2-client/conf/hive-site.xml,testconnection.properties --name Splinter --conf spark.executor.extraClassPath=/home/fdlhdpetl/jars/postgresql-42.1.4.jar
The following error messages are generated in the executor logs:
Container exited with a non-zero exit code 143.
Killed by external signal
18/10/03 15:37:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[SIGTERM handler,9,system]
java.lang.OutOfMemoryError: Java heap space
    at java.util.zip.InflaterInputStream.&lt;init&gt;(InflaterInputStream.java:88)
    at java.util.zip.ZipFile$ZipFileInflaterInputStream.&lt;init&gt;(ZipFile.java:393)
    at java.util.zip.ZipFile.getInputStream(ZipFile.java:374)
    at java.util.jar.JarFile.getManifestFromReference(JarFile.java:199)
    at java.util.jar.JarFile.getManifest(JarFile.java:180)
    at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:944)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:450)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:99)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:745)
I see in the logs that the read is being executed properly with the given number of partitions as below:
Scan JDBCRelation((select column_names from schema.tablename where period_year='2017' and period_num='12') as year2017) [numPartitions=50]
Below is the state of executors in stages:
The data is not being partitioned properly. One partition is smaller while the other one becomes huge. There is a skew problem here.
While inserting the data into Hive table the job fails at the line:spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"") but I understand this is happening because of the data skew problem.
I tried to increase number of executors, increasing the executor memory, driver memory, tried to just save as csv file instead of saving the dataframe into a Hive table but nothing affects the execution from giving the exception:
java.lang.OutOfMemoryError: GC overhead limit exceeded
Is there anything in the code that I need to correct ? Could anyone let me know how can I fix this problem ?
",<apache-spark><jdbc><hive><apache-spark-sql><partitioning>,7269,4,58,2009,9,60,136,41,14545,0.0,193,3,15,2018-10-02 6:38,2018-10-06 10:20,2018-10-06 10:20,4.0,4.0,Intermediate,23
57646511,What is the use of __table_args__ = {'extend_existing': True} in SQLAlchemy?,"I have a flask application which I'm trying to convert into Django. In one of the models which inherit an abstract base model, it is mentioned as
__table_args__ = {'extend_existing': True}
Can someone please explain what this means in SQLAlchemy with a small example.
I have gone through few articles but as I worked on Django and new to Flask-SQLAlchemy I couldn't understand it properly.
https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/
https://docs.sqlalchemy.org/en/13/orm/extensions/declarative/table_config.html
",<python><django><sqlalchemy>,546,4,1,4832,9,43,89,44,10524,,1152,1,15,2019-08-25 13:39,2021-12-13 4:30,2021-12-13 4:30,841.0,841.0,Basic,4
63282351,Django and Azure SQL key error 'deferrable' when start migrate command,"I try to connect Django to Azure SQL and have the error KeyError: deferrable when I start to migrate command.
I can't find a resolution for this issue.
I use this application:
asgiref==3.2.10
Django==3.1
django-mssql-backend==2.8.1
pyodbc==4.0.30
pytz==2020.1
sqlparse==0.3.1
and this is my config in settings.py:
DATABASES = {
    'default': {
        'ENGINE': 'sql_server.pyodbc',
        'NAME': 'DBNAME',
        'USER': 'DBUSER',
        'PASSWORD': 'PASSWORD',
        'HOST': 'databasename.database.windows.net',
        'PORT': '1433',
        'OPTIONS': {
            'driver': 'ODBC Driver 17 for SQL Server',
        },
    },
}
The error is when i try to run 'manage.py migrate`. Everything runs fine until the 8th step. Here's the output:
(venv) C:\Users\...\...\&gt;python manage.py migrate     
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length...Traceback (most recent call last):
  File &quot;manage.py&quot;, line 22, in &lt;module&gt;
    main()
  File &quot;manage.py&quot;, line 18, in main
    execute_from_command_line(sys.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\commands\migrate.py&quot;, line 243, in handle
    post_migrate_state = executor.migrate(
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 117, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 147, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 227, in apply_migration
    state = migration.apply(state, schema_editor)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\migration.py&quot;, line 124, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)    
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\operations\fields.py&quot;, line 236, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\base\schema.py&quot;, line 571, in alter_field
    self._alter_field(model, old_field, new_field, old_type, new_type,
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 479, in _alter_field
    self.execute(self._create_unique_sql(model, columns=[old_field.column]))
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 861, in execute
    sql = str(sql)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\ddl_references.py&quot;, line 200, in __str__
    return self.template % self.parts
KeyError: 'deferrable'
(venv) C:\Users\...\...&gt;
but if I connect to DB I see a table created a table from Django.
please help
Thanks!
",<python><django><azure><django-models><azure-sql-database>,4477,0,74,176,0,1,6,55,4415,0.0,1,2,15,2020-08-06 11:12,2020-08-06 18:52,,0.0,,Basic,3
58628889,ASP.NET Core Testing - get NullReferenceException when initializing InMemory SQLite dbcontext in fixture,"I have a test fixture in which I initialize my SQLite in-memory dbcontext, shown below:
public static MYAPPDBContext Create()
{
    var options = new DbContextOptionsBuilder&lt;MYAPPDBContext&gt;()
                    .UseSqlite(""DataSource=:memory:"")
                    .Options;
    var context = new MYAPPDBContext(options);
    context.Database.OpenConnection(); // this is where exception is thrown
    context.Database.EnsureCreated();
    return context;
}
When I call the Create() method, I get the following NullReferenceException:
System.NullReferenceException
  HResult=0x80004003
  Message=Object reference not set to an instance of an object.
  Source=Microsoft.Data.Sqlite
  StackTrace:
   at Microsoft.Data.Sqlite.SqliteConnection.Open()
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Sqlite.Storage.Internal.SqliteRelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.&lt;&gt;c.&lt;OpenConnection&gt;b__15_0(DatabaseFacade database)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, Func`2 operation, Func`2 verifySucceeded, TState state)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.OpenConnection(DatabaseFacade databaseFacade)
   at MYAPPPlus.UnitTests.TestInfrastructure.MYAPPContextFactory.Create() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\MYAPPContextFactory.cs:line 26
   at MYAPPPlus.UnitTests.TestInfrastructure.QueryTestFixture..ctor() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\QueryTestFixture.cs:line 24
Any ideas on what might be happening?
FYI: I'm basing my code on the blog post at https://garywoodfine.com/entity-framework-core-memory-testing-database/, among other resources. 
Also, my fixture works just fine when using basic ef core inmemory database.
",<c#><sqlite><asp.net-core><xunit><in-memory-database>,2250,2,27,816,1,11,21,36,6548,0.0,360,4,15,2019-10-30 15:34,2019-10-30 16:12,2019-10-30 20:27,0.0,0.0,Basic,12
52283751,Calculating percentage of total count for groupBy using pyspark,"I have the following code in pyspark, resulting in a table showing me the different values for a column and their counts. I want to have another column showing what percentage of the total count does each row represent. How do I do that?
difrgns = (df1
           .groupBy(""column_name"")
           .count()
           .sort(desc(""count""))
           .show())
Thanks in advance!
",<apache-spark><pyspark><apache-spark-sql>,379,0,5,353,2,3,10,45,31200,0.0,5,4,15,2018-09-11 20:27,2018-09-11 22:44,2018-09-11 22:44,0.0,0.0,Basic,2
59437429,"Error ""collection was modified enumeration operation may not execute"" when restoring database backup in Azure Data Studio","I'm extremely new to databases so please bear with me. 
I've set up local SQL Server running on a Docker container (using a Mac). I'm trying to restore SQL database using Azure Data Studio (v1.14.0) but it's not working. 
I used the guide on database.guide but keep getting errors. I have no clue what it means.
  Restore database failed: collection was modified; enumeration
  operation may not execute
I have tried restoring .bak-file from a backup made on my school computer (used SQL Server Management Studio on a PC), tried restoring with the bak-file from Database.guide. I also made a backup from my current DB in Azure and tried restoring that one - didn't work either. 
",<sql-server><azure><docker>,679,1,0,421,0,4,9,49,3747,0.0,0,1,15,2019-12-21 15:25,2019-12-22 11:07,2019-12-22 11:07,1.0,1.0,Basic,14
55288840,get parents and children of tree folder structure in my sql < 8 and no CTEs,"I have a folder table that joins to itself on an id, parent_id relationship:
CREATE TABLE folders (
  id int(10) unsigned NOT NULL AUTO_INCREMENT,
  title nvarchar(255) NOT NULL,
  parent_id int(10) unsigned DEFAULT NULL,
  PRIMARY KEY (id)
);
INSERT INTO folders(id, title, parent_id) VALUES(1, 'root', null);
INSERT INTO folders(id, title, parent_id) values(2, 'one', 1);
INSERT INTO folders(id, title, parent_id) values(3, 'target', 2);
INSERT INTO folders(id, title, parent_id) values(4, 'child one', 3);
INSERT INTO folders(id, title, parent_id) values(5, 'child two', 3);
INSERT INTO folders(id, title, parent_id) values(6, 'root 2', null);
INSERT INTO folders(id, title, parent_id) values(7, 'other child one', 6);
INSERT INTO folders(id, title, parent_id) values(8, 'other child two', 6);
I want a query that returns all the parents of that record, right back to the route and any children.
So if I ask for folder with id=3, I get records: 1, 2, 3, 4, 5. I am stuck how to get the parents.
The version of MYSQL is 5.7 and there are no immediate plans to upgrade so sadly CTEs are not an option.
I have created this sql fiddle
",<mysql><sql><hierarchical-data><recursive-query>,1134,1,19,27236,60,241,456,35,5198,0.0,526,8,15,2019-03-21 20:34,2019-03-21 21:30,,0.0,,Intermediate,17
55248938,TypeORM and Postgres competing naming styles,"We are using TypeORM and Postgresql and I'm curious about naming conventions.
Given that there are perfectly appropriate styles and naming conventions for databases that are separate from the perfectly good ones used for Javascript, is it considered better practice to force databases to use the code convention or to force code to use the database convention, or to translate everything?
For example:
It's common practice to use the SQl style defined in Joe Celko's SQL Programming Style for the database. This advocates for snake_case for the column names.
It's also common practice to name variables in camelCase when programming in JavaScript and all the documentation on typeorm.
So, when these two worlds collide, is it best practice to force one to the other or to translate every multi-word entity in the definitions to do the mapping.
This isn't really a question of how to do that but rather if there is a common practice one way of the other.
The three possibilities for a column representing User Id are:
1: Translate everything
@Column( { name: user_id } )
userId: number;
2: Use the database convention in the code
@Column()
user_id: number;
3: Use the coding convention in the database
@Column()
userId: number
",<postgresql><typeorm>,1226,1,11,369,1,2,9,40,15065,0.0,11,1,15,2019-03-19 19:46,2019-12-16 12:26,2019-12-16 12:26,272.0,272.0,Intermediate,20
49149511,Is Microsoft Sync Framework alive?,"According to the MS documentation Sync Framework Toolkit (https://code.msdn.microsoft.com/Sync-Framework-Toolkit-4dc10f0e) is a legacy open source product which MS no longer support:
https://msdn.microsoft.com/en-us/library/jj839436(v=sql.110).aspx
That's fine, but how about Microsoft Sync SDK which is not open source? 
Does it mean that open source part useless because server part can be removed by MS in the future?
The question is does it mean that Sync Framework SDK (Server side library) is dead? (Green Part)
",<c#><sql-server><synchronization><microsoft-sync-framework>,518,5,0,8903,4,43,73,67,14769,,908,3,15,2018-03-07 10:24,2019-01-30 6:58,,329.0,,Intermediate,20
58722202,What are the different use cases for using QueryBuilder vs. Repository in TypeORM?,"I'm building an API using NestJS with TypeORM. I've been querying a MySQL database using the TypeORM Repository API mostly because the NestJS Database documentation section provided an example using this.photoRepository.find(). As I get further along, I've noticed many of my exploratory search results recommending using the TypeORM QueryBuilder API for performance and flexibility reasons.
I'm getting the sense that the Repository approach is easier to use for simple needs and a great abstraction if I ever decide to switch my database framework. On the other hand, it also seems to me that QueryBuilder is more performant and customizable.
Could we outline the different use cases for QueryBuilder vs. Repository in TypeORM?
",<mysql><database><nestjs><typeorm>,730,3,1,173,0,1,10,53,7889,0.0,9,1,15,2019-11-06 2:22,2019-11-06 8:35,2019-11-06 8:35,0.0,0.0,Intermediate,20
