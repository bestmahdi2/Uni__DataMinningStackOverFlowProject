QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
51118815,"Error: could not determine PostgreSQL version from ""10.4""","
Complete output from command python setup.py egg_info: running
egg_info creating pip-egg-info/psycopg2.egg-info writing
pip-egg-info/psycopg2.egg-info/PKG-INFO writing top-level names to
pip-egg-info/psycopg2.egg-info/top_level.txt writing dependency_links
to pip-egg-info/psycopg2.egg-info/dependency_links.txt writing
manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt' Error:
could not determine PostgreSQL version from '10.4'
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-install-lR9u0X/psycopg2/
Does anyone know what's the issue? Trying to run pgadmin in virtualenv and can't figure out because of this error.
",<python><postgresql><psycopg2>,660,0,0,197,1,2,6,51,11373,0.0,3,2,18,2018-06-30 21:54,2018-08-04 14:42,2018-08-04 14:42,35.0,35.0,Basic,14
65478350,ERROR: column is of type json but expression is of type character varying in Hibernate,"I need to map two columns of entity class as json in postgres using spring data jpa. After reading multiple stackoverflow posts and baeldung post ,
How to map a map JSON column to Java Object with JPA
https://www.baeldung.com/hibernate-persist-json-object
I did configuration as below. However, I am facing error &quot;ERROR: column &quot;headers&quot; is of type json but expression is of type character varying&quot;
Please provide some pointer to resolve this issue.
I have an entity class as below
@Entity
@Data
@SuperBuilder
@NoArgsConstructor
@AllArgsConstructor
public class Task {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    private Integer id;
    private String url;
    private String httpMethod;
    @Convert(converter = HashMapConverter.class)
    @Column(columnDefinition = &quot;json&quot;)
    private Map&lt;String, String&gt; headers;
    @Convert(converter = HashMapConverter.class)
    @Column(columnDefinition = &quot;json&quot;)
    private Map&lt;String, String&gt; urlVariables;
}
I have created a test class to test if entity is persisted or not. On running this junit, below test case is failing with error as below
@SpringBootTest
class TaskRepositoryTest {
    private static Task randomTask = randomTask();
    @Autowired
    private TaskRepository taskRepository;
    @BeforeEach
    void setUp() {
        taskRepository.deleteAll();
        taskRepository.save(randomTask);
    }
    public static Task randomTask() {
        return randomTaskBuilder().build();
    }
    public static TaskBuilder randomTaskBuilder() {
        Map&lt;String,String&gt; headers = new HashMap&lt;&gt;();
        headers.put(randomAlphanumericString(10),randomAlphanumericString(10));
        Map&lt;String,String&gt; urlVariables = new HashMap&lt;&gt;();
        urlVariables.put(randomAlphanumericString(10),randomAlphanumericString(10));
        return builder()
                .id(randomPositiveInteger())
                .httpMethod(randomAlphanumericString(10))
                .headers(headers)
                .urlVariables(urlVariables)
                .url(randomAlphanumericString(10)));
    }
}
Using liquibase, I have created table in postgres DB and I could see column datatype as json.
databaseChangeLog:
  - changeSet:
      id: 1
      author: abc
      changes:
        - createTable:
            tableName: task
            columns:
              - column:
                  name: id
                  type: int
                  autoIncrement: true
                  constraints:
                    primaryKey: true
              - column:
                  name: url
                  type: varchar(250)
                  constraints:
                    nullable: false
                    unique: true
              - column:
                  name: http_method
                  type: varchar(50)
                  constraints:
                    nullable: false
              - column:
                  name: headers
                  type: json
              - column:
                  name: url_variables
                  type: json
      rollback:
        - dropTable:
            tableName: task
",<json><postgresql><spring-boot><spring-data-jpa><liquibase>,3153,4,88,2135,6,28,47,50,33104,0.0,48,5,18,2020-12-28 13:26,2020-12-29 9:05,2020-12-29 9:05,1.0,1.0,Basic,14
51814383,Why CTE (Common Table Expressions) in some cases slow down queries comparing to temporary tables in SQL Server,"I have several cases where my complex CTE (Common Table Expressions) are ten times slower than the same queries using the temporary tables in SQL Server.
My question here is in regards to how SQL Server process the CTE queries, it looks like it tries to join all the separated queries instead of storing the results of each one and then trying to run the following ones. So that might be the reason why it is so faster when using temporary tables.
For example:
Query 1: using Common Table Expression:
;WITH Orders AS
(
    SELECT
        ma.MasterAccountId,
        IIF(r.FinalisedDate IS NULL, 1, 0)) [Status]
    FROM 
        MasterAccount ma
    INNER JOIN 
        task.tblAccounts a ON a.AccountNumber = ma.TaskAccountId 
                           AND a.IsActive = 1
    LEFT OUTER JOIN 
        task.tblRequisitions r ON r.AccountNumber = a.AccountNumber 
    WHERE 
        ma.IsActive = 1
        AND CAST(r.BatchDateTime AS DATE) BETWEEN @fromDate AND @toDate
        AND r.BatchNumber &gt; 0
),
StockAvailability AS
(
    SELECT sa.AccountNumber,
           sa.RequisitionNumber,
           sa.RequisitionDate,
           sa.Lines,
           sa.HasStock,
           sa.NoStock,
           CASE WHEN sa.Lines = 0 THEN 'Empty'
                WHEN sa.HasStock = 0 THEN 'None'
                WHEN (sa.Lines &gt; 0 AND sa.Lines &gt; sa.HasStock) THEN 'Partial'
                WHEN (sa.Lines &gt; 0 AND sa.Lines &lt;= sa.HasStock) THEN 'Full'
            END AS [Status]
    FROM
    (
        SELECT
                r.AccountNumber,
                r.RequisitionNumber,
                r.RequisitionDate,
                COUNT(rl.ProductNumber) Lines,
                SUM(IIF(ISNULL(psoh.AvailableStock, 0) &gt;= ISNULL(rl.Quantity, 0), 1, 0)) AS HasStock,
                SUM(IIF(ISNULL(psoh.AvailableStock, 0) &lt; ISNULL(rl.Quantity, 0), 1, 0)) AS NoStock
        FROM task.tblrequisitions r 
        INNER JOIN task.tblRequisitionLines rl ON rl.RequisitionNumber = r.RequisitionNumber
        LEFT JOIN ProductStockOnHandSummary psoh ON psoh.ProductNumber = rl.ProductNumber
        WHERE dbo.fn_RemoveUnitPrefix(r.BatchNumber) = 0
          AND r.UnitId = 1
          AND r.FinalisedDate IS NULL
          AND r.RequisitionStatus = 1 
          AND r.TransactionTypeNumber = 301 
        GROUP BY r.AccountNumber, r.RequisitionNumber, r.RequisitionDate
    ) AS sa
),
Available AS
(
    SELECT  ma.MasterAccountId,
            SUM(IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                            CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END)) AS AvailableStock,
            SUM(IIF(sa.[Status] IN ('Full', 'Partial', 'None'), 1, 0))  AS OrdersAnyStock, 
            SUM(IIF(sa.RequisitionDate &lt; dbo.TicksToTime(ma.DailyOrderCutOffTime, @toDate),
                    IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                                CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END), 0)) AS AvailableBeforeCutOff                             
    FROM MasterAccount ma
    INNER JOIN StockAvailability sa ON sa.AccountNumber = ma.TaskAccountId
    GROUP BY ma.MasterAccountId, ma.IsPartialStock
),
Totals AS
(
    SELECT 
        o.MasterAccountId,
        COUNT(o.MasterAccountId) AS BatchedOrders
    FROM Orders o
    GROUP BY o.MasterAccountId
)
SELECT a.MasterAccountId,
       ISNULL(t.BatchedOrders, 0) BatchedOrders,
       ISNULL(t.PendingOrders, 0) PendingOrders,
       ISNULL(av.AvailableStock, 0) AvailableOrders,
       ISNULL(av.AvailableBeforeCutOff, 0) AvailableCutOff,
       ISNULL(av.OrdersAnyStock, 0) AllOrders
FROM MasterAccount a
LEFT OUTER JOIN Available av ON av.MasterAccountId = a.MasterAccountId
LEFT OUTER JOIN Totals t ON t.MasterAccountId = a.MasterAccountId
WHERE a.IsActive = 1
Query 2: using temporary tables:
DROP TABLE IF EXISTS #Orders
CREATE TABLE #Orders (MasterAccountId int, [Status] int);
INSERT INTO #Orders
SELECT
    ma.MasterAccountId,
    dbo.fn_GetBatchPickingStatus(ma.BatchPickingOnHold,
                                    iif(r.GroupNumber &gt; 0, 1, 0),
                                    iif(r.FinalisedDate is null, 1, 0)) [Status]
FROM MasterAccount ma (nolock)
INNER JOIN wh3.dbo.tblAccounts a (nolock) on a.AccountNumber = dbo.fn_RemoveUnitPrefix(ma.TaskAccountId) and a.IsActive = 1
LEFT OUTER JOIN wh3.dbo.tblRequisitions r (nolock) on r.AccountNumber = a.AccountNumber 
WHERE cast(r.BatchDateTime as date) between @fromDate and @toDate
    AND r.BatchNumber &gt; 0
    AND ma.IsActive = 1
DROP TABLE IF EXISTS #StockAvailability
Create Table #StockAvailability (AccountNumber int, RequisitionNumber int, RequisitionDate datetime, Lines int, HasStock int, NoStock int);
Insert Into #StockAvailability
SELECT
        r.AccountNumber,
        r.RequisitionNumber,
        r.RequisitionDate,
        COUNT(rl.ProductNumber) Lines,
        SUM(IIF(ISNULL(psoh.AvailableStock, 0) &gt;= ISNULL(rl.Quantity, 0), 1, 0)) AS HasStock,
        SUM(IIF(ISNULL(psoh.AvailableStock, 0) &lt; ISNULL(rl.Quantity, 0), 1, 0)) AS NoStock
FROM WH3.dbo.tblrequisitions r (nolock)
INNER JOIN WH3.dbo.tblRequisitionLines rl (nolock) ON rl.RequisitionNumber = r.RequisitionNumber
LEFT JOIN ProductStockOnHandSummary psoh (nolock) ON psoh.ProductNumber = rl.ProductNumber -- Joined with View          
WHERE r.BatchNumber = 0
    AND r.FinalisedDate is null
    AND r.RequisitionStatus = 1 
    AND r.TransactionTypeNumber = 301 
GROUP BY r.AccountNumber, r.RequisitionNumber, r.RequisitionDate
DROP TABLE IF EXISTS #StockAvailability2
Create Table #StockAvailability2 (AccountNumber int, RequisitionNumber int, RequisitionDate datetime, Lines int, HasStock int, NoStock int, [Status] nvarchar(7));
Insert Into #StockAvailability2
SELECT sa.AccountNumber,
        sa.RequisitionNumber,
        sa.RequisitionDate,
        sa.Lines,
        sa.HasStock,
        sa.NoStock,
        CASE WHEN sa.Lines = 0 THEN 'Empty'
            WHEN sa.HasStock = 0 THEN 'None'
            WHEN (sa.Lines &gt; 0 AND sa.Lines &gt; sa.HasStock) THEN 'Partial'
            WHEN (sa.Lines &gt; 0 AND sa.Lines &lt;= sa.HasStock) THEN 'Full'
        END AS [Status]
FROM #StockAvailability sa
DROP TABLE IF EXISTS #Available
Create Table #Available (MasterAccountId int, AvailableStock int, OrdersAnyStock int, AvailableBeforeCutOff int);
INSERT INTO #Available
SELECT  ma.MasterAccountId,
        SUM(IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                        CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END)) AS AvailableStock,
        SUM(IIF(sa.[Status] IN ('Full', 'Partial', 'None'), 1, 0))  AS OrdersAnyStock, 
        SUM(IIF(sa.RequisitionDate &lt; dbo.TicksToTime(ma.DailyOrderCutOffTime, @toDate),
                IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                            CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END), 0)) AS AvailableBeforeCutOff                             
FROM MasterAccount ma (NOLOCK)
INNER JOIN #StockAvailability2 sa ON sa.AccountNumber = dbo.fn_RemoveUnitPrefix(ma.TaskAccountId)
GROUP BY ma.MasterAccountId, ma.IsPartialStock
;WITH Totals AS
(
    SELECT 
        o.MasterAccountId,
        COUNT(o.MasterAccountId) AS BatchedOrders,
        SUM(IIF(o.[Status] IN (0,1,2), 1, 0)) PendingOrders
    FROM #Orders o (NOLOCK)
    GROUP BY o.MasterAccountId
)
SELECT a.MasterAccountId,
       ISNULL(t.BatchedOrders, 0) BatchedOrders,
       ISNULL(t.PendingOrders, 0) PendingOrders,
       ISNULL(av.AvailableStock, 0) AvailableOrders,
       ISNULL(av.AvailableBeforeCutOff, 0) AvailableCutOff,
       ISNULL(av.OrdersAnyStock, 0) AllOrders
FROM MasterAccount a (NOLOCK)
LEFT OUTER JOIN #Available av (NOLOCK) ON av.MasterAccountId = a.MasterAccountId
LEFT OUTER JOIN Totals t (NOLOCK) ON t.MasterAccountId = a.MasterAccountId
WHERE a.IsActive = 1
",<sql><sql-server><sql-server-2008><sql-server-2005><sql-server-2012>,8024,0,178,1591,2,27,56,61,6256,0.0,1672,3,18,2018-08-13 2:10,2018-08-13 2:15,2018-08-13 2:15,0.0,0.0,Intermediate,23
51596967,Is a PostgreSQL trigger asynchronous?,"Sorry but I am new to SQL triggers and need some help.
I am after creating a trigger on UPDATE to one of my PostgreSQL tables.  The trigger calls DELETE on a different table.   My question is:  Is the trigger portion of the request asynchronous or synchronous to the UPDATE request?
For more information here are my trigger and function:
CREATE OR REPLACE FUNCTION expire_table() RETURNS trigger AS $expire_table$
  BEGIN
     DELETE FROM object_store WHERE p_time &lt; NOW() - INTERVAL '6 months';
     RETURN NEW;
  END;
$expire_table$ LANGUAGE 'plpgsql';
CREATE TRIGGER expire_table_trigger
  AFTER UPDATE ON objects
EXECUTE PROCEDURE expire_table();
My hope is that the trigger portion is asynchronous or that there is a way of making it so.
",<postgresql>,746,0,13,759,0,10,29,68,7948,0.0,30,1,18,2018-07-30 15:03,2018-07-30 15:19,2018-07-30 15:19,0.0,0.0,Intermediate,23
57214850,Trying to upgrade SQLite on Amazon EC2,"I need SQLite minimum version 3.8 to support a MediaWiki install on Amazon EC2. Amazon Linux is based on CentOS and the latest version available in the yum repository is SQLite 3.7.17.
The downloads available from sqlite.org don't include 64-bit Linux. There is a GitHub repository that has a prebuilt 64-bit version, however it's only the command line version. I put it at /usr/bin:
$ which sqlite3
/usr/bin/sqlite3
$ sqlite3 --version
sqlite3: /lib64/libtinfo.so.5: no version information available (required by sqlite3)
3.26.0 2018-12-01 12:34:55 bf8c1b2b7a5960c282e543b9c293686dccff272512d08865f4600fb58238b4f9
But MediaWiki still complains I have SQLite 3.7.17 installed. When I test it I get:
$ cat x.php
&lt;?php
print_r(SQLite3::version());
?&gt;
Run it:
$ php7 x.php
Array
(
    [versionString] =&gt; 3.7.17
    [versionNumber] =&gt; 3007017
)
I am guessing this is because of these libraries:
$ sudo find / -name &quot;libsqlite*&quot;
/usr/lib64/libsqlite3.so.0
/usr/lib64/libsqlite3.so.0.8.6
How can I download/rebuild or otherwise install a later version of these SQLite libraries?
",<php><sqlite><mediawiki>,1095,1,20,509,0,6,17,46,9105,0.0,117,4,18,2019-07-26 7:10,2020-07-14 15:35,2020-07-14 15:35,354.0,354.0,Intermediate,23
60059084,What does Using join buffer (Block Nested Loop) mean with EXPLAIN mysql command in the Extra column?,"I am trying to optimize my query.
And getting Using join buffer (Block Nested Loop)
for one of the table with
EXPLAIN SELECT command.
I have no idea what does it mean.
I tried googling about, but I haven't found the explanation.
",<mysql><explain>,229,0,1,2189,3,11,35,62,21052,0.0,2513,3,18,2020-02-04 14:03,2020-02-04 14:35,2020-02-04 14:35,0.0,0.0,Intermediate,23
61545680,Postgresql partition and sqlalchemy,"SQLAlchemy doc explain how to create a partitioned table. But it does not explains how to create partitions.
So if I have this :
#Skipping create_engine and metadata
Base = declarative_base()
class Measure(Base):
    __tablename__ = 'measures'
    __table_args__ = {
        postgresql_partition_by: 'RANGE (log_date)'
    }
    city_id = Column(Integer, not_null=True)
    log_date = Columne(Date, not_null=True)
    peaktemp = Column(Integer)
    unitsales = Column(Integer)
class Measure2020(Base):
    """"""How am I suppposed to declare this ? """"""
I know that most of the I'll be doing SELECT * FROM measures WHERE logdate between XX and YY. But that seems interesting.
",<python><postgresql><sqlalchemy><postgresql-12>,672,1,16,888,1,6,20,77,15231,0.0,95,4,18,2020-05-01 15:38,2020-07-23 9:27,2021-06-20 21:18,83.0,415.0,Basic,11
48965204,"How to select rows, and nearby rows","SQL Fiddle
Background
I have a table of values that some need attention:
| ID      | AddedDate   |
|---------|-------------|
|       1 | 2010-04-01  |
|       2 | 2010-04-01  |
|       3 | 2010-04-02  |
|       4 | 2010-04-02  |
|       5 | NULL        | &lt;----------- needs attention
|       6 | 2010-04-02  |
|       7 | 2010-04-03  |
|       8 | 2010-04-04  |
|       9 | 2010-04-04  |
| 2432659 | 2016-06-15  |
| 2432650 | 2016-06-16  |
| 2432651 | 2016-06-17  |
| 2432672 | 2016-06-18  |
| 2432673 | NULL        | &lt;----------- needs attention
| 2432674 | 2016-06-20  |
| 2432685 | 2016-06-21  |
I want to select the rows where AddedDate is null, and i want to select rows around it. In this example question it would be sufficient to say rows where the ID is ±3. This means i want:
| ID      | AddedDate   |
|---------|-------------|
|       2 | 2010-04-01  | ─╮
|       3 | 2010-04-02  |  │
|       4 | 2010-04-02  |  │
|       5 | NULL        |  ├──ID values ±3
|       6 | 2010-04-02  |  │
|       7 | 2010-04-03  |  │
|       8 | 2010-04-04  | ─╯
| 2432672 | 2016-06-18  | ─╮
| 2432673 | NULL        |  ├──ID values ±3
| 2432674 | 2016-06-20  | ─╯
  Note: In reality it's a table of 9M rows, and 15k need attention.
Attempts
First i create a query that builds the ranges i'm interested in returning:
SELECT
  ID-3 AS [Low ID],
  ID+3 AS [High ID]
FROM Items
WHERE AddedDate IS NULL
Low ID   High ID
-------  -------
2        8 
2432670  2432676
So my initial attempt to use this does work:
WITH dt AS (
   SELECT ID-3 AS Low, ID+3 AS High
   FROM Items
   WHERE AddedDate IS NULL
)
SELECT * FROM Items
WHERE EXISTS(
    SELECT 1 FROM dt
    WHERE Items.ID BETWEEN dt.Low AND dt.High)
But when i try it on real data:
9 million total rows
15,000 interesting rows
subtree cost of 63,318,400
it takes hours (before i give up and cancel it)
There's probably a more efficient way.
Bonus Reading
Select a row and rows around it
Select Rows with matching columns from SQL Server
How can I search for rows &quot;around&quot; a given string value?
How to get N rows starting from row M from sorted table in T-SQL
",<sql><sql-server><sql-server-2012>,2117,6,52,248457,256,876,1230,81,2737,0.0,5616,4,18,2018-02-24 16:49,2018-02-24 16:59,2018-02-24 17:29,0.0,0.0,Intermediate,23
49956713,"pip install mysqlclient failed ""Running setup.py bdist_wheel for mysqlclient ... error""","I'm trying to run my Python 3 project on my newly formatted Mac OS High Sierra 10.13.4, by first running pipenv install to get the dependencies, but that fails.
Specifically, the part where it fails to install dependencies is the mysqlclient part.
This error message shows up:
_mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
              my_bool recon = reconnect;
              ^
_mysql.c:1895:58: error: use of undeclared identifier 'recon'
              mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                     ^
Configuration
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; python --version
Python 2.7.10
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; python3 --version
Python 3.6.5
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew install mysql-connector-c
Warning: mysql-connector-c 6.1.11 is already installed, its just not linked
You can use `brew link mysql-connector-c` to link this version.
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew link mysql-connector-c
Linking /usr/local/Cellar/mysql-connector-c/6.1.11... 
Error: Could not symlink bin/my_print_defaults
Target /usr/local/bin/my_print_defaults
is a symlink belonging to mysql. You can unlink it:
  brew unlink mysql
To force the link and overwrite all conflicting files:
  brew link --overwrite mysql-connector-c
To list all files that would be deleted:
  brew link --overwrite --dry-run mysql-connector-c
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; mysql --version
mysql  Ver 8.0.11 for macos10.13 on x86_64 (MySQL Community Server - GPL)
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew install mysql
Warning: mysql 5.7.22 is already installed and up-to-date
To reinstall 5.7.22, run `brew reinstall mysql`
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew info openssl
openssl: stable 1.0.2o (bottled) [keg-only]
SSL/TLS cryptography library
https://openssl.org/
/usr/local/Cellar/openssl/1.0.2o_1 (1,791 files, 12.3MB)
  Poured from bottle on 2018-04-20 at 13:06:42
From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/openssl.rb
==&gt; Dependencies
Build: makedepend ✘
==&gt; Options
--without-test
    Skip build-time tests (not recommended)
==&gt; Caveats
A CA file has been bootstrapped using certificates from the SystemRoots
keychain. To add additional certificates (e.g. the certificates added in
the System keychain), place .pem files in
  /usr/local/etc/openssl/certs
and run
  /usr/local/opt/openssl/bin/c_rehash
This formula is keg-only, which means it was not symlinked into /usr/local,
because Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries.
If you need to have this software first in your PATH run:
  echo 'export PATH=""/usr/local/opt/openssl/bin:$PATH""' &gt;&gt; ~/.bash_profile
For compilers to find this software you may need to set:
    LDFLAGS:  -L/usr/local/opt/openssl/lib
    CPPFLAGS: -I/usr/local/opt/openssl/include
Full terminal error log
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; pip install mysqlclient
Collecting mysqlclient
  Using cached https://files.pythonhosted.org/packages/6f/86/bad31f1c1bb0cc99e88ca2adb7cb5c71f7a6540c1bb001480513de76a931/mysqlclient-1.3.12.tar.gz
Building wheels for collected packages: mysqlclient
  Running setup.py bdist_wheel for mysqlclient ... error
  Complete output from command /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-wheel-_frnx3t4 --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.9-x86_64-3.6
  copying _mysql_exceptions.py -&gt; build/lib.macosx-10.9-x86_64-3.6
  creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/connections.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/converters.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/cursors.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/release.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/times.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  running build_ext
  building '_mysql' extension
  creating build/temp.macosx-10.9-x86_64-3.6
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch x86_64 -g -Dversion_info=(1,3,12,'final',0) -D__version__=1.3.12 -I/usr/local/mysql/include -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c _mysql.c -o build/temp.macosx-10.9-x86_64-3.6/_mysql.o
  _mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
                  my_bool recon = reconnect;
                  ^
  _mysql.c:1895:58: error: use of undeclared identifier 'recon'
                  mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                         ^
  2 errors generated.
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-record-vkhrazcr/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.9-x86_64-3.6
    copying _mysql_exceptions.py -&gt; build/lib.macosx-10.9-x86_64-3.6
    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/connections.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/converters.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/cursors.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/release.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/times.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    running build_ext
    building '_mysql' extension
    creating build/temp.macosx-10.9-x86_64-3.6
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch x86_64 -g -Dversion_info=(1,3,12,'final',0) -D__version__=1.3.12 -I/usr/local/mysql/include -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c _mysql.c -o build/temp.macosx-10.9-x86_64-3.6/_mysql.o
    _mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
                    my_bool recon = reconnect;
                    ^
    _mysql.c:1895:58: error: use of undeclared identifier 'recon'
                    mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                           ^
    2 errors generated.
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
Command ""/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-record-vkhrazcr/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/
I'm frustrated enough to want to uninstall all my python installations, but I'm just afraid I might screw something up with Mac that relies on the prebuilt outdated Python version.
How can I get my dependencies to be installed correctly?
",<mysql><python-3.x><macos><pipenv>,10445,3,152,1829,2,19,37,44,10329,0.0,914,3,18,2018-04-21 14:19,2018-05-15 4:13,2018-05-15 4:13,24.0,24.0,Intermediate,23
52377469,Failed to open the referenced table,"I am new with SQL and I am not entirely sure why I am getting the error: ERROR 1824 (HY000) at line 5: Failed to open the referenced table 'products'
Operation failed with exitcode 1
Here is my code
drop database if exists cc;
create database cc /*!40100 default character set utf8 */;
use cc;
create table Customers(
  CustomerID int not null,
  FirstName varchar(255),
  LastName varchar(255),
  address varchar(255),
  phoneNO varchar(11),
  prodID int,
  quantity int,
  primary key (CustomerID),
  foreign key (prodID) references Products(itemID)
);
create table Employees(
   EmployeeID int not null,
   FirstName varchar(255),
   LastName varchar(255),
   address varchar(255),
   phoneNO varchar(11),
   ManagerID int not null,
   primary key (EmployeeID),
   foreign key (managerID) references Managers(mgrID)
 );
create table Managers(
    mgrID int not null,
    salary float,
   MaxSupervisingCapacity int,
   foreign key (mgrID) references Employees(EmployeeID),
   primary key (mgrID) 
);
",<mysql><sql>,1003,0,34,191,1,1,5,66,79129,0.0,0,5,18,2018-09-18 0:51,2018-09-18 1:07,,0.0,,Basic,10
55708079,Spark: optimise writing a DataFrame to SQL Server,"I am using the code below to write a DataFrame of 43 columns and about 2,000,000 rows into a table in SQL Server:
dataFrame
  .write
  .format(""jdbc"")
  .mode(""overwrite"")
  .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
  .option(""url"", url)
  .option(""dbtable"", tablename)
  .option(""user"", user)
  .option(""password"", password)
  .save()
Sadly, while it does work for small DataFrames it's either extremely slow or gets timed out for large ones. Any hints on how to optimize it?
I've tried setting rewriteBatchedStatements=true
Thanks.
",<sql><sql-server><database><scala><apache-spark>,558,0,11,662,1,11,25,46,19707,0.0,15,5,18,2019-04-16 12:23,2019-04-16 22:05,2019-04-25 9:51,0.0,9.0,Intermediate,23
49660672,What to try to get BigQuery to CAST BYTES to STRING?,"BigQuery Standard SQL documentation suggests that BYTE fields can be coerced into STRINGS. 
We have a byte field that is the result of SHA256 hashing a field using BigQuery itself. 
We now want to coerce it to a STRING, yet when we run ""CAST(field_name to STRING)"" we get an error: 
  Query Failed Error: Invalid cast of bytes to UTF8 string
What is preventing us from getting a string from this byte field? Is it surmountable? If so, what is the solution?
",<sql><casting><google-bigquery>,457,1,0,5872,10,48,78,80,29688,0.0,565,3,18,2018-04-04 21:22,2018-04-04 21:35,,0.0,,Basic,2
52533487,STRING_AGG not behaving as expected,"I have the following query:
WITH cteCountryLanguageMapping AS (
    SELECT * FROM (
        VALUES
            ('Spain', 'English'),
            ('Spain', 'Spanish'),
            ('Sweden', 'English'),
            ('Switzerland', 'English'),
            ('Switzerland', 'French'),
            ('Switzerland', 'German'),
            ('Switzerland', 'Italian')
    ) x ([Country], [Language])
)
SELECT
    [Country],
    CASE COUNT([Language])
        WHEN 1 THEN MAX([Language])
        WHEN 2 THEN STRING_AGG([Language], ' and ')
        ELSE STRING_AGG([Language], ', ')
    END AS [Languages],
    COUNT([Language]) AS [LanguageCount]
FROM cteCountryLanguageMapping
GROUP BY [Country]
I was expecting the value inside Languages column for Switzerland to be comma separated i.e.:
  | Country     | Languages                                 | LanguageCount
--+-------------+-------------------------------------------+--------------
1 | Spain       | Spanish and English                       | 2
2 | Sweden      | English                                   | 1
3 | Switzerland | French, German, Italian, English          | 4
Instead I am getting the below output (the 4 values are separated by and):
  | Country     | Languages                                 | LanguageCount
--+-------------+-------------------------------------------+--------------
1 | Spain       | Spanish and English                       | 2
2 | Sweden      | English                                   | 1
3 | Switzerland | French and German and Italian and English | 4
What am I missing?
Here is another example:
SELECT y, STRING_AGG(z, '+') AS STRING_AGG_PLUS, STRING_AGG(z, '-') AS STRING_AGG_MINUS
FROM (
    VALUES
        (1, 'a'),
        (1, 'b')
) x (y, z)
GROUP by y
  | y | STRING_AGG_PLUS | STRING_AGG_MINUS
--+---+-----------------+-----------------
1 | 1 | a+b             | a+b
Is this a bug in SQL Server?
",<sql><sql-server><sql-server-2017><string-aggregation>,1896,0,44,5754,11,51,77,38,2667,0.0,479,1,18,2018-09-27 9:14,2018-09-27 9:52,2018-09-27 9:52,0.0,0.0,Basic,2
50436910,EF core not creating tables on migrate method,"Hey I just started using EF core and everything works fine. I call the the context.Database.Migrate() method and it creates a database. But even though my context object has a DBSet&lt;T&gt;, it doesn't create any tables except for the migration history.
Can anyone help me with this issue?
",<c#><sql-server><asp.net-core><entity-framework-core>,291,0,2,887,3,15,28,66,30706,0.0,18,4,18,2018-05-20 16:19,2018-05-20 16:27,2018-05-20 16:27,0.0,0.0,Basic,9
50612577,CSV copy to Postgres with array of custom type using JDBC,"I have a custom type defined in my database as
CREATE TYPE address AS (ip inet, port int);
And a table that uses this type in an array:
CREATE TABLE my_table (
  addresses  address[] NULL
)
I have a sample CSV file with the following contents
{(10.10.10.1,80),(10.10.10.2,443)}
{(10.10.10.3,8080),(10.10.10.4,4040)}
And I use the following code snippet to perform my COPY:
    Class.forName(""org.postgresql.Driver"");
    String input = loadCsvFromFile();
    Reader reader = new StringReader(input);
    Connection connection = DriverManager.getConnection(
            ""jdbc:postgresql://db_host:5432/db_name"", ""user"",
            ""password"");
    CopyManager copyManager = connection.unwrap(PGConnection.class).getCopyAPI();
    String copyCommand = ""COPY my_table (addresses) "" + 
                         ""FROM STDIN WITH ("" + 
                           ""DELIMITER '\t', "" + 
                           ""FORMAT csv, "" + 
                           ""NULL '\\N', "" + 
                           ""ESCAPE '\""', "" +
                           ""QUOTE '\""')"";
    copyManager.copyIn(copyCommand, reader);
Executing this program produces the following exception:
Exception in thread ""main"" org.postgresql.util.PSQLException: ERROR: malformed record literal: ""(10.10.10.1""
  Detail: Unexpected end of input.
  Where: COPY only_address, line 1, column addresses: ""{(10.10.10.1,80),(10.10.10.2,443)}""
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2422)
    at org.postgresql.core.v3.QueryExecutorImpl.processCopyResults(QueryExecutorImpl.java:1114)
    at org.postgresql.core.v3.QueryExecutorImpl.endCopy(QueryExecutorImpl.java:963)
    at org.postgresql.core.v3.CopyInImpl.endCopy(CopyInImpl.java:43)
    at org.postgresql.copy.CopyManager.copyIn(CopyManager.java:185)
    at org.postgresql.copy.CopyManager.copyIn(CopyManager.java:160)
I have tried with different combinations of the parentheses in the input but cannot seem to get the COPY working. Any ideas where I might be going wrong?
",<java><database><postgresql><jdbc><postgresql-9.5>,2031,0,36,13115,19,60,93,41,6180,0.0,221,3,18,2018-05-30 20:01,2018-06-03 18:04,2018-06-03 18:04,4.0,4.0,Basic,9
50803109,How to store pandas DataFrame in SQLite DB,"I am unable to find good tutorial on this topic. I am having a pandas data frame, df as
Fu(varchar)  val
aed          544.8
jfn          5488
vivj         89.3
vffv         87.5
I want to create a database and a table and store the dataframe in it
",<python><pandas><sqlite>,248,0,6,81,1,1,10,44,20429,0.0,17,2,18,2018-06-11 17:24,2018-06-11 17:35,2018-06-11 17:35,0.0,0.0,Basic,9
50120467,Order queryset by alternating value,"I have the following model:
class Entry(models.Model):
    name = models.Charfield(max_length=255)
    client = models.Charfield(max_length=255)
client is the name of the client as could have values like facebook, google, and so on.
Is it possible to order the queryset so that the result is alternating the values of client?
What I expect is something like this:
Entry.objects.order_by('alternate client') --&gt; 
| client   | name   |
| google   | robert |
| facebook | linda  |
| google   | kate   | 
| facebook | jack   |
| google   | nina   |
| facebook | pierre |    
I am using django2.x and postgres if that helps.
EDIT:
Some additional info / requirements.
I have about 10 to 20 differtent clients
Entry does also have a created DateField. If possible result should also be ordered by date
I want to use pagination for Entry so solution should be using django's ORM 
",<django><postgresql><django-models><django-queryset>,876,0,17,11229,5,52,75,51,8096,0.0,1615,3,18,2018-05-01 16:49,2018-05-01 17:46,2018-05-05 16:50,0.0,4.0,Basic,9
54922433,Postgresql fatal the database system is starting up - windows 10,"I have installed postgresql on windows 10 on usb disk.
Every day when i start my pc in work from sleep and plug in the disk again then trying to start postgresql i get this error:
FATAL: the database system is starting up
The service starts with following command:
E:\PostgresSql\pg96\pgservice.exe ""//RS//PostgreSQL 9.6 Server""
It is the default one.
logs from E:\PostgresSql\data\logs\pg96
2019-02-28 10:30:36 CET [21788]: [1-1] user=postgres,db=postgres,app=[unknown],client=::1 FATAL:  the database system is starting up
2019-02-28 10:31:08 CET [9796]: [1-1] user=postgres,db=postgres,app=[unknown],client=::1 FATAL:  the database system is starting up
I want this start up to happen faster.
",<postgresql>,696,0,10,1854,5,36,74,75,37196,0.0,326,1,18,2019-02-28 9:36,2019-02-28 10:38,2019-02-28 10:38,0.0,0.0,Basic,14
63489949,How to roll back a transaction on error in PostgreSQL?,"I'm writing a script for PostgreSQL and since I want it to be executed atomically, I'm wrapping it inside a transaction.
I expected the script to look something like this:
BEGIN
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
EXCEPTION
    WHEN OTHERS THEN
        ROLLBACK;
END; -- A.k.a. COMMIT;
However, in this case pgAdmin warns me about a syntax error right after the initial BEGIN. If I terminate the command there by appending a semicolon like so: BEGIN; it instead informs me about error near EXCEPTION.
I realize that perhaps I'm mixing up syntax for control structures and transactions, however I couldn't find any mention of how to roll back a failed transaction in the docs (nor in SO for that matter).
I also considered that perhaps the transaction is rolled back automatically on error, but it doesn't seem to be the case since the following script:
BEGIN;
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
COMMIT;
warns me that: ERROR: current transaction is aborted, commands ignored until end of transaction block and I have to then manually ROLLBACK; the transaction.
It seems I'm missing something fundamental here, but what?
EDIT:
I tried using DO as well like so:
DO $$
BEGIN
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
EXCEPTION
    WHEN OTHERS THEN
        ROLLBACK;
END; $$
pgAdmin hits me back with a: ERROR: cannot begin/end transactions in PL/pgSQL. HINT: Use a BEGIN block with an EXCEPTION clause instead. which confuses me to no end, because that is exactly what I am (I think) doing.
POST-ACCEPT EDIT:
Regarding Laurenz's comment: &quot;Your SQL script would contain a COMMIT. That ends the transaction and rolls it back.&quot; - this is not the behavior that I observe. Please consider the following example (which is just a concrete version of an example I already provided in my original question):
BEGIN;
-- Just a simple, self-referencing table.
CREATE TABLE &quot;Dummy&quot; (
    &quot;Id&quot; INT GENERATED ALWAYS AS IDENTITY,
    &quot;ParentId&quot; INT NULL,
    CONSTRAINT &quot;PK_Dummy&quot; PRIMARY KEY (&quot;Id&quot;),
    CONSTRAINT &quot;FK_Dummy_Dummy&quot; FOREIGN KEY (&quot;ParentId&quot;) REFERENCES &quot;Dummy&quot; (&quot;Id&quot;)
);
-- Foreign key violation terminates the transaction.
INSERT INTO &quot;Dummy&quot; (&quot;ParentId&quot;)
VALUES (99);
COMMIT;
When I execute the script above, I'm greeted with: ERROR: insert or update on table &quot;Dummy&quot; violates foreign key constraint &quot;FK_Dummy_Dummy&quot;. DETAIL: Key (ParentId)=(99) is not present in table &quot;Dummy&quot;. which is as expected.
However, if I then try to check whether my Dummy table was created or rolled back like so:
SELECT EXISTS (
    SELECT FROM information_schema.&quot;tables&quot;
    WHERE &quot;table_name&quot; = 'Dummy');
instead of a simple false, I get the same error that I already mentioned twice: ERROR: current transaction is aborted, commands ignored until end of transaction block. Then I have to manually terminate the transaction via issuing ROLLBACK;.
So to me it seems that either the comment mentioned above is false or at least I'm heavily misinterpreting something here.
",<postgresql><exception><transactions><plpgsql><rollback>,3252,0,49,631,2,7,18,46,25722,0.0,162,2,18,2020-08-19 15:06,2020-08-19 16:46,2020-08-19 16:46,0.0,0.0,Basic,14
55596620,Unterminated dollar quote,"How to use IF statement in the PostgreSql (11 version)? I tried just raw IF usage but got problem (syntax error at or near “IF”). To resolve this problem people propose to use 'do &amp;&amp;' but it does not work as well (Unterminated dollar quote started at position 3 in SQL DO $$ BEGIN IF ......). Here is my SQL code:
DO $$
BEGIN
  IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'categories')) THEN
    CREATE TABLE IF NOT EXISTS categories
    (
      id   SERIAL NOT NULL,
      name character varying(40),
      CONSTRAINT categories_pkey PRIMARY KEY (id)
    );
    INSERT INTO categories (name) VALUES ('Games');
    INSERT INTO categories (name) VALUES ('Multimedia');
    INSERT INTO categories (name) VALUES ('Productivity');
    INSERT INTO categories (name) VALUES ('Tools');
    INSERT INTO categories (name) VALUES ('Health');
    INSERT INTO categories (name) VALUES ('Lifestyle');
    INSERT INTO categories (name) VALUES ('Other');
  END IF;
END
$$;
All I need is to create table and insert some init data to it if table does not exist.
",<sql><postgresql>,1084,0,20,247,1,2,7,75,39611,0.0,51,3,18,2019-04-09 15:45,2019-04-09 15:49,2019-04-09 15:49,0.0,0.0,Basic,14
57586148,Handle same function running and processing the same data at the same time,"I have a php system that that allow customer to buy things (make an order) from our system using e-wallet (store credit).
here's the database example
**sales_order**
+--------+-------+----------+--------+--------------+-----------+
|order_id| price |product_id| status |already_refund|customer_id|
+--------+-------+----------+--------+--------------+-----------+
|   1    | 1000  |    1     |canceled|      1       |     2     |
|   2    | 2000  |    2     |pending |      0       |     2     |
|   3    | 3000  |    3     |complete|      0       |     1     | 
+--------+-------+----------+--------+--------------+-----------+
**ewallet**
+-----------+-------+
|customer_id|balance|
+-----------+-------+
|     1     | 43200 |
|     2     | 22500 |
|     3     | 78400 |
+-----------+-------+
table sales_order contain the order that customer made, the column already_refund is for a flag that canceled order already refunded.
I'm running a cron every 5 minutes to check if order with status pending can be canceled and after that it can refund the money to the customer ewallet
function checkPendingOrders(){
   $orders = $this-&gt;orderCollection-&gt;filter(['status'=&gt;'pending']);
   foreach($orders as $order){
     //check if order is ready to be canceled
     $isCanceled = $this-&gt;isCanceled($order-&gt;getId());
     if($isCanceled === false) continue;
     if($order-&gt;getAlreadyRefund() == '0'){ // check if already refund
       $order-&gt;setAlredyRefund('1')-&gt;save();
       $this-&gt;refund($order-&gt;getId()); //refund the money to customer ewallet
     }
     $order-&gt;setStatus('canceled')-&gt;save();
   }
}
The problem the 2 different cron schedule can  process the same data at the same time using this function and it will make  the refund process can be called twice , so the customer will receive double refund amount. How can i handle this kind of problem, when a 2 same function running at the same time to process same data ? the if clause that i made can't handle this kind of issue
update
i've tried to use microtime in session as validation and lock the table row in MySQL, so at the beginning i set the variable to contain the microtime , than when i stored in a unique session generated by order_id , and then i add a condition to match the microtime value with the session before locking the Table Row and update my ewallet table
function checkPendingOrders(){
   $orders = $this-&gt;orderCollection-&gt;filter(['status'=&gt;'pending']);
   foreach($orders as $order){
     //assign unique microtime to session
     $mt = round(microtime(true) * 1000);
     if(!isset($_SESSION['cancel'.$order-&gt;getId()])) $_SESSION['cancel'.$order-&gt;getId()] = $mt;
     //check if order is ready to be canceled
     $isCanceled = $this-&gt;isCanceled($order-&gt;getId());
     if($isCanceled === false) continue;
     if($order-&gt;getAlreadyRefund() == '0'){ // check if already refund
       $order-&gt;setAlreadyRefund('1')-&gt;save();
       //check if microtime is the same as the first one that running
       if($_SESSION['cancel'.$order-&gt;getId()] == $mt){
        //update using lock row
        $this-&gt;_dbConnection-&gt;beginTransaction(); 
        $sqlRaws[] =  ""SELECT * FROM ewallet WHERE customer_id = "".$order-&gt;getCustomerId()."" FOR UPDATE;"";
        $sqlRaws[] =  ""UPDATE ewallet SET balance =(balance+"".$order-&gt;getPrice()."") WHERE customer_id = "".$order-&gt;getCustomerId()."";"";
        foreach ($sqlRaws as $sqlRaw) {
          $this-&gt;_dbConnection-&gt;query($sqlRaw);
        }
        $this-&gt;_dbConnection-&gt;commit(); 
       }
     }
     unset($_SESSION['cancel'.$order-&gt;getId()]);
     $order-&gt;setStatus('canceled')-&gt;save();
   }
}
but the problem still persist when i'm doing a strees test, because there is a case when the same function process the same data at the same microtime and start mysql transaction at the same exact time
",<php><mysql><concurrency><race-condition>,3922,0,60,469,0,4,15,73,879,0.0,41,11,18,2019-08-21 7:02,2019-08-21 7:51,,0.0,,Advanced,32
48282321,ValueError: Cannot convert column into bool,"I'm trying build a new column on dataframe as below:
l = [(2, 1), (1,1)]
df = spark.createDataFrame(l)
def calc_dif(x,y):
    if (x&gt;y) and (x==1):
        return x-y
dfNew = df.withColumn(""calc"", calc_dif(df[""_1""], df[""_2""]))
dfNew.show()
But, I get:
Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-2807412651452069487.py"", line 346, in &lt;module&gt;
Exception: Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-2807412651452069487.py"", line 334, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 38, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 36, in calc_dif
  File ""/usr/hdp/current/spark2-client/python/pyspark/sql/column.py"", line 426, in __nonzero__
    raise ValueError(""Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', ""
ValueError: Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.
Why It happens? How can I fix It? 
",<apache-spark><pyspark><apache-spark-sql>,976,0,18,455,2,4,9,47,60521,0.0,0,4,18,2018-01-16 13:24,2018-01-16 13:29,2018-01-16 13:29,0.0,0.0,Basic,2
49275868,How to filter JSON Array in Django JSONField,"i am getting crazy with filtering a (postgres) JSONField in Django 2.0.3.
The json is stored as an array. E.g.
tasks = [{""task"":""test"",""level"":""10""},{""task"":""test 123"",""level"":""20""}]
What i've tried:
myModel.objects.filter(""tasks__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0__contains""=""test"")
myModel.objects.filter(""tasks__0__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0_x__contains""=""test"")
myModel.objects.filter(""tasks__0_x__task__contains""=""test"")
What goes wrong?
What i want to do is a icontains - but as i already read there is not support for icontains on jsonfields in Django right now...
",<python><django><postgresql><django-models>,635,0,6,912,1,9,27,55,17487,0.0,65,4,18,2018-03-14 10:54,2018-03-14 11:21,2018-12-27 9:09,0.0,288.0,Basic,3
51541561,Module not found: Can't resolve 'dns' in pg/lib,"I did create-react-app and also installed sequelize and pg. But when I do npm start, I get the following error - 
./node_modules/pg/lib/connection-parameters.js
Module not found: Can't resolve 'dns' in '/Users/vedant/Web Dev/device_psql/node_modules/pg/lib'
Here is the App.js file - 
import React, { Component } from 'react';
const Sequelize = require('sequelize');
const sequelize = new Sequelize('postgres', 'postgres', 'password', {
host: 'localhost',
dialect: 'postgres',
pool: {
  max: 5,
  min: 0,
  acquire: 30000,
  idle: 10000
},
// http://docs.sequelizejs.com/manual/tutorial/querying.html#operators
operatorsAliases: false
});
class App extends Component {
render() {
  return (
    &lt;div &gt;
      &lt;p&gt;Test&lt;/p&gt;
    &lt;/div&gt;
  );
}
}
export default App;
Also, in the package.json file, I have sequelize and pg. What could be the problem? I have tried to delete the node_modules folder and doing npm install, but no luck.
Thanks in advance.
",<node.js><reactjs><postgresql><sequelize.js>,970,1,28,277,2,5,13,61,33692,0.0,3,4,18,2018-07-26 14:40,2018-08-10 8:13,,15.0,,Basic,9
56343611,Insert sqlite flutter without freezing the interface,"I'm trying to insert a lot of rows (about 12k or more) in a sqlite memory database using flutter.
I get data from the API and use a compute function in order to process data from Json.
Now I need to add these data to a database in memory, in order to do so I use a transaction with a batch.
batchInsertEventSong(List&lt;EventSong&gt; rows) async {
   Database db = await instance.database;
   db.transaction((txn) async {
      Batch batch = txn.batch();
      for (var song in rows) {
         Map&lt;String, dynamic&gt; row = {
           DatabaseHelper.columnIdSong: song.id,
           DatabaseHelper.columnTitle: song.title,
           DatabaseHelper.columnInterpreter: song.interpreter
         };
         batch.insert(table, row);
       }
      batch.commit();
   }
}
But this function is blocking my UI during insertions, I tried also with compute but I can't pass the class db or the class batch.
I hadn't clear how to execute this process in another thread or (since I can't use isolates) executing without blocking my UI.
Any advice?
",<sqlite><flutter><dart><sqflite>,1047,0,15,415,0,5,11,38,7011,0.0,8,1,18,2019-05-28 13:42,2019-07-21 16:14,2019-07-21 16:14,54.0,54.0,Intermediate,23
65043926,Entity Framework Core Many to Many change navigation property names,"I have a table called &quot;LogBookSystemUsers&quot; and I want to setup many to many functionality in EF Core 5. I almost have it working but the problem is my ID columns are named SystemUserId and LogBookId but when EF does the join it tries to use SystemUserID and LogBookID. This is my current configuration code:
modelBuilder.Entity&lt;SystemUser&gt;()
            .HasMany(x =&gt; x.LogBooks)
            .WithMany(x =&gt; x.SystemUsers)
            .UsingEntity(x =&gt;
            {
                x.ToTable(&quot;LogBookSystemUsers&quot;, &quot;LogBooks&quot;);
            });
I tried this:
modelBuilder.Entity&lt;SystemUser&gt;()
            .HasMany(x =&gt; x.LogBooks)
            .WithMany(x =&gt; x.SystemUsers)
            .UsingEntity&lt;Dictionary&lt;string, object&gt;&gt;(&quot;LogBookSystemUsers&quot;,
                x =&gt; x.HasOne&lt;LogBook&gt;().WithMany().HasForeignKey(&quot;LogBookId&quot;),
                x =&gt; x.HasOne&lt;SystemUser&gt;().WithMany().HasForeignKey(&quot;SystemUserId&quot;),
                x =&gt; x.ToTable(&quot;LogBookSystemUsers&quot;, &quot;LogBooks&quot;));
But that just adds two new columns instead of setting the names of the current columns.
This is all database first. I don't want to have to use a class for the many to many table because I do this all over in my project and I don't want a bunch of useless classes floating around. Any ideas?
",<c#><postgresql><entity-framework-core><ef-core-5.0>,1411,0,18,1862,2,20,38,62,8508,0.0,313,1,18,2020-11-27 21:15,2020-11-28 3:55,2020-11-28 3:55,1.0,1.0,Intermediate,18
52993172,how to restart mysql service from mysql docker container,"I'm using tommylau/mysql docker image which provides mysql installed.
But after all i can't find how to restart mysql service inside running container (there is no mysql service or /etc/init.d/mysqld)
Any idea how to find how to restart mysql?
",<mysql><bash><docker>,244,0,0,401,1,3,9,37,39403,0.0,1,2,18,2018-10-25 15:35,2018-10-25 17:36,,0.0,,Basic,9
61783623,How to ping/test connection to SQL Server without software (like through cmd)?,"Is there a way to test a connection to a SQL Server through the command line or without any extra software? I've tried the ping and telnet methods shown in this article, but they both fail to find my SQL Server. Note that the connection is fine, I can connect to the server through SSMS, but it would be useful to be able to troubleshoot the connection otherwise.
For example, we have people working from home and I want to be able to test the connection to the database server without having to install SSMS on their machine.
The server name looks like: SERVER\SQLEXPRESS
And I tried ping SERVER\SQLEXPRESS (Ping request could not find host) and telnet SERVER 1433 (Could not open connection to the host, on port 1433)
EDIT: I can ping the server just fine ping SERVER
EDIT2: Everything I have tried I test with both the name and the ip. The DNS is fine so it shouldn't cause an issue to use one or the other
",<sql-server><cmd><ping>,910,1,4,629,2,8,20,51,47177,,60,1,18,2020-05-13 19:51,2021-01-28 14:46,2021-01-28 14:46,260.0,260.0,Basic,9
64412515,How to show generated SQL / raw SQL in TypeORM queryBuilder,"I developed typeorm querybuilder. For the purpose of debugging, I'd like to show the generated SQL query.
I tested printSql() method, but it didn't show any SQL query.
const Result = await this.attendanceRepository
  .createQueryBuilder(&quot;attendance&quot;)
  .innerJoin(&quot;attendance.child&quot;, &quot;child&quot;)
  .select([&quot;attendance.childId&quot;,&quot;child.class&quot;,&quot;CONCAT(child.firstName, child.lastName)&quot;])
  .where(&quot;attendance.id= :id&quot;, { id: id })
  .printSql()
  .getOne()
console.log(Result);
It returned the following:
Attendance { childId: 4, child: Child { class: 'S' } }
My desired result is to get the generated SQL query.
Is there any wrong point? Is there any good way to get the SQL query?
",<sql><typeorm>,748,0,13,4847,10,49,79,73,44685,0.0,448,2,18,2020-10-18 11:02,2020-10-18 13:06,2020-10-18 13:06,0.0,0.0,Basic,3
60711576,How do I write SQLAlchemy test fixtures for FastAPI applications,"I am writing a FastAPI application that uses a SQLAlchemy database. I have copied the example from the FastAPI documentation, simplifying the database schema for concisions' sake. The complete source is at the bottom of this post.
This works. I can run it with uvicorn sql_app.main:app and interact with the database via the Swagger docs. When it runs it creates a test.db in the working directory.
Now I want to add a unit test. Something like this.
from fastapi import status
from fastapi.testclient import TestClient
from pytest import fixture
from main import app
@fixture
def client() -&gt; TestClient:
    return TestClient(app)
def test_fast_sql(client: TestClient):
    response = client.get(&quot;/users/&quot;)
    assert response.status_code == status.HTTP_200_OK
    assert response.json() == []
Using the source code below, this takes the test.db in the working directory as the database. Instead I want to create a new database for every unit test that is deleted at the end of the test.
I could put the global database.engine and database.SessionLocal inside an object that is created at runtime, like so:
    class UserDatabase:
        def __init__(self, directory: Path):
            directory.mkdir(exist_ok=True, parents=True)
            sqlalchemy_database_url = f&quot;sqlite:///{directory}/store.db&quot;
            self.engine = create_engine(
                sqlalchemy_database_url, connect_args={&quot;check_same_thread&quot;: False}
            )
            self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
            models.Base.metadata.create_all(bind=self.engine)
but I don't know how to make that work with main.get_db, since the Depends(get_db) logic ultimately assumes database.engine and database.SessionLocal are available globally.
I'm used to working with Flask, whose unit testing facilities handle all this for you. I don't know how to write it myself. Can someone show me the minimal changes I'd have to make in order to generate a new database for each unit test in this framework?
The complete source of the simplified FastAPI/SQLAlchemy app is as follows.
database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
models.py
from sqlalchemy import Column, Integer, String
from database import Base
class User(Base):
    __tablename__ = &quot;users&quot;
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String)
    age = Column(Integer)
schemas.py
from pydantic import BaseModel
class UserBase(BaseModel):
    name: str
    age: int
class UserCreate(UserBase):
    pass
class User(UserBase):
    id: int
    class Config:
        orm_mode = True
crud.py
from sqlalchemy.orm import Session
import schemas
import models
def get_user(db: Session, user_id: int):
    return db.query(models.User).filter(models.User.id == user_id).first()
def get_users(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.User).offset(skip).limit(limit).all()
def create_user(db: Session, user: schemas.UserCreate):
    db_user = models.User(name=user.name, age=user.age)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user
main.py1
from typing import List
from fastapi import Depends, FastAPI, HTTPException
from sqlalchemy.orm import Session
import schemas
import models
import crud
from database import SessionLocal, engine
models.Base.metadata.create_all(bind=engine)
app = FastAPI()
# Dependency
def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()
@app.post(&quot;/users/&quot;, response_model=schemas.User)
def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    return crud.create_user(db=db, user=user)
@app.get(&quot;/users/&quot;, response_model=List[schemas.User])
def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    users = crud.get_users(db, skip=skip, limit=limit)
    return users
@app.get(&quot;/users/{user_id}&quot;, response_model=schemas.User)
def read_user(user_id: int, db: Session = Depends(get_db)):
    db_user = crud.get_user(db, user_id=user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail=&quot;User not found&quot;)
    return db_user
",<sqlalchemy><fastapi><test-fixture>,4587,2,135,16484,12,75,112,67,11667,0.0,627,1,18,2020-03-16 18:42,2020-04-05 15:56,2020-04-05 15:56,20.0,20.0,Intermediate,31
60925289,"HikariPool-1 - Connection marked as broken because of SQLSTATE(08006), ErrorCode(0)","After we setup PostgreSQL server with SSL certificates we get this error very often. 
This happens on the flows with a lot of interaction with the database(update a lot of entries/insert)   
HikariPool-1 - Connection marked as broken because of SQLSTATE(08006), ErrorCode(0)
j.n.SocketException: Connection reset by peer (Write failed)
    at j.n.SocketOutputStream.socketWrite0(SocketOutputStream.java)
    at j.n.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
    at j.n.SocketOutputStream.write(SocketOutpu`enter code here`tStream.java:150)
    at s.s.s.SSLSocketOutputRecord.deliver(SSLSocketOutputRecord.java:320)
    at s.s.s.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:983)
    ... 63 common frames omitted
Wrapped by: j.n.s.SSLProtocolException: Connection reset by peer (Write failed)
    at s.security.ssl.Alert.createSSLException(Alert.java:126)
    at s.s.s.TransportContext.fatal(TransportContext.java:321)
    at s.s.s.TransportContext.fatal(TransportContext.java:264)
    at s.s.s.TransportContext.fatal(TransportContext.java:259)
    at s.s.s.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:988)
    at j.i.BufferedOutputStream.write(BufferedOutputStream.java:123)
    at j.i.FilterOutputStream.write(FilterOutputStream.java:108)
    at o.p.core.PGStream.send(PGStream.java:252)
    at o.p.c.v.QueryExecutorImpl.sendParse(QueryExecutorImpl.java:1440)
    at o.p.c.v.QueryExecutorImpl.sendOneQuery(QueryExecutorImpl.java:1767)
    at o.p.c.v.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1328)
    at o.p.c.v.QueryExecutorImpl.execute(QueryExecutorImpl.java:300)
    ... 56 common frames omitted
Wrapped by: o.p.u.PSQLException: An I/O error occurred while sending to the backend
",<postgresql><hikaricp>,1731,0,23,351,2,3,11,69,33014,0.0,1,3,18,2020-03-30 6:42,2020-04-29 17:16,,30.0,,Advanced,39
56772235,How do I pass an environment variable into a .sql file?,"I'm setting environment variables in a Dockerfile which I want to reference in a .sql file. That file is in /docker-entrypoint-initdb.d since initdb will run it.
How can pass an environment variable? (e.g. SELECT * FROM $myEnvironmentVariableHere;)
",<postgresql><docker><environment-variables>,249,0,4,181,1,1,4,59,11396,,0,1,18,2019-06-26 11:56,2019-06-26 14:04,,0.0,,Intermediate,15
49490640,"How to read "".gz"" compressed file using spark DF or DS?","I have a compressed file with .gz format, Is it possible to read the file directly using spark DF/DS? 
Details : File is csv with tab delimited.
",<apache-spark><apache-spark-sql><gzip><apache-spark-dataset>,145,0,0,573,4,9,24,37,40282,0.0,24,1,18,2018-03-26 11:43,2018-03-27 1:17,2018-03-27 1:17,1.0,1.0,Basic,3
54128295,"""ModuleNotFoundError: No module named 'pysqlcipher3'"" error while using Python 3.7 on windows 10","I am trying to decrypt one database file using Python 3.7. To decrypt it, I have to use pysqlcipher3 version for python 3.7. To install it, I have tried by using both commands:
pip3 install pysqlcipher3
and
pip install pysqlcipher3
and both the commands have showed successful installation of the pysqlcipher package. However, when I try to import pysqlcipher3 in my Python project by using this line:
from pysqlcipher3 import dbapi2 as sqlite
it displays this error:
ModuleNotFoundError: No module named 'pysqlcipher3
I have checked various GitHub projects, but none of them provide a clear working solution. The Python packages website says to install libsqlcipher in your OS but this time the issue is same, no documentation and link regarding for the installation of libsqlcipher for Windows 10. Can anyone provide me with proper installation steps, or any document, or any video tutorial, regarding the same? Or is there some issue with the import statement?
",<python><python-3.x><pysqlcipher>,964,0,9,1240,0,18,37,40,5195,,171,7,18,2019-01-10 12:03,2020-10-10 6:30,,639.0,,Basic,14
48160627,Partition data for efficient joining for Spark dataframe/dataset,"I need to join many DataFrames together based on some shared key columns. For a key-value RDD, one can specify a partitioner so that data points with same key are shuffled to same executor so joining is more efficient (if one has shuffle related operations before the join). Can the same thing can be done on Spark DataFrames or DataSets?
",<apache-spark><apache-spark-sql><partitioning><apache-spark-dataset>,339,0,2,1192,2,14,30,80,38848,0.0,120,2,18,2018-01-09 2:22,2018-01-09 7:08,2018-01-09 13:32,0.0,0.0,Intermediate,22
57371164,Django + Postgres: A string literal cannot contain NUL (0x00) characters,"I'm syncing a large amount of data and I'm getting this error back: A string literal cannot contain NUL (0x00) characters. Obviously this is a postgres problem, but I'm not quite sure how to solve it. Is there a way to strip null characters out at the Django model level? I have a large set of fields that I'm syncing.
",<python><django><postgresql>,319,0,1,4786,8,48,84,42,18800,0.0,459,2,18,2019-08-06 7:41,2019-08-06 8:39,,0.0,,Basic,14
49081896,converting a struct to a json when querying athena,"I have an athena table which I did not create or manage, but can query. one of the fields is a struct type. for the sake of the example let's suppose it looks like this:
my_field struct&lt;a:string,
                b:string,
                c:struct&lt;d:string,e:string&gt;
                &gt;
Now, I know how to query specific fields within this struct. But in one of my queries I need to extract the complete struct. so I just use:
select my_field from my_table
and the result looks like a string:
{a=aaa, b=bbb, c={d=ddd, e=eee}}
I want to get the result as a json string:
{""a"":""aaa"", ""b"":""bbb"",""c"":{""d"":""ddd"", ""e"":""eee""}}
this string will then be processed by another application, this is why i need it in json format.
How can I achieve this?
EDIT:
Better still, is there a way to query the struct in a way that flattens it? so the result would look like:
a   |   b   |   c.d  |  c.e   |
-------------------------------
aaa |   bbb |   ddd  |  eee   |
",<sql><json><struct><amazon-athena>,958,0,10,3342,6,25,32,69,12934,,31,3,18,2018-03-03 7:41,2018-03-04 22:35,,1.0,,Basic,10
48519633,SQLAlchemy cast boolean column to int,"I have a table of user records and I wish to see the average when some condition is met. the condition column is a boolean.
In Postgresql I could cast it easily:
select id, avg(cond::INT) from table group by id;
But in SQLAlchemy I couldn't find anything equivalent to ::INT.
How do i deal with the type conversion?
I have
orm_query(Table.id, func.avg(Table.cond))
which, of course, returns an error.
",<python><postgresql><sqlalchemy>,401,0,3,1891,3,21,37,78,34334,0.0,79,2,18,2018-01-30 10:52,2018-01-30 10:58,2018-01-30 10:58,0.0,0.0,Basic,1
62127983,ERROR 1449 (HY000): The user specified as a definer ('mysql.infoschema'@'localhost') does not exist,"I am trying to list all tables from mysql database on ubuntu os. But I am getting this error all time;
mysql&gt; use mysql;
Database changed
mysql&gt; show tables;
ERROR 1449 (HY000): The user specified as a definer ('mysql.infoschema'@'localhost') does not exist
I have checked my mysql version:
mysql  Ver 8.0.20 for Linux on x86_64 (MySQL Community Server - GPL)
So it seems it is last version of mysql.
How can I fix this error?
Please help
",<mysql><ubuntu>,445,0,5,1049,1,24,50,58,38635,0.0,3,5,18,2020-06-01 8:08,2020-06-01 9:26,2020-06-01 9:26,0.0,0.0,Basic,14
54667071,MySQL 5.6 - table locks even when ALGORITHM=inplace is used,"I'm running the following ALTER command on a MySQL 5.6 database on a large table with 60 million rows:
ALTER TABLE `large_table` ADD COLUMN `note` longtext NULL, 
ALGORITHM=INPLACE, LOCK=NONE;
Despite specifying both ALGORITHM=INPLACE and LOCK=NONE, the table gets locked and essentially takes down the app until the migration is complete.
I verified that the table was indeed locked by checking the value of the In_use column on the output of the SHOW OPEN TABLES command. It was set to 1.
From what I gather in the MySQL documentation, this operation should not be locking the table. And, MySQL is supposed to fail the command if it is not able to proceed without a lock. I upgraded the database to MySQL 5.7 to see if it's any better, but I face the same problem on 5.7 too.
Is this an expected behavior? How do I find out what's going wrong here?
",<mysql><table-locking>,851,0,8,13480,18,79,130,62,16731,0.0,438,2,18,2019-02-13 9:47,2019-02-18 18:45,2019-02-18 18:45,5.0,5.0,Intermediate,23
56676303,mismatched input 'from' expecting <EOF> SQL,"I am running a process on Spark which uses SQL for the most part. In one of the workflows I am getting the following error:
mismatched input 'from' expecting 
The code is
 select a.ACCOUNT_IDENTIFIER,a.LAN_CD, a.BEST_CARD_NUMBER,  
 decision_id, 
 case when a.BEST_CARD_NUMBER = 1 then 'Y' else 'N' end as best_card_excl_flag 
 from (select a.ACCOUNT_IDENTIFIER,a.LAN_CD, a. decision_id row_number()
 over (partition by CUST_GRP_MBRP_ID 
    order by coalesce(BEST_CARD_RANK,999)) as BEST_CARD_NUMBER 
 from Accounts_Inclusions_Exclusions_Flagged a) a 
I cannot figure out what the error is for the life of me
I've tried checking for comma errors or unexpected brackets but that doesn't seem to be the issue.
",<sql><apache-spark-sql>,709,0,7,179,1,1,3,81,239712,,0,4,17,2019-06-19 21:44,2019-06-19 21:54,,0.0,,Basic,10
50483132,How to secure access from App Service To Azure Sql Database using virtual network?,"Scenario
I want to use virtual network in order to limit access to Azure Database only from my App Service, so that I can turn of ""Allow access to App Services"" in firewall settings
What I have done:
I went to App Service -> Networking -> VNET Integration -> Setup -> Create New Virtual Network
I've created new VNET with default settings.
When VNET was created I went to App Service -> Networking -> VNET Integration and ensured that the VNET is connected
I went to SQL Firewall settigs -> Virtual Network -> Add existing Virtual Newtork and selected my VNET. I've left default subnet and address space: ""default / 10.0.0.0/24"" and I've left IgnoreMissingServiceEndpoint flag unchecked.
I can now see Microsoft.Sql service endpoint in my VNET:
Question
However, I'm still getting 
  SqlException: Cannot open server 'my-sqlserver' requested by the
  login. Client with IP address '52.233..' is not allowed to
  access the server.:
What am I missing?
",<azure><azure-sql-database><azure-web-app-service><firewall><azure-virtual-network>,951,2,0,25638,30,155,304,40,9425,0.0,236,4,17,2018-05-23 8:11,2018-05-23 8:34,2018-05-23 8:34,0.0,0.0,Intermediate,29
51050590,Run SQL script after start of SQL Server on docker,"I have a Dockerfile with below code
FROM microsoft/mssql-server-windows-express
COPY ./create-db.sql .
ENV ACCEPT_EULA=Y
ENV sa_password=##$wo0RD!
CMD sqlcmd -i create-db.sql
and I can create image but when I run container with the image I don't see created database on the SQL Server because the script is executed before SQL Server was started.
Can I do that the script will be execute after start the service with SQL Server?
",<t-sql><docker><dockerfile><sql-server-express>,429,0,5,1769,4,15,23,72,32817,0.0,161,2,17,2018-06-26 20:02,2018-06-26 21:14,2018-06-26 21:14,0.0,0.0,Basic,14
56759646,Docker Laravel Mysql: could not find driver,"When I run docker-compose up and do some composer commands, I get error
  In Connection.php line 664: could not find driver (SQL: select id, name from users  In Connector.php line 68: could not find driver ...
Why cant Laravel connect to mysql? I can do mysql -h db in docker-compose exec web bash and it works. 
My setup
docker-compose.yml
version: '3'
services:
  web:
    build: ./webserver
    ports:
      - ""80:80""
      - ""443:443""
    volumes:
      - //docker/dockertest/webserver/app:/var/www/vhosts/app
    links:
      - db
    command:
       - /usr/local/bin/apache2_install_composer_dependencies.sh
  db:
    image: mysql:8.0
    container_name: db
    ports:
      - ""3306:3306""
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_DATABASE: myDb
      MYSQL_USER: user
      MYSQL_PASSWORD: test
      MYSQL_ROOT_PASSWORD: test
    volumes:
      - //docker/dockertest/install/db_dump:/docker-entrypoint-initdb.d
      - persistent:/var/lib/mysql
    networks:
      - default
  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    links:
      - db:db
    ports:
      - 8000:80
    environment:
      MYSQL_USER: user
      MYSQL_PASSWORD: test
      MYSQL_ROOT_PASSWORD: test
volumes:
  persistent:
Laravel .env (I reference these values in config/database.php)
...
DB_CONNECTION=mysql
#host points to Docker container
DB_HOST=db
DB_PORT=3306
DB_DATABASE=myDb
DB_USERNAME=user
DB_PASSWORD=test
....
webserver/Dockerfile
FROM php:7.2.19-apache-stretch
# Configure Apache server
COPY config_apache/sites-available /etc/apache2/sites-available
# Create symlink in sites-enabled
WORKDIR /etc/apache2/sites-enabled
RUN ln -s /etc/apache2/sites-available/app.conf app.conf
RUN mkdir -p /var/www/vhosts/app/logs
COPY /build_files/install_composer_dependencies.sh /usr/local/bin/apache2_install_composer_dependencies.sh
RUN apt-get update -y &amp;&amp; apt-get install -y  curl nano libapache2-mod-geoip git zip unzip mysql-client
# Install Composer
RUN curl -o /tmp/composer-setup.php https://getcomposer.org/installer \
&amp;&amp; curl -o /tmp/composer-setup.sig https://composer.github.io/installer.sig \
# Verify installer
&amp;&amp; php -r ""if (hash('SHA384', file_get_contents('/tmp/composer-setup.php')) !== trim(file_get_contents('/tmp/composer-setup.sig'))) { unlink('/tmp/composer-setup.php'); echo 'Invalid installer' . PHP_EOL; exit(1); }"" \
&amp;&amp; php /tmp/composer-setup.php --no-ansi --install-dir=/usr/local/bin --filename=composer --snapshot \
&amp;&amp; rm -f /tmp/composer-setup.*
RUN a2enmod rewrite
RUN a2enmod geoip
RUN service apache2 restart
",<php><mysql><laravel><docker><docker-compose>,2621,2,93,1306,1,16,30,39,27464,0.0,1512,2,17,2019-06-25 17:57,2019-06-25 18:11,2019-06-26 12:17,0.0,1.0,Basic,14
65117930,Postgresql ERROR: cannot drop columns from view,"I'm new to postgresql. Can anyone suggest the reason and solution to this error? However it works if I select an extra sum(s.length) but i don't won't this in my results.
code:
create or replace view q1(&quot;group&quot;,album,year)
as
select g.name, a.title, a.year
from Groups g, Albums a, Songs s
where a.made_by = g.id and s.on_album = a.id
group by a.title, g.name, a.year
order by sum(s.length) desc 
limit 1
;
Error message:
ERROR:  cannot drop columns from view
",<sql><postgresql>,470,0,9,267,2,3,5,41,15079,,0,1,17,2020-12-02 23:59,2020-12-03 1:07,,1.0,,Intermediate,18
52067058,How to autogenerate UUID for Postgres in Python?,"I'm trying to create objects in Postgres db.
I'm using this approach https://websauna.org/docs/narrative/modelling/models.html#uuid-primary-keys
class Role(Base):
    __tablename__ = 'role'
    # Pass `binary=False` to fallback to CHAR instead of BINARY
    id = sa.Column(UUIDType(binary=False), primary_key=True)
But when I create object 
user_role = Role(name='User')
db.session.add(user_role)
db.session.commit()
I have the following error:
sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) null value in column ""id"" violates not-null constraint
Looks like I didn't provide any ID. So, how I can make the database auto-generate it or generate on my own?
",<python><postgresql><sqlalchemy><uuid>,665,2,9,4991,7,54,95,78,21639,0.0,517,4,17,2018-08-28 22:33,2018-08-28 23:20,2018-08-29 6:56,0.0,1.0,Basic,9
54024991,Error in mysqldump: mysqldump: [ERROR] unknown variable 'database=someDb',"I am getting the following error:  
 mysqldump: [ERROR] unknown variable 'database=myDB1'
when I run this command:
 mysqldump -u root -p myDB2  &gt; someFile
There is a db by the name myDB1 and of course, a db by the name myDB2.
This worked before (or so I think). I do not recollect changing my.cnf or any other configuration file. 
",<mysql>,334,0,2,966,1,9,25,64,12414,,81,1,17,2019-01-03 15:11,2019-03-16 13:22,,72.0,,Basic,14
57107452,Why does Postgres choose index scan instead of index seek to fetch one record by primary key?,"I have the following table:
create table documents 
(
     id serial not null primary key,
     key varchar(50) not null,
     document jsonb
);
It has over 100M records and when I run a query to get 1 record by primary key:
 select * from documents where id = 20304050
It uses the index scan to get it:
Index Scan using documents_pkey on documents (cost=0.57..8.59 rows=1 width=533) (actual time=0.010..0.011 rows=0 loops=1)
  Index Cond: (id = 20304050)
Planning Time: 0.070 ms
Execution Time: 0.024 ms
Why does Postgres choose to use an index scan instead of an index seek?
Edit:
I came from the SQL Server world where it was a distinction between an index scan and an index seek. In Postgres, there is no such thing as an index seek.
",<sql><postgresql>,738,0,11,2251,0,19,22,50,3989,,84,1,17,2019-07-19 7:22,2022-10-26 4:42,2022-10-26 4:42,1195.0,1195.0,Intermediate,23
59843904,Postgres full text search and spelling mistakes (aka fuzzy full text search),"I have a scenario, where I have data for informal communications that I need to be able to search. Therefore I want full text search, but I also to make sense of spelling mistakes. Question is how do I take spelling mistakes into account in order to be able to do fuzzy full text search??
This is very briefly discussed in Postgres Full Text Search is Good Enough where the article discusses misspelling.
So I have built a table of ""documents"", created indexes etc.
CREATE TABLE data (
  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, 
  text TEXT NOT NULL);
I can create an additional column of type tsvector and index accordingly...
alter table data 
  add column search_index tsvector 
  generated always as (to_tsvector('english', coalesce(text, ''))) 
  STORED;
create index search_index_idx on data using gin (search_index);
I have for example, some text where the data says ""baloon"", but someone may search ""balloon"", so I insert two rows (one deliberately misspelled)...
insert into data (text) values ('baloon');
insert into data (text) values ('balloon');
select * from data;
id |  text   | search_index
----+---------+--------------
 1 | baloon  | 'baloon':1
 2 | balloon | 'balloon':1
... and perform full text searches against the data...
select * from data where search_index @@ plainto_tsquery('balloon');
 id |  text   | search_index
----+---------+--------------
  2 | balloon | 'balloon':1
(1 row)
But I don't get back results for the misspelled version ""baloon""... So using the suggestion in the linked article I've built a lookup table of all the words in my lexicon as follows...
  ""you may obtain good results by appending the similar lexeme to your tsquery""
CREATE TABLE data_words AS SELECT word FROM ts_stat('SELECT to_tsvector(''simple'', text) FROM data');
CREATE INDEX data_words_idx ON data_words USING GIN (word gin_trgm_ops);
... and I can search for similar words which may have been misspelled
select word, similarity(word, 'balloon') as similarity from data_words where similarity(word, 'balloon') > 0.4 order by similarity(word, 'balloon');
  word   | similarity
---------+------------
 baloon  |  0.6666667
 balloon |          1
... but how do I actually include misspelled words in my query?
Isn't this what the article above means?
select plainto_tsquery('balloon' || ' ' || (select string_agg(word, ' ') from data_words where similarity(word, 'balloon') &gt; 0.4));
         plainto_tsquery
----------------------------------
 'balloon' &amp; 'baloon' &amp; 'balloon'
(1 row)
... plugged into an actual search, and I get no rows!
select * from data where text @@ plainto_tsquery('balloon' || ' ' || (select string_agg(word, ' ') from data_words where similarity(word, 'balloon') &gt; 0.4));
select * from data where search_index @@ phraseto_tsquery('baloon balloon'); -- no rows returned
I'm not sure where I'm going wrong here - can any shed any light? I feel like I'm super close to getting this going...?
",<postgresql><full-text-search><fuzzy-search>,2953,1,40,4829,3,29,42,56,6413,0.0,81,2,17,2020-01-21 15:03,2020-01-21 17:29,2020-01-21 17:29,0.0,0.0,Advanced,40
