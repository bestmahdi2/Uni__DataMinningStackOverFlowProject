QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
49097346,How to speed up a SQLAlchemy Query?,"I have a table of over 10 million rows. There are roughly 50+ columns. The table stores sensors data/parameters. Let's say that I need to query data for the whole day or 86,400 seconds. It would need take roughly 20 or more seconds to complete this query.
I have added individual indices on a few columns such as recordTimestamp(that store when the data is captured), deviceId(the identification of the sensor), positionValid(whether GPS geolocation is valid). Then I added a composite index which includes all three columns. 
Below is my query:
t1 = time.time()
conn = engine.connect()
select_statement = select([Datatable]).where(and_(
    Datatable.recordTimestamp &gt;= start_date,
    Datatable.recordTimestamp &lt;= end_date,
    Datatable.deviceId == device_id,
    Datatable.positionValid != None,
    Datatable.recordTimestamp % query_interval == 0))
lol_data = conn.execute(select_statement).fetchall()    
conn.close() 
t2 = time.time()
time_taken = t2 - t1
print('Select: ' + time_taken)
Below is my EXPLAIN ANALYZE statement:
EXPLAIN ANALYZE SELECT datatable.id, datatable.""createdAt"", datatable.""analogInput01"", datatable.""analogInput02"", datatable.""analogInput03"", datatable.""analogInput04"", datatable.""analogInput05"", datatable.""analogInput06"", datatable.""analogInput07"", datatable.""canEngineRpm"", datatable.""canEngineTemperature"", datatable.""canFuelConsumedLiters"", datatable.""canFuelLevel"", datatable.""canVehicleMileage"", datatable.""deviceId"", datatable.""deviceTemperature"", datatable.""deviceInternalVoltage"", datatable.""deviceExternalVoltage"", datatable.""deviceAntennaCut"", datatable.""deviceEnum"", datatable.""deviceVehicleMileage"", datatable.""deviceSimSignal"", datatable.""deviceSimStatus"", datatable.""iButton01"", datatable.""iButton02"", datatable.""recordSequence"", datatable.""recordTimestamp"", datatable.""accelerationAbsolute"", datatable.""accelerationBrake"", datatable.""accelerationBump"", datatable.""accelerationTurn"", datatable.""accelerationX"", datatable.""accelerationY"", datatable.""accelerationZ"", datatable.""positionAltitude"", datatable.""positionDirection"", datatable.""positionSatellites"", datatable.""positionSpeed"", datatable.""positionLatitude"", datatable.""positionLongitude"", datatable.""positionHdop"", datatable.""positionMovement"", datatable.""positionValid"", datatable.""positionEngine"" FROM datatable WHERE datatable.""recordTimestamp"" &gt;= 1519744521 AND datatable.""recordTimestamp"" &lt;= 1519745181 AND datatable.""deviceId"" = '864495033990901' AND datatable.""positionValid"" IS NOT NULL AND datatable.""recordTimestamp"" % 1 = 0;
Below is the result from EXPLAIN ANALYZE of the SELECT:
Index Scan using ""ix_dataTable_recordTimestamp"" on dataTable (cost=0.44..599.35 rows=5 width=301) (actual time=0.070..10.487 rows=661 loops=1)
Index Cond: ((""recordTimestamp"" &gt;= 1519744521) AND (""recordTimestamp"" &lt;= 1519745181))
Filter: ((""positionValid"" IS NOT NULL) AND ((""deviceId"")::text = '864495033990901'::text) AND ((""recordTimestamp"" % 1) = 0))
Rows Removed by Filter: 6970
Planning time: 0.347 ms
Execution time: 10.658 ms
Whereas below is the result from time taken calculated by Python:
Select:  47.98712515830994 
JSON:  0.19731807708740234
Below is my code profiling:
10302 function calls (10235 primitive calls) in 12.612 seconds
Ordered by: cumulative time
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:882(execute)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/sql/elements.py:267(_execute_on_connection)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1016(_execute_clauseelement)
    1    0.000    0.000   12.592   12.592 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1111(_execute_context)
    1    0.000    0.000   12.590   12.590 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/default.py:506(do_execute)
    1   12.590   12.590   12.590   12.590 {method 'execute' of 'psycopg2.extensions.cursor' objects}
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1113(fetchall)
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1080(_fetchall_impl)
    1    0.008    0.008    0.017    0.017 {method 'fetchall' of 'psycopg2.extensions.cursor' objects}
",<python><postgresql><sqlalchemy><psycopg2>,4692,0,36,1347,2,12,26,45,15193,,37,3,14,2018-03-04 16:04,2018-04-03 7:06,,30.0,,Intermediate,23,"<python><postgresql><sqlalchemy><psycopg2>, How to speed up a SQLAlchemy Query?, I have a table of over 10 million rows. There are roughly 50+ columns. The table stores sensors data/parameters. Let's say that I need to query data for the whole day or 86,400 seconds. It would need take roughly 20 or more seconds to complete this query.
I have added individual indices on a few columns such as recordTimestamp(that store when the data is captured), deviceId(the identification of the sensor), positionValid(whether GPS geolocation is valid). Then I added a composite index which includes all three columns. 
Below is my query:
t1 = time.time()
conn = engine.connect()
select_statement = select([Datatable]).where(and_(
    Datatable.recordTimestamp &gt;= start_date,
    Datatable.recordTimestamp &lt;= end_date,
    Datatable.deviceId == device_id,
    Datatable.positionValid != None,
    Datatable.recordTimestamp % query_interval == 0))
lol_data = conn.execute(select_statement).fetchall()    
conn.close() 
t2 = time.time()
time_taken = t2 - t1
print('Select: ' + time_taken)
Below is my EXPLAIN ANALYZE statement:
EXPLAIN ANALYZE SELECT datatable.id, datatable.""createdAt"", datatable.""analogInput01"", datatable.""analogInput02"", datatable.""analogInput03"", datatable.""analogInput04"", datatable.""analogInput05"", datatable.""analogInput06"", datatable.""analogInput07"", datatable.""canEngineRpm"", datatable.""canEngineTemperature"", datatable.""canFuelConsumedLiters"", datatable.""canFuelLevel"", datatable.""canVehicleMileage"", datatable.""deviceId"", datatable.""deviceTemperature"", datatable.""deviceInternalVoltage"", datatable.""deviceExternalVoltage"", datatable.""deviceAntennaCut"", datatable.""deviceEnum"", datatable.""deviceVehicleMileage"", datatable.""deviceSimSignal"", datatable.""deviceSimStatus"", datatable.""iButton01"", datatable.""iButton02"", datatable.""recordSequence"", datatable.""recordTimestamp"", datatable.""accelerationAbsolute"", datatable.""accelerationBrake"", datatable.""accelerationBump"", datatable.""accelerationTurn"", datatable.""accelerationX"", datatable.""accelerationY"", datatable.""accelerationZ"", datatable.""positionAltitude"", datatable.""positionDirection"", datatable.""positionSatellites"", datatable.""positionSpeed"", datatable.""positionLatitude"", datatable.""positionLongitude"", datatable.""positionHdop"", datatable.""positionMovement"", datatable.""positionValid"", datatable.""positionEngine"" FROM datatable WHERE datatable.""recordTimestamp"" &gt;= 1519744521 AND datatable.""recordTimestamp"" &lt;= 1519745181 AND datatable.""deviceId"" = '864495033990901' AND datatable.""positionValid"" IS NOT NULL AND datatable.""recordTimestamp"" % 1 = 0;
Below is the result from EXPLAIN ANALYZE of the SELECT:
Index Scan using ""ix_dataTable_recordTimestamp"" on dataTable (cost=0.44..599.35 rows=5 width=301) (actual time=0.070..10.487 rows=661 loops=1)
Index Cond: ((""recordTimestamp"" &gt;= 1519744521) AND (""recordTimestamp"" &lt;= 1519745181))
Filter: ((""positionValid"" IS NOT NULL) AND ((""deviceId"")::text = '864495033990901'::text) AND ((""recordTimestamp"" % 1) = 0))
Rows Removed by Filter: 6970
Planning time: 0.347 ms
Execution time: 10.658 ms
Whereas below is the result from time taken calculated by Python:
Select:  47.98712515830994 
JSON:  0.19731807708740234
Below is my code profiling:
10302 function calls (10235 primitive calls) in 12.612 seconds
Ordered by: cumulative time
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:882(execute)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/sql/elements.py:267(_execute_on_connection)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1016(_execute_clauseelement)
    1    0.000    0.000   12.592   12.592 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1111(_execute_context)
    1    0.000    0.000   12.590   12.590 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/default.py:506(do_execute)
    1   12.590   12.590   12.590   12.590 {method 'execute' of 'psycopg2.extensions.cursor' objects}
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1113(fetchall)
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1080(_fetchall_impl)
    1    0.008    0.008    0.017    0.017 {method 'fetchall' of 'psycopg2.extensions.cursor' objects}
","<patron><postgresql><sqlalchemy><psycopg2>, speed sqlalchemi query?, table 10 million rows. roughly 50+ columns. table store senior data/parameter. let' say need query data whole day 86,400 seconds. would need take roughly 20 second complete query. ad individu india column recordtimestamp(that store data captured), devised(th identify senior), positionvalid(wyeth up geology valid). ad composite index include three columns. query: to = time.time() corn = engine.connect() select_stat = select([datatable]).where(and( datatable.recordtimestamp &it;= start_date, datatable.recordtimestamp &it;= end_date, datatable.devised == device_id, datatable.positionvalid != none, datatable.recordtimestamp % query_interv == 0)) lol_data = corn.execute(select_statement).fetchall() corn.close() to = time.time() time_taken = to - to print('select: ' + time_taken) explain analyze statement: explain analyze select datatable.id, datatable.""created"", datatable.""analoginput01"", datatable.""analoginput02"", datatable.""analoginput03"", datatable.""analoginput04"", datatable.""analoginput05"", datatable.""analoginput06"", datatable.""analoginput07"", datatable.""canenginerpm"", datatable.""canenginetemperature"", datatable.""canfuelconsumedliters"", datatable.""canfuellevel"", datatable.""canvehiclemileage"", datatable.""devised"", datatable.""devicetemperature"", datatable.""deviceinternalvoltage"", datatable.""deviceexternalvoltage"", datatable.""deviceantennacut"", datatable.""deviceenum"", datatable.""devicevehiclemileage"", datatable.""devicesimsignal"", datatable.""devicesimstatus"", datatable.""ibutton01"", datatable.""ibutton02"", datatable.""recordsequence"", datatable.""recordtimestamp"", datatable.""accelerationabsolute"", datatable.""accelerationbrake"", datatable.""accelerationbump"", datatable.""accelerationturn"", datatable.""accelerationx"", datatable.""accelerationy"", datatable.""accelerationz"", datatable.""positionaltitude"", datatable.""positiondirection"", datatable.""positionsatellites"", datatable.""positionspeed"", datatable.""positionlatitude"", datatable.""positionlongitude"", datatable.""positionhdop"", datatable.""positionmovement"", datatable.""positionvalid"", datatable.""positionengine"" data datatable.""recordtimestamp"" &it;= 1519744521 datatable.""recordtimestamp"" &it;= 1519745181 datatable.""devised"" = '864495033990901' datatable.""positionvalid"" null datatable.""recordtimestamp"" % 1 = 0; result explain analyze select: index scan use ""ix_datatable_recordtimestamp"" data (cost=0.44..599.35 rows=5 width=301) (actual time=0.070..10.487 rows=661 loops=1) index cond: ((""recordtimestamp"" &it;= 1519744521) (""recordtimestamp"" &it;= 1519745181)) filter: ((""positionvalid"" null) ((""devised"")::text = '864495033990901'::text) ((""recordtimestamp"" % 1) = 0)) row remove filter: 6970 plan time: 0.347 ms execute time: 10.658 ms where result time taken call patron: select: 47.98712515830994 son: 0.19731807708740234 code profiting: 10302 function call (10235 print calls) 12.612 second order by: curl time call bottom pencil custom pencil filename:linen(function) 1 0.000 0.000 12.595 12.595 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/base.by:882(execute) 1 0.000 0.000 12.595 12.595 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/sal/elements.by:267(_execute_on_connection) 1 0.000 0.000 12.595 12.595 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/base.by:1016(_execute_clauseelement) 1 0.000 0.000 12.592 12.592 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/base.by:1111(_execute_context) 1 0.000 0.000 12.590 12.590 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/default.by:506(do_execute) 1 12.590 12.590 12.590 12.590 {method 'execute' 'psycopg2.extensions.curses' objects} 1 0.000 0.000 0.017 0.017 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/result.by:1113(fetchall) 1 0.000 0.000 0.017 0.017 /users/afeezaziz/projects/bursa/backed/end/limb/python3.6/site-packages/sqlalchemy/engine/result.by:1080(_fetchall_impl) 1 0.008 0.008 0.017 0.017 {method 'fetchall' 'psycopg2.extensions.curses' objects}"
49395898,Does MySQL support partial indexes?,"Partial indexes only include a subset of the rows of a table.
I've been able to create partial indexes in Oracle, DB2, PostgreSQL, and SQL Server. For example, in SQL Server I can create the index as:
create index ix1_case on client_case (date) 
  where status = 'pending';
This index is cheap since it does not include all 5 million rows of the table, but only the pending cases, that should not exceed a thousand rows. 
How do I do it in MySQL?
",<mysql><sql><database><indexing><relational-database>,447,0,2,46447,10,41,77,46,12817,0.0,2923,3,14,2018-03-20 23:44,2018-03-21 2:03,2018-04-18 15:58,1.0,29.0,Basic,3,"<mysql><sql><database><indexing><relational-database>, Does MySQL support partial indexes?, Partial indexes only include a subset of the rows of a table.
I've been able to create partial indexes in Oracle, DB2, PostgreSQL, and SQL Server. For example, in SQL Server I can create the index as:
create index ix1_case on client_case (date) 
  where status = 'pending';
This index is cheap since it does not include all 5 million rows of the table, but only the pending cases, that should not exceed a thousand rows. 
How do I do it in MySQL?
","<myself><sal><database><indexing><relations-database>, myself support partial indexes?, partial index include sunset row table. i'v all great partial index oracle, by, postgresql, sal server. example, sal server great index as: great index ix1_cas client_cas (date) state = 'pending'; index cheap since include 5 million row table, end cases, exceed thousand rows. myself?"
59943384,Datatype for phone numbers in postgresql,"I am new to postgresql so can anyone tell me that is there any specific datatype to store phone numbers in postgresql while creating table in pgadmin or is it just string?
",<postgresql><database-design><sqldatatypes>,172,0,0,393,2,6,14,45,40259,0.0,4,2,14,2020-01-28 6:43,2020-01-28 7:35,2020-01-28 7:52,0.0,0.0,Basic,1,"<postgresql><database-design><sqldatatypes>, Datatype for phone numbers in postgresql, I am new to postgresql so can anyone tell me that is there any specific datatype to store phone numbers in postgresql while creating table in pgadmin or is it just string?
","<postgresql><database-design><sqldatatypes>, datatyp phone number postgresql, new postgresql anyone tell specie datatyp store phone number postgresql great table pgadmin string?"
48865416,How to use the latest sqlite3 version in python,"I need to use sqlite version 3.8 or higher with python in Amazon Linux. 
I updated my sqlite installation to the latest version:
$ sqlite3 -version
3.22.0 2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d
I also updated my pysqlite version
pip install --upgrade pysqlite
However, my pysqlite still only seems to support sqlite version 3.7:
$ python
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt;
&gt;&gt;&gt; from pysqlite2 import dbapi2 as sqlite
&gt;&gt;&gt; sqlite.version
'2.8.3'
&gt;&gt;&gt; sqlite.sqlite_version
'3.7.17'
How can I update the sqlite python API to support a newer version of sqlite?
",<python><sqlite><pysqlite>,716,0,15,331,1,2,11,73,13026,0.0,2,4,14,2018-02-19 11:40,2018-07-17 15:37,,148.0,,Basic,3,"<python><sqlite><pysqlite>, How to use the latest sqlite3 version in python, I need to use sqlite version 3.8 or higher with python in Amazon Linux. 
I updated my sqlite installation to the latest version:
$ sqlite3 -version
3.22.0 2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d
I also updated my pysqlite version
pip install --upgrade pysqlite
However, my pysqlite still only seems to support sqlite version 3.7:
$ python
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt;
&gt;&gt;&gt; from pysqlite2 import dbapi2 as sqlite
&gt;&gt;&gt; sqlite.version
'2.8.3'
&gt;&gt;&gt; sqlite.sqlite_version
'3.7.17'
How can I update the sqlite python API to support a newer version of sqlite?
","<patron><quite><pysqlite>, use latest sqlite3 version patron, need use quite version 3.8 higher patron amazon line. update quite instal latest version: $ sqlite3 -version 3.22.0 2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d also update pysqlit version pp instal --upgrade pysqlit however, pysqlit still seem support quite version 3.7: $ patron &it;&it;&it; import sqlite3 &it;&it;&it; sqlite3.very '2.6.0' &it;&it;&it; sqlite3.sqlite_vers '3.7.17' &it;&it;&it; &it;&it;&it; pysqlite2 import dbapi2 quite &it;&it;&it; quite.very '2.8.3' &it;&it;&it; quite.sqlite_vers '3.7.17' update quite patron apt support newer version quite?"
62602720,String to Date migration from Spark 2.0 to 3.0 gives Fail to recognize 'EEE MMM dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter,"I have a date string from a source in the format 'Fri May 24 00:00:00 BST 2019' that I would convert to a date and store in my dataframe as '2019-05-24' using code like my example which works for me under spark 2.0
from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime
df = spark.createDataFrame([(&quot;Fri May 24 00:00:00 BST 2019&quot;,)], ['date_str'])
df2 = df.select('date_str', to_date(from_unixtime(unix_timestamp('date_str', 'EEE MMM dd HH:mm:ss zzz yyyy'))).alias('date'))
df2.show(1, False)
In my sandbox environment I've updated to spark 3.0 and now get the following error for the above code, is there a new method of doing this in 3.0 to convert my string to a date
: org.apache.spark.SparkUpgradeException: You may get a different
result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM
dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter.
You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the
behavior before Spark 3.0.
You can form a valid datetime pattern with the guide from
https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
",<apache-spark><pyspark><apache-spark-sql>,1113,2,4,191,1,1,5,46,59095,0.0,0,4,14,2020-06-26 20:55,2020-06-27 7:46,,1.0,,Basic,3,"<apache-spark><pyspark><apache-spark-sql>, String to Date migration from Spark 2.0 to 3.0 gives Fail to recognize 'EEE MMM dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter, I have a date string from a source in the format 'Fri May 24 00:00:00 BST 2019' that I would convert to a date and store in my dataframe as '2019-05-24' using code like my example which works for me under spark 2.0
from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime
df = spark.createDataFrame([(&quot;Fri May 24 00:00:00 BST 2019&quot;,)], ['date_str'])
df2 = df.select('date_str', to_date(from_unixtime(unix_timestamp('date_str', 'EEE MMM dd HH:mm:ss zzz yyyy'))).alias('date'))
df2.show(1, False)
In my sandbox environment I've updated to spark 3.0 and now get the following error for the above code, is there a new method of doing this in 3.0 to convert my string to a date
: org.apache.spark.SparkUpgradeException: You may get a different
result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM
dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter.
You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the
behavior before Spark 3.0.
You can form a valid datetime pattern with the guide from
https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
","<apache-spark><spark><apache-spark-sal>, string date migrate spark 2.0 3.0 give fail record 'see mmm did he:mm:is ze yyyy' pattern datetimeformatter, date string source format 'fro may 24 00:00:00 but 2019' would convert date store datafram '2019-05-24' use code like example work spark 2.0 spark.sal.fact import to_date, unix_timestamp, from_unixtim of = spark.createdataframe([(&quit;fro may 24 00:00:00 but 2019&quit;,)], ['date_str']) of = of.select('date_str', to_date(from_unixtime(unix_timestamp('date_str', 'see mmm did he:mm:is ze yyyy'))).alias('date')) of.show(1, false) sandbag environs i'v update spark 3.0 get follow error code, new method 3.0 convert string date : org.apache.spark.sparkupgradeexception: may get differ result due upgrade spark 3.0: fail record 'see mmm did he:mm:is ze yyyy' pattern datetimeformatter. set spark.sal.legacy.timeparserpolici legacy restore behavior spark 3.0. form valid datetim pattern guide http://spark.apache.org/docs/latest/sal-red-daytime-pattern.html"
53579444,"Efficient text preprocessing using PySpark (clean, tokenize, stopwords, stemming, filter)","Recently, I began to learn the spark on the book ""Learning Spark"". In theory, everything is clear, in practice, I was faced with the fact that I first need to preprocess the text, but there were no actual tips on this topic.
The first thing that I took into account is that it is now preferable to use Dataframe instead of RDD, so my preprocessing attempt was made on dataframes.
Required operations:
Clearing text from punctuation (regexp_replace)
Tokenization (Tokenizer)
Delete stop words (StopWordsRemover)
Stematization (SnowballStemmer)
Filtering short words (udf)
My code is:
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, lower, regexp_replace
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from nltk.stem.snowball import SnowballStemmer
spark = SparkSession.builder \
    .config(""spark.executor.memory"", ""3g"") \
    .config(""spark.driver.cores"", ""4"") \
    .getOrCreate()
df = spark.read.json('datasets/entitiesFull/full').select('id', 'text')
# Clean text
df_clean = df.select('id', (lower(regexp_replace('text', ""[^a-zA-Z\\s]"", """")).alias('text')))
# Tokenize text
tokenizer = Tokenizer(inputCol='text', outputCol='words_token')
df_words_token = tokenizer.transform(df_clean).select('id', 'words_token')
# Remove stop words
remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')
df_words_no_stopw = remover.transform(df_words_token).select('id', 'words_clean')
# Stem text
stemmer = SnowballStemmer(language='english')
stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))
df_stemmed = df_words_no_stopw.withColumn(""words_stemmed"", stemmer_udf(""words_clean"")).select('id', 'words_stemmed')
# Filter length word &gt; 3
filter_length_udf = udf(lambda row: [x for x in row if len(x) &gt;= 3], ArrayType(StringType()))
df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))
Processing takes a very long time, the size of the entire document is 60 GB. Does it make sense to use RDD? Will caching help? How can I optimize preprocessing?
First I tested the implementation on the local computer, then I will try on the cluster. Local computer - Ubuntu RAM 6Gb, 4 CPU. Any alternative solution is also welcome. Thanks!
",<python><apache-spark><pyspark><apache-spark-sql><text-processing>,2270,0,30,169,1,1,7,64,15602,0.0,0,1,14,2018-12-02 10:40,2020-07-08 21:59,,584.0,,Intermediate,23,"<python><apache-spark><pyspark><apache-spark-sql><text-processing>, Efficient text preprocessing using PySpark (clean, tokenize, stopwords, stemming, filter), Recently, I began to learn the spark on the book ""Learning Spark"". In theory, everything is clear, in practice, I was faced with the fact that I first need to preprocess the text, but there were no actual tips on this topic.
The first thing that I took into account is that it is now preferable to use Dataframe instead of RDD, so my preprocessing attempt was made on dataframes.
Required operations:
Clearing text from punctuation (regexp_replace)
Tokenization (Tokenizer)
Delete stop words (StopWordsRemover)
Stematization (SnowballStemmer)
Filtering short words (udf)
My code is:
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, lower, regexp_replace
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from nltk.stem.snowball import SnowballStemmer
spark = SparkSession.builder \
    .config(""spark.executor.memory"", ""3g"") \
    .config(""spark.driver.cores"", ""4"") \
    .getOrCreate()
df = spark.read.json('datasets/entitiesFull/full').select('id', 'text')
# Clean text
df_clean = df.select('id', (lower(regexp_replace('text', ""[^a-zA-Z\\s]"", """")).alias('text')))
# Tokenize text
tokenizer = Tokenizer(inputCol='text', outputCol='words_token')
df_words_token = tokenizer.transform(df_clean).select('id', 'words_token')
# Remove stop words
remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')
df_words_no_stopw = remover.transform(df_words_token).select('id', 'words_clean')
# Stem text
stemmer = SnowballStemmer(language='english')
stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))
df_stemmed = df_words_no_stopw.withColumn(""words_stemmed"", stemmer_udf(""words_clean"")).select('id', 'words_stemmed')
# Filter length word &gt; 3
filter_length_udf = udf(lambda row: [x for x in row if len(x) &gt;= 3], ArrayType(StringType()))
df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))
Processing takes a very long time, the size of the entire document is 60 GB. Does it make sense to use RDD? Will caching help? How can I optimize preprocessing?
First I tested the implementation on the local computer, then I will try on the cluster. Local computer - Ubuntu RAM 6Gb, 4 CPU. Any alternative solution is also welcome. Thanks!
","<patron><apache-spark><spark><apache-spark-sal><text-processing>, effect text preprocess use spark (clean, tokenize, stopford, steaming, filter), recently, began learn spark book ""learn spark"". theory, every clear, practice, face fact first need preprocess text, actual tip topic. first thing took account prefer use datafram instead red, preprocess attempt made dataframes. require operations: clear text puncture (regexp_replace) token (tokenizer) delete stop word (stopwordsremover) seat (snowballstemmer) filter short word (utf) code is: spark.sal import sparkles spark.sal.fact import utf, col, lower, regexp_replac spark.my.feature import tokenizer, stopwordsremov not.stem.snowballs import snowballstemm spark = sparksession.build \ .confirm(""spark.executor.memory"", ""g"") \ .confirm(""spark.driver.comes"", ""4"") \ .getorcreate() of = spark.read.son('datasets/entitiesfull/full').select('id', 'text') # clean text df_clean = of.select('id', (lower(regexp_replace('text', ""[^a-a-z\\s]"", """")).alias('text'))) # token text token = tokenizer(inputcol='text', outputcol='words_token') df_words_token = tokenizer.transform(df_clean).select('id', 'words_token') # remove stop word remove = stopwordsremover(inputcol='words_token', outputcol='words_clean') df_words_no_stopw = removed.transform(df_words_token).select('id', 'words_clean') # stem text steamer = snowballstemmer(language='english') stemmer_udf = utf(labia tokens: [steamer.stem(token) token tokens], arraytype(stringtype())) df_stem = df_words_no_stopw.withcolumn(""words_stemmed"", stemmer_udf(""words_clean"")).select('id', 'words_stemmed') # filter length word &it; 3 filter_length_udf = utf(labia row: [x x row len(x) &it;= 3], arraytype(stringtype())) df_final_word = df_stemmed.withcolumn('words', filter_length_udf(col('words_stemmed'))) process take long time, size enter document 60 go. make sens use red? each help? optic preprocessing? first test implement local computer, try cluster. local compute - bunt ram go, 4 cup. alter slut also welcome. thanks!"
55455166,Special character (Hawaiian 'Okina) leads to weird string behavior,"The Hawaiian quote has some weird behavior in T-SQL when using it in conjunction with string functions. What's going on here? Am I missing something? Do other characters suffer from this same problem?
SELECT UNICODE(N'ʻ') -- Returns 699 as expected.
SELECT REPLACE(N'""ʻ', '""', '_') -- Returns ""ʻ, I expected _ʻ
SELECT REPLACE(N'aʻ', 'a', '_') -- Returns aʻ, I expected _ʻ
SELECT REPLACE(N'""ʻ', N'ʻ', '_') -- Returns __, I expected ""_
SELECT REPLACE(N'-', N'ʻ', '_') -- Returns -, I expected -
Also, strange when used in a LIKE for example:
DECLARE @table TABLE ([Name] NVARCHAR(MAX))
INSERT INTO
    @table
VALUES
    ('John'),
    ('Jane')
SELECT
    *
FROM
    @table
WHERE
    [Name] LIKE N'%ʻ%' -- This returns both records. I expected none.
",<sql-server><t-sql><unicode><collation>,746,1,23,2601,3,24,41,74,826,0.0,49,2,14,2019-04-01 12:29,2019-04-01 13:44,2019-04-04 20:23,0.0,3.0,Basic,2,"<sql-server><t-sql><unicode><collation>, Special character (Hawaiian 'Okina) leads to weird string behavior, The Hawaiian quote has some weird behavior in T-SQL when using it in conjunction with string functions. What's going on here? Am I missing something? Do other characters suffer from this same problem?
SELECT UNICODE(N'ʻ') -- Returns 699 as expected.
SELECT REPLACE(N'""ʻ', '""', '_') -- Returns ""ʻ, I expected _ʻ
SELECT REPLACE(N'aʻ', 'a', '_') -- Returns aʻ, I expected _ʻ
SELECT REPLACE(N'""ʻ', N'ʻ', '_') -- Returns __, I expected ""_
SELECT REPLACE(N'-', N'ʻ', '_') -- Returns -, I expected -
Also, strange when used in a LIKE for example:
DECLARE @table TABLE ([Name] NVARCHAR(MAX))
INSERT INTO
    @table
VALUES
    ('John'),
    ('Jane')
SELECT
    *
FROM
    @table
WHERE
    [Name] LIKE N'%ʻ%' -- This returns both records. I expected none.
","<sal-server><t-sal><unicorn><collection>, special character (hawaiian 'skin) lead weird string behavior, hawaiian quit weird behavior t-sal use conduct string functions. what' go here? miss something? character suffer problem? select unicorn(n'ʻ') -- return 699 expected. select replace(n'""ʻ', '""', '_') -- return ""ʻ, expect of select replace(n'a', 'a', '_') -- return a, expect of select replace(n'""ʻ', n'ʻ', '_') -- return of, expect ""_ select replace(n'-', n'ʻ', '_') -- return -, expect - also, strange use like example: declare @table table ([name] nvarchar(max)) insert @table value ('john'), ('jane') select * @table [name] like n'%ʻ%' -- return records. expect none."
63609570,Mysql 'VALUES function' is deprecated,"This is my python code which prints the sql query.
def generate_insert_statement(column_names, values_format, table_name, items, insert_template=INSERT_TEMPLATE, ):
    return insert_template.format(
        column_names=&quot;,&quot;.join(column_names),
        values=&quot;,&quot;.join(
            map(
                lambda x: generate_raw_values(values_format, x),
                items
            )
        ),
        table_name=table_name,
        updates_on=create_updates_on_columns(column_names)
    )
query = generate_insert_statement(table_name=property['table_name'],
        column_names=property['column_names'],
        values_format=property['values_format'], items=batch)
        print(query) #here
        execute_commit(query)
When printing the Mysql query my Django project shows following error in the terminal:
'VALUES function' is deprecated and will be removed in a future release. Please use an alias (INSERT INTO ... VALUES (...) AS alias) and replace VALUES(col) in the ON DUPLICATE KEY UPDATE clause with alias.col instead
Mysql doumentation does not say much about it.What does this mean and how to can i rectify it.
INSERT_TEMPLATE = &quot;INSERT INTO {table_name} ({column_names}) VALUES {values} ON DUPLICATE KEY UPDATE {updates_on};&quot;
",<python><mysql>,1276,0,19,187,0,1,9,80,9766,0.0,10,1,14,2020-08-27 5:09,2020-08-27 5:18,2020-08-27 5:18,0.0,0.0,Basic,2,"<python><mysql>, Mysql 'VALUES function' is deprecated, This is my python code which prints the sql query.
def generate_insert_statement(column_names, values_format, table_name, items, insert_template=INSERT_TEMPLATE, ):
    return insert_template.format(
        column_names=&quot;,&quot;.join(column_names),
        values=&quot;,&quot;.join(
            map(
                lambda x: generate_raw_values(values_format, x),
                items
            )
        ),
        table_name=table_name,
        updates_on=create_updates_on_columns(column_names)
    )
query = generate_insert_statement(table_name=property['table_name'],
        column_names=property['column_names'],
        values_format=property['values_format'], items=batch)
        print(query) #here
        execute_commit(query)
When printing the Mysql query my Django project shows following error in the terminal:
'VALUES function' is deprecated and will be removed in a future release. Please use an alias (INSERT INTO ... VALUES (...) AS alias) and replace VALUES(col) in the ON DUPLICATE KEY UPDATE clause with alias.col instead
Mysql doumentation does not say much about it.What does this mean and how to can i rectify it.
INSERT_TEMPLATE = &quot;INSERT INTO {table_name} ({column_names}) VALUES {values} ON DUPLICATE KEY UPDATE {updates_on};&quot;
","<patron><myself>, myself 'value function' deprecated, patron code print sal query. def generate_insert_statement(column_names, values_format, table_name, items, insert_template=insert_template, ): return insert_template.format( column_names=&quit;,&quit;.join(column_names), values=&quit;,&quit;.join( map( labia x: generate_raw_values(values_format, x), item ) ), table_name=table_name, updates_on=create_updates_on_columns(column_names) ) query = generate_insert_statement(table_name=property['table_name'], column_names=property['column_names'], values_format=property['values_format'], items=batch) print(query) #here execute_commit(query) print myself query django project show follow error terminal: 'value function' degree remove future release. pleas use asia (insert ... value (...) alias) replace values(col) public key update class alias.col instead myself document say much it.what mean rectify it. insert_templ = &quit;insert {table_name} ({column_names}) value {values} public key update {updates_on};&quit;"
52915923,Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured. SPRING,"I've already checked all the similar questions and every answer says that I need to specify a driverClassName which I already do. Here is my application.yml:
spring:
  application:
    name: cibus-backend
  datasource:
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/Cibus?useSSL=true
    username: root
    password: 1234567890
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: update
      properties:
      hibernate:
        format_sql: true
        type: trace
    database-platform: org.hibernate.dialect.MySQL5InnoDBDialect
    database: mysql
logging:
  level:
    org:
      hibernate:
        type: trace
Am I missing something? The weird thing is that a classmate of mine who has the same code can start the app perfectly well. This is why I think it has something to do with the path. Maybe spring isn't accessing the yml file. I included it in src.main.resources which is the default place where Spring looks for it.
Here is the stacktrace:
Error starting ApplicationContext. To display the conditions report re-run         
your application with 'debug' enabled.
2018-10-21 10:13:15.657 ERROR 10356 --- [JavaFX-Launcher]             
o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured.
Reason: Failed to determine a suitable driver class
Action:
Consider the following:
    If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.
    If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).
Exception in Application init method
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:389)
    at com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:767)
Caused by: java.lang.RuntimeException: Exception in Application init method
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:912)
    at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$155(LauncherImpl.java:182)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:732)
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:197)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1267)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1124)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:372)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1089)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:859)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:780)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:412)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:333)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1277)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1265)
    at labtic.AppStarter.init(AppStarter.java:25)
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:841)
    ... 2 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:590)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:251)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1135)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1062)
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:818)
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:724)
    ... 30 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:582)
    ... 43 more
Caused by: org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:236)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:83)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)
    ... 44 more
Exception running application labtic.AppStarter
Here is the Gradle Build as I was asked to include it:
buildscript {
    ext {
        springBootVersion = '2.0.5.RELEASE'
    }
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath(""org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}"")
    }
}
apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'org.springframework.boot'
apply plugin: 'io.spring.dependency-management'
group = ''
version = '0.0.1-SNAPSHOT'
sourceCompatibility = 1.8
repositories {
    mavenCentral()
    jcenter()
}
dependencies {
    compile('org.springframework.boot:spring-boot-starter-data-jpa')
    runtime('mysql:mysql-connector-java')
    testCompile('org.springframework.boot:spring-boot-starter-test')
    // https://mvnrepository.com/artifact/mysql/mysql-connector-java
    compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.12'
    compileOnly 'org.projectlombok:lombok:1.18.2'
    annotationProcessor ""org.projectlombok:lombok:1.18.2""
}
Any ideas? Thank you in advance.
",<java><mysql><spring><spring-boot>,12522,1,161,604,1,10,19,59,40228,0.0,135,6,14,2018-10-21 13:42,2018-10-21 15:13,2018-10-21 15:13,0.0,0.0,Advanced,39,"<java><mysql><spring><spring-boot>, Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured. SPRING, I've already checked all the similar questions and every answer says that I need to specify a driverClassName which I already do. Here is my application.yml:
spring:
  application:
    name: cibus-backend
  datasource:
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/Cibus?useSSL=true
    username: root
    password: 1234567890
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: update
      properties:
      hibernate:
        format_sql: true
        type: trace
    database-platform: org.hibernate.dialect.MySQL5InnoDBDialect
    database: mysql
logging:
  level:
    org:
      hibernate:
        type: trace
Am I missing something? The weird thing is that a classmate of mine who has the same code can start the app perfectly well. This is why I think it has something to do with the path. Maybe spring isn't accessing the yml file. I included it in src.main.resources which is the default place where Spring looks for it.
Here is the stacktrace:
Error starting ApplicationContext. To display the conditions report re-run         
your application with 'debug' enabled.
2018-10-21 10:13:15.657 ERROR 10356 --- [JavaFX-Launcher]             
o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured.
Reason: Failed to determine a suitable driver class
Action:
Consider the following:
    If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.
    If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).
Exception in Application init method
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:389)
    at com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:767)
Caused by: java.lang.RuntimeException: Exception in Application init method
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:912)
    at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$155(LauncherImpl.java:182)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:732)
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:197)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1267)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1124)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:372)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1089)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:859)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:780)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:412)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:333)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1277)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1265)
    at labtic.AppStarter.init(AppStarter.java:25)
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:841)
    ... 2 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:590)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:251)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1135)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1062)
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:818)
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:724)
    ... 30 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:582)
    ... 43 more
Caused by: org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:236)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:83)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)
    ... 44 more
Exception running application labtic.AppStarter
Here is the Gradle Build as I was asked to include it:
buildscript {
    ext {
        springBootVersion = '2.0.5.RELEASE'
    }
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath(""org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}"")
    }
}
apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'org.springframework.boot'
apply plugin: 'io.spring.dependency-management'
group = ''
version = '0.0.1-SNAPSHOT'
sourceCompatibility = 1.8
repositories {
    mavenCentral()
    jcenter()
}
dependencies {
    compile('org.springframework.boot:spring-boot-starter-data-jpa')
    runtime('mysql:mysql-connector-java')
    testCompile('org.springframework.boot:spring-boot-starter-test')
    // https://mvnrepository.com/artifact/mysql/mysql-connector-java
    compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.12'
    compileOnly 'org.projectlombok:lombok:1.18.2'
    annotationProcessor ""org.projectlombok:lombok:1.18.2""
}
Any ideas? Thank you in advance.
","<cava><myself><spring><spring-boot>, fail configur datasource: 'curl' attribute specific ebbed datasourc could configured. spring, i'v already check similar question every answer say need specific driverclassnam already do. application.you: spring: application: name: sinus-backed datasource: driverclassname: com.myself.c.job.drive curl: job:myself://localhost:3306/sinus?useful=true surname: root password: 1234567890 pa: show-sal: true liberate: del-auto: update properties: liberate: format_sql: true type: trace database-platform: org.liberate.dialect.mysql5innodbdialect database: myself logging: level: org: liberate: type: trace miss something? weird thing classmat mine code start pp perfectly well. think cometh path. may spring access you file. include sac.main.resource default place spring look it. stacktrace: error start applicationcontext. display conduct report re-run applied 'debut' enabled. 2018-10-21 10:13:15.657 error 10356 --- [javafx-launched] o.s.b.d.loggingfailureanalysisreport : *************************** applied fail start *************************** description: fail configur datasource: 'curl' attribute specific ebbed datasourc could configured. reason: fail determine suitable driver class action: consider following: want ebbed database (he, hill derby), pleas put classpath. database set load particular profit may need active (no profit current active). except applied knit method cava.long.reflect.invocationtargetexcept sun.reflect.nativemethodaccessorimpl.invoked(n method) sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.cava:62) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.cava:43) cava.long.reflect.method.invoke(method.cava:498) com.sun.javafx.application.launcherimpl.launchapplicationwithargs(launcherimpl.cava:389) com.sun.javafx.application.launcherimpl.launchapplication(launcherimpl.cava:328) sun.reflect.nativemethodaccessorimpl.invoked(n method) sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.cava:62) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.cava:43) cava.long.reflect.method.invoke(method.cava:498) sun.launched.launcherhelper$helper.main(launcherhelper.cava:767) cause by: cava.long.runtimeexception: except applied knit method com.sun.javafx.application.launcherimpl.launchapplication1(launcherimpl.cava:912) com.sun.javafx.application.launcherimpl.labia$launchapplication$155(launcherimpl.cava:182) cava.long.thread.run(thread.cava:748) cause by: org.springframework.beans.factory.unsatisfieddependencyexception: error great bean name 'org.springframework.boot.autoconfigure.or.pa.hibernatejpaconfiguration': unsatisfied depend express construction parapet 0; nest except org.springframework.beans.factory.beancreationexception: error great bean name 'datasource' define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]: bean instant via factor method failed; nest except org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except org.springframework.boot.autoconfigure.job.datasourceproperties$datasourcebeancreationexception: fail determine suitable driver class org.springframework.beans.factory.support.constructorresolver.createargumentarray(constructorresolver.cava:732) org.springframework.beans.factory.support.constructorresolver.autowireconstructor(constructorresolver.cava:197) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.autowireconstructor(abstractautowirecapablebeanfactory.cava:1267) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbeaninstance(abstractautowirecapablebeanfactory.cava:1124) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:535) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:495) org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:317) org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:315) org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) org.springframework.beans.factory.support.constructorresolver.instantiateusingfactorymethod(constructorresolver.cava:372) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.instantiateusingfactorymethod(abstractautowirecapablebeanfactory.cava:1247) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbeaninstance(abstractautowirecapablebeanfactory.cava:1096) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:535) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:495) org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:317) org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:315) org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1089) org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:859) org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:550) org.springframework.boot.springapplication.refresh(springapplication.cava:780) org.springframework.boot.springapplication.refreshcontext(springapplication.cava:412) org.springframework.boot.springapplication.run(springapplication.cava:333) org.springframework.boot.springapplication.run(springapplication.cava:1277) org.springframework.boot.springapplication.run(springapplication.cava:1265) lactic.appstarter.knit(appstarter.cava:25) com.sun.javafx.application.launcherimpl.launchapplication1(launcherimpl.cava:841) ... 2 cause by: org.springframework.beans.factory.beancreationexception: error great bean name 'datasource' define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]: bean instant via factor method failed; nest except org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except org.springframework.boot.autoconfigure.job.datasourceproperties$datasourcebeancreationexception: fail determine suitable driver class org.springframework.beans.factory.support.constructorresolver.instantiateusingfactorymethod(constructorresolver.cava:590) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.instantiateusingfactorymethod(abstractautowirecapablebeanfactory.cava:1247) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbeaninstance(abstractautowirecapablebeanfactory.cava:1096) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:535) org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:495) org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:317) org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:315) org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) org.springframework.beans.factory.confirm.dependencydescriptor.resolvecandidate(dependencydescriptor.cava:251) org.springframework.beans.factory.support.defaultlistablebeanfactory.doresolvedependency(defaultlistablebeanfactory.cava:1135) org.springframework.beans.factory.support.defaultlistablebeanfactory.resolvedependency(defaultlistablebeanfactory.cava:1062) org.springframework.beans.factory.support.constructorresolver.resolveautowiredargument(constructorresolver.cava:818) org.springframework.beans.factory.support.constructorresolver.createargumentarray(constructorresolver.cava:724) ... 30 cause by: org.springframework.beans.beaninstantiationexception: fail instant [com.baxter.kari.hikaridatasource]: factor method 'datasource' threw exception; nest except org.springframework.boot.autoconfigure.job.datasourceproperties$datasourcebeancreationexception: fail determine suitable driver class org.springframework.beans.factory.support.simpleinstantiationstrategy.instantiate(simpleinstantiationstrategy.cava:185) org.springframework.beans.factory.support.constructorresolver.instantiateusingfactorymethod(constructorresolver.cava:582) ... 43 cause by: org.springframework.boot.autoconfigure.job.datasourceproperties$datasourcebeancreationexception: fail determine suitable driver class org.springframework.boot.autoconfigure.job.datasourceproperties.determinedriverclassname(datasourceproperties.cava:236) org.springframework.boot.autoconfigure.job.datasourceproperties.initializedatasourcebuilder(datasourceproperties.cava:176) org.springframework.boot.autoconfigure.job.datasourceconfiguration.createdatasource(datasourceconfiguration.cava:43) org.springframework.boot.autoconfigure.job.datasourceconfiguration$kari.datasource(datasourceconfiguration.cava:83) sun.reflect.nativemethodaccessorimpl.invoked(n method) sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.cava:62) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.cava:43) cava.long.reflect.method.invoke(method.cava:498) org.springframework.beans.factory.support.simpleinstantiationstrategy.instantiate(simpleinstantiationstrategy.cava:154) ... 44 except run applied lactic.appstart grade build ask include it: buildscript { ext { springbootvers = '2.0.5.release' } depositors { mavencentral() } depend { classpath(""org.springframework.boot:spring-boot-grade-plain:${springbootversion}"") } } apply plain: 'cava' apply plain: 'ellipse' apply plain: 'org.springframework.boot' apply plain: 'to.spring.dependency-management' group = '' version = '0.0.1-snapshot' sourcecompat = 1.8 depositors { mavencentral() center() } depend { compile('org.springframework.boot:spring-boot-started-data-pa') auntie('myself:myself-connection-cava') testcompile('org.springframework.boot:spring-boot-started-test') // http://mvnrepository.com/artifice/myself/myself-connection-cava compel group: 'myself', name: 'myself-connection-cava', version: '8.0.12' compileonli 'org.projectlombok:look:1.18.2' annotationprocessor ""org.projectlombok:look:1.18.2"" } ideas? thank advance."
53433285,MySQL Update or Rename a Key in JSON,"I'm having this json stored in db
{
    ""endDate"": ""2018-10-10"",
    ""startDate"": ""2017-09-05"", 
    ""oldKeyValue"": {
        ""foo"": 1000, 
        ""bar"": 2000, 
        ""baz"": 3000
    },
    ""anotherValue"": 0
}
How can I rename ""oldKeyValue"" key to ""newKeyValue"" without knowing the index of the key in an UPDATE query? I'm looking for something like this
UPDATE `my_table` SET `my_col` = JSON()
NOTE: only the key needs to change, the values (i.e. {""foo"": 1000, ""bar"": 2000, ""baz"": 3000}) should remain the same
",<mysql><json>,515,0,15,4117,20,72,131,53,13753,0.0,170,3,14,2018-11-22 14:39,2018-11-22 14:56,2018-11-22 14:56,0.0,0.0,Basic,2,"<mysql><json>, MySQL Update or Rename a Key in JSON, I'm having this json stored in db
{
    ""endDate"": ""2018-10-10"",
    ""startDate"": ""2017-09-05"", 
    ""oldKeyValue"": {
        ""foo"": 1000, 
        ""bar"": 2000, 
        ""baz"": 3000
    },
    ""anotherValue"": 0
}
How can I rename ""oldKeyValue"" key to ""newKeyValue"" without knowing the index of the key in an UPDATE query? I'm looking for something like this
UPDATE `my_table` SET `my_col` = JSON()
NOTE: only the key needs to change, the values (i.e. {""foo"": 1000, ""bar"": 2000, ""baz"": 3000}) should remain the same
","<myself><son>, myself update renal key son, i'm son store do { ""exudate"": ""2018-10-10"", ""startdate"": ""2017-09-05"", ""oldkeyvalue"": { ""foo"": 1000, ""bar"": 2000, ""bad"": 3000 }, ""anothervalue"": 0 } renal ""oldkeyvalue"" key ""newkeyvalue"" without know index key update query? i'm look cometh like update `my_table` set `my_col` = son() note: key need change, value (i.e. {""foo"": 1000, ""bar"": 2000, ""bad"": 3000}) remain"
51933189,Character encoding (UTF-8) in PowerShell session,"Hei all,
as a console/terminal enthusiast and database administrator (PostgreSQL) it is essential for me to work with the correct charcater encoding.
Therefore, I want my client console/terminal window always set to e.g. UTF-8.
Back with Windows' CMD.EXE this attempt was as easy as typing the command chcp 65001 to set the desired code page identifier.
Now, I am in the process of switching to PowerShell and setting the character encoding seems very odd, IMHO.
I've done some research on how to set the PowerShell session to UTF-8 and I figured out, that I need three steps/commmnds to accomplish that.
PS C:\&gt; $OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; [Console]::OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; chcp 65001
Despite the fact that the first two commands are not intuitive and hard to remember...
Leaving out one of them leads to something not working out properly!
Also, setting just one of them seems to have no effect to the others.
So, I must set all three for working with the PostgreSQL's psql database client.
Otherwise I run into encoding issues while exporting/importing data.
Now my question is: ""Why the heck? Isn't there an easier way to simply set the character encoding in PowerShell?""
Unfortunately, I did not find any plausible documentation myself about setting the character enconding!
Thanks in advance
/EDIT
The second comment by TheIncorrigible1 led me to the best answer fo far: Displaying Unicode in Powershell
- So one can set the whole PowerShell with two separated statements to the desired encoding (UTF-8).
PS C:\&gt; $OutputEncoding = [System.Console]::OutputEncoding = [System.Console]::InputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; $PSDefaultParameterValues['*:Encoding'] = 'utf8'
Explanation:
$OutputEncoding sets the encoding for e.g. | (piping) and/or communication between programs and/or processes.
[System.Console]::OutputEncoding sets the encoding for STDOUT and the console/terminal output.
[System.Console]::InputEncoding sets the encoding for STDIN or keyboard input.
$PSDefaultParameterValues['*:Encoding'] sets the encoding for all cmdlets that support the -Encoding option like e.g. Out-File -Encoding.
",<windows><postgresql><powershell><character-encoding>,2200,3,14,358,1,4,14,70,19962,0.0,922,1,14,2018-08-20 14:40,2020-11-04 12:01,,807.0,,Basic,2,"<windows><postgresql><powershell><character-encoding>, Character encoding (UTF-8) in PowerShell session, Hei all,
as a console/terminal enthusiast and database administrator (PostgreSQL) it is essential for me to work with the correct charcater encoding.
Therefore, I want my client console/terminal window always set to e.g. UTF-8.
Back with Windows' CMD.EXE this attempt was as easy as typing the command chcp 65001 to set the desired code page identifier.
Now, I am in the process of switching to PowerShell and setting the character encoding seems very odd, IMHO.
I've done some research on how to set the PowerShell session to UTF-8 and I figured out, that I need three steps/commmnds to accomplish that.
PS C:\&gt; $OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; [Console]::OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; chcp 65001
Despite the fact that the first two commands are not intuitive and hard to remember...
Leaving out one of them leads to something not working out properly!
Also, setting just one of them seems to have no effect to the others.
So, I must set all three for working with the PostgreSQL's psql database client.
Otherwise I run into encoding issues while exporting/importing data.
Now my question is: ""Why the heck? Isn't there an easier way to simply set the character encoding in PowerShell?""
Unfortunately, I did not find any plausible documentation myself about setting the character enconding!
Thanks in advance
/EDIT
The second comment by TheIncorrigible1 led me to the best answer fo far: Displaying Unicode in Powershell
- So one can set the whole PowerShell with two separated statements to the desired encoding (UTF-8).
PS C:\&gt; $OutputEncoding = [System.Console]::OutputEncoding = [System.Console]::InputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; $PSDefaultParameterValues['*:Encoding'] = 'utf8'
Explanation:
$OutputEncoding sets the encoding for e.g. | (piping) and/or communication between programs and/or processes.
[System.Console]::OutputEncoding sets the encoding for STDOUT and the console/terminal output.
[System.Console]::InputEncoding sets the encoding for STDIN or keyboard input.
$PSDefaultParameterValues['*:Encoding'] sets the encoding for all cmdlets that support the -Encoding option like e.g. Out-File -Encoding.
","<windows><postgresql><powershell><character-encoding>, character end (utf-8) powershel session, he all, console/german enthusiast database administer (postgresql) essential work correct charcot encoding. therefore, want client console/german window away set e.g. utf-8. back windows' cod.ex attempt east type command chap 65001 set desire code page identified. now, process switch powershel set character end seem odd, who. i'v done research set powershel session utf-8 figure out, need three steps/command accomplish that. is c:\&it; $outputencod = [system.text.encoding]::utf is c:\&it; [console]::outputencod = [system.text.encoding]::utf is c:\&it; chap 65001 despite fact first two command intact hard remember... leave one lead cometh work properly! also, set one seem effect others. so, must set three work postgresql' pool database client. otherwise run end issue exporting/import data. question is: ""who neck? easier way simple set character end powershell?"" unfortunately, find plausible document set character encoding! thank advance /edit second comment theincorrigible1 led best answer fo far: display united powershel - one set whole powershel two spear statement desire end (utf-8). is c:\&it; $outputencod = [system.console]::outputencod = [system.console]::inputencod = [system.text.encoding]::utf is c:\&it; $psdefaultparametervalues['*:encoding'] = 'utf' explanation: $outputencod set end e.g. | (piping) and/or common program and/or processes. [system.console]::outputencod set end stout console/german output. [system.console]::inputencod set end stain keyboard input. $psdefaultparametervalues['*:encoding'] set end cadet support -end option like e.g. out-fig -encoding."
51950129,Execute raw query in migration - Sequelize 3.30,"I want to execute a raw query in my migrations up and down functions. 
When I try to do: Sequelize.query, it says ERROR: Sequelize.query is not a function.
This is my migration skeleton file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize, migration) =&gt; {
     return Sequelize.query(...);   //ERROR: Sequelize.query is not a Function
  },
  down: (queryInterface, Sequelize) =&gt; {
     return Sequelize.query(...);  //ERROR: Sequelize.query is not a Function
  }
};
",<node.js><postgresql><sequelize.js>,491,0,17,10327,20,80,123,50,13394,,1192,1,14,2018-08-21 13:42,2018-08-22 1:56,2018-08-22 1:56,1.0,1.0,Basic,3,"<node.js><postgresql><sequelize.js>, Execute raw query in migration - Sequelize 3.30, I want to execute a raw query in my migrations up and down functions. 
When I try to do: Sequelize.query, it says ERROR: Sequelize.query is not a function.
This is my migration skeleton file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize, migration) =&gt; {
     return Sequelize.query(...);   //ERROR: Sequelize.query is not a Function
  },
  down: (queryInterface, Sequelize) =&gt; {
     return Sequelize.query(...);  //ERROR: Sequelize.query is not a Function
  }
};
","<node.is><postgresql><sequelae.is>, execute raw query migrate - sequel 3.30, want execute raw query migrate functions. try do: sequelae.query, say error: sequelae.query function. migrate skeleton file: 'use strict'; module.export = { up: (queryinterface, sequelae, migration) =&it; { return sequelae.query(...); //error: sequelae.query function }, down: (queryinterface, sequelae) =&it; { return sequelae.query(...); //error: sequelae.query function } };"
55998961,RSConfig generates a Dsn Connection String doesn't work,"TL;DR. 
Repro steps, take a backup of your C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config 
Run this command to update the connection string in SSRS's config:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s &lt;ServerName&gt; -i &lt;instanceNameIfNotDefault&gt; -d ""reportserver$ssrs"" -a SQL -u sa -p ""YourSAPassword"" -t
Now browse to the SSRS website and it doesn't work! To fix it either restore your config file or run through the SSRS GUI tool and it works!
How does the RsConfig utility work?
Background
After I install SSRS on an Windows 2016 Server and restore the 2 databases I need to change the Connection String in SSRS configuration file to point to the new SQL server name/instance.
Problem
When I try to change the encrypted Connection String in C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config file using the RSConfig utility:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!"" -t
It changes the Dsn Connection String in the RsReportServer.config. 
Before:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAC2DBxZFsfVB16r0e3......
*
After:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAO2nOjFDJMo........
*
However after this change, browsing to the SSRS Website results in the error:
  The report server can’t connect to its database. Make sure the database is running and accessible. You can also check the report server trace log for details.
If I run the SQL Reporting Services Configuration Tool (GUI) and change the Dsn Connection String browsing to the SSRS Website works! 
Obviously it changes the Dsn but I can't work out what else it does whilst the GUI tool is running. I've used ProcessMonitor and I've seen that the GUI tool does NOT use   RSConfig.exe utility, it uses itself RsConfigTool.exe! So I can't even capture command-line arguments of what the actual command/connection string should be. Also each time we change the connection string a new random one is generated so not sure how to do comparison's of actual vs expected.
I did a WinDiff of Registry keys and apart from some encrypted hexadecimal diffs, nothing stood out.
I run SQLProfiler and there were a bunch of grants that I have emulated in my PowerShell script, eg: 
$sqls += @""
USE [ReportServer`$SSRSTempDB]
if not exists (select * from sysusers where issqlrole = 1 and name = 'RSExecRole')
BEGIN
 EXEC sp_addrole 'RSExecRole'
END;
GO
My hunch is the $ sign in the SQL Database Name and the @ in the ""made up/simulated"" password are not getting escaped when I run the commands, eg:
$MachineName = ""server0012""
$instanceName = ""SSRS""
$saPassword = ""P@ssw0rd!""
$rsConfigPath = ""C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn\rsconfig.exe""
$setupArgs = -join('-c -s ""', $MachineName,'"" -i ""', $instanceName,'"" -d ','""ReportServer`$SSRS"" -t -a SQL -u ""sa"" -p ""', $saPassword,"""""""")
Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope Process
Write-Host $rsConfigPath $setupArgs
$args = $setupArgs.Split("" "")
&amp; ""$rsConfigPath"" $args
Restart-Service -Force ""SQL Server ($instanceName)""
When I run these vanilla commands in Command Prompt (no need to escape PowerShell characters):
rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!""
It changes the Dsn Connection String but browsing to SSRS Website gives same error (above).
How can I find out what else RsConfigTool.exe does when changing the Current Report Server Database? Or any guesses why the Connection String generated using the RSConfig Utility is out of whack - I've tried many different combinations, seems like only the RSConfigTool can actually do it?
Note 1:
I'm scripting this all up as a DevOps project and we are baking these images with packer, so nothing can be done manually. 
Note 2:
The Machine is joined to the domain and renamed after SQL installed. So using a Configuration.ini file I don't think will work.
",<sql><powershell><encryption><reporting-services><connection-string>,4235,4,25,62781,37,202,325,79,3721,,6536,2,14,2019-05-06 4:53,2019-05-15 17:44,2019-05-30 2:06,9.0,24.0,Advanced,32,"<sql><powershell><encryption><reporting-services><connection-string>, RSConfig generates a Dsn Connection String doesn't work, TL;DR. 
Repro steps, take a backup of your C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config 
Run this command to update the connection string in SSRS's config:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s &lt;ServerName&gt; -i &lt;instanceNameIfNotDefault&gt; -d ""reportserver$ssrs"" -a SQL -u sa -p ""YourSAPassword"" -t
Now browse to the SSRS website and it doesn't work! To fix it either restore your config file or run through the SSRS GUI tool and it works!
How does the RsConfig utility work?
Background
After I install SSRS on an Windows 2016 Server and restore the 2 databases I need to change the Connection String in SSRS configuration file to point to the new SQL server name/instance.
Problem
When I try to change the encrypted Connection String in C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config file using the RSConfig utility:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!"" -t
It changes the Dsn Connection String in the RsReportServer.config. 
Before:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAC2DBxZFsfVB16r0e3......
*
After:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAO2nOjFDJMo........
*
However after this change, browsing to the SSRS Website results in the error:
  The report server can’t connect to its database. Make sure the database is running and accessible. You can also check the report server trace log for details.
If I run the SQL Reporting Services Configuration Tool (GUI) and change the Dsn Connection String browsing to the SSRS Website works! 
Obviously it changes the Dsn but I can't work out what else it does whilst the GUI tool is running. I've used ProcessMonitor and I've seen that the GUI tool does NOT use   RSConfig.exe utility, it uses itself RsConfigTool.exe! So I can't even capture command-line arguments of what the actual command/connection string should be. Also each time we change the connection string a new random one is generated so not sure how to do comparison's of actual vs expected.
I did a WinDiff of Registry keys and apart from some encrypted hexadecimal diffs, nothing stood out.
I run SQLProfiler and there were a bunch of grants that I have emulated in my PowerShell script, eg: 
$sqls += @""
USE [ReportServer`$SSRSTempDB]
if not exists (select * from sysusers where issqlrole = 1 and name = 'RSExecRole')
BEGIN
 EXEC sp_addrole 'RSExecRole'
END;
GO
My hunch is the $ sign in the SQL Database Name and the @ in the ""made up/simulated"" password are not getting escaped when I run the commands, eg:
$MachineName = ""server0012""
$instanceName = ""SSRS""
$saPassword = ""P@ssw0rd!""
$rsConfigPath = ""C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn\rsconfig.exe""
$setupArgs = -join('-c -s ""', $MachineName,'"" -i ""', $instanceName,'"" -d ','""ReportServer`$SSRS"" -t -a SQL -u ""sa"" -p ""', $saPassword,"""""""")
Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope Process
Write-Host $rsConfigPath $setupArgs
$args = $setupArgs.Split("" "")
&amp; ""$rsConfigPath"" $args
Restart-Service -Force ""SQL Server ($instanceName)""
When I run these vanilla commands in Command Prompt (no need to escape PowerShell characters):
rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!""
It changes the Dsn Connection String but browsing to SSRS Website gives same error (above).
How can I find out what else RsConfigTool.exe does when changing the Current Report Server Database? Or any guesses why the Connection String generated using the RSConfig Utility is out of whack - I've tried many different combinations, seems like only the RSConfigTool can actually do it?
Note 1:
I'm scripting this all up as a DevOps project and we are baking these images with packer, so nothing can be done manually. 
Note 2:
The Machine is joined to the domain and renamed after SQL installed. So using a Configuration.ini file I don't think will work.
","<sal><powershell><encryption><reporting-services><connection-string>, rsconfig genet don connect string work, to;dr. retro steps, take back c:\program files\microsoft sal server\msrs13.sir\report services\reportserver\rsreportserver.confirm run command update connect string sir' confirm: c:\program file (x)\microsoft sal server\130\tools\inn&it;rsconfig -c -s &it;servername&it; -i &it;instancenameifnotdefault&it; -d ""reportserver$sir"" -a sal -u sa -p ""yoursapassword"" -t brows sir west work! fix either restore confirm file run sir gun tool works! rsconfig until work? background instal sir window 2016 server restore 2 database need change connect string sir configur file point new sal server name/instance. problem try change encrypt connect string c:\program files\microsoft sal server\msrs13.sir\report services\reportserver\rsreportserver.confirm file use rsconfig utility: c:\program file (x)\microsoft sal server\130\tools\inn&it;rsconfig -c -s server0012 -i sir -d ""reportserver$sir"" -a sal -u sa -p ""p@sword!"" -t change don connect string rsreportserver.confirm. before: &it;don>aqaaancmnd8bfderjhoawe/ll+sbaaaae+tic/4vs0a0fdh0tcy8kgqaaaaiaaaaugblahaabwbyahqaaqbuagcaiabtaguacgb2aguacgaaabbmaaaaaqaaiaaaac2dbxzfsfvb16r0e3...... * after: &it;don>aqaaancmnd8bfderjhoawe/ll+sbaaaae+tic/4vs0a0fdh0tcy8kgqaaaaiaaaaugblahaabwbyahqaaqbuagcaiabtaguacgb2aguacgaaabbmaaaaaqaaiaaaao2nojfdjmo........ * howe change, brows sir west result error: report server can’t connect database. make sure database run accessible. also check report server trace log details. run sal report service configur tool (gun) change don connect string brows sir west works! obvious change don can't work else whilst gun tool running. i'v use processmonitor i'v seen gun tool use rsconfig.ex utility, use rsconfigtool.eye! can't even capture command-in argument actual command/connect string be. also time change connect string new random one genet sure comparison' actual vs expected. winding registry key apart encrypt hexadecim liffs, not stood out. run sqlprofil bunch grant soul powershel script, eg: $sal += @"" use [reportserver`$ssrstempdb] exist (select * cysts issqlrol = 1 name = 'rsexecrole') begin even sp_addrol 'rsexecrole' end; go lunch $ sign sal database name @ ""made up/simulated"" password get escape run commands, eg: $machinenam = ""server0012"" $instancenam = ""sir"" $password = ""p@sword!"" $rsconfigpath = ""c:\program file (x)\microsoft sal server\130\tools\inn\rsconfig.eye"" $setuparg = -join('-c -s ""', $machinename,'"" -i ""', $instancename,'"" -d ','""reportserver`$sir"" -t -a sal -u ""sa"" -p ""', $password,"""""""") set-executionpolici -executionpolici restrict -scope process write-host $rsconfigpath $setuparg $are = $setupargs.split("" "") &amp; ""$rsconfigpath"" $are start-service -for ""sal server ($instancename)"" run vanilla command command prompt (no need escape powershel characters): rsconfig -c -s server0012 -i sir -d ""reportserver$sir"" -a sal -u sa -p ""p@sword!"" change don connect string brows sir west give error (above). find else rsconfigtool.ex change current report server database? guess connect string genet use rsconfig until hack - i'v try man differ combinations, seem like rsconfigtool actual it? note 1: i'm script deep project bake image packer, not done mentally. note 2: machine join domain renal sal installed. use configuration.in file think work."
50037975,Wordpress cannot connect to mysql server,"I have run a mysql server in my macbook, where I can access via both mysql command mysql -u root and navicat application. However, when I open the install page of a brand new wordpress app in my macbook. During the installation, I had got：
",<mysql><wordpress>,240,1,1,339,1,3,11,40,14936,0.0,10,10,14,2018-04-26 8:11,2018-04-26 8:32,,0.0,,Basic,9,"<mysql><wordpress>, Wordpress cannot connect to mysql server, I have run a mysql server in my macbook, where I can access via both mysql command mysql -u root and navicat application. However, when I open the install page of a brand new wordpress app in my macbook. During the installation, I had got：
","<myself><wordpress>, wordpress cannot connect myself server, run myself server malbrook, access via myself command myself -u root navicat application. however, open instal page brand new wordpress pp malbrook. installation, got："
52146191,Why can MySQL not use a partial primary key index?,"The MySQL documentation describing the use of index extensions, gives the following table as an example, followed by the query below:
CREATE TABLE t1 (
    i1 INT NOT NULL DEFAULT 0,
    i2 INT NOT NULL DEFAULT 0,
    d DATE DEFAULT NULL,
    PRIMARY KEY (i1, i2),
    INDEX k_d (d)
) ENGINE = InnoDB;
SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = '2000-01-01';
InnoDB internally will convert the index k_d to include the primary key at the end.  That is, the actual index k_d will be on (d, i1, i2), three columns.
The documentation goes on to explain that (emphasis mine):
  The optimizer cannot use the primary key in this case because that comprises columns (i1, i2) and the query does not refer to i2. Instead, the optimizer can use the secondary index k_d on (d), and the execution plan depends on whether the extended index is used.
I am confused by the above statement.  First it says that i1 is not enough to use the primary key index of two columns (i1, i2).  Then, in the second sentence, it says that the index k_d on (d, i1, i2) can be used, despite that only d and i1 are being used, with i2 absent.
My general understanding of indices in MySQL, and in other flavors of SQL, is that a left portion of an index can be used if a subset of all columns in the index are present, starting from the left.
What is different about a primary key (clustered) index and a non clustered secondary index which allows the latter to use a partial index, but the former cannot?
",<mysql><indexing><clustered-index>,1475,1,19,505816,28,292,366,49,2049,0.0,17194,2,14,2018-09-03 8:47,2018-09-06 15:48,2018-09-06 15:48,3.0,3.0,Basic,5,"<mysql><indexing><clustered-index>, Why can MySQL not use a partial primary key index?, The MySQL documentation describing the use of index extensions, gives the following table as an example, followed by the query below:
CREATE TABLE t1 (
    i1 INT NOT NULL DEFAULT 0,
    i2 INT NOT NULL DEFAULT 0,
    d DATE DEFAULT NULL,
    PRIMARY KEY (i1, i2),
    INDEX k_d (d)
) ENGINE = InnoDB;
SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = '2000-01-01';
InnoDB internally will convert the index k_d to include the primary key at the end.  That is, the actual index k_d will be on (d, i1, i2), three columns.
The documentation goes on to explain that (emphasis mine):
  The optimizer cannot use the primary key in this case because that comprises columns (i1, i2) and the query does not refer to i2. Instead, the optimizer can use the secondary index k_d on (d), and the execution plan depends on whether the extended index is used.
I am confused by the above statement.  First it says that i1 is not enough to use the primary key index of two columns (i1, i2).  Then, in the second sentence, it says that the index k_d on (d, i1, i2) can be used, despite that only d and i1 are being used, with i2 absent.
My general understanding of indices in MySQL, and in other flavors of SQL, is that a left portion of an index can be used if a subset of all columns in the index are present, starting from the left.
What is different about a primary key (clustered) index and a non clustered secondary index which allows the latter to use a partial index, but the former cannot?
","<myself><indexing><clustered-index>, myself use partial primary key index?, myself document describe use index extensions, give follow table example, follow query below: great table to ( in in null default 0, in in null default 0, date default null, primary key (in, in), index kid (d) ) engine = innodb; select count(*) to in = 3 = '2000-01-01'; innodb inter convert index kid include primary key end. is, actual index kid (d, in, in), three columns. document go explain (emphasis mine): optic cannot use primary key case comprise column (in, in) query refer in. instead, optic use secondary index kid (d), execute plan depend whether extend index used. confuse statement. first say in enough use primary key index two column (in, in). then, second sentence, say index kid (d, in, in) used, despite in used, in absent. genet understand india myself, flavor sal, left portion index use sunset column index present, start left. differ primary key (clustered) index non cluster secondary index allow latter use partial index, former cannot?"
48460910,Electron App Getting Exception While requiring SQLITE3,"package.json 
""name"": ""billingapp"",
""version"": ""1.0.0"",
""description"": """",
""main"": ""index.js"",
""scripts"": {
""rebuild"": ""electron-rebuild -f -w billingapp""
},
""author"": ""S Kundu"",
""license"": ""ISC"",
""dependencies"": {
""electron"": ""^1.7.11"",
""sqlite3"": ""^3.1.13""
}
""devDependencies"": {
""electron-rebuild"": ""^1.7.3""
}
index.js 
const electron  = require('electron');
const path      = require('path');
const url       = require('url');
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
const {app, BrowserWindow, Menu, ipcMain} = electron;
let mainWindow;
app.on('ready', function(){
// Create the login window
mainWindow = new BrowserWindow({
  resizable: true,
  fullscreen: false
});
// Load html in window
mainWindow.loadURL(url.format({
  pathname: path.join(__dirname, 'login.html'),
  protocol: 'file:',
  slashes: true
}));
});
login.html 
&lt;h1&gt;Welcome to billing system&lt;/h1&gt;
These are the code files.
Steps to install NPM Packages
npm install electron
npm install sqlite3
Its working perfect when I remove bellow code:
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
But  with this code , while running 
npm start
is getting bellow error:
App threw an error during load
  Error: Cannot find module
  'C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64\node_sqlite3.node'
      at Module._resolveFilename (module.js:470:15)
      at Function.Module._resolveFilename (C:\Users\sintu\Desktop\BillingSystem\node_modules\electron\dist\resources\electron.asar\common\reset-search-paths.js:35:12)
      at Function.Module._load (module.js:418:25)
      at Module.require (module.js:498:17)
      at require (internal/module.js:20:19)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:4:15)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:190:3)
      at Module._compile (module.js:571:32)
      at Object.Module._extensions..js (module.js:580:10)
      at Module.load (module.js:488:32)
when I run npm run rebuild , I get bellow error 
× Rebuild Failed
An unhandled error occurred inside electron-rebuild
Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
Error: Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
    at SafeSubscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:277:84)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:198:26)
    at Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at MapSubscriber.Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at MapSubscriber.Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at SafeSubscriber._next (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:251:65)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.next (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:186:22)
npm ERR! code ELIFECYCLE
npm ERR! errno 4294967295
npm ERR! billingapp@1.0.0 rebuild: electron-rebuild -f -w billingapp
npm ERR! Exit status 4294967295
npm ERR!
npm ERR! Failed at the billingapp@1.0.0 rebuild script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\sintu\AppData\Roaming\npm-cache_logs\2018-01-30T15_36_46_678Z-debug.log
",<javascript><sqlite><electron>,6886,4,51,165,0,1,11,66,1369,,9,1,14,2018-01-26 11:56,2018-07-23 15:52,,178.0,,Advanced,37,"<javascript><sqlite><electron>, Electron App Getting Exception While requiring SQLITE3, package.json 
""name"": ""billingapp"",
""version"": ""1.0.0"",
""description"": """",
""main"": ""index.js"",
""scripts"": {
""rebuild"": ""electron-rebuild -f -w billingapp""
},
""author"": ""S Kundu"",
""license"": ""ISC"",
""dependencies"": {
""electron"": ""^1.7.11"",
""sqlite3"": ""^3.1.13""
}
""devDependencies"": {
""electron-rebuild"": ""^1.7.3""
}
index.js 
const electron  = require('electron');
const path      = require('path');
const url       = require('url');
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
const {app, BrowserWindow, Menu, ipcMain} = electron;
let mainWindow;
app.on('ready', function(){
// Create the login window
mainWindow = new BrowserWindow({
  resizable: true,
  fullscreen: false
});
// Load html in window
mainWindow.loadURL(url.format({
  pathname: path.join(__dirname, 'login.html'),
  protocol: 'file:',
  slashes: true
}));
});
login.html 
&lt;h1&gt;Welcome to billing system&lt;/h1&gt;
These are the code files.
Steps to install NPM Packages
npm install electron
npm install sqlite3
Its working perfect when I remove bellow code:
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
But  with this code , while running 
npm start
is getting bellow error:
App threw an error during load
  Error: Cannot find module
  'C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64\node_sqlite3.node'
      at Module._resolveFilename (module.js:470:15)
      at Function.Module._resolveFilename (C:\Users\sintu\Desktop\BillingSystem\node_modules\electron\dist\resources\electron.asar\common\reset-search-paths.js:35:12)
      at Function.Module._load (module.js:418:25)
      at Module.require (module.js:498:17)
      at require (internal/module.js:20:19)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:4:15)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:190:3)
      at Module._compile (module.js:571:32)
      at Object.Module._extensions..js (module.js:580:10)
      at Module.load (module.js:488:32)
when I run npm run rebuild , I get bellow error 
× Rebuild Failed
An unhandled error occurred inside electron-rebuild
Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
Error: Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
    at SafeSubscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:277:84)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:198:26)
    at Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at MapSubscriber.Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at MapSubscriber.Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at SafeSubscriber._next (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:251:65)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.next (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:186:22)
npm ERR! code ELIFECYCLE
npm ERR! errno 4294967295
npm ERR! billingapp@1.0.0 rebuild: electron-rebuild -f -w billingapp
npm ERR! Exit status 4294967295
npm ERR!
npm ERR! Failed at the billingapp@1.0.0 rebuild script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\sintu\AppData\Roaming\npm-cache_logs\2018-01-30T15_36_46_678Z-debug.log
","<javascript><quite><electron>, electron pp get except require sqlite3, package.son ""name"": ""billingapp"", ""version"": ""1.0.0"", ""description"": """", ""main"": ""index.is"", ""script"": { ""rebuild"": ""electron-rebuild -f -w billingapp"" }, ""author"": ""s kind"", ""license"": ""is"", ""dependencies"": { ""electron"": ""^1.7.11"", ""sqlite3"": ""^3.1.13"" } ""devdependencies"": { ""electron-rebuild"": ""^1.7.3"" } index.j cost electron = require('electron'); cost path = require('path'); cost curl = require('curl'); war sqlite3 = require('sqlite3').verse(); war do = new sqlite3.database(path.join(__dirname, 'sample.do')); cost {pp, browserwindow, menu, ipcmain} = electron; let mainwindow; pp.on('ready', function(){ // great login window mainwindow = new browserwindow({ reliable: true, fullscreen: fall }); // load html window mainwindow.loadurl(curl.format({ pathname: path.join(__dirname, 'login.html'), protocol: 'file:', lashes: true })); }); login.html &it;he&it;welcome bill system&it;/he&it; code files. step instal nom package nom instal electron nom instal sqlite3 work perfect remove fellow code: war sqlite3 = require('sqlite3').verse(); war do = new sqlite3.database(path.join(__dirname, 'sample.do')); code , run nom start get fellow error: pp threw error load error: cannot find model 'c:\users\situ\desktop\billingsystem\node_modules\sqlite3\limb\binding\electron-ve.7-wine-x\node_sqlite3.node' module._resolvefilenam (module.is:470:15) function.module._resolvefilenam (c:\users\situ\desktop\billingsystem\node_modules\electron\list\resources\electron.tsar\common\rest-search-paths.is:35:12) function.module.load (module.is:418:25) module.require (module.is:498:17) require (internal/module.is:20:19) object. (c:\users\situ\desktop\billingsystem\node_modules\sqlite3\limb\sqlite3.is:4:15) object. (c:\users\situ\desktop\billingsystem\node_modules\sqlite3\limb\sqlite3.is:190:3) module.compel (module.is:571:32) object.module.extensions..j (module.is:580:10) module.load (module.is:488:32) run nom run rebuild , get fellow error × rebuild fail unhandl error occur inside electron-rebuild build project slut one time. enable parallel build, pleas add ""/m"" switch. c:\users\situ\desktop\billingapp\node_modules\sqlite3\build\des\action_before_build.vcxproj(20,3): error msb4019: import project ""c:\microsoft.pp.default.drops"" found. confirm path declare correct, file exist disk. gap err! build error gap err! stick error: c:\windows\microsoft.net\framework\ve.0.30319\build.ex fail exit code: 1 gap err! stick childprocess.next (c:\users\situ\desktop\billingapp\node_modules\node-gap\limb\build.is:258:23) gap err! stick emitted (events.is:126:13) gap err! stick childprocess.emit (events.is:214:7) gap err! stick process.childprocess.handle.next (internal/child_process.is:198:12) gap err! system windows_nt 6.1.7601 gap err! command ""c:\program files\nodes\node.eye"" ""c:\users\situ\desktop\billingapp\node_modules\node-gap\bin\node-gap.is"" ""rebuild"" ""--target=1.7.11"" ""--arch=x"" ""--list-curl=http://atom.to/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=c:\users\situ\desktop\billingapp\node_modules\sqlite3\limb\binding\electron-ve.7-wine-x"" ""--host=http://mapbox-node-binary.s.amazonaws.com"" ""--remote_path=./{name}/ve.1.13/{tools}/"" ""--package_name=electron-ve.7-wine-x.tar.go"" gap err! cod c:\users\situ\desktop\billingapp\node_modules\sqlite3 gap err! node -v ve.9.1 gap err! node-gap -v ve.6.2 gap err! ok fail exit code: 1 error: build project slut one time. enable parallel build, pleas add ""/m"" switch. c:\users\situ\desktop\billingapp\node_modules\sqlite3\build\des\action_before_build.vcxproj(20,3): error msb4019: import project ""c:\microsoft.pp.default.drops"" found. confirm path declare correct, file exist disk. gap err! build error gap err! stick error: c:\windows\microsoft.net\framework\ve.0.30319\build.ex fail exit code: 1 gap err! stick childprocess.next (c:\users\situ\desktop\billingapp\node_modules\node-gap\limb\build.is:258:23) gap err! stick emitted (events.is:126:13) gap err! stick childprocess.emit (events.is:214:7) gap err! stick process.childprocess.handle.next (internal/child_process.is:198:12) gap err! system windows_nt 6.1.7601 gap err! command ""c:\program files\nodes\node.eye"" ""c:\users\situ\desktop\billingapp\node_modules\node-gap\bin\node-gap.is"" ""rebuild"" ""--target=1.7.11"" ""--arch=x"" ""--list-curl=http://atom.to/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=c:\users\situ\desktop\billingapp\node_modules\sqlite3\limb\binding\electron-ve.7-wine-x"" ""--host=http://mapbox-node-binary.s.amazonaws.com"" ""--remote_path=./{name}/ve.1.13/{tools}/"" ""--package_name=electron-ve.7-wine-x.tar.go"" gap err! cod c:\users\situ\desktop\billingapp\node_modules\sqlite3 gap err! node -v ve.9.1 gap err! node-gap -v ve.6.2 gap err! ok fail exit code: 1 safesubscriber.terror (c:\users\situ\desktop\billingapp\node_modules\spain-re\limb\sac\index.is:277:84) safesubscriber.__tryorunsub (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:239:16) safesubscriber.error (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:198:26) subscribe.terror (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:129:26) subscribe.error (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:103:18) mapsubscriber.subscribe.terror (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:129:26) mapsubscriber.subscribe.error (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:103:18) safesubscriber.next (c:\users\situ\desktop\billingapp\node_modules\spain-re\limb\sac\index.is:251:65) safesubscriber.__tryorunsub (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:239:16) safesubscriber.next (c:\users\situ\desktop\billingapp\node_modules\rays\subscribe.is:186:22) nom err! code elifecycl nom err! error 4294967295 nom err! billingapp@1.0.0 rebuild: electron-rebuild -f -w billingapp nom err! exit state 4294967295 nom err! nom err! fail billingapp@1.0.0 rebuild script. nom err! probably problem nom. like admit log output above. nom err! complete log run found in: nom err! c:\users\situ\appdata\roaming\nom-cache_logs\2018-01-30t15_36_46_678z-debut.log"
53560489,"EF Core SQLite in memory exception: SQLite Error 1: 'near ""MAX"": syntax error'","I am creating SQLite In Memory database for unit testing:
        var connection = new SqliteConnection(""DataSource=:memory:"");
        connection.Open();
        try
        {
            var options = new DbContextOptionsBuilder&lt;BloggingContext&gt;()
                .UseSqlite(connection)
                .Options;
            // Create the schema in the database
            using (var context = new BloggingContext(options))
            {
                context.Database.EnsureCreated();
            }
            // Run the test against one instance of the context
            using (var context = new BloggingContext(options))
            {
                var service = new BlogService(context);
                service.Add(""http://sample.com"");
            }
            // Use a separate instance of the context to verify correct data was saved to database
            using (var context = new BloggingContext(options))
            {
                Assert.AreEqual(1, context.Blogs.Count());
                Assert.AreEqual(""http://sample.com"", context.Blogs.Single().Url);
            }
        }
context.Database.EnsureCreated(); fails with with exception:
Message: Microsoft.Data.Sqlite.SqliteException : SQLite Error 1: 'near ""MAX"": syntax error'.
There is github issue saying: 
The issue here is varchar(max) is SqlServer specific type. The scaffolding should not add it as relational type which gets passed to migration in other providers which is prone to generate invalid sql at migration.
But how then can I use SQLite in Memory for unit tests if my database contains many varchar(max) columns?
",<c#><sqlite><entity-framework-core><ef-core-2.0><ef-core-2.1>,1619,3,29,2103,3,20,40,58,7400,,160,4,14,2018-11-30 15:33,2018-12-30 20:05,,30.0,,Intermediate,18,"<c#><sqlite><entity-framework-core><ef-core-2.0><ef-core-2.1>, EF Core SQLite in memory exception: SQLite Error 1: 'near ""MAX"": syntax error', I am creating SQLite In Memory database for unit testing:
        var connection = new SqliteConnection(""DataSource=:memory:"");
        connection.Open();
        try
        {
            var options = new DbContextOptionsBuilder&lt;BloggingContext&gt;()
                .UseSqlite(connection)
                .Options;
            // Create the schema in the database
            using (var context = new BloggingContext(options))
            {
                context.Database.EnsureCreated();
            }
            // Run the test against one instance of the context
            using (var context = new BloggingContext(options))
            {
                var service = new BlogService(context);
                service.Add(""http://sample.com"");
            }
            // Use a separate instance of the context to verify correct data was saved to database
            using (var context = new BloggingContext(options))
            {
                Assert.AreEqual(1, context.Blogs.Count());
                Assert.AreEqual(""http://sample.com"", context.Blogs.Single().Url);
            }
        }
context.Database.EnsureCreated(); fails with with exception:
Message: Microsoft.Data.Sqlite.SqliteException : SQLite Error 1: 'near ""MAX"": syntax error'.
There is github issue saying: 
The issue here is varchar(max) is SqlServer specific type. The scaffolding should not add it as relational type which gets passed to migration in other providers which is prone to generate invalid sql at migration.
But how then can I use SQLite in Memory for unit tests if my database contains many varchar(max) columns?
","<c#><quite><entity-framework-core><of-core-2.0><of-core-2.1>, of core quite memory exception: quite error 1: 'near ""max"": santa error', great quite memory database unit testing: war connect = new sqliteconnection(""datasource=:memory:""); connection.open(); try { war option = new dbcontextoptionsbuilder&it;bloggingcontext&it;() .usesqlite(connection) .option; // great scheme database use (war context = new bloggingcontext(option)) { context.database.ensurecreated(); } // run test one instant context use (war context = new bloggingcontext(option)) { war service = new blogservice(context); service.add(""http://sample.com""); } // use spear instant context verify correct data save database use (war context = new bloggingcontext(option)) { assert.areequal(1, context.blows.count()); assert.areequal(""http://sample.com"", context.blows.single().curl); } } context.database.ensurecreated(); fail exception: message: microsoft.data.quite.sqliteexcept : quite error 1: 'near ""max"": santa error'. github issue saying: issue varchar(max) sqlserver specie type. scaffold add relate type get pass migrate proved prone genet invalid sal migration. use quite memory unit test database contain man varchar(max) columns?"
53386872,Is it possible to use StringFormat or Constant Variable in android Room Query,"I want to query the user associations list with the following room query using public constant variable Association.MEMBER_STATUS_APPROVED.
@Query(""SELECT * FROM Association WHERE memberStatus = "" + Association.MEMBER_STATUS_APPROVED)
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations();
But, room gives me [SQLITE_ERROR] when build.
 It is possible to re-write that query by replacing the constant variable with parameter like the following.
@Query(""SELECT * FROM Association WHERE memberStatus = :statusApproved"")
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations(String statusApproved);
I would like to know that does Room support such kind of string concatenation or String Format? (or) May be I missing something?
",<android><database><android-sqlite><android-room>,741,0,4,571,1,5,8,39,3871,,6,3,14,2018-11-20 5:39,2018-11-20 6:12,,0.0,,Intermediate,18,"<android><database><android-sqlite><android-room>, Is it possible to use StringFormat or Constant Variable in android Room Query, I want to query the user associations list with the following room query using public constant variable Association.MEMBER_STATUS_APPROVED.
@Query(""SELECT * FROM Association WHERE memberStatus = "" + Association.MEMBER_STATUS_APPROVED)
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations();
But, room gives me [SQLITE_ERROR] when build.
 It is possible to re-write that query by replacing the constant variable with parameter like the following.
@Query(""SELECT * FROM Association WHERE memberStatus = :statusApproved"")
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations(String statusApproved);
I would like to know that does Room support such kind of string concatenation or String Format? (or) May be I missing something?
","<andros><database><andros-quite><andros-room>, possible use stringformat constant variable andros room query, want query user cassock list follow room query use public constant variable association.member_status_approved. @query(""select * cassock memberstatu = "" + association.member_status_approved) livedata&it;list&it;association&it;&it; loaduserassociations(); but, room give [sqlite_error] build. possible re-writ query replace constant variable parapet like following. @query(""select * cassock memberstatu = :statusapproved"") livedata&it;list&it;association&it;&it; loaduserassociations(sir statusapproved); would like know room support kind string concave string format? (or) may miss something?"
65481470,Connect to remote db with ssh tunneling in DBeaver,"I know this question was already asked before (like here), but still I could not find a solution and those posts are quite old.
So I am able to connect to the remote db with an ssh connection and then use the command line like this:
// Putty SSH Connection
host: ssh.strato.de
port: 22
username: xxxxxxx 
password: xxxxxxx 
// connect to mysql with terminal
mysql -h rdbms -u xxxxxxx -p xxxxxxxx
If I try the same with ssh-tunneling in DBeaver I get an connection error
The ssh-tunneling itself seems to work. If I use the same credentials as above and press &quot;Test tunnel configuration&quot; I get a success message.
I tried several other options for port and host (localhost, rdbms.strato.de, etc), which I found via mysql show variables; show processlist; show user();, but none of them worked.
The Strato Support told me that I can only connect to the db internally with phpmyadmin or remotely wiht putty and mysql, but since the last method is working, shouldn't ssh-tunneling also work?
",<mysql><ssh><ssh-tunnel><dbeaver>,997,3,11,511,1,4,14,45,37688,0.0,14,2,14,2020-12-28 17:23,2021-04-27 18:20,,120.0,,Basic,10,"<mysql><ssh><ssh-tunnel><dbeaver>, Connect to remote db with ssh tunneling in DBeaver, I know this question was already asked before (like here), but still I could not find a solution and those posts are quite old.
So I am able to connect to the remote db with an ssh connection and then use the command line like this:
// Putty SSH Connection
host: ssh.strato.de
port: 22
username: xxxxxxx 
password: xxxxxxx 
// connect to mysql with terminal
mysql -h rdbms -u xxxxxxx -p xxxxxxxx
If I try the same with ssh-tunneling in DBeaver I get an connection error
The ssh-tunneling itself seems to work. If I use the same credentials as above and press &quot;Test tunnel configuration&quot; I get a success message.
I tried several other options for port and host (localhost, rdbms.strato.de, etc), which I found via mysql show variables; show processlist; show user();, but none of them worked.
The Strato Support told me that I can only connect to the db internally with phpmyadmin or remotely wiht putty and mysql, but since the last method is working, shouldn't ssh-tunneling also work?
","<myself><ash><ash-tunnel><beaver>, connect remote do ash tunnel beaver, know question already ask (like here), still could find slut post quit old. all connect remote do ash connect use command line like this: // putty ash connect host: ash.strata.d port: 22 surname: xxxxxxx password: xxxxxxx // connect myself german myself -h room -u xxxxxxx -p xxxxxxxx try ash-tunnel beaver get connect error ash-tunnel seem work. use credenti press &quit;test tunnel configuration&quit; get success message. try never option port host (localhost, rooms.strata.de, etc), found via myself show variable; show processlist; show user();, none worked. strata support told connect do inter phpmyadmin remote with putty myself, since last method working, ash-tunnel also work?"
51835172,Sequelize Error: Relation does not exist,"I am able to use sequelize.js to do an INSERT INTO command for a table in my development database, but not in my test database.
Despite researching thoroughly, I have not been able to resolve the issue.
A similar question has been posted here, though I have not been able to answer my question with the answers:
sequelize with postgres database not working after migration from mysql
Here is my relevant migration file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize) =&gt; {
    return queryInterface.createTable('Trees', {
      id: {
        allowNull: false,
        autoIncrement: true,
        primaryKey: true,
        type: Sequelize.INTEGER
      },
      title: {
        type: Sequelize.STRING,
        allowNull: false
      },
      content: {
        type: Sequelize.STRING(10000),
        allowNull: false
      },
      createdAt: {
        allowNull: false,
        type: Sequelize.DATE
      },
      updatedAt: {
        allowNull: false,
        type: Sequelize.DATE
      }
    });
  },
  down: (queryInterface, Sequelize) =&gt; {
    return queryInterface.dropTable('Trees');
  }
};
Here is the model file:
'use strict';
module.exports = (sequelize, DataTypes) =&gt; {
  var Tree = sequelize.define('Tree', {
    title: {
      type: DataTypes.STRING,
      allowNull: false
    },
    content: {
      type: DataTypes.STRING(10000),
      allowNull: false
    }
  }, {});
  Tree.associate = function(models) {
    // associations can be defined here
  };
  return Tree;
};
Here is the code that accesses the database:
const setTree = (treeVal, callback) =&gt; {
  console.log(`this would be the part`);
  Tree.create({
    title: 'Tree',
    content: JSON.stringify(treeVal)
  })
  .then((treeStr) =&gt; {
    let primaryTopics = JSON.parse(treeStr.content);
    callback(null, primaryTopics);
  })
  .catch((err) =&gt; {
    callback(err);
  });
}
This is exported in the module.exports method:
  callTree(callback) {
    return Tree.findOne({
      where: {
        title: 'Tree'
      }
    })
    .then((treeStr) =&gt; {
      if (treeStr === null) {
        return callback(`not defined yet`);
      }
      let primaryTopics = treeStr.content;
      primaryTopics = JSON.parse(primaryTopics);
      callback(null, primaryTopics);
    })
    .catch((err) =&gt; {
      callback(err);
    });
  }
And I'm pulling this method for an integration test here (the PrimaryTopic table is in the same database, and I receive no errors trying to run it):
  beforeEach((done) =&gt; {
    this.primaryTopic;
    sequelize.sync({force: true}).then((res) =&gt; {
      PrimaryTopic.create({
        title: 'Title: Hello World',
        content: '&lt;p&gt;Content: Hello World&lt;/p&gt;'
      })
      .then((primaryTopic) =&gt; {
        this.primaryTopic = primaryTopic;
        treeQueries.buildTree((err, res) =&gt; {
          if (err) {
            console.error(err);
          }
        });
        done();
      })
      .catch((err) =&gt; {
        console.log(err);
        done();
      });
    });
  });
I've searched through all the code for possible errors, but haven't found anything yet. 
I can use psql to access the Trees table in the test database, though it is empty.
I can use the same code to insert a value into the Trees table in the development database with no issues.
Here is the error I receive when I try to run a test (using jasmine.js for testing):
{ SequelizeDatabaseError: relation ""Trees"" does not exist
    at Query.formatError (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:363:16)
    at query.catch.err (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:86:18)
    at tryCatcher (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:512:31)
    at Promise._settlePromise (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:569:18)
    at Promise._settlePromise0 (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:614:10)
    at Promise._settlePromises (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:689:18)
    at Async._drainQueue (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:133:16)
    at Async._drainQueues (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:143:10)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:756:18)
    at tryOnImmediate (timers.js:717:5)
    at processImmediate [as _immediateCallback] (timers.js:697:5)
  name: 'SequelizeDatabaseError',
  parent: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  original: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' }
Here's a link to the full repository.
",<javascript><node.js><postgresql><sequelize.js>,8517,2,185,221,2,3,7,49,36600,,0,3,14,2018-08-14 6:42,2020-10-14 14:19,,792.0,,Basic,1,"<javascript><node.js><postgresql><sequelize.js>, Sequelize Error: Relation does not exist, I am able to use sequelize.js to do an INSERT INTO command for a table in my development database, but not in my test database.
Despite researching thoroughly, I have not been able to resolve the issue.
A similar question has been posted here, though I have not been able to answer my question with the answers:
sequelize with postgres database not working after migration from mysql
Here is my relevant migration file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize) =&gt; {
    return queryInterface.createTable('Trees', {
      id: {
        allowNull: false,
        autoIncrement: true,
        primaryKey: true,
        type: Sequelize.INTEGER
      },
      title: {
        type: Sequelize.STRING,
        allowNull: false
      },
      content: {
        type: Sequelize.STRING(10000),
        allowNull: false
      },
      createdAt: {
        allowNull: false,
        type: Sequelize.DATE
      },
      updatedAt: {
        allowNull: false,
        type: Sequelize.DATE
      }
    });
  },
  down: (queryInterface, Sequelize) =&gt; {
    return queryInterface.dropTable('Trees');
  }
};
Here is the model file:
'use strict';
module.exports = (sequelize, DataTypes) =&gt; {
  var Tree = sequelize.define('Tree', {
    title: {
      type: DataTypes.STRING,
      allowNull: false
    },
    content: {
      type: DataTypes.STRING(10000),
      allowNull: false
    }
  }, {});
  Tree.associate = function(models) {
    // associations can be defined here
  };
  return Tree;
};
Here is the code that accesses the database:
const setTree = (treeVal, callback) =&gt; {
  console.log(`this would be the part`);
  Tree.create({
    title: 'Tree',
    content: JSON.stringify(treeVal)
  })
  .then((treeStr) =&gt; {
    let primaryTopics = JSON.parse(treeStr.content);
    callback(null, primaryTopics);
  })
  .catch((err) =&gt; {
    callback(err);
  });
}
This is exported in the module.exports method:
  callTree(callback) {
    return Tree.findOne({
      where: {
        title: 'Tree'
      }
    })
    .then((treeStr) =&gt; {
      if (treeStr === null) {
        return callback(`not defined yet`);
      }
      let primaryTopics = treeStr.content;
      primaryTopics = JSON.parse(primaryTopics);
      callback(null, primaryTopics);
    })
    .catch((err) =&gt; {
      callback(err);
    });
  }
And I'm pulling this method for an integration test here (the PrimaryTopic table is in the same database, and I receive no errors trying to run it):
  beforeEach((done) =&gt; {
    this.primaryTopic;
    sequelize.sync({force: true}).then((res) =&gt; {
      PrimaryTopic.create({
        title: 'Title: Hello World',
        content: '&lt;p&gt;Content: Hello World&lt;/p&gt;'
      })
      .then((primaryTopic) =&gt; {
        this.primaryTopic = primaryTopic;
        treeQueries.buildTree((err, res) =&gt; {
          if (err) {
            console.error(err);
          }
        });
        done();
      })
      .catch((err) =&gt; {
        console.log(err);
        done();
      });
    });
  });
I've searched through all the code for possible errors, but haven't found anything yet. 
I can use psql to access the Trees table in the test database, though it is empty.
I can use the same code to insert a value into the Trees table in the development database with no issues.
Here is the error I receive when I try to run a test (using jasmine.js for testing):
{ SequelizeDatabaseError: relation ""Trees"" does not exist
    at Query.formatError (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:363:16)
    at query.catch.err (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:86:18)
    at tryCatcher (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:512:31)
    at Promise._settlePromise (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:569:18)
    at Promise._settlePromise0 (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:614:10)
    at Promise._settlePromises (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:689:18)
    at Async._drainQueue (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:133:16)
    at Async._drainQueues (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:143:10)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:756:18)
    at tryOnImmediate (timers.js:717:5)
    at processImmediate [as _immediateCallback] (timers.js:697:5)
  name: 'SequelizeDatabaseError',
  parent: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  original: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' }
Here's a link to the full repository.
","<javascript><node.is><postgresql><sequelae.is>, sequel error: relate exist, all use sequelae.j insert command table develop database, test database. despite research thoroughly, all resolve issue. similar question post here, though all answer question answers: sequel poster database work migrate myself rule migrate file: 'use strict'; module.export = { up: (queryinterface, sequelae) =&it; { return queryinterface.createtable('trees', { id: { allownull: false, autoincrement: true, primarykey: true, type: sequelae.inter }, title: { type: sequelae.string, allownull: fall }, content: { type: sequelae.string(10000), allownull: fall }, created: { allownull: false, type: sequelae.d }, updated: { allownull: false, type: sequelae.d } }); }, down: (queryinterface, sequelae) =&it; { return queryinterface.droptable('trees'); } }; model file: 'use strict'; module.export = (sequelae, datatypes) =&it; { war tree = sequelae.define('tree', { title: { type: datatypes.string, allownull: fall }, content: { type: datatypes.string(10000), allownull: fall } }, {}); tree.cassock = function(models) { // cassock define }; return tree; }; code access database: cost settle = (reveal, callback) =&it; { console.log(`the would part`); tree.create({ title: 'tree', content: son.stringify(reveal) }) .then((trees) =&it; { let primarytop = son.pause(trees.content); callback(null, primarytopics); }) .catch((err) =&it; { callback(err); }); } export module.export method: calltree(callback) { return tree.finding({ where: { title: 'tree' } }) .then((trees) =&it; { (trees === null) { return callback(`not define yet`); } let primarytop = trees.content; primarytop = son.pause(primarytopics); callback(null, primarytopics); }) .catch((err) =&it; { callback(err); }); } i'm pull method inter test (the primarytop table database, receive error try run it): beforeeach((done) =&it; { this.primarytopic; sequelae.son({force: true}).then((yes) =&it; { primarytopic.create({ title: 'title: hello world', content: '&it;p&it;content: hello world&it;/p&it;' }) .then((primarytopic) =&it; { this.primarytop = primarytopic; treequeries.buildtree((err, yes) =&it; { (err) { console.error(err); } }); done(); }) .catch((err) =&it; { console.log(err); done(); }); }); }); i'v search code possible errors, found any yet. use pool access tree table test database, though empty. use code insert value tree table develop database issues. error receive try run test (use famine.j testing): { sequelizedatabaseerror: relate ""trees"" exist query.formaterror (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/sequelae/limb/dialect/postures/query.is:363:16) query.catch.err (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/sequelae/limb/dialect/postures/query.is:86:18) trycatch (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/until.is:16:23) promise._settlepromisefromhandl (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/promise.is:512:31) promise._settlepromis (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/promise.is:569:18) promise._settlepromise0 (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/promise.is:614:10) promise._settlepromis (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/promise.is:689:18) async._drainqueu (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/async.is:133:16) async._drainqueu (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/async.is:143:10) immediate.async.drainqueu [a _onimmediate] (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/bluebeard/is/release/async.is:17:14) runcallback (times.is:756:18) tryonimmedi (times.is:717:5) processimmedi [a _immediatecallback] (times.is:697:5) name: 'sequelizedatabaseerror', parent: { error: relate ""trees"" exist connection.pause (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:553:11) connection.parsemessag (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:378:19) socket.&it;anonymous&it; (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:119:22) socket.emit (events.is:160:13) addchunk (_stream_readable.is:269:12) readableaddchunk (_stream_readable.is:256:11) socket.readable.push (_stream_readable.is:213:10) top.read (net.is:599:20) name: 'error', length: 104, severity: 'error', code: '42p01', detail: undefined, hint: undefined, position: '13', internalposition: undefined, internalquery: undefined, where: undefined, scheme: undefined, table: undefined, column: undefined, datatype: undefined, constraint: undefined, file: 'parse_relation.c', line: '1160', routine: 'parseropentable', sal: 'insert ""trees"" (""id"",""title"",""content"",""created"",""updated"") value (default,\'tree\',\'[{""title"":""title: hello world"",""id"":1,""secondarytopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') return *;' }, original: { error: relate ""trees"" exist connection.pause (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:553:11) connection.parsemessag (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:378:19) socket.&it;anonymous&it; (/home/siddhartha/documents/01-studio/01-commercial-public/01-komodo/2018-resources/node_modules/pg/limb/connection.is:119:22) socket.emit (events.is:160:13) addchunk (_stream_readable.is:269:12) readableaddchunk (_stream_readable.is:256:11) socket.readable.push (_stream_readable.is:213:10) top.read (net.is:599:20) name: 'error', length: 104, severity: 'error', code: '42p01', detail: undefined, hint: undefined, position: '13', internalposition: undefined, internalquery: undefined, where: undefined, scheme: undefined, table: undefined, column: undefined, datatype: undefined, constraint: undefined, file: 'parse_relation.c', line: '1160', routine: 'parseropentable', sal: 'insert ""trees"" (""id"",""title"",""content"",""created"",""updated"") value (default,\'tree\',\'[{""title"":""title: hello world"",""id"":1,""secondarytopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') return *;' }, sal: 'insert ""trees"" (""id"",""title"",""content"",""created"",""updated"") value (default,\'tree\',\'[{""title"":""title: hello world"",""id"":1,""secondarytopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') return *;' } here' link full depositors."
49592794,Postgres: Are There Downsides to Using a JSON Column vs. an integer[] Column?,"
TLDR: If I want to save arrays of integers in a Postgres table, are there any pros or cons to using an array column (integer[]) vs. using a JSON column (eg. does one perform better than the other)?
Backstory:
I'm using a PostgreSQL database, and Node/Knex to manage it.  Knex doesn't have any way of directly defining a PostgreSQL integer[] column type, so someone filed a Knex bug asking for it ... but one of the Knex devs closed the ticket, essentially saying that there was no need to support PostgreSQL array column types when anyone can instead use the JSON column type.
My question is, what downsides (if any) are there to using a JSON column type to hold a simple array of integers?  Are there any benefits, such as improved performance, to using a true array column, or am I equally well off by just storing my arrays inside a JSON column?
EDIT: Just to be clear, all I'm looking for in an answer is either of the following:
A) an explanation of how JSON columns and integer[] columns in PostgreSQL work, including either how one is better than the other or how the two are (at least roughly) equal.
B) no explanation, but at least a reference to some benchmarks that show that one column type or the other performs better (or that the two are equal)
",<postgresql><knex.js>,1261,0,2,34011,31,163,237,52,4026,0.0,2733,1,14,2018-03-31 21:55,2018-04-01 16:37,2018-04-01 16:37,1.0,1.0,Intermediate,23,"<postgresql><knex.js>, Postgres: Are There Downsides to Using a JSON Column vs. an integer[] Column?, 
TLDR: If I want to save arrays of integers in a Postgres table, are there any pros or cons to using an array column (integer[]) vs. using a JSON column (eg. does one perform better than the other)?
Backstory:
I'm using a PostgreSQL database, and Node/Knex to manage it.  Knex doesn't have any way of directly defining a PostgreSQL integer[] column type, so someone filed a Knex bug asking for it ... but one of the Knex devs closed the ticket, essentially saying that there was no need to support PostgreSQL array column types when anyone can instead use the JSON column type.
My question is, what downsides (if any) are there to using a JSON column type to hold a simple array of integers?  Are there any benefits, such as improved performance, to using a true array column, or am I equally well off by just storing my arrays inside a JSON column?
EDIT: Just to be clear, all I'm looking for in an answer is either of the following:
A) an explanation of how JSON columns and integer[] columns in PostgreSQL work, including either how one is better than the other or how the two are (at least roughly) equal.
B) no explanation, but at least a reference to some benchmarks that show that one column type or the other performs better (or that the two are equal)
","<postgresql><knew.is>, postures: downed use son column vs. inter[] column?, old: want save array inter poster table, pro con use array column (inter[]) vs. use son column (eg. one perform better other)? backstory: i'm use postgresql database, node/knew manage it. knew way directly define postgresql inter[] column type, someone file knew bug ask ... one knew de close ticket, essential say need support postgresql array column type anyone instead use son column type. question is, downed (if any) use son column type hold simple array integers? benefits, improve performance, use true array column, equal well store array inside son column? edit: clear, i'm look answer either following: a) explain son column inter[] column postgresql work, include either one better two (at least roughly) equal. b) explanation, least refer benchmark show one column type perform better (or two equal)"
63354909,Is it possible to use Traefik to proxy PostgreSQL over SSL?,"Motivations
I am a running into an issue when trying to proxy PostgreSQL with Traefik over SSL using Let's Encrypt.
I did some research but it is not well documented and I would like to confirm my observations and leave a record to everyone who faces this situation.
Configuration
I use latest versions of PostgreSQL v12 and Traefik v2. I want to build a pure TCP flow from tcp://example.com:5432 -&gt; tcp://postgresql:5432 over TLS using Let's Encrypt.
Traefik service is configured as follow:
  version: &quot;3.6&quot;
    services:
      traefik:
        image: traefik:latest
        restart: unless-stopped
        volumes:
          - &quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot;
          - &quot;./configuration/traefik.toml:/etc/traefik/traefik.toml:ro&quot;
          - &quot;./configuration/dynamic_conf.toml:/etc/traefik/dynamic_conf.toml&quot;
          - &quot;./letsencrypt/acme.json:/acme.json&quot;
        networks:
          - backend
        ports:
          - &quot;80:80&quot;
          - &quot;443:443&quot;
          - &quot;5432:5432&quot;
    networks:
      backend:
        external: true
With the static setup:
[entryPoints]
  [entryPoints.web]
    address = &quot;:80&quot;
    [entryPoints.web.http]
      [entryPoints.web.http.redirections.entryPoint]
        to = &quot;websecure&quot;
        scheme = &quot;https&quot;
  [entryPoints.websecure]
    address = &quot;:443&quot;
    [entryPoints.websecure.http]
      [entryPoints.websecure.http.tls]
        certresolver = &quot;lets&quot;
  [entryPoints.postgres]
    address = &quot;:5432&quot;
PostgreSQL service is configured as follow:
version: &quot;3.6&quot;
services:
  postgresql:
    image: postgres:latest
    environment:
      - POSTGRES_PASSWORD=secret
    volumes:
      - ./configuration/trial_config.conf:/etc/postgresql/postgresql.conf:ro
      - ./configuration/trial_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./configuration/initdb:/docker-entrypoint-initdb.d
      - postgresql-data:/var/lib/postgresql/data
    networks:
      - backend
    #ports:
    #  - 5432:5432
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`example.com`)&quot;
      - &quot;traefic.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
networks:
  backend:
    external: true
volumes:
  postgresql-data:
It seems my Traefik configuration is correct. Everything is OK in the logs and all sections in dashboard are flagged as Success (no Warnings, no Errors). So I am confident with the Traefik configuration above. The complete flow is about:
EntryPoint(':5432') -&gt; HostSNI(`example.com`) -&gt; TcpRouter(`postgres`) -&gt; Service(`postgres@docker`)
But, it may have a limitation at PostgreSQL side.
Debug
The problem is that I cannot connect the PostgreSQL database. I always get a Timeout error.
I have checked PostgreSQL is listening properly (main cause of Timeout error):
# - Connection Settings -
listen_addresses = '*'
port = 5432
And I checked that I can connect PostgreSQL on the host (outside the container):
psql --host 172.19.0.4 -U postgres
Password for user postgres:
psql (12.2 (Ubuntu 12.2-4), server 12.3 (Debian 12.3-1.pgdg100+1))
Type &quot;help&quot; for help.
postgres=#
Thus I know PostgreSQL is listening outside its container, so Traefik should be able to bind the flow.
I also have checked external traefik can reach the server:
sudo tcpdump -i ens3 port 5432
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes
09:02:37.878614 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [S], seq 1027429527, win 64240, options [mss 1452,nop,wscale 8,nop,nop,sackOK], length 0
09:02:37.879858 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [S.], seq 3545496818, ack 1027429528, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
09:02:37.922591 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [.], ack 1, win 516, length 0
09:02:37.922718 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [P.], seq 1:9, ack 1, win 516, length 8
09:02:37.922750 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [.], ack 9, win 502, length 0
09:02:47.908808 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [F.], seq 9, ack 1, win 516, length 0
09:02:47.909578 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [P.], seq 1:104, ack 10, win 502, length 103
09:02:47.909754 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [F.], seq 104, ack 10, win 502, length 0
09:02:47.961826 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [R.], seq 10, ack 104, win 0, length 0
So, I am wondering why the connection cannot succeed. Something must be wrong between Traefik and PostgreSQL.
SNI incompatibility?
Even when I remove the TLS configuration, the problem is still there, so I don't expect the TLS to be the origin of this problem.
Then I searched and I found few posts relating similar issue:
Introducing SNI in TLS handshake for SSL connections
Traefik 2.0 TCP routing for multiple DBs;
As far as I understand it, the SSL protocol of PostgreSQL is a custom one and does not support SNI for now and might never support it. If it is correct, it will confirm that Traefik cannot proxy PostgreSQL for now and this is a limitation.
By writing this post I would like to confirm my observations and at the same time leave a visible record on Stack Overflow to anyone who faces the same problem and seek for help. My question is then: Is it possible to use Traefik to proxy PostgreSQL?
Update
Intersting observation, if using HostSNI('*') and Let's Encrypt:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Everything is flagged as success in Dashboard but of course Let's Encrypt cannot perform the DNS Challenge for wildcard *, it complaints in logs:
time=&quot;2020-08-12T10:25:22Z&quot; level=error msg=&quot;Unable to obtain ACME certificate for domains \&quot;*\&quot;: unable to generate a wildcard certificate in ACME provider for domain \&quot;*\&quot; : ACME needs a DNSChallenge&quot; providerName=lets.acme routerName=postgres@docker rule=&quot;HostSNI(`*`)&quot;
When I try the following configuration:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.domains[0].main=example.com&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
The error vanishes from logs and in both setups the dashboard seems ok but traffic is not routed to PostgreSQL (time out). Anyway, removing SSL from the configuration makes the flow complete (and unsecure):
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Then it is possible to connect PostgreSQL database:
time=&quot;2020-08-12T10:30:52Z&quot; level=debug msg=&quot;Handling connection from x.y.z.w:58389&quot;
",<postgresql><tcp><proxy><traefik><sni>,8147,3,122,7550,3,39,59,73,10855,0.0,2039,3,14,2020-08-11 9:00,2020-08-12 7:16,2023-01-13 8:21,1.0,885.0,Basic,9,"<postgresql><tcp><proxy><traefik><sni>, Is it possible to use Traefik to proxy PostgreSQL over SSL?, Motivations
I am a running into an issue when trying to proxy PostgreSQL with Traefik over SSL using Let's Encrypt.
I did some research but it is not well documented and I would like to confirm my observations and leave a record to everyone who faces this situation.
Configuration
I use latest versions of PostgreSQL v12 and Traefik v2. I want to build a pure TCP flow from tcp://example.com:5432 -&gt; tcp://postgresql:5432 over TLS using Let's Encrypt.
Traefik service is configured as follow:
  version: &quot;3.6&quot;
    services:
      traefik:
        image: traefik:latest
        restart: unless-stopped
        volumes:
          - &quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot;
          - &quot;./configuration/traefik.toml:/etc/traefik/traefik.toml:ro&quot;
          - &quot;./configuration/dynamic_conf.toml:/etc/traefik/dynamic_conf.toml&quot;
          - &quot;./letsencrypt/acme.json:/acme.json&quot;
        networks:
          - backend
        ports:
          - &quot;80:80&quot;
          - &quot;443:443&quot;
          - &quot;5432:5432&quot;
    networks:
      backend:
        external: true
With the static setup:
[entryPoints]
  [entryPoints.web]
    address = &quot;:80&quot;
    [entryPoints.web.http]
      [entryPoints.web.http.redirections.entryPoint]
        to = &quot;websecure&quot;
        scheme = &quot;https&quot;
  [entryPoints.websecure]
    address = &quot;:443&quot;
    [entryPoints.websecure.http]
      [entryPoints.websecure.http.tls]
        certresolver = &quot;lets&quot;
  [entryPoints.postgres]
    address = &quot;:5432&quot;
PostgreSQL service is configured as follow:
version: &quot;3.6&quot;
services:
  postgresql:
    image: postgres:latest
    environment:
      - POSTGRES_PASSWORD=secret
    volumes:
      - ./configuration/trial_config.conf:/etc/postgresql/postgresql.conf:ro
      - ./configuration/trial_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./configuration/initdb:/docker-entrypoint-initdb.d
      - postgresql-data:/var/lib/postgresql/data
    networks:
      - backend
    #ports:
    #  - 5432:5432
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`example.com`)&quot;
      - &quot;traefic.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
networks:
  backend:
    external: true
volumes:
  postgresql-data:
It seems my Traefik configuration is correct. Everything is OK in the logs and all sections in dashboard are flagged as Success (no Warnings, no Errors). So I am confident with the Traefik configuration above. The complete flow is about:
EntryPoint(':5432') -&gt; HostSNI(`example.com`) -&gt; TcpRouter(`postgres`) -&gt; Service(`postgres@docker`)
But, it may have a limitation at PostgreSQL side.
Debug
The problem is that I cannot connect the PostgreSQL database. I always get a Timeout error.
I have checked PostgreSQL is listening properly (main cause of Timeout error):
# - Connection Settings -
listen_addresses = '*'
port = 5432
And I checked that I can connect PostgreSQL on the host (outside the container):
psql --host 172.19.0.4 -U postgres
Password for user postgres:
psql (12.2 (Ubuntu 12.2-4), server 12.3 (Debian 12.3-1.pgdg100+1))
Type &quot;help&quot; for help.
postgres=#
Thus I know PostgreSQL is listening outside its container, so Traefik should be able to bind the flow.
I also have checked external traefik can reach the server:
sudo tcpdump -i ens3 port 5432
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes
09:02:37.878614 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [S], seq 1027429527, win 64240, options [mss 1452,nop,wscale 8,nop,nop,sackOK], length 0
09:02:37.879858 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [S.], seq 3545496818, ack 1027429528, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
09:02:37.922591 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [.], ack 1, win 516, length 0
09:02:37.922718 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [P.], seq 1:9, ack 1, win 516, length 8
09:02:37.922750 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [.], ack 9, win 502, length 0
09:02:47.908808 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [F.], seq 9, ack 1, win 516, length 0
09:02:47.909578 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [P.], seq 1:104, ack 10, win 502, length 103
09:02:47.909754 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [F.], seq 104, ack 10, win 502, length 0
09:02:47.961826 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [R.], seq 10, ack 104, win 0, length 0
So, I am wondering why the connection cannot succeed. Something must be wrong between Traefik and PostgreSQL.
SNI incompatibility?
Even when I remove the TLS configuration, the problem is still there, so I don't expect the TLS to be the origin of this problem.
Then I searched and I found few posts relating similar issue:
Introducing SNI in TLS handshake for SSL connections
Traefik 2.0 TCP routing for multiple DBs;
As far as I understand it, the SSL protocol of PostgreSQL is a custom one and does not support SNI for now and might never support it. If it is correct, it will confirm that Traefik cannot proxy PostgreSQL for now and this is a limitation.
By writing this post I would like to confirm my observations and at the same time leave a visible record on Stack Overflow to anyone who faces the same problem and seek for help. My question is then: Is it possible to use Traefik to proxy PostgreSQL?
Update
Intersting observation, if using HostSNI('*') and Let's Encrypt:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Everything is flagged as success in Dashboard but of course Let's Encrypt cannot perform the DNS Challenge for wildcard *, it complaints in logs:
time=&quot;2020-08-12T10:25:22Z&quot; level=error msg=&quot;Unable to obtain ACME certificate for domains \&quot;*\&quot;: unable to generate a wildcard certificate in ACME provider for domain \&quot;*\&quot; : ACME needs a DNSChallenge&quot; providerName=lets.acme routerName=postgres@docker rule=&quot;HostSNI(`*`)&quot;
When I try the following configuration:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.domains[0].main=example.com&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
The error vanishes from logs and in both setups the dashboard seems ok but traffic is not routed to PostgreSQL (time out). Anyway, removing SSL from the configuration makes the flow complete (and unsecure):
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Then it is possible to connect PostgreSQL database:
time=&quot;2020-08-12T10:30:52Z&quot; level=debug msg=&quot;Handling connection from x.y.z.w:58389&quot;
","<postgresql><top><prove><traffic><sin>, possible use traffic prove postgresql sal?, motive run issue try prove postgresql traffic sal use let' encrypt. research well document would like confirm observe leave record everyone face situation. configur use latest version postgresql ve traffic ve. want build pure top flow top://example.com:5432 -&it; top://postgresql:5432 to use let' encrypt. traffic service configur follow: version: &quit;3.6&quit; services: traffic: image: traffic:latest start: unless-stop volumes: - &quit;/war/run/doctor.sock:/war/run/doctor.sock:to&quit; - &quit;./configuration/traffic.toll:/etc/traffic/traffic.toll:to&quit; - &quit;./configuration/dynamic_conf.toll:/etc/traffic/dynamic_conf.toll&quit; - &quit;./letsencrypt/acme.son:/acme.son&quit; network: - backed ports: - &quit;80:80&quit; - &quit;443:443&quit; - &quit;5432:5432&quit; network: backed: external: true static set: [entrypoints] [entrypoints.web] address = &quit;:80&quit; [entrypoints.web.http] [entrypoints.web.http.directions.entrypoint] = &quit;websecure&quit; scheme = &quit;http&quit; [entrypoints.websecure] address = &quit;:443&quit; [entrypoints.websecure.http] [entrypoints.websecure.http.tis] certresolv = &quit;lets&quit; [entrypoints.postures] address = &quit;:5432&quit; postgresql service configur follow: version: &quit;3.6&quit; services: postgresql: image: postures:latest environment: - postgres_password=secret volumes: - ./configuration/trial_config.cone:/etc/postgresql/postgresql.cone:to - ./configuration/trial_hba.cone:/etc/postgresql/pg_hba.cone:to - ./configuration/initdb:/doctor-entrypoint-initdb.d - postgresql-data:/war/limb/postgresql/data network: - backed #ports: # - 5432:5432 labels: - &quit;traffic.enable=true&quit; - &quit;traffic.doctor.network=backed&quit; - &quit;traffic.top.routes.postures.entrypoints=postures&quit; - &quit;traffic.top.routes.postures.rule=hosts(`example.com`)&quit; - &quit;traffic.top.routes.postures.tis=true&quit; - &quit;traffic.top.routes.postures.tis.certresolver=lets&quit; - &quit;traffic.top.services.postures.loadbalancer.server.port=5432&quit; network: backed: external: true volumes: postgresql-data: seem traffic configur correct. every ok log section dashboard flag success (no warnings, errors). confide traffic configur above. complete flow about: entrypoint(':5432') -&it; hosts(`example.com`) -&it; tcprouter(`postures`) -&it; service(`postures@doctor`) but, may limit postgresql side. debut problem cannot connect postgresql database. away get timeout error. check postgresql listen properly (main cause timeout error): # - connect set - listen_address = '*' port = 5432 check connect postgresql host (outside container): pool --host 172.19.0.4 -u poster password user postures: pool (12.2 (bunt 12.2-4), server 12.3 (median 12.3-1.pgdg100+1)) type &quit;help&quit; help. postures=# the know postgresql listen outside container, traffic all bind flow. also check externa traffic reach server: so tcpdump -i end port 5432 tcpdump: verbs output suppressed, use -v -ve full protocol second listen end, link-type en10mb (ethernet), capture size 262144 bite 09:02:37.878614 in x.y-z-w.is.com.61229 &it; example.com.postgresql: flag [s], see 1027429527, win 64240, option [miss 1452,not,wascal 8,not,not,sack], length 0 09:02:37.879858 in example.com.postgresql &it; x.y-z-w.is.com.61229: flag [s.], see 3545496818, back 1027429528, win 64240, option [miss 1460,not,not,sack,not,wascal 7], length 0 09:02:37.922591 in x.y-z-w.is.com.61229 &it; example.com.postgresql: flag [.], back 1, win 516, length 0 09:02:37.922718 in x.y-z-w.is.com.61229 &it; example.com.postgresql: flag [p.], see 1:9, back 1, win 516, length 8 09:02:37.922750 in example.com.postgresql &it; x.y-z-w.is.com.61229: flag [.], back 9, win 502, length 0 09:02:47.908808 in x.y-z-w.is.com.61229 &it; example.com.postgresql: flag [f.], see 9, back 1, win 516, length 0 09:02:47.909578 in example.com.postgresql &it; x.y-z-w.is.com.61229: flag [p.], see 1:104, back 10, win 502, length 103 09:02:47.909754 in example.com.postgresql &it; x.y-z-w.is.com.61229: flag [f.], see 104, back 10, win 502, length 0 09:02:47.961826 in x.y-z-w.is.com.61229 &it; example.com.postgresql: flag [r.], see 10, back 104, win 0, length 0 so, wonder connect cannot succeed. cometh must wrong traffic postgresql. sin incompatibility? even remove to configuration, problem still there, expect to origin problem. search found post relate similar issue: introduce sin to handshak sal connect traffic 2.0 top rout multiple des; far understand it, sal protocol postgresql custom one support sin might never support it. correct, confirm traffic cannot prove postgresql limitation. write post would like confirm observe time leave visible record stick overflow anyone face problem seek help. question then: possible use traffic prove postgresql? update interest observation, use hosts('*') let' encrypt: labels: - &quit;traffic.enable=true&quit; - &quit;traffic.doctor.network=backed&quit; - &quit;traffic.top.routes.postures.entrypoints=postures&quit; - &quit;traffic.top.routes.postures.rule=hosts(`*`)&quit; - &quit;traffic.top.routes.postures.tis=true&quit; - &quit;traffic.top.routes.postures.tis.certresolver=lets&quit; - &quit;traffic.top.services.postures.loadbalancer.server.port=5432&quit; every flag success dashboard course let' encrypt cannot perform in challenge willard *, complaint logs: time=&quit;2020-08-12t10:25:z&quit; level=error mug=&quit;un obtain am certify domain \&quit;*\&quit;: unable genet willard certify am proved domain \&quit;*\&quit; : am need dnschallenge&quit; providername=lets.am routername=postures@dock rule=&quit;hosts(`*`)&quit; try follow configuration: labels: - &quit;traffic.enable=true&quit; - &quit;traffic.doctor.network=backed&quit; - &quit;traffic.top.routes.postures.entrypoints=postures&quit; - &quit;traffic.top.routes.postures.rule=hosts(`*`)&quit; - &quit;traffic.top.routes.postures.tis=true&quit; - &quit;traffic.top.routes.postures.tis.domain[0].main=example.com&quit; - &quit;traffic.top.routes.postures.tis.certresolver=lets&quit; - &quit;traffic.top.services.postures.loadbalancer.server.port=5432&quit; error vanish log set dashboard seem ok traffic rout postgresql (time out). anyway, remove sal configur make flow complete (and insecure): labels: - &quit;traffic.enable=true&quit; - &quit;traffic.doctor.network=backed&quit; - &quit;traffic.top.routes.postures.entrypoints=postures&quit; - &quit;traffic.top.routes.postures.rule=hosts(`*`)&quit; - &quit;traffic.top.services.postures.loadbalancer.server.port=5432&quit; possible connect postgresql database: time=&quit;2020-08-12t10:30:z&quit; level=debut mug=&quit;hand connect x.y.z.w:58389&quit;"
55485858,Using SQLite3 with Django 2.2 and Python 3.6.7 on Centos7,"I am moving my Django code from 2.1.7 directly to the new Django 2.2.  The only problem I encountered in my Centos7 development environment was that my local development database (sqlite3) version was incompatible using my Python 3.6.7.
The error I was getting from ""manage.py runserver"" was:
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later
I am unable to use another version of Python because this is the maximum supported by AWS elasticbeanstalk.  The Python 3.6.7 seems to come with sqlite module of version:
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt; 
I use a seperate development account on my local Centos7 workstation and issue pipenv shell to begin my code development and IDE.  
The only workaround I've found is to manually download SQLite3 autoconf version 3.27.2 and manually compile into that development account home folder using the following commands: 
wget https://www.sqlite.org/2019/sqlite-autoconf-3270200.tar.gz
gzip -d sqlite-autoconf-3270200.tar.gz
tar -xvf sqlite-autoconf-3270200.tar
cd sqlite-autoconf-3270200/
./configure --prefix=/home/devuser/opt/
make
make install
Following that, I have modified my .bashrc to reflect the following: 
export LD_LIBRARY_PATH=""${HOME}/opt/lib""
This seems to do the trick when I log back into my devuser account.  My app seems to run correctly using my local development database.
Python 3.6.7 (default, Dec  5 2018, 15:02:05)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux
&gt;&gt;&gt;import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.27.2'
My local development database is SQLite, but my settings.py does not load any SQLite3 database backend when it senses it's in production on AWS (uses Mysql production database as backend when environment variable flag PRODUCTION is checked).
Is my understanding of the problem correct and is my approach and implementation acceptable? 
I felt that recompiling python was a huge waste of time, and to be honest it may have been faster to stand up a local mysql version and stop wasting time with sqlite... but it's so nice to just copy or dump a file, migrate, and loaddata for a fresh start.
",<django><sqlite><python-3.6><pipenv><django-2.2>,2237,1,22,153,0,3,12,56,7109,0.0,28,1,14,2019-04-03 1:45,2019-06-14 8:47,,72.0,,Basic,9,"<django><sqlite><python-3.6><pipenv><django-2.2>, Using SQLite3 with Django 2.2 and Python 3.6.7 on Centos7, I am moving my Django code from 2.1.7 directly to the new Django 2.2.  The only problem I encountered in my Centos7 development environment was that my local development database (sqlite3) version was incompatible using my Python 3.6.7.
The error I was getting from ""manage.py runserver"" was:
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later
I am unable to use another version of Python because this is the maximum supported by AWS elasticbeanstalk.  The Python 3.6.7 seems to come with sqlite module of version:
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt; 
I use a seperate development account on my local Centos7 workstation and issue pipenv shell to begin my code development and IDE.  
The only workaround I've found is to manually download SQLite3 autoconf version 3.27.2 and manually compile into that development account home folder using the following commands: 
wget https://www.sqlite.org/2019/sqlite-autoconf-3270200.tar.gz
gzip -d sqlite-autoconf-3270200.tar.gz
tar -xvf sqlite-autoconf-3270200.tar
cd sqlite-autoconf-3270200/
./configure --prefix=/home/devuser/opt/
make
make install
Following that, I have modified my .bashrc to reflect the following: 
export LD_LIBRARY_PATH=""${HOME}/opt/lib""
This seems to do the trick when I log back into my devuser account.  My app seems to run correctly using my local development database.
Python 3.6.7 (default, Dec  5 2018, 15:02:05)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux
&gt;&gt;&gt;import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.27.2'
My local development database is SQLite, but my settings.py does not load any SQLite3 database backend when it senses it's in production on AWS (uses Mysql production database as backend when environment variable flag PRODUCTION is checked).
Is my understanding of the problem correct and is my approach and implementation acceptable? 
I felt that recompiling python was a huge waste of time, and to be honest it may have been faster to stand up a local mysql version and stop wasting time with sqlite... but it's so nice to just copy or dump a file, migrate, and loaddata for a fresh start.
","<django><quite><patron-3.6><pipe><django-2.2>, use sqlite3 django 2.2 patron 3.6.7 cents, move django code 2.1.7 directly new django 2.2. problem count cents develop environs local develop database (sqlite3) version incompat use patron 3.6.7. error get ""manage.i runserver"" was: django.core.exceptions.improperlyconfigured: quite 3.8.3 later unable use not version patron maximum support a elasticbeanstalk. patron 3.6.7 seem come quite model version: &it;&it;&it; import sqlite3 &it;&it;&it; sqlite3.very '2.6.0' &it;&it;&it; sqlite3.sqlite_vers '3.7.17' &it;&it;&it; use super develop account local cents workstat issue pipe shell begin code develop side. workaround i'v found manual download sqlite3 autoconf version 3.27.2 manual compel develop account home older use follow commands: get http://www.quite.org/2019/quite-autoconf-3270200.tar.go grip -d quite-autoconf-3270200.tar.go tar -xvi quite-autoconf-3270200.tar d quite-autoconf-3270200/ ./configur --prefix=/home/devised/opt/ make make instal follow that, modify .basic reflect following: export ld_library_path=""${home}/opt/limb"" seem trick log back devil account. pp seem run correctly use local develop database. patron 3.6.7 (default, dec 5 2018, 15:02:05) [go 4.8.5 20150623 (red hat 4.8.5-36)] line &it;&it;&it;import sqlite3 &it;&it;&it; sqlite3.very '2.6.0' &it;&it;&it; sqlite3.sqlite_vers '3.27.2' local develop database quite, settings.i load sqlite3 database backed sens product a (use myself product database backed environs variable flag product checked). understand problem correct approach implement acceptable? felt recoil patron huge wast time, honest may faster stand local myself version stop wast time quite... nice copy dump file, migrate, loaddata fresh start."
51968981,"Merging duplicated records together with ""Merge"" syntax","I am using SQL Server 2014. I am currently trying to combine millions of personnel application records in to a single personnel record.  
The records contain the following columns:
ID, First_Name, Last_Name, DOB, Post_Code, Mobile, Email
A person can enter their details numerous times but due to fat fingers or fraud they can sometimes put in, incorrect details.  
In my example Christopher has filled his details in 5 times, First_Name, Last_Name, DOB are always correct, Post_Code, Mobile and Email contain various connotations.  
What I want to do is take the min(id) associated with this group in this case 84015283 and put it in to a new table, this will be the primary key and then you will see the other id's that are associated with it.
Examples
NID       CID
------------------
84015283  84015283
84015283  84069198
84015283  84070263
84015283  84369603
84015283  85061159
Where it gets a little complicated is, where 2 different people can have the same First_Name, Last_Name and DOB, at least one of the other fields must match ""post_code, mobile or email"" as per my example to another record within the group.
Though first_name, last_name, DoB match between ID's 84015283, 84069198, 84070263. 84015283, 84069198 are identical so they would match without an issue, 84070263 matches on the postcode, 84369603 matches on the mobile to a previous record and 85061159 matches on a previous mobile/email but not post_code.
If putting the NID within the original dataset is easier I can go with this rather than putting it all in a separate table.
After some googling and trying to get my head around this, I believe that using ""Merge"" might be a good way to achieve what I am after but I am concerned it will take a very long time due to the number of records involved.
Also going forward any routine would have to be run on subsequent new records.
I have listed the code for the example if anyone can help
DROP TABLE customer_dist
CREATE TABLE [dbo].customer_dist
(
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL,
)
INSERT INTO customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('84015283', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84069198', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84070263', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559822', 'CHigg@AOL.com'),
       ('84369603', 'Christopher', 'Higg', '1956-01-13', 'CH2 3ZA', '07089559829', 'Higg@emailme.com'),
       ('85061159', 'CHRISTOPHER', 'Higg', '1956-01-13', 'CH2 3RA', '07089559829', 'CH@hotmail.com'),
       ('87065122', 'Matthew', 'Davis', '1978-05-10', 'CH5 1TS', '07077084692', 'Matt@gamil.com')
SELECT * FROM customer_dist
Below is the expected results, sorry I should of made it clearer what I wanted at the end.
Output Table Results
    NID         id          First_Name  Last_Name   DoB         post_code   mobile          Email
    84015283    84015283    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84069198    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84070263    Christopher Higg            1/13/1956   CH2 3AZ         7089559822  CHigg@AOL.com
    84015283    84369603    Christopher Higg            1/13/1956   CH2 3ZA         7089559829  Higg@emailme.com
    84015283    85061159    CHRISTOPHER Higg            1/13/1956   CH2 3RA         7089559829  CH@hotmail.com
    78065122    87065122    Matthew Davis               05/10/1978  CH5 1TS
7077084692  Matt@gamil.com
OR                          
NID         id
84015283    84015283
84015283    84069198
84015283    84070263
84015283    84369603
84015283    85061159
87065122    87065122
Apologies for the slow response.
I have updated my required output, I was asked to include an extra record that was not a match to the other records but did not include this in my required output.
HABO's response was the closest to what was needed unfortunately on further testing with other sample data, duplicates were created and the logic broke down.  Other Sample data would be :-
declare @customer_dist as Table (
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL );
INSERT INTO @customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('32006455', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07706212920',  'nastie220@yahoo.com'),
       ('35963960', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863324',  'nastie@hotmail.com'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com'),
       ('46653041', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie2010@yahoo.com'),
       ('48023677', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie@hotmail.com'),
       ('49560434', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie@hotmail.com'),
       ('49861032', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie2001@yahoo.com'),
       ('53130969', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'Nastie@hotmail.cm'),
       ('33843283', 'Mary', 'Wilson',   '1983-09-20',   'BT148HU',  '07484863478',  'nastie2010@yahoo.co.uk'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com')
SELECT * FROM @customer_dist;
",<sql><sql-server><t-sql><sql-server-2014>,5802,1,84,743,2,13,38,58,976,,8,9,14,2018-08-22 14:20,2018-08-22 15:05,,0.0,,Basic,10,"<sql><sql-server><t-sql><sql-server-2014>, Merging duplicated records together with ""Merge"" syntax, I am using SQL Server 2014. I am currently trying to combine millions of personnel application records in to a single personnel record.  
The records contain the following columns:
ID, First_Name, Last_Name, DOB, Post_Code, Mobile, Email
A person can enter their details numerous times but due to fat fingers or fraud they can sometimes put in, incorrect details.  
In my example Christopher has filled his details in 5 times, First_Name, Last_Name, DOB are always correct, Post_Code, Mobile and Email contain various connotations.  
What I want to do is take the min(id) associated with this group in this case 84015283 and put it in to a new table, this will be the primary key and then you will see the other id's that are associated with it.
Examples
NID       CID
------------------
84015283  84015283
84015283  84069198
84015283  84070263
84015283  84369603
84015283  85061159
Where it gets a little complicated is, where 2 different people can have the same First_Name, Last_Name and DOB, at least one of the other fields must match ""post_code, mobile or email"" as per my example to another record within the group.
Though first_name, last_name, DoB match between ID's 84015283, 84069198, 84070263. 84015283, 84069198 are identical so they would match without an issue, 84070263 matches on the postcode, 84369603 matches on the mobile to a previous record and 85061159 matches on a previous mobile/email but not post_code.
If putting the NID within the original dataset is easier I can go with this rather than putting it all in a separate table.
After some googling and trying to get my head around this, I believe that using ""Merge"" might be a good way to achieve what I am after but I am concerned it will take a very long time due to the number of records involved.
Also going forward any routine would have to be run on subsequent new records.
I have listed the code for the example if anyone can help
DROP TABLE customer_dist
CREATE TABLE [dbo].customer_dist
(
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL,
)
INSERT INTO customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('84015283', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84069198', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84070263', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559822', 'CHigg@AOL.com'),
       ('84369603', 'Christopher', 'Higg', '1956-01-13', 'CH2 3ZA', '07089559829', 'Higg@emailme.com'),
       ('85061159', 'CHRISTOPHER', 'Higg', '1956-01-13', 'CH2 3RA', '07089559829', 'CH@hotmail.com'),
       ('87065122', 'Matthew', 'Davis', '1978-05-10', 'CH5 1TS', '07077084692', 'Matt@gamil.com')
SELECT * FROM customer_dist
Below is the expected results, sorry I should of made it clearer what I wanted at the end.
Output Table Results
    NID         id          First_Name  Last_Name   DoB         post_code   mobile          Email
    84015283    84015283    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84069198    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84070263    Christopher Higg            1/13/1956   CH2 3AZ         7089559822  CHigg@AOL.com
    84015283    84369603    Christopher Higg            1/13/1956   CH2 3ZA         7089559829  Higg@emailme.com
    84015283    85061159    CHRISTOPHER Higg            1/13/1956   CH2 3RA         7089559829  CH@hotmail.com
    78065122    87065122    Matthew Davis               05/10/1978  CH5 1TS
7077084692  Matt@gamil.com
OR                          
NID         id
84015283    84015283
84015283    84069198
84015283    84070263
84015283    84369603
84015283    85061159
87065122    87065122
Apologies for the slow response.
I have updated my required output, I was asked to include an extra record that was not a match to the other records but did not include this in my required output.
HABO's response was the closest to what was needed unfortunately on further testing with other sample data, duplicates were created and the logic broke down.  Other Sample data would be :-
declare @customer_dist as Table (
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL );
INSERT INTO @customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('32006455', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07706212920',  'nastie220@yahoo.com'),
       ('35963960', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863324',  'nastie@hotmail.com'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com'),
       ('46653041', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie2010@yahoo.com'),
       ('48023677', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie@hotmail.com'),
       ('49560434', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie@hotmail.com'),
       ('49861032', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie2001@yahoo.com'),
       ('53130969', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'Nastie@hotmail.cm'),
       ('33843283', 'Mary', 'Wilson',   '1983-09-20',   'BT148HU',  '07484863478',  'nastie2010@yahoo.co.uk'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com')
SELECT * FROM @customer_dist;
","<sal><sal-server><t-sal><sal-server-2014>, berg public record together ""merge"" santa, use sal server 2014. current try combine million personnel applied record single personnel record. record contain follow columns: id, first_name, last_name, do, post_code, mobile, email person enter detail number time due fat finger fraud sometime put in, incorrect details. example christopher fill detail 5 times, first_name, last_name, do away correct, post_code, mobile email contain various consolations. want take min(id) cassock group case 84015283 put new table, primary key see id' cassock it. example did did ------------------ 84015283 84015283 84015283 84069198 84015283 84070263 84015283 84369603 84015283 85061159 get little complex is, 2 differ people first_name, last_nam do, least one field must match ""post_code, mobile email"" per example not record within group. though first_name, last_name, do match id' 84015283, 84069198, 84070263. 84015283, 84069198 went would match without issue, 84070263 match postpone, 84369603 match mobile previous record 85061159 match previous mobile/email post_code. put did within origin dataset easier go rather put spear table. good try get head around this, believe use ""merge"" might good way achieve concern take long time due number record involved. also go forward routine would run subsequ new records. list code example anyone help drop table customer_dist great table [do].customer_dist ( [id] [in] null, [first_name] [varchar](50) null, [last_name] [varchar](50) null, [do] [date] null, [post_code] [varchar](50) null, [mobile] [varchar](50) null, [email] [varchar](100) null, ) insert customer_dist (id, first_name, last_name, do, post_code, mobile, email) value ('84015283', 'christopher', 'high', '1956-01-13', 'ch a', '07089559829', 'ch@hotmail.com'), ('84069198', 'christopher', 'high', '1956-01-13', 'ch a', '07089559829', 'ch@hotmail.com'), ('84070263', 'christopher', 'high', '1956-01-13', 'ch a', '07089559822', 'chief@all.com'), ('84369603', 'christopher', 'high', '1956-01-13', 'ch a', '07089559829', 'high@email.com'), ('85061159', 'christopher', 'high', '1956-01-13', 'ch era', '07089559829', 'ch@hotmail.com'), ('87065122', 'matthew', 'davis', '1978-05-10', 'ch its', '07077084692', 'matt@family.com') select * customer_dist expect results, sorry made clearer want end. output table result did id first_nam last_nam do post_cod mobile email 84015283 84015283 christopher high 1/13/1956 ch a 7089559829 ch@hotmail.com 84015283 84069198 christopher high 1/13/1956 ch a 7089559829 ch@hotmail.com 84015283 84070263 christopher high 1/13/1956 ch a 7089559822 chief@all.com 84015283 84369603 christopher high 1/13/1956 ch a 7089559829 high@email.com 84015283 85061159 christopher high 1/13/1956 ch era 7089559829 ch@hotmail.com 78065122 87065122 matthew davis 05/10/1978 ch it 7077084692 matt@family.com did id 84015283 84015283 84015283 84069198 84015283 84070263 84015283 84369603 84015283 85061159 87065122 87065122 apology slow response. update require output, ask include extra record match record include require output. halo' response closest need unfortun test sample data, public great logic broke down. sample data would :- declare @customer_dist table ( [id] [in] null, [first_name] [varchar](50) null, [last_name] [varchar](50) null, [do] [date] null, [post_code] [varchar](50) null, [mobile] [varchar](50) null, [email] [varchar](100) null ); insert @customer_dist (id, first_name, last_name, do, post_code, mobile, email) value ('32006455', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07706212920', 'nastie220@yakov.com'), ('35963960', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07484863324', 'native@hotmail.com'), ('38627975', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07484863478', 'nastie2001@yakov.com'), ('46653041', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07483888179', 'nastie2010@yakov.com'), ('48023677', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07483888179', 'native@hotmail.com'), ('49560434', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07849727199', 'native@hotmail.com'), ('49861032', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07849727199', 'nastie2001@yakov.com'), ('53130969', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07849727199', 'native@hotmail.cm'), ('33843283', 'mary', 'wilson', '1983-09-20', 'bt148hu', '07484863478', 'nastie2010@yakov.co.up'), ('38627975', 'mary', 'wilson', '1983-09-20', 'bt62ja', '07484863478', 'nastie2001@yakov.com') select * @customer_dist;"
55006231,Query to get nextval from sequence with Spring JPA,"I have a repository with which I am trying to get next value from sequence. The sequence name I need to be parameterized.
The repository query looks a like:
  @Query(value = ""SELECT ?1.NEXTVAL from dual;"", nativeQuery = true)
  String getSeqID(@Param(""seqName"") String seqName);
But, I am getting following exception:
org.hibernate.QueryException: JPA-style positional param was not an integral ordinal
    at org.hibernate.engine.query.spi.ParameterParser.parse(ParameterParser.java:187) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.ParamLocationRecognizer.parseLocations(ParamLocationRecognizer.java:59) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.internal.NativeQueryInterpreterStandardImpl.getParameterMetadata(NativeQueryInterpreterStandardImpl.java:34) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.QueryPlanCache.getSQLParameterMetadata(QueryPlanCache.java:125) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
",<java><sql><spring><jpa><spring-data-jpa>,1036,0,5,486,2,8,23,76,60988,0.0,13,5,14,2019-03-05 15:28,2019-03-05 15:40,2020-04-01 20:04,0.0,393.0,Basic,10,"<java><sql><spring><jpa><spring-data-jpa>, Query to get nextval from sequence with Spring JPA, I have a repository with which I am trying to get next value from sequence. The sequence name I need to be parameterized.
The repository query looks a like:
  @Query(value = ""SELECT ?1.NEXTVAL from dual;"", nativeQuery = true)
  String getSeqID(@Param(""seqName"") String seqName);
But, I am getting following exception:
org.hibernate.QueryException: JPA-style positional param was not an integral ordinal
    at org.hibernate.engine.query.spi.ParameterParser.parse(ParameterParser.java:187) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.ParamLocationRecognizer.parseLocations(ParamLocationRecognizer.java:59) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.internal.NativeQueryInterpreterStandardImpl.getParameterMetadata(NativeQueryInterpreterStandardImpl.java:34) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.QueryPlanCache.getSQLParameterMetadata(QueryPlanCache.java:125) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
","<cava><sal><spring><pa><spring-data-pa>, query get neutral sequence spring pa, depositors try get next value sequence. sequence name need parameterized. depositors query look like: @query(value = ""select ?1.neutral dual;"", nativequeri = true) string getseqid(@parma(""senate"") string senate); but, get follow exception: org.liberate.queryexception: pa-style post parma inter ordain org.liberate.engine.query.spy.parameterparser.pause(parameterparser.cava:187) ~[liberate-core-5.0.12.final.jar:5.0.12.final] org.liberate.engine.query.spy.paramlocationrecognizer.parselocations(paramlocationrecognizer.cava:59) ~[liberate-core-5.0.12.final.jar:5.0.12.final] org.liberate.engine.query.internal.nativequeryinterpreterstandardimpl.getparametermetadata(nativequeryinterpreterstandardimpl.cava:34) ~[liberate-core-5.0.12.final.jar:5.0.12.final] org.liberate.engine.query.spy.queryplancache.getsqlparametermetadata(queryplancache.cava:125) ~[liberate-core-5.0.12.final.jar:5.0.12.final]"
54518722,MySQL Connector could not process parameters,"I'm trying to loop through an array and insert each element into a table.  As far as I can see my syntax is correct and I took this code straight from Microsoft Azure's documentation.  
try:
   conn = mysql.connector.connect(**config)
   print(""Connection established"")
except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print(""Something is wrong with the user name or password"")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print(""Database does not exist"")
  else:
    print(err)
else:
  cursor = conn.cursor()
data = ['1','2','3','4','5']
for x in data:
   cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(x))
   print(""Inserted"",cursor.rowcount,""row(s) of data."")
conn.commit()
cursor.close()
conn.close()
print(""Done."")
When I run this is gets to cursor.execute(...) and then fails.  Here is the stack trace.
  Traceback (most recent call last):
    File ""test.py"", line 29, in 
      cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(""test""))
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\cursor_cext.py"", line 248, in execute
      prepared = self._cnx.prepare_for_mysql(params)
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\connection_cext.py"", line 538, in prepare_for_mysql
      raise ValueError(""Could not process parameters"")
  ValueError: Could not process parameters
",<python><mysql><mysql-connector-python>,1446,1,24,514,2,7,21,43,39576,,29,3,14,2019-02-04 14:52,2019-02-04 15:03,2019-02-04 15:03,0.0,0.0,Basic,14,"<python><mysql><mysql-connector-python>, MySQL Connector could not process parameters, I'm trying to loop through an array and insert each element into a table.  As far as I can see my syntax is correct and I took this code straight from Microsoft Azure's documentation.  
try:
   conn = mysql.connector.connect(**config)
   print(""Connection established"")
except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print(""Something is wrong with the user name or password"")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print(""Database does not exist"")
  else:
    print(err)
else:
  cursor = conn.cursor()
data = ['1','2','3','4','5']
for x in data:
   cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(x))
   print(""Inserted"",cursor.rowcount,""row(s) of data."")
conn.commit()
cursor.close()
conn.close()
print(""Done."")
When I run this is gets to cursor.execute(...) and then fails.  Here is the stack trace.
  Traceback (most recent call last):
    File ""test.py"", line 29, in 
      cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(""test""))
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\cursor_cext.py"", line 248, in execute
      prepared = self._cnx.prepare_for_mysql(params)
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\connection_cext.py"", line 538, in prepare_for_mysql
      raise ValueError(""Could not process parameters"")
  ValueError: Could not process parameters
","<patron><myself><myself-connection-patron>, myself connection could process parameter, i'm try loop array insert element table. far see santa correct took code straight microsoft azure' documentation. try: corn = myself.connection.connect(**confirm) print(""connect established"") except myself.connection.error err: err.error == errorcode.er_access_denied_error: print(""cometh wrong user name password"") if err.error == errorcode.er_bad_db_error: print(""database exist"") else: print(err) else: curses = corn.curses() data = ['1','2','3','4','5'] x data: curses.execute(""insert test (aerial) value (%s)"",(x)) print(""inserted"",curses.rowcount,""row(s) data."") corn.commit() curses.close() corn.close() print(""done."") run get curses.execute(...) fails. stick trace. traceback (most recent call last): file ""test.by"", line 29, curses.execute(""insert test (aerial) value (%s)"",(""test"")) file ""c:\users\alert\appdata\local\programs\patron\python37\limb\site-packages\myself\connection\cursor_cext.by"", line 248, execute prepare = self.acne.prepare_for_mysql(parts) file ""c:\users\alert\appdata\local\programs\patron\python37\limb\site-packages\myself\connection\connection_cext.by"", line 538, prepare_for_mysql rays valueerror(""could process parameter"") valueerror: could process parapet"
61081569,No exception being thrown when opening MySqlConnection?,"I'm just starting out with async and Task's and my code has stopped processing. It happens when I have an incoming network packet and I try and communicate with the database inside the packet handler.
public class ClientConnectedPacket : IClientPacket
{
    private readonly EntityFactory _entityFactory;
    public ClientConnectedPacket(EntityFactory entityFactory)
    {
        _entityFactory= entityFactory;
    }
    public async Task Handle(NetworkClient client, ClientPacketReader reader)
    {
        client.Entity = await _entityFactory.CreateInstanceAsync( reader.GetValueByKey(""unique_device_id""));
        // this Console.WriteLine never gets reached
        Console.WriteLine($""Client [{reader.GetValueByKey(""unique_device_id"")}] has connected"");
    }
}
The Handle method gets called from an async task
if (_packetRepository.TryGetPacketByName(packetName, out var packet))
{
    await packet.Handle(this, new ClientPacketReader(packetName, packetData));
}
else
{
    Console.WriteLine(""Unknown packet: "" + packetName);
}
Here is the method which I think is causing the issue
public async Task&lt;Entity&gt; CreateInstanceAsync(string uniqueId)
{
    await using (var dbConnection = _databaseProvider.GetConnection())
    { 
        dbConnection.SetQuery(""SELECT COUNT(NULL) FROM `entities` WHERE `unique_id` = @uniqueId"");
        dbConnection.AddParameter(""uniqueId"", uniqueId);
        var row = await dbConnection.ExecuteRowAsync();
        if (row != null)
        {
            return new Entity(uniqueId, false);
        }
    }
    return new Entity(uniqueId,true);
}
DatabaseProvider's GetConnection method:
public DatabaseConnection GetConnection()
{
    var connection = new MySqlConnection(_connectionString);
    var command = connection.CreateCommand();
    return new DatabaseConnection(_logFactory.GetLogger(), connection, command);
}
DatabaseConnection's constructor:
public DatabaseConnection(ILogger logger, MySqlConnection connection, MySqlCommand command)
{
    _logger = logger;
    _connection = connection;    
    _command = command;
    _connection.Open();
}
When I comment out this line, it reaches the Console.WriteLine
_connection.Open();
",<c#><mysql>,2182,0,60,1,8,31,101,36,340,,81,2,14,2020-04-07 13:49,2020-04-12 18:20,,5.0,,Basic,13,"<c#><mysql>, No exception being thrown when opening MySqlConnection?, I'm just starting out with async and Task's and my code has stopped processing. It happens when I have an incoming network packet and I try and communicate with the database inside the packet handler.
public class ClientConnectedPacket : IClientPacket
{
    private readonly EntityFactory _entityFactory;
    public ClientConnectedPacket(EntityFactory entityFactory)
    {
        _entityFactory= entityFactory;
    }
    public async Task Handle(NetworkClient client, ClientPacketReader reader)
    {
        client.Entity = await _entityFactory.CreateInstanceAsync( reader.GetValueByKey(""unique_device_id""));
        // this Console.WriteLine never gets reached
        Console.WriteLine($""Client [{reader.GetValueByKey(""unique_device_id"")}] has connected"");
    }
}
The Handle method gets called from an async task
if (_packetRepository.TryGetPacketByName(packetName, out var packet))
{
    await packet.Handle(this, new ClientPacketReader(packetName, packetData));
}
else
{
    Console.WriteLine(""Unknown packet: "" + packetName);
}
Here is the method which I think is causing the issue
public async Task&lt;Entity&gt; CreateInstanceAsync(string uniqueId)
{
    await using (var dbConnection = _databaseProvider.GetConnection())
    { 
        dbConnection.SetQuery(""SELECT COUNT(NULL) FROM `entities` WHERE `unique_id` = @uniqueId"");
        dbConnection.AddParameter(""uniqueId"", uniqueId);
        var row = await dbConnection.ExecuteRowAsync();
        if (row != null)
        {
            return new Entity(uniqueId, false);
        }
    }
    return new Entity(uniqueId,true);
}
DatabaseProvider's GetConnection method:
public DatabaseConnection GetConnection()
{
    var connection = new MySqlConnection(_connectionString);
    var command = connection.CreateCommand();
    return new DatabaseConnection(_logFactory.GetLogger(), connection, command);
}
DatabaseConnection's constructor:
public DatabaseConnection(ILogger logger, MySqlConnection connection, MySqlCommand command)
{
    _logger = logger;
    _connection = connection;    
    _command = command;
    _connection.Open();
}
When I comment out this line, it reaches the Console.WriteLine
_connection.Open();
","<c#><myself>, except thrown open mysqlconnection?, i'm start async task' code stop processing. happen income network packet try common database inside packet handle. public class clientconnectedpacket : iclientpacket { privat readonli entityfactori _entityfactory; public clientconnectedpacket(entityfactori entityfactory) { _entityfactory= entityfactory; } public async task handle(networkcli client, clientpacketread reader) { client.went = await _entityfactory.createinstanceasync( reader.getvaluebykey(""unique_device_id"")); // console.writelin never get reach console.writeline($""coli [{reader.getvaluebykey(""unique_device_id"")}] connected""); } } hand method get call async task (_packetrepository.trygetpacketbyname(packetname, war packet)) { await packet.handle(this, new clientpacketreader(packetname, packetdata)); } else { console.writeline(""unknown packet: "" + packetname); } method think cause issue public async task&it;entity&it; createinstanceasync(sir unique) { await use (war connect = _databaseprovider.getconnection()) { connection.setquery(""select count(null) `entitles` `unique_id` = @unique""); connection.addparameter(""unique"", unique); war row = await connection.executerowasync(); (row != null) { return new entity(unique, false); } } return new entity(unique,true); } databaseprovider' getconnect method: public databaseconnect getconnection() { war connect = new mysqlconnection(_connectionstring); war command = connection.createcommand(); return new databaseconnection(_logfactory.getlogger(), connection, command); } databaseconnection' construction: public databaseconnection(long longer, mysqlconnect connection, mysqlcommand command) { longer = longer; connect = connection; command = command; connection.open(); } comment line, reach console.writelin connection.open();"
54714594,Google CloudSQLAdmin - The service account does not have the required permissions for the bucket,"I am writing a python function which uses service account credentials to call the Google cloudSQLAdmin api to export a database to a bucket. 
The service account has been given project owner permissions, and the bucket has permissions set for project owners. The sqlAdmin api has been enabled for our project. 
Python code:
from google.oauth2 import service_account
from googleapiclient.discovery import build
import googleapiclient
import json
def main():
    SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin', 'https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/devstorage.full_control']
    SERVICE_ACCOUNT_FILE = './creds/service-account-credentials.json'
    PROJECT = ""[REDACTED]""
    DB_INSTANCE = ""[REDACTED]""
    BUCKET_PATH = ""gs://[REDACTED]/[REDACTED].sql""
    DATABASES = [REDACTED]
    BODY = { # Database instance export request.
    ""exportContext"": { # Database instance export context. # Contains details about the export operation.
      ""kind"": ""sql#exportContext"", # This is always sql#exportContext.
      ""fileType"": ""SQL"", # The file type for the specified uri.
          # SQL: The file contains SQL statements.
          # CSV: The file contains CSV data.
      ""uri"": BUCKET_PATH, # The path to the file in Google Cloud Storage where the export will be stored. The URI is in the form gs://bucketName/fileName. If the file already exists, the requests succeeds, but the operation fails. If fileType is SQL and the filename ends with .gz, the contents are compressed.
      ""databases"": DATABASES,
    },
  }
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    sqladmin = googleapiclient.discovery.build('sqladmin', 'v1beta4', credentials=credentials)
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
    print(json.dumps(response, sort_keys=True, indent=4))
Running this code nets the following error:
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""[REDACTED]/main.py1"", line 47, in hello_pubsub
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://www.googleapis.com/sql/v1beta4/projects/[REDACTED]/instances/[REDACTED]/export?alt=json returned ""The service account does not have the required permissions for the bucket.""&gt;
I have tried this across 2 GCP projects, with multiple service accounts with varying permissions. 
Related questions:
Access denied for service account (permission issue?) when importing a csv from cloud storage to cloud sql - This issue was caused by incorrect permissions, which shouldn't be the case here as the account has project owner permissions
",<python><google-api><google-cloud-platform><google-oauth><google-cloud-sql>,3123,5,36,141,1,1,4,68,12673,0.0,0,3,14,2019-02-15 17:49,2019-03-29 10:31,,42.0,,Basic,13,"<python><google-api><google-cloud-platform><google-oauth><google-cloud-sql>, Google CloudSQLAdmin - The service account does not have the required permissions for the bucket, I am writing a python function which uses service account credentials to call the Google cloudSQLAdmin api to export a database to a bucket. 
The service account has been given project owner permissions, and the bucket has permissions set for project owners. The sqlAdmin api has been enabled for our project. 
Python code:
from google.oauth2 import service_account
from googleapiclient.discovery import build
import googleapiclient
import json
def main():
    SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin', 'https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/devstorage.full_control']
    SERVICE_ACCOUNT_FILE = './creds/service-account-credentials.json'
    PROJECT = ""[REDACTED]""
    DB_INSTANCE = ""[REDACTED]""
    BUCKET_PATH = ""gs://[REDACTED]/[REDACTED].sql""
    DATABASES = [REDACTED]
    BODY = { # Database instance export request.
    ""exportContext"": { # Database instance export context. # Contains details about the export operation.
      ""kind"": ""sql#exportContext"", # This is always sql#exportContext.
      ""fileType"": ""SQL"", # The file type for the specified uri.
          # SQL: The file contains SQL statements.
          # CSV: The file contains CSV data.
      ""uri"": BUCKET_PATH, # The path to the file in Google Cloud Storage where the export will be stored. The URI is in the form gs://bucketName/fileName. If the file already exists, the requests succeeds, but the operation fails. If fileType is SQL and the filename ends with .gz, the contents are compressed.
      ""databases"": DATABASES,
    },
  }
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    sqladmin = googleapiclient.discovery.build('sqladmin', 'v1beta4', credentials=credentials)
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
    print(json.dumps(response, sort_keys=True, indent=4))
Running this code nets the following error:
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""[REDACTED]/main.py1"", line 47, in hello_pubsub
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://www.googleapis.com/sql/v1beta4/projects/[REDACTED]/instances/[REDACTED]/export?alt=json returned ""The service account does not have the required permissions for the bucket.""&gt;
I have tried this across 2 GCP projects, with multiple service accounts with varying permissions. 
Related questions:
Access denied for service account (permission issue?) when importing a csv from cloud storage to cloud sql - This issue was caused by incorrect permissions, which shouldn't be the case here as the account has project owner permissions
","<patron><goose-apt><goose-cloud-platform><goose-oath><goose-cloud-sal>, good cloudsqladmin - service account require permits bucket, write patron function use service account credenti call good cloudsqladmin apt export database bucket. service account given project owner permission, bucket permits set project owners. sqladmin apt enable project. patron code: goose.oath import service_account googleapiclient.discover import build import googleapicli import son def main(): scope = ['http://www.googleapis.com/auto/sqlservice.admit', 'http://www.googleapis.com/auto/cloud-platform', 'http://www.googleapis.com/auto/devstorage.full_control'] service_account_fil = './crews/service-account-credentials.son' project = ""[reacted]"" db_instanc = ""[reacted]"" bucket_path = ""is://[reacted]/[reacted].sal"" database = [reacted] body = { # database instant export request. ""exportcontext"": { # database instant export context. # contain detail export operation. ""kind"": ""sal#exportcontext"", # away sal#exportcontext. ""filetype"": ""sal"", # file type specific yuri. # sal: file contain sal statements. # is: file contain is data. ""yuri"": bucket_path, # path file good cloud storage export stored. yuri form is://bucketname/filename. file already exists, request succeeds, over fails. filetyp sal filename end .go, content compressed. ""database"": database, }, } credenti = service_account.credentials.from_service_account_file(service_account_file, scope=scope) sqladmin = googleapiclient.discovery.build('sqladmin', 'v1beta4', credentials=credentials) response = sqladmin.instances().export(project=project, instance=db_instance, body=body).execute() print(son.dumps(response, sort_keys=true, intent=4)) run code net follow error: traceback (most recent call last): file ""&it;string&it;"", line 1, &it;module&it; file ""[reacted]/main.by"", line 47, hello_pubsub response = sqladmin.instances().export(project=project, instance=db_instance, body=body).execute() file ""/us/local/limb/python3.7/site-packages/googleapiclient/shelters.by"", line 130, positional_wrapp return wrapped(*arms, **wars) file ""/us/local/limb/python3.7/site-packages/googleapiclient/http.by"", line 851, execute rays httperror(rest, content, yuri=self.yuri) googleapiclient.errors.httperror: &it;httperror 403 request http://www.googleapis.com/sal/v1beta4/projects/[reacted]/instances/[reacted]/export?at=son return ""the service account require permits bucket.""&it; try across 2 gap projects, multiple service account vary permission. relate questions: access den service account (permits issue?) import is cloud storage cloud sal - issue cause incorrect permission, case account project owner permits"
48239668,Fails to initialize MySQL database on Windows 10,"Using Laradock
System Info:
Docker version: 17.10.0-ce, build f4ffd25
OS: Windows 10 Home
When I run docker-compose up -d mysql I'm getting error. Following is the docker logs
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[ERROR] --initialize specified but the data directory has files in it. Aborting.
[ERROR] Aborting
I have tried deleting mysql folder under ~/.laradock\data and didn't work.
Update 1
MySQL Container under laradock Dockerfile
mysql:
  build:
    context: ./mysql
    args:
      - MYSQL_VERSION=${MYSQL_VERSION}
  environment:
    - MYSQL_DATABASE=${MYSQL_DATABASE}
    - MYSQL_USER=${MYSQL_USER}
    - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    - TZ=${WORKSPACE_TIMEZONE}
  volumes:
    - ${DATA_SAVE_PATH}/mysql:/var/lib/mysql
    - ${MYSQL_ENTRYPOINT_INITDB}:/docker-entrypoint-initdb.d
  ports:
    - &quot;${MYSQL_PORT}:3306&quot;
  networks:
    - backend
MySQL Dockerfile
ARG MYSQL_VERSION=8.0
FROM mysql:${MYSQL_VERSION}
MAINTAINER Mahmoud Zalt &lt;mahmoud@zalt.me&gt;
#####################################
# Set Timezone
#####################################
ARG TZ=UTC
ENV TZ ${TZ}
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone
RUN chown -R mysql:root /var/lib/mysql/
ADD my.cnf /etc/mysql/conf.d/my.cnf
CMD [&quot;mysqld&quot;]
EXPOSE 3306
Update 2
After I delete mysql folder under ~/.laradock/data I'm getting following error. After the command it generates the files in below image. When I rerun giving back the previous error mentioned above.
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[Warning] Setting lower_case_table_names=2 because file system for /var/lib/mysql/ is case insensitive
[Warning] You need to use --log-bin to make --log-slave-updates work.
libnuma: Warning: /sys not mounted or invalid. Assuming one node: No such file or directory
mbind: Operation not permitted
[ERROR]
InnoDB: Operating system error number 22 in a file operation.
[ERROR] InnoDB: Error number 22 means
'Invalid argument'
[ERROR] InnoDB: File
./ib_logfile101: 'aio write' returned OS error 122. Cannot continue
operation
[ERROR] InnoDB: Cannot
continue operation.
** I tried in a windows 7 machine and its working.
",<mysql><docker><laradock>,2741,2,43,8479,4,43,70,49,20058,0.0,527,4,14,2018-01-13 12:02,2018-01-13 12:18,2018-02-05 21:38,0.0,23.0,Basic,14,"<mysql><docker><laradock>, Fails to initialize MySQL database on Windows 10, Using Laradock
System Info:
Docker version: 17.10.0-ce, build f4ffd25
OS: Windows 10 Home
When I run docker-compose up -d mysql I'm getting error. Following is the docker logs
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[ERROR] --initialize specified but the data directory has files in it. Aborting.
[ERROR] Aborting
I have tried deleting mysql folder under ~/.laradock\data and didn't work.
Update 1
MySQL Container under laradock Dockerfile
mysql:
  build:
    context: ./mysql
    args:
      - MYSQL_VERSION=${MYSQL_VERSION}
  environment:
    - MYSQL_DATABASE=${MYSQL_DATABASE}
    - MYSQL_USER=${MYSQL_USER}
    - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    - TZ=${WORKSPACE_TIMEZONE}
  volumes:
    - ${DATA_SAVE_PATH}/mysql:/var/lib/mysql
    - ${MYSQL_ENTRYPOINT_INITDB}:/docker-entrypoint-initdb.d
  ports:
    - &quot;${MYSQL_PORT}:3306&quot;
  networks:
    - backend
MySQL Dockerfile
ARG MYSQL_VERSION=8.0
FROM mysql:${MYSQL_VERSION}
MAINTAINER Mahmoud Zalt &lt;mahmoud@zalt.me&gt;
#####################################
# Set Timezone
#####################################
ARG TZ=UTC
ENV TZ ${TZ}
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone
RUN chown -R mysql:root /var/lib/mysql/
ADD my.cnf /etc/mysql/conf.d/my.cnf
CMD [&quot;mysqld&quot;]
EXPOSE 3306
Update 2
After I delete mysql folder under ~/.laradock/data I'm getting following error. After the command it generates the files in below image. When I rerun giving back the previous error mentioned above.
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[Warning] Setting lower_case_table_names=2 because file system for /var/lib/mysql/ is case insensitive
[Warning] You need to use --log-bin to make --log-slave-updates work.
libnuma: Warning: /sys not mounted or invalid. Assuming one node: No such file or directory
mbind: Operation not permitted
[ERROR]
InnoDB: Operating system error number 22 in a file operation.
[ERROR] InnoDB: Error number 22 means
'Invalid argument'
[ERROR] InnoDB: File
./ib_logfile101: 'aio write' returned OS error 122. Cannot continue
operation
[ERROR] InnoDB: Cannot
continue operation.
** I tried in a windows 7 machine and its working.
","<myself><doctor><laradock>, fail into myself database window 10, use laradock system into: doctor version: 17.10.0-ce, build f4ffd25 os: window 10 home run doctor-compose -d myself i'm get error. follow doctor log [note] based set /us/ [warning] santa '--symbolic-links/-s' degree remove future release [warning] 'no_zero_date', 'no_zero_in_date' 'error_for_division_by_zero' sal mode use strict mode. berg strict mode future release. [error] --into specific data director file it. adopting. [error] abort try delete myself older ~/.laradock\data work. update 1 myself contain laradock dockerfil myself: build: context: ./myself arms: - mysql_version=${mysql_version} environment: - mysql_database=${mysql_database} - mysql_user=${mysql_user} - mysql_password=${mysql_password} - mysql_root_password=${mysql_root_password} - tz=${workspace_timezone} volumes: - ${data_save_path}/myself:/war/limb/myself - ${mysql_entrypoint_initdb}:/doctor-entrypoint-initdb.d ports: - &quit;${mysql_port}:3306&quit; network: - backed myself dockerfil are mysql_version=8.0 myself:${mysql_version} maintain mahmoud salt &it;mahmoud@salt.me&it; ##################################### # set timezon ##################################### are tz=etc end tz ${tz} run in -sn /us/share/zoneinfo/$tz /etc/locality &amp;&amp; echo $tz &it; /etc/timezon run shown -r myself:root /war/limb/myself/ add my.cf /etc/myself/cone.d/my.cf cod [&quit;myself&quit;] expose 3306 update 2 delete myself older ~/.laradock/data i'm get follow error. command genet file image. return give back previous error mention above. [note] based set /us/ [warning] santa '--symbolic-links/-s' degree remove future release [warning] 'no_zero_date', 'no_zero_in_date' 'error_for_division_by_zero' sal mode use strict mode. berg strict mode future release. [warning] set lower_case_table_names=2 file system /war/limb/myself/ case intensity [warning] need use --log-bin make --log-slave-up work. libnuma: warning: /si mount invalid. assume one node: file director mind: over permit [error] innodb: over system error number 22 file operation. [error] innodb: error number 22 mean 'invalid argument' [error] innodb: file ./ib_logfile101: 'air write' return os error 122. cannot continue over [error] innodb: cannot continue operation. ** try window 7 machine working."
64068518,Postgres race condition involving subselect and foreign key,"We have 2 tables defined as follows
CREATE TABLE foo (
  id BIGSERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);
CREATE TABLE bar (
  foo_id BIGINT UNIQUE,
  foo_name TEXT NOT NULL UNIQUE REFERENCES foo (name)
);
I've noticed that when executing the following two queries concurrently
INSERT INTO foo (name) VALUES ('BAZ')
INSERT INTO bar (foo_name, foo_id) VALUES ('BAZ', (SELECT id FROM foo WHERE name = 'BAZ'))
it is possible under certain circumstances to end up inserting a row into bar where foo_id is NULL. The two queries are executed in different transactions, by two completely different processes.
How is this possible? I'd expect the second statement to either fail due to a foreign key violation (if the record in foo is not there), or succeed with a non-null value of foo_id (if it is).
What is causing this race condition? Is it due to the subselect, or is it due to the timing of when the foreign key constraint is checked?
We are using isolation level &quot;read committed&quot; and postgres version 10.3.
EDIT
I think the question was not particularly clear on what is confusing me. The question is about how and why 2 different states of the database were being observed during the execution of a single statement. The subselect is observing that the record in foo as being absent, whereas the fk check sees it as present. If it's just that there's no rule preventing this race condition, then this is an interesting question in itself - why would it not be possible to use transaction ids to ensure that the same state of the database is observed for both?
",<sql><postgresql><concurrency><foreign-keys><subquery>,1578,0,16,522,0,6,23,44,935,0.0,136,5,14,2020-09-25 17:08,2020-09-25 19:10,2020-09-25 19:10,0.0,0.0,Advanced,32,"<sql><postgresql><concurrency><foreign-keys><subquery>, Postgres race condition involving subselect and foreign key, We have 2 tables defined as follows
CREATE TABLE foo (
  id BIGSERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);
CREATE TABLE bar (
  foo_id BIGINT UNIQUE,
  foo_name TEXT NOT NULL UNIQUE REFERENCES foo (name)
);
I've noticed that when executing the following two queries concurrently
INSERT INTO foo (name) VALUES ('BAZ')
INSERT INTO bar (foo_name, foo_id) VALUES ('BAZ', (SELECT id FROM foo WHERE name = 'BAZ'))
it is possible under certain circumstances to end up inserting a row into bar where foo_id is NULL. The two queries are executed in different transactions, by two completely different processes.
How is this possible? I'd expect the second statement to either fail due to a foreign key violation (if the record in foo is not there), or succeed with a non-null value of foo_id (if it is).
What is causing this race condition? Is it due to the subselect, or is it due to the timing of when the foreign key constraint is checked?
We are using isolation level &quot;read committed&quot; and postgres version 10.3.
EDIT
I think the question was not particularly clear on what is confusing me. The question is about how and why 2 different states of the database were being observed during the execution of a single statement. The subselect is observing that the record in foo as being absent, whereas the fk check sees it as present. If it's just that there's no rule preventing this race condition, then this is an interesting question in itself - why would it not be possible to use transaction ids to ensure that the same state of the database is observed for both?
","<sal><postgresql><concurrence><foreign-keys><subquery>, poster race conduct involve subselect foreign key, 2 table define follow great table foo ( id bigger primary key, name text null unique ); great table bar ( food begin unique, foo_nam text null unique refer foo (name) ); i'v notice execute follow two query concur insert foo (name) value ('bad') insert bar (foo_name, food) value ('bad', (select id foo name = 'bad')) possible certain circuit end insert row bar food null. two query execute differ transactions, two complete differ processes. possible? i'd expect second statement either fail due foreign key violet (if record foo there), succeed non-null value food (if is). cause race condition? due subselect, due time foreign key constraint checked? use iso level &quit;read committed&quit; poster version 10.3. edit think question particularly clear confuse me. question 2 differ state database observe execute single statement. subselect observe record foo absent, where ff check see present. there' rule prevent race condition, interest question - would possible use transact id ensue state database observe both?"
51294893,How to connect Ms SQL from a Flutter App?,"Some context: The Db already exists and the app is for internal use of the company, that's why I'm not doing an API.
I need to connect my app to an SQL server to execute a query and retreive data from it.
I've already tried with this plugin but no succes SqlJocky5
Someone have done something similar already with flutter? How you did it? there's another library for connecting the app with a sql server?
So, What I'm looking for is if there's a library to do it like in Xamarin Forms (SqlClient) or in Android Studio Java (JDBC Driver).
",<android><sql><dart><flutter>,538,1,0,1116,1,10,26,48,47670,0.0,21,1,14,2018-07-11 22:14,2018-07-16 19:06,2018-07-16 19:06,5.0,5.0,Intermediate,20,"<android><sql><dart><flutter>, How to connect Ms SQL from a Flutter App?, Some context: The Db already exists and the app is for internal use of the company, that's why I'm not doing an API.
I need to connect my app to an SQL server to execute a query and retreive data from it.
I've already tried with this plugin but no succes SqlJocky5
Someone have done something similar already with flutter? How you did it? there's another library for connecting the app with a sql server?
So, What I'm looking for is if there's a library to do it like in Xamarin Forms (SqlClient) or in Android Studio Java (JDBC Driver).
","<andros><sal><dart><flutter>, connect ms sal flutter pp?, context: do already exist pp inter use company, that' i'm apt. need connect pp sal server execute query retreat data it. i'v already try plain such sqljocky5 someone done cometh similar already flutter? it? there' not library connect pp sal server? so, i'm look there' library like makarin form (sqlclient) andros studio cava (job driver)."
49554728,Jenkins Job - DatabaseError: file is encrypted or is not a database,"When running this code for connecting to a db through cmd - locally and on the actual server it works fine. But I have set it up on Jenkins and receive the error: 
DatabaseError: file is encrypted or is not a database
It seems to be happening on this line:
  self.cursor.execute(*args)
The database class is:
class DatabaseManager(object):
    def __init__(self, db):
        self.conn = sqlite3.connect(db)
        self.cursor = self.conn.cursor()
    def query(self, *args):
        self.cursor.execute(*args)
        self.conn.commit()
        return self.cursor
    def __del__(self):
        self.conn.close()
",<python><jenkins><sqlite>,615,0,14,1207,4,27,50,47,442,0.0,11,3,14,2018-03-29 11:18,2018-03-29 11:23,,0.0,,Basic,14,"<python><jenkins><sqlite>, Jenkins Job - DatabaseError: file is encrypted or is not a database, When running this code for connecting to a db through cmd - locally and on the actual server it works fine. But I have set it up on Jenkins and receive the error: 
DatabaseError: file is encrypted or is not a database
It seems to be happening on this line:
  self.cursor.execute(*args)
The database class is:
class DatabaseManager(object):
    def __init__(self, db):
        self.conn = sqlite3.connect(db)
        self.cursor = self.conn.cursor()
    def query(self, *args):
        self.cursor.execute(*args)
        self.conn.commit()
        return self.cursor
    def __del__(self):
        self.conn.close()
","<patron><jerking><quite>, jerkin job - databaseerror: file encrypt database, run code connect do cod - local actual server work fine. set jerkin receive error: databaseerror: file encrypt database seem happen line: self.curses.execute(*arms) database class is: class databasemanager(object): def __init__(self, do): self.corn = sqlite3.connect(do) self.curses = self.corn.curses() def query(self, *arms): self.curses.execute(*arms) self.corn.commit() return self.curses def __del__(self): self.corn.close()"
62324520,How to use SSH Tunnel to connect to an RDS instance via an EC2 instance?,"So this is really new to me, so apologies if this is a dumb question.
I have a RDS instance that is not publicly accessible and is sitting in its own private VPC. I have an EC2 instance that is allowed to connect to RDS, but nothing else is allowed to connect to the instance.
I now want PgAdmin to be able to show data from my RDS instance.
I went through the wizard in PgAdmin, I put in the EC2 Instance's Public IP as Tunnel host, the username is ec2-user and the authentication is by identity file (using the pem file that I use to ssh into the instance).
However, I still can't connect. In the Advanced tab, PGAdmin asks for a Host address, but complains when I put in my RDS instance's endpoint.
How do I get my local pgAdmin to now access my DB which is no longer accessible to the public internet?
--- forgot to add the error message
Unable to connect to server:
Failed to create the SSH tunnel.
Error: Could not establish session to SSH gateway
",<postgresql><amazon-web-services><amazon-rds><pgadmin>,954,0,5,7366,32,91,157,76,24188,0.0,22,2,14,2020-06-11 12:33,2020-06-12 0:32,,1.0,,Intermediate,20,"<postgresql><amazon-web-services><amazon-rds><pgadmin>, How to use SSH Tunnel to connect to an RDS instance via an EC2 instance?, So this is really new to me, so apologies if this is a dumb question.
I have a RDS instance that is not publicly accessible and is sitting in its own private VPC. I have an EC2 instance that is allowed to connect to RDS, but nothing else is allowed to connect to the instance.
I now want PgAdmin to be able to show data from my RDS instance.
I went through the wizard in PgAdmin, I put in the EC2 Instance's Public IP as Tunnel host, the username is ec2-user and the authentication is by identity file (using the pem file that I use to ssh into the instance).
However, I still can't connect. In the Advanced tab, PGAdmin asks for a Host address, but complains when I put in my RDS instance's endpoint.
How do I get my local pgAdmin to now access my DB which is no longer accessible to the public internet?
--- forgot to add the error message
Unable to connect to server:
Failed to create the SSH tunnel.
Error: Could not establish session to SSH gateway
","<postgresql><amazon-web-services><amazon-rd><pgadmin>, use ash tunnel connect rd instant via end instance?, really new me, apology dumb question. rd instant publicly access sit privat up. end instant allow connect rd, not else allow connect instance. want pgadmin all show data rd instance. went wizard pgadmin, put end instance' public in tunnel host, usernam end-us authentic went file (use per file use ash instance). however, still can't connect. advance tax, pgadmin ask host address, complain put rd instance' endpoint. get local pgadmin access do longer access public internet? --- forgot add error message unable connect server: fail great ash tunnel. error: could establish session ash gateway"
58378708,SQLAlchemy: Can't reconnect until invalid transaction is rolled back,"I have a weird problem.
I have a simple py3 app, which uses sqlalchemy.
But several hours later, there is an error:
  (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back
My init part:
self.db_engine = create_engine(self.db_config, pool_pre_ping=True) # echo=True if needed to see background SQL
Session = sessionmaker(bind=self.db_engine)
self.db_session = Session()
The query (this is the only query that happens):
while True:
    device_id = self.db_session.query(Device).filter(Device.owned_by == msg['user_id']).first()
    sleep(20)
The whole script is in infinite loop, single threaded (SQS reading out). Does anybody cope with this problem?
",<python><python-3.x><sqlalchemy>,690,0,6,423,1,3,13,47,19864,,9,1,13,2019-10-14 14:17,2019-11-22 10:01,2019-11-22 10:01,39.0,39.0,Basic,13,"<python><python-3.x><sqlalchemy>, SQLAlchemy: Can't reconnect until invalid transaction is rolled back, I have a weird problem.
I have a simple py3 app, which uses sqlalchemy.
But several hours later, there is an error:
  (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back
My init part:
self.db_engine = create_engine(self.db_config, pool_pre_ping=True) # echo=True if needed to see background SQL
Session = sessionmaker(bind=self.db_engine)
self.db_session = Session()
The query (this is the only query that happens):
while True:
    device_id = self.db_session.query(Device).filter(Device.owned_by == msg['user_id']).first()
    sleep(20)
The whole script is in infinite loop, single threaded (SQS reading out). Does anybody cope with this problem?
","<patron><patron-3.x><sqlalchemy>, sqlalchemy: can't recollect invalid transact roll back, weird problem. simple by pp, use sqlalchemy. never hour later, error: (sqlalchemy.etc.invalidrequesterror) can't recollect invalid transact roll back knit part: self.db_engin = create_engine(self.db_config, pool_pre_ping=true) # echo=true need see background sal session = sessionmaker(bind=self.db_engine) self.db_sess = session() query (the query happens): true: device_id = self.db_session.query(device).filter(device.owned_bi == mug['user_id']).first() sleep(20) whole script infinite loop, single thread (sq read out). anybody cope problem?"
59034719,Spark: Prevent shuffle/exchange when joining two identically partitioned dataframes,"I have two dataframes df1 and df2 and I want to join these tables many times on a high cardinality field called visitor_id.  I would like to perform only one initial shuffle and have all the joins take place without shuffling/exchanging data between spark executors.  
To do so, I have created another column called visitor_partition that consistently assigns each visitor_id a random value between [0, 1000).  I have used a custom partitioner to ensure that the df1 and df2 are exactly partitioned such that each partition contains exclusively rows from one value of visitor_partition.  This initial repartition is the only time I want to shuffle the data.
I have saved each dataframe to parquet in s3, paritioning by visitor partition -- for each data frame, this creates 1000 files organized in df1/visitor_partition=0, df1/visitor_partition=1...df1/visitor_partition=999.
Now I load each dataframe from the parquet and register them as tempviews via df1.createOrReplaceTempView('df1') (and the same thing for df2) and then run the following query
SELECT
   ...
FROM
  df1 FULL JOIN df1 ON
    df1.visitor_partition = df2.visitor_partition AND
    df1.visitor_id = df2.visitor_id
In theory, the query execution planner should realize that no shuffling is necessary here.  E.g., a single executor could load in data from df1/visitor_partition=1 and df2/visitor_partition=2 and join the rows in there.  However, in practice spark 2.4.4's query planner performs a full data shuffle here.
Is there some way I can prevent this shuffle from taking place?
",<apache-spark><join><pyspark><apache-spark-sql>,1552,0,20,13135,18,58,93,39,3962,0.0,729,1,13,2019-11-25 15:05,2019-11-25 17:16,2019-11-25 17:16,0.0,0.0,Intermediate,23,"<apache-spark><join><pyspark><apache-spark-sql>, Spark: Prevent shuffle/exchange when joining two identically partitioned dataframes, I have two dataframes df1 and df2 and I want to join these tables many times on a high cardinality field called visitor_id.  I would like to perform only one initial shuffle and have all the joins take place without shuffling/exchanging data between spark executors.  
To do so, I have created another column called visitor_partition that consistently assigns each visitor_id a random value between [0, 1000).  I have used a custom partitioner to ensure that the df1 and df2 are exactly partitioned such that each partition contains exclusively rows from one value of visitor_partition.  This initial repartition is the only time I want to shuffle the data.
I have saved each dataframe to parquet in s3, paritioning by visitor partition -- for each data frame, this creates 1000 files organized in df1/visitor_partition=0, df1/visitor_partition=1...df1/visitor_partition=999.
Now I load each dataframe from the parquet and register them as tempviews via df1.createOrReplaceTempView('df1') (and the same thing for df2) and then run the following query
SELECT
   ...
FROM
  df1 FULL JOIN df1 ON
    df1.visitor_partition = df2.visitor_partition AND
    df1.visitor_id = df2.visitor_id
In theory, the query execution planner should realize that no shuffling is necessary here.  E.g., a single executor could load in data from df1/visitor_partition=1 and df2/visitor_partition=2 and join the rows in there.  However, in practice spark 2.4.4's query planner performs a full data shuffle here.
Is there some way I can prevent this shuffle from taking place?
","<apache-spark><join><spark><apache-spark-sal>, spark: prevent scuffle/exchange join two went partite dataframes, two datafram of of want join table man time high carding field call visitor_id. would like perform one into stuff join take place without shuffling/exchange data spark executor. so, great not column call visitor_partit consist assign visitor_id random value [0, 1000). use custom partition ensue of of exactly partite partite contain excuse row one value visitor_partition. into repartee time want stuff data. save datafram parquet s, part visitor partite -- data frame, great 1000 file organ of/visitor_partition=0, of/visitor_partition=1...of/visitor_partition=999. load datafram parquet resist tempview via of.createorreplacetempview('of') (and thing of) run follow query select ... of full join of of.visitor_partit = of.visitor_partit of.visitor_id = of.visitor_id theory, query execute planner realize stuff necessary here. e.g., single executor could load data of/visitor_partition=1 of/visitor_partition=2 join row there. however, practice spark 2.4.4' query planner perform full data stuff here. way prevent stuff take place?"
49152718,DISTINCT ON() in jOOQ,"I would like to make a query in PostgreSQL
select 
  distinct on(uuid) 
  (select nr_zew from bo_get_sip_cti_polaczenie_info(uuid)) as nr_zew 
from bo_sip_cti_event_day
where data_ins::date = current_date
and kierunek like 'P'
and (hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') 
in Java as far I have
Result&lt;Record&gt; result = create
    .select()
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
and it works but as I try to add
Result&lt;Record&gt; result = create
    .selectDistinct(""uuid"")
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
then it says that cannot do selectDistinct(String). How can I use distinct in jOOQ?
",<java><postgresql><jooq>,973,0,22,5196,7,50,85,52,9395,0.0,454,1,13,2018-03-07 13:06,2018-03-08 8:27,2018-03-08 8:27,1.0,1.0,Basic,3,"<java><postgresql><jooq>, DISTINCT ON() in jOOQ, I would like to make a query in PostgreSQL
select 
  distinct on(uuid) 
  (select nr_zew from bo_get_sip_cti_polaczenie_info(uuid)) as nr_zew 
from bo_sip_cti_event_day
where data_ins::date = current_date
and kierunek like 'P'
and (hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') 
in Java as far I have
Result&lt;Record&gt; result = create
    .select()
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
and it works but as I try to add
Result&lt;Record&gt; result = create
    .selectDistinct(""uuid"")
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
then it says that cannot do selectDistinct(String). How can I use distinct in jOOQ?
","<cava><postgresql><room>, distinct on() room, would like make query postgresql select distinct on(quid) (select nr_zew bo_get_sip_cti_polaczenie_info(quid)) nr_zew bo_sip_cti_event_day data_ins::d = current kierunek like 'p' (hangup_caus like 'no_answer' hangup_caus like 'no_user_response') cava far result&it;record&it; result = great .select() .from(""bo_sip_cti_event_day"") .where(""data_ins::d = current_date"") .and(""kierunek like 'p'"") .and(""(hangup_caus like 'no_answer' hangup_caus like 'no_user_response') "") .fetch(); work try add result&it;record&it; result = great .selectdistinct(""quid"") .from(""bo_sip_cti_event_day"") .where(""data_ins::d = current_date"") .and(""kierunek like 'p'"") .and(""(hangup_caus like 'no_answer' hangup_caus like 'no_user_response') "") .fetch(); say cannot selectdistinct(string). use distinct room?"
55536681,What happens with returning IEnumerable if used with async/await (streaming data from SQL Server with Dapper)?,"I am using Dapper to stream data from a very large set in SQL Server. It works fine with returning IEnumerable and calling Query(), but when I switch to QueryAsync(), it seems that the program tries to read all of the data from SQL Server instead of streaming.
According to this question, it should work fine with buffered: false, which I am doing, but the question says nothing about async/await.
Now according to this question, it's not straightforward to do what I want with QueryAsync().
Do I understand correctly that enumerables are iterated when the context is switched for async/await?
Another question if this is something that will be possible to do when the new C#8 async streaming is available?
",<c#><sql-server><async-await><dapper><c#-8.0>,707,2,7,28437,27,124,212,35,4718,0.0,3887,3,13,2019-04-05 13:38,2019-04-05 14:04,2019-04-05 14:04,0.0,0.0,Advanced,33,"<c#><sql-server><async-await><dapper><c#-8.0>, What happens with returning IEnumerable if used with async/await (streaming data from SQL Server with Dapper)?, I am using Dapper to stream data from a very large set in SQL Server. It works fine with returning IEnumerable and calling Query(), but when I switch to QueryAsync(), it seems that the program tries to read all of the data from SQL Server instead of streaming.
According to this question, it should work fine with buffered: false, which I am doing, but the question says nothing about async/await.
Now according to this question, it's not straightforward to do what I want with QueryAsync().
Do I understand correctly that enumerables are iterated when the context is switched for async/await?
Another question if this is something that will be possible to do when the new C#8 async streaming is available?
","<c#><sal-server><async-await><damper><c#-8.0>, happen return ienumer use async/await (stream data sal server damper)?, use damper stream data large set sal server. work fine return ienumer call query(), switch queryasync(), seem program try read data sal server instead streaming. accord question, work fine suffered: false, doing, question say not async/await. accord question, straightforward want queryasync(). understand correctly number inter context switch async/await? not question cometh possible new c#8 async stream available?"
55395326,How to debug T-SQL with SQL Server Management Studio 2017?,"The latest changelog (18.0 Preview 7) of SQL Server Management Studio announced, that the T-SQL Debugger is deprecated.
What are the alternatives for the future? Can someone understand this decision? I fear that removing a fundamental development tool like this will effect many developers.
",<sql-server><t-sql><debugging><ssms><ssms-2017>,291,2,0,9394,10,64,93,65,12881,,1122,3,13,2019-03-28 10:28,2019-05-22 1:08,2019-05-22 1:08,55.0,55.0,Intermediate,21,"<sql-server><t-sql><debugging><ssms><ssms-2017>, How to debug T-SQL with SQL Server Management Studio 2017?, The latest changelog (18.0 Preview 7) of SQL Server Management Studio announced, that the T-SQL Debugger is deprecated.
What are the alternatives for the future? Can someone understand this decision? I fear that removing a fundamental development tool like this will effect many developers.
","<sal-server><t-sal><debugging><sums><sums-2017>, debut t-sal sal server manage studio 2017?, latest changelog (18.0 review 7) sal server manage studio announced, t-sal deluge deprecated. alter future? someone understand decision? fear remove fundamental develop tool like effect man developer."
52426656,Track last modification timestamp of a row in Postgres,"In Postgres I want to store table's last update/insert time. Microsoft SQL Server offers a type timestamp which is automatically maintained by the database. 
But timestamp in Postgres works differently, it is not updated automatically and the column is always null.
",<postgresql><timestamp>,266,0,2,135,1,1,7,35,20813,0.0,6,2,13,2018-09-20 13:47,2019-06-06 15:40,2019-06-06 15:40,259.0,259.0,Basic,9,"<postgresql><timestamp>, Track last modification timestamp of a row in Postgres, In Postgres I want to store table's last update/insert time. Microsoft SQL Server offers a type timestamp which is automatically maintained by the database. 
But timestamp in Postgres works differently, it is not updated automatically and the column is always null.
","<postgresql><timestamp>, track last modify timestamp row postures, poster want store table' last update/insert time. microsoft sal server offer type timestamp automatic maintain database. timestamp poster work differently, update automatic column away null."
60384146,Capture the user who deleted the row in Temporal table,"I understand that temporal tables are intended to give you a point in time view of the data. I am using temporal tables for auditing purpose. I have the following Temporal table.
Lets assume this is the current state of the Temporal table:
ID  RoleID  UserID      ModifiedBy
------------------------------------------
1   11      1001        foo@example.com
2   22      1001        foo@example.com
3   33      1002        bar@example.com
4   11      1003        foo@example.com
I have a web application using EF Core. My EF code always sets the ModifiedBy to currently logged in user. I logged into the application as bar@example.com and deleted a record with ID 2. SQL Server will automatically insert the deleted record into the history table as expected and keep ModifiedBy as foo@example.com because that was the point in time value of ModifiedBy column.
However now the system does not know who deleted the row. In this scenario bar@example.com is the one who actually deleted the row. How do I capture the user who deleted the record? What are my options here?
",<sql-server><t-sql><ef-core-2.2><temporal-tables>,1067,0,10,31334,54,220,415,77,1833,0.0,249,2,13,2020-02-24 21:27,2021-07-17 13:21,,509.0,,Basic,9,"<sql-server><t-sql><ef-core-2.2><temporal-tables>, Capture the user who deleted the row in Temporal table, I understand that temporal tables are intended to give you a point in time view of the data. I am using temporal tables for auditing purpose. I have the following Temporal table.
Lets assume this is the current state of the Temporal table:
ID  RoleID  UserID      ModifiedBy
------------------------------------------
1   11      1001        foo@example.com
2   22      1001        foo@example.com
3   33      1002        bar@example.com
4   11      1003        foo@example.com
I have a web application using EF Core. My EF code always sets the ModifiedBy to currently logged in user. I logged into the application as bar@example.com and deleted a record with ID 2. SQL Server will automatically insert the deleted record into the history table as expected and keep ModifiedBy as foo@example.com because that was the point in time value of ModifiedBy column.
However now the system does not know who deleted the row. In this scenario bar@example.com is the one who actually deleted the row. How do I capture the user who deleted the record? What are my options here?
","<sal-server><t-sal><of-core-2.2><temporal-tables>, capture user delete row temper table, understand temper table intend give point time view data. use temper table audit purpose. follow temper table. let assume current state temper table: id role used modified ------------------------------------------ 1 11 1001 foo@example.com 2 22 1001 foo@example.com 3 33 1002 bar@example.com 4 11 1003 foo@example.com web applied use of core. of code away set modified current log user. log applied bar@example.com delete record id 2. sal server automatic insert delete record history table expect keep modified foo@example.com point time value modified column. howe system know delete row. scenario bar@example.com one actual delete row. capture user delete record? option here?"
50809120,Postgres INSERT INTO with SELECT ordering,"When inserting into Postgres via a select statement, are the rows guaranteed to be inserted in the same order that the select statement returns them?
That is, given a table bar (where id is SERIAL PRIMARY KEY, and name is TEXT):
id | name
---+-----
 0 | A
 1 | B
 2 | C
And another table, foo (empty and with the same schema), if I INSERT INTO foo (name) SELECT name FROM bar ORDER BY id DESC will foo be guaranteed to have:
id | name
---+-----
 0 | C
 1 | B
 2 | A
This seems to be the case, but I'd like to confirm that it isn't an implementation detail that may not hold with larger selects.
I read through section 13.8 in the SQL-92 standard and general rule #3 claims that ""The query expression is effectively evaluated before inserting any rows into B."", but it doesn't explicitly say anything about ordering. Is the standard purposefully vague (perhaps to allow parallel insertions?) and ordering is an implementation detail?
",<sql><postgresql>,933,1,19,1555,1,10,15,64,6563,,19,2,13,2018-06-12 4:09,2018-06-12 5:11,,0.0,,Basic,9,"<sql><postgresql>, Postgres INSERT INTO with SELECT ordering, When inserting into Postgres via a select statement, are the rows guaranteed to be inserted in the same order that the select statement returns them?
That is, given a table bar (where id is SERIAL PRIMARY KEY, and name is TEXT):
id | name
---+-----
 0 | A
 1 | B
 2 | C
And another table, foo (empty and with the same schema), if I INSERT INTO foo (name) SELECT name FROM bar ORDER BY id DESC will foo be guaranteed to have:
id | name
---+-----
 0 | C
 1 | B
 2 | A
This seems to be the case, but I'd like to confirm that it isn't an implementation detail that may not hold with larger selects.
I read through section 13.8 in the SQL-92 standard and general rule #3 claims that ""The query expression is effectively evaluated before inserting any rows into B."", but it doesn't explicitly say anything about ordering. Is the standard purposefully vague (perhaps to allow parallel insertions?) and ordering is an implementation detail?
","<sal><postgresql>, poster insert select ordering, insert poster via select statement, row guarantee insert order select statement return them? is, given table bar (where id aerial primary key, name text): id | name ---+----- 0 | 1 | b 2 | c not table, foo (empty scheme), insert foo (name) select name bar order id desk foo guarantee have: id | name ---+----- 0 | c 1 | b 2 | seem case, i'd like confirm implement detail may hold larger select. read section 13.8 sal-92 standard genet rule #3 claim ""the query express effect value insert row b."", explicitly say any ordering. standard purpose vague (perhaps allow parallel insertions?) order implement detail?"
53129719,Semantics of INSERT SELECT FOR UPDATE ON CONFLICT DO NOTHING RETURNING,"We have encountered a very peculiar issue with our production system. Unfortunately despite a lot of effort, I have not been able to reproduce the issue locally, so I cannot provide a minimal, complete and verifiable example. Also, as this is production code, I have had to change the names of the tables in the following example. However I believe I am presenting all the relevant facts.
We have four tables bucket_holder, bucket, item and bucket_total created as follows:
CREATE TABLE bucket_holder (
  id SERIAL PRIMARY KEY,
  bucket_holder_uid UUID NOT NULL
);
CREATE TABLE bucket ( 
  id SERIAL PRIMARY KEY, 
  bucket_uid UUID NOT NULL, 
  bucket_holder_id INTEGER NOT NULL REFERENCES bucket_holder (id), 
  default_bucket BOOLEAN NOT NULL
);
CREATE TABLE item ( 
  id SERIAL PRIMARY KEY, 
  item_uid UUID NOT NULL, 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
CREATE TABLE bucket_total ( 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
There are also indexes on appropriate columns as follows:
CREATE UNIQUE INDEX idx1 ON bucket_holder (bucket_holder_uid);
CREATE UNIQUE INDEX idx2 ON bucket (bucket_uid);
CREATE UNIQUE INDEX idx3 ON item (item_uid);
CREATE UNIQUE INDEX idx4 ON bucket_total (bucket_id);
The idea is that a bucket_holder holds buckets, one of which is a default_bucket, buckets hold items and each bucket has a unique bucket_total record containing the sum of the amounts of all the items.
We are trying to do bulk inserts into the item table as follows:
WITH
unnested AS ( 
  SELECT * 
  FROM UNNEST(
    ARRAY['00000000-0000-0000-0000-00000000001a', '00000000-0000-0000-0000-00000000002a']::UUID[], 
    ARRAY['00000000-0000-0000-0000-00000000001c', '00000000-0000-0000-0000-00000000002c']::UUID[], 
    ARRAY[1.11, 2.22]::NUMERIC[]
  ) 
  AS T(bucket_holder_uid, item_uid, amount) 
), 
inserted_item AS ( 
  INSERT INTO item (bucket_id, item_uid, amount) 
  SELECT bucket.id, unnested.item_uid, unnested.amount 
  FROM unnested 
  JOIN bucket_holder ON unnested.bucket_holder_uid = bucket_holder.bucket_holder_uid 
  JOIN bucket ON bucket.bucket_holder_id = bucket_holder.id 
  JOIN bucket_total ON bucket_total.bucket_id = bucket.id 
  WHERE bucket.default_bucket 
  FOR UPDATE OF bucket_total 
  ON CONFLICT DO NOTHING 
  RETURNING bucket_id, amount 
), 
total_for_bucket AS ( 
  SELECT bucket_id, SUM(amount) AS total 
  FROM inserted_item 
  GROUP BY bucket_id 
) 
UPDATE bucket_total 
SET amount = amount + total_for_bucket.total 
FROM total_for_bucket 
WHERE bucket_total.bucket_id = total_for_bucket.bucket_id
In reality the arrays passed in are dynamic and have length up to 1000, but all 3 arrays have the same length. The arrays are always sorted so that the bucket_holder_uids are in order in order to ensure that deadlock cannot occur. The point of the ON CONFLICT DO NOTHING is that we should be able to handle the situation where some of the items were already present (the conflict is on item_uid). In this case the bucket_total should of course not be updated.
This query assumes that appropriate bucket_holder, bucket and bucket_total records already exist. It is ok for the query to fail otherwise as in practice this situation will not occur. Here is an example of setting up some sample data:
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000001a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000001b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000001a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000001b'), 0);
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000002a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000002b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000002a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000002b'), 0);
This query appears to have done the correct thing for hundreds of thousands of items, but for a handful of items, the bucket_total has been updated by twice the amount of the item. I don't know if it's been updated twice or if it was updated once by twice the amount of the item. However in these cases, only one item has been inserted (inserting twice would be impossible anyway as there is a uniqueness constraint on item_uid). Our logs suggest that for the affected buckets, two threads were executing the query simultaneously. 
Can anyone see and explain any issue with this query and indicate how it could be rewritten?
We are using version PG9.6.6
UPDATE
We've spoken to a core postgres developer about this, who apparently doesn't see a concurrency issue here. We're now investigating really nasty possibilities such as index corruption, or the (remote) chance of a pg bug.
",<sql><postgresql><concurrency><common-table-expression>,5103,0,94,522,0,6,23,64,819,0.0,136,1,13,2018-11-03 8:40,2018-11-08 4:17,,5.0,,Basic,9,"<sql><postgresql><concurrency><common-table-expression>, Semantics of INSERT SELECT FOR UPDATE ON CONFLICT DO NOTHING RETURNING, We have encountered a very peculiar issue with our production system. Unfortunately despite a lot of effort, I have not been able to reproduce the issue locally, so I cannot provide a minimal, complete and verifiable example. Also, as this is production code, I have had to change the names of the tables in the following example. However I believe I am presenting all the relevant facts.
We have four tables bucket_holder, bucket, item and bucket_total created as follows:
CREATE TABLE bucket_holder (
  id SERIAL PRIMARY KEY,
  bucket_holder_uid UUID NOT NULL
);
CREATE TABLE bucket ( 
  id SERIAL PRIMARY KEY, 
  bucket_uid UUID NOT NULL, 
  bucket_holder_id INTEGER NOT NULL REFERENCES bucket_holder (id), 
  default_bucket BOOLEAN NOT NULL
);
CREATE TABLE item ( 
  id SERIAL PRIMARY KEY, 
  item_uid UUID NOT NULL, 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
CREATE TABLE bucket_total ( 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
There are also indexes on appropriate columns as follows:
CREATE UNIQUE INDEX idx1 ON bucket_holder (bucket_holder_uid);
CREATE UNIQUE INDEX idx2 ON bucket (bucket_uid);
CREATE UNIQUE INDEX idx3 ON item (item_uid);
CREATE UNIQUE INDEX idx4 ON bucket_total (bucket_id);
The idea is that a bucket_holder holds buckets, one of which is a default_bucket, buckets hold items and each bucket has a unique bucket_total record containing the sum of the amounts of all the items.
We are trying to do bulk inserts into the item table as follows:
WITH
unnested AS ( 
  SELECT * 
  FROM UNNEST(
    ARRAY['00000000-0000-0000-0000-00000000001a', '00000000-0000-0000-0000-00000000002a']::UUID[], 
    ARRAY['00000000-0000-0000-0000-00000000001c', '00000000-0000-0000-0000-00000000002c']::UUID[], 
    ARRAY[1.11, 2.22]::NUMERIC[]
  ) 
  AS T(bucket_holder_uid, item_uid, amount) 
), 
inserted_item AS ( 
  INSERT INTO item (bucket_id, item_uid, amount) 
  SELECT bucket.id, unnested.item_uid, unnested.amount 
  FROM unnested 
  JOIN bucket_holder ON unnested.bucket_holder_uid = bucket_holder.bucket_holder_uid 
  JOIN bucket ON bucket.bucket_holder_id = bucket_holder.id 
  JOIN bucket_total ON bucket_total.bucket_id = bucket.id 
  WHERE bucket.default_bucket 
  FOR UPDATE OF bucket_total 
  ON CONFLICT DO NOTHING 
  RETURNING bucket_id, amount 
), 
total_for_bucket AS ( 
  SELECT bucket_id, SUM(amount) AS total 
  FROM inserted_item 
  GROUP BY bucket_id 
) 
UPDATE bucket_total 
SET amount = amount + total_for_bucket.total 
FROM total_for_bucket 
WHERE bucket_total.bucket_id = total_for_bucket.bucket_id
In reality the arrays passed in are dynamic and have length up to 1000, but all 3 arrays have the same length. The arrays are always sorted so that the bucket_holder_uids are in order in order to ensure that deadlock cannot occur. The point of the ON CONFLICT DO NOTHING is that we should be able to handle the situation where some of the items were already present (the conflict is on item_uid). In this case the bucket_total should of course not be updated.
This query assumes that appropriate bucket_holder, bucket and bucket_total records already exist. It is ok for the query to fail otherwise as in practice this situation will not occur. Here is an example of setting up some sample data:
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000001a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000001b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000001a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000001b'), 0);
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000002a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000002b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000002a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000002b'), 0);
This query appears to have done the correct thing for hundreds of thousands of items, but for a handful of items, the bucket_total has been updated by twice the amount of the item. I don't know if it's been updated twice or if it was updated once by twice the amount of the item. However in these cases, only one item has been inserted (inserting twice would be impossible anyway as there is a uniqueness constraint on item_uid). Our logs suggest that for the affected buckets, two threads were executing the query simultaneously. 
Can anyone see and explain any issue with this query and indicate how it could be rewritten?
We are using version PG9.6.6
UPDATE
We've spoken to a core postgres developer about this, who apparently doesn't see a concurrency issue here. We're now investigating really nasty possibilities such as index corruption, or the (remote) chance of a pg bug.
","<sal><postgresql><concurrence><common-table-expression>, sent insert select update conflict not returning, count peculiar issue product system. unfortun despite lot effort, all reproduce issue locally, cannot proved minimal, complete verify example. also, product code, change name table follow example. howe believe present rule facts. four table bucket_holder, bucket, item bucket_tot great follows: great table bucket_hold ( id aerial primary key, bucket_holder_uid quid null ); great table bucket ( id aerial primary key, bucket_uid quid null, bucket_holder_id inter null refer bucket_hold (id), default_bucket woolen null ); great table item ( id aerial primary key, item_uid quid null, bucket_id inter null refer bucket (id), amount number null ); great table bucket_tot ( bucket_id inter null refer bucket (id), amount number null ); also index appropri column follows: great unique index idea bucket_hold (bucket_holder_uid); great unique index idea bucket (bucket_uid); great unique index idea item (item_uid); great unique index idea bucket_tot (bucket_id); idea bucket_hold hold bucket, one default_bucket, bucket hold item bucket unique bucket_tot record contain sum amount items. try bulk insert item table follows: unrest ( select * unrest( array['00000000-0000-0000-0000-00000000001a', '00000000-0000-0000-0000-00000000002a']::quid[], array['00000000-0000-0000-0000-00000000001c', '00000000-0000-0000-0000-00000000002c']::quid[], array[1.11, 2.22]::numerical[] ) t(bucket_holder_uid, item_uid, amount) ), inserted_item ( insert item (bucket_id, item_uid, amount) select bucket.id, invested.item_uid, invested.amount unrest join bucket_hold invested.bucket_holder_uid = bucket_holder.bucket_holder_uid join bucket bucket.bucket_holder_id = bucket_holder.id join bucket_tot bucket_total.bucket_id = bucket.id bucket.default_bucket update bucket_tot conflict not return bucket_id, amount ), total_for_bucket ( select bucket_id, sum(amount) total inserted_item group bucket_id ) update bucket_tot set amount = amount + total_for_bucket.to total_for_bucket bucket_total.bucket_id = total_for_bucket.bucket_id reality array pass dream length 1000, 3 array length. array away sort bucket_holder_uid order order ensue deadlock cannot occur. point conflict not all hand situate item already present (the conflict item_uid). case bucket_tot course updated. query assume appropri bucket_holder, bucket bucket_tot record already exist. ok query fail otherwise practice situate occur. example set sample data: insert bucket_hold (bucket_holder_uid) value ('00000000-0000-0000-0000-00000000001a'); insert bucket (bucket_uid, bucket_holder_id, default_bucket) value ('00000000-0000-0000-0000-00000000001b', (select id bucket_hold bucket_holder_uid = '00000000-0000-0000-0000-00000000001a'), true); insert bucket_tot (bucket_id, amount) value ((select id bucket bucket_uid = '00000000-0000-0000-0000-00000000001b'), 0); insert bucket_hold (bucket_holder_uid) value ('00000000-0000-0000-0000-00000000002a'); insert bucket (bucket_uid, bucket_holder_id, default_bucket) value ('00000000-0000-0000-0000-00000000002b', (select id bucket_hold bucket_holder_uid = '00000000-0000-0000-0000-00000000002a'), true); insert bucket_tot (bucket_id, amount) value ((select id bucket bucket_uid = '00000000-0000-0000-0000-00000000002b'), 0); query appear done correct thing under thousand items, hand items, bucket_tot update twice amount item. know update twice update twice amount item. howe cases, one item insert (insert twice would impose anyway unique constraint item_uid). log suggest affect bucket, two thread execute query simultaneously. anyone see explain issue query india could rewritten? use version pg.6.6 update we'v spoken core poster develop this, appear see concur issue here. we'r investing really nasty possible index corruption, (remote) chance pg bug."
48249783,Search Query not accurate enough,"I have a search query done by me to the best of my knowledge in PHP but there are some improvements required:
When I search say 'what is food' and I have 'what is food' in the database all results containing one of the keywords 'what', 'is', 'food' are shown. The desired behaviour is to display results containing the exact phrase 'what is food' (first)
Only the last word in the query is highlighted and I want to highlight all words
Desired behaviour: The right answer shows at the top, regardless of its position in the database.
My current code is like this:
if (isset($_GET[""mainSearch""]))
{
  $condition = '';
  $mainSearch = SQLite3::escapeString($_GET['mainSearch']);
  $keyword = $_GET['mainSearch'];
  $query = explode("" "", $keyword);
  $perpageview=7;
  if ($_GET[""pageno""])
  {
      $page=$_GET[""pageno""];
  }
  else
  {
      $page=1;
  }
  $frompage = $page*$perpageview-$perpageview;
  foreach ($query as $text)
  {
      $condition .= ""question LIKE '%"".SQLite3::escapeString($text).""%' OR answer LIKE '%"".SQLite3::escapeString($text).""%' OR "";
  }
  foreach ($query as $text_2)
  {
      $condition_2 .= ""bname LIKE '%"".SQLite3::escapeString($text_2).""%' OR bankreq LIKE '%"".SQLite3::escapeString($text_2).""%' OR "";
  }
  $condition = substr($condition, 0, -4);
  $condition_2 = substr($condition_2, 0, -4);
  $order = "" ORDER BY quiz_id DESC "";
  $order_2 = "" ORDER BY id DESC "";
  $sql_query = ""SELECT * FROM questions WHERE "" . $condition . ' '. $order.' LIMIT '.$frompage.','.$perpageview;
  $sql_query_count = ""SELECT COUNT(*) as count FROM questions WHERE "" . $condition .' '. $order;
  //$mainAnswer = ""SELECT * FROM questions WHERE question LIKE '%$mainSearch%' or answer LIKE '%$mainSearch%'"";
  $bank_query = ""SELECT * FROM banks WHERE "" . $condition_2 . ' LIMIT 1';
  $result = $db-&gt;query($sql_query);
  $resultCount = $db-&gt;querySingle($sql_query_count);
  $bankret = $db-&gt;query($bank_query);
  //$mainAnsRet = $db-&gt;query($mainAnswer);
  $pagecount = ceil($resultCount/$perpageview);
  if ($resultCount &gt; 0)
  {
  if ($result &amp;&amp; $bankret)
  {
      while ($row = $result-&gt;fetchArray(SQLITE3_ASSOC))
      {
          $wording = str_replace($text, ""&lt;span style='font-weight: bold; color: #1a0dab;'&gt;"".$text.""&lt;/span&gt;"", $row['answer']);
           echo '&lt;div class=""quesbox_3""&gt;
            &lt;div class=""questitle""&gt;
                &lt;h2&gt;'.$row[""question""].'&lt;/h2&gt;
            &lt;/div&gt;
            &lt;div class=""quesanswer""&gt;'.$wording.'&lt;/div&gt;
        &lt;/div&gt;';
      }
      while ($brow = $bankret-&gt;fetchArray(SQLITE3_ASSOC))
      {
            $bname = $brow['bname'];
            $bankbrief = $brow['bankbrief'];
            $bankreq = $brow['bankreq'];
            $bankaddress = $brow['bankaddress'];
            $banklogo = $brow['banklogo'];
            $founded = $brow['founded'];
            $owner = $brow['owner'];
            $available = $brow['available'];
           echo '&lt;div class=""modulecontent""&gt;
            &lt;div class=""modulename""&gt;
                &lt;div class=""mname""&gt;'.$bname.'&lt;/div&gt;
                &lt;div class=""mlogo""&gt;&lt;img src=""'.$banklogo.'""&gt;&lt;/div&gt;
            &lt;/div&gt;';
            if (strlen($bankreq) &gt; 300)
            {
                $bankcut = substr($bankreq, 0, 300);
                $bankreq = substr($bankcut, 0, strrpos($bankcut, ' ')).'... &lt;a href=""bankprofile.php?bname='.$bname.'""&gt;Read More&lt;/a&gt;';
                echo '&lt;div class=""modulebrief""&gt;'.$bankreq.'&lt;/div&gt;';
            }
            echo '&lt;div class=""modulelinks""&gt;
                &lt;div class=""mfound""&gt;Founded: &lt;span&gt;'.$founded.'&lt;/span&gt;&lt;/div&gt;
                &lt;div class=""mowned""&gt;Ownd By: &lt;span&gt;'.$owner.'&lt;/span&gt;&lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;';
               // &lt;div class=""mavailable""&gt;Available for Export Loan: &lt;span&gt;'.$available.'&lt;/span&gt;&lt;/div&gt;
      }
      ?&gt;
      &lt;div class=""page_num""&gt;
      &lt;?php
      for ($i=1; $i &lt;= $pagecount; $i++) {
         echo '&lt;a href=""searchresult.php?mainSearch='.$mainSearch.'&amp;pageno='.$i.'""&gt;'.$i.'&lt;/a&gt;';
      }
      ?&gt;
      &lt;/div&gt;
      &lt;?php
  }
  }
  else
  {
      $session_n = $_SESSION['log_id'];
      $sesdate = date('d/M/Y');
      echo ""&lt;div class='searchNone'&gt;&lt;p&gt;No results found&lt;/p&gt;&lt;/div&gt;
      &lt;div class='sendSearchQ'&gt;
      &lt;p&gt;Please send us your question.&lt;/p&gt;
      &lt;form action='sendquestion.php' method='post' encytype='multipart/form-data'&gt;
      &lt;div class='searchQinputs'&gt;
          &lt;input type='text' name='searchQuestion' id='searchQuestion'placeholder='Whats your question'&gt;&lt;br&gt;
          &lt;input type='submit' name='sendQuestion' id='sendQuestion' value='Send'&gt;
          &lt;input type='text' name='user' id='user' value='$session_n' style='display: none'&gt;
          &lt;input type='text' name='qDate' id='qDate' value='$sesdate' style='display: none'&gt;
          &lt;input type='text' name='status' id='status' value='0' style='display: none'&gt;
          &lt;/div&gt;
      &lt;/form&gt;
      &lt;/div&gt;"";
  }
}
",<php><search><sqlite>,5282,0,124,493,1,7,29,68,444,0.0,25,5,13,2018-01-14 12:55,2018-01-18 18:59,2018-01-18 18:59,4.0,4.0,Basic,9,"<php><search><sqlite>, Search Query not accurate enough, I have a search query done by me to the best of my knowledge in PHP but there are some improvements required:
When I search say 'what is food' and I have 'what is food' in the database all results containing one of the keywords 'what', 'is', 'food' are shown. The desired behaviour is to display results containing the exact phrase 'what is food' (first)
Only the last word in the query is highlighted and I want to highlight all words
Desired behaviour: The right answer shows at the top, regardless of its position in the database.
My current code is like this:
if (isset($_GET[""mainSearch""]))
{
  $condition = '';
  $mainSearch = SQLite3::escapeString($_GET['mainSearch']);
  $keyword = $_GET['mainSearch'];
  $query = explode("" "", $keyword);
  $perpageview=7;
  if ($_GET[""pageno""])
  {
      $page=$_GET[""pageno""];
  }
  else
  {
      $page=1;
  }
  $frompage = $page*$perpageview-$perpageview;
  foreach ($query as $text)
  {
      $condition .= ""question LIKE '%"".SQLite3::escapeString($text).""%' OR answer LIKE '%"".SQLite3::escapeString($text).""%' OR "";
  }
  foreach ($query as $text_2)
  {
      $condition_2 .= ""bname LIKE '%"".SQLite3::escapeString($text_2).""%' OR bankreq LIKE '%"".SQLite3::escapeString($text_2).""%' OR "";
  }
  $condition = substr($condition, 0, -4);
  $condition_2 = substr($condition_2, 0, -4);
  $order = "" ORDER BY quiz_id DESC "";
  $order_2 = "" ORDER BY id DESC "";
  $sql_query = ""SELECT * FROM questions WHERE "" . $condition . ' '. $order.' LIMIT '.$frompage.','.$perpageview;
  $sql_query_count = ""SELECT COUNT(*) as count FROM questions WHERE "" . $condition .' '. $order;
  //$mainAnswer = ""SELECT * FROM questions WHERE question LIKE '%$mainSearch%' or answer LIKE '%$mainSearch%'"";
  $bank_query = ""SELECT * FROM banks WHERE "" . $condition_2 . ' LIMIT 1';
  $result = $db-&gt;query($sql_query);
  $resultCount = $db-&gt;querySingle($sql_query_count);
  $bankret = $db-&gt;query($bank_query);
  //$mainAnsRet = $db-&gt;query($mainAnswer);
  $pagecount = ceil($resultCount/$perpageview);
  if ($resultCount &gt; 0)
  {
  if ($result &amp;&amp; $bankret)
  {
      while ($row = $result-&gt;fetchArray(SQLITE3_ASSOC))
      {
          $wording = str_replace($text, ""&lt;span style='font-weight: bold; color: #1a0dab;'&gt;"".$text.""&lt;/span&gt;"", $row['answer']);
           echo '&lt;div class=""quesbox_3""&gt;
            &lt;div class=""questitle""&gt;
                &lt;h2&gt;'.$row[""question""].'&lt;/h2&gt;
            &lt;/div&gt;
            &lt;div class=""quesanswer""&gt;'.$wording.'&lt;/div&gt;
        &lt;/div&gt;';
      }
      while ($brow = $bankret-&gt;fetchArray(SQLITE3_ASSOC))
      {
            $bname = $brow['bname'];
            $bankbrief = $brow['bankbrief'];
            $bankreq = $brow['bankreq'];
            $bankaddress = $brow['bankaddress'];
            $banklogo = $brow['banklogo'];
            $founded = $brow['founded'];
            $owner = $brow['owner'];
            $available = $brow['available'];
           echo '&lt;div class=""modulecontent""&gt;
            &lt;div class=""modulename""&gt;
                &lt;div class=""mname""&gt;'.$bname.'&lt;/div&gt;
                &lt;div class=""mlogo""&gt;&lt;img src=""'.$banklogo.'""&gt;&lt;/div&gt;
            &lt;/div&gt;';
            if (strlen($bankreq) &gt; 300)
            {
                $bankcut = substr($bankreq, 0, 300);
                $bankreq = substr($bankcut, 0, strrpos($bankcut, ' ')).'... &lt;a href=""bankprofile.php?bname='.$bname.'""&gt;Read More&lt;/a&gt;';
                echo '&lt;div class=""modulebrief""&gt;'.$bankreq.'&lt;/div&gt;';
            }
            echo '&lt;div class=""modulelinks""&gt;
                &lt;div class=""mfound""&gt;Founded: &lt;span&gt;'.$founded.'&lt;/span&gt;&lt;/div&gt;
                &lt;div class=""mowned""&gt;Ownd By: &lt;span&gt;'.$owner.'&lt;/span&gt;&lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;';
               // &lt;div class=""mavailable""&gt;Available for Export Loan: &lt;span&gt;'.$available.'&lt;/span&gt;&lt;/div&gt;
      }
      ?&gt;
      &lt;div class=""page_num""&gt;
      &lt;?php
      for ($i=1; $i &lt;= $pagecount; $i++) {
         echo '&lt;a href=""searchresult.php?mainSearch='.$mainSearch.'&amp;pageno='.$i.'""&gt;'.$i.'&lt;/a&gt;';
      }
      ?&gt;
      &lt;/div&gt;
      &lt;?php
  }
  }
  else
  {
      $session_n = $_SESSION['log_id'];
      $sesdate = date('d/M/Y');
      echo ""&lt;div class='searchNone'&gt;&lt;p&gt;No results found&lt;/p&gt;&lt;/div&gt;
      &lt;div class='sendSearchQ'&gt;
      &lt;p&gt;Please send us your question.&lt;/p&gt;
      &lt;form action='sendquestion.php' method='post' encytype='multipart/form-data'&gt;
      &lt;div class='searchQinputs'&gt;
          &lt;input type='text' name='searchQuestion' id='searchQuestion'placeholder='Whats your question'&gt;&lt;br&gt;
          &lt;input type='submit' name='sendQuestion' id='sendQuestion' value='Send'&gt;
          &lt;input type='text' name='user' id='user' value='$session_n' style='display: none'&gt;
          &lt;input type='text' name='qDate' id='qDate' value='$sesdate' style='display: none'&gt;
          &lt;input type='text' name='status' id='status' value='0' style='display: none'&gt;
          &lt;/div&gt;
      &lt;/form&gt;
      &lt;/div&gt;"";
  }
}
","<pp><search><quite>, search query occur enough, search query done best knowledge pp improve required: search say 'what food' 'what food' database result contain one eyford 'what', 'is', 'food' shown. desire behaviour display result contain exact phrase 'what food' (first) last word query highlight want highlight word desire behaviour: right answer show top, regardless post database. current code like this: (asset($get[""mainsearch""])) { $conduct = ''; $mainsearch = sqlite3::escapestring($get['mainsearch']); $eyford = $get['mainsearch']; $query = explode("" "", $eyford); $perpageview=7; ($get[""page""]) { $page=$get[""page""]; } else { $page=1; } $frompag = $page*$perpageview-$perpageview; french ($query $text) { $conduct .= ""question like '%"".sqlite3::escapestring($text).""%' answer like '%"".sqlite3::escapestring($text).""%' ""; } french ($query $text) { $condition .= ""name like '%"".sqlite3::escapestring($text).""%' banker like '%"".sqlite3::escapestring($text).""%' ""; } $conduct = substr($condition, 0, -4); $condition = substr($condition, 0, -4); $order = "" order quiz_id desk ""; $order = "" order id desk ""; $sql_queri = ""select * question "" . $conduct . ' '. $order.' limit '.$frompage.','.$perpageview; $sql_query_count = ""select count(*) count question "" . $conduct .' '. $order; //$mainansw = ""select * question question like '%$mainsearch%' answer like '%$mainsearch%'""; $bank_queri = ""select * bank "" . $condition . ' limit 1'; $result = $do-&it;query($sql_query); $resultcount = $do-&it;querysingle($sql_query_count); $banker = $do-&it;query($bank_query); //$mainansret = $do-&it;query($mainanswer); $pagecount = veil($resultcount/$perpageview); ($resultcount &it; 0) { ($result &amp;&amp; $banker) { ($row = $result-&it;fetcharray(sqlite3_assoc)) { $word = str_replace($text, ""&it;span style='font-weight: bold; color: #1a0dab;'&it;"".$text.""&it;/span&it;"", $row['answer']); echo '&it;did class=""quesbox_3""&it; &it;did class=""questitle""&it; &it;he&it;'.$row[""question""].'&it;/he&it; &it;/did&it; &it;did class=""quesanswer""&it;'.$wording.'&it;/did&it; &it;/did&it;'; } ($brow = $banker-&it;fetcharray(sqlite3_assoc)) { $name = $brow['name']; $bankbrief = $brow['bankbrief']; $banker = $brow['banker']; $bankaddress = $brow['bankaddress']; $banklogo = $brow['banklogo']; $found = $brow['founded']; $owner = $brow['owner']; $avail = $brow['available']; echo '&it;did class=""modulecontent""&it; &it;did class=""modulename""&it; &it;did class=""name""&it;'.$name.'&it;/did&it; &it;did class=""log""&it;&it;ing sac=""'.$banklogo.'""&it;&it;/did&it; &it;/did&it;'; (stolen($banker) &it; 300) { $bankrupt = substr($banker, 0, 300); $banker = substr($bankrupt, 0, strips($bankrupt, ' ')).'... &it;a he=""bankprofile.pp?name='.$name.'""&it;read more&it;/a&it;'; echo '&it;did class=""modulebrief""&it;'.$banker.'&it;/did&it;'; } echo '&it;did class=""modulelinks""&it; &it;did class=""found""&it;founded: &it;span&it;'.$founded.'&it;/span&it;&it;/did&it; &it;did class=""owned""&it;own by: &it;span&it;'.$owner.'&it;/span&it;&it;/did&it; &it;/did&it; &it;/did&it;'; // &it;did class=""available""&it;avail export loan: &it;span&it;'.$available.'&it;/span&it;&it;/did&it; } ?&it; &it;did class=""page_num""&it; &it;?pp ($i=1; $i &it;= $pagecount; $i++) { echo '&it;a he=""searchresult.pp?mainsearch='.$mainsearch.'&amp;page='.$i.'""&it;'.$i.'&it;/a&it;'; } ?&it; &it;/did&it; &it;?pp } } else { $session = $session['logic']; $seat = date('d/m/y'); echo ""&it;did class='searchnone'&it;&it;p&it;no result found&it;/p&it;&it;/did&it; &it;did class='sendsearchq'&it; &it;p&it;pleas send us question.&it;/p&it; &it;form action='sendquestion.pp' method='post' encytype='multipart/form-data'&it; &it;did class='searchqinputs'&it; &it;input type='text' name='searchquestion' id='searchquestion'slaveholder='what question'&it;&it;br&it; &it;input type='submit' name='sendquestion' id='sendquestion' value='send'&it; &it;input type='text' name='user' id='user' value='$session' style='display: none'&it; &it;input type='text' name='date' id='date' value='$sedate' style='display: none'&it; &it;input type='text' name='status' id='status' value='0' style='display: none'&it; &it;/did&it; &it;/form&it; &it;/did&it;""; } }"
51033689,How to fix error on postgres install ubuntu,"I'm struggling to fix an error on install of the postgres client. I'm installing this on a Continuous Integration build, so I need it to install without error. The thing is, the client is installed, and I can even run psql commands, if I ssh into the server, but I need this to run without my touch, which means the install has to happen without error. 
I've done all the google-foo, and none of the suggestions I've seen on Ubuntu forums, or here seem to point in the right direction. This is all on ubuntu 14.04.
Alternatively, maybe I can just silence the errors, as long as the client is usable.
Following is the error I run into:
sudo apt-get install postgresql-client
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following additional packages will be installed:
      libpq5 postgresql-client-9.6 postgresql-client-common
    Suggested packages:
      postgresql-9.6 postgresql-doc-9.6
    The following NEW packages will be installed:
      libpq5 postgresql-client postgresql-client-9.6 postgresql-client-common
    0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.
    Need to get 1494 kB of archives.
    After this operation, 6121 kB of additional disk space will be used.
    Get:1 http://deb.debian.org/debian stretch/main amd64 libpq5 amd64 9.6.7-0+deb9u1 [132 kB]
    Get:2 http://deb.debian.org/debian stretch/main amd64 postgresql-client-common all 181+deb9u1 [79.0 kB]
    Get:3 http://deb.debian.org/debian stretch/main amd64 postgresql-client-9.6 amd64 9.6.7-0+deb9u1 [1228 kB]
    Get:4 http://deb.debian.org/debian stretch/main amd64 postgresql-client all 9.6+181+deb9u1 [55.7 kB]
    Fetched 1494 kB in 0s (55.5 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    Selecting previously unselected package libpq5:amd64.
    (Reading database ... 31433 files and directories currently installed.)
    Preparing to unpack .../libpq5_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking libpq5:amd64 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client-common.
    Preparing to unpack .../postgresql-client-common_181+deb9u1_all.deb ...
    Unpacking postgresql-client-common (181+deb9u1) ...
    Selecting previously unselected package postgresql-client-9.6.
    Preparing to unpack .../postgresql-client-9.6_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client.
    Preparing to unpack .../postgresql-client_9.6+181+deb9u1_all.deb ...
    Unpacking postgresql-client (9.6+181+deb9u1) ...
    Setting up libpq5:amd64 (9.6.7-0+deb9u1) ...
    Processing triggers for libc-bin (2.24-11+deb9u3) ...
    Setting up postgresql-client-common (181+deb9u1) ...
    Setting up postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    update-alternatives: using /usr/share/postgresql/9.6/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode
    update-alternatives: error: error creating symbolic link '/usr/share/man/man7/ABORT.7.gz.dpkg-tmp': No such file or directory
    dpkg: error processing package postgresql-client-9.6 (--configure):
     subprocess installed post-installation script returned error exit status 2
    dpkg: dependency problems prevent configuration of postgresql-client:
     postgresql-client depends on postgresql-client-9.6; however:
      Package postgresql-client-9.6 is not configured yet.
    dpkg: error processing package postgresql-client (--configure):
     dependency problems - leaving unconfigured
    Errors were encountered while processing:
     postgresql-client-9.6
     postgresql-client
    E: Sub-process /usr/bin/dpkg returned an error code (1)
    Exited with code 100
I've tried the following to fix:
    sudo apt-get purge postgr*
    sudo apt-get autoremove
    sudo apt-get install synaptic
    sudo apt-get update
from: https://ubuntuforums.org/showthread.php?t=2277582
    which psql
    /usr/bin/psql
And
    more /etc/apt/sources.list
    deb http://deb.debian.org/debian stretch main
    deb http://deb.debian.org/debian stretch-updates main
    deb http://security.debian.org/debian-security stretch/updates main
I'm stumped on how to move forward. 
",<postgresql><ubuntu><debian>,4276,4,2,1049,1,14,40,68,10181,0.0,51,1,13,2018-06-26 1:20,2018-10-04 20:55,2018-10-04 20:55,100.0,100.0,Basic,9,"<postgresql><ubuntu><debian>, How to fix error on postgres install ubuntu, I'm struggling to fix an error on install of the postgres client. I'm installing this on a Continuous Integration build, so I need it to install without error. The thing is, the client is installed, and I can even run psql commands, if I ssh into the server, but I need this to run without my touch, which means the install has to happen without error. 
I've done all the google-foo, and none of the suggestions I've seen on Ubuntu forums, or here seem to point in the right direction. This is all on ubuntu 14.04.
Alternatively, maybe I can just silence the errors, as long as the client is usable.
Following is the error I run into:
sudo apt-get install postgresql-client
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following additional packages will be installed:
      libpq5 postgresql-client-9.6 postgresql-client-common
    Suggested packages:
      postgresql-9.6 postgresql-doc-9.6
    The following NEW packages will be installed:
      libpq5 postgresql-client postgresql-client-9.6 postgresql-client-common
    0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.
    Need to get 1494 kB of archives.
    After this operation, 6121 kB of additional disk space will be used.
    Get:1 http://deb.debian.org/debian stretch/main amd64 libpq5 amd64 9.6.7-0+deb9u1 [132 kB]
    Get:2 http://deb.debian.org/debian stretch/main amd64 postgresql-client-common all 181+deb9u1 [79.0 kB]
    Get:3 http://deb.debian.org/debian stretch/main amd64 postgresql-client-9.6 amd64 9.6.7-0+deb9u1 [1228 kB]
    Get:4 http://deb.debian.org/debian stretch/main amd64 postgresql-client all 9.6+181+deb9u1 [55.7 kB]
    Fetched 1494 kB in 0s (55.5 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    Selecting previously unselected package libpq5:amd64.
    (Reading database ... 31433 files and directories currently installed.)
    Preparing to unpack .../libpq5_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking libpq5:amd64 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client-common.
    Preparing to unpack .../postgresql-client-common_181+deb9u1_all.deb ...
    Unpacking postgresql-client-common (181+deb9u1) ...
    Selecting previously unselected package postgresql-client-9.6.
    Preparing to unpack .../postgresql-client-9.6_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client.
    Preparing to unpack .../postgresql-client_9.6+181+deb9u1_all.deb ...
    Unpacking postgresql-client (9.6+181+deb9u1) ...
    Setting up libpq5:amd64 (9.6.7-0+deb9u1) ...
    Processing triggers for libc-bin (2.24-11+deb9u3) ...
    Setting up postgresql-client-common (181+deb9u1) ...
    Setting up postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    update-alternatives: using /usr/share/postgresql/9.6/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode
    update-alternatives: error: error creating symbolic link '/usr/share/man/man7/ABORT.7.gz.dpkg-tmp': No such file or directory
    dpkg: error processing package postgresql-client-9.6 (--configure):
     subprocess installed post-installation script returned error exit status 2
    dpkg: dependency problems prevent configuration of postgresql-client:
     postgresql-client depends on postgresql-client-9.6; however:
      Package postgresql-client-9.6 is not configured yet.
    dpkg: error processing package postgresql-client (--configure):
     dependency problems - leaving unconfigured
    Errors were encountered while processing:
     postgresql-client-9.6
     postgresql-client
    E: Sub-process /usr/bin/dpkg returned an error code (1)
    Exited with code 100
I've tried the following to fix:
    sudo apt-get purge postgr*
    sudo apt-get autoremove
    sudo apt-get install synaptic
    sudo apt-get update
from: https://ubuntuforums.org/showthread.php?t=2277582
    which psql
    /usr/bin/psql
And
    more /etc/apt/sources.list
    deb http://deb.debian.org/debian stretch main
    deb http://deb.debian.org/debian stretch-updates main
    deb http://security.debian.org/debian-security stretch/updates main
I'm stumped on how to move forward. 
","<postgresql><bunt><median>, fix error poster instal bunt, i'm struggle fix error instal poster client. i'm instal continue inter build, need instal without error. thing is, client installed, even run pool commands, ash server, need run without touch, mean instal happen without error. i'v done goose-foo, none suggest i'v seen bunt forms, seem point right direction. bunt 14.04. alternatively, may silent errors, long client unable. follow error run into: so apt-get instal postgresql-coli read package lists... done build depend tree read state information... done follow admit package installed: libpq5 postgresql-client-9.6 postgresql-client-common suggest packages: postgresql-9.6 postgresql-do-9.6 follow new package installed: libpq5 postgresql-coli postgresql-client-9.6 postgresql-client-common 0 upgrade, 4 newly installed, 0 remove 7 upgrade. need get 1494 b archives. operation, 6121 b admit disk space used. get:1 http://de.median.org/median stretch/main amd64 libpq5 amd64 9.6.7-0+debut [132 b] get:2 http://de.median.org/median stretch/main amd64 postgresql-client-common 181+debut [79.0 b] get:3 http://de.median.org/median stretch/main amd64 postgresql-client-9.6 amd64 9.6.7-0+debut [1228 b] get:4 http://de.median.org/median stretch/main amd64 postgresql-coli 9.6+181+debut [55.7 b] fetch 1494 b is (55.5 mb/s) deacon: delay package configuration, since apt-until instal select previous select package libpq5:amd64. (read database ... 31433 file director current installed.) prepare unpack .../libpq5_9.6.7-0+deb9u1_amd64.de ... unpack libpq5:amd64 (9.6.7-0+debut) ... select previous select package postgresql-client-common. prepare unpack .../postgresql-client-common_181+deb9u1_all.de ... unpack postgresql-client-common (181+debut) ... select previous select package postgresql-client-9.6. prepare unpack .../postgresql-client-9.6_9.6.7-0+deb9u1_amd64.de ... unpack postgresql-client-9.6 (9.6.7-0+debut) ... select previous select package postgresql-client. prepare unpack .../postgresql-client.6+181+deb9u1_all.de ... unpack postgresql-coli (9.6+181+debut) ... set libpq5:amd64 (9.6.7-0+debut) ... process trigger like-bin (2.24-11+debut) ... set postgresql-client-common (181+debut) ... set postgresql-client-9.6 (9.6.7-0+debut) ... update-alternatives: use /us/share/postgresql/9.6/man/man/pool.1.go proved /us/share/man/man/pool.1.go (pool.1.go) auto mode update-alternatives: error: error great symbol link '/us/share/man/man/abort.7.go.dog-tm': file director dog: error process package postgresql-client-9.6 (--configure): subprocess instal post-instal script return error exit state 2 dog: depend problem prevent configur postgresql-client: postgresql-coli depend postgresql-client-9.6; however: package postgresql-client-9.6 configur yet. dog: error process package postgresql-coli (--configure): depend problem - leave unconfigur error count processing: postgresql-client-9.6 postgresql-coli e: sub-process /us/bin/dog return error code (1) exit code 100 i'v try follow fix: so apt-get pure poster* so apt-get autoremov so apt-get instal snap so apt-get update from: http://ubuntuforums.org/showthread.pp?t=2277582 pool /us/bin/pool /etc/apt/sources.list de http://de.median.org/median stretch main de http://de.median.org/median stretch-up main de http://security.median.org/median-secure stretch/up main i'm stump move forward."
56017410,Pyspark Error:- dataType <class 'pyspark.sql.types.StringType'> should be an instance of <class 'pyspark.sql.types.DataType'>,"I need to extract some data from a pipelinedRDD but while converting it to Dataframe it is giving the following error:
Traceback (most recent call last):
  File ""/home/karan/Desktop/meds.py"", line 42, in &lt;module&gt;
    relevantToSymEntered(newrdd)
  File ""/home/karan/Desktop/meds.py"", line 26, in relevantToSymEntered
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed 
medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms 
Recorded"",""Severeness""],ArrayType)]))
  File ""/home/karan/Downloads/spark-2.4.2-bin-
hadoop2.7/python/pyspark/sql/types.py"", line 409, in __init__
    ""dataType %s should be an instance of %s"" % (dataType, DataType)
AssertionError: dataType &lt;class 'pyspark.sql.types.StringType'&gt; should be an 
instance of &lt;class 'pyspark.sql.types.DataType'&gt;
1. Thing my error is of different type it is TypeError while I got problems with AssertionError.
My problem has nothing to do with casting of data types.
I've already tried using toDF() but it changes the column names which is undesirable.
import findspark
findspark.init('/home/karan/Downloads/spark-2.4.2-bin-hadoop2.7')
from pyspark.sql import SQLContext
from pyspark.sql.types import StructType, StringType, IntegerType, StructField, ArrayType
from pyspark import SparkConf, SparkContext
import pandas as pd
def reduceColoumns(self):
    try:
        filtered=self.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""Disease""],x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return filtered
def cleanData(self,s):
    try:
        self.zipWithIndex
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return self.filter(lambda x: x[1][0]==s)
def relevantToSymEntered(self):
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms Recorded"",""Severeness""],ArrayType)]))
    #mat = mat.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    print(type(mat))
conf = SparkConf().setMaster(""local[*]"").setAppName(""MovieSimilarities"")
sc = SparkContext(conf = conf)
spark=SQLContext(sc)
rdd = spark.read.csv(""/home/karan/Desktop/ExportExcel2.csv"",header=True,sep="","",multiLine=""True"")
print(rdd)
newrdd=reduceColoumns(rdd)
x=input(""Enter the disease-"")
newrdd=cleanData(newrdd,x)
relevantToSymEntered(newrdd)
",<python><apache-spark><pyspark><apache-spark-sql>,2474,0,61,149,1,2,7,59,24530,0.0,0,1,13,2019-05-07 7:16,2020-05-05 3:53,,364.0,,Basic,9,"<python><apache-spark><pyspark><apache-spark-sql>, Pyspark Error:- dataType <class 'pyspark.sql.types.StringType'> should be an instance of <class 'pyspark.sql.types.DataType'>, I need to extract some data from a pipelinedRDD but while converting it to Dataframe it is giving the following error:
Traceback (most recent call last):
  File ""/home/karan/Desktop/meds.py"", line 42, in &lt;module&gt;
    relevantToSymEntered(newrdd)
  File ""/home/karan/Desktop/meds.py"", line 26, in relevantToSymEntered
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed 
medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms 
Recorded"",""Severeness""],ArrayType)]))
  File ""/home/karan/Downloads/spark-2.4.2-bin-
hadoop2.7/python/pyspark/sql/types.py"", line 409, in __init__
    ""dataType %s should be an instance of %s"" % (dataType, DataType)
AssertionError: dataType &lt;class 'pyspark.sql.types.StringType'&gt; should be an 
instance of &lt;class 'pyspark.sql.types.DataType'&gt;
1. Thing my error is of different type it is TypeError while I got problems with AssertionError.
My problem has nothing to do with casting of data types.
I've already tried using toDF() but it changes the column names which is undesirable.
import findspark
findspark.init('/home/karan/Downloads/spark-2.4.2-bin-hadoop2.7')
from pyspark.sql import SQLContext
from pyspark.sql.types import StructType, StringType, IntegerType, StructField, ArrayType
from pyspark import SparkConf, SparkContext
import pandas as pd
def reduceColoumns(self):
    try:
        filtered=self.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""Disease""],x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return filtered
def cleanData(self,s):
    try:
        self.zipWithIndex
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return self.filter(lambda x: x[1][0]==s)
def relevantToSymEntered(self):
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms Recorded"",""Severeness""],ArrayType)]))
    #mat = mat.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    print(type(mat))
conf = SparkConf().setMaster(""local[*]"").setAppName(""MovieSimilarities"")
sc = SparkContext(conf = conf)
spark=SQLContext(sc)
rdd = spark.read.csv(""/home/karan/Desktop/ExportExcel2.csv"",header=True,sep="","",multiLine=""True"")
print(rdd)
newrdd=reduceColoumns(rdd)
x=input(""Enter the disease-"")
newrdd=cleanData(newrdd,x)
relevantToSymEntered(newrdd)
","<patron><apache-spark><spark><apache-spark-sal>, spark error:- datatyp <class 'spark.sal.types.stringtype'> instant <class 'spark.sal.types.datatype'>, need extract data pipelinedrdd convert datafram give follow error: traceback (most recent call last): file ""/home/karay/desktop/beds.by"", line 42, &it;module&it; relevanttosymentered(neared) file ""/home/karay/desktop/beds.by"", line 26, relevanttosyment mat = spark.createdataframe(self,structtype([structfield(""prescribe medicine"",stringtype), structfield([""disease"",""id"",""symptom recorded"",""severeness""],arraytype)])) file ""/home/karay/download/spark-2.4.2-bin- hadoop2.7/patron/spark/sal/types.by"", line 409, __init__ ""datatyp %s instant %s"" % (datatype, datatype) assertionerror: datatyp &it;class 'spark.sal.types.stringtype'&it; instant &it;class 'spark.sal.types.datatype'&it; 1. thing error differ type typeerror got problem assertionerror. problem not cast data types. i'v already try use of() change column name undesirable. import findspark findspark.knit('/home/karay/download/spark-2.4.2-bin-hadoop2.7') spark.sal import sqlcontext spark.sal.type import structtype, stringtype, integertype, structfield, arraytyp spark import sparkconf, sparkcontext import and pp def reducecoloumns(self): try: filtered=self.red.map(labia x: (x[""prescribe medicine""],list([x[""disease""],x[""id""],x[""symptom recorded""],x[""severeness""]]))) except except e: print(""error cleandata:- "") print(e) return filter def cleandata(self,s): try: self.zipwithindex except except e: print(""error cleandata:- "") print(e) return self.filter(labia x: x[1][0]==s) def relevanttosymentered(self): mat = spark.createdataframe(self,structtype([structfield(""prescribe medicine"",stringtype), structfield([""disease"",""id"",""symptom recorded"",""severeness""],arraytype)])) #mat = mat.red.map(labia x: (x[""prescribe medicine""],list([x[""id""],x[""symptom recorded""],x[""severeness""]]))) print(type(mat)) cone = sparkconf().setmaster(""local[*]"").setappname(""moviesimilarities"") s = sparkcontext(cone = cone) spark=sqlcontext(s) red = spark.read.is(""/home/karay/desktop/exportexcel2.is"",header=true,see="","",multiline=""true"") print(red) neared=reducecoloumns(red) x=input(""went disease-"") neared=cleandata(neared,x) relevanttosymentered(neared)"
48189015,How to perform case insensitive ORDER BY in mysql?,"I want to perform case-insensitive ORDER BY in MySQL.
I have the data in my database like 
A, C, b, e, D etc
I'm getting the result as 
A, C, D, b, e 
But, I want the result as
A, b, C, D, e 
How can I get that?
",<mysql>,212,0,0,668,1,9,22,76,10626,0.0,313,4,13,2018-01-10 13:55,2018-01-10 13:57,2018-01-10 13:57,0.0,0.0,Basic,10,"<mysql>, How to perform case insensitive ORDER BY in mysql?, I want to perform case-insensitive ORDER BY in MySQL.
I have the data in my database like 
A, C, b, e, D etc
I'm getting the result as 
A, C, D, b, e 
But, I want the result as
A, b, C, D, e 
How can I get that?
","<myself>, perform case intensity order myself?, want perform case-intensity order myself. data database like a, c, b, e, etc i'm get result a, c, d, b, e but, want result a, b, c, d, e get that?"
53986727,"""Insert Into"" statement causing errors due to ""Parameter 7 (""""): The supplied value is not a valid instance of data type float.""","I'm loading a batch of CSV files into a SQL Server table using Python one row at a time.  The files each contain a number of free text fields and erroneous data which I trim and rename before attempting to insert.  
In general (about 95% of the time), the code seems to work however exceptions appear with the error message described below.
I'm confused as a) I only have four columns in my table, and can't understand why it would be looking for Parameter 7, and b) the text columns are being loaded into nvarchar(max) formatted columns, so I wouldn't expect a data type error.  
I've checked the source files to see which rows threw an error, there seems to be no discernible difference between the problem rows and others that are successfully loaded.
I've trimmed the process right back to only insert the JobID (as a bigint) and it works without issue, but as soon as I bring in the text fields, it causes an error.
I'm using Python 3.7.0 and loading into SQL Server 14.0
import numpy as np
import pyodbc
import os
import glob
import pandas as pd
import csv
import config
import urllib
import shutil
import codecs
path = ""C:\\myFilePath""
allFiles = glob.glob(os.path.join(path, ""*.csv""))
for file_ in allFiles:
    df = pd.concat((pd.read_csv(f, encoding='utf8') for f in allFiles))
cnxn = pyodbc.connect(""Driver={ODBC Driver 13 for SQL Server};""                                             
                  ""Server=myServer;""
                  ""Database=myDatabase;""
                  ""Trusted_Connection=yes;""
                  ""SelectMethod=cursor;""
                  )
df2 = df[['JobID', 'NPS_score', 'Obtuse_Column_Name_1', 'Obtuse_Column_Name_2']].copy()
df2.columns = ['JobID', 'Score','Q1', 'Q2']
cursor = cnxn.cursor()
for index,row in df2.iterrows():
    try:
        counter = counter + 1
        cursor.execute(""""""insert into [myDB].[dbo].[input_test]( [JobID], [Score], [Q1], [Q2]) VALUES (?, ?, ?, ?)"""""", row['JobID'],row['Score'],row['Q1'], row['Q2'])
        cursor.commit()
        print(counter)
    except Exception as e:
        print(e) 
        continue    
cursor.close()  
cnxn.close()
I expect the data to be loaded but on some lines get the following error code: 
  ('42000', '[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL
  Server]The incoming tabular data stream (TDS) remote procedure call
  (RPC) protocol stream is incorrect. Parameter 7 (""""): The supplied
  value is not a valid instance of data type float. Check the source
  data for invalid values. An example of an invalid value is data of
  numeric type with scale greater than precision. (8023)
  (SQLExecDirectW)')
",<python><sql-server><pandas>,2620,0,41,379,1,2,10,71,27292,0.0,0,3,13,2018-12-31 11:06,2018-12-31 13:12,2018-12-31 13:12,0.0,0.0,Basic,9,"<python><sql-server><pandas>, ""Insert Into"" statement causing errors due to ""Parameter 7 (""""): The supplied value is not a valid instance of data type float."", I'm loading a batch of CSV files into a SQL Server table using Python one row at a time.  The files each contain a number of free text fields and erroneous data which I trim and rename before attempting to insert.  
In general (about 95% of the time), the code seems to work however exceptions appear with the error message described below.
I'm confused as a) I only have four columns in my table, and can't understand why it would be looking for Parameter 7, and b) the text columns are being loaded into nvarchar(max) formatted columns, so I wouldn't expect a data type error.  
I've checked the source files to see which rows threw an error, there seems to be no discernible difference between the problem rows and others that are successfully loaded.
I've trimmed the process right back to only insert the JobID (as a bigint) and it works without issue, but as soon as I bring in the text fields, it causes an error.
I'm using Python 3.7.0 and loading into SQL Server 14.0
import numpy as np
import pyodbc
import os
import glob
import pandas as pd
import csv
import config
import urllib
import shutil
import codecs
path = ""C:\\myFilePath""
allFiles = glob.glob(os.path.join(path, ""*.csv""))
for file_ in allFiles:
    df = pd.concat((pd.read_csv(f, encoding='utf8') for f in allFiles))
cnxn = pyodbc.connect(""Driver={ODBC Driver 13 for SQL Server};""                                             
                  ""Server=myServer;""
                  ""Database=myDatabase;""
                  ""Trusted_Connection=yes;""
                  ""SelectMethod=cursor;""
                  )
df2 = df[['JobID', 'NPS_score', 'Obtuse_Column_Name_1', 'Obtuse_Column_Name_2']].copy()
df2.columns = ['JobID', 'Score','Q1', 'Q2']
cursor = cnxn.cursor()
for index,row in df2.iterrows():
    try:
        counter = counter + 1
        cursor.execute(""""""insert into [myDB].[dbo].[input_test]( [JobID], [Score], [Q1], [Q2]) VALUES (?, ?, ?, ?)"""""", row['JobID'],row['Score'],row['Q1'], row['Q2'])
        cursor.commit()
        print(counter)
    except Exception as e:
        print(e) 
        continue    
cursor.close()  
cnxn.close()
I expect the data to be loaded but on some lines get the following error code: 
  ('42000', '[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL
  Server]The incoming tabular data stream (TDS) remote procedure call
  (RPC) protocol stream is incorrect. Parameter 7 (""""): The supplied
  value is not a valid instance of data type float. Check the source
  data for invalid values. An example of an invalid value is data of
  numeric type with scale greater than precision. (8023)
  (SQLExecDirectW)')
","<patron><sal-server><hands>, ""insert into"" statement cause error due ""parapet 7 (""""): supply value valid instant data type float."", i'm load batch is file sal server table use patron one row time. file contain number free text field error data trim renal attempt insert. genet (about 95% time), code seem work howe except appear error message describe below. i'm confuse a) four column table, can't understand would look parapet 7, b) text column load nvarchar(max) format columns, expect data type error. i'v check source file see row threw error, seem discern differ problem row other success loaded. i'v trim process right back insert join (a begin) work without issue, soon bring text fields, cause error. i'm use patron 3.7.0 load sal server 14.0 import jump no import pyodbc import os import glow import and pp import is import confirm import urllib import shut import code path = ""c:\\myfilepath"" fulfil = glow.glow(os.path.join(path, ""*.is"")) file allies: of = pp.coat((pp.read_csv(f, encoding='utf') f allies)) can = pyodbc.connect(""driver={duc driver 13 sal server};"" ""server=observer;"" ""database=database;"" ""trusted_connection=yes;"" ""selectmethod=curses;"" ) of = of[['join', 'nps_score', 'obtuse_column_name_1', 'obtuse_column_name_2']].copy() of.column = ['join', 'score','qu', 'qu'] curses = can.curses() index,row of.terrors(): try: counter = counter + 1 curses.execute(""""""insert [my].[do].[input_test]( [join], [score], [qu], [qu]) value (?, ?, ?, ?)"""""", row['join'],row['score'],row['qu'], row['qu']) curses.commit() print(counter) except except e: print(e) continue curses.close() can.close() expect data load line get follow error code: ('42000', '[42000] [microsoft][duc driver 13 sal server][sal server]th income tubular data stream (tis) remote procedure call (rec) protocol stream incorrect. parapet 7 (""""): supply value valid instant data type float. check source data invalid values. example invalid value data number type scale greater precision. (8023) (sqlexecdirectw)')"
53529974,What does df.repartition with no column arguments partition on?,"In PySpark the repartition module has an optional columns argument which will of course repartition your dataframe by that key.
My question is - how does Spark repartition when there's no key? I couldn't dig any further into the source code to find where this goes through Spark itself.
def repartition(self, numPartitions, *cols):
    """"""
    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The
    resulting DataFrame is hash partitioned.
    :param numPartitions:
        can be an int to specify the target number of partitions or a Column.
        If it is a Column, it will be used as the first partitioning column. If not specified,
        the default number of partitions is used.
    .. versionchanged:: 1.6
       Added optional arguments to specify the partitioning columns. Also made numPartitions
       optional if partitioning columns are specified.
    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()
    10
    &gt;&gt;&gt; data = df.union(df).repartition(""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  5|  Bob|
    |  5|  Bob|
    |  2|Alice|
    |  2|Alice|
    +---+-----+
    &gt;&gt;&gt; data = data.repartition(7, ""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  2|Alice|
    |  5|  Bob|
    |  2|Alice|
    |  5|  Bob|
    +---+-----+
    &gt;&gt;&gt; data.rdd.getNumPartitions()
    7
    """"""
    if isinstance(numPartitions, int):
        if len(cols) == 0:
            return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)
        else:
            return DataFrame(
                self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)
    elif isinstance(numPartitions, (basestring, Column)):
        cols = (numPartitions, ) + cols
        return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)
    else:
        raise TypeError(""numPartitions should be an int or Column"")
For example: it's totally fine to call these lines but I have no idea what it's actually doing. Is it a hash of the entire line? Perhaps the first column in the dataframe?
df_2 = df_1\
       .where(sf.col('some_column') == 1)\
       .repartition(32)\
       .alias('df_2')
",<python><apache-spark><pyspark><apache-spark-sql>,2260,0,54,177,0,1,11,47,4970,0.0,19,1,13,2018-11-29 0:04,2018-11-29 0:31,,0.0,,Basic,2,"<python><apache-spark><pyspark><apache-spark-sql>, What does df.repartition with no column arguments partition on?, In PySpark the repartition module has an optional columns argument which will of course repartition your dataframe by that key.
My question is - how does Spark repartition when there's no key? I couldn't dig any further into the source code to find where this goes through Spark itself.
def repartition(self, numPartitions, *cols):
    """"""
    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The
    resulting DataFrame is hash partitioned.
    :param numPartitions:
        can be an int to specify the target number of partitions or a Column.
        If it is a Column, it will be used as the first partitioning column. If not specified,
        the default number of partitions is used.
    .. versionchanged:: 1.6
       Added optional arguments to specify the partitioning columns. Also made numPartitions
       optional if partitioning columns are specified.
    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()
    10
    &gt;&gt;&gt; data = df.union(df).repartition(""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  5|  Bob|
    |  5|  Bob|
    |  2|Alice|
    |  2|Alice|
    +---+-----+
    &gt;&gt;&gt; data = data.repartition(7, ""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  2|Alice|
    |  5|  Bob|
    |  2|Alice|
    |  5|  Bob|
    +---+-----+
    &gt;&gt;&gt; data.rdd.getNumPartitions()
    7
    """"""
    if isinstance(numPartitions, int):
        if len(cols) == 0:
            return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)
        else:
            return DataFrame(
                self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)
    elif isinstance(numPartitions, (basestring, Column)):
        cols = (numPartitions, ) + cols
        return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)
    else:
        raise TypeError(""numPartitions should be an int or Column"")
For example: it's totally fine to call these lines but I have no idea what it's actually doing. Is it a hash of the entire line? Perhaps the first column in the dataframe?
df_2 = df_1\
       .where(sf.col('some_column') == 1)\
       .repartition(32)\
       .alias('df_2')
","<patron><apache-spark><spark><apache-spark-sal>, of.repartee column argument partite on?, spark repartee model option column argument course repartee datafram key. question - spark repartee there' key? dig source code find go spark itself. def repetition(self, numpartitions, *cold): """""" return new :class:`dataframe` partite given partite expressions. result datafram has partition. :parma numpartitions: in specific target number partite column. column, use first partite column. specified, default number partite used. .. versionchanged:: 1.6 ad option argument specific partite columns. also made numpartit option partite column specified. &it;&it;&it; of.repetition(10).red.getnumpartitions() 10 &it;&it;&it; data = of.union(of).repetition(""age"") &it;&it;&it; data.show() +---+-----+ |age| name| +---+-----+ | 5| bob| | 5| bob| | 2|alice| | 2|alice| +---+-----+ &it;&it;&it; data = data.repetition(7, ""age"") &it;&it;&it; data.show() +---+-----+ |age| name| +---+-----+ | 2|alice| | 5| bob| | 2|alice| | 5| bob| +---+-----+ &it;&it;&it; data.red.getnumpartitions() 7 """""" instance(numpartitions, in): len(cold) == 0: return dataframe(self._jdf.repetition(numpartitions), self.sql_ctx) else: return dataframe( self._jdf.repetition(numpartitions, self._jcols(*cold)), self.sql_ctx) if instance(numpartitions, (basestring, column)): col = (numpartitions, ) + col return dataframe(self._jdf.repetition(self._jcols(*cold)), self.sql_ctx) else: rays typeerror(""numpartit in column"") example: total fine call line idea actual doing. has enter line? perhaps first column dataframe? df_2 = df_1\ .where(of.col('some_column') == 1)\ .repetition(32)\ .alias('df_2')"
53536206,Android Room database query does not return id column,"The problem is that the query returns all columns except  'id'
I use fts4 and in docs it says:
  FTS-enabled tables always use a primary key of type INTEGER and with
  the column name ""rowid"". If your FTS-table-backed entity defines a
  primary key, it must use that type and column name.
here is my entity class:
@Fts4
@Entity(tableName = ""projects"")
public class Project {
    @ColumnInfo(name = ""rowid"")
    @PrimaryKey(autoGenerate = true)
    private int id;
    private String name;
    @ColumnInfo(name = ""start_date"")
    private String startDate;
    @ColumnInfo(name = ""end_date"")
    private String endDate;
    private String description;
    @ColumnInfo(name = ""icon_path"")
    private String iconPath;
    private long budget;
public Project(String name, String startDate, String endDate, String description, String iconPath, long budget) {
    this.name = name;
    this.startDate = startDate;
    this.endDate = endDate;
    this.description = description;
    this.iconPath = iconPath;
    this.budget = budget;
}
public int getId() {
    return id;
}
public void setId(int id) {
    this.id = id;
}
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getStartDate() {
    return startDate;
}
public void setStartDate(String startDate) {
    this.startDate = startDate;
}
public String getEndDate() {
    return endDate;
}
public void setEndDate(String endDate) {
    this.endDate = endDate;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getIconPath() {
    return iconPath;
}
public void setIconPath(String iconPath) {
    this.iconPath = iconPath;
}
public long getBudget() {
    return budget;
}
public void setBudget(long budget) {
    this.budget = budget;
}
and here is my simple query:
@Query(""SELECT * FROM projects"")
public LiveData&lt;List&lt;Project&gt;&gt; getAllProjectsI);
I got a warning :
  app.aarsham.projeno.data.Model.Project has some fields [rowid] which
  are not returned by the query. If they are not supposed to be read
  from the result, you can mark them with @Ignore annotation. You can
  suppress this warning by annotating the method with
  @SuppressWarnings(RoomWarnings.CURSOR_MISMATCH). Columns returned by
  the query: name, start_date, end_date, description, icon_path, budget.
  Fields in app.aarsham.projeno.data.Model.Project: rowid, name,
  start_date, end_date, description, icon_path, budget.
and an error:
  The columns returned by the query does not have the fields [id] in
  app.aarsham.projeno.data.Model.Project even though they are annotated
  as non-null or primitive. Columns returned by the query:
  [name,start_date,end_date,description,icon_path,budget]
can anyone help about this?
",<android><sqlite><android-room><fts4>,2833,0,85,161,0,2,9,76,4006,0.0,67,1,13,2018-11-29 9:55,2018-12-30 3:09,2018-12-30 3:09,31.0,31.0,Basic,10,"<android><sqlite><android-room><fts4>, Android Room database query does not return id column, The problem is that the query returns all columns except  'id'
I use fts4 and in docs it says:
  FTS-enabled tables always use a primary key of type INTEGER and with
  the column name ""rowid"". If your FTS-table-backed entity defines a
  primary key, it must use that type and column name.
here is my entity class:
@Fts4
@Entity(tableName = ""projects"")
public class Project {
    @ColumnInfo(name = ""rowid"")
    @PrimaryKey(autoGenerate = true)
    private int id;
    private String name;
    @ColumnInfo(name = ""start_date"")
    private String startDate;
    @ColumnInfo(name = ""end_date"")
    private String endDate;
    private String description;
    @ColumnInfo(name = ""icon_path"")
    private String iconPath;
    private long budget;
public Project(String name, String startDate, String endDate, String description, String iconPath, long budget) {
    this.name = name;
    this.startDate = startDate;
    this.endDate = endDate;
    this.description = description;
    this.iconPath = iconPath;
    this.budget = budget;
}
public int getId() {
    return id;
}
public void setId(int id) {
    this.id = id;
}
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getStartDate() {
    return startDate;
}
public void setStartDate(String startDate) {
    this.startDate = startDate;
}
public String getEndDate() {
    return endDate;
}
public void setEndDate(String endDate) {
    this.endDate = endDate;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getIconPath() {
    return iconPath;
}
public void setIconPath(String iconPath) {
    this.iconPath = iconPath;
}
public long getBudget() {
    return budget;
}
public void setBudget(long budget) {
    this.budget = budget;
}
and here is my simple query:
@Query(""SELECT * FROM projects"")
public LiveData&lt;List&lt;Project&gt;&gt; getAllProjectsI);
I got a warning :
  app.aarsham.projeno.data.Model.Project has some fields [rowid] which
  are not returned by the query. If they are not supposed to be read
  from the result, you can mark them with @Ignore annotation. You can
  suppress this warning by annotating the method with
  @SuppressWarnings(RoomWarnings.CURSOR_MISMATCH). Columns returned by
  the query: name, start_date, end_date, description, icon_path, budget.
  Fields in app.aarsham.projeno.data.Model.Project: rowid, name,
  start_date, end_date, description, icon_path, budget.
and an error:
  The columns returned by the query does not have the fields [id] in
  app.aarsham.projeno.data.Model.Project even though they are annotated
  as non-null or primitive. Columns returned by the query:
  [name,start_date,end_date,description,icon_path,budget]
can anyone help about this?
","<andros><quite><andros-room><its>, andros room database query return id column, problem query return column except 'id' use its do says: its-enable table away use primary key type inter column name ""round"". its-table-back entity define primary key, must use type column name. entity class: @its @entity(tablenam = ""projects"") public class project { @columninfo(am = ""round"") @primarykey(autogener = true) privat in id; privat string name; @columninfo(am = ""start_date"") privat string startdate; @columninfo(am = ""end_date"") privat string exudate; privat string description; @columninfo(am = ""icon_path"") privat string iconpath; privat long budget; public project(sir name, string startdate, string exudate, string description, string iconpath, long budget) { this.am = name; this.started = startdate; this.end = exudate; this.rescript = description; this.iconpath = iconpath; this.budget = budget; } public in get() { return id; } public void said(in id) { this.id = id; } public string getname() { return name; } public void senate(sir name) { this.am = name; } public string getstartdate() { return startdate; } public void setstartdate(sir startdate) { this.started = startdate; } public string getenddate() { return exudate; } public void setenddate(sir exudate) { this.end = exudate; } public string getdescription() { return description; } public void setdescription(sir description) { this.rescript = description; } public string geticonpath() { return iconpath; } public void seticonpath(sir iconpath) { this.iconpath = iconpath; } public long getbudget() { return budget; } public void setbudget(long budget) { this.budget = budget; } simple query: @query(""select * projects"") public livedata&it;list&it;project&it;&it; getallprojectsi); got warn : pp.marshal.project.data.model.project field [round] return query. suppose read result, mark @ignore annexation. suppress warn cannot method @suppresswarnings(roomwarnings.cursor_mismatch). column return query: name, start_date, end_date, description, icon_path, budget. field pp.marshal.project.data.model.project: round, name, start_date, end_date, description, icon_path, budget. error: column return query field [id] pp.marshal.project.data.model.project even though cannot non-null primitive. column return query: [name,start_date,end_date,description,icon_path,budget] anyone help this?"
