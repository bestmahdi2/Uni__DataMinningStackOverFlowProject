QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
51916630,MongoDB mongoose Deprecation Warning,"While querying the documents by using collection.find I started getting following warning in my console
  DeprecationWarning: collection.find option [fields] is deprecated and
  will be removed in a later version
Why am I seeing this and how do I fix this? (Possible alternatives)
EDIT: Query Added
Session
        .find({ sessionCode: '18JANMON', completed: false })
        .limit(10)
        .sort({time: 1})
        .select({time: 1, sessionCode: 1});
Mongoose version 5.2.9
",<javascript><node.js><mongodb><mongoose><nosql>,479,0,6,4537,11,46,83,58,42623,0.0,512,14,44,2018-08-19 10:14,2018-08-19 14:43,2018-08-19 14:43,0.0,0.0,Advanced,38,"<javascript><node.js><mongodb><mongoose><nosql>, MongoDB mongoose Deprecation Warning, While querying the documents by using collection.find I started getting following warning in my console
  DeprecationWarning: collection.find option [fields] is deprecated and
  will be removed in a later version
Why am I seeing this and how do I fix this? (Possible alternatives)
EDIT: Query Added
Session
        .find({ sessionCode: '18JANMON', completed: false })
        .limit(10)
        .sort({time: 1})
        .select({time: 1, sessionCode: 1});
Mongoose version 5.2.9
","<javascript><node.is><mongodb><mongoose><nose>, mongodb mongoos degree warning, query document use collection.find start get follow warn console deprecationwarning: collection.find option [fields] degree remove later version see fix this? (possible alternatives) edit: query ad session .find({ sessioncode: '18janmon', completed: fall }) .limit(10) .sort({time: 1}) .select({time: 1, sessioncode: 1}); mongoos version 5.2.9"
54601529,Efficiently mapping one-to-many many-to-many database to struct in Golang,"Question
When dealing with a one-to-many or many-to-many SQL relationship in Golang, what is the best (efficient, recommended, ""Go-like"") way of mapping the rows to a struct?
Taking the example setup below I have tried to detail some approaches with Pros and Cons of each but was wondering what the community recommends.
Requirements
Works with PostgreSQL (can be generic but not include MySQL/Oracle specific features)
Efficiency - No brute forcing every combination
No ORM - Ideally using only database/sql and jmoiron/sqlx
Example
For sake of clarity I have removed error handling
Models
type Tag struct {
  ID int
  Name string
}
type Item struct {
  ID int
  Tags []Tag
}
Database
CREATE TABLE item (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY
);
CREATE TABLE tag (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  name                    VARCHAR(160),
  item_id                 INT REFERENCES item(id)
);
Approach 1 - Select all Items, then select tags per item
var items []Item
sqlxdb.Select(&amp;items, ""SELECT * FROM item"")
for i, item := range items {
  var tags []Tag
  sqlxdb.Select(&amp;tags, ""SELECT * FROM tag WHERE item_id = $1"", item.ID)
  items[i].Tags = tags
}
Pros
Simple
Easy to understand
Cons
Inefficient with the number of database queries increasing proportional with number of items
Approach 2 - Construct SQL join and loop through rows manually
var itemTags = make(map[int][]Tag)
var items = []Item{}
rows, _ := sqlxdb.Queryx(""SELECT i.id, t.id, t.name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
for rows.Next() {
  var (
    itemID  int
    tagID   int
    tagName string
  )
  rows.Scan(&amp;itemID, &amp;tagID, &amp;tagName)
  if tags, ok := itemTags[itemID]; ok {
    itemTags[itemID] = append(tags, Tag{ID: tagID, Name: tagName,})
  } else {
    itemTags[itemID] = []Tag{Tag{ID: tagID, Name: tagName,}}
  }
}
for itemID, tags := range itemTags {
  items = append(Item{
    ID: itemID,
    Tags: tags,
  })
}
Pros
A single database call and cursor that can be looped through without eating too much memory
Cons
Complicated and harder to develop with multiple joins and many attributes on the struct
Not too performant; more memory usage and processing time vs. more network calls
Failed approach 3 - sqlx struct scanning
Despite failing I want to include this approach as I find it to be my current aim of efficiency paired with development simplicity. My hope was by explicitly setting the db tag on each struct field sqlx could do some advanced struct scanning 
var items []Item
sqlxdb.Select(&amp;items, ""SELECT i.id AS item_id, t.id AS tag_id, t.name AS tag_name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
Unfortunately this errors out as missing destination name tag_id in *[]Item leading me to believe the StructScan is not advanced enough to recursively loop through rows (no criticism - it is a complicated scenario)
Possible approach 4 - PostgreSQL array aggregators and GROUP BY
While I am sure this will not work I have included this untested option to see if it could be improved upon so it may work.
var items = []Item{}
sqlxdb.Select(&amp;items, ""SELECT i.id as item_id, array_agg(t.*) as tags FROM item AS i JOIN tag AS t ON t.item_id = i.id GROUP BY i.id"")
When I have some time I will try and run some experiments here. 
",<sql><go><struct><sqlx>,3346,0,60,14702,6,48,62,53,18469,0.0,65,5,44,2019-02-08 23:17,2019-02-09 21:31,2019-02-10 9:26,1.0,2.0,Intermediate,22,"<sql><go><struct><sqlx>, Efficiently mapping one-to-many many-to-many database to struct in Golang, Question
When dealing with a one-to-many or many-to-many SQL relationship in Golang, what is the best (efficient, recommended, ""Go-like"") way of mapping the rows to a struct?
Taking the example setup below I have tried to detail some approaches with Pros and Cons of each but was wondering what the community recommends.
Requirements
Works with PostgreSQL (can be generic but not include MySQL/Oracle specific features)
Efficiency - No brute forcing every combination
No ORM - Ideally using only database/sql and jmoiron/sqlx
Example
For sake of clarity I have removed error handling
Models
type Tag struct {
  ID int
  Name string
}
type Item struct {
  ID int
  Tags []Tag
}
Database
CREATE TABLE item (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY
);
CREATE TABLE tag (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  name                    VARCHAR(160),
  item_id                 INT REFERENCES item(id)
);
Approach 1 - Select all Items, then select tags per item
var items []Item
sqlxdb.Select(&amp;items, ""SELECT * FROM item"")
for i, item := range items {
  var tags []Tag
  sqlxdb.Select(&amp;tags, ""SELECT * FROM tag WHERE item_id = $1"", item.ID)
  items[i].Tags = tags
}
Pros
Simple
Easy to understand
Cons
Inefficient with the number of database queries increasing proportional with number of items
Approach 2 - Construct SQL join and loop through rows manually
var itemTags = make(map[int][]Tag)
var items = []Item{}
rows, _ := sqlxdb.Queryx(""SELECT i.id, t.id, t.name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
for rows.Next() {
  var (
    itemID  int
    tagID   int
    tagName string
  )
  rows.Scan(&amp;itemID, &amp;tagID, &amp;tagName)
  if tags, ok := itemTags[itemID]; ok {
    itemTags[itemID] = append(tags, Tag{ID: tagID, Name: tagName,})
  } else {
    itemTags[itemID] = []Tag{Tag{ID: tagID, Name: tagName,}}
  }
}
for itemID, tags := range itemTags {
  items = append(Item{
    ID: itemID,
    Tags: tags,
  })
}
Pros
A single database call and cursor that can be looped through without eating too much memory
Cons
Complicated and harder to develop with multiple joins and many attributes on the struct
Not too performant; more memory usage and processing time vs. more network calls
Failed approach 3 - sqlx struct scanning
Despite failing I want to include this approach as I find it to be my current aim of efficiency paired with development simplicity. My hope was by explicitly setting the db tag on each struct field sqlx could do some advanced struct scanning 
var items []Item
sqlxdb.Select(&amp;items, ""SELECT i.id AS item_id, t.id AS tag_id, t.name AS tag_name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
Unfortunately this errors out as missing destination name tag_id in *[]Item leading me to believe the StructScan is not advanced enough to recursively loop through rows (no criticism - it is a complicated scenario)
Possible approach 4 - PostgreSQL array aggregators and GROUP BY
While I am sure this will not work I have included this untested option to see if it could be improved upon so it may work.
var items = []Item{}
sqlxdb.Select(&amp;items, ""SELECT i.id as item_id, array_agg(t.*) as tags FROM item AS i JOIN tag AS t ON t.item_id = i.id GROUP BY i.id"")
When I have some time I will try and run some experiments here. 
","<sal><go><struck><self>, effect map one-to-man many-to-man database struck going, question deal one-to-man many-to-man sal relationship going, best (efficient, recommended, ""go-like"") way map row struck? take example set try detail approach pro con wonder common recommends. require work postgresql (can genet include myself/oral specie features) effect - brute for every combine or - ideal use database/sal jmoiron/self example sake clarity remove error hand model type tag struck { id in name string } type item struck { id in tag []tag } database great table item ( id in genet default went primary key ); great table tag ( id in genet default went primary key, name varchar(160), item_id in refer item(id) ); approach 1 - select items, select tag per item war item []item sqlxdb.select(&amp;items, ""select * item"") i, item := rang item { war tag []tag sqlxdb.select(&amp;tags, ""select * tag item_id = $1"", item.id) items[i].tag = tag } pro simple east understand con ineffici number database query increase property number item approach 2 - construct sal join loop row manual war itemtag = make(map[in][]tag) war item = []item{} rows, _ := sqlxdb.query(""select i.id, t.id, t.name item join tag t.item_id = i.id"") rows.next() { war ( timid in said in wagram string ) rows.scan(&amp;timid, &amp;said, &amp;magnate) tags, ok := itemtags[timid]; ok { itemtags[timid] = happened(tags, tag{id: said, name: magnate,}) } else { itemtags[timid] = []tag{tag{id: said, name: magnate,}} } } timid, tag := rang itemtag { item = happened(item{ id: timid, tags: tags, }) } pro single database call curses loop without eat much memory con complex harder develop multiple join man attribute struck performance; memory usage process time vs. network call fail approach 3 - self struck scan despite fail want include approach find current aim effect pair develop simplicity. hope explicitly set do tag struck field self could advance struck scan war item []item sqlxdb.select(&amp;items, ""select i.id item_id, t.id tag_id, t.name tag_nam item join tag t.item_id = i.id"") unfortun error miss destiny name tag_id *[]item lead believe structscan advance enough recurs loop row (no critic - complex scenario) possible approach 4 - postgresql array agree group sure work include unrest option see could improve upon may work. war item = []item{} sqlxdb.select(&amp;items, ""select i.id item_id, array_agg(t.*) tag item join tag t.item_id = i.id group i.id"") time try run expert here."
53975234,Instance of 'SQLAlchemy' has no 'Column' member (no-member),"I'm currently trying to implement steam login into website. But I'm unable to get pass this error within the code. I've created the database object but it keeps showing the error I mentioned earlier. I'm not sure whether SQLAlchemy has changed or what since I used it.
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
app = Flask(__name__)
db = SQLAlchemy(app)
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
The message emitted by pylint is
E1101: Instance of 'SQLAlchemy' has no 'Column' member (no-member)
",<python><python-3.x><flask><sqlalchemy><pylint>,547,0,10,535,1,5,8,75,42567,0.0,0,6,43,2018-12-30 4:15,2019-02-28 14:45,,60.0,,Basic,13,"<python><python-3.x><flask><sqlalchemy><pylint>, Instance of 'SQLAlchemy' has no 'Column' member (no-member), I'm currently trying to implement steam login into website. But I'm unable to get pass this error within the code. I've created the database object but it keeps showing the error I mentioned earlier. I'm not sure whether SQLAlchemy has changed or what since I used it.
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
app = Flask(__name__)
db = SQLAlchemy(app)
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
The message emitted by pylint is
E1101: Instance of 'SQLAlchemy' has no 'Column' member (no-member)
","<patron><patron-3.x><flask><sqlalchemy><point>, instant 'sqlalchemy' 'column' member (no-member), i'm current try implement steam login webster. i'm unable get pass error within code. i'v great database object keep show error mention earlier. i'm sure whether sqlalchemi change since use it. flask import flask flask_sqlalchemi import sqlalchemi pp = flask(__name__) do = sqlalchemy(pp) class user(do.model): id = do.column(do.inter, primary_key=true) message emit point e1101: instant 'sqlalchemy' 'column' member (no-member)"
51933421,System.Data.SQLite vs Microsoft.Data.Sqlite,"What are the differences between System.Data.SQLite and Microsoft.Data.Sqlite?
I understand that System.Data.SQLite is older and got .NETStandard support after Microsoft.Data.Sqlite, but now both of them support .NETStandard 2.
What are the advantages of one over the other?
",<.net><sqlite><system.data.sqlite>,275,0,0,601,1,5,11,49,21127,0.0,5,4,43,2018-08-20 14:54,2018-08-26 10:57,2018-08-26 10:57,6.0,6.0,Intermediate,19,"<.net><sqlite><system.data.sqlite>, System.Data.SQLite vs Microsoft.Data.Sqlite, What are the differences between System.Data.SQLite and Microsoft.Data.Sqlite?
I understand that System.Data.SQLite is older and got .NETStandard support after Microsoft.Data.Sqlite, but now both of them support .NETStandard 2.
What are the advantages of one over the other?
","<.net><quite><system.data.quite>, system.data.split vs microsoft.data.quite, differ system.data.split microsoft.data.quite? understand system.data.split older got .netstandard support microsoft.data.quite, support .netstandard 2. advantage one other?"
49984267,java.sql.SQLException: Unknown system variable 'query_cache_size',"I have a app running with JDBC and get data from MySQL, but I can't build it because of this error : 
java.sql.SQLException: Unknown system variable 'query_cache_size'
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2497) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2455) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1369) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.loadServerVariables(ConnectionImpl.java:3777) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:3240) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2249) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2035) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:790) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_131]
I have file application.properties here 
#specs.dir=/specs/
#
#################### Spring Boot Data Source Configuration ############
#spring.datasource.driver-class-name=com.mysql.jdbc.Driver
#spring.datasource.url=jdbc:mysql://localhost:3306/savingbooking?useSSL=false
#spring.datasource.username=root
#spring.datasource.password=ZAQ!2wsx
#spring.datasource.initialize=true
#spring.jpa.hibernate.ddl-auto=update
#spring.jpa.properties.hibernate.format_sql=true
#spring.jpa.show-sql=true
#spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
Mysql workbench is 8.0 version
",<java><mysql><spring-boot><jdbc>,2462,0,28,347,1,5,16,62,80235,0.0,48,11,43,2018-04-23 15:04,2018-04-23 15:10,2018-04-23 15:10,0.0,0.0,Basic,13,"<java><mysql><spring-boot><jdbc>, java.sql.SQLException: Unknown system variable 'query_cache_size', I have a app running with JDBC and get data from MySQL, but I can't build it because of this error : 
java.sql.SQLException: Unknown system variable 'query_cache_size'
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2497) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2455) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1369) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.loadServerVariables(ConnectionImpl.java:3777) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:3240) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2249) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2035) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:790) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_131]
I have file application.properties here 
#specs.dir=/specs/
#
#################### Spring Boot Data Source Configuration ############
#spring.datasource.driver-class-name=com.mysql.jdbc.Driver
#spring.datasource.url=jdbc:mysql://localhost:3306/savingbooking?useSSL=false
#spring.datasource.username=root
#spring.datasource.password=ZAQ!2wsx
#spring.datasource.initialize=true
#spring.jpa.hibernate.ddl-auto=update
#spring.jpa.properties.hibernate.format_sql=true
#spring.jpa.show-sql=true
#spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
Mysql workbench is 8.0 version
","<cava><myself><spring-boot><job>, cava.sal.sqlexception: unknown system variable 'query_cache_size', pp run job get data myself, can't build error : cava.sal.sqlexception: unknown system variable 'query_cache_size' com.myself.job.sqlerror.createsqlexception(sqlerror.cava:964) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.mysqlio.checkerrorpacket(mysqlio.cava:3973) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.mysqlio.checkerrorpacket(mysqlio.cava:3909) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.mysqlio.sendcommand(mysqlio.cava:2527) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.mysqlio.sqlquerydirect(mysqlio.cava:2680) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.execsql(connectionimpl.cava:2497) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.execsql(connectionimpl.cava:2455) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.statementimpl.executequery(statementimpl.cava:1369) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.loadservervariables(connectionimpl.cava:3777) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.initializepropsfromserver(connectionimpl.cava:3240) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.connectonetryonly(connectionimpl.cava:2249) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.createnewio(connectionimpl.cava:2035) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.connectionimpl.&it;knit&it;(connectionimpl.cava:790) ~[myself-connection-cava-5.1.41.jar:5.1.41] com.myself.job.jdbc4connection.&it;knit&it;(jdbc4connection.cava:47) ~[myself-connection-cava-5.1.41.jar:5.1.41] sun.reflect.nativeconstructoraccessorimpl.newinstance0(n method) ~[na:1.8.0_131] file application.property #speck.did=/speck/ # #################### spring boot data source configur ############ #spring.datasource.driver-class-name=com.myself.job.drive #spring.datasource.curl=job:myself://localhost:3306/savingbooking?useful=fall #spring.datasource.surname=root #spring.datasource.password=zat!wax #spring.datasource.initiative=true #spring.pa.liberate.del-auto=up #spring.pa.properties.liberate.format_sql=true #spring.pa.show-sal=true #spring.pa.properties.liberate.dialect=org.liberate.dialect.mysqldialect myself workbench 8.0 version"
50409788,MySQL 8 create new user with password not working,"I am using MySQL for several years and the command for the creating the new user till the MySQL 5.x version is as follow:
GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password';
Recently I installed the MySQL 8. In that, this command is not working.
It is throwing following error while firing above command:
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'password'' at line 1
Is there any change of syntax in MySQL 8? What is the correct syntax for creating new user command in MySQL 8?
Note: I tried this syntax in MySQL 5.x versions. It is working correctly in that.
",<mysql><sql><database>,717,0,2,2440,2,29,42,42,53618,0.0,1886,2,42,2018-05-18 10:56,2018-05-18 15:11,2018-05-18 15:11,0.0,0.0,Basic,5,"<mysql><sql><database>, MySQL 8 create new user with password not working, I am using MySQL for several years and the command for the creating the new user till the MySQL 5.x version is as follow:
GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password';
Recently I installed the MySQL 8. In that, this command is not working.
It is throwing following error while firing above command:
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'password'' at line 1
Is there any change of syntax in MySQL 8? What is the correct syntax for creating new user command in MySQL 8?
Note: I tried this syntax in MySQL 5.x versions. It is working correctly in that.
","<myself><sal><database>, myself 8 great new user password working, use myself never year command great new user till myself 5.x version follow: grant privilege *.* 'surname'@'localhost' identify 'password'; recent instal myself 8. that, command working. throw follow error fire command: error 1064 (42000): error sal santa; check manual correspond myself server version right santa use near 'identify 'password'' line 1 change santa myself 8? correct santa great new user command myself 8? note: try santa myself 5.x versions. work correctly that."
50180667,How can I connect to a database as another user?,"Im currently creating an API for a school project and everything is working good. My setup is: Node v10, Postgres, Koa and so on...
I currently have this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
\c master;
When the init script runs in the docker machine the output I get is this one:
CREATE ROLE
CREATE DATABASE
You are now connected to database ""master"" as user ""postgres"".
So I did change the file to something like this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
CONNECT TO master AS main USER sa;
And I get a syntax error:
STATEMENT:  CONNECT TO master AS sa USER sa;
psql:/docker-entrypoint-initdb.d/init.sql:4: ERROR:  syntax error at or near ""CONNECT""
I can't find anywhere in docs (or haven't look very good) how to connect from a .sql file to a database with an specific user.
How would I connect to 'master' with its owner, which is 'sa' from a .sql file?
",<postgresql><psql>,966,0,11,563,1,5,12,68,101774,0.0,5,4,42,2018-05-04 17:57,2018-05-04 18:45,2018-05-04 18:45,0.0,0.0,Basic,9,"<postgresql><psql>, How can I connect to a database as another user?, Im currently creating an API for a school project and everything is working good. My setup is: Node v10, Postgres, Koa and so on...
I currently have this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
\c master;
When the init script runs in the docker machine the output I get is this one:
CREATE ROLE
CREATE DATABASE
You are now connected to database ""master"" as user ""postgres"".
So I did change the file to something like this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
CONNECT TO master AS main USER sa;
And I get a syntax error:
STATEMENT:  CONNECT TO master AS sa USER sa;
psql:/docker-entrypoint-initdb.d/init.sql:4: ERROR:  syntax error at or near ""CONNECT""
I can't find anywhere in docs (or haven't look very good) how to connect from a .sql file to a database with an specific user.
How would I connect to 'master' with its owner, which is 'sa' from a .sql file?
","<postgresql><pool>, connect database not user?, in current great apt school project every work good. set is: node ve, postures, oka on... current this: great role sa login password 'some-password.'; great database master owner sa; \c master; knit script run doctor machine output get one: great role great database connect database ""master"" user ""postures"". change file cometh like this: great role sa login password 'some-password.'; great database master owner sa; connect master main user sa; get santa error: statement: connect master sa user sa; pool:/doctor-entrypoint-initdb.d/knit.sal:4: error: santa error near ""connect"" can't find anywhere do (or look good) connect .sal file database specie user. would connect 'master' owner, 'sa' .sal file?"
48956743,Embedded Postgres for Spring Boot Tests,"I'm building a Spring Boot app, backed by Postgres, using Flyway for database migrations. I've been bumping up against issues where I cannot produce a migration that generates the desired outcome in both Postgres, and the embedded unit test database (even with Postgres compatibility mode enabled). So I am looking at using embedded Postgres for unit tests.
I came across an embedded postgres implementation that looks promising, but don't really see how to set it up to run within Spring Boot's unit test framework only (for testing Spring Data repositories). How would one set this up using the mentioned tool or an alternative embedded version of Postgres?
",<spring><postgresql><spring-boot><flyway>,660,1,0,18871,14,73,102,39,75788,0.0,958,5,42,2018-02-23 21:52,2018-02-23 22:21,2018-02-27 15:11,0.0,4.0,Intermediate,30,"<spring><postgresql><spring-boot><flyway>, Embedded Postgres for Spring Boot Tests, I'm building a Spring Boot app, backed by Postgres, using Flyway for database migrations. I've been bumping up against issues where I cannot produce a migration that generates the desired outcome in both Postgres, and the embedded unit test database (even with Postgres compatibility mode enabled). So I am looking at using embedded Postgres for unit tests.
I came across an embedded postgres implementation that looks promising, but don't really see how to set it up to run within Spring Boot's unit test framework only (for testing Spring Data repositories). How would one set this up using the mentioned tool or an alternative embedded version of Postgres?
","<spring><postgresql><spring-boot><anyway>, ebbed poster spring boot tests, i'm build spring boot pp, back postures, use anyway database migrations. i'v bump issue cannot produce migrate genet desire outcome postures, ebbed unit test database (even poster compact mode enabled). look use ebbed poster unit tests. came across ebbed poster implement look promising, really see set run within spring boot' unit test framework (for test spring data depositaries). would one set use mention tool alter ebbed version postures?"
51082758,How to explode multiple columns of a dataframe in pyspark,"I have a dataframe which consists lists in columns similar to the following. The length of the lists in all columns is not same.
Name  Age  Subjects                  Grades
[Bob] [16] [Maths,Physics,Chemistry] [A,B,C]
I want to explode the dataframe in such a way that i get the following output-
Name Age Subjects Grades
Bob  16   Maths     A
Bob  16  Physics    B
Bob  16  Chemistry  C
How can I achieve this?
",<python><dataframe><apache-spark><pyspark><apache-spark-sql>,412,0,6,698,3,9,25,58,57601,0.0,19,7,42,2018-06-28 12:19,2018-06-28 12:25,2018-06-28 14:14,0.0,0.0,Basic,9,"<python><dataframe><apache-spark><pyspark><apache-spark-sql>, How to explode multiple columns of a dataframe in pyspark, I have a dataframe which consists lists in columns similar to the following. The length of the lists in all columns is not same.
Name  Age  Subjects                  Grades
[Bob] [16] [Maths,Physics,Chemistry] [A,B,C]
I want to explode the dataframe in such a way that i get the following output-
Name Age Subjects Grades
Bob  16   Maths     A
Bob  16  Physics    B
Bob  16  Chemistry  C
How can I achieve this?
","<patron><dataframe><apache-spark><spark><apache-spark-sal>, explode multiple column datafram spark, datafram consist list column similar following. length list column same. name age subject grade [bob] [16] [baths,physics,chemistry] [a,b,c] want explode datafram way get follow output- name age subject grade bob 16 path bob 16 physics b bob 16 chemistry c achieve this?"
48629799,Postgres image is not creating database,"According to these docs, I can specify the name of the database created by the postgres docker image with the env var POSTGRES_DB. I have set it in my docker-compose file, but it isn't being created.
Here's relevant section from the compose file:
pg:
    image: postgres:10
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: user-auth
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
Here are the logs:
Attaching to userauth_pg_1
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 18:05:54.806 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 18:05:54.817 UTC [24] LOG:  database system was interrupted; last known up at 2018-02-05 18:03:26 UTC
pg_1   | 2018-02-05 18:05:54.942 UTC [24] LOG:  database system was not properly shut down; automatic recovery in progress
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo starts at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  invalid record length at 0/1633F08: wanted 24, got 0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo done at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.955 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 18:05:59.140 UTC [31] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:06:15.528 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.120 UTC [33] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.151 UTC [34] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.138 UTC [35] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.926 UTC [36] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.244 UTC [37] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.273 UTC [38] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.602 UTC [39] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.910 UTC [40] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.777 UTC [41] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.823 UTC [42] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.878 UTC [43] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.663 UTC [44] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.716 UTC [45] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:16:32.713 UTC [46] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:47:04.603 UTC [47] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:51:34.413 UTC [1] LOG:  received smart shutdown request
pg_1   | 2018-02-05 18:51:34.417 UTC [1] LOG:  worker process: logical replication launcher (PID 30) exited with exit code 1
pg_1   | 2018-02-05 18:51:34.419 UTC [25] LOG:  shutting down
pg_1   | 2018-02-05 18:51:34.434 UTC [1] LOG:  database system is shut down
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 19:08:42.937 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 19:08:42.951 UTC [25] LOG:  database system was shut down at 2018-02-05 18:51:34 UTC
pg_1   | 2018-02-05 19:08:42.956 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 19:09:04.316 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 19:09:18.081 UTC [33] FATAL:  database ""user-auth"" does not exist
",<postgresql><docker>,3825,1,47,2349,2,19,17,36,46297,0.0,490,6,42,2018-02-05 19:11,2019-01-15 13:50,2019-01-15 13:50,344.0,344.0,Basic,14,"<postgresql><docker>, Postgres image is not creating database, According to these docs, I can specify the name of the database created by the postgres docker image with the env var POSTGRES_DB. I have set it in my docker-compose file, but it isn't being created.
Here's relevant section from the compose file:
pg:
    image: postgres:10
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: user-auth
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
Here are the logs:
Attaching to userauth_pg_1
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 18:05:54.806 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 18:05:54.817 UTC [24] LOG:  database system was interrupted; last known up at 2018-02-05 18:03:26 UTC
pg_1   | 2018-02-05 18:05:54.942 UTC [24] LOG:  database system was not properly shut down; automatic recovery in progress
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo starts at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  invalid record length at 0/1633F08: wanted 24, got 0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo done at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.955 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 18:05:59.140 UTC [31] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:06:15.528 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.120 UTC [33] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.151 UTC [34] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.138 UTC [35] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.926 UTC [36] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.244 UTC [37] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.273 UTC [38] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.602 UTC [39] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.910 UTC [40] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.777 UTC [41] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.823 UTC [42] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.878 UTC [43] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.663 UTC [44] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.716 UTC [45] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:16:32.713 UTC [46] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:47:04.603 UTC [47] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:51:34.413 UTC [1] LOG:  received smart shutdown request
pg_1   | 2018-02-05 18:51:34.417 UTC [1] LOG:  worker process: logical replication launcher (PID 30) exited with exit code 1
pg_1   | 2018-02-05 18:51:34.419 UTC [25] LOG:  shutting down
pg_1   | 2018-02-05 18:51:34.434 UTC [1] LOG:  database system is shut down
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 19:08:42.937 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 19:08:42.951 UTC [25] LOG:  database system was shut down at 2018-02-05 18:51:34 UTC
pg_1   | 2018-02-05 19:08:42.956 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 19:09:04.316 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 19:09:18.081 UTC [33] FATAL:  database ""user-auth"" does not exist
","<postgresql><doctor>, poster image great database, accord docs, specific name database great poster doctor image end war postgres_db. set doctor-compose file, created. here' rule section compose file: pg: image: postures:10 volumes: - do-data:/war/limb/postgresql/data environment: postgres_db: user-auto postgres_user: poster postgres_password: poster logs: attach userauth_pg_1 pg | 2018-02-05 18:05:54.803 etc [1] log: listen iv address ""0.0.0.0"", port 5432 pg | 2018-02-05 18:05:54.803 etc [1] log: listen iv address ""::"", port 5432 pg | 2018-02-05 18:05:54.806 etc [1] log: listen unit socket ""/war/run/postgresql/.s.pgsql.5432"" pg | 2018-02-05 18:05:54.817 etc [24] log: database system interrupted; last known 2018-02-05 18:03:26 etc pg | 2018-02-05 18:05:54.942 etc [24] log: database system properly shut down; automatic recovery progress pg | 2018-02-05 18:05:54.944 etc [24] log: red start 0/1633ed0 pg | 2018-02-05 18:05:54.944 etc [24] log: invalid record length 0/1633f08: want 24, got 0 pg | 2018-02-05 18:05:54.944 etc [24] log: red done 0/1633ed0 pg | 2018-02-05 18:05:54.955 etc [1] log: database system ready accept connect pg | 2018-02-05 18:05:59.140 etc [31] fatal: database ""user-auto"" exist pg | 2018-02-05 18:06:15.528 etc [32] fatal: database ""user-auto"" exist pg | 2018-02-05 18:08:46.120 etc [33] fatal: database ""user-auto"" exist pg | 2018-02-05 18:08:46.151 etc [34] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:02.138 etc [35] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:02.926 etc [36] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:04.244 etc [37] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:04.273 etc [38] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:04.602 etc [39] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:04.910 etc [40] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:05.777 etc [41] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:05.823 etc [42] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:05.878 etc [43] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:06.663 etc [44] fatal: database ""user-auto"" exist pg | 2018-02-05 18:14:06.716 etc [45] fatal: database ""user-auto"" exist pg | 2018-02-05 18:16:32.713 etc [46] fatal: database ""user-auto"" exist pg | 2018-02-05 18:47:04.603 etc [47] fatal: database ""user-auto"" exist pg | 2018-02-05 18:51:34.413 etc [1] log: receive smart shutdown request pg | 2018-02-05 18:51:34.417 etc [1] log: worker process: logic relic launched (did 30) exit exit code 1 pg | 2018-02-05 18:51:34.419 etc [25] log: shut pg | 2018-02-05 18:51:34.434 etc [1] log: database system shut pg | 2018-02-05 19:08:42.934 etc [1] log: listen iv address ""0.0.0.0"", port 5432 pg | 2018-02-05 19:08:42.934 etc [1] log: listen iv address ""::"", port 5432 pg | 2018-02-05 19:08:42.937 etc [1] log: listen unit socket ""/war/run/postgresql/.s.pgsql.5432"" pg | 2018-02-05 19:08:42.951 etc [25] log: database system shut 2018-02-05 18:51:34 etc pg | 2018-02-05 19:08:42.956 etc [1] log: database system ready accept connect pg | 2018-02-05 19:09:04.316 etc [32] fatal: database ""user-auto"" exist pg | 2018-02-05 19:09:18.081 etc [33] fatal: database ""user-auto"" exist"
51121889,Why do we need template0 and template1 in PostgreSQL?,"I'm a beginner in PostgreSQL. I wonder why the \l command in psql shows databases template0 and template1.
I searched the web but unfortunately didn't find the right resources. But I did find that after removing both (template0 &amp; template1) we can't create new databases any more.
",<database><postgresql>,285,0,5,543,1,4,7,45,23198,0.0,0,1,41,2018-07-01 9:05,2018-07-01 11:45,,0.0,,Basic,4,"<database><postgresql>, Why do we need template0 and template1 in PostgreSQL?, I'm a beginner in PostgreSQL. I wonder why the \l command in psql shows databases template0 and template1.
I searched the web but unfortunately didn't find the right resources. But I did find that after removing both (template0 &amp; template1) we can't create new databases any more.
","<database><postgresql>, need template0 template1 postgresql?, i'm begin postgresql. wonder \l command pool show database template0 template1. search web unfortun find right resources. find remove (template0 &amp; template1) can't great new database more."
51276703,how to store PostgreSQL jsonb using SpringBoot + JPA?,"I'm working on a migration software that will consume unknown data from REST services.
I already think about use MongoDB but I decide to not use it and use PostgreSQL.
After read this I'm trying to implement it in my SpringBoot app using Spring JPA but I don't know to map jsonb in my entity.
Tried this but understood nothing!
Here is where I am:
@Repository
@Transactional
public interface DnitRepository extends JpaRepository&lt;Dnit, Long&gt; {
    @Query(value = ""insert into dnit(id,data) VALUES (:id,:data)"", nativeQuery = true)
    void insertdata( @Param(""id"")Integer id,@Param(""data"") String data );
}
and ...
@RestController
public class TestController {
    @Autowired
    DnitRepository dnitRepository;  
    @RequestMapping(value = ""/dnit"", method = RequestMethod.GET)
    public String testBig() {
        dnitRepository.insertdata(2, someJsonDataAsString );
    }
}
and the table:
CREATE TABLE public.dnit
(
    id integer NOT NULL,
    data jsonb,
    CONSTRAINT dnit_pkey PRIMARY KEY (id)
)
How can I do this?
Note: I don't want/need an Entity to work on. My JSON will always be String but I need jsonb to query the DB
",<postgresql><spring-boot><spring-data-jpa><jsonb>,1137,2,27,1972,4,28,53,64,108223,0.0,407,5,41,2018-07-11 3:13,2018-07-11 13:28,2018-07-11 13:28,0.0,0.0,Basic,10,"<postgresql><spring-boot><spring-data-jpa><jsonb>, how to store PostgreSQL jsonb using SpringBoot + JPA?, I'm working on a migration software that will consume unknown data from REST services.
I already think about use MongoDB but I decide to not use it and use PostgreSQL.
After read this I'm trying to implement it in my SpringBoot app using Spring JPA but I don't know to map jsonb in my entity.
Tried this but understood nothing!
Here is where I am:
@Repository
@Transactional
public interface DnitRepository extends JpaRepository&lt;Dnit, Long&gt; {
    @Query(value = ""insert into dnit(id,data) VALUES (:id,:data)"", nativeQuery = true)
    void insertdata( @Param(""id"")Integer id,@Param(""data"") String data );
}
and ...
@RestController
public class TestController {
    @Autowired
    DnitRepository dnitRepository;  
    @RequestMapping(value = ""/dnit"", method = RequestMethod.GET)
    public String testBig() {
        dnitRepository.insertdata(2, someJsonDataAsString );
    }
}
and the table:
CREATE TABLE public.dnit
(
    id integer NOT NULL,
    data jsonb,
    CONSTRAINT dnit_pkey PRIMARY KEY (id)
)
How can I do this?
Note: I don't want/need an Entity to work on. My JSON will always be String but I need jsonb to query the DB
","<postgresql><spring-boot><spring-data-pa><son>, store postgresql son use springboot + pa?, i'm work migrate software consume unknown data rest services. already think use mongodb decide use use postgresql. read i'm try implement springboot pp use spring pa know map son entity. try understood nothing! am: @depositors @transact public interface dnitrepositori extend jparepository&it;knit, long&it; { @query(value = ""insert knit(id,data) value (:id,:data)"", nativequeri = true) void insertdata( @parma(""id"")inter id,@parma(""data"") string data ); } ... @restcontrol public class testcontrol { @autowir dnitrepositori dnitrepository; @requestmapping(value = ""/knit"", method = requestmethod.get) public string testing() { dnitrepository.insertdata(2, somejsondataasstr ); } } table: great table public.knit ( id inter null, data son, constraint dnit_pkey primary key (id) ) this? note: want/ne entity work on. son away string need son query do"
54885178,What's the difference between utf8_unicode_ci and utf8mb4_0900_ai_ci,"What is the difference between utf8mb4_0900_ai_ci and utf8_unicode_ci database text coding in mysql (especially in terms of performance) ?
Update:
There are similar differences between utf8mb4_unicode_ci and utf8mb4_0900_ai_ci?
",<mysql><unicode>,228,0,2,86767,31,374,349,77,34302,0.0,7499,1,41,2019-02-26 12:04,2019-03-06 16:16,2019-03-06 16:16,8.0,8.0,Intermediate,19,"<mysql><unicode>, What's the difference between utf8_unicode_ci and utf8mb4_0900_ai_ci, What is the difference between utf8mb4_0900_ai_ci and utf8_unicode_ci database text coding in mysql (especially in terms of performance) ?
Update:
There are similar differences between utf8mb4_unicode_ci and utf8mb4_0900_ai_ci?
","<myself><unicorn>, what' differ utf8_unicode_ci utf8mb4_0900_ai_ci, differ utf8mb4_0900_ai_ci utf8_unicode_ci database text code myself (respect term performance) ? update: similar differ utf8mb4_unicode_ci utf8mb4_0900_ai_ci?"
53149484,error: ALTER TYPE ... ADD cannot run inside a transaction block,"I am trying to add new type value to my existing types in PostgreSQL. But I get the following error
  error: ALTER TYPE ... ADD cannot run inside a transaction block
The query I used to add a new value to the type is 
ALTER TYPE public.request_type ADD VALUE ""Check"";
I am actually running above query in migrations file which is created using node-pg-migrate
Here public is my schema.
Any idea why this is failing?
Edit:
The below query executes fine when execute it in pgadmin
ALTER TYPE public.request_type ADD VALUE ""Check"";
But when I run above command through node-pg-migrate migrations it fails and throws above error
",<postgresql><enums><alter>,625,0,3,32938,39,121,164,69,32890,0.0,1300,7,41,2018-11-05 6:39,2018-11-05 8:18,,0.0,,Basic,10,"<postgresql><enums><alter>, error: ALTER TYPE ... ADD cannot run inside a transaction block, I am trying to add new type value to my existing types in PostgreSQL. But I get the following error
  error: ALTER TYPE ... ADD cannot run inside a transaction block
The query I used to add a new value to the type is 
ALTER TYPE public.request_type ADD VALUE ""Check"";
I am actually running above query in migrations file which is created using node-pg-migrate
Here public is my schema.
Any idea why this is failing?
Edit:
The below query executes fine when execute it in pgadmin
ALTER TYPE public.request_type ADD VALUE ""Check"";
But when I run above command through node-pg-migrate migrations it fails and throws above error
","<postgresql><enemy><alter>, error: alter type ... add cannot run inside transact block, try add new type value exist type postgresql. get follow error error: alter type ... add cannot run inside transact block query use add new value type alter type public.request_typ add value ""check""; actual run query migrate file great use node-pg-might public scheme. idea failing? edit: query execute fine execute pgadmin alter type public.request_typ add value ""check""; run command node-pg-might migrate fail throw error"
52789531,"How do I solve «panic: sql: unknown driver ""postgres"" (forgotten import?)»?","I'm trying to INSERT data into POSTGRES from a .csv (pre-fixed width / tabular ) with GO.
What I've done:
package main
import (
    ""bufio""
    ""database/sql""
    ""encoding/csv""
    ""encoding/json""
    ""fmt""
    ""io""
    ""log""
    ""os""
)
type Consumidor struct {
    CPF string   `json:""CPF""`
    Private  string   `json:""Private""`
    Incompleto  string   `json:""Incompleto""`
    Compras   *Compras `json:""Compras,omitempty""`
}
type Compras struct {
    DataUltimacompra  string `json:""DataUltimacompra""`
    TicketMedio string `json:""TicketMedio""`
    TicketUltimaCompra string `json:""TicketUltimaCompra""`
    LojaMaisFrequente string `json:""LojaMaisFrequente""`
    LojaUltimaCompra string `json:""LojaUltimaCompra""`
}
const (
    host     = ""localhost""
    port     = 5432
    user     = ""postgres""
    password = """"
    dbname   = ""neoway""
)
func main() {
    csvFile, _ := os.Open(""data.csv"")
    reader := csv.NewReader(bufio.NewReader(csvFile))
    var dadosinsert []Consumidor
    for {
        line, error := reader.Read()
        if error == io.EOF {
            break
        } else if error != nil {
            log.Fatal(error)
        }
        dadosinsert = append(dadosinsert, Consumidor{
            CPF: line[0],
            Private:  line[1],
            Incompleto: line[2],
            Compras: &amp;Compras{
                DataUltimacompra:  line[3],
                TicketMedio:  line[4],
                TicketUltimaCompra: line[5],
                LojaMaisFrequente:  line[6],
                LojaUltimaCompra: line[7],
            },
        })
    }
    peopleJson, _ := json.Marshal(dadosinsert)
    fmt.Println(string(peopleJson))
    psqlInfo := fmt.Sprintf(""host=%s port=%d user=%s ""+
        ""password=%s dbname=%s sslmode=disable"",
        host, port, user, password, dbname)
    db, err := sql.Open(""postgres"", psqlInfo)
    if err != nil {
        panic(err)
    }
    defer db.Close()
    sqlStatement := `
INSERT INTO base_teste (CPF,""PRIVATE"",""INCOMPLETO"",""DATA DA ÚLTIMA COMPRA"",""TICKET MÉDIO"",""TICKET DA ÚLTIMA COMPRA"",""LOJA MAIS FREQUÊNTE"",""LOJA DA ÚLTIMA COMPRA"")
)
VALUES ($1, $2, $3, $4, $5, $6, 7$, 8$)
RETURNING id`
    id := 0
    err = db.QueryRow(sqlStatement, 30, ""a"", ""b"", ""c"").Scan(&amp;id)
    if err != nil {
        panic(err)
    }
    fmt.Println(""New record ID is:"", id)
}
when I run, I get this error
  [{""CPF"":""xxxxx"",""Private"":""TRUE"",""Incompleto"":""FALSE"",""Compras"":{""DataUltimacompra"":""12/10/2018"",""TicketMedio"":""200"",""TicketUltimaCompra"":""250"",""LojaMaisFrequente"":""111.111.111-99"",""LojaUltimaCompra"":""111.111.111-88""}}]
  panic: sql: unknown driver ""postgres"" (forgotten import?)
  goroutine 1 [running]: main.main()    C:/Users/Willian/Desktop/NEOWAY
  PROJECT/neoway csv prefixed width importer/main.go:70 +0xbed
  Process finished with exit code 2
",<postgresql><go>,2813,0,85,411,1,4,3,65,50141,0.0,0,1,41,2018-10-13 4:29,2018-10-13 10:23,,0.0,,Basic,7,"<postgresql><go>, How do I solve «panic: sql: unknown driver ""postgres"" (forgotten import?)»?, I'm trying to INSERT data into POSTGRES from a .csv (pre-fixed width / tabular ) with GO.
What I've done:
package main
import (
    ""bufio""
    ""database/sql""
    ""encoding/csv""
    ""encoding/json""
    ""fmt""
    ""io""
    ""log""
    ""os""
)
type Consumidor struct {
    CPF string   `json:""CPF""`
    Private  string   `json:""Private""`
    Incompleto  string   `json:""Incompleto""`
    Compras   *Compras `json:""Compras,omitempty""`
}
type Compras struct {
    DataUltimacompra  string `json:""DataUltimacompra""`
    TicketMedio string `json:""TicketMedio""`
    TicketUltimaCompra string `json:""TicketUltimaCompra""`
    LojaMaisFrequente string `json:""LojaMaisFrequente""`
    LojaUltimaCompra string `json:""LojaUltimaCompra""`
}
const (
    host     = ""localhost""
    port     = 5432
    user     = ""postgres""
    password = """"
    dbname   = ""neoway""
)
func main() {
    csvFile, _ := os.Open(""data.csv"")
    reader := csv.NewReader(bufio.NewReader(csvFile))
    var dadosinsert []Consumidor
    for {
        line, error := reader.Read()
        if error == io.EOF {
            break
        } else if error != nil {
            log.Fatal(error)
        }
        dadosinsert = append(dadosinsert, Consumidor{
            CPF: line[0],
            Private:  line[1],
            Incompleto: line[2],
            Compras: &amp;Compras{
                DataUltimacompra:  line[3],
                TicketMedio:  line[4],
                TicketUltimaCompra: line[5],
                LojaMaisFrequente:  line[6],
                LojaUltimaCompra: line[7],
            },
        })
    }
    peopleJson, _ := json.Marshal(dadosinsert)
    fmt.Println(string(peopleJson))
    psqlInfo := fmt.Sprintf(""host=%s port=%d user=%s ""+
        ""password=%s dbname=%s sslmode=disable"",
        host, port, user, password, dbname)
    db, err := sql.Open(""postgres"", psqlInfo)
    if err != nil {
        panic(err)
    }
    defer db.Close()
    sqlStatement := `
INSERT INTO base_teste (CPF,""PRIVATE"",""INCOMPLETO"",""DATA DA ÚLTIMA COMPRA"",""TICKET MÉDIO"",""TICKET DA ÚLTIMA COMPRA"",""LOJA MAIS FREQUÊNTE"",""LOJA DA ÚLTIMA COMPRA"")
)
VALUES ($1, $2, $3, $4, $5, $6, 7$, 8$)
RETURNING id`
    id := 0
    err = db.QueryRow(sqlStatement, 30, ""a"", ""b"", ""c"").Scan(&amp;id)
    if err != nil {
        panic(err)
    }
    fmt.Println(""New record ID is:"", id)
}
when I run, I get this error
  [{""CPF"":""xxxxx"",""Private"":""TRUE"",""Incompleto"":""FALSE"",""Compras"":{""DataUltimacompra"":""12/10/2018"",""TicketMedio"":""200"",""TicketUltimaCompra"":""250"",""LojaMaisFrequente"":""111.111.111-99"",""LojaUltimaCompra"":""111.111.111-88""}}]
  panic: sql: unknown driver ""postgres"" (forgotten import?)
  goroutine 1 [running]: main.main()    C:/Users/Willian/Desktop/NEOWAY
  PROJECT/neoway csv prefixed width importer/main.go:70 +0xbed
  Process finished with exit code 2
","<postgresql><go>, sole «panic: sal: unknown driver ""postures"" (forgotten import?)»?, i'm try insert data poster .is (pre-fix width / tubular ) go. i'v done: package main import ( ""bubo"" ""database/sal"" ""encoding/is"" ""encoding/son"" ""fat"" ""to"" ""log"" ""os"" ) type consumidor struck { cf string `son:""cf""` privat string `son:""private""` incomplete string `son:""incomplete""` compare *compare `son:""compare,omitempty""` } type compare struck { dataultimacompra string `son:""dataultimacompra""` ticketmedio string `son:""ticketmedio""` ticketultimacompra string `son:""ticketultimacompra""` lojamaisfrequent string `son:""lojamaisfrequente""` lojaultimacompra string `son:""lojaultimacompra""` } cost ( host = ""localhost"" port = 5432 user = ""postures"" password = """" name = ""newly"" ) fun main() { csvfile, _ := os.open(""data.is"") reader := is.newreader(bubo.newreader(csvfile)) war dadosinsert []consumidor { line, error := reader.read() error == to.of { break } else error != nail { log.fatal(error) } dadosinsert = happened(dadosinsert, consumidor{ cf: line[0], private: line[1], incomplete: line[2], compare: &amp;compare{ dataultimacompra: line[3], ticketmedio: line[4], ticketultimacompra: line[5], lojamaisfrequente: line[6], lojaultimacompra: line[7], }, }) } peoplejson, _ := son.marshal(dadosinsert) fat.print(string(peoplejson)) psqlinfo := fat.spring(""host=% port=%d user=% ""+ ""password=% name=% sslmode=disabled"", host, port, user, password, name) do, err := sal.open(""postures"", psqlinfo) err != nail { panic(err) } defer do.close() sqlstatement := ` insert base_test (cf,""private"",""incomplete"",""data da intima compare"",""ticket media"",""ticket da intima compare"",""long may frequent"",""long da intima compare"") ) value ($1, $2, $3, $4, $5, $6, 7$, 8$) return id` id := 0 err = do.queryrow(sqlstatement, 30, ""a"", ""b"", ""c"").scan(&amp;id) err != nail { panic(err) } fat.print(""new record id is:"", id) } run, get error [{""cf"":""xxxix"",""private"":""true"",""incomplete"":""false"",""compare"":{""dataultimacompra"":""12/10/2018"",""ticketmedio"":""200"",""ticketultimacompra"":""250"",""lojamaisfrequente"":""111.111.111-99"",""lojaultimacompra"":""111.111.111-88""}}] panic: sal: unknown driver ""postures"" (forgotten import?) goroutin 1 [running]: main.main() c:/users/william/desktop/newly project/newly is prefix width imported/main.go:70 +bed process finish exit code 2"
48477861,"Laravel: String data, right truncated: 1406 Data too long for column","I have a table with a column 'hotel'. The project is created in Laravel 5.4, so I used Migrations.
$table-&gt;string('hotel', 50);
This is MYSQL VARCHAR (50). It was working good, because when I was developing I used short hotel names like &quot;HILTON NEW YORK 5&quot;*.
Now the project is on production and customer asked why they can't input long hotel names. I've tested it with such a mock hotel name as &quot;Long long long long long long long long long and very-very-very long hotel name 5 stars&quot;
It gave me an error:
&quot;SQLSTATE[22001]: String data, right truncated: 1406 Data too long for
column 'hotel' at row 1&quot;
I've opened database in my Sequel Pro and changed it
first to VARCHAR (255)
then to TEXT
After each change I tested it with the same &quot;Long long long long long long long long long and very-very-very long hotel name 5 starts&quot; and get the same error (see above).
I've checked the type of column with
SHOW FIELDS FROM table_name
and it gave me
Field | Type
hotel | text
so the type of the field is 'text' indeed  (65 535 characters).
Maybe it's somehow connected with Laravel Migration file (see above) where I set VARCHAR (50) in the beginning? But I can't re-run migration on production, because the table has data now.
Would appreciate any help.
UPDATE:
I discovered that it actually saves that long hotel name in the DB. But user still gets this annoying mistake every time after submitting the form...
",<php><mysql><laravel><text><types>,1449,0,2,1922,3,25,43,64,140378,0.0,207,12,41,2018-01-27 16:26,2018-01-27 16:29,2018-01-27 16:29,0.0,0.0,Basic,9,"<php><mysql><laravel><text><types>, Laravel: String data, right truncated: 1406 Data too long for column, I have a table with a column 'hotel'. The project is created in Laravel 5.4, so I used Migrations.
$table-&gt;string('hotel', 50);
This is MYSQL VARCHAR (50). It was working good, because when I was developing I used short hotel names like &quot;HILTON NEW YORK 5&quot;*.
Now the project is on production and customer asked why they can't input long hotel names. I've tested it with such a mock hotel name as &quot;Long long long long long long long long long and very-very-very long hotel name 5 stars&quot;
It gave me an error:
&quot;SQLSTATE[22001]: String data, right truncated: 1406 Data too long for
column 'hotel' at row 1&quot;
I've opened database in my Sequel Pro and changed it
first to VARCHAR (255)
then to TEXT
After each change I tested it with the same &quot;Long long long long long long long long long and very-very-very long hotel name 5 starts&quot; and get the same error (see above).
I've checked the type of column with
SHOW FIELDS FROM table_name
and it gave me
Field | Type
hotel | text
so the type of the field is 'text' indeed  (65 535 characters).
Maybe it's somehow connected with Laravel Migration file (see above) where I set VARCHAR (50) in the beginning? But I can't re-run migration on production, because the table has data now.
Would appreciate any help.
UPDATE:
I discovered that it actually saves that long hotel name in the DB. But user still gets this annoying mistake every time after submitting the form...
","<pp><myself><travel><text><types>, travel: string data, right truncated: 1406 data long column, table column 'hotel'. project great travel 5.4, use migrations. $table-&it;string('hotel', 50); myself varchar (50). work good, develop use short hotel name like &quit;hilton new york 5&quit;*. project product custom ask can't input long hotel names. i'v test mock hotel name &quit;long long long long long long long long long very-very-very long hotel name 5 stars&quit; gave error: &quit;sqlstate[22001]: string data, right truncated: 1406 data long column 'hotel' row 1&quit; i'v open database sequel pro change first varchar (255) text change test &quit;long long long long long long long long long very-very-very long hotel name 5 starts&quit; get error (see above). i'v check type column show field table_nam gave field | type hotel | text type field 'text' index (65 535 characters). may somehow connect travel migrate file (see above) set varchar (50) beginning? can't re-run migrate production, table data now. would appreci help. update: disco actual save long hotel name do. user still get annoy mistake every time submit form..."
51119248,Electron app with database,"I'm creating a web app for ticket reservation. The only problem is the database. I don't want to tell my client to install XAMPP or set a database, etc. 
Is there any way to package the app with the database? 
",<javascript><sql><node.js><database><electron>,210,0,0,720,2,7,9,44,39844,0.0,5,1,41,2018-06-30 23:18,2018-07-01 1:12,2018-07-01 1:12,1.0,1.0,Intermediate,20,"<javascript><sql><node.js><database><electron>, Electron app with database, I'm creating a web app for ticket reservation. The only problem is the database. I don't want to tell my client to install XAMPP or set a database, etc. 
Is there any way to package the app with the database? 
","<javascript><sal><node.is><database><electron>, electron pp database, i'm great web pp ticket reservation. problem database. want tell client instal camp set database, etc. way package pp database?"
54149272,Equivalent of ON CONFLICT DO NOTHING for UPDATE postgres,"I want to update rows in my postgres database if the updated version wouldn't violate the primary key constraint. If it would, I want to leave the row as it is.
Assuming the table has primary keys on col1, col2, col3, if I run a query like this:
UPDATE table SET (col1, col2) = ('A', 'B') 
      WHERE col1='D' AND col2='E';
The query will fail and I will get a duplicate key error if there exists two entries:
'A', 'B', 'C'
'D', 'E', 'C'
i.e col3 is the same between an existing row and a row to be updated.
If I was INSERTing rows I would use ON CONFLICT DO NOTHING but I can't find an implementation of this for UPDATE. Does an equivalent exist?
",<sql><postgresql><sql-update><subquery><sql-insert>,649,0,9,909,1,7,20,77,50575,0.0,130,2,41,2019-01-11 15:15,2019-01-11 16:14,2019-01-11 17:12,0.0,0.0,Basic,9,"<sql><postgresql><sql-update><subquery><sql-insert>, Equivalent of ON CONFLICT DO NOTHING for UPDATE postgres, I want to update rows in my postgres database if the updated version wouldn't violate the primary key constraint. If it would, I want to leave the row as it is.
Assuming the table has primary keys on col1, col2, col3, if I run a query like this:
UPDATE table SET (col1, col2) = ('A', 'B') 
      WHERE col1='D' AND col2='E';
The query will fail and I will get a duplicate key error if there exists two entries:
'A', 'B', 'C'
'D', 'E', 'C'
i.e col3 is the same between an existing row and a row to be updated.
If I was INSERTing rows I would use ON CONFLICT DO NOTHING but I can't find an implementation of this for UPDATE. Does an equivalent exist?
","<sal><postgresql><sal-update><subquery><sal-insert>, equal conflict not update postures, want update row poster database update version violet primary key constraint. would, want leave row is. assume table primary key cold, cold, cold, run query like this: update table set (cold, cold) = ('a', 'b') cold='d' cold='e'; query fail get public key error exist two entries: 'a', 'b', 'c' 'd', 'e', 'c' i.e cold exist row row updated. insert row would use conflict not can't find implement update. equal exist?"
53673763,"Azure Storage Emulator fails to init with ""The database 'AzureStorageEmulatorDb57' does not exist""","I am having an issue with Azure Storage Emulator. I tried to re-initialise the database and got the error below. 
This was after installing Visual Studio 2019 Preview but this may just be a co-incidence. I tried for an hour or so to get it running and then gave up and just  reset my machine with the ""keep my files"" option, re-installed Visual Studio 2017 and the Azure Tools but still see the same problem.  
I know a reset sounds a bit drastic but VS 2019 broke my Azure Functions in VS2017, they would not launch so I wanted a clean install. 
If I manually create the DB with sqllocaldb create (version 13.1.4001.0), the DB gets created fine but the init still fails with the same message.
Any ideas?
  C:\Program Files (x86)\Microsoft SDKs\Azure\Storage
  Emulator>AzureStorageEmulator.exe init
      Windows Azure Storage Emulator 5.7.0.0 command line tool
      Found SQL Instance (localdb)\MSSQLLocalDB.
      Creating database AzureStorageEmulatorDb57 on SQL instance '(localdb)\MSSQLLocalDB'.
      Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid database
  name. To see available databases, use sys.databases..
      One or more initialization actions have failed. Resolve these errors before attempting to run the storage emulator again.
      Error: Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid
  database name. To see available databases, use sys.databases..
",<azure-storage><sql-server-express><azure-storage-emulator>,1526,0,0,12970,7,60,87,54,15885,0.0,311,17,40,2018-12-07 16:51,2018-12-12 9:18,,5.0,,Basic,9,"<azure-storage><sql-server-express><azure-storage-emulator>, Azure Storage Emulator fails to init with ""The database 'AzureStorageEmulatorDb57' does not exist"", I am having an issue with Azure Storage Emulator. I tried to re-initialise the database and got the error below. 
This was after installing Visual Studio 2019 Preview but this may just be a co-incidence. I tried for an hour or so to get it running and then gave up and just  reset my machine with the ""keep my files"" option, re-installed Visual Studio 2017 and the Azure Tools but still see the same problem.  
I know a reset sounds a bit drastic but VS 2019 broke my Azure Functions in VS2017, they would not launch so I wanted a clean install. 
If I manually create the DB with sqllocaldb create (version 13.1.4001.0), the DB gets created fine but the init still fails with the same message.
Any ideas?
  C:\Program Files (x86)\Microsoft SDKs\Azure\Storage
  Emulator>AzureStorageEmulator.exe init
      Windows Azure Storage Emulator 5.7.0.0 command line tool
      Found SQL Instance (localdb)\MSSQLLocalDB.
      Creating database AzureStorageEmulatorDb57 on SQL instance '(localdb)\MSSQLLocalDB'.
      Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid database
  name. To see available databases, use sys.databases..
      One or more initialization actions have failed. Resolve these errors before attempting to run the storage emulator again.
      Error: Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid
  database name. To see available databases, use sys.databases..
","<azure-storage><sal-server-express><azure-storage-emulation>, azur storage soul fail knit ""the database 'azurestorageemulatordb57' exist"", issue azur storage emulation. try re-initials database got error below. instal visual studio 2019 review may co-incidence. try hour get run gave rest machine ""keep files"" option, re-instal visual studio 2017 azur tool still see problem. know rest sound bit drastic vs 2019 broke azur function vs2017, would launch want clean install. manual great do sqllocaldb great (version 13.1.4001.0), do get great fine knit still fail message. ideas? c:\program file (x)\microsoft sides\azure\storage emulation>azurestorageemulator.ex knit window azur storage soul 5.7.0.0 command line tool found sal instant (local)\mssqllocaldb. great database azurestorageemulatordb57 sal instant '(local)\mssqllocaldb'. cannot great database 'azurestorageemulatordb57' : database 'azurestorageemulatordb57' exist. supply valid database name. see avail database, use says.database.. one into action failed. resolve error attempt run storage soul again. error: cannot great database 'azurestorageemulatordb57' : database 'azurestorageemulatordb57' exist. supply valid database name. see avail database, use says.database.."
50372487,"Android Room database file is empty - .db, .db-shm, .db-wal","Using room in android for database. When I tried to see the data in sqlviewer then no tables found in database file
Myapp.db file is empty.
Data/data/packageName/databases/Myapp.db
",<android><sql><android-room>,181,0,0,1304,2,10,22,58,20934,0.0,40,5,40,2018-05-16 13:39,2018-05-28 13:56,2018-05-28 13:56,12.0,12.0,Basic,9,"<android><sql><android-room>, Android Room database file is empty - .db, .db-shm, .db-wal, Using room in android for database. When I tried to see the data in sqlviewer then no tables found in database file
Myapp.db file is empty.
Data/data/packageName/databases/Myapp.db
","<andros><sal><andros-room>, andros room database file empty - .do, .do-she, .do-was, use room andros database. try see data sqlviewer table found database file map.do file empty. data/data/packagename/database/map.do"
50014017,Why Presto is faster than Spark SQL,"Why is Presto faster than Spark SQL? 
Besides what is the difference between Presto and Spark SQL in computing architectures and memory management?
",<apache-spark-sql><presto>,148,0,0,1097,2,11,16,48,31727,0.0,10,3,40,2018-04-25 4:20,2018-04-26 17:41,2018-04-26 19:56,1.0,1.0,Intermediate,23,"<apache-spark-sql><presto>, Why Presto is faster than Spark SQL, Why is Presto faster than Spark SQL? 
Besides what is the difference between Presto and Spark SQL in computing architectures and memory management?
","<apache-spark-sal><preston>, preston faster spark sal, preston faster spark sal? beside differ preston spark sal compute architecture memory management?"
57595926,Could not import package. Warning SQL72012: The object exists in the target,"I exported my Azure database using Tasks > Export Data-tier Application in to a .bacpac file. Recently when I tried to import it into my local database server (Tasks > Import Data-tier Application), I encountered this error:
Could not import package.
Warning SQL72012: The object [MyDatabase_Data] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Warning SQL72012: The object [MyDatabase_Log] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Error SQL72014: .Net SqlClient Data Provider: Msg 12824, Level 16, State 1, Line 5 The sp_configure value 'contained database authentication' must be set to 1 in order to alter a contained database.  You may need to use RECONFIGURE to set the value_in_use.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
Error SQL72014: .Net SqlClient Data Provider: Msg 5069, Level 16, State 1, Line 5 ALTER DATABASE statement failed.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
 (Microsoft.SqlServer.Dac)
I followed the advice on other posts and tried to run this on SQL Azure database:
sp_configure 'contained database authentication', 1;  
GO  
RECONFIGURE;  
GO
However, it says 
Could not find stored procedure 'sp_configure'.
I understand the equivalent statement in Azure is: 
https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-database-scoped-configuration-transact-sql?view=sql-server-2017
What is the equivalent statement to ""sp_configure 'contained database authentication', 1;""?
",<sql-server><azure-sql-database>,2244,2,32,1741,1,15,27,40,25251,0.0,120,4,40,2019-08-21 16:34,2019-08-22 1:20,,1.0,,Basic,2,"<sql-server><azure-sql-database>, Could not import package. Warning SQL72012: The object exists in the target, I exported my Azure database using Tasks > Export Data-tier Application in to a .bacpac file. Recently when I tried to import it into my local database server (Tasks > Import Data-tier Application), I encountered this error:
Could not import package.
Warning SQL72012: The object [MyDatabase_Data] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Warning SQL72012: The object [MyDatabase_Log] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Error SQL72014: .Net SqlClient Data Provider: Msg 12824, Level 16, State 1, Line 5 The sp_configure value 'contained database authentication' must be set to 1 in order to alter a contained database.  You may need to use RECONFIGURE to set the value_in_use.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
Error SQL72014: .Net SqlClient Data Provider: Msg 5069, Level 16, State 1, Line 5 ALTER DATABASE statement failed.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
 (Microsoft.SqlServer.Dac)
I followed the advice on other posts and tried to run this on SQL Azure database:
sp_configure 'contained database authentication', 1;  
GO  
RECONFIGURE;  
GO
However, it says 
Could not find stored procedure 'sp_configure'.
I understand the equivalent statement in Azure is: 
https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-database-scoped-configuration-transact-sql?view=sql-server-2017
What is the equivalent statement to ""sp_configure 'contained database authentication', 1;""?
","<sal-server><azure-sal-database>, could import package. warn sql72012: object exist target, export azur database use task > export data-ti applied .balzac file. recent try import local database server (task > import data-ti application), count error: could import package. warn sql72012: object [mydatabase_data] exist target, drop even though select 'genet drop statement object target database source' check box. warn sql72012: object [mydatabase_log] exist target, drop even though select 'genet drop statement object target database source' check box. error sql72014: .net sqlclient data provider: mug 12824, level 16, state 1, line 5 sp_configur value 'contain database authentication' must set 1 order alter contain database. may need use reconfigur set value_in_use. error sql72045: script execute error. execute script: exist (select 1 [master].[do].[sysdatabases] [name] = n'$(databasename)') begin alter database [$(databasename)] set contain = partial rollback immediate; end error sql72014: .net sqlclient data provider: mug 5069, level 16, state 1, line 5 alter database statement failed. error sql72045: script execute error. execute script: exist (select 1 [master].[do].[sysdatabases] [name] = n'$(databasename)') begin alter database [$(databasename)] set contain = partial rollback immediate; end (microsoft.sqlserver.day) follow advice post try run sal azur database: sp_configur 'contain database authentication', 1; go reconfigure; go however, say could find store procedure 'sp_configure'. understand equal statement azur is: http://learn.microsoft.com/en-us/sal/t-sal/statements/alter-database-scope-configuration-transact-sal?view=sal-server-2017 equal statement ""sp_configur 'contain database authentication', 1;""?"
48184300,"When I run test cases I get this error: psycopg2.OperationalError: cursor ""_django_curs_140351416325888_23"" does not exist","I'm trying to run test cases, but I get below error.
Run command : python manage.py test
Type 'yes' if you would like to try deleting the test database 'test_project_management_db', or 'no' to cancel: yes
Destroying old test database for alias 'default'...
Traceback (most recent call last):
  File &quot;manage.py&quot;, line 24, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 363, in execute_from_command_line
    utility.execute()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 355, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 29, in run_from_argv
    super(Command, self).run_from_argv(argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 330, in execute
    output = self.handle(*args, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 62, in handle
    failures = test_runner.run_tests(test_labels)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 601, in run_tests
    old_config = self.setup_databases()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 546, in setup_databases
    self.parallel, **kwargs
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/utils.py&quot;, line 187, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 77, in create_test_db
    self.connection._test_serialized_contents = self.serialize_db_to_string()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 121, in serialize_db_to_string
    serializers.serialize(&quot;json&quot;, get_objects(), indent=None, stream=out)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/__init__.py&quot;, line 129, in serialize
    s.serialize(queryset, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/base.py&quot;, line 80, in serialize
    for count, obj in enumerate(queryset, start=1):
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 117, in get_objects
    for obj in queryset.iterator():
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 53, in __iter__
    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py&quot;, line 880, in execute_sql
    cursor.close()
psycopg2.OperationalError: cursor &quot;_django_curs_140351416325888_23&quot; does not exist
",<python><django><postgresql><django-tests>,3900,0,40,513,1,4,7,40,16924,0.0,4,4,40,2018-01-10 9:37,2018-03-19 15:18,,68.0,,Basic,13,"<python><django><postgresql><django-tests>, When I run test cases I get this error: psycopg2.OperationalError: cursor ""_django_curs_140351416325888_23"" does not exist, I'm trying to run test cases, but I get below error.
Run command : python manage.py test
Type 'yes' if you would like to try deleting the test database 'test_project_management_db', or 'no' to cancel: yes
Destroying old test database for alias 'default'...
Traceback (most recent call last):
  File &quot;manage.py&quot;, line 24, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 363, in execute_from_command_line
    utility.execute()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 355, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 29, in run_from_argv
    super(Command, self).run_from_argv(argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 330, in execute
    output = self.handle(*args, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 62, in handle
    failures = test_runner.run_tests(test_labels)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 601, in run_tests
    old_config = self.setup_databases()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 546, in setup_databases
    self.parallel, **kwargs
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/utils.py&quot;, line 187, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 77, in create_test_db
    self.connection._test_serialized_contents = self.serialize_db_to_string()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 121, in serialize_db_to_string
    serializers.serialize(&quot;json&quot;, get_objects(), indent=None, stream=out)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/__init__.py&quot;, line 129, in serialize
    s.serialize(queryset, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/base.py&quot;, line 80, in serialize
    for count, obj in enumerate(queryset, start=1):
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 117, in get_objects
    for obj in queryset.iterator():
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 53, in __iter__
    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py&quot;, line 880, in execute_sql
    cursor.close()
psycopg2.OperationalError: cursor &quot;_django_curs_140351416325888_23&quot; does not exist
","<patron><django><postgresql><django-tests>, run test case get error: psycopg2.operationalerror: curses ""_django_curs_140351416325888_23"" exist, i'm try run test cases, get error. run command : patron manage.i test type 'yes' would like try delete test database 'test_project_management_db', 'no' cancel: ye destroy old test database asia 'default'... traceback (most recent call last): file &quit;manage.by&quit;, line 24, &it;module&it; execute_from_command_line(says.are) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/__init__.by&quit;, line 363, execute_from_command_lin utility.execute() file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/__init__.by&quit;, line 355, execute self.fetch_command(subcommand).run_from_argv(self.are) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/commands/test.by&quit;, line 29, run_from_argv super(command, self).run_from_argv(are) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/base.by&quit;, line 283, run_from_argv self.execute(*arms, **cmd_options) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/base.by&quit;, line 330, execute output = self.handle(*arms, **option) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/management/commands/test.by&quit;, line 62, hand failure = test_runner.run_tests(test_labels) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/test/runner.by&quit;, line 601, run_test old_config = self.setup_databases() file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/test/runner.by&quit;, line 546, setup_databas self.parallel, **war file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/test/still.by&quit;, line 187, setup_databas serialize=connection.settings_dict.get('test', {}).get('serialize', true), file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/do/backed/base/creation.by&quit;, line 77, create_test_db self.connection._test_serialized_cont = self.serialize_db_to_string() file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/do/backed/base/creation.by&quit;, line 121, serialize_db_to_str serializers.serialize(&quit;son&quit;, get_objects(), intent=none, stream=out) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/serializers/__init__.by&quit;, line 129, aerial s.serialize(queryset, **option) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/core/serializers/base.by&quit;, line 80, aerial count, obs enumerate(queryset, start=1): file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/do/backed/base/creation.by&quit;, line 117, get_object obs queryset.operator(): file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/do/models/query.by&quit;, line 53, __iter__ result = compilers.execute_sql(chunked_fetch=self.chunked_fetch) file &quit;/home/rails/desktop/projects/ends/project_manage_env/local/limb/python2.7/site-packages/django/do/models/sal/compilers.by&quit;, line 880, execute_sql curses.close() psycopg2.operationalerror: curses &quit;_django_curs_140351416325888_23&quit; exist"
50603953,How to add 'created_at' and 'updated_at' columns?,"I need to add 'updated_at' and 'created_at' columns to some already existing table in MySQL database. I've added those colums using MySQL Workbench, but what query should I use to make them work properly? Thanks in advance ;)
",<mysql><sql><database>,226,0,0,471,1,6,12,73,108844,0.0,19,2,40,2018-05-30 11:50,2018-05-30 11:57,2018-05-30 11:57,0.0,0.0,Basic,9,"<mysql><sql><database>, How to add 'created_at' and 'updated_at' columns?, I need to add 'updated_at' and 'created_at' columns to some already existing table in MySQL database. I've added those colums using MySQL Workbench, but what query should I use to make them work properly? Thanks in advance ;)
","<myself><sal><database>, add 'created_at' 'updated_at' columns?, need add 'updated_at' 'created_at' column already exist table myself database. i'v ad column use myself workbench, query use make work properly? thank advance ;)"
49796452,WampServer - mysqld.exe can't start because MSVCR120.dll is missing,"I've tried to run wampserver on my local side, but mysql server doesn't run. 
when I try to install service, it give me error. I searched the answer all day and found some answers on here and there.
but any solution doesn't work for me. I tried to install warpserver on windows7 home OS vmware
Any help for me?
",<mysql><wordpress><windows-7><vmware><wampserver>,311,0,0,943,1,8,19,53,147725,0.0,9,9,39,2018-04-12 12:23,2018-04-12 12:27,2018-04-12 15:11,0.0,0.0,Basic,14,"<mysql><wordpress><windows-7><vmware><wampserver>, WampServer - mysqld.exe can't start because MSVCR120.dll is missing, I've tried to run wampserver on my local side, but mysql server doesn't run. 
when I try to install service, it give me error. I searched the answer all day and found some answers on here and there.
but any solution doesn't work for me. I tried to install warpserver on windows7 home OS vmware
Any help for me?
","<myself><wordpress><windows-7><aware><wampserver>, wampserv - myself.ex can't start msvcr120.all missing, i'v try run wampserv local side, myself server run. try instal service, give error. search answer day found answer there. slut work me. try instal warpserv windows home os aware help me?"
53735305,How to rename a column name in maria DB,"I am new to SQL, I was trying to change column name in my database's table. I am using 'xampp' with 'maria DB' (OS - Ubuntu 18.04)  
I tried all of the followings:  
ALTER TABLE subject RENAME COLUMN course_number TO course_id;
ALTER TABLE subject CHANGE course_number course_id;
ALTER TABLE subject CHANGE 'course_number' 'course_id';
ALTER TABLE subject  CHANGE COLUMN 'course_number'  course_id varchar(255);
ALTER TABLE subject CHANGE 'course_number' 'course_id' varchar(255);
But the only output I got was:  
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'column course_number to course_id' at line 1  
Could someone please tell me what is the correct answer. I have no idea what to do further.
",<mysql><mariadb><rename><alter>,812,0,5,515,1,4,10,66,47244,0.0,59,4,39,2018-12-12 2:40,2018-12-16 4:32,2018-12-16 4:32,4.0,4.0,Basic,10,"<mysql><mariadb><rename><alter>, How to rename a column name in maria DB, I am new to SQL, I was trying to change column name in my database's table. I am using 'xampp' with 'maria DB' (OS - Ubuntu 18.04)  
I tried all of the followings:  
ALTER TABLE subject RENAME COLUMN course_number TO course_id;
ALTER TABLE subject CHANGE course_number course_id;
ALTER TABLE subject CHANGE 'course_number' 'course_id';
ALTER TABLE subject  CHANGE COLUMN 'course_number'  course_id varchar(255);
ALTER TABLE subject CHANGE 'course_number' 'course_id' varchar(255);
But the only output I got was:  
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'column course_number to course_id' at line 1  
Could someone please tell me what is the correct answer. I have no idea what to do further.
","<myself><maria><renamed><alter>, renal column name maria do, new sal, try change column name database' table. use 'camp' 'maria do' (o - bunt 18.04) try following: alter table subject renal column course_numb course_id; alter table subject change course_numb course_id; alter table subject change 'course_number' 'course_id'; alter table subject change column 'course_number' course_id varchar(255); alter table subject change 'course_number' 'course_id' varchar(255); output got was: error 1064 (42000): error sal santa; check manual correspond maria server version right santa use near 'column course_numb course_id' line 1 could someone pleas tell correct answer. idea further."
51014647,"AWS Postgres DB ""does not exist"" when connecting with PG","I can't seem to connect to my DB instance in AWS. I'm using the pg package and following the examples from the website is not working.
A search for ""aws postgres database does not exist"" really isn't returning anything helpful. Going through the open/closed issues on the PG github isnt helpful either.
Running $nc &lt;RDS endpoint&gt; &lt;port number&gt; returns a success message so it's definitely there. Every value placed in the Client config is copy/pasted from my DB instance.
I'm starting to wonder if the databases have a different name than what it shows in the ""Instances"" section of RDS on AWS?
const client = new Client({
  host     : '&lt;&lt;RDS ENDPOINT&gt;&gt;',
  database : '&lt;&lt;RDS NAME&gt;&gt;', // maybe this isnt the real name?
  user     : '&lt;&lt;username&gt;&gt;',
  password : '&lt;&lt;password&gt;&gt;',
  port     : &lt;&lt;port&gt;&gt;
});
client.connect()
  .then(data =&gt; {
    console.log('connected');
  })
  .catch(err =&gt; {
    console.log(err);
  })
",<postgresql><amazon-web-services><amazon-rds>,996,1,16,1564,1,21,37,44,16680,0.0,838,7,39,2018-06-24 23:03,2020-01-19 2:19,,574.0,,Basic,10,"<postgresql><amazon-web-services><amazon-rds>, AWS Postgres DB ""does not exist"" when connecting with PG, I can't seem to connect to my DB instance in AWS. I'm using the pg package and following the examples from the website is not working.
A search for ""aws postgres database does not exist"" really isn't returning anything helpful. Going through the open/closed issues on the PG github isnt helpful either.
Running $nc &lt;RDS endpoint&gt; &lt;port number&gt; returns a success message so it's definitely there. Every value placed in the Client config is copy/pasted from my DB instance.
I'm starting to wonder if the databases have a different name than what it shows in the ""Instances"" section of RDS on AWS?
const client = new Client({
  host     : '&lt;&lt;RDS ENDPOINT&gt;&gt;',
  database : '&lt;&lt;RDS NAME&gt;&gt;', // maybe this isnt the real name?
  user     : '&lt;&lt;username&gt;&gt;',
  password : '&lt;&lt;password&gt;&gt;',
  port     : &lt;&lt;port&gt;&gt;
});
client.connect()
  .then(data =&gt; {
    console.log('connected');
  })
  .catch(err =&gt; {
    console.log(err);
  })
","<postgresql><amazon-web-services><amazon-rd>, a poster do ""doe exist"" connect pg, can't seem connect do instant was. i'm use pg package follow example west working. search ""a poster database exist"" really return any helpful. go open/close issue pg github isn help either. run $no &it;rd endpoint&it; &it;port number&it; return success message definite there. every value place client confirm copy/past do instance. i'm start wonder database differ name show ""instances"" section rd was? cost client = new client({ host : '&it;&it;rd endpoint&it;&it;', database : '&it;&it;rd name&it;&it;', // may isn real name? user : '&it;&it;surname&it;&it;', password : '&it;&it;password&it;&it;', port : &it;&it;port&it;&it; }); client.connect() .then(data =&it; { console.log('connected'); }) .catch(err =&it; { console.log(err); })"
48102295,Rename column only if exists,"PostgreSQL does not allow 
ALTER TABLE t RENAME COLUMN IF EXISTS c1 TO c2
...or anything like that.  However, it's very convenient to be able to write scripts which modify DB structure which can be run again without first checking if it has already been run.
How do I write a PostgreSQL function to do exactly this?
",<postgresql><ddl><alter-table>,316,0,1,765,1,6,15,55,42429,0.0,63,5,39,2018-01-04 19:33,2018-01-06 17:52,2019-11-20 12:13,2.0,685.0,Basic,9,"<postgresql><ddl><alter-table>, Rename column only if exists, PostgreSQL does not allow 
ALTER TABLE t RENAME COLUMN IF EXISTS c1 TO c2
...or anything like that.  However, it's very convenient to be able to write scripts which modify DB structure which can be run again without first checking if it has already been run.
How do I write a PostgreSQL function to do exactly this?
","<postgresql><del><alter-table>, renal column exists, postgresql allow alter table renal column exist c c ...or any like that. however, convent all write script modify do structure run without first check already run. write postgresql function exactly this?"
48128714,How to make an Inner Join in django?,"I want to show in an Html the name of the city, state, and country of a publication. But they are in different tables.
Here is my models.py
class country(models.Model):
    country_name = models.CharField(max_length=200, null=True)
    country_subdomain = models.CharField(max_length=3, null=True)
    def __str__(self):
        return self.country_name
class countrystate(models.Model):
    state_name = models.CharField(max_length=200, null=True)
    country = models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    importance = models.IntegerField(null=True)
    def __str__(self):
        return self.state_name
class city(models.Model):
    city_name = models.CharField(max_length=200, null=True)
    countrystate = models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.city_name
class publication(models.Model):
    user = ForeignKey(users, on_delete=models.CASCADE, null=False)
    title= models.CharField(max_length=300, null=True)
    country=models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    countrystate=models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    city=models.ForeignKey(city, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.title
Here is my views.py
def publications(request):
    mypublications = publication.objects.filter(user_id=request.session['account_id'])
    dic.update({""plist"": mypublications })
    return render(request, 'blog/mypublications.html', dic)
In a django view, what is the equivalent of the next sql query?
SELECT p.user_id, p.title, c.cuntry_id, c.country_name, s.state_id, s.state_name, y.city_id, y.city_name FROM publication AS p
INNER JOIN country AS c ON c.id = p.country_id
INNER JOIN countrystate AS s ON s.id = p.countrystate_id
INNER JOIN city AS y ON y.id = p.city_id
",<python><mysql><django><orm><inner-join>,1865,0,36,1341,8,34,57,64,95825,0.0,61,5,39,2018-01-06 15:17,2018-01-06 15:29,2018-01-06 15:29,0.0,0.0,Basic,10,"<python><mysql><django><orm><inner-join>, How to make an Inner Join in django?, I want to show in an Html the name of the city, state, and country of a publication. But they are in different tables.
Here is my models.py
class country(models.Model):
    country_name = models.CharField(max_length=200, null=True)
    country_subdomain = models.CharField(max_length=3, null=True)
    def __str__(self):
        return self.country_name
class countrystate(models.Model):
    state_name = models.CharField(max_length=200, null=True)
    country = models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    importance = models.IntegerField(null=True)
    def __str__(self):
        return self.state_name
class city(models.Model):
    city_name = models.CharField(max_length=200, null=True)
    countrystate = models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.city_name
class publication(models.Model):
    user = ForeignKey(users, on_delete=models.CASCADE, null=False)
    title= models.CharField(max_length=300, null=True)
    country=models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    countrystate=models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    city=models.ForeignKey(city, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.title
Here is my views.py
def publications(request):
    mypublications = publication.objects.filter(user_id=request.session['account_id'])
    dic.update({""plist"": mypublications })
    return render(request, 'blog/mypublications.html', dic)
In a django view, what is the equivalent of the next sql query?
SELECT p.user_id, p.title, c.cuntry_id, c.country_name, s.state_id, s.state_name, y.city_id, y.city_name FROM publication AS p
INNER JOIN country AS c ON c.id = p.country_id
INNER JOIN countrystate AS s ON s.id = p.countrystate_id
INNER JOIN city AS y ON y.id = p.city_id
","<patron><myself><django><or><inner-join>, make inner join django?, want show html name city, state, country publication. differ tables. models.i class country(models.model): country_nam = models.garfield(max_length=200, null=true) country_subdomain = models.garfield(max_length=3, null=true) def __str__(self): return self.country_nam class countrystate(models.model): state_nam = models.garfield(max_length=200, null=true) country = models.foreigner(country, on_delete=models.cascade, null=true) import = models.integerfield(null=true) def __str__(self): return self.state_nam class city(models.model): city_nam = models.garfield(max_length=200, null=true) country = models.foreigner(countrystate, on_delete=models.cascade, null=true) def __str__(self): return self.city_nam class publication(models.model): user = foreigner(users, on_delete=models.cascade, null=false) title= models.garfield(max_length=300, null=true) country=models.foreigner(country, on_delete=models.cascade, null=true) countrystate=models.foreigner(countrystate, on_delete=models.cascade, null=true) city=models.foreigner(city, on_delete=models.cascade, null=true) def __str__(self): return self.till views.i def publications(request): mypubl = publication.objects.filter(user_id=request.session['accounted']) did.update({""list"": mypubl }) return render(request, 'blow/publications.html', did) django view, equal next sal query? select p.user_id, p.title, c.cuntry_id, c.country_name, s.stated, s.state_name, y.city_id, y.city_nam public p inner join country c c.id = p.countryside inner join country s.id = p.countrystate_id inner join city y.id = p.city_id"
60409585,How to upgrade postgresql database from 10 to 12 without losing data for openproject,"My OpenProject management software is installed with default postgresql 10.
Currently the postgresql DB is 12, It is having lot of new features.
I want to upgrade my Postgres DB without losing the data in the DB.
My system is ubuntu 18.04 and hosted  openproject.
I searched the internet and could not find a step by step to upgrade postgresql.
Can you please guide me to install new DB and all data should be in the new DB.
thanks for your help.
",<postgresql><ubuntu-18.04><openproject>,447,0,0,859,3,14,22,45,73377,0.0,13,4,39,2020-02-26 8:22,2020-04-14 13:11,2020-06-04 15:55,48.0,99.0,Basic,14,"<postgresql><ubuntu-18.04><openproject>, How to upgrade postgresql database from 10 to 12 without losing data for openproject, My OpenProject management software is installed with default postgresql 10.
Currently the postgresql DB is 12, It is having lot of new features.
I want to upgrade my Postgres DB without losing the data in the DB.
My system is ubuntu 18.04 and hosted  openproject.
I searched the internet and could not find a step by step to upgrade postgresql.
Can you please guide me to install new DB and all data should be in the new DB.
thanks for your help.
","<postgresql><bunt-18.04><openproject>, upgrade postgresql database 10 12 without lose data openproject, openproject manage software instal default postgresql 10. current postgresql do 12, lot new features. want upgrade poster do without lose data do. system bunt 18.04 host openproject. search internet could find step step upgrade postgresql. pleas guide instal new do data new do. thank help."
49776619,sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string,"I'm learning flask web microframework and after initialization of my database I run flask db init I run flask db migrate, to migrate my models classes to the database and i got an error.  I work on Windows 10, the database is MySQL, and extensions install are flask-migrate, flask-sqlalchemy, flask-login.
(env) λ flask db migrate
Traceback (most recent call last):
  File ""c:\python36\Lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python36\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\aka\Dev\dream-team\env\Scripts\flask.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 513, in main
    cli.main(args=args, prog_name=name)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 380, in main
    return AppGroup.main(self, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 257, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\cli.py"", line 90, in migrate
    rev_id, x_arg)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\__init__.py"", line 197, in migrate
    version_path=version_path, rev_id=rev_id)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\command.py"", line 176, in revision
    script_directory.run_env()
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\script\base.py"", line 427, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\pyfiles.py"", line 81, in load_python_file
    module = load_module_py(module_id, path)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\compat.py"", line 83, in load_module_py
    spec.loader.exec_module(module)
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""migrations\env.py"", line 87, in &lt;module&gt;
    run_migrations_online()
  File ""migrations\env.py"", line 70, in run_migrations_online
    poolclass=pool.NullPool)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 465, in engine_from_config
    return create_engine(url, **options)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 424, in create_engine
    return strategy.create(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\strategies.py"", line 50, in create
    u = url.make_url(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 211, in make_url
    return _parse_rfc1738_args(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 270, in _parse_rfc1738_args
    ""Could not parse rfc1738 URL from string '%s'"" % name)
sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string 'mysql/dt_admin:dt2016@localhost/dreamteam_db'
",<python><flask><flask-sqlalchemy><flask-login><flask-migrate>,4154,0,61,473,1,4,8,65,109734,0.0,2,6,38,2018-04-11 13:36,2018-04-12 15:46,,1.0,,Basic,14,"<python><flask><flask-sqlalchemy><flask-login><flask-migrate>, sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string, I'm learning flask web microframework and after initialization of my database I run flask db init I run flask db migrate, to migrate my models classes to the database and i got an error.  I work on Windows 10, the database is MySQL, and extensions install are flask-migrate, flask-sqlalchemy, flask-login.
(env) λ flask db migrate
Traceback (most recent call last):
  File ""c:\python36\Lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python36\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\aka\Dev\dream-team\env\Scripts\flask.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 513, in main
    cli.main(args=args, prog_name=name)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 380, in main
    return AppGroup.main(self, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 257, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\cli.py"", line 90, in migrate
    rev_id, x_arg)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\__init__.py"", line 197, in migrate
    version_path=version_path, rev_id=rev_id)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\command.py"", line 176, in revision
    script_directory.run_env()
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\script\base.py"", line 427, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\pyfiles.py"", line 81, in load_python_file
    module = load_module_py(module_id, path)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\compat.py"", line 83, in load_module_py
    spec.loader.exec_module(module)
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""migrations\env.py"", line 87, in &lt;module&gt;
    run_migrations_online()
  File ""migrations\env.py"", line 70, in run_migrations_online
    poolclass=pool.NullPool)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 465, in engine_from_config
    return create_engine(url, **options)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 424, in create_engine
    return strategy.create(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\strategies.py"", line 50, in create
    u = url.make_url(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 211, in make_url
    return _parse_rfc1738_args(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 270, in _parse_rfc1738_args
    ""Could not parse rfc1738 URL from string '%s'"" % name)
sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string 'mysql/dt_admin:dt2016@localhost/dreamteam_db'
","<patron><flask><flask-sqlalchemy><flask-login><flask-migrate>, sqlalchemy.etc.argumenterror: could part rfc1738 curl string, i'm learn flask web microframework into database run flask do knit run flask do migrate, migrate model class database got error. work window 10, database myself, extent instal flask-migrate, flask-sqlalchemy, flask-login. (end) λ flask do migrate traceback (most recent call last): file ""c:\python36\limb\run.by"", line 193, _run_module_as_main ""__main__"", mod_spec) file ""c:\python36\limb\run.by"", line 85, _run_cod even(code, run_globals) file ""c:\users\ak\de\dream-team\end\script\flask.eye\__main__.by"", line 9, &it;module&it; file ""c:\users\ak\de\dream-team\end\limb\site-packages\flask\coli.by"", line 513, main coli.main(arms=arms, prog_name=name) file ""c:\users\ak\de\dream-team\end\limb\site-packages\flask\coli.by"", line 380, main return appgroup.main(self, *arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 697, main re = self.invoke(cox) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 1066, invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 1066, invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 895, invoke return cox.invoke(self.callback, **cox.parts) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 535, invoke return callback(*arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\decorations.by"", line 17, new_func return f(get_current_context(), *arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\flask\coli.by"", line 257, door return __ctx.invoke(f, *arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\click\core.by"", line 535, invoke return callback(*arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\flask_migrate\coli.by"", line 90, migrate devoid, x_arg) file ""c:\users\ak\de\dream-team\end\limb\site-packages\flask_migrate\__init__.by"", line 197, migrate version_path=version_path, devoid=devoid) file ""c:\users\ak\de\dream-team\end\limb\site-packages\alembic\command.by"", line 176, revise script_directory.run_env() file ""c:\users\ak\de\dream-team\end\limb\site-packages\alembic\script\base.by"", line 427, run_env until.load_python_file(self.did, 'end.by') file ""c:\users\ak\de\dream-team\end\limb\site-packages\alembic\until\files.by"", line 81, load_python_fil model = load_module_py(module_id, path) file ""c:\users\ak\de\dream-team\end\limb\site-packages\alembic\until\compact.by"", line 83, load_module_pi speck.leader.exec_module(module) file ""&it;frozen importlib._bootstrap_external&it;"", line 678, exec_modul file ""&it;frozen importlib._bootstrap&it;"", line 219, _call_with_frames_remov file ""migrations\end.by"", line 87, &it;module&it; run_migrations_online() file ""migrations\end.by"", line 70, run_migrations_onlin poolclass=pool.millpool) file ""c:\users\ak\de\dream-team\end\limb\site-packages\sqlalchemy\engine\__init__.by"", line 465, engine_from_config return create_engine(curl, **option) file ""c:\users\ak\de\dream-team\end\limb\site-packages\sqlalchemy\engine\__init__.by"", line 424, create_engin return strategy.create(*arms, **wars) file ""c:\users\ak\de\dream-team\end\limb\site-packages\sqlalchemy\engine\strategics.by"", line 50, great u = curl.make_url(name_or_url) file ""c:\users\ak\de\dream-team\end\limb\site-packages\sqlalchemy\engine\curl.by"", line 211, make_url return _parse_rfc1738_args(name_or_url) file ""c:\users\ak\de\dream-team\end\limb\site-packages\sqlalchemy\engine\curl.by"", line 270, _parse_rfc1738_arg ""could part rfc1738 curl string '%s'"" % name) sqlalchemy.etc.argumenterror: could part rfc1738 curl string 'myself/dt_admin:dt2016@localhost/dreamteam_db'"
50166869,Connect to SQL Server in local machine (host) from docker using host.docker.internal,"I'm trying to connect to my SQL Server instance running in my local computer using host.docker.internal (as recommended in https://docs.docker.com/docker-for-windows/networking/#use-cases-and-workarounds)
The host.docker.internal is successfully resolved to an IP, and it's ping-able
And I've opened up the port 1433 in my firewall configuration
Error message
  Connection refused 192.168.65.2:1433
My connection string
  Data Source=host.docker.internal,1433;Initial Catalog=;Persist Security Info=False;User ID=;Password=;MultipleActiveResultSets=True;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
docker version
Client:
 Version:      18.03.1-ce
 API version:  1.37
 Go version:   go1.9.5
 Git commit:   9ee9f40
 Built:        Thu Apr 26 07:12:48 2018
 OS/Arch:      windows/amd64
 Experimental: false
 Orchestrator: swarm
Server:
 Engine:
  Version:      18.03.1-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.5
  Git commit:   9ee9f40
  Built:        Thu Apr 26 07:22:38 2018
  OS/Arch:      linux/amd64
  Experimental: true
Docker for windows version
",<sql-server><docker><docker-networking><docker-for-windows>,1098,3,21,4765,1,18,31,71,27103,0.0,463,1,38,2018-05-04 3:49,2018-05-04 8:11,2018-05-04 8:11,0.0,0.0,Basic,14,"<sql-server><docker><docker-networking><docker-for-windows>, Connect to SQL Server in local machine (host) from docker using host.docker.internal, I'm trying to connect to my SQL Server instance running in my local computer using host.docker.internal (as recommended in https://docs.docker.com/docker-for-windows/networking/#use-cases-and-workarounds)
The host.docker.internal is successfully resolved to an IP, and it's ping-able
And I've opened up the port 1433 in my firewall configuration
Error message
  Connection refused 192.168.65.2:1433
My connection string
  Data Source=host.docker.internal,1433;Initial Catalog=;Persist Security Info=False;User ID=;Password=;MultipleActiveResultSets=True;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
docker version
Client:
 Version:      18.03.1-ce
 API version:  1.37
 Go version:   go1.9.5
 Git commit:   9ee9f40
 Built:        Thu Apr 26 07:12:48 2018
 OS/Arch:      windows/amd64
 Experimental: false
 Orchestrator: swarm
Server:
 Engine:
  Version:      18.03.1-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.5
  Git commit:   9ee9f40
  Built:        Thu Apr 26 07:22:38 2018
  OS/Arch:      linux/amd64
  Experimental: true
Docker for windows version
","<sal-server><doctor><doctor-networking><doctor-for-windows>, connect sal server local machine (host) doctor use host.doctor.internal, i'm try connect sal server instant run local compute use host.doctor.inter (a recommend http://docs.doctor.com/doctor-for-windows/networking/#use-cases-and-workarounds) host.doctor.inter success resolve in, king-all i'v open port 1433 firewal configur error message connect refuse 192.168.65.2:1433 connect string data source=host.doctor.internal,1433;into catalogue=;persist secure into=false;us id=;password=;multipleactiveresultsets=true;encrypt=true;trustservercertificate=false;connect timeout=30; doctor version client: version: 18.03.1-ce apt version: 1.37 go version: go.9.5 git commit: 9ee9f40 built: the air 26 07:12:48 2018 os/arch: windows/amd64 experimental: fall orchestrator: swarm server: engine: version: 18.03.1-ce apt version: 1.37 (minimum version 1.12) go version: go.9.5 git commit: 9ee9f40 built: the air 26 07:22:38 2018 os/arch: line/amd64 experimental: true doctor window version"
51292905,"Flask app with ArcGIS, Arcpy does not run","I have a script that gets a table from MSSQL database and then registers it with ArcGIS. It uses several other arcpy methods as well. I tried to combine it with Flask and developed an HTML interface where you can specify tables. The script runs on console perfectly well, however, when running with Flask on http://127.0.0.1:5000/ , the arcpy functions do not run, then the app throws errors.
I am using my local python directory, so I do not have any problem with importing arcpy on flask. So, I am able to use pymssql functions and create a new table, however when it comes to arcpy function, It throws does not exist error, however, the table exists. I feel like there is something wrong with running arcpy with Flask, but any help would be appreciated.
(2) I tried the same thing in Django but I am having the same problem.
Thanks
forms.py
class createGISLayer(FlaskForm):
    tCreateLayer = SubmitField('Create GIS Layer')
DashboardMain()
   try:
        cursor.execute(QueryCreate)
        print (""Table Created."")
        print(self.dbTablePath)
        descTable = arcpy.Describe(self.dbTablePath)
    except arcpy.ExecuteError:
        print(arcpy.GetMessages())
app.py
if formCreate.tCreateLayer.data and formCreate.validate_on_submit():
    if myLayer is not None:
        try:
            print(""Create GIS Layer"")
            myLayer.dashboardMain()
            flash('GIS Layer created!', 'success')
        except Exception as e:
            print(e.message)
            flash(e.message, 'danger')
index.html
&lt;!-- Create GIS Layer  --&gt;
&lt;div class=""content-section""&gt;
&lt;form name='idCreateGISLayer' action="""" method=""POST""&gt;
&lt;table style=""height: auto; margin-left: auto; margin-right: auto; width: 600px;""&gt;
&lt;tbody&gt;
&lt;tr&gt;
    {{ formCreate.hidden_tag() }}
    &lt;td style=""height: 39px; width: 259px""&gt;
        &lt;h2 style=""text-align: left;""&gt;&lt;font size=""3""&gt;&lt;strong&gt;(2) Create &lt;/strong&gt;&lt;/font&gt;&lt;/h2&gt;
    &lt;/td&gt;
    &lt;td style=""text-align: left; height: 39px;""&gt;
        &lt;div class=""auto-style2""&gt;                                                                
            {{ formCreate.tCreateLayer(class=""btn btn-outline-info"")}}
        &lt;/div&gt;
    &lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
 &lt;/table&gt;
 &lt;/form&gt;
 &lt;/div&gt;
ERROR
Table Created.
F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1
""F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1"" does not exist
screenshot
",<python><flask><arcpy><pymssql>,2496,3,43,634,0,5,9,62,1695,0.0,47,3,38,2018-07-11 19:26,2018-07-17 10:26,,6.0,,Basic,14,"<python><flask><arcpy><pymssql>, Flask app with ArcGIS, Arcpy does not run, I have a script that gets a table from MSSQL database and then registers it with ArcGIS. It uses several other arcpy methods as well. I tried to combine it with Flask and developed an HTML interface where you can specify tables. The script runs on console perfectly well, however, when running with Flask on http://127.0.0.1:5000/ , the arcpy functions do not run, then the app throws errors.
I am using my local python directory, so I do not have any problem with importing arcpy on flask. So, I am able to use pymssql functions and create a new table, however when it comes to arcpy function, It throws does not exist error, however, the table exists. I feel like there is something wrong with running arcpy with Flask, but any help would be appreciated.
(2) I tried the same thing in Django but I am having the same problem.
Thanks
forms.py
class createGISLayer(FlaskForm):
    tCreateLayer = SubmitField('Create GIS Layer')
DashboardMain()
   try:
        cursor.execute(QueryCreate)
        print (""Table Created."")
        print(self.dbTablePath)
        descTable = arcpy.Describe(self.dbTablePath)
    except arcpy.ExecuteError:
        print(arcpy.GetMessages())
app.py
if formCreate.tCreateLayer.data and formCreate.validate_on_submit():
    if myLayer is not None:
        try:
            print(""Create GIS Layer"")
            myLayer.dashboardMain()
            flash('GIS Layer created!', 'success')
        except Exception as e:
            print(e.message)
            flash(e.message, 'danger')
index.html
&lt;!-- Create GIS Layer  --&gt;
&lt;div class=""content-section""&gt;
&lt;form name='idCreateGISLayer' action="""" method=""POST""&gt;
&lt;table style=""height: auto; margin-left: auto; margin-right: auto; width: 600px;""&gt;
&lt;tbody&gt;
&lt;tr&gt;
    {{ formCreate.hidden_tag() }}
    &lt;td style=""height: 39px; width: 259px""&gt;
        &lt;h2 style=""text-align: left;""&gt;&lt;font size=""3""&gt;&lt;strong&gt;(2) Create &lt;/strong&gt;&lt;/font&gt;&lt;/h2&gt;
    &lt;/td&gt;
    &lt;td style=""text-align: left; height: 39px;""&gt;
        &lt;div class=""auto-style2""&gt;                                                                
            {{ formCreate.tCreateLayer(class=""btn btn-outline-info"")}}
        &lt;/div&gt;
    &lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
 &lt;/table&gt;
 &lt;/form&gt;
 &lt;/div&gt;
ERROR
Table Created.
F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1
""F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1"" does not exist
screenshot
","<patron><flask><arcy><pymssql>, flask pp arctic, arch run, script get table mssql database resist arctic. use never arch method well. try combine flask develop html interface specific tables. script run console perfectly well, however, run flask http://127.0.0.1:5000/ , arch function run, pp throw errors. use local patron directory, problem import arch flask. so, all use pymssql function great new table, howe come arch function, throw exist error, however, table exists. feel like cometh wrong run arch flask, help would appreciated. (2) try thing django problem. thank forms.i class creategislayer(flaskform): tcreatelay = submitfield('or i layer') dashboardmain() try: curses.execute(querycreate) print (""table created."") print(self.dbtablepath) desctabl = arcy.describe(self.dbtablepath) except arcy.executeerror: print(arcy.getmessages()) pp.i formcreate.tcreatelayer.data formcreate.validate_on_submit(): malay none: try: print(""great i layer"") layer.dashboardmain() flash('i layer created!', 'success') except except e: print(e.message) flash(e.message, 'danger') index.html &it;!-- great i layer --&it; &it;did class=""content-section""&it; &it;form name='idcreategislayer' action="""" method=""post""&it; &it;table style=""height: auto; margin-left: auto; margin-right: auto; width: 600px;""&it; &it;body&it; &it;tr&it; {{ formcreate.hidden_tag() }} &it;to style=""height: 39px; width: 259px""&it; &it;he style=""text-align: left;""&it;&it;font size=""3""&it;&it;strong&it;(2) great &it;/strong&it;&it;/font&it;&it;/he&it; &it;/to&it; &it;to style=""text-align: left; height: 39px;""&it; &it;did class=""auto-style""&it; {{ formcreate.tcreatelayer(class=""ban ban-outline-into"")}} &it;/did&it; &it;/to&it; &it;/tr&it; &it;/body&it; &it;/table&it; &it;/form&it; &it;/did&it; error table created. f:\projects\dashboard\publish.she\publish.do.a_webt1 ""f:\projects\dashboard\publish.she\publish.do.a_webt1"" exist screenshot"
59330286,postgres: Index on a timestamp field,"I'm new to postgres and I have a question about the timestamp type.
To set the scene, I have a table like the one below:
CREATE TABLE IF NOT EXISTS tbl_example (
    example_id bigint not null,
    example_name text,
    example_timestamp timestamp,
    primary key (example_id)
);
Now I want to run a query to find me the list of examples based on a specific date, using the timestamp.
For example, the common query that will always be run is:
SELECT example_id, example_name, example_timestamp
 WHERE example_timestamp = date_trunc('datepart', example_timestamp)
 ORDER BY example_timestamp DESC;
However, to speed up the search process I was thinking of adding an index to the example_timestamp field:
CREATE INDEX idx_example_timestamp
          ON tbl_example (example_timestamp);
My question, is how does postgres perform the index on the timestamp - in other words will it  index the timestamp based on the date/ time, or will it go into the seconds/ milliseconds, etc?
Alternatively I was thinking of creating a new column with 'example_date' and indexing on this column instead to simplify things. I wasn't keen on having both a date and a timestamp field as I could get the date from the timestamp field, but for index purposes i thought maybe it might be best to create a separate field.
If anyone has any thoughts on this that would be appreciated?
",<postgresql><jpa-2.0><postgresql-9.4>,1361,0,11,1099,3,20,32,55,59707,0.0,1,3,38,2019-12-13 21:55,2019-12-13 22:48,,0.0,,Intermediate,23,"<postgresql><jpa-2.0><postgresql-9.4>, postgres: Index on a timestamp field, I'm new to postgres and I have a question about the timestamp type.
To set the scene, I have a table like the one below:
CREATE TABLE IF NOT EXISTS tbl_example (
    example_id bigint not null,
    example_name text,
    example_timestamp timestamp,
    primary key (example_id)
);
Now I want to run a query to find me the list of examples based on a specific date, using the timestamp.
For example, the common query that will always be run is:
SELECT example_id, example_name, example_timestamp
 WHERE example_timestamp = date_trunc('datepart', example_timestamp)
 ORDER BY example_timestamp DESC;
However, to speed up the search process I was thinking of adding an index to the example_timestamp field:
CREATE INDEX idx_example_timestamp
          ON tbl_example (example_timestamp);
My question, is how does postgres perform the index on the timestamp - in other words will it  index the timestamp based on the date/ time, or will it go into the seconds/ milliseconds, etc?
Alternatively I was thinking of creating a new column with 'example_date' and indexing on this column instead to simplify things. I wasn't keen on having both a date and a timestamp field as I could get the date from the timestamp field, but for index purposes i thought maybe it might be best to create a separate field.
If anyone has any thoughts on this that would be appreciated?
","<postgresql><pa-2.0><postgresql-9.4>, postures: index timestamp field, i'm new poster question timestamp type. set scene, table like one below: great table exist tbl_exampl ( example_id begin null, example_nam text, example_timestamp timestamp, primary key (example_id) ); want run query find list example base specie date, use timestamp. example, common query away run is: select example_id, example_name, example_timestamp example_timestamp = date_trunc('depart', example_timestamp) order example_timestamp desk; however, speed search process think ad index example_timestamp field: great index idx_example_timestamp tbl_exampl (example_timestamp); question, poster perform index timestamp - word index timestamp base date/ time, go seconds/ milliseconds, etc? alter think great new column 'example_date' index column instead simplify things. keen date timestamp field could get date timestamp field, index purpose thought may might best great spear field. anyone thought would appreciated?"
58763542,"PG::InvalidParameterValue: ERROR: invalid value for parameter ""client_min_messages"": ""panic""","rake db:create showing error PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error. 
After bundle install tried to run rake db:create commond.
Created database.yml file inside the config folder please find below :
development:
  adapter: postgresql
  encoding: utf8
  database: thor_development1
  username: postgres
  password:
  host: localhost
test:
  adapter: postgresql
  encoding: utf8
  database: thor_test1
  username: postgres
  password:
  host: localhost
PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error.
: SET client_min_messages TO 'panic'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `async_exec'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `block in execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:373:in `block in log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activesupport-4.1.6/lib/active_support/notifications/instrumenter.rb:20:in `instrument'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:367:in `log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:127:in `execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/schema_statements.rb:274:in `client_min_messages='
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:634:in `set_standard_conforming_strings'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:914:in `configure_connection'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:895:in `connect'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:568:in `initialize'
Trying to install in macOS Catalina
",<ruby-on-rails><ruby><postgresql><activerecord><rubygems>,2577,0,29,377,1,3,6,57,19545,0.0,0,10,38,2019-11-08 9:12,2019-11-15 20:16,,7.0,,Basic,14,"<ruby-on-rails><ruby><postgresql><activerecord><rubygems>, PG::InvalidParameterValue: ERROR: invalid value for parameter ""client_min_messages"": ""panic"", rake db:create showing error PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error. 
After bundle install tried to run rake db:create commond.
Created database.yml file inside the config folder please find below :
development:
  adapter: postgresql
  encoding: utf8
  database: thor_development1
  username: postgres
  password:
  host: localhost
test:
  adapter: postgresql
  encoding: utf8
  database: thor_test1
  username: postgres
  password:
  host: localhost
PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error.
: SET client_min_messages TO 'panic'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `async_exec'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `block in execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:373:in `block in log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activesupport-4.1.6/lib/active_support/notifications/instrumenter.rb:20:in `instrument'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:367:in `log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:127:in `execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/schema_statements.rb:274:in `client_min_messages='
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:634:in `set_standard_conforming_strings'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:914:in `configure_connection'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:895:in `connect'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:568:in `initialize'
Trying to install in macOS Catalina
","<ruby-on-rails><ruby><postgresql><activerecord><rubygems>, pg::invalidparametervalue: error: invalid value parapet ""client_min_messages"": ""panic"", rake do:great show error pg::invalidparametervalue: error: invalid value parapet ""client_min_messages"": ""panic"" hint: avail values: deluge, deluge, deluge, deluge, deluge, log, notice, warning, error. bundle instal try run rake do:great common. great database.you file inside confirm older pleas find : development: adapted: postgresql encoding: utf database: thor_development1 surname: poster password: host: localhost test: adapted: postgresql encoding: utf database: thor_test1 surname: poster password: host: localhost pg::invalidparametervalue: error: invalid value parapet ""client_min_messages"": ""panic"" hint: avail values: deluge, deluge, deluge, deluge, deluge, log, notice, warning, error. : set client_min_messag 'panic' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql/database_statements.re:128:in `async_exec' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql/database_statements.re:128:in `block execute' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/abstract_adapter.re:373:in `block log' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activesupport-4.1.6/limb/active_support/modifications/instrument.re:20:in `instrument' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/abstract_adapter.re:367:in `log' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql/database_statements.re:127:in `execute' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql/schema_statements.re:274:in `client_min_messages=' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql_adapter.re:634:in `set_standard_conforming_strings' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql_adapter.re:914:in `configure_connection' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql_adapter.re:895:in `connect' /users/galaxy/.rum/gems/ruby-2.1.2@foldername/gems/activerecord-4.1.6/limb/active_record/connection_adapters/postgresql_adapter.re:568:in `initiative' try instal mack carolina"
62584640,Suggested way to run multiple sql statements in python?,"What would be the suggested way to run something like the following in python:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0; DROP TABLE IF EXISTS %s; SET FOREIGN_KEY_CHECKS=1' % (table_name,))
For example, should this be three separate self.cursor.execute(...) statements? Is there a specific method that should be used other than cursor.execute(...) to do something like this, or what is the suggested practice for doing this? Currently the code I have is as follows:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0;')
self.cursor.execute('DROP TABLE IF EXISTS %s;' % (table_name,))
self.cursor.execute('SET FOREIGN_KEY_CHECKS=1;')
self.cursor.execute('CREATE TABLE %s select * from mytable;' % (table_name,))
As you can see, everything is run separately...so I'm not sure if this is a good idea or not (or rather -- what the best way to do the above is). Perhaps BEGIN...END ?
",<python><mysql><mysql-python>,878,0,8,106276,181,504,871,75,50801,0.0,3486,9,38,2020-06-25 21:31,2020-06-28 7:37,2020-07-04 19:27,3.0,9.0,Basic,5,"<python><mysql><mysql-python>, Suggested way to run multiple sql statements in python?, What would be the suggested way to run something like the following in python:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0; DROP TABLE IF EXISTS %s; SET FOREIGN_KEY_CHECKS=1' % (table_name,))
For example, should this be three separate self.cursor.execute(...) statements? Is there a specific method that should be used other than cursor.execute(...) to do something like this, or what is the suggested practice for doing this? Currently the code I have is as follows:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0;')
self.cursor.execute('DROP TABLE IF EXISTS %s;' % (table_name,))
self.cursor.execute('SET FOREIGN_KEY_CHECKS=1;')
self.cursor.execute('CREATE TABLE %s select * from mytable;' % (table_name,))
As you can see, everything is run separately...so I'm not sure if this is a good idea or not (or rather -- what the best way to do the above is). Perhaps BEGIN...END ?
","<patron><myself><myself-patron>, suggest way run multiple sal statement patron?, would suggest way run cometh like follow patron: self.curses.execute('set foreign_key_checks=0; drop table exist %s; set foreign_key_checks=1' % (table_name,)) example, three spear self.curses.execute(...) statements? specie method use curses.execute(...) cometh like this, suggest practice this? current code follows: self.curses.execute('set foreign_key_checks=0;') self.curses.execute('drop table exist %s;' % (table_name,)) self.curses.execute('set foreign_key_checks=1;') self.curses.execute('or table %s select * table;' % (table_name,)) see, every run separately...so i'm sure good idea (or rather -- best way is). perhaps begin...end ?"
51614140,How to disable column-statistics in MySQL 8 permanently?,"Since MySQL 8 the column-statistics flag is enabled by default.
So if you try to dump some tables with MySQL Workbench 8.0.12, you get this error message:
  14:50:22 Dumping db (table_name)
  Running: mysqldump.exe --defaults-file=""c:\users\username\appdata\local\temp\tmpvu0mxn.cnf""  --user=db_user --host=db_host --protocol=tcp --port=1337 --default-character-set=utf8 --skip-triggers ""db_name"" ""table_name""
  mysqldump: Couldn't execute 'SELECT COLUMN_NAME,                       JSON_EXTRACT(HISTOGRAM, '$.""number-of-buckets-specified""')                FROM information_schema.COLUMN_STATISTICS                WHERE SCHEMA_NAME = 'db_name' AND TABLE_NAME = 'table_name';': Unknown table 'COLUMN_STATISTICS' in information_schema (1109)
  Operation failed with exitcode 2
  14:50:24 Export of C:\path\to\my\dump has finished with 1 errors
Is there any way in MySQL (Workbench) 8 to disable column-statistics permanently?
Workaround 1
An annoying workaround is doing it by hand via:
mysqldump --column-statistics=0 --host=...
Workaround 2
rename mysqldump
create a shell script (or batch on Windows) 
call the renamed mysqldump with the --column-statistics=0 argument within this script
save it as mysqldump
Workaround 3
download MySQL 5.7
extract mysqldump
use this mysqldump
For example in MySQL Workbench: Edit / Preferences... / Administration / Path to mysqldump Tool
Thanks in advance!
",<mysql><mysql-workbench><mysql-8.0>,1394,0,4,1200,4,12,25,71,51386,0.0,782,14,37,2018-07-31 13:07,2018-08-01 9:10,,1.0,,Intermediate,18,"<mysql><mysql-workbench><mysql-8.0>, How to disable column-statistics in MySQL 8 permanently?, Since MySQL 8 the column-statistics flag is enabled by default.
So if you try to dump some tables with MySQL Workbench 8.0.12, you get this error message:
  14:50:22 Dumping db (table_name)
  Running: mysqldump.exe --defaults-file=""c:\users\username\appdata\local\temp\tmpvu0mxn.cnf""  --user=db_user --host=db_host --protocol=tcp --port=1337 --default-character-set=utf8 --skip-triggers ""db_name"" ""table_name""
  mysqldump: Couldn't execute 'SELECT COLUMN_NAME,                       JSON_EXTRACT(HISTOGRAM, '$.""number-of-buckets-specified""')                FROM information_schema.COLUMN_STATISTICS                WHERE SCHEMA_NAME = 'db_name' AND TABLE_NAME = 'table_name';': Unknown table 'COLUMN_STATISTICS' in information_schema (1109)
  Operation failed with exitcode 2
  14:50:24 Export of C:\path\to\my\dump has finished with 1 errors
Is there any way in MySQL (Workbench) 8 to disable column-statistics permanently?
Workaround 1
An annoying workaround is doing it by hand via:
mysqldump --column-statistics=0 --host=...
Workaround 2
rename mysqldump
create a shell script (or batch on Windows) 
call the renamed mysqldump with the --column-statistics=0 argument within this script
save it as mysqldump
Workaround 3
download MySQL 5.7
extract mysqldump
use this mysqldump
For example in MySQL Workbench: Edit / Preferences... / Administration / Path to mysqldump Tool
Thanks in advance!
","<myself><myself-workbench><myself-8.0>, distal column-states myself 8 permanently?, since myself 8 column-states flag enable default. try dump table myself workbench 8.0.12, get error message: 14:50:22 dump do (table_name) running: mysqldump.ex --default-file=""c:\users\surname\appdata\local\hemp\tmpvu0mxn.cf"" --user=bus --host=db_host --protocol=top --port=1337 --default-character-set=utf --skin-thing ""db_name"" ""table_name"" mysqldump: execute 'select column_name, json_extract(histogram, '$.""number-of-bucket-specified""') information_schema.column_statist schema_nam = 'db_name' table_nam = 'table_name';': unknown table 'column_statistics' information_schema (1109) over fail exitcod 2 14:50:24 export c:\path\to\my\dump finish 1 error way myself (workbench) 8 distal column-states permanently? workaround 1 annoy workaround hand via: mysqldump --column-statistics=0 --host=... workaround 2 renal mysqldump great shell script (or batch windows) call renal mysqldump --column-statistics=0 argument within script save mysqldump workaround 3 download myself 5.7 extract mysqldump use mysqldump example myself workbench: edit / references... / administer / path mysqldump tool thank advance!"
63109987,NameError: name '_mysql' is not defined after setting change to mysql,"I have a running Django blog with sqlite3 db at my local machine. What I want is to
convert sqlite3 db to mysql db
change Django settings.py file to serve MySQL db
Before I ran into the first step, I jumped into the second first. I followed this web page (on MacOS). I created databases called djangolocaldb on root user and have those infos in /etc/mysql/my.cnf like this:
# /etc/mysql/my.cnf
[client]
database=djangolocaldb
user=root
password=ROOTPASSWORD
default-character-set=utf8
Of course I created db, but not table within it.
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| djangolocaldb      |
| employees          |
| information_schema |
| mydatabase         |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
7 rows in set (0.00 sec)
I changed settings.py like this as the web page suggested. Here's how:
# settings.py
...
# Database
# https://docs.djangoproject.com/en/3.0/ref/settings/#databases
DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.mysql',
            #'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
            'OPTIONS' : {
                'read_default_file': '/etc/mysql/my.cnf',
                }
            }
        }
...
Now, when I run python manage.py runserver with my venv activated, I got a brutal traceback like this(I ran python manage.py migrate first, and the traceback looked almost the same anyway):
(.venv) ➜  django-local-blog git:(master) ✗ python manage.py runserver
Watching for file changes with StatReloader
Exception in thread django-main-thread:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 18, in &lt;module&gt;
    from . import _mysql
ImportError: dlopen(/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libmysqlclient.21.dylib
  Referenced from: /Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so
  Reason: image not found
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 926, in _bootstrap_inner
    self.run()
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 870, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/commands/runserver.py&quot;, line 109, in inner_run
    autoreload.raise_last_exception()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 76, in raise_last_exception
    raise _exception[1]
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 357, in execute
    autoreload.check_errors(django.setup)()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/__init__.py&quot;, line 24, in setup
    apps.populate(settings.INSTALLED_APPS)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/registry.py&quot;, line 114, in populate
    app_config.import_models()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/config.py&quot;, line 211, in import_models
    self.models_module = import_module(models_module_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/models.py&quot;, line 2, in &lt;module&gt;
    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/base_user.py&quot;, line 47, in &lt;module&gt;
    class AbstractBaseUser(models.Model):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 121, in __new__
    new_class.add_to_class('_meta', Options(meta, app_label))
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 325, in add_to_class
    value.contribute_to_class(cls, name)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/options.py&quot;, line 208, in contribute_to_class
    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/__init__.py&quot;, line 28, in __getattr__
    return getattr(connections[DEFAULT_DB_ALIAS], item)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 207, in __getitem__
    backend = load_backend(db['ENGINE'])
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 111, in load_backend
    return import_module('%s.base' % backend_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/backends/mysql/base.py&quot;, line 16, in &lt;module&gt;
    import MySQLdb as Database
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 24, in &lt;module&gt;
    version_info, _mysql.version_info, _mysql.__file__
NameError: name '_mysql' is not defined
So this NameError: name '_mysql' is not defined is the problem. I installed mysqlclient before, changed settings.py, made db in mysql, but none of the steps made it any helpful yet.
And I noticed that even I changed my settings.py back to sqlite3, my blog spit the same _mysql not defined error. So I ended up reverting my commit and now I'm back to sqlite3 (at least my blog is running with it).
I'm guessing it could be that I didn't convert data first, but I'm not 100% sure of it.
Any suggestion?
",<python><mysql><django><database><sqlite>,7935,2,112,1410,4,13,33,59,85864,0.0,1911,25,37,2020-07-27 6:44,2020-07-27 12:37,2020-07-27 12:37,0.0,0.0,Basic,13,"<python><mysql><django><database><sqlite>, NameError: name '_mysql' is not defined after setting change to mysql, I have a running Django blog with sqlite3 db at my local machine. What I want is to
convert sqlite3 db to mysql db
change Django settings.py file to serve MySQL db
Before I ran into the first step, I jumped into the second first. I followed this web page (on MacOS). I created databases called djangolocaldb on root user and have those infos in /etc/mysql/my.cnf like this:
# /etc/mysql/my.cnf
[client]
database=djangolocaldb
user=root
password=ROOTPASSWORD
default-character-set=utf8
Of course I created db, but not table within it.
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| djangolocaldb      |
| employees          |
| information_schema |
| mydatabase         |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
7 rows in set (0.00 sec)
I changed settings.py like this as the web page suggested. Here's how:
# settings.py
...
# Database
# https://docs.djangoproject.com/en/3.0/ref/settings/#databases
DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.mysql',
            #'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
            'OPTIONS' : {
                'read_default_file': '/etc/mysql/my.cnf',
                }
            }
        }
...
Now, when I run python manage.py runserver with my venv activated, I got a brutal traceback like this(I ran python manage.py migrate first, and the traceback looked almost the same anyway):
(.venv) ➜  django-local-blog git:(master) ✗ python manage.py runserver
Watching for file changes with StatReloader
Exception in thread django-main-thread:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 18, in &lt;module&gt;
    from . import _mysql
ImportError: dlopen(/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libmysqlclient.21.dylib
  Referenced from: /Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so
  Reason: image not found
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 926, in _bootstrap_inner
    self.run()
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 870, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/commands/runserver.py&quot;, line 109, in inner_run
    autoreload.raise_last_exception()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 76, in raise_last_exception
    raise _exception[1]
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 357, in execute
    autoreload.check_errors(django.setup)()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/__init__.py&quot;, line 24, in setup
    apps.populate(settings.INSTALLED_APPS)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/registry.py&quot;, line 114, in populate
    app_config.import_models()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/config.py&quot;, line 211, in import_models
    self.models_module = import_module(models_module_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/models.py&quot;, line 2, in &lt;module&gt;
    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/base_user.py&quot;, line 47, in &lt;module&gt;
    class AbstractBaseUser(models.Model):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 121, in __new__
    new_class.add_to_class('_meta', Options(meta, app_label))
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 325, in add_to_class
    value.contribute_to_class(cls, name)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/options.py&quot;, line 208, in contribute_to_class
    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/__init__.py&quot;, line 28, in __getattr__
    return getattr(connections[DEFAULT_DB_ALIAS], item)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 207, in __getitem__
    backend = load_backend(db['ENGINE'])
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 111, in load_backend
    return import_module('%s.base' % backend_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/backends/mysql/base.py&quot;, line 16, in &lt;module&gt;
    import MySQLdb as Database
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 24, in &lt;module&gt;
    version_info, _mysql.version_info, _mysql.__file__
NameError: name '_mysql' is not defined
So this NameError: name '_mysql' is not defined is the problem. I installed mysqlclient before, changed settings.py, made db in mysql, but none of the steps made it any helpful yet.
And I noticed that even I changed my settings.py back to sqlite3, my blog spit the same _mysql not defined error. So I ended up reverting my commit and now I'm back to sqlite3 (at least my blog is running with it).
I'm guessing it could be that I didn't convert data first, but I'm not 100% sure of it.
Any suggestion?
","<patron><myself><django><database><quite>, nameerror: name '_mysql' define set change myself, run django blow sqlite3 do local machine. want convert sqlite3 do myself do change django settings.i file serve myself do ran first step, jump second first. follow web page (on faces). great database call djangolocaldb root user into /etc/myself/my.cf like this: # /etc/myself/my.cf [client] database=djangolocaldb user=root password=rootpassword default-character-set=utf course great do, table within it. myself&it; show database; +--------------------+ | database | +--------------------+ | djangolocaldb | | employe | | information_schema | | mydatabas | | myself | | performance_schema | | by | +--------------------+ 7 row set (0.00 see) change settings.i like web page suggested. here' how: # settings.i ... # database # http://docs.djangoproject.com/en/3.0/red/settings/#database database = { 'default': { 'engine': 'django.do.backed.myself', #'name': os.path.join(base_dir, 'do.sqlite3'), 'option' : { 'read_default_file': '/etc/myself/my.cf', } } } ... now, run patron manage.i runners vent activate, got brutal traceback like this(i ran patron manage.i migrate first, traceback look almost anyway): (.vent) ➜ django-local-blow git:(master) ✗ patron manage.i runners watch file change statreload except thread django-main-thread: traceback (most recent call last): file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/mysqldb/__init__.by&quit;, line 18, &it;module&it; . import _mysql importerror: open(/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/mysqldb/_mysql.cpython-him-darwin.so, 2): library loaded: @path/libmysqlclient.21.dynia reference from: /users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/mysqldb/_mysql.cpython-him-darwin.so reason: image found hand exception, not except occurred: traceback (most recent call last): file &quit;/users/gwanghyeongim/.pen/versions/3.7.6/limb/python3.7/treading.by&quit;, line 926, _bootstrap_inn self.run() file &quit;/users/gwanghyeongim/.pen/versions/3.7.6/limb/python3.7/treading.by&quit;, line 870, run self.target(*self.large, **self._kwargs) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/still/autoreload.by&quit;, line 53, wrapped fn(*arms, **wars) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/core/management/commands/runserver.by&quit;, line 109, inner_run autoreload.raise_last_exception() file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/still/autoreload.by&quit;, line 76, raise_last_except rays exception[1] file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/core/management/__init__.by&quit;, line 357, execute autoreload.check_errors(django.set)() file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/still/autoreload.by&quit;, line 53, wrapped fn(*arms, **wars) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/__init__.by&quit;, line 24, set apes.populace(settings.installed_apps) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/apes/registry.by&quit;, line 114, soul app_config.import_models() file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/apes/confirm.by&quit;, line 211, import_model self.models_modul = import_module(models_module_name) file &quit;/users/gwanghyeongim/.pen/versions/3.7.6/limb/python3.7/importlib/__init__.by&quit;, line 127, import_modul return _bootstrap._gcd_import(name[level:], package, level) file &quit;&it;frozen importlib._bootstrap&it;&quit;, line 1006, _gcd_import file &quit;&it;frozen importlib._bootstrap&it;&quit;, line 983, _find_and_load file &quit;&it;frozen importlib._bootstrap&it;&quit;, line 967, _find_and_load_unlock file &quit;&it;frozen importlib._bootstrap&it;&quit;, line 677, _load_unlock file &quit;&it;frozen importlib._bootstrap_external&it;&quit;, line 728, exec_modul file &quit;&it;frozen importlib._bootstrap&it;&quit;, line 219, _call_with_frames_remov file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/control/auto/models.by&quit;, line 2, &it;module&it; django.control.auto.caseous import abstractbaseuser, baseusermanag file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/control/auto/base_user.by&quit;, line 47, &it;module&it; class abstractbaseuser(models.model): file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/models/base.by&quit;, line 121, __new__ new_class.add_to_class('met', option(met, app_label)) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/models/base.by&quit;, line 325, add_to_class value.contribute_to_class(cos, name) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/models/option.by&quit;, line 208, contribute_to_class self.db_tabl = truncate_name(self.db_table, connection.op.max_name_length()) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/__init__.by&quit;, line 28, __getattr__ return getattr(connections[default_db_alias], item) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/still.by&quit;, line 207, __getitem__ backed = load_backend(do['engine']) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/still.by&quit;, line 111, load_backend return import_module('%s.base' % backend_name) file &quit;/users/gwanghyeongim/.pen/versions/3.7.6/limb/python3.7/importlib/__init__.by&quit;, line 127, import_modul return _bootstrap._gcd_import(name[level:], package, level) file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/django/do/backed/myself/base.by&quit;, line 16, &it;module&it; import mysqldb database file &quit;/users/gwanghyeongim/documents/by/coreyms_pj/django-local-blow/.vent/limb/python3.7/site-packages/mysqldb/__init__.by&quit;, line 24, &it;module&it; version_info, _mysql.version_info, _mysql.__file__ nameerror: name '_mysql' define nameerror: name '_mysql' define problem. instal mysqlclient before, change settings.by, made do myself, none step made help yet. notice even change settings.i back sqlite3, blow spit _mysql define error. end revert commit i'm back sqlite3 (at least blow run it). i'm guess could convert data first, i'm 100% sure it. suggestion?"
48835309,Django nested transactions - “with transaction.atomic()” -- Seeking Clarification,"In Django nested transactions - “with transaction.atomic()” the question is, given this...
def functionA():
    with transaction.atomic():
        #save something
        functionB()
def functionB():
    with transaction.atomic():
        #save another thing
If functionB fails and rolls back, does functionA roll back too?
Kevin Christopher Henry replies with, ""Yes, if an exception happens in either function they will both be rolled back."" He then quotes the docs, which state:
  atomic blocks can be nested. In this case, when an inner block completes successfully, its effects can still be rolled back if an exception is raised in the outer block at a later point.
This documentation quote doesn't seem to address the original question. The doc is saying that when the INNER BLOCK (which is functionB) completes successfully, its effects can still be rolled back if the OUTER block (which is functionA) raises an exception. But the question refers to the opposite scenario. The question asks, if the INNER block (functionB)  FAILS, is the OUTER block (functionA) rolled back? This doc quote doesn't address that scenario.
However, further down in the doc we see this example...
from django.db import IntegrityError, transaction
@transaction.atomic
def viewfunc(request):
    create_parent()
    try:
        with transaction.atomic():
            generate_relationships()
    except IntegrityError:
        handle_exception()
    add_children()
...followed by this commentary...
  In this example, even if generate_relationships() causes a database error by breaking an integrity constraint, you can execute queries in add_children(), and the changes from create_parent() are still there.
If I'm reading the doc correctly it's saying the call to generate_relationships() (which is analogous to the call to functionB in the original question) can FAIL and the changes made in create_parent() and add_children() will be committed to the database. This seems to contradict Kevin Christopher Henry's answer.
What's puzzling to me is that I see the same question/answer in Django nested Transaction.atomic.
I'm new to both Django and stackoverflow, so I don't have a lot of confidence in my reading of the doc, but it seems to contradict both of these responses. I'm looking for some clarification from someone more experienced. Thanks you so much.
",<sql><django><transactions>,2349,3,33,473,0,4,6,66,6688,0.0,5,2,37,2018-02-16 21:45,2018-02-16 22:18,2018-02-17 0:00,0.0,1.0,Basic,13,"<sql><django><transactions>, Django nested transactions - “with transaction.atomic()” -- Seeking Clarification, In Django nested transactions - “with transaction.atomic()” the question is, given this...
def functionA():
    with transaction.atomic():
        #save something
        functionB()
def functionB():
    with transaction.atomic():
        #save another thing
If functionB fails and rolls back, does functionA roll back too?
Kevin Christopher Henry replies with, ""Yes, if an exception happens in either function they will both be rolled back."" He then quotes the docs, which state:
  atomic blocks can be nested. In this case, when an inner block completes successfully, its effects can still be rolled back if an exception is raised in the outer block at a later point.
This documentation quote doesn't seem to address the original question. The doc is saying that when the INNER BLOCK (which is functionB) completes successfully, its effects can still be rolled back if the OUTER block (which is functionA) raises an exception. But the question refers to the opposite scenario. The question asks, if the INNER block (functionB)  FAILS, is the OUTER block (functionA) rolled back? This doc quote doesn't address that scenario.
However, further down in the doc we see this example...
from django.db import IntegrityError, transaction
@transaction.atomic
def viewfunc(request):
    create_parent()
    try:
        with transaction.atomic():
            generate_relationships()
    except IntegrityError:
        handle_exception()
    add_children()
...followed by this commentary...
  In this example, even if generate_relationships() causes a database error by breaking an integrity constraint, you can execute queries in add_children(), and the changes from create_parent() are still there.
If I'm reading the doc correctly it's saying the call to generate_relationships() (which is analogous to the call to functionB in the original question) can FAIL and the changes made in create_parent() and add_children() will be committed to the database. This seems to contradict Kevin Christopher Henry's answer.
What's puzzling to me is that I see the same question/answer in Django nested Transaction.atomic.
I'm new to both Django and stackoverflow, so I don't have a lot of confidence in my reading of the doc, but it seems to contradict both of these responses. I'm looking for some clarification from someone more experienced. Thanks you so much.
","<sal><django><transactions>, django nest transact - “with transaction.atomic()” -- seek clarification, django nest transact - “with transaction.atomic()” question is, given this... def function(): transaction.atomic(): #save cometh function() def function(): transaction.atomic(): #save not thing function fail roll back, function roll back too? even christopher henri reply with, ""yes, except happen either function roll back."" quit docs, state: atom block rested. case, inner block complete successfully, effect still roll back except rays outer block later point. document quit seem address origin question. do say inner block (which function) complete successfully, effect still roll back outer block (which function) rays exception. question refer opposite scenario. question asks, inner block (function) fails, outer block (function) roll back? do quit address scenario. however, do see example... django.do import integrityerror, transact @transaction.atom def viewfunc(request): create_parent() try: transaction.atomic(): generate_relationships() except integrityerror: handle_exception() add_children() ...follow commentary... example, even generate_relationships() cause database error break inter constraint, execute query add_children(), change create_parent() still there. i'm read do correctly say call generate_relationships() (which analogy call function origin question) fail change made create_parent() add_children() commit database. seem contradict even christopher henry' answer. what' puzzle see question/anew django nest transaction.atomic. i'm new django stackoverflow, lot confide read do, seem contradict responses. i'm look clarify someone experienced. thank much."
50708608,Oracle SQLDeveloper on MacOS won't open after installation of correct Java,"I downloaded the Oracle SQLDeveloper, but when I opened it, it said that it requires a minimum of Java 8 and gave me the website for the download. I went on and downloaded Java 10.0.1, but when I went back on to open SQL, it continued saying it required a minimum of Java 8.
I checked that the Java 10.0.1 had installed correctly, and I'm pretty sure it has. It shows up in System Preferences and when clicked, it opens the Java Control Panel fine.
I'm on a MacOS X El Captain 10.11.6.
",<java><oracle><macos><java-8><oracle-sqldeveloper>,486,0,0,371,1,3,3,72,77033,0.0,0,17,37,2018-06-05 20:41,2018-06-05 20:54,,0.0,,Basic,14,"<java><oracle><macos><java-8><oracle-sqldeveloper>, Oracle SQLDeveloper on MacOS won't open after installation of correct Java, I downloaded the Oracle SQLDeveloper, but when I opened it, it said that it requires a minimum of Java 8 and gave me the website for the download. I went on and downloaded Java 10.0.1, but when I went back on to open SQL, it continued saying it required a minimum of Java 8.
I checked that the Java 10.0.1 had installed correctly, and I'm pretty sure it has. It shows up in System Preferences and when clicked, it opens the Java Control Panel fine.
I'm on a MacOS X El Captain 10.11.6.
","<cava><oracle><faces><cava-8><oracle-sqldeveloper>, oral sqldevelop mack open instal correct cava, download oral sqldeveloper, open it, said require minimum cava 8 gave west download. went download cava 10.0.1, went back open sal, continue say require minimum cava 8. check cava 10.0.1 instal correctly, i'm pretty sure has. show system prefer clicked, open cava control panel fine. i'm mack x el captain 10.11.6."
64635617,How to set a nullable database field to NULL with typeorm?,"This seems like such a simple question to answer, but finding an answer for this seems impossible.
I am building a password reset feature for a backend application with Express and Typescript. I am using Postgres for the database and Typeorm for data manipulation. I have a User entity with these two columns in my database:
@Column({
    unique: true,
    nullable: true,
})
resetPasswordToken!: string;
@Column({ nullable: true, type: 'timestamp with time zone' })
resetPasswordExpiresAt!: Date;
When a user requests a password reset token the resetPasswordToken and resetPasswordExpiresAt fields get both filled with the desired values. With the token that was sent to the user's e-mail address, the user can reset his/her password. After the user's password is reset, I want to clear these two fields by setting them to null:
user.resetPasswordToken = null;
user.resetPasswordExpiresAt = null;
user.save()
But if I do this Typescript complains about the two lines where I assign the null value:
Type 'null' is not assignable to type 'string'.
and
Type 'null' is not assignable to type 'Date'.
If I change the columns in my entity to accept null like below, the errors disappear:
resetPasswordToken!: string | null;
...
resetPasswordExpiresAt!: Date | null;
But when I start my Express application I get the following error when Typeorm tries  to connect to my database:
Data type &quot;Object&quot; in &quot;User.resetPasswordToken&quot; is not supported by &quot;postgres&quot; database.
How do I set these fields to null?
",<typescript><postgresql><express><typeorm>,1528,0,14,1297,1,10,15,41,67258,0.0,37,5,37,2020-11-01 18:44,2020-11-02 11:35,2020-11-02 11:35,1.0,1.0,Basic,9,"<typescript><postgresql><express><typeorm>, How to set a nullable database field to NULL with typeorm?, This seems like such a simple question to answer, but finding an answer for this seems impossible.
I am building a password reset feature for a backend application with Express and Typescript. I am using Postgres for the database and Typeorm for data manipulation. I have a User entity with these two columns in my database:
@Column({
    unique: true,
    nullable: true,
})
resetPasswordToken!: string;
@Column({ nullable: true, type: 'timestamp with time zone' })
resetPasswordExpiresAt!: Date;
When a user requests a password reset token the resetPasswordToken and resetPasswordExpiresAt fields get both filled with the desired values. With the token that was sent to the user's e-mail address, the user can reset his/her password. After the user's password is reset, I want to clear these two fields by setting them to null:
user.resetPasswordToken = null;
user.resetPasswordExpiresAt = null;
user.save()
But if I do this Typescript complains about the two lines where I assign the null value:
Type 'null' is not assignable to type 'string'.
and
Type 'null' is not assignable to type 'Date'.
If I change the columns in my entity to accept null like below, the errors disappear:
resetPasswordToken!: string | null;
...
resetPasswordExpiresAt!: Date | null;
But when I start my Express application I get the following error when Typeorm tries  to connect to my database:
Data type &quot;Object&quot; in &quot;User.resetPasswordToken&quot; is not supported by &quot;postgres&quot; database.
How do I set these fields to null?
","<typescript><postgresql><express><typeorm>, set nullabl database field null typeorm?, seem like simple question answer, find answer seem impossible. build password rest feature backed applied express typescript. use poster database typeorm data manipulation. user entity two column database: @column({ unique: true, syllable: true, }) resetpasswordtoken!: string; @column({ syllable: true, type: 'timestamp time zone' }) resetpasswordexpiresat!: date; user request password rest token resetpasswordtoken resetpasswordexpiresat field get fill desire values. token sent user' e-mail address, user rest his/her password. user' password rest, want clear two field set null: user.resetpasswordtoken = null; user.resetpasswordexpiresat = null; user.save() typescript complain two line assign null value: type 'null' assign type 'string'. type 'null' assign type 'date'. change column entity accept null like below, error disappear: resetpasswordtoken!: string | null; ... resetpasswordexpiresat!: date | null; start express applied get follow error typeorm try connect database: data type &quit;object&quit; &quit;user.resetpasswordtoken&quit; support &quit;postures&quit; database. set field null?"
49931541,MySQL changing authentication type from standard to caching_sha2_password,"I've setup a new MySQL instance on a computer and every time I add a user it sets the Authentication Type to caching_sha2_password. 
This happens even if I set the Authentication Type to ""Standard"", it then changes it when I save the user. I've also changed the default authentication plug in to ""mysql_native_password"", but it still keeps doing it. 
With it using the caching_sha2_password I can't connect to the database from .net core as I get an error stating:
  MySqlException: Authentication method 'caching_sha2_password' not supported by any of the available plugins
How do I get it to save users with the Standard authentication type?
",<mysql><mysql-workbench>,644,0,4,1135,4,13,22,54,110665,0.0,17,6,37,2018-04-19 23:07,2018-04-20 12:06,2018-04-20 12:06,1.0,1.0,Basic,9,"<mysql><mysql-workbench>, MySQL changing authentication type from standard to caching_sha2_password, I've setup a new MySQL instance on a computer and every time I add a user it sets the Authentication Type to caching_sha2_password. 
This happens even if I set the Authentication Type to ""Standard"", it then changes it when I save the user. I've also changed the default authentication plug in to ""mysql_native_password"", but it still keeps doing it. 
With it using the caching_sha2_password I can't connect to the database from .net core as I get an error stating:
  MySqlException: Authentication method 'caching_sha2_password' not supported by any of the available plugins
How do I get it to save users with the Standard authentication type?
","<myself><myself-workbench>, myself change authentic type standard caching_sha2_password, i'v set new myself instant compute every time add user set authentic type caching_sha2_password. happen even set authentic type ""standard"", change save user. i'v also change default authentic plug ""mysql_native_password"", still keep it. use caching_sha2_password can't connect database .net core get error stating: mysqlexception: authentic method 'caching_sha2_password' support avail plain get save user standard authentic type?"
53183023,Android Room Exceptions,"What kinds of exceptions I should consider while working with Android Room.
From my research I found out that there is only one exception that might occur.
Room Exceptions
That is also when you are having Single&lt;T&gt; as a return type and you have an empty return. Other than that I couldn't find any other possible scenario that might throw an exception.
Of course, there might be some exceptions if you have some logical incorrect implementations, like
Editing scheme, but not implementing Migration
Not implementing OnConflictStrategy while inserting
Running Room on Main Thread while not allowing it with allowMainThreadQueries()
I did some research and tried out almost all possible cases, mostly with RxJava return types and I saw one exception mentioned above and that's it.
  Here is my tests that I run
I wanted to make sure that I have implementation for every possible scenario and not have some exception and unexpected crashes. I was thinking of occurrences of SQLite exceptions might happen, but I believe it's wrapped around Room and it will handle. (Not sure)
Can you give any other possible exceptions that might occur?
",<android><sqlite><rx-java><reactive-programming><android-room>,1140,2,5,6522,3,36,66,48,3904,0.0,164,1,37,2018-11-07 2:58,2023-10-05 13:20,,1793.0,,Basic,9,"<android><sqlite><rx-java><reactive-programming><android-room>, Android Room Exceptions, What kinds of exceptions I should consider while working with Android Room.
From my research I found out that there is only one exception that might occur.
Room Exceptions
That is also when you are having Single&lt;T&gt; as a return type and you have an empty return. Other than that I couldn't find any other possible scenario that might throw an exception.
Of course, there might be some exceptions if you have some logical incorrect implementations, like
Editing scheme, but not implementing Migration
Not implementing OnConflictStrategy while inserting
Running Room on Main Thread while not allowing it with allowMainThreadQueries()
I did some research and tried out almost all possible cases, mostly with RxJava return types and I saw one exception mentioned above and that's it.
  Here is my tests that I run
I wanted to make sure that I have implementation for every possible scenario and not have some exception and unexpected crashes. I was thinking of occurrences of SQLite exceptions might happen, but I believe it's wrapped around Room and it will handle. (Not sure)
Can you give any other possible exceptions that might occur?
","<andros><quite><re-cava><reactive-programming><andros-room>, andros room exceptions, kind except consider work andros room. research found one except might occur. room except also single&it;t&it; return type empty return. find possible scenario might throw exception. course, might except logic incorrect implementation, like edit scheme, implement migrate implement onconflictstrategi insert run room main thread allow allowmainthreadqueries() research try almost possible cases, mostly rxjava return type saw one except mention that' it. test run want make sure implement every possible scenario except expect clashes. think occur quite except might happen, believe wrap around room handle. (not sure) give possible except might occur?"
50987119,Backup Room database,"I'm trying to backup a room database programmatically.
For that, I'm simply copying the .sqlite file that contains the whole database
But, before copying, due to the fact that room has write ahead logging enabled, we must close the database so that -shm file and -wal file merge into a single .sqlite file. As pointed out here
I run .close() on RoomDatabase object: 
Everything works fine with the backup, BUT, later on, when I try to execute an INSERT query, I get this error: 
android.database.sqlite.SQLiteException: no such table: room_table_modification_log (code 1)
How can I properly re-open room db after I close it?
PS: .isOpen() on RoomDatabase object returns true before INSERT
Room version: 1.1.1-rc1
",<android><sqlite><android-room><sqliteopenhelper><android-architecture-components>,713,2,13,1022,1,12,24,50,16982,0.0,224,7,37,2018-06-22 11:39,2018-07-27 14:30,2018-07-27 14:30,35.0,35.0,Basic,9,"<android><sqlite><android-room><sqliteopenhelper><android-architecture-components>, Backup Room database, I'm trying to backup a room database programmatically.
For that, I'm simply copying the .sqlite file that contains the whole database
But, before copying, due to the fact that room has write ahead logging enabled, we must close the database so that -shm file and -wal file merge into a single .sqlite file. As pointed out here
I run .close() on RoomDatabase object: 
Everything works fine with the backup, BUT, later on, when I try to execute an INSERT query, I get this error: 
android.database.sqlite.SQLiteException: no such table: room_table_modification_log (code 1)
How can I properly re-open room db after I close it?
PS: .isOpen() on RoomDatabase object returns true before INSERT
Room version: 1.1.1-rc1
","<andros><quite><andros-room><sqliteopenhelper><andros-architecture-components>, back room database, i'm try back room database programmatically. that, i'm simple copy .quite file contain whole database but, copying, due fact room write ahead log enabled, must close database -she file -was file berg single .quite file. point run .close() roomdatabas object: every work fine back, but, later on, try execute insert query, get error: andros.database.quite.sqliteexception: table: room_table_modification_log (code 1) properly re-open room do close it? is: .open() roomdatabas object return true insert room version: 1.1.1-ran"
52320576,In MySQL SERVER 8.0 the PASSWORD function not working,"Error while executing the PASSWORD function in MySQL Server version 8.0.12
I have the following query:
SELECT * 
FROM users 
WHERE login = 'FABIO' 
  AND pwd = PASSWORD('2018') 
LIMIT 0, 50000
I am getting this error:
  Error Code: 1064. You have an error in your SQL syntax; check the
  manual that corresponds to your MySQL server version for the right
  syntax to use near
",<mysql><mysql-8.0>,376,0,6,499,1,4,11,36,41276,0.0,19,4,37,2018-09-13 19:30,2018-09-13 19:38,2018-09-13 19:38,0.0,0.0,Basic,2,"<mysql><mysql-8.0>, In MySQL SERVER 8.0 the PASSWORD function not working, Error while executing the PASSWORD function in MySQL Server version 8.0.12
I have the following query:
SELECT * 
FROM users 
WHERE login = 'FABIO' 
  AND pwd = PASSWORD('2018') 
LIMIT 0, 50000
I am getting this error:
  Error Code: 1064. You have an error in your SQL syntax; check the
  manual that corresponds to your MySQL server version for the right
  syntax to use near
","<myself><myself-8.0>, myself server 8.0 password function working, error execute password function myself server version 8.0.12 follow query: select * user login = 'habit' pad = password('2018') limit 0, 50000 get error: error code: 1064. error sal santa; check manual correspond myself server version right santa use near"
49963383,"""Authentication plugin 'caching_sha2_password'","I'm new to MySql environment and installed :
MySQL with the following commands:
sudo apt-get update
sudo apt-get install mysql-server
mysql_secure_installation
and also installed mysql workbench.
But when I'm trying to connect my localhost getting the follow error:
&quot;Authentication plugin 'caching_sha2_password' cannot be loaded: /usr/lib/mysql/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory&quot;
and even this is the first time I'm posting a question in stackoverflow, sorry for my presentation errors and syntax.
",<mysql>,566,1,3,373,1,3,8,49,64061,0.0,1,5,37,2018-04-22 7:00,2018-04-25 17:11,,3.0,,Advanced,38,"<mysql>, ""Authentication plugin 'caching_sha2_password', I'm new to MySql environment and installed :
MySQL with the following commands:
sudo apt-get update
sudo apt-get install mysql-server
mysql_secure_installation
and also installed mysql workbench.
But when I'm trying to connect my localhost getting the follow error:
&quot;Authentication plugin 'caching_sha2_password' cannot be loaded: /usr/lib/mysql/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory&quot;
and even this is the first time I'm posting a question in stackoverflow, sorry for my presentation errors and syntax.
","<myself>, ""authentic plain 'caching_sha2_password', i'm new myself environs instal : myself follow commands: so apt-get update so apt-get instal myself-serve mysql_secure_instal also instal myself workbench. i'm try connect localhost get follow error: &quit;authentic plain 'caching_sha2_password' cannot loaded: /us/limb/myself/plain/caching_sha2_password.so: cannot open share object file: file directory&quit; even first time i'm post question stackoverflow, sorry present error santa."
57872910,The LINQ expression could not be translated and will be evaluated locally,"Im getting this WARNING in EntityFramework Core what is wrong?
I already set MSSQL Datebase to Case Sensitive.
Latin1_General_100_CS_AS
var test = await _context.Students
                .FirstOrDefaultAsync(m =&gt; m.LastName.Equals(""ALEXANDER"", StringComparison.InvariantCultureIgnoreCase));
  Microsoft.EntityFrameworkCore.Query:Warning: The LINQ expression
  'where [m].LastName.Equals(""ALEXANDER"", InvariantCultureIgnoreCase)'
  could not be translated and will be evaluated locally.
",<c#><sql-server><entity-framework><linq><entity-framework-core>,489,0,2,371,1,3,6,81,70644,0.0,0,4,37,2019-09-10 14:24,2019-09-10 15:24,,0.0,,Advanced,35,"<c#><sql-server><entity-framework><linq><entity-framework-core>, The LINQ expression could not be translated and will be evaluated locally, Im getting this WARNING in EntityFramework Core what is wrong?
I already set MSSQL Datebase to Case Sensitive.
Latin1_General_100_CS_AS
var test = await _context.Students
                .FirstOrDefaultAsync(m =&gt; m.LastName.Equals(""ALEXANDER"", StringComparison.InvariantCultureIgnoreCase));
  Microsoft.EntityFrameworkCore.Query:Warning: The LINQ expression
  'where [m].LastName.Equals(""ALEXANDER"", InvariantCultureIgnoreCase)'
  could not be translated and will be evaluated locally.
","<c#><sal-server><entity-framework><line><entity-framework-core>, line express could translate value locally, in get warn entityframework core wrong? already set mssql dates case sensitive. latin1_general_100_cs_a war test = await context.stud .firstordefaultasync(m =&it; m.lastname.equals(""alexander"", stringcomparison.invariantcultureignorecase)); microsoft.entityframeworkcore.query:warning: line express 'where [m].lastname.equals(""alexander"", invariantcultureignorecase)' could translate value locally."
49922023,MYSQL 8.0 - unsupported redo log format,"I have recently updated mysql that was located under my xampp folder, and i've got the following errors, reporting from the log file :
2018-04-19T12:59:19.667059Z 0 [System] [MY-010116] [Server] C:\xampp\mysql\bin\mysqld.exe (mysqld 8.0.11) starting as process 9324
2018-04-19T12:59:20.025280Z 1 [ERROR] [MY-013090] [InnoDB] InnoDB: Unsupported redo log format (0). The redo log was created before MySQL 5.7.9
2018-04-19T12:59:20.026140Z 1 [ERROR] [MY-012930] [InnoDB] InnoDB: Plugin initialization aborted with error Generic error.
2018-04-19T12:59:20.229069Z 1 [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.
2018-04-19T12:59:20.230803Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
2018-04-19T12:59:20.231371Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-04-19T12:59:20.233136Z 0 [System] [MY-010910] [Server] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
I have been told if updating your mysql, you should comment out the deprecated configs that are located in your my.ini, but i had no idea what to comment out, so i left it as it is.
Any idea what causes this?
",<php><mysql><phpmyadmin><xampp>,1163,0,0,445,1,5,10,66,52141,0.0,14,4,36,2018-04-19 13:12,2018-05-17 21:52,2018-05-17 21:52,28.0,28.0,Advanced,38,"<php><mysql><phpmyadmin><xampp>, MYSQL 8.0 - unsupported redo log format, I have recently updated mysql that was located under my xampp folder, and i've got the following errors, reporting from the log file :
2018-04-19T12:59:19.667059Z 0 [System] [MY-010116] [Server] C:\xampp\mysql\bin\mysqld.exe (mysqld 8.0.11) starting as process 9324
2018-04-19T12:59:20.025280Z 1 [ERROR] [MY-013090] [InnoDB] InnoDB: Unsupported redo log format (0). The redo log was created before MySQL 5.7.9
2018-04-19T12:59:20.026140Z 1 [ERROR] [MY-012930] [InnoDB] InnoDB: Plugin initialization aborted with error Generic error.
2018-04-19T12:59:20.229069Z 1 [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.
2018-04-19T12:59:20.230803Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
2018-04-19T12:59:20.231371Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-04-19T12:59:20.233136Z 0 [System] [MY-010910] [Server] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
I have been told if updating your mysql, you should comment out the deprecated configs that are located in your my.ini, but i had no idea what to comment out, so i left it as it is.
Any idea what causes this?
","<pp><myself><phpmyadmin><camp>, myself 8.0 - support red log format, recent update myself local camp older, i'v got follow errors, report log file : 2018-04-19t12:59:19.667059z 0 [system] [my-010116] [server] c:\camp\myself\bin\myself.ex (myself 8.0.11) start process 9324 2018-04-19t12:59:20.025280z 1 [error] [my-013090] [innodb] innodb: support red log format (0). red log great myself 5.7.9 2018-04-19t12:59:20.026140z 1 [error] [my-012930] [innodb] innodb: plain into abort error genet error. 2018-04-19t12:59:20.229069z 1 [error] [my-011013] [server] fail into did storage engine. 2018-04-19t12:59:20.230803z 0 [error] [my-010020] [server] data dictionary into failed. 2018-04-19t12:59:20.231371z 0 [error] [my-010119] [server] abort 2018-04-19t12:59:20.233136z 0 [system] [my-010910] [server] c:\camp\myself\bin\myself.eye: shutdown complete (myself 8.0.11) myself common server - gal. told update myself, comment degree confirm local my.in, idea comment out, left is. idea cause this?"
50838199,Pyspark: Select all columns except particular columns,"I have a large number of columns in a PySpark dataframe, say 200. I want to select all the columns except say 3-4 of the columns. How do I select this columns without having to manually type the names of all the columns I want to select? 
",<python><sql><dataframe><pyspark>,239,0,0,7521,6,36,49,75,65342,0.0,1311,4,36,2018-06-13 13:13,2018-09-04 7:05,2018-09-04 7:05,83.0,83.0,Basic,2,"<python><sql><dataframe><pyspark>, Pyspark: Select all columns except particular columns, I have a large number of columns in a PySpark dataframe, say 200. I want to select all the columns except say 3-4 of the columns. How do I select this columns without having to manually type the names of all the columns I want to select? 
","<patron><sal><dataframe><spark>, spark: select column except particular columns, large number column spark dataframe, say 200. want select column except say 3-4 columns. select column without manual type name column want select?"
