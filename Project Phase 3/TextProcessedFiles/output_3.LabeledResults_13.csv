QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
49286687,"Managing database connections in Node.js, best practices?","I'm building an Node application which will query simple and more complex (multiple joins) queries. I'm looking for suggestions on how I should manage the mySQL connections. 
I have the following elements: 
server.js : Express
router1.js (fictive name) : Express Router middleware
router2.js (fictive name) : Express Router middleware
    //this is router1
    router.get('/', function (req, res){
    connection.connect(function(Err){...});
      connection.query('SELECT* FROM table WHERE id = ""blah""', function(err,results,fields){
        console.log(results);
      });
      ...
    connection.end();
    })
Should I connect to mysql everytime '/router1/' is requested, like in this example, or it's better to leave one connection open one at start up? As: connection.connect(); outside of:  router.get('/',function(req,res){
...
}); ?
",<mysql><node.js><database>,842,0,0,277,1,4,8,48,17538,,76,4,16,2018-03-14 19:59,2019-11-03 19:31,,599.0,,Intermediate,22,"<mysql><node.js><database>, Managing database connections in Node.js, best practices?, I'm building an Node application which will query simple and more complex (multiple joins) queries. I'm looking for suggestions on how I should manage the mySQL connections. 
I have the following elements: 
server.js : Express
router1.js (fictive name) : Express Router middleware
router2.js (fictive name) : Express Router middleware
    //this is router1
    router.get('/', function (req, res){
    connection.connect(function(Err){...});
      connection.query('SELECT* FROM table WHERE id = ""blah""', function(err,results,fields){
        console.log(results);
      });
      ...
    connection.end();
    })
Should I connect to mysql everytime '/router1/' is requested, like in this example, or it's better to leave one connection open one at start up? As: connection.connect(); outside of:  router.get('/',function(req,res){
...
}); ?
","<myself><node.is><database>, manage database connect node.is, best practices?, i'm build node applied query simple complex (multiple joins) queried. i'm look suggest manage myself connections. follow elements: server.j : express outer.j (victim name) : express outer middlewar outer.j (victim name) : express outer middlewar //the outer outer.get('/', function (red, yes){ connection.connect(function(err){...}); connection.query('select* table id = ""bah""', function(err,results,fields){ console.log(results); }); ... connection.end(); }) connect myself everytim '/outer/' requested, like example, better leave one connect open one start up? as: connection.connect(); outside of: outer.get('/',function(red,yes){ ... }); ?"
48974135,`Error: file is not a database` in SQLite,"I have created a database named database.db.
When I create a table in the database, I get the error Error: file is not a database as shown below:
nehal@nehal-Inspiron-5559:~/Desktop/UAV$ sqlite3 database.db 
SQLite version 3.20.1 2017-08-24 16:21:36
Enter &quot;.help&quot; for usage hints.
sqlite&gt; CREATE TABLE users(
   ...&gt; password varchar(10),
   ...&gt; email text,
   ...&gt; name text
   ...&gt; );
Error: file is not a database
How do I resolve the error?
",<database><sqlite>,471,0,11,407,2,5,11,54,48664,0.0,0,5,16,2018-02-25 13:44,2018-02-25 16:58,2018-02-25 16:58,0.0,0.0,Basic,9,"<database><sqlite>, `Error: file is not a database` in SQLite, I have created a database named database.db.
When I create a table in the database, I get the error Error: file is not a database as shown below:
nehal@nehal-Inspiron-5559:~/Desktop/UAV$ sqlite3 database.db 
SQLite version 3.20.1 2017-08-24 16:21:36
Enter &quot;.help&quot; for usage hints.
sqlite&gt; CREATE TABLE users(
   ...&gt; password varchar(10),
   ...&gt; email text,
   ...&gt; name text
   ...&gt; );
Error: file is not a database
How do I resolve the error?
","<database><quite>, `error: file database` quite, great database name database.do. great table database, get error error: file database shown below: near@near-inspired-5559:~/desktop/a$ sqlite3 database.do quite version 3.20.1 2017-08-24 16:21:36 enter &quit;.help&quit; usage hints. quite&it; great table users( ...&it; password varchar(10), ...&it; email text, ...&it; name text ...&it; ); error: file database resolve error?"
56319638,EntityFrameworkCore SQLite in-memory db tables are not created,"For integration tests I am using an EntityFrameworkCore SQLite in-memory db and creating its schema as per Microsoft docs, but when I attempt to seed data an exception is thrown that tables do not exist.
The mouse-over docs for DbContext.Database.EnsureCreated(); :
  Ensure that the database for the context exists.  If it exists, no
  action is taken.  If it does not exist then the database and all its
  schema are created.  If the database exists, then no action is made to
  ensure it is compatible with the model for this context.
I've read that an EntityFrameworkCore in-memory db only exists as long as an open connection exists, and so I tried explicitly creating a var connection = new SqliteConnection(""DataSource=:memory:""); instance and wrapping the below code in a using(connection) {} block and passing the connection instance options.UseSqlite(connection);, but DbContext.Database.EnsureCreated(); still doesn't create any db-objects
public class CustomWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;Startup&gt;
{
    protected override IWebHostBuilder CreateWebHostBuilder()
    {
        return WebHost.CreateDefaultBuilder()
            .UseStartup&lt;Startup&gt;();
    }
    protected override void ConfigureWebHost(IWebHostBuilder builder)
    {
      using (var connection = new SqliteConnection(""DataSource=MySharedInMemoryDb;mode=memory;cache=shared""))
      {
          connection.Open();
          builder.ConfigureServices(services =&gt;
          {
              var serviceProvider = new ServiceCollection()
                  .AddEntityFrameworkSqlite()
                  .BuildServiceProvider();
              services.AddDbContext&lt;MyDbContext&gt;(options =&gt;
              {
                  options.UseSqlite(connection);
                  options.UseInternalServiceProvider(serviceProvider);
              });
              var contextServiceProvider = services.BuildServiceProvider();
              // we need a scope to obtain a reference to the database contexts
              using (var scope = contextServiceProvider.CreateScope())
              {
                  var scopedProvider = scope.ServiceProvider;
                  var logger = scopedProvider.GetRequiredService&lt;ILogger&lt;CustomWebApplicationFactory&lt;TStartup&gt;&gt;&gt;();
                  using (var myDb = scopedProvider.GetRequiredService&lt;MyDbContext&gt;())
                  {
                      // DEBUG CODE
                      // this returns script to create db objects as expected
                      // proving that MyDbContext is setup correctly
                      var script = myDb.Database.GenerateCreateScript();
                      // DEBUG CODE
                      // this does not create the db objects ( tables etc )
                      // this is not as expected and contrary to ms docs
                      var result = myDb.Database.EnsureCreated();
                      try
                      {
                          SeedData.PopulateTestData(myDb);
                      }
                      catch (Exception e)
                      {
                          // exception is thrown that tables don't exist
                          logger.LogError(e, $""SeedData.PopulateTestData(myDb) threw exception=[{e.Message}]"");
                      }
                  }
              }
          });
        }
        builder.UseContentRoot(""."");
        base.ConfigureWebHost(builder);
    }
Please note that in this post I'm only asking the question why doesn't DbContext.Database.EnsureCreated(); create the schema as expected. I'm not presenting the above code as a general pattern for running integration-tests.
",<c#><sqlite><integration-testing><in-memory-database><entity-framework-core-2.2>,3702,0,71,8295,17,61,91,80,15929,0.0,96,1,16,2019-05-27 4:20,2019-05-29 20:07,,2.0,,Advanced,32,"<c#><sqlite><integration-testing><in-memory-database><entity-framework-core-2.2>, EntityFrameworkCore SQLite in-memory db tables are not created, For integration tests I am using an EntityFrameworkCore SQLite in-memory db and creating its schema as per Microsoft docs, but when I attempt to seed data an exception is thrown that tables do not exist.
The mouse-over docs for DbContext.Database.EnsureCreated(); :
  Ensure that the database for the context exists.  If it exists, no
  action is taken.  If it does not exist then the database and all its
  schema are created.  If the database exists, then no action is made to
  ensure it is compatible with the model for this context.
I've read that an EntityFrameworkCore in-memory db only exists as long as an open connection exists, and so I tried explicitly creating a var connection = new SqliteConnection(""DataSource=:memory:""); instance and wrapping the below code in a using(connection) {} block and passing the connection instance options.UseSqlite(connection);, but DbContext.Database.EnsureCreated(); still doesn't create any db-objects
public class CustomWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;Startup&gt;
{
    protected override IWebHostBuilder CreateWebHostBuilder()
    {
        return WebHost.CreateDefaultBuilder()
            .UseStartup&lt;Startup&gt;();
    }
    protected override void ConfigureWebHost(IWebHostBuilder builder)
    {
      using (var connection = new SqliteConnection(""DataSource=MySharedInMemoryDb;mode=memory;cache=shared""))
      {
          connection.Open();
          builder.ConfigureServices(services =&gt;
          {
              var serviceProvider = new ServiceCollection()
                  .AddEntityFrameworkSqlite()
                  .BuildServiceProvider();
              services.AddDbContext&lt;MyDbContext&gt;(options =&gt;
              {
                  options.UseSqlite(connection);
                  options.UseInternalServiceProvider(serviceProvider);
              });
              var contextServiceProvider = services.BuildServiceProvider();
              // we need a scope to obtain a reference to the database contexts
              using (var scope = contextServiceProvider.CreateScope())
              {
                  var scopedProvider = scope.ServiceProvider;
                  var logger = scopedProvider.GetRequiredService&lt;ILogger&lt;CustomWebApplicationFactory&lt;TStartup&gt;&gt;&gt;();
                  using (var myDb = scopedProvider.GetRequiredService&lt;MyDbContext&gt;())
                  {
                      // DEBUG CODE
                      // this returns script to create db objects as expected
                      // proving that MyDbContext is setup correctly
                      var script = myDb.Database.GenerateCreateScript();
                      // DEBUG CODE
                      // this does not create the db objects ( tables etc )
                      // this is not as expected and contrary to ms docs
                      var result = myDb.Database.EnsureCreated();
                      try
                      {
                          SeedData.PopulateTestData(myDb);
                      }
                      catch (Exception e)
                      {
                          // exception is thrown that tables don't exist
                          logger.LogError(e, $""SeedData.PopulateTestData(myDb) threw exception=[{e.Message}]"");
                      }
                  }
              }
          });
        }
        builder.UseContentRoot(""."");
        base.ConfigureWebHost(builder);
    }
Please note that in this post I'm only asking the question why doesn't DbContext.Database.EnsureCreated(); create the schema as expected. I'm not presenting the above code as a general pattern for running integration-tests.
","<c#><quite><integration-testing><in-memory-database><entity-framework-core-2.2>, entityframeworkcor quite in-memory do table created, inter test use entityframeworkcor quite in-memory do great scheme per microsoft docs, attempt seed data except thrown table exist. mouse-of do context.database.ensurecreated(); : ensue database context exists. exists, action taken. exist database scheme created. database exists, action made ensue compact model context. i'v read entityframeworkcor in-memory do exist long open connect exists, try explicitly great war connect = new sqliteconnection(""datasource=:memory:""); instant wrap code using(connection) {} block pass connect instant option.usesqlite(connection);, context.database.ensurecreated(); still great do-object public class customwebapplicationfactory&it;tstartup&it; : webapplicationfactory&it;started&it; { protect overdid iwebhostbuild createwebhostbuilder() { return webhost.createdefaultbuilder() .usestartup&it;started&it;(); } protect overdid void configurewebhost(iwebhostbuild builder) { use (war connect = new sqliteconnection(""datasource=mysharedinmemorydb;mode=memory;ache=shared"")) { connection.open(); builder.configureservices(service =&it; { war serviceprovid = new servicecollection() .addentityframeworksqlite() .buildserviceprovider(); services.adddbcontext&it;mydbcontext&it;(opt =&it; { option.usesqlite(connection); option.useinternalserviceprovider(serviceprovider); }); war contextserviceprovid = services.buildserviceprovider(); // need scope obtain refer database context use (war scope = contextserviceprovider.createscope()) { war scopedprovid = scope.serviceprovider; war longer = scopedprovider.getrequiredservice&it;longer&it;customwebapplicationfactory&it;tstartup&it;&it;&it;(); use (war my = scopedprovider.getrequiredservice&it;mydbcontext&it;()) { // debut code // return script great do object expect // prove mydbcontext set correctly war script = my.database.generatecreatescript(); // debut code // great do object ( table etc ) // expect contrary ms do war result = my.database.ensurecreated(); try { seeddata.populatetestdata(my); } catch (except e) { // except thrown table exist longer.logerror(e, $""seeddata.populatetestdata(my) threw exception=[{e.message}]""); } } } }); } builder.usecontentroot("".""); base.configurewebhost(builder); } pleas note post i'm ask question context.database.ensurecreated(); great scheme expected. i'm present code genet pattern run integration-tests."
54325397,How to connect MySQL database to ReactJS app?,"I have build a Todo App with create-react-app. The store I'm using is based on Local Storage(JS attribute of object window). Now I created a MySQL databases and want to connect to that database, so the state will show the values from database, and will be updated through actions. 
I've tried to connect to db and output values through 'node' console using db.js. It works.
const mysql = require('mysql');
const con = mysql.createConnection({
  host: ""localhost"",
  user: ""root"",
  password: ""root"",
  database: 'root'
});
con.connect(function(err) {
  if (err) throw err;
  con.query(""SELECT * FROM tasks"", function (err, result, fields) {
    if (err) throw err;
    console.log(result);
  });
});
Is it possible to connect the state of app to database using this script?
",<javascript><mysql><reactjs>,774,0,16,175,1,1,9,76,98021,0.0,3,2,16,2019-01-23 10:43,2019-01-23 10:45,2019-01-23 10:45,0.0,0.0,Basic,3,"<javascript><mysql><reactjs>, How to connect MySQL database to ReactJS app?, I have build a Todo App with create-react-app. The store I'm using is based on Local Storage(JS attribute of object window). Now I created a MySQL databases and want to connect to that database, so the state will show the values from database, and will be updated through actions. 
I've tried to connect to db and output values through 'node' console using db.js. It works.
const mysql = require('mysql');
const con = mysql.createConnection({
  host: ""localhost"",
  user: ""root"",
  password: ""root"",
  database: 'root'
});
con.connect(function(err) {
  if (err) throw err;
  con.query(""SELECT * FROM tasks"", function (err, result, fields) {
    if (err) throw err;
    console.log(result);
  });
});
Is it possible to connect the state of app to database using this script?
","<javascript><myself><reacts>, connect myself database react pp?, build too pp create-react-pp. store i'm use base local storage(j attribute object window). great myself database want connect database, state show value database, update actions. i'v try connect do output value 'node' console use do.is. works. cost myself = require('myself'); cost con = myself.createconnection({ host: ""localhost"", user: ""root"", password: ""root"", database: 'root' }); con.connect(function(err) { (err) throw err; con.query(""select * tasks"", function (err, result, fields) { (err) throw err; console.log(result); }); }); possible connect state pp database use script?"
50477550,Lookup against MYSQL TEXT type column,"My table/model has TEXT type column, and when filtering for the records on the model itself, the AR where produces the correct SQL and returns correct results, here is what I mean :
MyNamespace::MyValue.where(value: 'Good Quality')
Produces this SQL :
SELECT `my_namespace_my_values`.* 
FROM `my_namespace_my_values` 
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Take another example where I m joining MyNamespace::MyValue and filtering on the same value column but from the other model (has relation on the model to my_values). See this (query #2) :
OtherModel.joins(:my_values).where(my_values: { value: 'Good Quality' })
This does not produce correct query, this filters on the value column as if it was a String column and not Text, therefore producing incorrect results like so (only pasting relevant where) :
WHERE my_namespace_my_values`.`value` = 'Good Quality'
Now I can get past this by doing LIKE inside my AR where, which will produce the correct result but slightly different query. This is what I mean :
OtherModel.joins(:my_values).where('my_values.value LIKE ?, '%Good Quality%')
Finally arriving to my questions. What is this and how it's being generated for where on the model (for text column type)?
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Maybe most important question what is the difference in terms of performance using :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
and this :
(my_namespace_my_values.value LIKE '%Good Quality%')
and more importantly how do I get my query with joins (query #2) produce where like this :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
",<mysql><ruby-on-rails><activerecord>,1670,0,18,22672,36,134,182,63,578,0.0,1642,4,16,2018-05-22 22:58,2018-05-25 13:44,2018-05-30 14:54,3.0,8.0,Basic,2,"<mysql><ruby-on-rails><activerecord>, Lookup against MYSQL TEXT type column, My table/model has TEXT type column, and when filtering for the records on the model itself, the AR where produces the correct SQL and returns correct results, here is what I mean :
MyNamespace::MyValue.where(value: 'Good Quality')
Produces this SQL :
SELECT `my_namespace_my_values`.* 
FROM `my_namespace_my_values` 
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Take another example where I m joining MyNamespace::MyValue and filtering on the same value column but from the other model (has relation on the model to my_values). See this (query #2) :
OtherModel.joins(:my_values).where(my_values: { value: 'Good Quality' })
This does not produce correct query, this filters on the value column as if it was a String column and not Text, therefore producing incorrect results like so (only pasting relevant where) :
WHERE my_namespace_my_values`.`value` = 'Good Quality'
Now I can get past this by doing LIKE inside my AR where, which will produce the correct result but slightly different query. This is what I mean :
OtherModel.joins(:my_values).where('my_values.value LIKE ?, '%Good Quality%')
Finally arriving to my questions. What is this and how it's being generated for where on the model (for text column type)?
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Maybe most important question what is the difference in terms of performance using :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
and this :
(my_namespace_my_values.value LIKE '%Good Quality%')
and more importantly how do I get my query with joins (query #2) produce where like this :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
","<myself><ruby-on-rails><activerecord>, lockup myself text type column, table/model text type column, filter record model itself, ar produce correct sal return correct results, mean : mynamespace::value.where(value: 'good quality') produce sal : select `my_namespace_my_values`.* `my_namespace_my_values` `my_namespace_my_values`.`value` = '\\\""good quality\\\""' take not example join mynamespace::myvalu filter value column model (ha relate model my_values). see (query #2) : othermodel.joins(:my_values).where(my_values: { value: 'good quality' }) produce correct query, filter value column string column text, therefore produce incorrect result like (only past rule where) : my_namespace_my_values`.`value` = 'good quality' get past like inside ar where, produce correct result slightly differ query. mean : othermodel.joins(:my_values).where('my_values.value like ?, '%good quality%') final arrive questions. genet model (for text column type)? `my_namespace_my_values`.`value` = '\\\""good quality\\\""' may import question differ term perform use : `my_namespace_my_values`.`value` = '\\\""good quality\\\""' : (my_namespace_my_values.value like '%good quality%') importantly get query join (query #2) produce like : `my_namespace_my_values`.`value` = '\\\""good quality\\\""'"
52109026,Firestore schema versioning and backward compatibility with android app to prevent crashes,"Firestore a NOSQL is a Document oriented database. Now how to manage versioning of the data as I use it with Firebase SDK and Android applications?
For e.g. let's say I have a JSON schema that I launch with my 1.0.0version of my android app. Later 1.0.1 comes up where I have to add some extra fields for the newer documents. Since I changed the structure to have additional information, it only applies to new documents.
Therefore, using this logic, I can see my Android application must be able to deal with all versions of JSON tree if used against this project I create in the firebase console with Firestore. But this can be very painful right, that I have carry the deadweight of backward compatibility endlessly? Is there a way to have some sort of version like in protobuf or something the android app can send to firestore server side so that automatically we can do something to prevent crashes on the android app when it sees new fields? 
See also this thread, the kind of problem the engineer has posted. You can end up with this kind of problem as new fields get discovered in your JSON tree by the android app
Add new field or change the structure on all Firestore documents
Any suggestions for how we should go about this?
In node.js architecture we handle this with default-> v1.1/update or 
default-> v1.0/update, that way we can manage the routes.
But for android+firebase SKD-> talking to Firestore NOSQL, how do I manage the versioning of the json schema. 
",<android><firebase><nosql><google-cloud-firestore><versioning>,1477,1,0,597,0,7,18,43,1602,0.0,0,1,16,2018-08-31 5:35,2020-04-08 9:21,,586.0,,Advanced,32,"<android><firebase><nosql><google-cloud-firestore><versioning>, Firestore schema versioning and backward compatibility with android app to prevent crashes, Firestore a NOSQL is a Document oriented database. Now how to manage versioning of the data as I use it with Firebase SDK and Android applications?
For e.g. let's say I have a JSON schema that I launch with my 1.0.0version of my android app. Later 1.0.1 comes up where I have to add some extra fields for the newer documents. Since I changed the structure to have additional information, it only applies to new documents.
Therefore, using this logic, I can see my Android application must be able to deal with all versions of JSON tree if used against this project I create in the firebase console with Firestore. But this can be very painful right, that I have carry the deadweight of backward compatibility endlessly? Is there a way to have some sort of version like in protobuf or something the android app can send to firestore server side so that automatically we can do something to prevent crashes on the android app when it sees new fields? 
See also this thread, the kind of problem the engineer has posted. You can end up with this kind of problem as new fields get discovered in your JSON tree by the android app
Add new field or change the structure on all Firestore documents
Any suggestions for how we should go about this?
In node.js architecture we handle this with default-> v1.1/update or 
default-> v1.0/update, that way we can manage the routes.
But for android+firebase SKD-> talking to Firestore NOSQL, how do I manage the versioning of the json schema. 
","<andros><firebase><nose><goose-cloud-restore><versioning>, director scheme version backward compact andros pp prevent clashes, director nose document orient database. manage version data use fires sd andros applications? e.g. let' say son scheme launch 1.0.version andros pp. later 1.0.1 come add extra field newer documents. since change structure admit information, apply new documents. therefore, use logic, see andros applied must all deal version son tree use project great fires console restore. pain right, carry deadweight backward compact endless? way sort version like protobuf cometh andros pp send director server side automatic cometh prevent crash andros pp see new fields? see also thread, kind problem engine posted. end kind problem new field get disco son tree andros pp add new field change structure director document suggest go this? node.j architecture hand default-> ve.1/update default-> ve.0/update, way manage routes. andros+fires sad-> talk director nose, manage version son scheme."
54332942,spark windowing function VS group by performance issue,"I have a dataframe done like
| id | date      |  KPI_1 | ... | KPI_n
| 1  |2012-12-12 |   0.1  | ... |  0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46
| 4  |2012-12-14 |   0.2  | ... |  0.45 
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
we have 
X identifiers
a row for every identifier for a given date
n KPIs
I have to calculate some derived KPI for every row, and this KPI depends on the previous values of every ID.
Let's say my derived KPI is a diff, it would be:
| id | date      |  KPI_1 | ... | KPI_n | KPI_1_diff | KPI_n_diff
| 1  |2012-12-12 |   0.1  | ... |  0.5  |   0.1      | 0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4  |   0.2      |0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 |   0.66     | 0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46 |   0.2-0.1  | 0.46 - 0.66
| 4  |2012-12-13 |   0.2  | ... |  0.45  ...
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
Now: what I would do is:
val groupedDF = myDF.groupBy(""id"").agg(
    collect_list(struct(col(""date"",col(""KPI_1""))).as(""wrapped_KPI_1""),
    collect_list(struct(col(""date"",col(""KPI_2""))).as(""wrapped_KPI_2"")
    // up until nth KPI
)
I would get aggregated data such as:
[(""2012-12-12"",0.1),(""2012-12-12"",0.2) ...
Then I would sort these wrapped data, unwrap and map over these aggregated result with some UDF and produce the output (compute diffs and other statistics).
Another approach is to use the window functions such as:
val window = Window.partitionBy(col(""id"")).orderBy(col(""date"")).rowsBetween(Window.unboundedPreceding,0L) 
and do :
val windowedDF = df.select (
  col(""id""),
  col(""date""),
  col(""KPI_1""),
  collect_list(struct(col(""date""),col(""KPI_1""))).over(window),  
  collect_list(struct(col(""date""),col(""KPI_2""))).over(window)
   )   
This way I get:
[(""2012-12-12"",0.1)]
[(""2012-12-12"",0.1), (""2012-12-13"",0.1)]
...
That look nicer to process, but I suspect that repeating the window would produce unnecessary grouping and sorting for every KPI.
So here are the questions:
I'd rather go for the grouping approach?
Would I go for the window? If so what is the most efficient approach to do it?
",<apache-spark><apache-spark-sql>,2156,0,33,851,1,10,17,38,21302,0.0,348,2,16,2019-01-23 17:49,2019-01-24 7:28,2019-01-24 7:28,1.0,1.0,Intermediate,23,"<apache-spark><apache-spark-sql>, spark windowing function VS group by performance issue, I have a dataframe done like
| id | date      |  KPI_1 | ... | KPI_n
| 1  |2012-12-12 |   0.1  | ... |  0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46
| 4  |2012-12-14 |   0.2  | ... |  0.45 
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
we have 
X identifiers
a row for every identifier for a given date
n KPIs
I have to calculate some derived KPI for every row, and this KPI depends on the previous values of every ID.
Let's say my derived KPI is a diff, it would be:
| id | date      |  KPI_1 | ... | KPI_n | KPI_1_diff | KPI_n_diff
| 1  |2012-12-12 |   0.1  | ... |  0.5  |   0.1      | 0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4  |   0.2      |0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 |   0.66     | 0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46 |   0.2-0.1  | 0.46 - 0.66
| 4  |2012-12-13 |   0.2  | ... |  0.45  ...
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
Now: what I would do is:
val groupedDF = myDF.groupBy(""id"").agg(
    collect_list(struct(col(""date"",col(""KPI_1""))).as(""wrapped_KPI_1""),
    collect_list(struct(col(""date"",col(""KPI_2""))).as(""wrapped_KPI_2"")
    // up until nth KPI
)
I would get aggregated data such as:
[(""2012-12-12"",0.1),(""2012-12-12"",0.2) ...
Then I would sort these wrapped data, unwrap and map over these aggregated result with some UDF and produce the output (compute diffs and other statistics).
Another approach is to use the window functions such as:
val window = Window.partitionBy(col(""id"")).orderBy(col(""date"")).rowsBetween(Window.unboundedPreceding,0L) 
and do :
val windowedDF = df.select (
  col(""id""),
  col(""date""),
  col(""KPI_1""),
  collect_list(struct(col(""date""),col(""KPI_1""))).over(window),  
  collect_list(struct(col(""date""),col(""KPI_2""))).over(window)
   )   
This way I get:
[(""2012-12-12"",0.1)]
[(""2012-12-12"",0.1), (""2012-12-13"",0.1)]
...
That look nicer to process, but I suspect that repeating the window would produce unnecessary grouping and sorting for every KPI.
So here are the questions:
I'd rather go for the grouping approach?
Would I go for the window? If so what is the most efficient approach to do it?
","<apache-spark><apache-spark-sal>, spark window function vs group perform issue, datafram done like | id | date | kpi_1 | ... | pin | 1 |2012-12-12 | 0.1 | ... | 0.5 | 2 |2012-12-12 | 0.2 | ... | 0.4 | 3 |2012-12-12 | 0.66 | ... | 0.66 | 1 |2012-12-13 | 0.2 | ... | 0.46 | 4 |2012-12-14 | 0.2 | ... | 0.45 | ... | 55| 2013-03-15 | 0.5 | ... | 0.55 x identify row every identify given date n epi call derive epi every row, epi depend previous value every id. let' say derive epi if, would be: | id | date | kpi_1 | ... | pin | kpi_1_diff | kpi_n_diff | 1 |2012-12-12 | 0.1 | ... | 0.5 | 0.1 | 0.5 | 2 |2012-12-12 | 0.2 | ... | 0.4 | 0.2 |0.4 | 3 |2012-12-12 | 0.66 | ... | 0.66 | 0.66 | 0.66 | 1 |2012-12-13 | 0.2 | ... | 0.46 | 0.2-0.1 | 0.46 - 0.66 | 4 |2012-12-13 | 0.2 | ... | 0.45 ... | ... | 55| 2013-03-15 | 0.5 | ... | 0.55 now: would is: val grouped = my.group(""id"").age( collect_list(struck(col(""date"",col(""kpi_1""))).as(""wrapped_kpi_1""), collect_list(struck(col(""date"",col(""kpi_2""))).as(""wrapped_kpi_2"") // th epi ) would get agree data as: [(""2012-12-12"",0.1),(""2012-12-12"",0.2) ... would sort wrap data, wrap map agree result utf produce output (compute if statistics). not approach use window function as: val window = window.partition(col(""id"")).orderly(col(""date"")).rowsbetween(window.unboundedpreceding,ll) : val windoweddf = of.select ( col(""id""), col(""date""), col(""kpi_1""), collect_list(struck(col(""date""),col(""kpi_1""))).over(window), collect_list(struck(col(""date""),col(""kpi_2""))).over(window) ) way get: [(""2012-12-12"",0.1)] [(""2012-12-12"",0.1), (""2012-12-13"",0.1)] ... look nicer process, suspect repeat window would produce unnecessary group sort every epi. questions: i'd rather go group approach? would go window? effect approach it?"
54301624,[ERR_HTTP_HEADERS_SENT]: Cannot set headers after they are sent to the client,"I'm working with PostgreSQL and NodeJS with its ""PG Module"". 
CRUD works but sometimes doesn't update automatically the views when i save or delete some item. this is my code and I think that the error is here but i cannot find it, i tried everything :'( 
Error Message:
const controller = {};
const { Pool } = require('pg');
var connectionString = 'postgres://me:system@localhost/recipebookdb';
const pool = new Pool({
    connectionString: connectionString,
})
controller.list = (request, response) =&gt; {
    pool.query('SELECT * FROM recipes', (err, result) =&gt; {
        if (err) {
            return next(err);
        }
           return response.render('recipes', { data: result.rows });
    });
};
controller.save = (req, res) =&gt; {
    pool.query('INSERT INTO recipes(name, ingredients, directions) VALUES ($1, $2, $3)',
        [req.body.name, req.body.ingredients, req.body.directions]);
    return res.redirect('/');
};
controller.delete = (req, res) =&gt; {
    pool.query('DELETE FROM RECIPES WHERE ID = $1', [req.params.id]);
    return res.redirect('/');
}
module.exports = controller;
PD: CRUD works but sometimes appears that error.
",<javascript><node.js><postgresql>,1157,1,29,256,1,3,16,63,71152,0.0,40,3,16,2019-01-22 5:13,2019-01-22 5:24,2019-01-22 5:52,0.0,0.0,Basic,13,"<javascript><node.js><postgresql>, [ERR_HTTP_HEADERS_SENT]: Cannot set headers after they are sent to the client, I'm working with PostgreSQL and NodeJS with its ""PG Module"". 
CRUD works but sometimes doesn't update automatically the views when i save or delete some item. this is my code and I think that the error is here but i cannot find it, i tried everything :'( 
Error Message:
const controller = {};
const { Pool } = require('pg');
var connectionString = 'postgres://me:system@localhost/recipebookdb';
const pool = new Pool({
    connectionString: connectionString,
})
controller.list = (request, response) =&gt; {
    pool.query('SELECT * FROM recipes', (err, result) =&gt; {
        if (err) {
            return next(err);
        }
           return response.render('recipes', { data: result.rows });
    });
};
controller.save = (req, res) =&gt; {
    pool.query('INSERT INTO recipes(name, ingredients, directions) VALUES ($1, $2, $3)',
        [req.body.name, req.body.ingredients, req.body.directions]);
    return res.redirect('/');
};
controller.delete = (req, res) =&gt; {
    pool.query('DELETE FROM RECIPES WHERE ID = $1', [req.params.id]);
    return res.redirect('/');
}
module.exports = controller;
PD: CRUD works but sometimes appears that error.
","<javascript><node.is><postgresql>, [err_http_headers_sent]: cannot set header sent client, i'm work postgresql nodes ""pg module"". crude work sometime update automatic view save delete item. code think error cannot find it, try every :'( error message: cost control = {}; cost { pool } = require('pg'); war connections = 'postures://me:system@localhost/recipebookdb'; cost pool = new pool({ connectionstring: connectionstring, }) controller.list = (request, response) =&it; { pool.query('select * recipe', (err, result) =&it; { (err) { return next(err); } return response.render('recipe', { data: result.row }); }); }; controller.say = (red, yes) =&it; { pool.query('insert recipe(name, ingredient, directions) value ($1, $2, $3)', [red.body.name, red.body.ingredient, red.body.directions]); return yes.direct('/'); }; controller.delete = (red, yes) =&it; { pool.query('delete recipe id = $1', [red.parts.id]); return yes.direct('/'); } module.export = controller; pp: crude work sometime appear error."
53934294,How to count null values in postgresql?,"select distinct ""column"" from table;
output:
    column
1     0.0
2     [null]
3     1.0
But when I try to count the null values
select count(""column"") from train where ""column"" is NULL;
Gives output 0 (zero)
Can you suggest where it's going wrong?
",<sql><postgresql><count><null>,249,0,6,7771,25,82,130,53,25494,0.0,121,6,16,2018-12-26 16:01,2018-12-26 16:03,2018-12-26 16:03,0.0,0.0,Basic,9,"<sql><postgresql><count><null>, How to count null values in postgresql?, select distinct ""column"" from table;
output:
    column
1     0.0
2     [null]
3     1.0
But when I try to count the null values
select count(""column"") from train where ""column"" is NULL;
Gives output 0 (zero)
Can you suggest where it's going wrong?
","<sal><postgresql><count><null>, count null value postgresql?, select distinct ""column"" table; output: column 1 0.0 2 [null] 3 1.0 try count null value select count(""column"") train ""column"" null; give output 0 (zero) suggest go wrong?"
56977456,varchar is missing on pgadmin4 UI,"I am very new on Postgresql. I have installed pgadmin4 on my Mac OS X. I try to create a very basic table with a column which has type varchar but it doesn't seem to have ti on the list of the UI.
Am I missing something?
",<postgresql><macos><pgadmin>,221,1,1,2309,7,31,63,81,14545,0.0,3517,1,16,2019-07-10 19:31,2019-07-16 14:55,2019-07-16 14:55,6.0,6.0,Intermediate,15,"<postgresql><macos><pgadmin>, varchar is missing on pgadmin4 UI, I am very new on Postgresql. I have installed pgadmin4 on my Mac OS X. I try to create a very basic table with a column which has type varchar but it doesn't seem to have ti on the list of the UI.
Am I missing something?
","<postgresql><faces><pgadmin>, varchar miss pgadmin4 i, new postgresql. instal pgadmin4 mac os x. try great basic table column type varchar seem ti list i. miss something?"
53526505,Granting Full SQL Server Permissions for a Database,"How can I give a user (who was created WITHOUT LOGIN) full control over a contained database without having to specify that database's name, like GRANT CONTROL ON DATABASE::DatabaseName TO UserName, but without using a database name?  I figure it'll include GRANT ALTER ANY SCHEMA TO UserName, but I'm not sure what else I'd need to grant, or if there's a better way.  Thanks.
",<sql-server><permissions><roles>,377,0,0,657,1,6,17,54,38229,0.0,13,2,16,2018-11-28 19:10,2018-11-28 19:35,2018-11-28 19:35,0.0,0.0,Basic,5,"<sql-server><permissions><roles>, Granting Full SQL Server Permissions for a Database, How can I give a user (who was created WITHOUT LOGIN) full control over a contained database without having to specify that database's name, like GRANT CONTROL ON DATABASE::DatabaseName TO UserName, but without using a database name?  I figure it'll include GRANT ALTER ANY SCHEMA TO UserName, but I'm not sure what else I'd need to grant, or if there's a better way.  Thanks.
","<sal-server><permission><roles>, grant full sal server permits database, give user (who great without login) full control contain database without specific database' name, like grant control database::databasenam surname, without use database name? figure it'll include grant alter scheme surname, i'm sure else i'd need grant, there' better way. thanks."
56897416,HikariPool-1 - Exception during pool initialization,"I'm deploying Spring-Boot4 project and I have got an error called HikariPool-1 - Exception during pool initialization.
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.3.10.Final}
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
2019-07-05 11:29:43.792  INFO 5084 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.4.Final}
2019-07-05 11:29:43.877  INFO 5084 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2019-07-05 11:29:44.964 ERROR 5084 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: database ""test2"" does not exist
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2559) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.&lt;init&gt;(QueryExecutorImpl.java:133) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:250) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.makeConnection(Driver.java:454) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.connect(Driver.java:256) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) [HikariCP-3.2.0.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) ~[classes/:na]
2019-07-05 11:29:44.980  WARN 5084 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : FATAL: database ""test2"" does not exist
2019-07-05 11:29:44.980  WARN 5084 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
2019-07-05 11:29:44.980  INFO 5084 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2019-07-05 11:29:45.011  INFO 5084 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2019-07-05 11:29:45.027 ERROR 5084 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) [classes/:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:275) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    ... 16 common frames omitted
Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.determineDialect(DialectFactoryImpl.java:100) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.buildDialect(DialectFactoryImpl.java:54) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:137) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    ... 33 common frames omitted
Process finished with exit code 1
I can't understand what does it means. I have already searched for the error in StackOverflow. There are some questions about HikariPool but none of them are working for me.
My application.properties is as follows,
spring.datasource.url=jdbc:postgresql://localhost/test2
spring.datasource.username=postgres
spring.datasource.password=root
spring.jpa.generate-ddl=true
My pom.xml dependancies are,
&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
",<spring><postgresql><spring-boot><hikaricp>,19330,0,147,3977,11,43,64,78,127318,0.0,2342,6,16,2019-07-05 6:12,2019-07-05 6:29,2019-07-05 13:50,0.0,0.0,Basic,9,"<spring><postgresql><spring-boot><hikaricp>, HikariPool-1 - Exception during pool initialization, I'm deploying Spring-Boot4 project and I have got an error called HikariPool-1 - Exception during pool initialization.
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.3.10.Final}
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
2019-07-05 11:29:43.792  INFO 5084 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.4.Final}
2019-07-05 11:29:43.877  INFO 5084 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2019-07-05 11:29:44.964 ERROR 5084 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: database ""test2"" does not exist
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2559) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.&lt;init&gt;(QueryExecutorImpl.java:133) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:250) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.makeConnection(Driver.java:454) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.connect(Driver.java:256) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) [HikariCP-3.2.0.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) ~[classes/:na]
2019-07-05 11:29:44.980  WARN 5084 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : FATAL: database ""test2"" does not exist
2019-07-05 11:29:44.980  WARN 5084 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
2019-07-05 11:29:44.980  INFO 5084 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2019-07-05 11:29:45.011  INFO 5084 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2019-07-05 11:29:45.027 ERROR 5084 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) [classes/:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:275) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    ... 16 common frames omitted
Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.determineDialect(DialectFactoryImpl.java:100) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.buildDialect(DialectFactoryImpl.java:54) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:137) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    ... 33 common frames omitted
Process finished with exit code 1
I can't understand what does it means. I have already searched for the error in StackOverflow. There are some questions about HikariPool but none of them are working for me.
My application.properties is as follows,
spring.datasource.url=jdbc:postgresql://localhost/test2
spring.datasource.username=postgres
spring.datasource.password=root
spring.jpa.generate-ddl=true
My pom.xml dependancies are,
&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
","<spring><postgresql><spring-boot><hikaricp>, hikaripool-1 - except pool initialization, i'm deploy spring-boots project got error call hikaripool-1 - except pool initialization. 2019-07-05 11:29:43.600 into 5084 --- [ main] org.liberate.very : hhh000412: wiberd core {5.3.10.final} 2019-07-05 11:29:43.600 into 5084 --- [ main] org.liberate.cf.environs : hhh000206: liberate.property found 2019-07-05 11:29:43.792 into 5084 --- [ main] o.liberate.innovations.common.very : hcann000001: wiberd common cannot {5.0.4.final} 2019-07-05 11:29:43.877 into 5084 --- [ main] com.baxter.kari.hikaridatasourc : hikaripool-1 - starting... 2019-07-05 11:29:44.964 error 5084 --- [ main] com.baxter.kari.pool.hikaripool : hikaripool-1 - except pool initialization. org.postgresql.until.psqlexception: fatal: database ""test"" exist org.postgresql.core.ve.queryexecutorimpl.receiveerrorresponse(queryexecutorimpl.cava:2440) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.ve.queryexecutorimpl.readstartupmessages(queryexecutorimpl.cava:2559) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.ve.queryexecutorimpl.&it;knit&it;(queryexecutorimpl.cava:133) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.ve.connectionfactoryimpl.openconnectionimpl(connectionfactoryimpl.cava:250) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.core.connectionfactory.openconnection(connectionfactory.cava:49) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.job.connection.&it;knit&it;(connection.cava:195) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.driver.makeconnection(driver.cava:454) ~[postgresql-42.2.5.jar:42.2.5] org.postgresql.driver.connect(driver.cava:256) ~[postgresql-42.2.5.jar:42.2.5] com.baxter.kari.until.driverdatasource.getconnection(driverdatasource.cava:136) ~[hikaricp-3.2.0.jar:na] com.baxter.kari.pool.poolbase.newconnection(poolbase.cava:369) ~[hikaricp-3.2.0.jar:na] com.baxter.kari.pool.poolbase.newpoolentry(poolbase.cava:198) ~[hikaricp-3.2.0.jar:na] com.baxter.kari.pool.hikaripool.createpoolentry(hikaripool.cava:467) [hikaricp-3.2.0.jar:na] com.baxter.kari.pool.hikaripool.checkfailfast(hikaripool.cava:541) [hikaricp-3.2.0.jar:na] com.baxter.kari.pool.hikaripool.&it;knit&it;(hikaripool.cava:115) [hikaricp-3.2.0.jar:na] com.baxter.kari.hikaridatasource.getconnection(hikaridatasource.cava:112) [hikaricp-3.2.0.jar:na] org.liberate.engine.job.connections.internal.datasourceconnectionproviderimpl.getconnection(datasourceconnectionproviderimpl.cava:122) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.end.internal.jdbcenvironmentinitiator$connectionproviderjdbcconnectionaccess.obtainconnection(jdbcenvironmentinitiator.cava:180) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.end.internal.jdbcenvironmentinitiator.initiateservice(jdbcenvironmentinitiator.cava:68) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.end.internal.jdbcenvironmentinitiator.initiateservice(jdbcenvironmentinitiator.cava:35) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.registry.internal.standardserviceregistryimpl.initiateservice(standardserviceregistryimpl.cava:94) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.createservice(abstractserviceregistryimpl.cava:263) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.initializeservice(abstractserviceregistryimpl.cava:237) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.getservice(abstractserviceregistryimpl.cava:214) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.id.factory.internal.defaultidentifiergeneratorfactory.injectservices(defaultidentifiergeneratorfactory.cava:152) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.injectdependencies(abstractserviceregistryimpl.cava:286) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.initializeservice(abstractserviceregistryimpl.cava:243) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.getservice(abstractserviceregistryimpl.cava:214) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.internal.inflightmetadatacollectorimpl.&it;knit&it;(inflightmetadatacollectorimpl.cava:179) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.model.process.spy.metadatabuildingprocess.complete(metadatabuildingprocess.cava:119) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.metadata(entitymanagerfactorybuilderimpl.cava:904) [liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.build(entitymanagerfactorybuilderimpl.cava:935) [liberate-core-5.3.10.final.jar:5.3.10.final] org.springframework.or.pa.vendor.springhibernatejpapersistenceprovider.createcontainerentitymanagerfactory(springhibernatejpapersistenceprovider.cava:57) [spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.createnativeentitymanagerfactory(localcontainerentitymanagerfactorybean.cava:365) [spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.abstractentitymanagerfactorybean.buildnativeentitymanagerfactory(abstractentitymanagerfactorybean.cava:390) [spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.abstractentitymanagerfactorybean.afterpropertiesset(abstractentitymanagerfactorybean.cava:377) [spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.afterpropertiesset(localcontainerentitymanagerfactorybean.cava:341) [spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.invokeinitmethods(abstractautowirecapablebeanfactory.cava:1837) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1774) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:593) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:515) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:320) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:318) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) [spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1105) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:867) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:549) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.boot.web.serve.context.servletwebserverapplicationcontext.refresh(servletwebserverapplicationcontext.cava:140) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.refresh(springapplication.cava:742) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.refreshcontext(springapplication.cava:389) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:311) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:1213) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:1202) ~[spring-boot-2.1.6.release.jar:2.1.6.release] com.example.test.test6application.main(test6application.cava:10) ~[classes/:na] 2019-07-05 11:29:44.980 warn 5084 --- [ main] o.h.e.j.e.i.jdbcenvironmentiniti : hhh000342: could obtain connect query metadata : fatal: database ""test"" exist 2019-07-05 11:29:44.980 warn 5084 --- [ main] configservletwebserverapplicationcontext : except count context into - cancel refresh attempt: org.springframework.beans.factory.beancreationexception: error great bean name 'entitymanagerfactory' define class path resource [org/springframework/boot/autoconfigure/or/pa/hibernatejpaconfiguration.class]: into knit method failed; nest except org.liberate.service.spy.serviceexception: unable great request service [org.liberate.engine.job.end.spy.jdbcenvironment] 2019-07-05 11:29:44.980 into 5084 --- [ main] o.apache.carolina.core.standardservic : stop service [combat] 2019-07-05 11:29:45.011 into 5084 --- [ main] conditionevaluationreportlogginglisten : error start applicationcontext. display conduct report re-run applied 'debut' enabled. 2019-07-05 11:29:45.027 error 5084 --- [ main] o.s.boot.springappl : applied run fail org.springframework.beans.factory.beancreationexception: error great bean name 'entitymanagerfactory' define class path resource [org/springframework/boot/autoconfigure/or/pa/hibernatejpaconfiguration.class]: into knit method failed; nest except org.liberate.service.spy.serviceexception: unable great request service [org.liberate.engine.job.end.spy.jdbcenvironment] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1778) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:593) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:515) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:320) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:318) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1105) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:867) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:549) ~[spring-context-5.1.8.release.jar:5.1.8.release] org.springframework.boot.web.serve.context.servletwebserverapplicationcontext.refresh(servletwebserverapplicationcontext.cava:140) ~[spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.refresh(springapplication.cava:742) [spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.refreshcontext(springapplication.cava:389) [spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:311) [spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:1213) [spring-boot-2.1.6.release.jar:2.1.6.release] org.springframework.boot.springapplication.run(springapplication.cava:1202) [spring-boot-2.1.6.release.jar:2.1.6.release] com.example.test.test6application.main(test6application.cava:10) [classes/:na] cause by: org.liberate.service.spy.serviceexception: unable great request service [org.liberate.engine.job.end.spy.jdbcenvironment] org.liberate.service.internal.abstractserviceregistryimpl.createservice(abstractserviceregistryimpl.cava:275) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.initializeservice(abstractserviceregistryimpl.cava:237) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.getservice(abstractserviceregistryimpl.cava:214) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.id.factory.internal.defaultidentifiergeneratorfactory.injectservices(defaultidentifiergeneratorfactory.cava:152) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.injectdependencies(abstractserviceregistryimpl.cava:286) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.initializeservice(abstractserviceregistryimpl.cava:243) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.getservice(abstractserviceregistryimpl.cava:214) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.internal.inflightmetadatacollectorimpl.&it;knit&it;(inflightmetadatacollectorimpl.cava:179) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.model.process.spy.metadatabuildingprocess.complete(metadatabuildingprocess.cava:119) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.metadata(entitymanagerfactorybuilderimpl.cava:904) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.build(entitymanagerfactorybuilderimpl.cava:935) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.springframework.or.pa.vendor.springhibernatejpapersistenceprovider.createcontainerentitymanagerfactory(springhibernatejpapersistenceprovider.cava:57) ~[spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.createnativeentitymanagerfactory(localcontainerentitymanagerfactorybean.cava:365) ~[spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.abstractentitymanagerfactorybean.buildnativeentitymanagerfactory(abstractentitymanagerfactorybean.cava:390) ~[spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.abstractentitymanagerfactorybean.afterpropertiesset(abstractentitymanagerfactorybean.cava:377) ~[spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.afterpropertiesset(localcontainerentitymanagerfactorybean.cava:341) ~[spring-or-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.invokeinitmethods(abstractautowirecapablebeanfactory.cava:1837) ~[spring-beans-5.1.8.release.jar:5.1.8.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1774) ~[spring-beans-5.1.8.release.jar:5.1.8.release] ... 16 common frame omit cause by: org.liberate.hibernateexception: access dialectresolutioninfo cannot null 'liberate.dialect' set org.liberate.engine.job.dialect.internal.dialectfactoryimpl.determinedialect(dialectfactoryimpl.cava:100) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.dialect.internal.dialectfactoryimpl.builddialect(dialectfactoryimpl.cava:54) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.end.internal.jdbcenvironmentinitiator.initiateservice(jdbcenvironmentinitiator.cava:137) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.engine.job.end.internal.jdbcenvironmentinitiator.initiateservice(jdbcenvironmentinitiator.cava:35) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.boot.registry.internal.standardserviceregistryimpl.initiateservice(standardserviceregistryimpl.cava:94) ~[liberate-core-5.3.10.final.jar:5.3.10.final] org.liberate.service.internal.abstractserviceregistryimpl.createservice(abstractserviceregistryimpl.cava:263) ~[liberate-core-5.3.10.final.jar:5.3.10.final] ... 33 common frame omit process finish exit code 1 can't understand means. already search error stackoverflow. question hikaripool none work me. application.property follows, spring.datasource.curl=job:postgresql://localhost/test spring.datasource.surname=poster spring.datasource.password=root spring.pa.generate-del=true pot.all depend are, &it;dependencies&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-data-pa&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-web&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.postgresql&it;/grouped&it; &it;artifactid&it;postgresql&it;/artifactid&it; &it;scope&it;auntie&it;/scope&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-test&it;/artifactid&it; &it;scope&it;test&it;/scope&it; &it;/dependency&it; &it;/dependencies&it;"
49095292,Exclude primary key attributes from a sequelize query,"I have a sequelize query from multiple tables inner joined together. I need to group them by on a nested include model but the sequelize query throws the primary key every time, even if I mention the attributes as: attributes:[].
However attributes:[] is working for nested include models.
",<mysql><node.js><sequelize.js>,290,0,4,195,1,1,10,69,33371,0.0,26,4,16,2018-03-04 12:16,2018-03-04 18:12,,0.0,,Basic,10,"<mysql><node.js><sequelize.js>, Exclude primary key attributes from a sequelize query, I have a sequelize query from multiple tables inner joined together. I need to group them by on a nested include model but the sequelize query throws the primary key every time, even if I mention the attributes as: attributes:[].
However attributes:[] is working for nested include models.
","<myself><node.is><sequelae.is>, exclude primary key attribute sequel query, sequel query multiple table inner join together. need group nest include model sequel query throw primary key every time, even mention attribute as: attributes:[]. howe attributes:[] work nest include models."
58206572,AWS RDS Mysql kill transaction by ID (without thread_id),"I'm currently having an issue with some long running transactions that are holding locks to a single row in a table that I'm being unable to kill.
Innodb_trx holds the following information - special attention to thread_id = null;
select * from information_schema.innodb_trx\G
*************************** 2. row ***************************
                    trx_id: 153261728
                 trx_state: RUNNING
               trx_started: 2019-10-02 10:05:42
     trx_requested_lock_id: NULL
          trx_wait_started: NULL
                trx_weight: 5
       trx_mysql_thread_id: 0
                 trx_query: NULL
       trx_operation_state: NULL
         trx_tables_in_use: 0
         trx_tables_locked: 2
          trx_lock_structs: 3
     trx_lock_memory_bytes: 1136
           trx_rows_locked: 1
         trx_rows_modified: 2
   trx_concurrency_tickets: 0
       trx_isolation_level: READ COMMITTED
         trx_unique_checks: 1
    trx_foreign_key_checks: 1
trx_last_foreign_key_error: NULL
 trx_adaptive_hash_latched: 0
 trx_adaptive_hash_timeout: 0
          trx_is_read_only: 0
trx_autocommit_non_locking: 0
and
show engine innodb status\G;
------------------
---TRANSACTION 153261728, ACTIVE (PREPARED) 27716 sec recovered trx
3 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 2
---TRANSACTION 96697370, ACTIVE (PREPARED) 988082 sec recovered trx
0 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 3
Is there as way to kill/stop/rollback a transaction like this?
",<mysql><amazon-rds>,1510,0,32,168,0,0,6,49,1641,0.0,1,1,16,2019-10-02 17:39,2022-10-07 1:53,,1101.0,,Basic,10,"<mysql><amazon-rds>, AWS RDS Mysql kill transaction by ID (without thread_id), I'm currently having an issue with some long running transactions that are holding locks to a single row in a table that I'm being unable to kill.
Innodb_trx holds the following information - special attention to thread_id = null;
select * from information_schema.innodb_trx\G
*************************** 2. row ***************************
                    trx_id: 153261728
                 trx_state: RUNNING
               trx_started: 2019-10-02 10:05:42
     trx_requested_lock_id: NULL
          trx_wait_started: NULL
                trx_weight: 5
       trx_mysql_thread_id: 0
                 trx_query: NULL
       trx_operation_state: NULL
         trx_tables_in_use: 0
         trx_tables_locked: 2
          trx_lock_structs: 3
     trx_lock_memory_bytes: 1136
           trx_rows_locked: 1
         trx_rows_modified: 2
   trx_concurrency_tickets: 0
       trx_isolation_level: READ COMMITTED
         trx_unique_checks: 1
    trx_foreign_key_checks: 1
trx_last_foreign_key_error: NULL
 trx_adaptive_hash_latched: 0
 trx_adaptive_hash_timeout: 0
          trx_is_read_only: 0
trx_autocommit_non_locking: 0
and
show engine innodb status\G;
------------------
---TRANSACTION 153261728, ACTIVE (PREPARED) 27716 sec recovered trx
3 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 2
---TRANSACTION 96697370, ACTIVE (PREPARED) 988082 sec recovered trx
0 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 3
Is there as way to kill/stop/rollback a transaction like this?
","<myself><amazon-rd>, a rd myself kill transact id (without thread_id), i'm current issue long run transact hold lock single row table i'm unable kill. innodb_trx hold follow inform - special attend thread_id = null; select * information_schema.innodb_trx\g *************************** 2. row *************************** trx_id: 153261728 trx_state: run trx_started: 2019-10-02 10:05:42 trx_requested_lock_id: null trx_wait_started: null trx_weight: 5 trx_mysql_thread_id: 0 trx_query: null trx_operation_state: null trx_tables_in_use: 0 trx_tables_locked: 2 trx_lock_structs: 3 trx_lock_memory_bytes: 1136 trx_rows_locked: 1 trx_rows_modified: 2 trx_concurrency_tickets: 0 trx_isolation_level: read commit trx_unique_checks: 1 trx_foreign_key_checks: 1 trx_last_foreign_key_error: null trx_adaptive_hash_latched: 0 trx_adaptive_hash_timeout: 0 trx_is_read_only: 0 trx_autocommit_non_locking: 0 show engine innodb status\g; ------------------ ---transact 153261728, active (prepared) 27716 see remove try 3 lock struck(s), heap size 1136, 1 row lock(s), undo log entry 2 ---transact 96697370, active (prepared) 988082 see remove try 0 lock struck(s), heap size 1136, 3 row lock(s), undo log entry 3 way kill/stop/rollback transact like this?"
52647648,Remove last n chars of a varchar column,"I wanted to know if there's a ""compact"" way of deleting last n chars from a column in PostGreSQL.
I have a column born as a DATE formatted like this: yyyy-MM-dd.
I altered the table to make this column a varchar, replaced all of the unnecessary dashes but I can't think about a quick and reliable way to delete the last two chars since I'd like the varchar to show only yyyyMM.
With ""quick and reliable"" I mean something that won't ask me to use a temp table.
Thanks in advance
",<postgresql>,478,0,0,665,4,10,33,53,14820,0.0,20,2,16,2018-10-04 13:12,2018-10-04 13:48,2018-10-04 13:48,0.0,0.0,Basic,2,"<postgresql>, Remove last n chars of a varchar column, I wanted to know if there's a ""compact"" way of deleting last n chars from a column in PostGreSQL.
I have a column born as a DATE formatted like this: yyyy-MM-dd.
I altered the table to make this column a varchar, replaced all of the unnecessary dashes but I can't think about a quick and reliable way to delete the last two chars since I'd like the varchar to show only yyyyMM.
With ""quick and reliable"" I mean something that won't ask me to use a temp table.
Thanks in advance
","<postgresql>, remove last n chair varchar column, want know there' ""compact"" way delete last n chair column postgresql. column born date format like this: yyyy-mm-did. alter table make column varchar, replace unnecessary dash can't think quick reliable way delete last two chair since i'd like varchar show yyyymm. ""quick reliable"" mean cometh ask use hemp table. thank advance"
54844651,I dont know how Postgresql created user on my mac,"Two days ago, i started learning postgresql. Most tutorials I followed online were either old or the codes wont just work on my mac. I followed a lot of tutorials that did a lot of totally different things.
When i switched on my system today. I noticed Postgresql created a user on my mac. I don't know what this is or maybe i used the wrong CLI code.
When I tried viewing the user, I saw this
should I delete this user or it has a function?
",<postgresql><macos><terminal>,442,2,0,423,2,5,13,40,20634,0.0,11,2,16,2019-02-23 18:20,2019-02-23 20:04,2019-02-23 20:04,0.0,0.0,Basic,14,"<postgresql><macos><terminal>, I dont know how Postgresql created user on my mac, Two days ago, i started learning postgresql. Most tutorials I followed online were either old or the codes wont just work on my mac. I followed a lot of tutorials that did a lot of totally different things.
When i switched on my system today. I noticed Postgresql created a user on my mac. I don't know what this is or maybe i used the wrong CLI code.
When I tried viewing the user, I saw this
should I delete this user or it has a function?
","<postgresql><faces><terminal>, dont know postgresql great user mac, two day ago, start learn postgresql. tutor follow online either old code wont work mac. follow lot tutor lot total differ things. switch system today. notice postgresql great user mac. know may use wrong coli code. try view user, saw delete user function?"
50093776,"SQLITE, Create a temp table then select from it","just wondering how i can create a temp table and then select from it further down the script.
Example. 
CREATE TEMPORARY TABLE TEMP_TABLE1 AS 
Select 
L.ID,
SUM(L.cost)/2 as Costs,
from Table1 L
JOIN Table2 C on L.ID = C.ID
Where C.name  = 'mike'
Group by L.ID
Select 
Count(L.ID)
from Table1 L
JOIN TEMP_TABLE1 TT1 on L.ID = TT1.ID;
Where L.ID not in (TT1) 
And Sum(L.Cost) &gt; TT1.Costs
Ideally I want to have a temp table then use it later in the script to reference from.
Any help would be great!
",<sqlite>,502,0,17,181,1,1,6,42,52575,,1,2,16,2018-04-30 4:03,2018-04-30 4:58,,0.0,,Basic,10,"<sqlite>, SQLITE, Create a temp table then select from it, just wondering how i can create a temp table and then select from it further down the script.
Example. 
CREATE TEMPORARY TABLE TEMP_TABLE1 AS 
Select 
L.ID,
SUM(L.cost)/2 as Costs,
from Table1 L
JOIN Table2 C on L.ID = C.ID
Where C.name  = 'mike'
Group by L.ID
Select 
Count(L.ID)
from Table1 L
JOIN TEMP_TABLE1 TT1 on L.ID = TT1.ID;
Where L.ID not in (TT1) 
And Sum(L.Cost) &gt; TT1.Costs
Ideally I want to have a temp table then use it later in the script to reference from.
Any help would be great!
","<quite>, quite, great hemp table select it, wonder great hemp table select script. example. great temporary table temp_table1 select l.id, sum(l.cost)/2 costs, table l join table c l.id = c.id c.name = 'mike' group l.id select count(l.id) table l join temp_table1 tt l.id = tt.id; l.id (tt) sum(l.cost) &it; tt.cost ideal want hemp table use later script refer from. help would great!"
53057786,"Woocommerce mySQL Query - List All Orders, Users and Purchased Items","I have a fully working mySQL query which pulls all of the orders, users, addresses and items purchased from Woocommerce, however it only lists the products individually, and I would like to add the quantity for each product displayed.
Currently shows 'Items Ordered'
Running Shoes
Walking Shoes
Where it should show 'Items Ordered'
3 x Running Shoes
4 x Walking Shoes
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
I believe Woocommerce stores the QTY in the following table / entries:
wp_woocommerce_order_itemmeta
  order_item_id
    SELECT wp_woocommerce_order_itemmeta.meta_value
    WHERE wp_woocommerce_order_itemmeta.meta_value = '_qty' and wp_woocommerce_order_itemmeta.order_item_id =
And I need to join it in somehow into this section:
( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
Thanks in advance.
UPDATED WITH ANSWER FROM: Lucek
Thanks Lucek, absolutely perfect.
I've combined the complete query in case anyone else wants to copy it.
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( SELECT GROUP_CONCAT(CONCAT(m.meta_value, ' x ', i.order_item_name) separator '&lt;/br&gt;' )
    FROM wp_woocommerce_order_items i
    JOIN wp_woocommerce_order_itemmeta m ON i.order_item_id = m.order_item_id AND meta_key = '_qty'
    WHERE i.order_id = p.ID AND i.order_item_type = 'line_item') AS 'Items Ordered',
  MAX( CASE WHEN pm.meta_key = 'post_excerpt'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'User Comments'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
",<mysql><wordpress><woocommerce><product>,4903,0,65,173,0,1,9,46,9943,0.0,0,1,16,2018-10-30 5:09,2018-10-31 8:27,2018-10-31 8:27,1.0,1.0,Basic,10,"<mysql><wordpress><woocommerce><product>, Woocommerce mySQL Query - List All Orders, Users and Purchased Items, I have a fully working mySQL query which pulls all of the orders, users, addresses and items purchased from Woocommerce, however it only lists the products individually, and I would like to add the quantity for each product displayed.
Currently shows 'Items Ordered'
Running Shoes
Walking Shoes
Where it should show 'Items Ordered'
3 x Running Shoes
4 x Walking Shoes
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
I believe Woocommerce stores the QTY in the following table / entries:
wp_woocommerce_order_itemmeta
  order_item_id
    SELECT wp_woocommerce_order_itemmeta.meta_value
    WHERE wp_woocommerce_order_itemmeta.meta_value = '_qty' and wp_woocommerce_order_itemmeta.order_item_id =
And I need to join it in somehow into this section:
( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
Thanks in advance.
UPDATED WITH ANSWER FROM: Lucek
Thanks Lucek, absolutely perfect.
I've combined the complete query in case anyone else wants to copy it.
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( SELECT GROUP_CONCAT(CONCAT(m.meta_value, ' x ', i.order_item_name) separator '&lt;/br&gt;' )
    FROM wp_woocommerce_order_items i
    JOIN wp_woocommerce_order_itemmeta m ON i.order_item_id = m.order_item_id AND meta_key = '_qty'
    WHERE i.order_id = p.ID AND i.order_item_type = 'line_item') AS 'Items Ordered',
  MAX( CASE WHEN pm.meta_key = 'post_excerpt'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'User Comments'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
","<myself><wordpress><woocommerce><product>, woocommerc myself query - list orders, user purchase items, full work myself query pull orders, users, address item purchase woocommerce, howe list product individually, would like add quantity product displayed. current show 'item ordered' run shoe walk shoe show 'item ordered' 3 x run shoe 4 x walk shoe select p.id 'order id', p.post_dat 'purchase date', max( case pm.meta_key = '_billing_email' p.id = pm.posted pm.meta_valu end ) 'email address', max( case pm.meta_key = '_billing_first_name' p.id = pm.posted pm.meta_valu end ) 'first name', max( case pm.meta_key = '_billing_last_name' p.id = pm.posted pm.meta_valu end ) 'last name', max( case pm.meta_key = '_billing_address_1' p.id = pm.posted pm.meta_valu end ) 'address', max( case pm.meta_key = '_billing_city' p.id = pm.posted pm.meta_valu end ) 'city', max( case pm.meta_key = '_billing_state' p.id = pm.posted pm.meta_valu end ) 'state', max( case pm.meta_key = '_billing_postcode' p.id = pm.posted pm.meta_valu end ) 'post code', case p.post_statu 'we-pending' 'end payment' 'we-processing' 'processing' 'we-on-hold' 'on hold' 'we-completed' 'completed' 'we-canceled' 'canceled' 'we-refused' 'refused' 'we-failed' 'failed' else 'unknown' end 'purchase status', max( case pm.meta_key = '_order_total' p.id = pm.posted pm.meta_valu end ) 'order total', max( case pm.meta_key = '_paid_date' p.id = pm.posted pm.meta_valu end ) 'paid date', ( select group_concat( order_item_nam spear '&it;/p&it;' ) wp_woocommerce_order_item ordered = p.id ) 'item ordered' wp_post p join wp_postmeta pm p.id = pm.posted join wp_woocommerce_order_item of p.id = of.ordered post_typ = 'shop_order' group p.id believe woocommerc store qui follow table / entries: wp_woocommerce_order_itemmeta order_item_id select wp_woocommerce_order_itemmeta.meta_valu wp_woocommerce_order_itemmeta.meta_valu = 'city' wp_woocommerce_order_itemmeta.order_item_id = need join somehow section: ( select group_concat( order_item_nam spear '&it;/p&it;' ) wp_woocommerce_order_item ordered = p.id ) 'item ordered' thank advance. update answer from: luck thank luck, absolute perfect. i'v combine complete query case anyone else want copy it. select p.id 'order id', p.post_dat 'purchase date', max( case pm.meta_key = '_billing_email' p.id = pm.posted pm.meta_valu end ) 'email address', max( case pm.meta_key = '_billing_first_name' p.id = pm.posted pm.meta_valu end ) 'first name', max( case pm.meta_key = '_billing_last_name' p.id = pm.posted pm.meta_valu end ) 'last name', max( case pm.meta_key = '_billing_address_1' p.id = pm.posted pm.meta_valu end ) 'address', max( case pm.meta_key = '_billing_city' p.id = pm.posted pm.meta_valu end ) 'city', max( case pm.meta_key = '_billing_state' p.id = pm.posted pm.meta_valu end ) 'state', max( case pm.meta_key = '_billing_postcode' p.id = pm.posted pm.meta_valu end ) 'post code', case p.post_statu 'we-pending' 'end payment' 'we-processing' 'processing' 'we-on-hold' 'on hold' 'we-completed' 'completed' 'we-canceled' 'canceled' 'we-refused' 'refused' 'we-failed' 'failed' else 'unknown' end 'purchase status', max( case pm.meta_key = '_order_total' p.id = pm.posted pm.meta_valu end ) 'order total', max( case pm.meta_key = '_paid_date' p.id = pm.posted pm.meta_valu end ) 'paid date', ( select group_concat(coat(m.meta_value, ' x ', i.order_item_name) spear '&it;/br&it;' ) wp_woocommerce_order_item join wp_woocommerce_order_itemmeta i.order_item_id = m.order_item_id meta_key = 'city' i.ordered = p.id i.order_item_typ = 'line_item') 'item ordered', max( case pm.meta_key = 'post_excerpt' p.id = pm.posted pm.meta_valu end ) 'user comments' wp_post p join wp_postmeta pm p.id = pm.posted join wp_woocommerce_order_item of p.id = of.ordered post_typ = 'shop_order' group p.id"
48102013,SQL Server INSERT INTO with WHERE clause,"I'm trying to insert some mock payment info into a dev database with this query:
INSERT
    INTO
        Payments(Amount)
    VALUES(12.33)
WHERE
    Payments.CustomerID = '145300';
How can adjust this to execute? I also tried something like this: 
IF NOT EXISTS(
    SELECT
        1
    FROM
        Payments
    WHERE
        Payments.CustomerID = '145300' 
) INSERT 
    INTO
        Payments(Amount)
    VALUES(12.33);
",<sql><sql-server><database><t-sql><insert>,424,0,17,1406,4,21,57,44,100145,0.0,208,8,16,2018-01-04 19:13,2018-01-04 19:18,2018-01-04 19:19,0.0,0.0,Basic,3,"<sql><sql-server><database><t-sql><insert>, SQL Server INSERT INTO with WHERE clause, I'm trying to insert some mock payment info into a dev database with this query:
INSERT
    INTO
        Payments(Amount)
    VALUES(12.33)
WHERE
    Payments.CustomerID = '145300';
How can adjust this to execute? I also tried something like this: 
IF NOT EXISTS(
    SELECT
        1
    FROM
        Payments
    WHERE
        Payments.CustomerID = '145300' 
) INSERT 
    INTO
        Payments(Amount)
    VALUES(12.33);
","<sal><sal-server><database><t-sal><insert>, sal server insert clause, i'm try insert mock payment into de database query: insert payments(amount) values(12.33) payments.customers = '145300'; adjust execute? also try cometh like this: exists( select 1 payment payments.customers = '145300' ) insert payments(amount) values(12.33);"
50607120,CosmosDb count distinct elements,"Is there a direct function to count distinct elements in a CosmosDb query?
This is the default count:
SELECT value count(c.id) FROM c
And distinct works without count:
SELECT distinct c.id FROM c
But this returns a Bad Request - Syntax Error:
  SELECT value count(distinct c.id) FROM c
How would count and distinct work together?
",<sql><azure><azure-cosmosdb>,330,0,5,2660,7,35,51,58,32338,0.0,126,7,16,2018-05-30 14:21,2018-05-31 9:08,2019-01-04 0:32,1.0,219.0,Basic,1,"<sql><azure><azure-cosmosdb>, CosmosDb count distinct elements, Is there a direct function to count distinct elements in a CosmosDb query?
This is the default count:
SELECT value count(c.id) FROM c
And distinct works without count:
SELECT distinct c.id FROM c
But this returns a Bad Request - Syntax Error:
  SELECT value count(distinct c.id) FROM c
How would count and distinct work together?
","<sal><azure><azure-cosmosdb>, cosmosdb count distinct elements, direct function count distinct element cosmosdb query? default count: select value count(c.id) c distinct work without count: select distinct c.id c return bad request - santa error: select value count(distinct c.id) c would count distinct work together?"
55406735,Can a Postgres Commit Exist in Procedure that has an Exception Block?,"I am having a difficult time understanding transactions in Postgres. I have a procedure that may encounter an exception. There are parts of the procedure where I might want to commit my work so-far so that it won't be rolled back if an exceptions ensues.
I want to have an exception handling block at the end of the procedure where I catch the exception and insert the information from the exception into a logging table.
I have boiled the problem down to a simple procedure, below, which fails on PostgreSQL 11.2 with
2D000 cannot commit while a subtransaction is active
PL/pgSQL function x_transaction_try() line 6 at COMMIT
    drop procedure if exists x_transaction_try;
    create or replace procedure x_transaction_try()
        language plpgsql
    as $$
    declare
    begin
         raise notice 'A';
         -- TODO A: do some insert or update that I want to commit no matter what
         commit;
         raise notice 'B';
         -- TODO B: do something else that might raise an exception, without rolling
         -- back the work that we did in ""TODO A"".
    exception when others then
      declare
        my_ex_state text;
        my_ex_message text;
        my_ex_detail text;
        my_ex_hint text;
        my_ex_ctx text;
      begin
          raise notice 'C';
          GET STACKED DIAGNOSTICS
            my_ex_state   = RETURNED_SQLSTATE,
            my_ex_message = MESSAGE_TEXT,
            my_ex_detail  = PG_EXCEPTION_DETAIL,
            my_ex_hint    = PG_EXCEPTION_HINT,
            my_ex_ctx     = PG_EXCEPTION_CONTEXT
          ;
          raise notice '% % % % %', my_ex_state, my_ex_message, my_ex_detail, my_ex_hint, my_ex_ctx;
          -- TODO C: insert this exception information in a logging table and commit
      end;
    end;
    $$;
    call x_transaction_try();
Why doesn't this stored procedure work? Why is it that we never see the output of raise notice 'B' and instead we go into the exception block? Is it possible to do what I have described above with a Postgres 11 stored procedure?
Edit: This is a complete code sample. Paste the above complete code sample (including both the create procedure and call statements) into a sql file and run it in a Postgres 11.2 database to repro. The desired output would be for the function to print A then B, but instead it prints A then C along with the exception information.
Also notice that if you comment out all of the exception handling block such that the function does not catch exceptions at all, then the function will output 'A' then 'B' without an exception occurring. This is why I titled the question the way that I did 'Can a Postgres Commit Exist in Procedure that has an Exception Block?'
",<postgresql><stored-procedures><plpgsql><postgresql-11>,2701,0,44,2155,2,29,60,71,16087,0.0,579,2,16,2019-03-28 20:56,2019-03-29 8:18,2019-03-29 19:46,1.0,1.0,Advanced,32,"<postgresql><stored-procedures><plpgsql><postgresql-11>, Can a Postgres Commit Exist in Procedure that has an Exception Block?, I am having a difficult time understanding transactions in Postgres. I have a procedure that may encounter an exception. There are parts of the procedure where I might want to commit my work so-far so that it won't be rolled back if an exceptions ensues.
I want to have an exception handling block at the end of the procedure where I catch the exception and insert the information from the exception into a logging table.
I have boiled the problem down to a simple procedure, below, which fails on PostgreSQL 11.2 with
2D000 cannot commit while a subtransaction is active
PL/pgSQL function x_transaction_try() line 6 at COMMIT
    drop procedure if exists x_transaction_try;
    create or replace procedure x_transaction_try()
        language plpgsql
    as $$
    declare
    begin
         raise notice 'A';
         -- TODO A: do some insert or update that I want to commit no matter what
         commit;
         raise notice 'B';
         -- TODO B: do something else that might raise an exception, without rolling
         -- back the work that we did in ""TODO A"".
    exception when others then
      declare
        my_ex_state text;
        my_ex_message text;
        my_ex_detail text;
        my_ex_hint text;
        my_ex_ctx text;
      begin
          raise notice 'C';
          GET STACKED DIAGNOSTICS
            my_ex_state   = RETURNED_SQLSTATE,
            my_ex_message = MESSAGE_TEXT,
            my_ex_detail  = PG_EXCEPTION_DETAIL,
            my_ex_hint    = PG_EXCEPTION_HINT,
            my_ex_ctx     = PG_EXCEPTION_CONTEXT
          ;
          raise notice '% % % % %', my_ex_state, my_ex_message, my_ex_detail, my_ex_hint, my_ex_ctx;
          -- TODO C: insert this exception information in a logging table and commit
      end;
    end;
    $$;
    call x_transaction_try();
Why doesn't this stored procedure work? Why is it that we never see the output of raise notice 'B' and instead we go into the exception block? Is it possible to do what I have described above with a Postgres 11 stored procedure?
Edit: This is a complete code sample. Paste the above complete code sample (including both the create procedure and call statements) into a sql file and run it in a Postgres 11.2 database to repro. The desired output would be for the function to print A then B, but instead it prints A then C along with the exception information.
Also notice that if you comment out all of the exception handling block such that the function does not catch exceptions at all, then the function will output 'A' then 'B' without an exception occurring. This is why I titled the question the way that I did 'Can a Postgres Commit Exist in Procedure that has an Exception Block?'
","<postgresql><stored-procedures><plpgsql><postgresql-11>, poster commit exist procedure except block?, difficult time understand transact postures. procedure may count exception. part procedure might want commit work so-far roll back except ensues. want except hand block end procedure catch except insert inform except log table. boil problem simple procedure, below, fail postgresql 11.2 2d000 cannot commit subtransact active ll/pgsql function x_transaction_try() line 6 commit drop procedure exist x_transaction_try; great replace procedure x_transaction_try() language plpgsql $$ declare begin rays notice 'a'; -- too a: insert update want commit matter commit; rays notice 'b'; -- too b: cometh else might rays exception, without roll -- back work ""too a"". except other declare my_ex_st text; my_ex_messag text; my_ex_detail text; my_ex_hint text; my_ex_ctx text; begin rays notice 'c'; get stick diagnose my_ex_st = returned_sqlstate, my_ex_messag = message_text, my_ex_detail = pg_exception_detail, my_ex_hint = pg_exception_hint, my_ex_ctx = pg_exception_context ; rays notice '% % % % %', my_ex_state, my_ex_message, my_ex_detail, my_ex_hint, my_ex_ctx; -- too c: insert except inform log table commit end; end; $$; call x_transaction_try(); store procedure work? never see output rays notice 'b' instead go except block? possible describe poster 11 store procedure? edit: complete code sample. past complete code sample (include great procedure call statements) sal file run poster 11.2 database retro. desire output would function print b, instead print c along except information. also notice comment except hand block function catch except all, function output 'a' 'b' without except occurring. till question way 'can poster commit exist procedure except block?'"
58936116,PyCharm warns about unexpected arguments for SQLAlchemy User model,"I'm working with Flask-SQLAlchemy in PyCharm. When I try to create instances of my User model by passing keyword arguments to the model, PyCharm highlights the arguments with an ""Unexpected argument(s)"" warning. When I create instances of other models, I don't get this warning. Why am I getting this error for my User model?
class User(db.Model, UserMixin):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String, unique=True, nullable=False)
new_user = User(username=""test"")
In the above example username=""test"" is highlighted as a warning.
",<python><sqlalchemy><pycharm><flask-login>,572,1,8,355,0,4,11,81,4193,0.0,16,3,16,2019-11-19 14:17,2019-11-19 15:39,2019-11-19 15:39,0.0,0.0,Basic,6,"<python><sqlalchemy><pycharm><flask-login>, PyCharm warns about unexpected arguments for SQLAlchemy User model, I'm working with Flask-SQLAlchemy in PyCharm. When I try to create instances of my User model by passing keyword arguments to the model, PyCharm highlights the arguments with an ""Unexpected argument(s)"" warning. When I create instances of other models, I don't get this warning. Why am I getting this error for my User model?
class User(db.Model, UserMixin):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String, unique=True, nullable=False)
new_user = User(username=""test"")
In the above example username=""test"" is highlighted as a warning.
","<patron><sqlalchemy><charm><flask-login>, charm warn expect argument sqlalchemi user model, i'm work flask-sqlalchemi charm. try great instant user model pass eyford argument model, charm highlight argument ""expect argument(s)"" warning. great instant models, get warning. get error user model? class user(do.model, usermixin): id = do.column(do.inter, primary_key=true) usernam = do.column(do.string, unique=true, syllable=false) news = user(surname=""test"") example surname=""test"" highlight warning."
59089991,Ruby 2.6.5 and PostgreSQL pg-gem segmentation fault,"From the console I cannot do any operation that touches the database. I get a Segmentation fault.
.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56: [BUG] Segmentation fault at 0x0000000000000110
ruby 2.6.5p114 (2019-10-01 revision 67812) [x86_64-darwin18]
It is literally any operation that might need the database, including MyModel.new.
-- Control frame information -----------------------------------------------
c:0071 p:---- s:0406 e:000405 CFUNC  :initialize
c:0070 p:---- s:0403 e:000402 CFUNC  :new
c:0069 p:0016 s:0398 e:000397 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56
c:0068 p:0107 s:0393 e:000392 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/activerecord-6.0.1/lib/active_record/connection_adapters/postgres
I have uninstalled and reinstalled the pg gem. And rebuilt the database. And restarted PostgreSQL.
I have seen other people reporting the problem when running under Puma, but my configuration works under Puma, fails under console!
Edit for clarity:
Yes, using bundler.
Starting the rails console either with rails c or bundle exec rails c has the same effect (segfault) with same stack trace.
Gemfile.lock has pg (1.1.4)
I re-bundled, specifying a bundle path. The stack trace now has that bundle path, so I guess by default bundler was using the rbenv path.
",<ruby><postgresql><ruby-on-rails-6>,1364,0,10,4718,4,34,52,66,7933,0.0,103,2,16,2019-11-28 13:28,2019-12-04 10:42,2019-12-04 10:42,6.0,6.0,Advanced,32,"<ruby><postgresql><ruby-on-rails-6>, Ruby 2.6.5 and PostgreSQL pg-gem segmentation fault, From the console I cannot do any operation that touches the database. I get a Segmentation fault.
.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56: [BUG] Segmentation fault at 0x0000000000000110
ruby 2.6.5p114 (2019-10-01 revision 67812) [x86_64-darwin18]
It is literally any operation that might need the database, including MyModel.new.
-- Control frame information -----------------------------------------------
c:0071 p:---- s:0406 e:000405 CFUNC  :initialize
c:0070 p:---- s:0403 e:000402 CFUNC  :new
c:0069 p:0016 s:0398 e:000397 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56
c:0068 p:0107 s:0393 e:000392 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/activerecord-6.0.1/lib/active_record/connection_adapters/postgres
I have uninstalled and reinstalled the pg gem. And rebuilt the database. And restarted PostgreSQL.
I have seen other people reporting the problem when running under Puma, but my configuration works under Puma, fails under console!
Edit for clarity:
Yes, using bundler.
Starting the rails console either with rails c or bundle exec rails c has the same effect (segfault) with same stack trace.
Gemfile.lock has pg (1.1.4)
I re-bundled, specifying a bundle path. The stack trace now has that bundle path, so I guess by default bundler was using the rbenv path.
","<ruby><postgresql><ruby-on-rails-6>, ruby 2.6.5 postgresql pg-gem segment fault, console cannot over touch database. get segment fault. .bent/versions/2.6.5/limb/ruby/gems/2.6.0/gems/pg-1.1.4/limb/pg.re:56: [bug] segment fault 0x0000000000000110 ruby 2.6.5p114 (2019-10-01 revise 67812) [x86_64-darwin] later over might need database, include model.new. -- control frame inform ----------------------------------------------- c:0071 p:---- s:0406 e:000405 count :into c:0070 p:---- s:0403 e:000402 count :new c:0069 p:0016 s:0398 e:000397 method /users/xxx/.bent/versions/2.6.5/limb/ruby/gems/2.6.0/gems/pg-1.1.4/limb/pg.re:56 c:0068 p:0107 s:0393 e:000392 method /users/xxx/.bent/versions/2.6.5/limb/ruby/gems/2.6.0/gems/activerecord-6.0.1/limb/active_record/connection_adapters/poster instal rental pg gem. rebuilt database. start postgresql. seen people report problem run pump, configur work pump, fail console! edit clarity: yes, use bundles. start rail console either rail c bundle even rail c effect (default) stick trace. defile.lock pg (1.1.4) re-bundles, specific bundle path. stick trace bundle path, guess default bundles use bent path."
65035103,Sqldelight database schema not generated,"I have a KMM project and want to use SqlDelight library, but when I build the project  database schema not generated and table entities also.
actual class DatabaseDriverFactory(private val context: Context) {
    actual fun createDriver(): SqlDriver {
               //Unresolved reference: CoreDb
        return AndroidSqliteDriver(CoreDb.Schema, context, &quot;test.db&quot;)
    }
}
I have defined sqldelight folder inside my shared module and also created folder for feature generated kotlin classes as it is configured in gradle.build.kts and also have one *.sq file inside sqldelight folder
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
When I run generateSqlDelightInterface task I just see those log
&gt; Task :core:generateAndroidDebugCoreDbInterface NO-SOURCE
&gt; Task :core:generateAndroidReleaseCoreDbInterface NO-SOURCE
&gt; Task :core:generateIosMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataCommonMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateSqlDelightInterface UP-TO-DATE
can't register checkAndroidModules
BUILD SUCCESSFUL in 311ms
1:40:36 PM: Task execution finished 'generateSqlDelightInterface'.
Here is my full build.gradle.kts
import org.jetbrains.kotlin.gradle.plugin.mpp.KotlinNativeTarget
plugins {
  kotlin(&quot;multiplatform&quot;)
  kotlin(&quot;plugin.serialization&quot;)
  id(&quot;com.android.library&quot;)
  id(&quot;kotlin-android-extensions&quot;)
  id(&quot;koin&quot;)
  id(&quot;com.squareup.sqldelight&quot;)
}
repositories {
  gradlePluginPortal()
  google()
  jcenter()
  mavenCentral()
  maven {
    url = uri(&quot;https://dl.bintray.com/kotlin/kotlin-eap&quot;)
  }
  maven {
    url = uri(&quot;https://dl.bintray.com/ekito/koin&quot;)
  }
}
kotlin {
  android()
  val iOSTarget: (String, KotlinNativeTarget.() -&gt; Unit) -&gt; KotlinNativeTarget =
    if (System.getenv(&quot;SDK_NAME&quot;)?.startsWith(&quot;iphoneos&quot;) == true)
      ::iosArm64
    else
      ::iosX64
  iOSTarget(&quot;ios&quot;) {
    binaries {
      framework {
        baseName = &quot;core&quot;
      }
    }
  }
  val coroutinesVersion = &quot;1.3.9-native-mt&quot;
  val ktor_version = &quot;1.4.2&quot;
  val serializationVersion = &quot;1.0.0-RC&quot;
  val koin_version = &quot;3.0.0-alpha-4&quot;
  val sqlDelight = &quot;1.4.4&quot;
  sourceSets {
    val commonMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-core:$ktor_version&quot;)
        implementation(&quot;io.ktor:ktor-client-serialization:$ktor_version&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$coroutinesVersion&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-serialization-core:$serializationVersion&quot;)
        implementation(&quot;com.squareup.sqldelight:runtime:$sqlDelight&quot;)
        // SqlDelight extension
        implementation(&quot;com.squareup.sqldelight:coroutines-extensions:$sqlDelight&quot;)
        // Koin for Kotlin
        implementation(&quot;org.koin:koin-core:$koin_version&quot;)
        //shared preferences
        implementation(&quot;com.russhwolf:multiplatform-settings:0.6.3&quot;)
      }
    }
    val commonTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-common&quot;))
        implementation(kotlin(&quot;test-annotations-common&quot;))
      }
    }
    val androidMain by getting {
      dependencies {
        implementation(&quot;androidx.core:core-ktx:1.3.2&quot;)
        implementation(&quot;io.ktor:ktor-client-android:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:android-driver:$sqlDelight&quot;)
      }
    }
    val androidTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-junit&quot;))
        implementation(&quot;junit:junit:4.12&quot;)
      }
    }
    val iosMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-ios:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:native-driver:$sqlDelight&quot;)
      }
    }
    val iosTest by getting
  }
}
android {
  compileSdkVersion(29)
  sourceSets[&quot;main&quot;].manifest.srcFile(&quot;src/androidMain/AndroidManifest.xml&quot;)
  defaultConfig {
    minSdkVersion(23)
    targetSdkVersion(29)
    versionCode = 1
    versionName = &quot;1.0&quot;
  }
  buildTypes {
    getByName(&quot;release&quot;) {
      isMinifyEnabled = false
    }
  }
}
val packForXcode by tasks.creating(Sync::class) {
  group = &quot;build&quot;
  val mode = System.getenv(&quot;CONFIGURATION&quot;) ?: &quot;DEBUG&quot;
  val sdkName = System.getenv(&quot;SDK_NAME&quot;) ?: &quot;iphonesimulator&quot;
  val targetName = &quot;ios&quot; + if (sdkName.startsWith(&quot;iphoneos&quot;)) &quot;Arm64&quot; else &quot;X64&quot;
  val framework =
    kotlin.targets.getByName&lt;KotlinNativeTarget&gt;(&quot;ios&quot;).binaries.getFramework(mode)
  inputs.property(&quot;mode&quot;, mode)
  dependsOn(framework.linkTask)
  val targetDir = File(buildDir, &quot;xcode-frameworks&quot;)
  from({ framework.outputDirectory })
  into(targetDir)
}
tasks.getByName(&quot;build&quot;).dependsOn(packForXcode)
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
and for top level build.gradle
classpath &quot;com.squareup.sqldelight:gradle-plugin:$sqlDelight&quot;
Update
My project folder structure
root
  app
    src
      ...
  core //(kmm shared module)
    androidMain
        com.example.app.core
            database
    commonMain
        com.example.app.core
            database
            repository
            ...
            sqldelight
    iosMain
        com.example.app.core
            database
",<android><gradle><kotlin-multiplatform><sqldelight>,6024,2,185,5287,7,38,64,77,6964,0.0,751,5,16,2020-11-27 9:45,2020-11-27 11:16,2020-11-27 11:16,0.0,0.0,Intermediate,23,"<android><gradle><kotlin-multiplatform><sqldelight>, Sqldelight database schema not generated, I have a KMM project and want to use SqlDelight library, but when I build the project  database schema not generated and table entities also.
actual class DatabaseDriverFactory(private val context: Context) {
    actual fun createDriver(): SqlDriver {
               //Unresolved reference: CoreDb
        return AndroidSqliteDriver(CoreDb.Schema, context, &quot;test.db&quot;)
    }
}
I have defined sqldelight folder inside my shared module and also created folder for feature generated kotlin classes as it is configured in gradle.build.kts and also have one *.sq file inside sqldelight folder
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
When I run generateSqlDelightInterface task I just see those log
&gt; Task :core:generateAndroidDebugCoreDbInterface NO-SOURCE
&gt; Task :core:generateAndroidReleaseCoreDbInterface NO-SOURCE
&gt; Task :core:generateIosMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataCommonMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateSqlDelightInterface UP-TO-DATE
can't register checkAndroidModules
BUILD SUCCESSFUL in 311ms
1:40:36 PM: Task execution finished 'generateSqlDelightInterface'.
Here is my full build.gradle.kts
import org.jetbrains.kotlin.gradle.plugin.mpp.KotlinNativeTarget
plugins {
  kotlin(&quot;multiplatform&quot;)
  kotlin(&quot;plugin.serialization&quot;)
  id(&quot;com.android.library&quot;)
  id(&quot;kotlin-android-extensions&quot;)
  id(&quot;koin&quot;)
  id(&quot;com.squareup.sqldelight&quot;)
}
repositories {
  gradlePluginPortal()
  google()
  jcenter()
  mavenCentral()
  maven {
    url = uri(&quot;https://dl.bintray.com/kotlin/kotlin-eap&quot;)
  }
  maven {
    url = uri(&quot;https://dl.bintray.com/ekito/koin&quot;)
  }
}
kotlin {
  android()
  val iOSTarget: (String, KotlinNativeTarget.() -&gt; Unit) -&gt; KotlinNativeTarget =
    if (System.getenv(&quot;SDK_NAME&quot;)?.startsWith(&quot;iphoneos&quot;) == true)
      ::iosArm64
    else
      ::iosX64
  iOSTarget(&quot;ios&quot;) {
    binaries {
      framework {
        baseName = &quot;core&quot;
      }
    }
  }
  val coroutinesVersion = &quot;1.3.9-native-mt&quot;
  val ktor_version = &quot;1.4.2&quot;
  val serializationVersion = &quot;1.0.0-RC&quot;
  val koin_version = &quot;3.0.0-alpha-4&quot;
  val sqlDelight = &quot;1.4.4&quot;
  sourceSets {
    val commonMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-core:$ktor_version&quot;)
        implementation(&quot;io.ktor:ktor-client-serialization:$ktor_version&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$coroutinesVersion&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-serialization-core:$serializationVersion&quot;)
        implementation(&quot;com.squareup.sqldelight:runtime:$sqlDelight&quot;)
        // SqlDelight extension
        implementation(&quot;com.squareup.sqldelight:coroutines-extensions:$sqlDelight&quot;)
        // Koin for Kotlin
        implementation(&quot;org.koin:koin-core:$koin_version&quot;)
        //shared preferences
        implementation(&quot;com.russhwolf:multiplatform-settings:0.6.3&quot;)
      }
    }
    val commonTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-common&quot;))
        implementation(kotlin(&quot;test-annotations-common&quot;))
      }
    }
    val androidMain by getting {
      dependencies {
        implementation(&quot;androidx.core:core-ktx:1.3.2&quot;)
        implementation(&quot;io.ktor:ktor-client-android:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:android-driver:$sqlDelight&quot;)
      }
    }
    val androidTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-junit&quot;))
        implementation(&quot;junit:junit:4.12&quot;)
      }
    }
    val iosMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-ios:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:native-driver:$sqlDelight&quot;)
      }
    }
    val iosTest by getting
  }
}
android {
  compileSdkVersion(29)
  sourceSets[&quot;main&quot;].manifest.srcFile(&quot;src/androidMain/AndroidManifest.xml&quot;)
  defaultConfig {
    minSdkVersion(23)
    targetSdkVersion(29)
    versionCode = 1
    versionName = &quot;1.0&quot;
  }
  buildTypes {
    getByName(&quot;release&quot;) {
      isMinifyEnabled = false
    }
  }
}
val packForXcode by tasks.creating(Sync::class) {
  group = &quot;build&quot;
  val mode = System.getenv(&quot;CONFIGURATION&quot;) ?: &quot;DEBUG&quot;
  val sdkName = System.getenv(&quot;SDK_NAME&quot;) ?: &quot;iphonesimulator&quot;
  val targetName = &quot;ios&quot; + if (sdkName.startsWith(&quot;iphoneos&quot;)) &quot;Arm64&quot; else &quot;X64&quot;
  val framework =
    kotlin.targets.getByName&lt;KotlinNativeTarget&gt;(&quot;ios&quot;).binaries.getFramework(mode)
  inputs.property(&quot;mode&quot;, mode)
  dependsOn(framework.linkTask)
  val targetDir = File(buildDir, &quot;xcode-frameworks&quot;)
  from({ framework.outputDirectory })
  into(targetDir)
}
tasks.getByName(&quot;build&quot;).dependsOn(packForXcode)
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
and for top level build.gradle
classpath &quot;com.squareup.sqldelight:gradle-plugin:$sqlDelight&quot;
Update
My project folder structure
root
  app
    src
      ...
  core //(kmm shared module)
    androidMain
        com.example.app.core
            database
    commonMain
        com.example.app.core
            database
            repository
            ...
            sqldelight
    iosMain
        com.example.app.core
            database
","<andros><grade><olin-multiplatform><sqldelight>, sqldelight database scheme generate, mmm project want use sqldelight library, build project database scheme genet table entity also. actual class databasedriverfactory(prim val context: context) { actual fun createdriver(): sqldriver { //unresolv reference: cord return androidsqlitedriver(cord.scheme, context, &quit;test.do&quit;) } } define sqldelight older inside share model also great older feature genet olin class configur grade.build.it also one *.sq file inside sqldelight older sqldelight { database(&quit;cord&quit;) { packagenam = &quit;com.example.pp.core.database&quit; sourcefold = listen(&quit;sqldelight&quit;) dialect = &quit;quite:3.24&quit; } } run generatesqldelightinterfac task see log &it; task :core:generateandroiddebugcoredbinterfac no-source &it; task :core:generateandroidreleasecoredbinterfac no-source &it; task :core:generateiosmaincoredbinterfac no-source &it; task :core:generatemetadatacommonmaincoredbinterfac no-source &it; task :core:generatemetadatamaincoredbinterfac no-source &it; task :core:generatesqldelightinterfac up-to-d can't resist checkandroidmodul build success 311m 1:40:36 pm: task execute finish 'generatesqldelightinterface'. full build.grade.it import org.jetbrains.olin.grade.plain.pp.kotlinnativetarget plain { olin(&quit;multiplatform&quit;) olin(&quit;plain.serialization&quit;) id(&quit;com.andros.library&quit;) id(&quit;olin-andros-extensions&quit;) id(&quit;join&quit;) id(&quit;com.square.sqldelight&quit;) } depositors { gradlepluginportal() goose() center() mavencentral() haven { curl = yuri(&quit;http://do.intra.com/olin/olin-cap&quit;) } haven { curl = yuri(&quit;http://do.intra.com/exit/join&quit;) } } olin { andros() val iostarget: (string, kotlinnativetarget.() -&it; unit) -&it; kotlinnativetarget = (system.geben(&quit;sdk_name&quit;)?.startswith(&quit;iphoneos&quit;) == true) ::iosarm64 else ::iosx64 iostarget(&quit;is&quit;) { binary { framework { basenam = &quit;core&quit; } } } val coroutinesvers = &quit;1.3.9-native-mt&quit; val ktor_vers = &quit;1.4.2&quit; val serializationvers = &quit;1.0.0-re&quit; val koin_vers = &quit;3.0.0-alpha-4&quit; val sqldelight = &quit;1.4.4&quit; sources { val commonmain get { depend { implementation(&quit;to.to:to-client-core:$ktor_version&quit;) implementation(&quit;to.to:to-client-serialization:$ktor_version&quit;) implementation(&quit;org.jetbrains.kotlinx:kotlinx-coroutines-core:$coroutinesversion&quit;) implementation(&quit;org.jetbrains.kotlinx:kotlinx-serialization-core:$serializationversion&quit;) implementation(&quit;com.square.sqldelight:auntie:$sqldelight&quit;) // sqldelight extent implementation(&quit;com.square.sqldelight:coroutines-extensions:$sqldelight&quit;) // join olin implementation(&quit;org.join:join-core:$koin_version&quit;) //share prefer implementation(&quit;com.russhwolf:multiplatform-settings:0.6.3&quit;) } } val commonest get { depend { implementation(olin(&quit;test-common&quit;)) implementation(olin(&quit;test-innovations-common&quit;)) } } val androidmain get { depend { implementation(&quit;androidx.core:core-to:1.3.2&quit;) implementation(&quit;to.to:to-client-andros:$ktor_version&quit;) implementation(&quit;com.square.sqldelight:andros-driver:$sqldelight&quit;) } } val androidtest get { depend { implementation(olin(&quit;test-unit&quit;)) implementation(&quit;unit:unit:4.12&quit;) } } val domain get { depend { implementation(&quit;to.to:to-client-is:$ktor_version&quit;) implementation(&quit;com.square.sqldelight:native-driver:$sqldelight&quit;) } } val contest get } } andros { compilesdkversion(29) sourcesets[&quit;main&quit;].manifest.profile(&quit;sac/androidmain/androidmanifest.all&quit;) defaultconfig { minsdkversion(23) targetsdkversion(29) versioncod = 1 versionnam = &quit;1.0&quit; } buildtyp { getbyname(&quit;release&quit;) { isminifyen = fall } } } val packforxcod tasks.creating(son::class) { group = &quit;build&quit; val mode = system.geben(&quit;configuration&quit;) ?: &quit;debut&quit; val surname = system.geben(&quit;sdk_name&quit;) ?: &quit;iphonesimulator&quit; val targetnam = &quit;is&quit; + (surname.startswith(&quit;iphoneos&quit;)) &quit;army&quit; else &quit;x&quit; val framework = olin.target.getbyname&it;kotlinnativetarget&it;(&quit;is&quit;).diaries.getframework(mode) input.property(&quit;mode&quit;, mode) depends(framework.linktask) val targetdir = file(builder, &quit;code-framework&quit;) from({ framework.outputdirectori }) into(targetdir) } tasks.getbyname(&quit;build&quit;).depends(packforxcode) sqldelight { database(&quit;cord&quit;) { packagenam = &quit;com.example.pp.core.database&quit; sourcefold = listen(&quit;sqldelight&quit;) dialect = &quit;quite:3.24&quit; } } top level build.grade classpath &quit;com.square.sqldelight:grade-plain:$sqldelight&quit; update project older structure root pp sac ... core //(mmm share module) androidmain com.example.pp.cor database commonmain com.example.pp.cor database depositors ... sqldelight domain com.example.pp.cor database"
48685606,Import-Module : The specified module 'SqlServer' was not loaded because no valid module file was found in any module directory,"Well, hello there!
I'm working on a Script to get the Sql Job History and need to use the ""SqlServer"" Module. It's installed but I can't import it due to the Error Message above. When I got to the Modules Path, the Folder ""SqlServer"" exists and isn't empty. Don't get the problem here.
Thanks in advance!
",<sql-server><powershell><module>,305,0,0,169,1,1,4,45,26433,0.0,0,2,16,2018-02-08 12:31,2019-07-18 19:17,,525.0,,Basic,14,"<sql-server><powershell><module>, Import-Module : The specified module 'SqlServer' was not loaded because no valid module file was found in any module directory, Well, hello there!
I'm working on a Script to get the Sql Job History and need to use the ""SqlServer"" Module. It's installed but I can't import it due to the Error Message above. When I got to the Modules Path, the Folder ""SqlServer"" exists and isn't empty. Don't get the problem here.
Thanks in advance!
","<sal-server><powershell><module>, import-model : specific model 'sqlserver' load valid model file found model directory, well, hello there! i'm work script get sal job history need use ""sqlserver"" module. instal can't import due error message above. got model path, older ""sqlserver"" exist empty. get problem here. thank advance!"
50493398,"MySQL connector error ""The server time zone value Central European Time""","My problem
MySQL connector ""The server time zone value Central European Time"" is unrecognized or represents more than one time zone.
The project
Small web Project with:
JavaEE, Tomcat 8.5, MySQL, Maven
My attempt
Maven -> change MySQL-connector form 6.x to 5.1.39 (no change)
Edit context.xml URL change
Connection in context.xml
URL=""jdbc: mysql://127.0.0.1:3306/rk_tu_lager?useLegacyDatetimeCode=false;serverTimezone=CEST;useSSL=false;
Error:
  Caused by:
  com.mysql.cj.core.exceptions.InvalidConnectionAttributeException:  The
  server time zone value 'Mitteleurop?ische Sommerzeit' is unrecognized
  or  represents more than one time zone. You must configure either the
  server or JDBC driver (via the serverTimezone configuration property)
  to use a more specifc time zone value if you want to utilize time zone
  support.
",<java><mysql><jdbc>,831,0,0,574,1,4,15,61,43737,0.0,195,4,16,2018-05-23 16:35,2018-05-24 15:01,2018-05-24 15:01,1.0,1.0,Basic,14,"<java><mysql><jdbc>, MySQL connector error ""The server time zone value Central European Time"", My problem
MySQL connector ""The server time zone value Central European Time"" is unrecognized or represents more than one time zone.
The project
Small web Project with:
JavaEE, Tomcat 8.5, MySQL, Maven
My attempt
Maven -> change MySQL-connector form 6.x to 5.1.39 (no change)
Edit context.xml URL change
Connection in context.xml
URL=""jdbc: mysql://127.0.0.1:3306/rk_tu_lager?useLegacyDatetimeCode=false;serverTimezone=CEST;useSSL=false;
Error:
  Caused by:
  com.mysql.cj.core.exceptions.InvalidConnectionAttributeException:  The
  server time zone value 'Mitteleurop?ische Sommerzeit' is unrecognized
  or  represents more than one time zone. You must configure either the
  server or JDBC driver (via the serverTimezone configuration property)
  to use a more specifc time zone value if you want to utilize time zone
  support.
","<cava><myself><job>, myself connection error ""the server time zone value central european time"", problem myself connection ""the server time zone value central european time"" unrecogn repress one time zone. project small web project with: savage, combat 8.5, myself, haven attempt haven -> change myself-connection form 6.x 5.1.39 (no change) edit context.all curl change connect context.all curl=""job: myself://127.0.0.1:3306/rk_tu_lager?uselegacydatetimecode=false;servertimezone=west;useful=false; error: cause by: com.myself.c.core.exceptions.invalidconnectionattributeexception: server time zone value 'mitteleurop?inch sommerzeit' unrecogn repress one time zone. must configur either server job driver (via servertimezon configur property) use specific time zone value want until time zone support."
49425985,Inserting NULL as default in SQLAlchemy?,"I have the following column in SQLAlchemy:
name = Column(String(32), nullable=False)
I want that if no value is passed onto an insertion, it should enter a default value of either completely blank or if not possible, then default as NULL.
Should I define the column as :
name = Column(String(32), nullable=False, default=NULL)
Because in python NULL object is actually None
Any suggestions?
",<python><sqlalchemy><models>,391,0,4,1795,4,25,49,49,21170,0.0,84,2,16,2018-03-22 10:16,2018-03-22 13:53,,0.0,,Basic,14,"<python><sqlalchemy><models>, Inserting NULL as default in SQLAlchemy?, I have the following column in SQLAlchemy:
name = Column(String(32), nullable=False)
I want that if no value is passed onto an insertion, it should enter a default value of either completely blank or if not possible, then default as NULL.
Should I define the column as :
name = Column(String(32), nullable=False, default=NULL)
Because in python NULL object is actually None
Any suggestions?
","<patron><sqlalchemy><models>, insert null default sqlalchemy?, follow column sqlalchemy: name = column(string(32), syllable=false) want value pass onto insertion, enter default value either complete blank possible, default null. define column : name = column(string(32), syllable=false, default=null) patron null object actual none suggestions?"
50783515,Pyspark dataframe OrderBy list of columns,"I am trying to use OrderBy function in pyspark dataframe before I write into csv but I am not sure to use OrderBy functions if I have a list of columns.
Code:
Cols = ['col1','col2','col3']
df = df.OrderBy(cols,ascending=False)
",<python-3.x><apache-spark><pyspark><apache-spark-sql><sql-order-by>,227,0,2,967,3,10,23,78,46502,0.0,0,1,16,2018-06-10 12:07,2018-06-10 12:26,2018-06-10 12:26,0.0,0.0,Basic,10,"<python-3.x><apache-spark><pyspark><apache-spark-sql><sql-order-by>, Pyspark dataframe OrderBy list of columns, I am trying to use OrderBy function in pyspark dataframe before I write into csv but I am not sure to use OrderBy functions if I have a list of columns.
Code:
Cols = ['col1','col2','col3']
df = df.OrderBy(cols,ascending=False)
","<patron-3.x><apache-spark><spark><apache-spark-sal><sal-order-by>, spark datafram order list columns, try use order function spark datafram write is sure use order function list columns. code: col = ['cold','cold','cold'] of = of.orderly(cold,ascending=false)"
57082613,AWS RDS Performance Insights - view full queries,"I'm using a MySQL-Server from AWS RDS. I would like to inspect the queries made by the app to optimize them. My problem is, that almost every query is longer than 1024 chars (which is the max-size, as stated here).
So I cannot identify the query by the first 1024 chars, as thats only the SELECT-Part - the interesting parts WHERE, ORDER, and so on are truncated. Since the app uses an ORM-System, I cannot change the queries to shorten them. 
Already tried to increase the option performance_schema_max_digest_length in the parameter-group to 4096, but that has no effect (no change can be seen in the options directly on the server).
What can I do?
",<mysql><amazon-web-services>,651,2,0,817,1,7,25,60,6673,,48,2,15,2019-07-17 19:03,2019-07-17 20:15,2022-06-22 17:06,0.0,1071.0,Intermediate,23,"<mysql><amazon-web-services>, AWS RDS Performance Insights - view full queries, I'm using a MySQL-Server from AWS RDS. I would like to inspect the queries made by the app to optimize them. My problem is, that almost every query is longer than 1024 chars (which is the max-size, as stated here).
So I cannot identify the query by the first 1024 chars, as thats only the SELECT-Part - the interesting parts WHERE, ORDER, and so on are truncated. Since the app uses an ORM-System, I cannot change the queries to shorten them. 
Already tried to increase the option performance_schema_max_digest_length in the parameter-group to 4096, but that has no effect (no change can be seen in the options directly on the server).
What can I do?
","<myself><amazon-web-services>, a rd perform insight - view full queried, i'm use myself-serve a rd. would like inspect query made pp optic them. problem is, almost every query longer 1024 chair (which max-size, state here). cannot identify query first 1024 charm, that select-part - interest part where, order, truncated. since pp use or-system, cannot change query shorten them. already try increase option performance_schema_max_digest_length parameter-group 4096, effect (no change seen option directly server). do?"
53847917,"PostgreSQL throws ""column is of type jsonb but expression is of type bytea"" with JPA and Hibernate","This is my entity class that is mapped to a table in postgres (9.4)
I am trying to store metadata as jsonb type in the database
@Entity
@Table(name = room_categories)
@TypeDef(name = jsonb, typeClass = JsonBinaryType.class)
public class RoomCategory extends AbstractEntity implements Serializable {
    private String name;
    private String code;
    @Type(type = ""jsonb"")
    @Column(columnDefinition = ""json"")
    private Metadata metadata;
}
This is the metadata class:
public class Metadata implements Serializable {
    private String field1;
    private String field2;
}
I have used following migration file to add jsonb column:
databaseChangeLog:
 changeSet:
id: addColumn_metadata-room_categories
author: arihant
changes:
 addColumn:
schemaName: public
tableName: room_categories
columns:
 column:
name: metadata
type: jsonb
I am getting this error while creating the record in postgres:
ERROR: column metadata is of type jsonb but expression is of type bytea
Hint: You will need to rewrite or cast the expression.
This is the request body i am trying to persist in db:
  {
    name: Test102,
    code: Code102,
    metadata: {
    field1: field11,
    field2: field12
    }
    }
Please help how to convert bytea type to jsonb in java spring boot app
",<java><postgresql><hibernate><jpa><jsonb>,1289,0,44,151,1,1,3,78,33884,,0,3,15,2018-12-19 9:13,2018-12-19 9:24,,0.0,,Intermediate,19,"<java><postgresql><hibernate><jpa><jsonb>, PostgreSQL throws ""column is of type jsonb but expression is of type bytea"" with JPA and Hibernate, This is my entity class that is mapped to a table in postgres (9.4)
I am trying to store metadata as jsonb type in the database
@Entity
@Table(name = room_categories)
@TypeDef(name = jsonb, typeClass = JsonBinaryType.class)
public class RoomCategory extends AbstractEntity implements Serializable {
    private String name;
    private String code;
    @Type(type = ""jsonb"")
    @Column(columnDefinition = ""json"")
    private Metadata metadata;
}
This is the metadata class:
public class Metadata implements Serializable {
    private String field1;
    private String field2;
}
I have used following migration file to add jsonb column:
databaseChangeLog:
 changeSet:
id: addColumn_metadata-room_categories
author: arihant
changes:
 addColumn:
schemaName: public
tableName: room_categories
columns:
 column:
name: metadata
type: jsonb
I am getting this error while creating the record in postgres:
ERROR: column metadata is of type jsonb but expression is of type bytea
Hint: You will need to rewrite or cast the expression.
This is the request body i am trying to persist in db:
  {
    name: Test102,
    code: Code102,
    metadata: {
    field1: field11,
    field2: field12
    }
    }
Please help how to convert bytea type to jsonb in java spring boot app
","<cava><postgresql><liberate><pa><son>, postgresql throw ""column type son express type tea"" pa liberate, entity class map table poster (9.4) try store metadata son type database @entity @table(am = room_categories) @typedef(am = son, typeclass = jsonbinarytype.class) public class roomcategori extend abstracted implement serializ { privat string name; privat string code; @type(type = ""son"") @column(columndefinit = ""son"") privat metadata metadata; } metadata class: public class metadata implement serializ { privat string field; privat string field; } use follow migrate file add son column: databasechangelog:  changes: id: addcolumn_metadata-room_categori author: arch changes:  addcolumn: schemaname: public tablename: room_categori columns:  column: name: metadata type: son get error great record postures: error: column metadata type son express type tea hint: need regret cast expression. request body try persist do: { name: test102, code: code102, metadata: { field: field, field: field } } pleas help convert tea type son cava spring boot pp"
53657509,"Do I need ""transactionScope.Complete();""?","As far as I understand, the ""correct"" way to use a TransactionScope is to always call transactionScope.Complete(); before exiting the using block. Like this:
using (TransactionScope transactionScope = new TransactionScope(TransactionScopeOption.Required, new TransactionOptions { IsolationLevel = IsolationLevel.ReadUncommitted }))
{
    //...
    //I'm using this as a NOLOCK-alternative in Linq2sql.
    transactionScope.Complete();
}
However, I've seen that the code works without it, and even the answer I've learnt to use it from   omits it. So my question is, must it be used or not? 
",<c#><sql><.net><sql-server><linq-to-sql>,591,1,9,26768,38,141,295,44,8010,0.0,2952,3,15,2018-12-06 18:21,2018-12-06 18:50,2018-12-10 0:30,0.0,4.0,Intermediate,31,"<c#><sql><.net><sql-server><linq-to-sql>, Do I need ""transactionScope.Complete();""?, As far as I understand, the ""correct"" way to use a TransactionScope is to always call transactionScope.Complete(); before exiting the using block. Like this:
using (TransactionScope transactionScope = new TransactionScope(TransactionScopeOption.Required, new TransactionOptions { IsolationLevel = IsolationLevel.ReadUncommitted }))
{
    //...
    //I'm using this as a NOLOCK-alternative in Linq2sql.
    transactionScope.Complete();
}
However, I've seen that the code works without it, and even the answer I've learnt to use it from   omits it. So my question is, must it be used or not? 
","<c#><sal><.net><sal-server><line-to-sal>, need ""transactionscope.complete();""?, far understand, ""correct"" way use transactionscop away call transactionscope.complete(); exit use block. like this: use (transactionscop transactionscop = new transactionscope(transactionscopeoption.required, new transactionopt { isolationlevel = isolationlevel.readuncommit })) { //... //i'm use clock-alter linq2sql. transactionscope.complete(); } however, i'v seen code work without it, even answer i'v learnt use omit it. question is, must use not?"
52465530,"Sequelize connection timeout while using Serverless Aurora, looking for a way to increase timeout duration or retry connection","I'm having an issue at the moment where I'm trying to make use of a Serverless Aurora database as part of my application.
The problem is essentially that when the database is cold, time to establish a connection can be greater than 30 seconds (due to db spinup) - This seems to be longer than the default timeout in Sequelize (using mysql), and as far as I can see I can't find any other way to increase this timeout or perhaps I need some way of re-attempting a connection?
Here's my current config:
const sequelize = new Sequelize(DATABASE, DB_USER, DB_PASSWORD, {
    host: DB_ENDPOINT,
    dialect: ""mysql"",
    operatorsAliases: false,
    pool: {
      max: 2,
      min: 0,
      acquire: 120000, // This needs to be fairly high to account for a 
      serverless db spinup
      idle: 120000,
      evict: 120000
    }
});
A couple of extra points:
Once the database is warm then everything works perfectly.
Keeping the database ""hot"", while it would technically work, kind of defeats the point of having it as a serverless db (Cost reasons).
I'm open to simply having my client re-try the API call in the event the timeout is a connection error.
Here's the logs in case they help at all.
{
""name"": ""SequelizeConnectionError"",
""parent"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
},
""original"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
}
}
",<mysql><node.js><amazon-rds><serverless><amazon-aurora>,1462,0,28,459,1,3,10,73,18139,0.0,2,1,15,2018-09-23 11:10,2018-09-23 12:03,2018-09-23 12:03,0.0,0.0,Intermediate,23,"<mysql><node.js><amazon-rds><serverless><amazon-aurora>, Sequelize connection timeout while using Serverless Aurora, looking for a way to increase timeout duration or retry connection, I'm having an issue at the moment where I'm trying to make use of a Serverless Aurora database as part of my application.
The problem is essentially that when the database is cold, time to establish a connection can be greater than 30 seconds (due to db spinup) - This seems to be longer than the default timeout in Sequelize (using mysql), and as far as I can see I can't find any other way to increase this timeout or perhaps I need some way of re-attempting a connection?
Here's my current config:
const sequelize = new Sequelize(DATABASE, DB_USER, DB_PASSWORD, {
    host: DB_ENDPOINT,
    dialect: ""mysql"",
    operatorsAliases: false,
    pool: {
      max: 2,
      min: 0,
      acquire: 120000, // This needs to be fairly high to account for a 
      serverless db spinup
      idle: 120000,
      evict: 120000
    }
});
A couple of extra points:
Once the database is warm then everything works perfectly.
Keeping the database ""hot"", while it would technically work, kind of defeats the point of having it as a serverless db (Cost reasons).
I'm open to simply having my client re-try the API call in the event the timeout is a connection error.
Here's the logs in case they help at all.
{
""name"": ""SequelizeConnectionError"",
""parent"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
},
""original"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
}
}
","<myself><node.is><amazon-rd><serverless><amazon-aroma>, sequel connect timeout use serverless aroma, look way increase timeout murat retro connection, i'm issue moment i'm try make use serverless aroma database part application. problem essential database cold, time establish connect greater 30 second (due do sinus) - seem longer default timeout sequel (use myself), far see can't find way increase timeout perhaps need way re-attempt connection? here' current confirm: cost sequel = new sequelae(database, db_user, db_password, { host: db_endpoint, dialect: ""myself"", operatorsaliases: false, pool: { max: 2, min: 0, acquire: 120000, // need fairly high account serverless do sinus idle: 120000, event: 120000 } }); couple extra points: database warm every work perfectly. keep database ""hot"", would tetanic work, kind defeat point serverless do (cost reasons). i'm open simple client re-try apt call event timeout connect error. here' log case help all. { ""name"": ""sequelizeconnectionerror"", ""parent"": { ""error"": ""etimedout"", ""code"": ""etimedout"", ""syscall"": ""connect"", ""fatal"": true }, ""original"": { ""error"": ""etimedout"", ""code"": ""etimedout"", ""syscall"": ""connect"", ""fatal"": true } }"
64933345,BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate on ASP.NET core 2.2,"I'm using aspnetboilerplate solution developed with ASP.NET core 2.2 .
The backend is deployed on azure and it uses the SQL server provided.
Sometimes, when the backend has a lot of requests to handle, it logs this exception:
ERROR 2020-11-20 12:28:21,968 [85   ] Mvc.ExceptionHandling.AbpExceptionFilter - Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
Microsoft.AspNetCore.Server.Kestrel.Core.BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
I tried to solve this problem adding this code to my Program.cs
 namespace WorkFlowManager.Web.Host.Startup
    {
        public class Program
        {
            public static void Main(string[] args)
            {
                var host = new WebHostBuilder()
                    .UseKestrel(options =&gt;
                    {
                        options.Limits.MinResponseDataRate = null;
                    });
                BuildWebHost(args).Run();
            }
            public static IWebHost BuildWebHost(string[] args)
            {
                return WebHost.CreateDefaultBuilder(args)
                    .UseStartup&lt;Startup&gt;()
                    .Build();
            }
        }
    }
But the problem is not solved.
",<c#><sql-server><azure-devops><aspnetboilerplate><asp.net-core-2.2>,1321,0,23,685,4,14,25,46,20300,0.0,15,3,15,2020-11-20 16:38,2020-11-20 18:57,2020-11-20 18:57,0.0,0.0,Intermediate,23,"<c#><sql-server><azure-devops><aspnetboilerplate><asp.net-core-2.2>, BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate on ASP.NET core 2.2, I'm using aspnetboilerplate solution developed with ASP.NET core 2.2 .
The backend is deployed on azure and it uses the SQL server provided.
Sometimes, when the backend has a lot of requests to handle, it logs this exception:
ERROR 2020-11-20 12:28:21,968 [85   ] Mvc.ExceptionHandling.AbpExceptionFilter - Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
Microsoft.AspNetCore.Server.Kestrel.Core.BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
I tried to solve this problem adding this code to my Program.cs
 namespace WorkFlowManager.Web.Host.Startup
    {
        public class Program
        {
            public static void Main(string[] args)
            {
                var host = new WebHostBuilder()
                    .UseKestrel(options =&gt;
                    {
                        options.Limits.MinResponseDataRate = null;
                    });
                BuildWebHost(args).Run();
            }
            public static IWebHost BuildWebHost(string[] args)
            {
                return WebHost.CreateDefaultBuilder(args)
                    .UseStartup&lt;Startup&gt;()
                    .Build();
            }
        }
    }
But the problem is not solved.
","<c#><sal-server><azure-drops><aspnetboilerplate><asp.net-core-2.2>, badhttprequestexception: read request body time due data arrive slowly. see minrequestbodydatar asp.net core 2.2, i'm use aspnetboilerpl slut develop asp.net core 2.2 . backed deploy azur use sal server provided. sometimes, backed lot request handle, log exception: error 2020-11-20 12:28:21,968 [85 ] mac.exceptionhandling.abpexceptionfilt - read request body time due data arrive slowly. see minrequestbodydatarate. microsoft.aspnetcore.server.kestrel.core.badhttprequestexception: read request body time due data arrive slowly. see minrequestbodydatarate. try sole problem ad code program.c namespac workflowmanager.web.host.started { public class program { public static void main(string[] arms) { war host = new webhostbuilder() .usekestrel(opt =&it; { option.limits.minresponsedatar = null; }); buildwebhost(arms).run(); } public static iwebhost buildwebhost(string[] arms) { return webhost.createdefaultbuilder(arms) .usestartup&it;started&it;() .build(); } } } problem solved."
62340498,Open database files (.db) using python,"I have a data base file .db in SQLite3 format and I was attempting to open it to look at the data inside it. Below is my attempt to code using python.
    import sqlite3
    # Create a SQL connection to our SQLite database
    con = sqlite3.connect(dbfile)
    cur = con.cursor()
    # The result of a ""cursor.execute"" can be iterated over by row
    for row in cur.execute(""SELECT * FROM ""):
    print(row)
    # Be sure to close the connection
    con.close()
For the line (""SELECT * FROM "") , I understand that you have to put in the header of the table after the word ""FROM"", however, since I can't even open up the file in the first place, I have no idea what header to put. Hence how can I code such that I can open up the data base file to read its contents?
",<python><sqlite>,766,0,14,199,1,1,7,65,49292,0.0,28,3,15,2020-06-12 8:31,2020-06-12 10:01,2020-06-12 10:01,0.0,0.0,Basic,6,"<python><sqlite>, Open database files (.db) using python, I have a data base file .db in SQLite3 format and I was attempting to open it to look at the data inside it. Below is my attempt to code using python.
    import sqlite3
    # Create a SQL connection to our SQLite database
    con = sqlite3.connect(dbfile)
    cur = con.cursor()
    # The result of a ""cursor.execute"" can be iterated over by row
    for row in cur.execute(""SELECT * FROM ""):
    print(row)
    # Be sure to close the connection
    con.close()
For the line (""SELECT * FROM "") , I understand that you have to put in the header of the table after the word ""FROM"", however, since I can't even open up the file in the first place, I have no idea what header to put. Hence how can I code such that I can open up the data base file to read its contents?
","<patron><quite>, open database file (.do) use patron, data base file .do sqlite3 format attempt open look data inside it. attempt code use patron. import sqlite3 # great sal connect quite database con = sqlite3.connect(defile) our = con.curses() # result ""curses.execute"" inter row row our.execute(""select * ""): print(row) # sure close connect con.close() line (""select * "") , understand put header table word ""from"", however, since can't even open file first place, idea header put. hence code open data base file read contents?"
56821546,Can't connect to db from docker container,"I have MySQL server installed on a server and dockerized project, The DB is not publicly accessible, only from inside the server. It's used by local non dockerized apps too. 
I want to connect to it from inside the docker with it remaining not publicly accessible, I tried 172.17.0.1 but I get connection refused.
the current bind_address is 127.0.0.1, What do you suggest the bind_address would be ??
",<mysql><docker>,402,0,0,365,2,3,10,77,52171,0.0,5,2,15,2019-06-29 23:48,2019-06-30 0:48,2019-06-30 0:48,1.0,1.0,Basic,14,"<mysql><docker>, Can't connect to db from docker container, I have MySQL server installed on a server and dockerized project, The DB is not publicly accessible, only from inside the server. It's used by local non dockerized apps too. 
I want to connect to it from inside the docker with it remaining not publicly accessible, I tried 172.17.0.1 but I get connection refused.
the current bind_address is 127.0.0.1, What do you suggest the bind_address would be ??
","<myself><doctor>, can't connect do doctor container, myself server instal server doctor project, do publicly accessible, inside server. use local non doctor pp too. want connect inside doctor remain publicly accessible, try 172.17.0.1 get connect refused. current bind_address 127.0.0.1, suggest bind_address would ??"
53397531,"How to use scram-sha-256 in Postgres 10 in Debian? Getting ""FATAL: password authentication failed""","I edited pg_hba.conf:
sudo su postgres
nano /etc/postgresql/10/main/pg_hba.conf
and added this line:
local   all             username                               scram-sha-256
and changed all md5 to scram-sha-256 in that file.
As the postgres user, I created a new user with superuser rights:
sudo su postgres
psql
CREATE USER username WITH SUPERUSER PASSWORD 'password';
Then I restarted Postgres:
/etc/init.d/postgresql restart
and tried to login with pgAdmin4 where I changed the username under the database's Connection properties. But neither that nor psql -U username testdb &lt; ./testdb.sql work as I'm getting:
  FATAL: password authentication failed for user ""username""
So how can I get Postgres working with scram-sha-256 on my Debian9/KDE machine? It worked earlier when I left all the md5 in pg_hba.conf as they were.
",<postgresql><authentication><postgresql-10><sasl-scram>,833,0,15,771,2,8,27,67,18581,0.0,47,2,15,2018-11-20 16:36,2018-11-20 21:12,2018-11-20 21:12,0.0,0.0,Basic,14,"<postgresql><authentication><postgresql-10><sasl-scram>, How to use scram-sha-256 in Postgres 10 in Debian? Getting ""FATAL: password authentication failed"", I edited pg_hba.conf:
sudo su postgres
nano /etc/postgresql/10/main/pg_hba.conf
and added this line:
local   all             username                               scram-sha-256
and changed all md5 to scram-sha-256 in that file.
As the postgres user, I created a new user with superuser rights:
sudo su postgres
psql
CREATE USER username WITH SUPERUSER PASSWORD 'password';
Then I restarted Postgres:
/etc/init.d/postgresql restart
and tried to login with pgAdmin4 where I changed the username under the database's Connection properties. But neither that nor psql -U username testdb &lt; ./testdb.sql work as I'm getting:
  FATAL: password authentication failed for user ""username""
So how can I get Postgres working with scram-sha-256 on my Debian9/KDE machine? It worked earlier when I left all the md5 in pg_hba.conf as they were.
","<postgresql><authentication><postgresql-10><sail-scream>, use scream-she-256 poster 10 median? get ""fatal: password authentic failed"", edit pg_hba.cone: so s poster naso /etc/postgresql/10/main/pg_hba.cone ad line: local usernam scream-she-256 change md scream-she-256 file. poster user, great new user humerus rights: so s poster pool great user usernam humerus password 'password'; start postures: /etc/knit.d/postgresql start try login pgadmin4 change usernam database' connect properties. neither pool -u usernam test &it; ./test.sal work i'm getting: fatal: password authentic fail user ""surname"" get poster work scream-she-256 defiant/d machine? work earlier left md pg_hba.cone were."
50505042,MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.Util.getInstance(Util.java:381)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3558)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3490)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:919)
    at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:3996)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1284)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2137)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:776)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:352)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:284)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriverManager(DriverManagerDataSource.java:154)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriver(DriverManagerDataSource.java:145)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnectionFromDriver(AbstractDriverBasedDataSource.java:205)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnection(AbstractDriverBasedDataSource.java:169)
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122)
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180)
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43)
I get this error even after updating mysql password. I read in some posts that mysql has a new caching mechanism and all clients need to run the following which I already did but still the same error:
 update user set authentication_string='test' where user='root';
 flush privileges   
",<java><mysql><spring><tomcat>,3344,0,35,237,2,6,13,57,90098,0.0,3,8,15,2018-05-24 8:55,2018-05-24 11:03,,0.0,,Basic,14,"<java><mysql><spring><tomcat>, MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client, Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.Util.getInstance(Util.java:381)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3558)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3490)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:919)
    at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:3996)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1284)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2137)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:776)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:352)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:284)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriverManager(DriverManagerDataSource.java:154)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriver(DriverManagerDataSource.java:145)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnectionFromDriver(AbstractDriverBasedDataSource.java:205)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnection(AbstractDriverBasedDataSource.java:169)
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122)
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180)
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43)
I get this error even after updating mysql password. I read in some posts that mysql has a new caching mechanism and all clients need to run the following which I already did but still the same error:
 update user set authentication_string='test' where user='root';
 flush privileges   
","<cava><myself><spring><combat>, mysqlnontransientconnectionexception: client support authentic protocol request server; consider upgrade myself client, cause by: com.myself.job.exceptions.jdbc4.mysqlnontransientconnectionexception: client support authentic protocol request server; consider upgrade myself client cava.base/do.internal.reflect.nativeconstructoraccessorimpl.newinstance0(n method) cava.base/do.internal.reflect.nativeconstructoraccessorimpl.newinstance(unknown source) cava.base/do.internal.reflect.delegatingconstructoraccessorimpl.newinstance(unknown source) cava.base/cava.long.reflect.construction.newinstance(unknown source) com.myself.job.until.handlenewinstance(until.cava:406) com.myself.job.until.getinstance(until.cava:381) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:984) com.myself.job.sqlerror.createsqlexception(sqlerror.cava:956) com.myself.job.mysqlio.checkerrorpacket(mysqlio.cava:3558) com.myself.job.mysqlio.checkerrorpacket(mysqlio.cava:3490) com.myself.job.mysqlio.checkerrorpacket(mysqlio.cava:919) com.myself.job.mysqlio.secureauth411(mysqlio.cava:3996) com.myself.job.mysqlio.dohandshake(mysqlio.cava:1284) com.myself.job.connectionimpl.createnewio(connectionimpl.cava:2137) com.myself.job.connectionimpl.&it;knit&it;(connectionimpl.cava:776) com.myself.job.jdbc4connection.&it;knit&it;(jdbc4connection.cava:46) cava.base/do.internal.reflect.nativeconstructoraccessorimpl.newinstance0(n method) cava.base/do.internal.reflect.nativeconstructoraccessorimpl.newinstance(unknown source) cava.base/do.internal.reflect.delegatingconstructoraccessorimpl.newinstance(unknown source) cava.base/cava.long.reflect.construction.newinstance(unknown source) com.myself.job.until.handlenewinstance(until.cava:406) com.myself.job.connectionimpl.getinstance(connectionimpl.cava:352) com.myself.job.nonregisteringdriver.connect(nonregisteringdriver.cava:284) cava.sal/cava.sal.drivermanager.getconnection(unknown source) cava.sal/cava.sal.drivermanager.getconnection(unknown source) org.springframework.job.datasource.drivermanagerdatasource.getconnectionfromdrivermanager(drivermanagerdatasource.cava:154) org.springframework.job.datasource.drivermanagerdatasource.getconnectionfromdriver(drivermanagerdatasource.cava:145) org.springframework.job.datasource.abstractdriverbaseddatasource.getconnectionfromdriver(abstractdriverbaseddatasource.cava:205) org.springframework.job.datasource.abstractdriverbaseddatasource.getconnection(abstractdriverbaseddatasource.cava:169) org.liberate.engine.job.connections.internal.datasourceconnectionproviderimpl.getconnection(datasourceconnectionproviderimpl.cava:122) org.liberate.engine.job.end.internal.jdbcenvironmentinitiator$connectionproviderjdbcconnectionaccess.obtainconnection(jdbcenvironmentinitiator.cava:180) org.liberate.resource.transaction.backed.job.internal.ddltransactionisolatornonjtaimpl.getisolatedconnection(ddltransactionisolatornonjtaimpl.cava:43) get error even update myself password. read post myself new each mean client need run follow already still error: update user set authentication_string='test' user='root'; flush privilege"
54357532,What is the execution order of the PARTITION BY clause compared to other SQL clauses?,"I cannot find any source mentioning execution order for Partition By window functions in SQL.
Is it in the same order as Group By?
For example table like:
Select *, row_number() over (Partition by Name) 
from NPtable 
Where Name = 'Peter'
I understand if Where gets executed first, it will only look at Name = 'Peter', then execute window function that just aggregates this particular person instead of entire table aggregation, which is much more efficient.
But when the query is:
Select top 1 *, row_number() over (Partition by Name order by Date) 
from NPtable 
Where Date &gt; '2018-01-02 00:00:00'
Doesn't the window function need to be executed against the entire table first then applies the Date&gt; condition otherwise the result is wrong?
",<sql><sql-server>,749,2,11,643,1,5,15,51,10947,0.0,32,3,15,2019-01-25 0:48,2019-01-25 2:36,2019-01-25 2:36,0.0,0.0,Basic,5,"<sql><sql-server>, What is the execution order of the PARTITION BY clause compared to other SQL clauses?, I cannot find any source mentioning execution order for Partition By window functions in SQL.
Is it in the same order as Group By?
For example table like:
Select *, row_number() over (Partition by Name) 
from NPtable 
Where Name = 'Peter'
I understand if Where gets executed first, it will only look at Name = 'Peter', then execute window function that just aggregates this particular person instead of entire table aggregation, which is much more efficient.
But when the query is:
Select top 1 *, row_number() over (Partition by Name order by Date) 
from NPtable 
Where Date &gt; '2018-01-02 00:00:00'
Doesn't the window function need to be executed against the entire table first then applies the Date&gt; condition otherwise the result is wrong?
","<sal><sal-server>, execute order partite class compare sal clauses?, cannot find source mention execute order partite window function sal. order group by? example table like: select *, row_number() (partite name) notably name = 'peter' understand get execute first, look name = 'peter', execute window function agree particular person instead enter table aggregation, much efficient. query is: select top 1 *, row_number() (partite name order date) notably date &it; '2018-01-02 00:00:00' window function need execute enter table first apply date&it; conduct otherwise result wrong?"
50761433,How to optimize count and order by query in millions of row,"Needed help in optimizing order by and count query, I have tables having millions (approx 3 millions) rows.
I have to join 4 tables and fetch the records, When i run the simple query it takes only millisecond to complete but as I try to count or order by having left join table it get stuck for unlimited of time.
Please see the cases below.
DB Server Configuration:
CPU Number of virtual cores: 4
Memory(RAM): 16 GiB
Network Performance: High
Rows in each table:
tbl_customers -  #Rows: 20 million.
tbl_customers_address -  #Row 25 million.
tbl_shop_setting - #Rows 50k
aio_customer_tracking - #Rows 5k
Tables Schema:
CREATE TABLE `tbl_customers` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(225) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `accepts_marketing` TINYINT(1) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_order_id` BIGINT(20) NULL DEFAULT NULL,
    `total_spent` DECIMAL(12,2) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `verified_email` TINYINT(4) NULL DEFAULT NULL,
    `updated_at` DATETIME NULL DEFAULT NULL,
    `created_at` DATETIME NULL DEFAULT NULL,
    `date_updated` DATETIME NULL DEFAULT NULL,
    `date_created` DATETIME NULL DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE INDEX `shopify_customer_id_unique` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `shop_id` (`shop_id`)
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_customers_address` (
    `id` BIGINT(20) NOT NULL AUTO_INCREMENT,
    `customer_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_address_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_customer_id` BIGINT(20) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL,
    `last_name` VARCHAR(50) NULL DEFAULT NULL,
    `company` VARCHAR(50) NULL DEFAULT NULL,
    `address1` VARCHAR(250) NULL DEFAULT NULL,
    `address2` VARCHAR(250) NULL DEFAULT NULL,
    `city` VARCHAR(50) NULL DEFAULT NULL,
    `province` VARCHAR(50) NULL DEFAULT NULL,
    `country` VARCHAR(50) NULL DEFAULT NULL,
    `zip` VARCHAR(15) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL,
    `name` VARCHAR(50) NULL DEFAULT NULL,
    `province_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_name` VARCHAR(50) NULL DEFAULT NULL,
    `longitude` VARCHAR(250) NULL DEFAULT NULL,
    `latitude` VARCHAR(250) NULL DEFAULT NULL,
    `default` TINYINT(1) NULL DEFAULT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `customer_id` (`customer_id`),
    INDEX `shopify_address_id` (`shopify_address_id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_shop_setting` (
    `id` INT(11) NOT NULL AUTO_INCREMENT,   
    `shop_name` VARCHAR(300) NOT NULL COLLATE 'latin1_swedish_ci',
     PRIMARY KEY (`id`),
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `aio_customer_tracking` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(255) NULL DEFAULT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `domain` VARCHAR(255) NULL DEFAULT NULL,
    `web_session_count` INT(11) NOT NULL,
    `last_seen_date` DATETIME NULL DEFAULT NULL,
    `last_contact_date` DATETIME NULL DEFAULT NULL,
    `last_email_open` DATETIME NULL DEFAULT NULL,
    `created_date` DATETIME NOT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id_shop_id` (`shopify_customer_id`, `shop_id`),
    INDEX `last_seen_date` (`last_seen_date`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
Query Cases Running and Not Running:
1. Running:  Below query fetch the records by joining all the 4 tables, It takes only 0.300 ms.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
2. Not running: Simply when try to get the count of these row stuk the query, I waited 10 min but still running.
SELECT 
     COUNT(DISTINCT c.shopify_customer_id)   -- what makes #2 different
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
3. Not running: In the #1 query we simply put the 1 Order by clause and it get stuck, I waited 10 min but still running. I study query optimization some article and tried by indexing, Right Join etc.. but still not working.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
  ORDER BY `t`.`last_seen_date`    -- what makes #3 different
LIMIT 20
EXPLAIN QUERY #1:
EXPLAIN QUERY #2:
EXPLAIN QUERY #3:
Any suggestion to optimize the query, table structure are welcome.
WHAT I'M TRYING TO DO:
tbl_customers table contains the customer info, tbl_customer_address table contains the addresses of the customers(one customer may have multiple address), And aio_customer_tracking table contains visiting records of the customer last_seen_date is the visiting date.
Now, simply I want to fetch and count the customers, with their one of the address, and visiting info. Also, I may order by any of the column from these 3 tables, In my example i am ordering by last_seen_date (the default order). Hope this explanation helps to understand what i am trying to do. 
",<mysql><database><database-administration>,7174,3,129,688,3,13,35,57,4231,0.0,12,4,15,2018-06-08 12:44,2018-06-11 4:51,,3.0,,Intermediate,23,"<mysql><database><database-administration>, How to optimize count and order by query in millions of row, Needed help in optimizing order by and count query, I have tables having millions (approx 3 millions) rows.
I have to join 4 tables and fetch the records, When i run the simple query it takes only millisecond to complete but as I try to count or order by having left join table it get stuck for unlimited of time.
Please see the cases below.
DB Server Configuration:
CPU Number of virtual cores: 4
Memory(RAM): 16 GiB
Network Performance: High
Rows in each table:
tbl_customers -  #Rows: 20 million.
tbl_customers_address -  #Row 25 million.
tbl_shop_setting - #Rows 50k
aio_customer_tracking - #Rows 5k
Tables Schema:
CREATE TABLE `tbl_customers` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(225) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `accepts_marketing` TINYINT(1) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_order_id` BIGINT(20) NULL DEFAULT NULL,
    `total_spent` DECIMAL(12,2) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `verified_email` TINYINT(4) NULL DEFAULT NULL,
    `updated_at` DATETIME NULL DEFAULT NULL,
    `created_at` DATETIME NULL DEFAULT NULL,
    `date_updated` DATETIME NULL DEFAULT NULL,
    `date_created` DATETIME NULL DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE INDEX `shopify_customer_id_unique` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `shop_id` (`shop_id`)
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_customers_address` (
    `id` BIGINT(20) NOT NULL AUTO_INCREMENT,
    `customer_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_address_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_customer_id` BIGINT(20) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL,
    `last_name` VARCHAR(50) NULL DEFAULT NULL,
    `company` VARCHAR(50) NULL DEFAULT NULL,
    `address1` VARCHAR(250) NULL DEFAULT NULL,
    `address2` VARCHAR(250) NULL DEFAULT NULL,
    `city` VARCHAR(50) NULL DEFAULT NULL,
    `province` VARCHAR(50) NULL DEFAULT NULL,
    `country` VARCHAR(50) NULL DEFAULT NULL,
    `zip` VARCHAR(15) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL,
    `name` VARCHAR(50) NULL DEFAULT NULL,
    `province_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_name` VARCHAR(50) NULL DEFAULT NULL,
    `longitude` VARCHAR(250) NULL DEFAULT NULL,
    `latitude` VARCHAR(250) NULL DEFAULT NULL,
    `default` TINYINT(1) NULL DEFAULT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `customer_id` (`customer_id`),
    INDEX `shopify_address_id` (`shopify_address_id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_shop_setting` (
    `id` INT(11) NOT NULL AUTO_INCREMENT,   
    `shop_name` VARCHAR(300) NOT NULL COLLATE 'latin1_swedish_ci',
     PRIMARY KEY (`id`),
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `aio_customer_tracking` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(255) NULL DEFAULT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `domain` VARCHAR(255) NULL DEFAULT NULL,
    `web_session_count` INT(11) NOT NULL,
    `last_seen_date` DATETIME NULL DEFAULT NULL,
    `last_contact_date` DATETIME NULL DEFAULT NULL,
    `last_email_open` DATETIME NULL DEFAULT NULL,
    `created_date` DATETIME NOT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id_shop_id` (`shopify_customer_id`, `shop_id`),
    INDEX `last_seen_date` (`last_seen_date`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
Query Cases Running and Not Running:
1. Running:  Below query fetch the records by joining all the 4 tables, It takes only 0.300 ms.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
2. Not running: Simply when try to get the count of these row stuk the query, I waited 10 min but still running.
SELECT 
     COUNT(DISTINCT c.shopify_customer_id)   -- what makes #2 different
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
3. Not running: In the #1 query we simply put the 1 Order by clause and it get stuck, I waited 10 min but still running. I study query optimization some article and tried by indexing, Right Join etc.. but still not working.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
  ORDER BY `t`.`last_seen_date`    -- what makes #3 different
LIMIT 20
EXPLAIN QUERY #1:
EXPLAIN QUERY #2:
EXPLAIN QUERY #3:
Any suggestion to optimize the query, table structure are welcome.
WHAT I'M TRYING TO DO:
tbl_customers table contains the customer info, tbl_customer_address table contains the addresses of the customers(one customer may have multiple address), And aio_customer_tracking table contains visiting records of the customer last_seen_date is the visiting date.
Now, simply I want to fetch and count the customers, with their one of the address, and visiting info. Also, I may order by any of the column from these 3 tables, In my example i am ordering by last_seen_date (the default order). Hope this explanation helps to understand what i am trying to do. 
","<myself><database><database-administration>, optic count order query million row, need help optic order count query, table million (approve 3 millions) rows. join 4 table fetch records, run simple query take millisecond complete try count order left join table get stuck limit time. pleas see case below. do server configuration: cup number virtual comes: 4 memory(ram): 16 rib network performance: high row table: tbl_custom - #rows: 20 million. tbl_customers_address - #row 25 million. tbl_shop_set - #row ask aio_customer_track - #row k table scheme: great table `tbl_customers` ( `id` begin(20) ensign null auto_increment, `shopify_customer_id` begin(20) ensign null, `shop_id` begin(20) ensign null, `email` varchar(225) null default null collar 'latin1_swedish_ci', `accepts_marketing` tinyint(1) null default null, `first_name` varchar(50) null default null collar 'latin1_swedish_ci', `last_name` varchar(50) null default null collar 'latin1_swedish_ci', `last_order_id` begin(20) null default null, `total_spent` denial(12,2) null default null, `phone` varchar(20) null default null collar 'latin1_swedish_ci', `verified_email` tinyint(4) null default null, `updated_at` datetim null default null, `created_at` datetim null default null, `date_updated` datetim null default null, `date_created` datetim null default null, primary key (`id`), unique index `shopify_customer_id_unique` (`shopify_customer_id`), index `email` (`email`), index `shopify_customer_id` (`shopify_customer_id`), index `shop_id` (`shop_id`) ) collar='utf8mb4_general_ci' engine=innodb; great table `tbl_customers_address` ( `id` begin(20) null auto_increment, `customer_id` begin(20) null default null, `shopify_address_id` begin(20) null default null, `shopify_customer_id` begin(20) null default null, `first_name` varchar(50) null default null, `last_name` varchar(50) null default null, `company` varchar(50) null default null, `address` varchar(250) null default null, `address` varchar(250) null default null, `city` varchar(50) null default null, `province` varchar(50) null default null, `country` varchar(50) null default null, `zip` varchar(15) null default null, `phone` varchar(20) null default null, `name` varchar(50) null default null, `province_code` varchar(5) null default null, `country_code` varchar(5) null default null, `country_name` varchar(50) null default null, `longitude` varchar(250) null default null, `latitude` varchar(250) null default null, `default` tinyint(1) null default null, `is_geo_fetched` tinyint(1) null default '0', primary key (`id`), index `customer_id` (`customer_id`), index `shopify_address_id` (`shopify_address_id`), index `shopify_customer_id` (`shopify_customer_id`) ) collar='latin1_swedish_ci' engine=innodb; great table `tbl_shop_setting` ( `id` in(11) null auto_increment, `shop_name` varchar(300) null collar 'latin1_swedish_ci', primary key (`id`), ) collar='utf8mb4_general_ci' engine=innodb; great table `aio_customer_tracking` ( `id` begin(20) ensign null auto_increment, `shopify_customer_id` begin(20) ensign null, `email` varchar(255) null default null, `shop_id` begin(20) ensign null, `domain` varchar(255) null default null, `web_session_count` in(11) null, `last_seen_date` datetim null default null, `last_contact_date` datetim null default null, `last_email_open` datetim null default null, `created_date` datetim null, `is_geo_fetched` tinyint(1) null default '0', primary key (`id`), index `shopify_customer_id` (`shopify_customer_id`), index `email` (`email`), index `shopify_customer_id_shop_id` (`shopify_customer_id`, `shop_id`), index `last_seen_date` (`last_seen_date`) ) collar='latin1_swedish_ci' engine=innodb; query case run running: 1. running: query fetch record join 4 tables, take 0.300 ms. select `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ash`.`shop_name`, ca.`company`, ca.`address`, ca.`address`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code` `tbl_customers` `c` join `tbl_shop_setting` `ash` c.shop_id = ash.id left join (select shopify_customer_id, last_seen_date, last_contact_d aio_customer_track group shopify_customer_id) t.shopify_customer_id = c.shopify_customer_id left join `tbl_customers_address` ca (c.shopify_customer_id = ca.shopify_customer_id ca.default = 1) group c.shopify_customer_id limit 20 2. running: simple try get count row stuck query, wait 10 min still running. select count(distinct c.shopify_customer_id) -- make #2 differ `tbl_customers` `c` join `tbl_shop_setting` `ash` c.shop_id = ash.id left join (select shopify_customer_id, last_seen_date, last_contact_d aio_customer_track group shopify_customer_id) t.shopify_customer_id = c.shopify_customer_id left join `tbl_customers_address` ca (c.shopify_customer_id = ca.shopify_customer_id ca.default = 1) group c.shopify_customer_id limit 20 3. running: #1 query simple put 1 order class get stuck, wait 10 min still running. study query optic article try indexing, right join etc.. still working. select `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ash`.`shop_name`, ca.`company`, ca.`address`, ca.`address`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code` `tbl_customers` `c` join `tbl_shop_setting` `ash` c.shop_id = ash.id left join (select shopify_customer_id, last_seen_date, last_contact_d aio_customer_track group shopify_customer_id) t.shopify_customer_id = c.shopify_customer_id left join `tbl_customers_address` ca (c.shopify_customer_id = ca.shopify_customer_id ca.default = 1) group c.shopify_customer_id order `t`.`last_seen_date` -- make #3 differ limit 20 explain query #1: explain query #2: explain query #3: suggest optic query, table structure welcome. i'm try do: tbl_custom table contain custom into, tbl_customer_address table contain address customers(on custom may multiple address), aio_customer_track table contain visit record custom last_seen_d visit date. now, simple want fetch count customers, one address, visit into. also, may order column 3 tables, example order last_seen_d (the default order). hope explain help understand try do."
53663954,Trouble connecting to postgres from outside Kubernetes cluster,"I've launched a postgresql server in minikube, and I'm having difficulty connecting to it from outside the cluster.
Update
It turned out my cluster was suffering from unrelated problems, causing all sorts of broken behavior. I ended up nuking the whole cluster and vm and starting from scratch. Now I've got working. I changed the deployment to a statefulset, though I think it could work either way.
Setup and test:
kubectl --context=minikube create -f postgres-statefulset.yaml
kubectl --context=minikube create -f postgres-service.yaml
url=$(minikube service postgres --url --format={{.IP}}:{{.Port}})
psql --host=${url%:*} --port=${url#*:} --username=postgres --dbname=postgres \
     --command='SELECT refobjid FROM pg_depend LIMIT 1'
Password for user postgres:
 refobjid
----------
     1247
postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  selector:
    app: postgres
  type: NodePort
  ports:
    - name: postgres
      port: 5432
      targetPort: 5432
      protocol: TCP
postgres-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
      role: service
  serviceName: postgres
  template:
    metadata:
      labels:
        app: postgres
        role: service
    spec:
      containers:
        - name: postgres
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
              name: postgres
              protocol: TCP
Original question
I created a deployment running one container (postgres-container) and a NodePort (postgres-service). I can connect to postgresql from within the pod itself:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- psql --port=5432 --username=postgres --dbname=postgres
But I can't connect through the service.
$ minikube service --url postgres-service
http://192.168.99.100:32254
$ psql --host=192.168.99.100 --port=32254 --username=postgres --dbname=postgres
psql: could not connect to server: Connection refused
        Is the server running on host ""192.168.99.100"" and accepting
        TCP/IP connections on port 32254?
I think postgres is correctly configured to accept remote TCP connections:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- tail /var/lib/postgresql/data/pg_hba.conf
host    all             all             127.0.0.1/32            trust
...
host all all all md5
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- grep listen_addresses /var/lib/postgresql/data/postgresql.conf
listen_addresses = '*'
My service definition looks like:
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres-container
  type: NodePort
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
And the deployment is:
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-container
  template:
    metadata:
      labels:
        app: postgres-container
    spec:
      containers:
        - name: postgres-container
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
The resulting service configuration:
$ kubectl --context=minikube get service postgres-service -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-12-07T05:29:22Z
  name: postgres-service
  namespace: default
  resourceVersion: ""194827""
  selfLink: /api/v1/namespaces/default/services/postgres-service
  uid: 0da6bc36-f9e1-11e8-84ea-080027a52f02
spec:
  clusterIP: 10.109.120.251
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32254
    port: 5432
    protocol: TCP
    targetPort: 5432
  selector:
    app: postgres-container
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
I can connect if I use port-forward, but I'd like to use the nodePort instead. What am I missing?
",<postgresql><kubernetes>,4501,1,143,6082,9,43,67,74,30418,0.0,135,2,15,2018-12-07 5:47,2018-12-08 8:13,,1.0,,Intermediate,23,"<postgresql><kubernetes>, Trouble connecting to postgres from outside Kubernetes cluster, I've launched a postgresql server in minikube, and I'm having difficulty connecting to it from outside the cluster.
Update
It turned out my cluster was suffering from unrelated problems, causing all sorts of broken behavior. I ended up nuking the whole cluster and vm and starting from scratch. Now I've got working. I changed the deployment to a statefulset, though I think it could work either way.
Setup and test:
kubectl --context=minikube create -f postgres-statefulset.yaml
kubectl --context=minikube create -f postgres-service.yaml
url=$(minikube service postgres --url --format={{.IP}}:{{.Port}})
psql --host=${url%:*} --port=${url#*:} --username=postgres --dbname=postgres \
     --command='SELECT refobjid FROM pg_depend LIMIT 1'
Password for user postgres:
 refobjid
----------
     1247
postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  selector:
    app: postgres
  type: NodePort
  ports:
    - name: postgres
      port: 5432
      targetPort: 5432
      protocol: TCP
postgres-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
      role: service
  serviceName: postgres
  template:
    metadata:
      labels:
        app: postgres
        role: service
    spec:
      containers:
        - name: postgres
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
              name: postgres
              protocol: TCP
Original question
I created a deployment running one container (postgres-container) and a NodePort (postgres-service). I can connect to postgresql from within the pod itself:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- psql --port=5432 --username=postgres --dbname=postgres
But I can't connect through the service.
$ minikube service --url postgres-service
http://192.168.99.100:32254
$ psql --host=192.168.99.100 --port=32254 --username=postgres --dbname=postgres
psql: could not connect to server: Connection refused
        Is the server running on host ""192.168.99.100"" and accepting
        TCP/IP connections on port 32254?
I think postgres is correctly configured to accept remote TCP connections:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- tail /var/lib/postgresql/data/pg_hba.conf
host    all             all             127.0.0.1/32            trust
...
host all all all md5
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- grep listen_addresses /var/lib/postgresql/data/postgresql.conf
listen_addresses = '*'
My service definition looks like:
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres-container
  type: NodePort
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
And the deployment is:
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-container
  template:
    metadata:
      labels:
        app: postgres-container
    spec:
      containers:
        - name: postgres-container
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
The resulting service configuration:
$ kubectl --context=minikube get service postgres-service -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-12-07T05:29:22Z
  name: postgres-service
  namespace: default
  resourceVersion: ""194827""
  selfLink: /api/v1/namespaces/default/services/postgres-service
  uid: 0da6bc36-f9e1-11e8-84ea-080027a52f02
spec:
  clusterIP: 10.109.120.251
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32254
    port: 5432
    protocol: TCP
    targetPort: 5432
  selector:
    app: postgres-container
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
I can connect if I use port-forward, but I'd like to use the nodePort instead. What am I missing?
","<postgresql><kubernetes>, trouble connect poster outside kubernet cluster, i'v launch postgresql server minikube, i'm difficult connect outside cluster. update turn cluster suffer unreal problems, cause sort broken behavior. end duke whole cluster am start scratch. i'v got working. change deploy statefulset, though think could work either way. set test: kubectl --context=minimum great -f postures-statefulset.all kubectl --context=minimum great -f postures-service.all curl=$(minimum service poster --curl --format={{.in}}:{{.port}}) pool --host=${curl%:*} --port=${curl#*:} --surname=poster --name=poster \ --command='select refobjid pg_depend limit 1' password user postures: refobjid ---------- 1247 postures-service.all diversion: ve kind: service metadata: name: poster labels: pp: poster role: service speck: elector: pp: poster type: nodeport ports: - name: poster port: 5432 targetport: 5432 protocol: top postures-statefulset.all diversion: apes/v1beta2 kind: statefulset metadata: name: poster labels: pp: poster role: service speck: replies: 1 elector: matchlabels: pp: poster role: service servicename: poster temple: metadata: labels: pp: poster role: service speck: container: - name: poster image: postures:9.6 end: - name: postgres_us value: poster - name: postgres_password value: poster - name: postgres_db value: poster ports: - containerport: 5432 name: poster protocol: top origin question great deploy run one contain (postures-container) nodeport (postures-service). connect postgresql within god itself: $ kubectl --context=minimum even -it postures-employment-7fbf655986-r49s2 \ -- pool --port=5432 --surname=poster --name=poster can't connect service. $ minimum service --curl postures-service http://192.168.99.100:32254 $ pool --host=192.168.99.100 --port=32254 --surname=poster --name=poster pool: could connect server: connect refuse server run host ""192.168.99.100"" accept top/in connect port 32254? think poster correctly configur accept remote top connections: $ kubectl --context=minimum even -it postures-employment-7fbf655986-r49s2 \ -- tail /war/limb/postgresql/data/pg_hba.cone host 127.0.0.1/32 trust ... host md $ kubectl --context=minimum even -it postures-employment-7fbf655986-r49s2 \ -- grew listen_address /war/limb/postgresql/data/postgresql.cone listen_address = '*' service definite look like: diversion: ve kind: service metadata: name: postures-service speck: elector: pp: postures-contain type: nodeport ports: - port: 5432 targetport: 5432 protocol: top deploy is: diversion: apes/v1beta2 kind: deploy metadata: name: postures-deploy speck: replies: 1 elector: matchlabels: pp: postures-contain temple: metadata: labels: pp: postures-contain speck: container: - name: postures-contain image: postures:9.6 end: - name: postgres_us value: poster - name: postgres_password value: poster - name: postgres_db value: poster ports: - containerport: 5432 result service configuration: $ kubectl --context=minimum get service postures-service -o all diversion: ve kind: service metadata: creationtimestamp: 2018-12-07t05:29:z name: postures-service namesake: default resourceversion: ""194827"" selling: /apt/ve/namespaces/default/services/postures-service did: 0da6bc36-few-11e8-area-080027a52f02 speck: cluster: 10.109.120.251 externaltrafficpolicy: cluster ports: - nodeport: 32254 port: 5432 protocol: top targetport: 5432 elector: pp: postures-contain sessionaffinity: none type: nodeport status: loadbalancer: {} connect use port-forward, i'd like use nodeport instead. missing?"
50547724,How to resolve the error: SQL authentication method unknown in Laravel-MySql,"I using docker and I have a container of Laravel Framework 5.5.25 and other with mysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL). in my configuration of docker compose I have this: 
version: ""2""
services:
    mysql:
    image: mysql
        ports:
            - ""3307:3306""
        command: --sql_mode=""""
So, when Laravel try to connect to MySql I have this error: 
  SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from
",<php><docker><docker-compose><laravel-5.5><mysql-8.0>,497,0,7,351,2,3,12,46,21493,0.0,25,5,15,2018-05-26 23:11,2018-06-27 7:00,,32.0,,Basic,14,"<php><docker><docker-compose><laravel-5.5><mysql-8.0>, How to resolve the error: SQL authentication method unknown in Laravel-MySql, I using docker and I have a container of Laravel Framework 5.5.25 and other with mysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL). in my configuration of docker compose I have this: 
version: ""2""
services:
    mysql:
    image: mysql
        ports:
            - ""3307:3306""
        command: --sql_mode=""""
So, when Laravel try to connect to MySql I have this error: 
  SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from
","<pp><doctor><doctor-compose><travel-5.5><myself-8.0>, resolve error: sal authentic method unknown travel-myself, use doctor contain travel framework 5.5.25 myself ver 8.0.11 line x86_64 (myself common server - gal). configur doctor compose this: version: ""2"" services: myself: image: myself ports: - ""3307:3306"" command: --sql_mode="""" so, travel try connect myself error: sqlstate[hy000] [2054] server request authentic method unknown client (sal: select *"
48630408,SSIS Script Tasks losing code,"I have a very strange issue happening that is causing Script Task code to clear out. I have been able to test on 2-3 different machines. We are running SSDT 15.4 preview. The steps to reproduce were as follows.
Create a script task inside of a foreach loop container. 
Create a comment in the script task. 
Change or add a variable mapping in the foreach. 
Save package. 
Close the package.
Open the package.
Open the script task and the comment will have vanished.
As my last attempt for success,I have upgraded to 15.5.1 and the problem still exists.
",<sql-server><ssis><etl><sql-server-data-tools><script-task>,553,0,0,349,0,4,12,40,3756,0.0,6,3,15,2018-02-05 19:50,2018-03-27 21:04,,50.0,,Basic,14,"<sql-server><ssis><etl><sql-server-data-tools><script-task>, SSIS Script Tasks losing code, I have a very strange issue happening that is causing Script Task code to clear out. I have been able to test on 2-3 different machines. We are running SSDT 15.4 preview. The steps to reproduce were as follows.
Create a script task inside of a foreach loop container. 
Create a comment in the script task. 
Change or add a variable mapping in the foreach. 
Save package. 
Close the package.
Open the package.
Open the script task and the comment will have vanished.
As my last attempt for success,I have upgraded to 15.5.1 and the problem still exists.
","<sal-server><suis><etc><sal-server-data-tools><script-task>, si script task lose code, strange issue happen cause script task code clear out. all test 2-3 differ machines. run side 15.4 review. step reproduce follows. great script task inside french loop container. great comment script task. change add variable map french. save package. close package. open package. open script task comment vanished. last attempt success,i upgrade 15.5.1 problem still exists."
48087980,Annotate QuerySet with first value of ordered related model,"I have a QuerySet of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM.
Background
Let's say that I have two related models: Book, and BlogPost, each with a foreign key to an Author:
class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()
class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing.
Working solution in PostgreSQL 9.6
WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
Translating to Django's ORM
While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further
books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now.
Attempted solution
I'd first built a Q object to filter down to mystery books published after the blog post.
published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
From here, I attempted to piece together django.db.models.Min and additional F objects to acheive my desired results, but with no success.
Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.
",<python><django><postgresql><django-queryset><django-1.10>,2289,0,39,16645,14,66,76,55,2258,,2247,2,15,2018-01-04 2:47,2018-01-11 9:29,2018-01-11 9:29,7.0,7.0,Intermediate,16,"<python><django><postgresql><django-queryset><django-1.10>, Annotate QuerySet with first value of ordered related model, I have a QuerySet of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM.
Background
Let's say that I have two related models: Book, and BlogPost, each with a foreign key to an Author:
class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()
class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing.
Working solution in PostgreSQL 9.6
WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
Translating to Django's ORM
While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further
books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now.
Attempted solution
I'd first built a Q object to filter down to mystery books published after the blog post.
published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
From here, I attempted to piece together django.db.models.Min and additional F objects to acheive my desired results, but with no success.
Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.
","<patron><django><postgresql><django-queryset><django-1.10>, cannot queryset first value order relate model, queryset objects. one, wish cannot minimum value relate model (join conditions, order date). express desire result neatly sal, curious translate django' or. background let' say two relate models: book, blogpost, foreign key author: class book(models.model): till = models.garfield(max_length=255) gear = models.garfield(max_length=63) author = models.foreigner(author) date_publish = models.oatfield() class blogpost(models.model): author = models.foreigner(author) date_publish = models.oatfield() i'm try find first mystery book given author publish blow post write. sal, achieve nice winding. work slut postgresql 9.6 order ( select blog_post.id, book.title, row_number() ( partite blog_post.id order book.date_publish ) in blog_post left join book book.author_id = blog_post.author_id book.gear = 'mystery' book.date_publish &it;= blog_post.date_publish ) select id, till order in = 1; translate django' or sal suit need well (and could use raw sal needed), i'm curious one would queryset. exist queryset i'd like cannot even book = models.book.objects.filter(...).select_related(...).prefetch_related(...) annotated_book = books.annette( most_recent_title=... ) i'm war django 2.0 support window functions, i'm django 1.10 now. attempt slut i'd first built q object filter mystery book publish blow post. published_aft = q( author__book__date_published__gte=f('date_published'), author__book__genre='mystery' ) here, attempt piece together django.do.models.min admit f object achieve desire results, success. note: django 2.0 introduce window expressions, i'm current django 1.10, curious one would queryset feature avail there."
51227958,SQLAlchemy - check if query found any results,"How can I check if a query found any results?
result = db.engine.execute(sql, id=foo)
// check if result has rows ...
for row in result:
  ...
Tried this:
if result is None:
    print(""reuslt is None!"")
and checked length:
print(""result len"", len(result))
",<sqlalchemy><flask-sqlalchemy>,256,0,7,11791,26,86,143,68,26581,0.0,880,6,15,2018-07-08 0:31,2018-07-09 5:48,,1.0,,Basic,9,"<sqlalchemy><flask-sqlalchemy>, SQLAlchemy - check if query found any results, How can I check if a query found any results?
result = db.engine.execute(sql, id=foo)
// check if result has rows ...
for row in result:
  ...
Tried this:
if result is None:
    print(""reuslt is None!"")
and checked length:
print(""result len"", len(result))
","<sqlalchemy><flask-sqlalchemy>, sqlalchemi - check query found results, check query found results? result = do.engine.execute(sal, id=foo) // check result row ... row result: ... try this: result none: print(""result none!"") check length: print(""result len"", len(result))"
48315442,Error while exploding a struct column in Spark,"I have a dataframe whose schema looks like this:
event: struct (nullable = true)
|    | event_category: string (nullable = true)
|    | event_name: string (nullable = true)
|    | properties: struct (nullable = true)
|    |    | ErrorCode: string (nullable = true)
|    |    | ErrorDescription: string (nullable = true)
I am trying to explode the struct column properties using the following code: 
df_json.withColumn(""event_properties"", explode($""event.properties""))
But it is throwing the following exception: 
cannot resolve 'explode(`event`.`properties`)' due to data type mismatch: 
input to function explode should be array or map type, 
not StructType(StructField(IDFA,StringType,true),
How to explode the column properties?
",<scala><apache-spark><pyspark><apache-spark-sql>,732,0,13,463,1,8,18,43,30529,0.0,45,3,15,2018-01-18 6:55,2018-01-18 8:12,2018-01-18 9:48,0.0,0.0,Basic,2,"<scala><apache-spark><pyspark><apache-spark-sql>, Error while exploding a struct column in Spark, I have a dataframe whose schema looks like this:
event: struct (nullable = true)
|    | event_category: string (nullable = true)
|    | event_name: string (nullable = true)
|    | properties: struct (nullable = true)
|    |    | ErrorCode: string (nullable = true)
|    |    | ErrorDescription: string (nullable = true)
I am trying to explode the struct column properties using the following code: 
df_json.withColumn(""event_properties"", explode($""event.properties""))
But it is throwing the following exception: 
cannot resolve 'explode(`event`.`properties`)' due to data type mismatch: 
input to function explode should be array or map type, 
not StructType(StructField(IDFA,StringType,true),
How to explode the column properties?
","<scala><apache-spark><spark><apache-spark-sal>, error explode struck column spark, datafram whose scheme look like this: event: struck (nullabl = true) | | event_category: string (nullabl = true) | | event_name: string (nullabl = true) | | properties: struck (nullabl = true) | | | errorcode: string (nullabl = true) | | | errordescription: string (nullabl = true) try explode struck column property use follow code: df_json.withcolumn(""event_properties"", explode($""event.properties"")) throw follow exception: cannot resolve 'explode(`event`.`properties`)' due data type dispatch: input function explode array map type, structtype(structfield(idea,stringtype,true), explode column properties?"
51083593,Parent count based on pairing of multiple children,"In the below example, I'm trying to count the number of drinks I can make based on the availability of ingredients per bar location that I have.
To further clarify, as seen in the below example: based on the figures highlighted in the chart below; I know that I can only make 1 Margarita on 6/30/2018 (in either DC or FL if I ship the supplies to the location).
Sample of data table
Please use the below code to enter the relevant data above:
    CREATE TABLE #drinks 
    (
        a_date      DATE,
        loc         NVARCHAR(2),
        parent      NVARCHAR(20),
        line_num    INT,
        child       NVARCHAR(20),
        avail_amt   INT
    );
INSERT INTO #drinks VALUES ('6/26/2018','CA','Long Island','1','Vodka','7');
INSERT INTO #drinks VALUES ('6/27/2018','CA','Long Island','2','Gin','5');
INSERT INTO #drinks VALUES ('6/28/2018','CA','Long Island','3','Rum','26');
INSERT INTO #drinks VALUES ('6/26/2018','DC','Long Island','1','Vodka','15');
INSERT INTO #drinks VALUES ('6/27/2018','DC','Long Island','2','Gin','18');
INSERT INTO #drinks VALUES ('6/28/2018','DC','Long Island','3','Rum','5');
INSERT INTO #drinks VALUES ('6/26/2018','FL','Long Island','1','Vodka','34');
INSERT INTO #drinks VALUES ('6/27/2018','FL','Long Island','2','Gin','14');
INSERT INTO #drinks VALUES ('6/28/2018','FL','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Margarita','1','Tequila','6');
INSERT INTO #drinks VALUES ('7/1/2018','DC','Margarita','2','Triple Sec','3');
INSERT INTO #drinks VALUES ('6/29/2018','FL','Margarita','1','Tequila','1');
INSERT INTO #drinks VALUES ('6/30/2018','FL','Margarita','2','Triple Sec','0');
INSERT INTO #drinks VALUES ('7/2/2018','CA','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/8/2018','CA','Cuba Libre','2','Coke','5');
INSERT INTO #drinks VALUES ('7/13/2018','CA','Cuba Libre','3','Lime','14');
INSERT INTO #drinks VALUES ('7/5/2018','DC','Cuba Libre','1','Rum','0');
INSERT INTO #drinks VALUES ('7/19/2018','DC','Cuba Libre','2','Coke','12');
INSERT INTO #drinks VALUES ('7/31/2018','DC','Cuba Libre','3','Lime','9');
INSERT INTO #drinks VALUES ('7/2/2018','FL','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/19/2018','FL','Cuba Libre','2','Coke','3');
INSERT INTO #drinks VALUES ('7/17/2018','FL','Cuba Libre','3','Lime','2');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('7/7/2018','FL','Cosmopolitan','5','Triple Sec','7');
The expected results are as follows:
Please note, as seen in the expected results, children are interchangeable. For example, on 7/7/2018 Triple Sec arrived for the drink cosmopolitan; however because the child is also rum, it changes the availability of Margaritas for FL.
Also not the update to the DC region for Cuba Libre's on both 06/30 and 06/31.
Please take into consideration that parts are interchangeable and also that each time a new item arrives it makes available any item previously now.
Lastly - It would be awesome if I could add another column that shows kit availability regardless of location based only on availability of the child. For Ex. If there is a child #3 in DC and none in FL they FL can assume that they have enough inventory to make drink based on inventory in another location!
",<sql><sql-server><parent-child><cross-apply><outer-apply>,3294,2,34,165,0,0,6,37,375,0.0,0,4,15,2018-06-28 13:00,2018-06-29 18:15,,1.0,,Advanced,32,"<sql><sql-server><parent-child><cross-apply><outer-apply>, Parent count based on pairing of multiple children, In the below example, I'm trying to count the number of drinks I can make based on the availability of ingredients per bar location that I have.
To further clarify, as seen in the below example: based on the figures highlighted in the chart below; I know that I can only make 1 Margarita on 6/30/2018 (in either DC or FL if I ship the supplies to the location).
Sample of data table
Please use the below code to enter the relevant data above:
    CREATE TABLE #drinks 
    (
        a_date      DATE,
        loc         NVARCHAR(2),
        parent      NVARCHAR(20),
        line_num    INT,
        child       NVARCHAR(20),
        avail_amt   INT
    );
INSERT INTO #drinks VALUES ('6/26/2018','CA','Long Island','1','Vodka','7');
INSERT INTO #drinks VALUES ('6/27/2018','CA','Long Island','2','Gin','5');
INSERT INTO #drinks VALUES ('6/28/2018','CA','Long Island','3','Rum','26');
INSERT INTO #drinks VALUES ('6/26/2018','DC','Long Island','1','Vodka','15');
INSERT INTO #drinks VALUES ('6/27/2018','DC','Long Island','2','Gin','18');
INSERT INTO #drinks VALUES ('6/28/2018','DC','Long Island','3','Rum','5');
INSERT INTO #drinks VALUES ('6/26/2018','FL','Long Island','1','Vodka','34');
INSERT INTO #drinks VALUES ('6/27/2018','FL','Long Island','2','Gin','14');
INSERT INTO #drinks VALUES ('6/28/2018','FL','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Margarita','1','Tequila','6');
INSERT INTO #drinks VALUES ('7/1/2018','DC','Margarita','2','Triple Sec','3');
INSERT INTO #drinks VALUES ('6/29/2018','FL','Margarita','1','Tequila','1');
INSERT INTO #drinks VALUES ('6/30/2018','FL','Margarita','2','Triple Sec','0');
INSERT INTO #drinks VALUES ('7/2/2018','CA','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/8/2018','CA','Cuba Libre','2','Coke','5');
INSERT INTO #drinks VALUES ('7/13/2018','CA','Cuba Libre','3','Lime','14');
INSERT INTO #drinks VALUES ('7/5/2018','DC','Cuba Libre','1','Rum','0');
INSERT INTO #drinks VALUES ('7/19/2018','DC','Cuba Libre','2','Coke','12');
INSERT INTO #drinks VALUES ('7/31/2018','DC','Cuba Libre','3','Lime','9');
INSERT INTO #drinks VALUES ('7/2/2018','FL','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/19/2018','FL','Cuba Libre','2','Coke','3');
INSERT INTO #drinks VALUES ('7/17/2018','FL','Cuba Libre','3','Lime','2');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('7/7/2018','FL','Cosmopolitan','5','Triple Sec','7');
The expected results are as follows:
Please note, as seen in the expected results, children are interchangeable. For example, on 7/7/2018 Triple Sec arrived for the drink cosmopolitan; however because the child is also rum, it changes the availability of Margaritas for FL.
Also not the update to the DC region for Cuba Libre's on both 06/30 and 06/31.
Please take into consideration that parts are interchangeable and also that each time a new item arrives it makes available any item previously now.
Lastly - It would be awesome if I could add another column that shows kit availability regardless of location based only on availability of the child. For Ex. If there is a child #3 in DC and none in FL they FL can assume that they have enough inventory to make drink based on inventory in another location!
","<sal><sal-server><parent-child><cross-apply><outer-apply>, parent count base pair multiple children, example, i'm try count number drink make base avail inured per bar local have. clarify, seen example: base figure highlight chart below; know make 1 margaret 6/30/2018 (in either do ll ship supply location). sample data table pleas use code enter rule data above: great table #drink ( arbat date, low nvarchar(2), parent nvarchar(20), line_num in, child nvarchar(20), avail_amt in ); insert #drink value ('6/26/2018','ca','long island','1','vodka','7'); insert #drink value ('6/27/2018','ca','long island','2','gin','5'); insert #drink value ('6/28/2018','ca','long island','3','rum','26'); insert #drink value ('6/26/2018','do','long island','1','vodka','15'); insert #drink value ('6/27/2018','do','long island','2','gin','18'); insert #drink value ('6/28/2018','do','long island','3','rum','5'); insert #drink value ('6/26/2018','ll','long island','1','vodka','34'); insert #drink value ('6/27/2018','ll','long island','2','gin','14'); insert #drink value ('6/28/2018','ll','long island','3','rum','4'); insert #drink value ('6/30/2018','do','margaret','1','tutuila','6'); insert #drink value ('7/1/2018','do','margaret','2','trial see','3'); insert #drink value ('6/29/2018','ll','margaret','1','tutuila','1'); insert #drink value ('6/30/2018','ll','margaret','2','trial see','0'); insert #drink value ('7/2/2018','ca','cuba fibre','1','rum','1'); insert #drink value ('7/8/2018','ca','cuba fibre','2','come','5'); insert #drink value ('7/13/2018','ca','cuba fibre','3','lime','14'); insert #drink value ('7/5/2018','do','cuba fibre','1','rum','0'); insert #drink value ('7/19/2018','do','cuba fibre','2','come','12'); insert #drink value ('7/31/2018','do','cuba fibre','3','lime','9'); insert #drink value ('7/2/2018','ll','cuba fibre','1','rum','1'); insert #drink value ('7/19/2018','ll','cuba fibre','2','come','3'); insert #drink value ('7/17/2018','ll','cuba fibre','3','lime','2'); insert #drink value ('6/30/2018','do','long island','3','rum','4'); insert #drink value ('7/7/2018','ll','cosmopolitan','5','trial see','7'); expect result follows: pleas note, seen expect results, children interchangeable. example, 7/7/2018 trial see arrive drink cosmopolitan; howe child also rum, change avail margaret ll. also update do region cuba fibre' 06/30 06/31. pleas take consider part interchange also time new item arrive make avail item previous now. lastly - would awesom could add not column show kit avail regardless local base avail child. ex. child #3 do none ll ll assume enough inventor make drink base inventor not location!"
49370562,Comma separating numbers without advance knowledge of number of digits in Postgres,"In Postgres 9.5:
select to_char(111, 'FM9,999'); -- gives 111
select to_char(1111, 'FM9,999'); -- gives 1,111
select to_char(11111, 'FM9,999'); -- gives #,### !!!
How can I format my numbers with commas without advance knowledge/assumption of maximum number of possible digits preferably by using built-in stuff like above?
",<postgresql>,324,0,3,7848,5,44,48,37,14202,0.0,2543,2,15,2018-03-19 19:03,2018-03-19 19:08,2018-03-19 19:08,0.0,0.0,Basic,1,"<postgresql>, Comma separating numbers without advance knowledge of number of digits in Postgres, In Postgres 9.5:
select to_char(111, 'FM9,999'); -- gives 111
select to_char(1111, 'FM9,999'); -- gives 1,111
select to_char(11111, 'FM9,999'); -- gives #,### !!!
How can I format my numbers with commas without advance knowledge/assumption of maximum number of possible digits preferably by using built-in stuff like above?
","<postgresql>, comma spear number without advance knowledge number digit postures, poster 9.5: select to_char(111, 'for,999'); -- give 111 select to_char(1111, 'for,999'); -- give 1,111 select to_char(11111, 'for,999'); -- give #,### !!! format number comma without advance knowledge/assume maximum number possible digit prefer use built-in stuff like above?"
52243652,GCP SQL Postgres problem with privileges : can't run a query with postgres user with generated symfony db,"I am struggling to solve this problem with Google Cloud Platform's Cloud SQL component. My tech stack consists of hosting my application in a Google Kubernetes Engine (GKE) deployment, using the Cloud SQL proxy sidecar to connect to the database within the pods. The backend is a Symfony project.
I follow these steps to create and populate the database (without success):
Create Cloud SQL Postgres instance
Add proxy to k8s container to connect to the Cloud SQL instance with all credentials, as described in the GCP documentation
Enter my Symfony (phpfpm) pod and run the command php bin/console doctrine:schema:update --force to update the schema. The queries are executed in the database, so the schema is created and so on.
I try to open the database from the SQL console connection within GCP with the postgres user and try to execute a simple select * from foo; query. The response is Insufficient privilege: 7 ERROR:  permission denied for relation
How can I query the data in the database with the postgres user?
EDIT :
I have this situation about users :
     Role name     |                         Attributes                         |           Member of           
-------------------+------------------------------------------------------------+-------------------------------
 acando14          | Create role, Create DB                                     | {cloudsqlsuperuser,proxyuser}
 cloudsqladmin     | Superuser, Create role, Create DB, Replication, Bypass RLS | {}
 cloudsqlagent     | Create role, Create DB                                     | {cloudsqlsuperuser}
 cloudsqlreplica   | Replication                                                | {}
 cloudsqlsuperuser | Create role, Create DB                                     | {}
 postgres          | Create role, Create DB                                     | {cloudsqlsuperuser,acando14}
 proxyuser         | Create role, Create DB                                     | {cloudsqlsuperuser}
And I have this situation in the tables :
              List of relations
 Schema |      Name       | Type  |  Owner   
--------+-----------------+-------+----------
 public | article         | table | acando14
If I use postgres user logged in my db symfony works :
symfony =&gt; select * from article;
 id | model_id | code | size 
----+----------+------+------
(0 rows)
But if I use the server to execute the code the response is :
  SQLSTATE[42501]: Insufficient privilege: 7 ERROR: permission denied
  for relation employee at PDOException(code: 42501): SQLSTATE[42501]:
  Insufficient privilege: 7 ERROR: permission denied for relation .. at
And another problem is that I didn't generate all the tables with the command but I have to generate it executing all the queries, so strange...
Thank you, regards
",<php><postgresql><symfony><google-cloud-platform><google-cloud-sql>,2784,1,24,562,2,7,21,58,14934,0.0,16,1,15,2018-09-09 10:39,2018-09-09 12:47,2018-09-09 12:47,0.0,0.0,Basic,3,"<php><postgresql><symfony><google-cloud-platform><google-cloud-sql>, GCP SQL Postgres problem with privileges : can't run a query with postgres user with generated symfony db, I am struggling to solve this problem with Google Cloud Platform's Cloud SQL component. My tech stack consists of hosting my application in a Google Kubernetes Engine (GKE) deployment, using the Cloud SQL proxy sidecar to connect to the database within the pods. The backend is a Symfony project.
I follow these steps to create and populate the database (without success):
Create Cloud SQL Postgres instance
Add proxy to k8s container to connect to the Cloud SQL instance with all credentials, as described in the GCP documentation
Enter my Symfony (phpfpm) pod and run the command php bin/console doctrine:schema:update --force to update the schema. The queries are executed in the database, so the schema is created and so on.
I try to open the database from the SQL console connection within GCP with the postgres user and try to execute a simple select * from foo; query. The response is Insufficient privilege: 7 ERROR:  permission denied for relation
How can I query the data in the database with the postgres user?
EDIT :
I have this situation about users :
     Role name     |                         Attributes                         |           Member of           
-------------------+------------------------------------------------------------+-------------------------------
 acando14          | Create role, Create DB                                     | {cloudsqlsuperuser,proxyuser}
 cloudsqladmin     | Superuser, Create role, Create DB, Replication, Bypass RLS | {}
 cloudsqlagent     | Create role, Create DB                                     | {cloudsqlsuperuser}
 cloudsqlreplica   | Replication                                                | {}
 cloudsqlsuperuser | Create role, Create DB                                     | {}
 postgres          | Create role, Create DB                                     | {cloudsqlsuperuser,acando14}
 proxyuser         | Create role, Create DB                                     | {cloudsqlsuperuser}
And I have this situation in the tables :
              List of relations
 Schema |      Name       | Type  |  Owner   
--------+-----------------+-------+----------
 public | article         | table | acando14
If I use postgres user logged in my db symfony works :
symfony =&gt; select * from article;
 id | model_id | code | size 
----+----------+------+------
(0 rows)
But if I use the server to execute the code the response is :
  SQLSTATE[42501]: Insufficient privilege: 7 ERROR: permission denied
  for relation employee at PDOException(code: 42501): SQLSTATE[42501]:
  Insufficient privilege: 7 ERROR: permission denied for relation .. at
And another problem is that I didn't generate all the tables with the command but I have to generate it executing all the queries, so strange...
Thank you, regards
","<pp><postgresql><symfony><goose-cloud-platform><goose-cloud-sal>, gap sal poster problem privilege : can't run query poster user genet symfoni do, struggle sole problem good cloud platform' cloud sal component. teach stick consist host applied good kubernet engine (ke) employment, use cloud sal prove sidecar connect database within pads. backed symfoni project. follow step great soul database (without success): great cloud sal poster instant add prove k contain connect cloud sal instant credentials, describe gap document enter symfoni (phpfpm) god run command pp bin/console doctrine:scheme:up --for update scheme. query execute database, scheme great on. try open database sal console connect within gap poster user try execute simple select * foo; query. response insuffici privilege: 7 error: permits den relate query data database poster user? edit : situate user : role name | attribute | member -------------------+------------------------------------------------------------+------------------------------- acando14 | great role, great do | {cloudsqlsuperuser,proxyuser} cloudsqladmin | superuser, great role, great do, application, pass ll | {} cloudsqlag | great role, great do | {cloudsqlsuperuser} cloudsqlreplica | relic | {} cloudsqlsuperus | great role, great do | {} poster | great role, great do | {cloudsqlsuperuser,acando14} proxyus | great role, great do | {cloudsqlsuperuser} situate table : list relate scheme | name | type | owner --------+-----------------+-------+---------- public | article | table | acando14 use poster user log do symfoni work : symfoni =&it; select * article; id | modeled | code | size ----+----------+------+------ (0 rows) use server execute code response : sqlstate[42501]: insuffici privilege: 7 error: permits den relate employe pdoexception(code: 42501): sqlstate[42501]: insuffici privilege: 7 error: permits den relate .. not problem genet table command genet execute queried, strange... thank you, regard"
49945649,MySQL Error: Authentication plugin 'caching_sha2_password' cannot be loaded,"I just installed MySQL Ver 14.14 Distrib 5.7.22 with Homebrew on my macOS v10.13.4.
I ran the command:
brew install mysql 
After the installation completed, as directed by Homebrew, I ran the command:
mysql_secure_installation
and was returned the error: Error: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found   
I tried a few things like changing default_authentication_plugin to mysql_native_password in the my.cnf file but it still throws the same error. 
Next I tried running:
mysql_upgrade -u root
and I was thrown the same error again mysql_upgrade: Got error: 2059: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found while connecting to the MySQL server
Upgrade process encountered error and will not continue.
Any help is appreciated.
",<mysql><macos><macos-high-sierra>,962,0,6,587,2,6,15,75,29574,0.0,242,4,15,2018-04-20 16:01,2018-04-20 16:57,2018-04-21 12:50,0.0,1.0,Basic,9,"<mysql><macos><macos-high-sierra>, MySQL Error: Authentication plugin 'caching_sha2_password' cannot be loaded, I just installed MySQL Ver 14.14 Distrib 5.7.22 with Homebrew on my macOS v10.13.4.
I ran the command:
brew install mysql 
After the installation completed, as directed by Homebrew, I ran the command:
mysql_secure_installation
and was returned the error: Error: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found   
I tried a few things like changing default_authentication_plugin to mysql_native_password in the my.cnf file but it still throws the same error. 
Next I tried running:
mysql_upgrade -u root
and I was thrown the same error again mysql_upgrade: Got error: 2059: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found while connecting to the MySQL server
Upgrade process encountered error and will not continue.
Any help is appreciated.
","<myself><faces><faces-high-pierre>, myself error: authentic plain 'caching_sha2_password' cannot loaded, instal myself ver 14.14 district 5.7.22 hebrew mack ve.13.4. ran command: grew instal myself instal completed, direct hebrew, ran command: mysql_secure_instal return error: error: authentic plain 'caching_sha2_password' cannot loaded: open(/us/local/cellar/myself/5.7.22/limb/plain/caching_sha2_password.so, 2): image found try thing like change default_authentication_plugin mysql_native_password my.cf file still throw error. next try running: mysql_upgrad -u root thrown error mysql_upgrade: got error: 2059: authentic plain 'caching_sha2_password' cannot loaded: open(/us/local/cellar/myself/5.7.22/limb/plain/caching_sha2_password.so, 2): image found connect myself server upgrade process count error continue. help appreciated."
