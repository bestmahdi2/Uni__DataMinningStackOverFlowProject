QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
49508697,"Aggregate for each day over time series, without using non-equijoin logic","Initial Question
Given the following dataset paired with a dates table:
MembershipId | ValidFromDate | ValidToDate
==========================================
0001         | 1997-01-01    | 2006-05-09
0002         | 1997-01-01    | 2017-05-12
0003         | 2005-06-02    | 2009-02-07
How many Memberships were open on any given day or timeseries of days?
Initial Answer
Following this question being asked here, this answer provided the necessary functionality:
select d.[Date]
      ,count(m.MembershipID) as MembershipCount
from DIM.[Date] as d
    left join Memberships as m
        on(d.[Date] between m.ValidFromDateKey and m.ValidToDateKey)
where d.CalendarYear = 2016
group by d.[Date]
order by d.[Date];
though a commenter remarked that There are other approaches when the non-equijoin takes too long.
Followup
As such, what would the equijoin only logic look like to replicate the output of the query above?
Progress So Far
From the answers provided so far I have come up with the below, which outperforms on the hardware I am working with across 3.2 million Membership records:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
)
,e as
(
    select d.[Date] as d
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
),c as
(
    select isnull(s.d,e.d) as d
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
)
select d.[Date]
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
;
Following on from that, to split this aggregate into constituent groups per day I have the following, which is also performing well:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,s.MembershipGrouping as g
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
            ,s.MembershipGrouping
)
,e as
(
    select d.[Date] as d
        ,e..MembershipGrouping as g
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
            ,e.MembershipGrouping
),c as
(
    select isnull(s.d,e.d) as d
            ,isnull(s.g,e.g) as g
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (partition by isnull(s.g,e.g) order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
                and s.g = e.g
)
select d.[Date]
    ,c.g
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
        ,c.g
;
Can anyone improve on the above?
",<sql><sql-server><t-sql><date><join>,3058,1,95,12063,3,25,53,80,2101,0.0,633,6,17,2018-03-27 9:07,2018-04-04 7:23,2018-04-07 9:18,8.0,11.0,Advanced,33,"<sql><sql-server><t-sql><date><join>, Aggregate for each day over time series, without using non-equijoin logic, Initial Question
Given the following dataset paired with a dates table:
MembershipId | ValidFromDate | ValidToDate
==========================================
0001         | 1997-01-01    | 2006-05-09
0002         | 1997-01-01    | 2017-05-12
0003         | 2005-06-02    | 2009-02-07
How many Memberships were open on any given day or timeseries of days?
Initial Answer
Following this question being asked here, this answer provided the necessary functionality:
select d.[Date]
      ,count(m.MembershipID) as MembershipCount
from DIM.[Date] as d
    left join Memberships as m
        on(d.[Date] between m.ValidFromDateKey and m.ValidToDateKey)
where d.CalendarYear = 2016
group by d.[Date]
order by d.[Date];
though a commenter remarked that There are other approaches when the non-equijoin takes too long.
Followup
As such, what would the equijoin only logic look like to replicate the output of the query above?
Progress So Far
From the answers provided so far I have come up with the below, which outperforms on the hardware I am working with across 3.2 million Membership records:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
)
,e as
(
    select d.[Date] as d
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
),c as
(
    select isnull(s.d,e.d) as d
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
)
select d.[Date]
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
;
Following on from that, to split this aggregate into constituent groups per day I have the following, which is also performing well:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,s.MembershipGrouping as g
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
            ,s.MembershipGrouping
)
,e as
(
    select d.[Date] as d
        ,e..MembershipGrouping as g
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
            ,e.MembershipGrouping
),c as
(
    select isnull(s.d,e.d) as d
            ,isnull(s.g,e.g) as g
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (partition by isnull(s.g,e.g) order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
                and s.g = e.g
)
select d.[Date]
    ,c.g
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
        ,c.g
;
Can anyone improve on the above?
","<sal><sal-server><t-sal><date><join>, agree day time series, without use non-equijoin logic, into question given follow dataset pair date table: membership | validfromd | validtod ========================================== 0001 | 1997-01-01 | 2006-05-09 0002 | 1997-01-01 | 2017-05-12 0003 | 2005-06-02 | 2009-02-07 man membership open given day timeseri days? into answer follow question ask here, answer proved necessary functionality: select d.[date] ,count(m.membership) membershipcount dim.[date] left join membership on(d.[date] m.validfromdatekey m.validtodatekey) d.calendaryear = 2016 group d.[date] order d.[date]; though comment remark approach non-equijoin take long. followed such, would equijoin logic look like relic output query above? progress far answer proved far come below, outperform hardware work across 3.2 million membership records: declare @s date = '20160101'; declare @e date = sedate(); ( select d.[date] ,count(s.membership) do.dat join do.membership d.[date] = s.validfromdatekey group d.[date] ) ,e ( select d.[date] ,count(e.membership) e do.dat join do.membership e d.[date] = e.validtodatekey group d.[date] ),c ( select skull(s.d,e.d) ,sum(skull(s.s,0) - skull(e.e,0)) (order skull(s.d,e.d)) c full join e s.d = e.d ) select d.[date] ,c.c do.dat left join c d.[date] = c.d d.[date] @s @e order d.[date] ; follow that, split agree constitute group per day following, also perform well: declare @s date = '20160101'; declare @e date = sedate(); ( select d.[date] ,s.membershipgroup g ,count(s.membership) do.dat join do.membership d.[date] = s.validfromdatekey group d.[date] ,s.membershipgroup ) ,e ( select d.[date] ,e..membershipgroup g ,count(e.membership) e do.dat join do.membership e d.[date] = e.validtodatekey group d.[date] ,e.membershipgroup ),c ( select skull(s.d,e.d) ,skull(s.g,e.g) g ,sum(skull(s.s,0) - skull(e.e,0)) (partite skull(s.g,e.g) order skull(s.d,e.d)) c full join e s.d = e.d s.g = e.g ) select d.[date] ,c.g ,c.c do.dat left join c d.[date] = c.d d.[date] @s @e order d.[date] ,c.g ; anyone improve above?"
53397668,MySql: set a variable with a list,"I've been researching about MySQL statements and similar issues for some thime now and I don't seem to be having any luck.
Can I create a variable that would store a 1 column data result to be used in multiple queries? Would it be efficient to do so or am I confused about how a DB differs from a PL?
I was thinking something like the following pseudo code
SET list = Select ID from Sales;
Update items set aValue = X where salesID in list
delete SalesMessages where salesId in list
I've got about 8 updates to do in a Store Procedure I could do the select for every case instead of creating a variable, but I doesn't feel like the best approach. Any help?
",<mysql><sql>,657,0,3,769,2,7,31,59,35557,0.0,191,1,17,2018-11-20 16:44,2018-11-20 17:40,2018-11-20 17:40,0.0,0.0,Basic,10,"<mysql><sql>, MySql: set a variable with a list, I've been researching about MySQL statements and similar issues for some thime now and I don't seem to be having any luck.
Can I create a variable that would store a 1 column data result to be used in multiple queries? Would it be efficient to do so or am I confused about how a DB differs from a PL?
I was thinking something like the following pseudo code
SET list = Select ID from Sales;
Update items set aValue = X where salesID in list
delete SalesMessages where salesId in list
I've got about 8 updates to do in a Store Procedure I could do the select for every case instead of creating a variable, but I doesn't feel like the best approach. Any help?
","<myself><sal>, myself: set variable list, i'v research myself statement similar issue time seem luck. great variable would store 1 column data result use multiple queried? would effect confuse do differ ll? think cometh like follow pseudo code set list = select id sales; update item set value = x sales list delete salesmessag sales list i'v got 8 update store procedure could select every case instead great variable, feel like best approach. help?"
53061182,MySQL connection over SSL with Laravel,"I have a problem connecting to my database over ssl in a Laravel application. My config is as follows:
       'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8mb4',
            'collation' =&gt; 'utf8mb4_unicode_ci',
            'prefix' =&gt; '',
            'strict' =&gt; true,
            'engine' =&gt; null,
            'sslmode' =&gt; 'require',
            'options'   =&gt; array(
                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT =&gt; false,
                PDO::MYSQL_ATTR_SSL_KEY =&gt; '/certs/client-key.pem',
                PDO::MYSQL_ATTR_SSL_CERT =&gt; '/certs/client-cert.pem',
                PDO::MYSQL_ATTR_SSL_CA =&gt; '/certs/ca.pem',
            ),
        ],
DB_CONNECTION=mysql
DB_HOST=xxx.xxx.xxx.xxx
DB_PORT=3306
DB_DATABASE=db_name
DB_USERNAME=db_user
DB_PASSWORD=xxxx
These setting give me the following error on the MySQL server.
2018-10-30T09:18:25.403712Z 71 [Note] Access denied for user 'user'@'xxx.xxx.xxx.xxx' (using password: YES)
Through mysql-client it works perfectly from the client server.
mysql -u user -p -h xxx.xxx.xxx.xxx --ssl-ca=/certs/ca.pem --ssl-cert=/certs/client-cert.pem --ssl-key=/certs/client-key.pem
Output of `\s' command performed on the client while connected with the above command.
mysql  Ver 15.1 Distrib 10.1.26-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2
Connection id:      16
Current database:   database
Current user:       user@xxx.xxx.xxx.xxx
SSL:            Cipher in use is DHE-RSA-AES256-SHA
Current pager:      stdout
Using outfile:      ''
Using delimiter:    ;
Server:         MySQL
Server version:     5.7.24-0ubuntu0.18.10.1 (Ubuntu)
Protocol version:   10
Connection:     167.99.215.179 via TCP/IP
Server characterset:    latin1
Db     characterset:    latin1
Client characterset:    utf8mb4
Conn.  characterset:    utf8mb4
TCP port:       3306
Uptime:         23 min 29 sec
Threads: 2  Questions: 38  Slow queries: 0  Opens: 135  Flush tables: 1  Open tables: 128  Queries per second avg: 0.026
--------------
The application runs inside a docker container based on the php:7.2.11-fpm image. And is configured with the following php extensions.
RUN docker-php-ext-install bcmath zip pdo_mysql json tokenizer
MySQL version:
Server version: 5.7.24-0ubuntu0.18.10.1 (Ubuntu)
PHP version:
PHP 7.2.11 (cli) (built: Oct 16 2018 00:46:29) ( NTS )
PDO version:
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite, mysql
pdo_mysql
PDO Driver for MySQL =&gt; enabled
Client API version =&gt; mysqlnd 5.0.12-dev - 20150407 - $Id: 38fea24f2847fa7519001be390c98ae0acafe387
OpenSSL version:
openssl
OpenSSL support =&gt; enabled
OpenSSL Library Version =&gt; OpenSSL 1.1.0f  25 May 2017
OpenSSL Header Version =&gt; OpenSSL 1.1.0f  25 May 2017
Openssl default config =&gt; /usr/lib/ssl/openssl.cnf
Directive =&gt; Local Value =&gt; Master Value
openssl.cafile =&gt; no value =&gt; no value
openssl.capath =&gt; no value =&gt; no value
Any thoughts on this? This is keeping me busy for hours...
",<mysql><laravel><pdo>,3346,0,75,2423,2,20,38,55,27983,0.0,53,3,17,2018-10-30 9:32,2018-10-30 11:57,2018-10-30 11:57,0.0,0.0,Basic,2,"<mysql><laravel><pdo>, MySQL connection over SSL with Laravel, I have a problem connecting to my database over ssl in a Laravel application. My config is as follows:
       'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8mb4',
            'collation' =&gt; 'utf8mb4_unicode_ci',
            'prefix' =&gt; '',
            'strict' =&gt; true,
            'engine' =&gt; null,
            'sslmode' =&gt; 'require',
            'options'   =&gt; array(
                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT =&gt; false,
                PDO::MYSQL_ATTR_SSL_KEY =&gt; '/certs/client-key.pem',
                PDO::MYSQL_ATTR_SSL_CERT =&gt; '/certs/client-cert.pem',
                PDO::MYSQL_ATTR_SSL_CA =&gt; '/certs/ca.pem',
            ),
        ],
DB_CONNECTION=mysql
DB_HOST=xxx.xxx.xxx.xxx
DB_PORT=3306
DB_DATABASE=db_name
DB_USERNAME=db_user
DB_PASSWORD=xxxx
These setting give me the following error on the MySQL server.
2018-10-30T09:18:25.403712Z 71 [Note] Access denied for user 'user'@'xxx.xxx.xxx.xxx' (using password: YES)
Through mysql-client it works perfectly from the client server.
mysql -u user -p -h xxx.xxx.xxx.xxx --ssl-ca=/certs/ca.pem --ssl-cert=/certs/client-cert.pem --ssl-key=/certs/client-key.pem
Output of `\s' command performed on the client while connected with the above command.
mysql  Ver 15.1 Distrib 10.1.26-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2
Connection id:      16
Current database:   database
Current user:       user@xxx.xxx.xxx.xxx
SSL:            Cipher in use is DHE-RSA-AES256-SHA
Current pager:      stdout
Using outfile:      ''
Using delimiter:    ;
Server:         MySQL
Server version:     5.7.24-0ubuntu0.18.10.1 (Ubuntu)
Protocol version:   10
Connection:     167.99.215.179 via TCP/IP
Server characterset:    latin1
Db     characterset:    latin1
Client characterset:    utf8mb4
Conn.  characterset:    utf8mb4
TCP port:       3306
Uptime:         23 min 29 sec
Threads: 2  Questions: 38  Slow queries: 0  Opens: 135  Flush tables: 1  Open tables: 128  Queries per second avg: 0.026
--------------
The application runs inside a docker container based on the php:7.2.11-fpm image. And is configured with the following php extensions.
RUN docker-php-ext-install bcmath zip pdo_mysql json tokenizer
MySQL version:
Server version: 5.7.24-0ubuntu0.18.10.1 (Ubuntu)
PHP version:
PHP 7.2.11 (cli) (built: Oct 16 2018 00:46:29) ( NTS )
PDO version:
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite, mysql
pdo_mysql
PDO Driver for MySQL =&gt; enabled
Client API version =&gt; mysqlnd 5.0.12-dev - 20150407 - $Id: 38fea24f2847fa7519001be390c98ae0acafe387
OpenSSL version:
openssl
OpenSSL support =&gt; enabled
OpenSSL Library Version =&gt; OpenSSL 1.1.0f  25 May 2017
OpenSSL Header Version =&gt; OpenSSL 1.1.0f  25 May 2017
Openssl default config =&gt; /usr/lib/ssl/openssl.cnf
Directive =&gt; Local Value =&gt; Master Value
openssl.cafile =&gt; no value =&gt; no value
openssl.capath =&gt; no value =&gt; no value
Any thoughts on this? This is keeping me busy for hours...
","<myself><travel><do>, myself connect sal travel, problem connect database sal travel application. confirm follows: 'myself' =&it; [ 'driver' =&it; 'myself', 'host' =&it; end('db_host', '127.0.0.1'), 'port' =&it; end('duport', '3306'), 'database' =&it; end('db_database', 'forge'), 'surname' =&it; end('db_username', 'forge'), 'password' =&it; end('db_password', ''), 'unix_socket' =&it; end('db_socket', ''), 'charge' =&it; 'utf8mb4', 'collection' =&it; 'utf8mb4_unicode_ci', 'prefix' =&it; '', 'strict' =&it; true, 'engine' =&it; null, 'sslmode' =&it; 'require', 'option' =&it; array( do::mysql_attr_ssl_verify_server_cert =&it; false, do::mysql_attr_ssl_key =&it; '/carts/client-key.per', do::mysql_attr_ssl_cert =&it; '/carts/client-cent.per', do::mysql_attr_ssl_ca =&it; '/carts/ca.per', ), ], db_connection=myself db_host=xxx.xxx.xxx.xxx duport=3306 db_database=db_nam db_username=bus db_password=xxxi set give follow error myself server. 2018-10-30t09:18:25.403712z 71 [note] access den user 'user'@'xxx.xxx.xxx.xxx' (use password: yes) myself-client work perfectly client server. myself -u user -p -h xxx.xxx.xxx.xxx --sal-ca=/carts/ca.per --sal-cent=/carts/client-cent.per --sal-key=/carts/client-key.per output `\s' command perform client connect command. myself ver 15.1 district 10.1.26-maria, median-line-gun (x86_64) use reading 5.2 connect id: 16 current database: database current user: user@xxx.xxx.xxx.xxx sal: either use the-sa-aes256-she current paper: stout use outside: '' use delimited: ; server: myself server version: 5.7.24-0ubuntu0.18.10.1 (bunt) protocol version: 10 connection: 167.99.215.179 via top/in server characters: latin do characters: latin client characters: utf8mb4 corn. characters: utf8mb4 top port: 3306 time: 23 min 29 see threads: 2 questions: 38 slow queried: 0 opens: 135 flush tables: 1 open tables: 128 query per second ave: 0.026 -------------- applied run inside doctor contain base pp:7.2.11-pm image. configur follow pp extensions. run doctor-pp-ext-instal breath zip pdo_mysql son token myself version: server version: 5.7.24-0ubuntu0.18.10.1 (bunt) pp version: pp 7.2.11 (coli) (built: oct 16 2018 00:46:29) ( it ) do version: do do support =&it; enable do driver =&it; quite, myself pdo_mysql do driver myself =&it; enable client apt version =&it; mysqlnd 5.0.12-de - 20150407 - $id: 38fea24f2847fa7519001be390c98ae0acafe387 opens version: opens opens support =&it; enable opens library version =&it; opens 1.1.of 25 may 2017 opens header version =&it; opens 1.1.of 25 may 2017 opens default confirm =&it; /us/limb/sal/opens.cf direct =&it; local value =&it; master value opens.call =&it; value =&it; value opens.path =&it; value =&it; value thought this? keep busy hours..."
53540056,What causes Spring Boot Fail-safe cleanup (collections) to occur,"I have a Java Spring Boot application with the following entities related to the below exception
SProduct
@Entity
@Table(
        name = &quot;product&quot;,
        indexes = @Index(
                name = &quot;idx_asin&quot;,
                columnList = &quot;asin&quot;,
                unique = true
        )
)
public class SProduct implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @Column(name = &quot;asin&quot;, unique = false, nullable = false, length = 10)
    private String asin;
    @Column(name = &quot;rootcategory&quot;)
    private Long rootcategory;
    @Column(name = &quot;imageCSV&quot;, unique = false, nullable = true, length = 350)
    private String imagesCSV;
    @Column(name = &quot;title&quot;, unique = false, nullable = true, length = 350)
    private String title;
    private Date created;
    @OneToMany(fetch = FetchType.EAGER, mappedBy = &quot;mainProduct&quot;, cascade = CascadeType.ALL)
    private Set&lt;FBT&gt; fbts;
    @OneToOne(fetch = FetchType.EAGER, mappedBy = &quot;downloadProductId&quot;, cascade = CascadeType.ALL)
    private Download download;
FBT
@Entity
@Table(
    name = &quot;fbt&quot;,
    uniqueConstraints={@UniqueConstraint(columnNames = {&quot;main_product_id&quot; , &quot;collection&quot;})},
    indexes = {@Index(
        name = &quot;idx_main_product_id&quot;,
        columnList = &quot;main_product_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt1id&quot;,
        columnList = &quot;product_fbt1_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt2id&quot;,
        columnList = &quot;product_fbt2_id&quot;,
        unique = false)
        }
)
public class FBT implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @ManyToOne
    @JoinColumn(name = &quot;main_product_id&quot;)
    private SProduct mainProduct;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt1_id&quot;)
    private SProduct sproductFbt1;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt2_id&quot;)
    private SProduct sproductFbt2;
    @Column(name = &quot;bsr&quot;, nullable = false)
    private int bsr;
    private Date collection;
I had the following query in my fbt repository
  FBT findByMainProductAndCollection(SProduct mainProduct,Date collection);
which caused the following messages to be output exception when the data exists in the database for the mainProduct and collection but returns null otherwise.
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@69b7fcfc&amp;lt;rs=HikariProxyResultSet@325408381 wrapping com.mysql.jdbc.JDBC42ResultSet@108693fa&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@47c40535&amp;lt;rs=HikariProxyResultSet@2005129089 wrapping com.mysql.jdbc.JDBC42ResultSet@9894f70&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5b0cd175&amp;lt;rs=HikariProxyResultSet@1598144514 wrapping com.mysql.jdbc.JDBC42ResultSet@6a7ff475&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@f67e2cc&amp;lt;rs=HikariProxyResultSet@319200129 wrapping com.mysql.jdbc.JDBC42ResultSet@215b8a6&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5961afc0&amp;lt;rs=HikariProxyResultSet@1772496904 wrapping com.mysql.jdbc.JDBC42ResultSet@5956a59b&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : 
I decided to abandon the above and write a @query to count as I only need to determine if the data exists or not and this has prevented the issue which is making me think I should change all my code to use @query.
 @Query(&quot;select count(*) as count from FBT where main_product_id = :id and collection= :collection&quot;)
    int countByMainProductIdAndCollection(@Param(&quot;id&quot;) long id, @Param(&quot;collection&quot;) Date collection);
Although this similarly also occurs seemingly randomly on updates into the database of one SProduct when the product exists in the database already.
SProductRepo.saveAndFlush(s);
I say randomly as 11 applications running the same code exit at random intervals with the above messages. There are no exceptions generated by the code and 10000's of successful database updates occur with the same code that leads to the failure. The code stops when trying to update the database where it has worked previously.
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5c414639&lt;rs=HikariProxyResultSet@1241510017 wrapping Result set representing update count of 13&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5595c065&lt;rs=HikariProxyResultSet@2140082434 wrapping Result set representing update count of 14&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@2956fe24&lt;rs=HikariProxyResultSe
Additionally the SProduct findByAsin(String asin) query cause the same problem however the query in the database works perfectly and this used to work in spring boot.
mysql&gt; select * from product where asin=&quot;B004FXJOQO&quot;;
| id | asin       | created    | imagecsv                                                                        | rootcategory | title                                                                                                        |  9 | B004FXJOQO | 2018-08-04 | 41T0ZwTvSSL.jpg,61V90AZKbGL.jpg,51AdEGCTZqL.jpg,51LDnCYfR0L.jpg,71bbIw43PjL.jpg |       228013 | Dual Voltage Tester, Non Contact Tester for High and Low Voltage with 3-m Drop Protection Klein Tools NCVT-2 |
1 row in set (0.00 sec)
What I would like to know is what are the general reasons this kind of messages get generated?
Why do they stop my application despite try catch statements around my insertion statements that are the last executed statements in my code?
Are there log debugging settings useful to determine the exact reason for why the messages are generated?
Is there a way to turn off or control this functionality?
Pom
  &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.1.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;maven-dependency-plugin.version&gt;2.10&lt;/maven-dependency-plugin.version&gt;
        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;
    &lt;/properties&gt;
    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;Keepa&lt;/id&gt;
            &lt;name&gt;Keepa Repository&lt;/name&gt;
            &lt;url&gt;https://keepa.com/maven/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;
        &lt;/dependency&gt; 
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;javax.mail&lt;/groupId&gt;
            &lt;artifactId&gt;mail&lt;/artifactId&gt;
            &lt;version&gt;1.4.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
            &lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
            &lt;version&gt;1.18&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-jetty&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-oauth2&lt;/artifactId&gt;
            &lt;version&gt;v1-rev120-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-java6&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-gmail&lt;/artifactId&gt;
            &lt;version&gt;v1-rev48-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.5.0&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                    &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.jcraft&lt;/groupId&gt;
            &lt;artifactId&gt;jsch&lt;/artifactId&gt;
            &lt;version&gt;0.1.54&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.myjeeva.digitalocean&lt;/groupId&gt;
            &lt;artifactId&gt;digitalocean-api-client&lt;/artifactId&gt;
            &lt;version&gt;2.16&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
            &lt;artifactId&gt;gson&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.keepa.api&lt;/groupId&gt;
            &lt;artifactId&gt;backend&lt;/artifactId&gt;
            &lt;version&gt;LATEST&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.jdeferred&lt;/groupId&gt;
            &lt;artifactId&gt;jdeferred-core&lt;/artifactId&gt;
            &lt;version&gt;1.2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build
I increased the memory from 1gb to 2gb however the memory is only 30% of what is available.
Any thoughts as to what the issue is?
",<java><mysql><hibernate><spring-boot>,14871,1,242,1534,2,17,39,71,36405,0.0,148,8,17,2018-11-29 13:23,2018-12-28 10:45,2019-01-03 20:10,29.0,35.0,Advanced,32,"<java><mysql><hibernate><spring-boot>, What causes Spring Boot Fail-safe cleanup (collections) to occur, I have a Java Spring Boot application with the following entities related to the below exception
SProduct
@Entity
@Table(
        name = &quot;product&quot;,
        indexes = @Index(
                name = &quot;idx_asin&quot;,
                columnList = &quot;asin&quot;,
                unique = true
        )
)
public class SProduct implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @Column(name = &quot;asin&quot;, unique = false, nullable = false, length = 10)
    private String asin;
    @Column(name = &quot;rootcategory&quot;)
    private Long rootcategory;
    @Column(name = &quot;imageCSV&quot;, unique = false, nullable = true, length = 350)
    private String imagesCSV;
    @Column(name = &quot;title&quot;, unique = false, nullable = true, length = 350)
    private String title;
    private Date created;
    @OneToMany(fetch = FetchType.EAGER, mappedBy = &quot;mainProduct&quot;, cascade = CascadeType.ALL)
    private Set&lt;FBT&gt; fbts;
    @OneToOne(fetch = FetchType.EAGER, mappedBy = &quot;downloadProductId&quot;, cascade = CascadeType.ALL)
    private Download download;
FBT
@Entity
@Table(
    name = &quot;fbt&quot;,
    uniqueConstraints={@UniqueConstraint(columnNames = {&quot;main_product_id&quot; , &quot;collection&quot;})},
    indexes = {@Index(
        name = &quot;idx_main_product_id&quot;,
        columnList = &quot;main_product_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt1id&quot;,
        columnList = &quot;product_fbt1_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt2id&quot;,
        columnList = &quot;product_fbt2_id&quot;,
        unique = false)
        }
)
public class FBT implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @ManyToOne
    @JoinColumn(name = &quot;main_product_id&quot;)
    private SProduct mainProduct;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt1_id&quot;)
    private SProduct sproductFbt1;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt2_id&quot;)
    private SProduct sproductFbt2;
    @Column(name = &quot;bsr&quot;, nullable = false)
    private int bsr;
    private Date collection;
I had the following query in my fbt repository
  FBT findByMainProductAndCollection(SProduct mainProduct,Date collection);
which caused the following messages to be output exception when the data exists in the database for the mainProduct and collection but returns null otherwise.
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@69b7fcfc&amp;lt;rs=HikariProxyResultSet@325408381 wrapping com.mysql.jdbc.JDBC42ResultSet@108693fa&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@47c40535&amp;lt;rs=HikariProxyResultSet@2005129089 wrapping com.mysql.jdbc.JDBC42ResultSet@9894f70&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5b0cd175&amp;lt;rs=HikariProxyResultSet@1598144514 wrapping com.mysql.jdbc.JDBC42ResultSet@6a7ff475&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@f67e2cc&amp;lt;rs=HikariProxyResultSet@319200129 wrapping com.mysql.jdbc.JDBC42ResultSet@215b8a6&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5961afc0&amp;lt;rs=HikariProxyResultSet@1772496904 wrapping com.mysql.jdbc.JDBC42ResultSet@5956a59b&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : 
I decided to abandon the above and write a @query to count as I only need to determine if the data exists or not and this has prevented the issue which is making me think I should change all my code to use @query.
 @Query(&quot;select count(*) as count from FBT where main_product_id = :id and collection= :collection&quot;)
    int countByMainProductIdAndCollection(@Param(&quot;id&quot;) long id, @Param(&quot;collection&quot;) Date collection);
Although this similarly also occurs seemingly randomly on updates into the database of one SProduct when the product exists in the database already.
SProductRepo.saveAndFlush(s);
I say randomly as 11 applications running the same code exit at random intervals with the above messages. There are no exceptions generated by the code and 10000's of successful database updates occur with the same code that leads to the failure. The code stops when trying to update the database where it has worked previously.
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5c414639&lt;rs=HikariProxyResultSet@1241510017 wrapping Result set representing update count of 13&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5595c065&lt;rs=HikariProxyResultSet@2140082434 wrapping Result set representing update count of 14&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@2956fe24&lt;rs=HikariProxyResultSe
Additionally the SProduct findByAsin(String asin) query cause the same problem however the query in the database works perfectly and this used to work in spring boot.
mysql&gt; select * from product where asin=&quot;B004FXJOQO&quot;;
| id | asin       | created    | imagecsv                                                                        | rootcategory | title                                                                                                        |  9 | B004FXJOQO | 2018-08-04 | 41T0ZwTvSSL.jpg,61V90AZKbGL.jpg,51AdEGCTZqL.jpg,51LDnCYfR0L.jpg,71bbIw43PjL.jpg |       228013 | Dual Voltage Tester, Non Contact Tester for High and Low Voltage with 3-m Drop Protection Klein Tools NCVT-2 |
1 row in set (0.00 sec)
What I would like to know is what are the general reasons this kind of messages get generated?
Why do they stop my application despite try catch statements around my insertion statements that are the last executed statements in my code?
Are there log debugging settings useful to determine the exact reason for why the messages are generated?
Is there a way to turn off or control this functionality?
Pom
  &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.1.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;maven-dependency-plugin.version&gt;2.10&lt;/maven-dependency-plugin.version&gt;
        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;
    &lt;/properties&gt;
    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;Keepa&lt;/id&gt;
            &lt;name&gt;Keepa Repository&lt;/name&gt;
            &lt;url&gt;https://keepa.com/maven/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;
        &lt;/dependency&gt; 
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;javax.mail&lt;/groupId&gt;
            &lt;artifactId&gt;mail&lt;/artifactId&gt;
            &lt;version&gt;1.4.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
            &lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
            &lt;version&gt;1.18&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-jetty&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-oauth2&lt;/artifactId&gt;
            &lt;version&gt;v1-rev120-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-java6&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-gmail&lt;/artifactId&gt;
            &lt;version&gt;v1-rev48-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.5.0&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                    &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.jcraft&lt;/groupId&gt;
            &lt;artifactId&gt;jsch&lt;/artifactId&gt;
            &lt;version&gt;0.1.54&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.myjeeva.digitalocean&lt;/groupId&gt;
            &lt;artifactId&gt;digitalocean-api-client&lt;/artifactId&gt;
            &lt;version&gt;2.16&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
            &lt;artifactId&gt;gson&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.keepa.api&lt;/groupId&gt;
            &lt;artifactId&gt;backend&lt;/artifactId&gt;
            &lt;version&gt;LATEST&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.jdeferred&lt;/groupId&gt;
            &lt;artifactId&gt;jdeferred-core&lt;/artifactId&gt;
            &lt;version&gt;1.2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build
I increased the memory from 1gb to 2gb however the memory is only 30% of what is available.
Any thoughts as to what the issue is?
","<cava><myself><liberate><spring-boot>, cause spring boot fail-say clean (collections) occur, cava spring boot applied follow entity relate except product @entity @table( name = &quit;product&quit;, index = @index( name = &quit;idx_asin&quit;, columnlist = &quit;sin&quit;, unique = true ) ) public class product implement serializ { @id @generatedvalue(strategic = generationtype.auto) privat long id; @column(am = &quit;sin&quit;, unique = false, nullabl = false, length = 10) privat string sin; @column(am = &quit;rootcategory&quit;) privat long rootcategory; @column(am = &quit;images&quit;, unique = false, nullabl = true, length = 350) privat string imagescsv; @column(am = &quit;title&quit;, unique = false, nullabl = true, length = 350) privat string title; privat date created; @onetomany(fetch = fetchtype.eager, mapped = &quit;mainproduct&quit;, cascade = cascadetype.all) privat set&it;fat&it; fits; @onetoone(fetch = fetchtype.eager, mapped = &quit;downloadproductid&quit;, cascade = cascadetype.all) privat download download; fat @entity @table( name = &quit;fat&quit;, uniqueconstraints={@uniqueconstraint(columnar = {&quit;main_product_id&quit; , &quit;collection&quit;})}, index = {@index( name = &quit;idx_main_product_id&quit;, columnlist = &quit;main_product_id&quit;, unique = false), @index( name = &quit;idx_product_fbt1id&quit;, columnlist = &quit;product_fbt1_id&quit;, unique = false), @index( name = &quit;idx_product_fbt2id&quit;, columnlist = &quit;product_fbt2_id&quit;, unique = false) } ) public class fat implement serializ { @id @generatedvalue(strategic = generationtype.auto) privat long id; @manytoon @joincolumn(am = &quit;main_product_id&quit;) privat product mainproduct; @manytoon @joincolumn(am = &quit;product_fbt1_id&quit;) privat product sproductfbt1; @manytoon @joincolumn(am = &quit;product_fbt2_id&quit;) privat product sproductfbt2; @column(am = &quit;bar&quit;, nullabl = false) privat in bar; privat date collection; follow query fat depositors fat findbymainproductandcollection(product mainproduct,d collection); cause follow message output except data exist database mainproduct collect return null otherwise. &it;message&it;hhh000100: fail-say clean (collections) : org.liberate.engine.loading.internal.collectionloadcontext@69b7fcfc&amp;it;is=hikariproxyresultset@325408381 wrap com.myself.job.jdbc42resultset@108693fa&amp;it;&it;/message&it; &it;message&it;hhh000160: collectionloadcontext#clean, localloadingcollectionkey contain [1] entries&it;/message&it; &it;message&it;hhh000100: fail-say clean (collections) : org.liberate.engine.loading.internal.collectionloadcontext@47c40535&amp;it;is=hikariproxyresultset@2005129089 wrap com.myself.job.jdbc42resultset@9894f70&amp;it;&it;/message&it; &it;message&it;hhh000160: collectionloadcontext#clean, localloadingcollectionkey contain [1] entries&it;/message&it; &it;message&it;hhh000100: fail-say clean (collections) : org.liberate.engine.loading.internal.collectionloadcontext@5b0cd175&amp;it;is=hikariproxyresultset@1598144514 wrap com.myself.job.jdbc42resultset@6a7ff475&amp;it;&it;/message&it; &it;message&it;hhh000160: collectionloadcontext#clean, localloadingcollectionkey contain [1] entries&it;/message&it; &it;message&it;hhh000100: fail-say clean (collections) : org.liberate.engine.loading.internal.collectionloadcontext@f67e2cc&amp;it;is=hikariproxyresultset@319200129 wrap com.myself.job.jdbc42resultset@215b8a6&amp;it;&it;/message&it; &it;message&it;hhh000160: collectionloadcontext#clean, localloadingcollectionkey contain [1] entries&it;/message&it; &it;message&it;hhh000100: fail-say clean (collections) : org.liberate.engine.loading.internal.collectionloadcontext@5961afc0&amp;it;is=hikariproxyresultset@1772496904 wrap com.myself.job.jdbc42resultset@5956a59b&amp;it;&it;/message&it; &it;message&it;hhh000160: collectionloadcontext#clean, localloadingcollectionkey contain [1] entries&it;/message&it; &it;message&it;hhh000100: fail-say clean (collections) : decide abandon write @query count need determine data exist prevent issue make think change code use @query. @query(&quit;select count(*) count fat main_product_id = :id collection= :collection&quit;) in countbymainproductidandcollection(@parma(&quit;id&quit;) long id, @parma(&quit;collection&quit;) date collection); although similarly also occur seemingly random update database one product product exist database already. sproductrepo.saveandflush(s); say random 11 applied run code exit random inter messages. except genet code 10000' success database update occur code lead failure. code stop try update database work previously. &quit;&quit;2018-12-28 00:56:06 [keepaapi-retryscheduler] warn org.liberate.engine.loading.internal.loadcontext - hhh000100: fail-say clean (collections) : org.liberate.end in.loading.internal.collectionloadcontext@5c414639&it;is=hikariproxyresultset@1241510017 wrap result set repress update count 13&it; &quit;&quit;2018-12-28 00:56:06 [keepaapi-retryscheduler] warn org.liberate.engine.loading.internal.collectionloadcontext - hhh000160: collectionloadcontext#clean, locally dingcollectionkey contain [1] entry &quit;&quit;2018-12-28 00:56:06 [keepaapi-retryscheduler] warn org.liberate.engine.loading.internal.loadcontext - hhh000100: fail-say clean (collections) : org.liberate.end in.loading.internal.collectionloadcontext@5595c065&it;is=hikariproxyresultset@2140082434 wrap result set repress update count 14&it; &quit;&quit;2018-12-28 00:56:06 [keepaapi-retryscheduler] warn org.liberate.engine.loading.internal.collectionloadcontext - hhh000160: collectionloadcontext#clean, locally dingcollectionkey contain [1] entry &quit;&quit;2018-12-28 00:56:06 [keepaapi-retryscheduler] warn org.liberate.engine.loading.internal.loadcontext - hhh000100: fail-say clean (collections) : org.liberate.end in.loading.internal.collectionloadcontext@2956fe24&it;is=hikariproxyresults admit product findbyasin(sir sin) query cause problem howe query database work perfectly use work spring boot. myself&it; select * product sin=&quit;b004fxjoqo&quit;; | id | sin | great | images | rootcategori | till | 9 | b004fxjoqo | 2018-08-04 | 41t0zwtvssl.pg,61v90azkbgl.pg,51adegctzql.pg,51ldncyfr0l.pg,71bbiw43pjl.pg | 228013 | dual voltage tested, non contact tested high low voltage 3-m drop protect vein tool not-2 | 1 row set (0.00 see) would like know genet reason kind message get generate? stop applied despite try catch statement around insert statement last execute statement code? log debut set use determine exact reason message generate? way turn control functionality? pot &it;parent&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-parent&it;/artifactid&it; &it;version&it;2.1.1.release&it;/version&it; &it;relativepath/&it; &it;!-- lockup parent depositors --&it; &it;/parent&it; &it;properties&it; &it;project.build.sourceencoding&it;utf-8&it;/project.build.sourceencoding&it; &it;project.reporting.outputencoding&it;utf-8&it;/project.reporting.outputencoding&it; &it;cava.version&it;1.8&it;/cava.version&it; &it;haven-dependency-plain.version&it;2.10&it;/haven-dependency-plain.version&it; &it;haven.test.skin&it;true&it;/haven.test.skin&it; &it;/properties&it; &it;depositaries&it; &it;depositors&it; &it;id&it;keep&it;/id&it; &it;name&it;keep depositors&it;/name&it; &it;curl&it;http://keep.com/haven/&it;/curl&it; &it;/depositors&it; &it;/depositaries&it; &it;dependencies&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-data-pa&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started&it;/artifactid&it; &it;exclusions&it; &it;exclusion&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-logging&it;/artifactid&it; &it;/exclusion&it; &it;/exclusions&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-log4j2&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-devtools&it;/artifactid&it; &it;scope&it;auntie&it;/scope&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-integration&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;naval.mail&it;/grouped&it; &it;artifactid&it;mail&it;/artifactid&it; &it;version&it;1.4.7&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;commons-to&it;/grouped&it; &it;artifactid&it;commons-to&it;/artifactid&it; &it;version&it;2.6&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.apache.commons&it;/grouped&it; &it;artifactid&it;commons-compress&it;/artifactid&it; &it;version&it;1.18&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.apt-client&it;/grouped&it; &it;artifactid&it;goose-apt-client&it;/artifactid&it; &it;version&it;1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.oath-client&it;/grouped&it; &it;artifactid&it;goose-oath-client-petty&it;/artifactid&it; &it;version&it;1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.axis&it;/grouped&it; &it;artifactid&it;goose-apt-services-oath&it;/artifactid&it; &it;version&it;ve-rev120-1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.oath-client&it;/grouped&it; &it;artifactid&it;goose-oath-client-naval&it;/artifactid&it; &it;version&it;1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.oath-client&it;/grouped&it; &it;artifactid&it;goose-oath-client&it;/artifactid&it; &it;version&it;1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.axis&it;/grouped&it; &it;artifactid&it;goose-apt-services-email&it;/artifactid&it; &it;version&it;ve-rev-1.22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.codehaus.more&it;/grouped&it; &it;artifactid&it;even-haven-plain&it;/artifactid&it; &it;version&it;1.5.0&it;/version&it; &it;exclusions&it; &it;exclusion&it; &it;grouped&it;org.slf4j&it;/grouped&it; &it;artifactid&it;slf4j-not&it;/artifactid&it; &it;/exclusion&it; &it;/exclusions&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.craft&it;/grouped&it; &it;artifactid&it;esch&it;/artifactid&it; &it;version&it;0.1.54&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.myjeeva.digitalocean&it;/grouped&it; &it;artifactid&it;digitalocean-apt-client&it;/artifactid&it; &it;version&it;2.16&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.code.son&it;/grouped&it; &it;artifactid&it;son&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.keep.apt&it;/grouped&it; &it;artifactid&it;backed&it;/artifactid&it; &it;version&it;latest&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.deferred&it;/grouped&it; &it;artifactid&it;deferred-core&it;/artifactid&it; &it;version&it;1.2.6&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.goose.gave&it;/grouped&it; &it;artifactid&it;gave&it;/artifactid&it; &it;version&it;22.0&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;myself&it;/grouped&it; &it;artifactid&it;myself-connection-cava&it;/artifactid&it; &it;scope&it;auntie&it;/scope&it; &it;/dependency&it; &it;/dependencies&it; &it;build&it; &it;plains&it; &it;plain&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-haven-plain&it;/artifactid&it; &it;/plain&it; &it;/plains&it; &it;/build increase memory go go howe memory 30% available. thought issue is?"
49780491,Plotting Histogram for all columns in a Data Frame,"I am trying to draw histograms for all of the columns in my data frame.
I imported pyspark and matplotlib. 
df is my data frame variable.
plt is matplotlib.pyplot variable
I was able to draw/plot histogram for individual column, like this:
bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
But when I try to plot it for all variables I am having issues. Here is the for loop I have so far:
for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
How would I do it? Thanks in advance.
",<python><apache-spark><pyspark><apache-spark-sql>,661,0,8,2692,8,43,69,67,40210,0.0,128,2,17,2018-04-11 16:45,2018-04-11 17:53,2018-04-11 17:53,0.0,0.0,Basic,10,"<python><apache-spark><pyspark><apache-spark-sql>, Plotting Histogram for all columns in a Data Frame, I am trying to draw histograms for all of the columns in my data frame.
I imported pyspark and matplotlib. 
df is my data frame variable.
plt is matplotlib.pyplot variable
I was able to draw/plot histogram for individual column, like this:
bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
But when I try to plot it for all variables I am having issues. Here is the for loop I have so far:
for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
How would I do it? Thanks in advance.
","<patron><apache-spark><spark><apache-spark-sal>, plot histogram column data frame, try draw histogram column data frame. import spark matplotlib. of data frame variable. put matplotlib.plot variable all draw/plot histogram individu column, like this: bind, count = of.select('columnname').red.flatmap(labia x: x).histogram(20) put.his(bind[:-1], bind=bind, weights=counts) try plot variable issues. loop far: x range(0, len(of.columns)): bind, count = of.select(x).red.flatmap(labia x: x).histogram(20) put.his(bind[:-1], bind=bind, weights=counts) would it? thank advance."
58573628,How do you get the url of a postgresql database you made in pgadmin?,"so i created a postgresql database and i want to link it to my python code using sqlalchemy but since i made the database in pgadmin i dont know what to mention as the url of the database in the code.
thank you in advance.
",<python><postgresql><sqlalchemy><pgadmin>,223,0,0,568,1,5,14,40,39250,0.0,25,3,17,2019-10-26 18:38,2019-10-26 19:56,,0.0,,Basic,10,"<python><postgresql><sqlalchemy><pgadmin>, How do you get the url of a postgresql database you made in pgadmin?, so i created a postgresql database and i want to link it to my python code using sqlalchemy but since i made the database in pgadmin i dont know what to mention as the url of the database in the code.
thank you in advance.
","<patron><postgresql><sqlalchemy><pgadmin>, get curl postgresql database made pgadmin?, great postgresql database want link patron code use sqlalchemi since made database pgadmin dont know mention curl database code. thank advance."
51823868,Activity Monitor always Paused in SSMS 17.8.1,"Whenever I open the Activity Monitor in SQL Server Management Studio 17.8.1(14.0.17277) the overview always switches to (paused).  Even if I choose resume, it quickly changes back to paused.
This happens on a variety of SQL Servers and SQL Server versions (2005 through 2016) so I don't believe it is a conflict with old vs new SQL Setups.
I can run Activity Monitor in SSMS 2012 (11.0.2100.60) on the same servers with no error which confirms that the service is actually running and functional.
Any help or insights would be appreciated.  I'm not a fan of switching back and forth between two management studios if I can help it. (I uses 17 so I can have context menus when right clicking on items in SSMS which wont work on 2016 servers in older versions of the studio).
",<sql-server><ssms-2012><activity-monitor><ssms-2017>,774,0,0,373,1,4,10,46,31182,0.0,35,4,17,2018-08-13 13:45,2019-03-06 12:20,,205.0,,Basic,14,"<sql-server><ssms-2012><activity-monitor><ssms-2017>, Activity Monitor always Paused in SSMS 17.8.1, Whenever I open the Activity Monitor in SQL Server Management Studio 17.8.1(14.0.17277) the overview always switches to (paused).  Even if I choose resume, it quickly changes back to paused.
This happens on a variety of SQL Servers and SQL Server versions (2005 through 2016) so I don't believe it is a conflict with old vs new SQL Setups.
I can run Activity Monitor in SSMS 2012 (11.0.2100.60) on the same servers with no error which confirms that the service is actually running and functional.
Any help or insights would be appreciated.  I'm not a fan of switching back and forth between two management studios if I can help it. (I uses 17 so I can have context menus when right clicking on items in SSMS which wont work on 2016 servers in older versions of the studio).
","<sal-server><sums-2012><activity-monitor><sums-2017>, active monitor away pus sum 17.8.1, when open active monitor sal server manage studio 17.8.1(14.0.17277) overview away switch (paused). even choose resume, quickly change back paused. happen variety sal server sal server version (2005 2016) believe conflict old vs new sal steps. run active monitor sum 2012 (11.0.2100.60) server error confirm service actual run functional. help insight would appreciated. i'm fan switch back forth two manage studio help it. (i use 17 context menu right click item sum wont work 2016 server older version studio)."
48868156,How to install the specific version of postgres?,"I need to install a postgres 9.6.6 on Ubuntu 14.04, but the command
apt-get install postgresql-9.6
installes  9.6.7.
How can I install 9.6.6?
An output of command
apt-cache policy postgresql-9.6
is 
postgresql-9.6:
  Installed: 9.6.7-1.pgdg14.04+1
  Candidate: 9.6.7-1.pgdg14.04+1
  Version table:
 *** 9.6.7-1.pgdg14.04+1 0
        500 http://apt.postgresql.org/pub/repos/apt/ trusty-pgdg/main amd64 Packages
        100 /var/lib/dpkg/status
",<linux><postgresql><ubuntu>,443,1,9,957,2,15,30,57,38158,0.0,19,3,17,2018-02-19 14:19,2018-02-19 14:24,,0.0,,Basic,14,"<linux><postgresql><ubuntu>, How to install the specific version of postgres?, I need to install a postgres 9.6.6 on Ubuntu 14.04, but the command
apt-get install postgresql-9.6
installes  9.6.7.
How can I install 9.6.6?
An output of command
apt-cache policy postgresql-9.6
is 
postgresql-9.6:
  Installed: 9.6.7-1.pgdg14.04+1
  Candidate: 9.6.7-1.pgdg14.04+1
  Version table:
 *** 9.6.7-1.pgdg14.04+1 0
        500 http://apt.postgresql.org/pub/repos/apt/ trusty-pgdg/main amd64 Packages
        100 /var/lib/dpkg/status
","<line><postgresql><bunt>, instal specie version postures?, need instal poster 9.6.6 bunt 14.04, command apt-get instal postgresql-9.6 instal 9.6.7. instal 9.6.6? output command apt-each policy postgresql-9.6 postgresql-9.6: installed: 9.6.7-1.pgdg14.04+1 candidate: 9.6.7-1.pgdg14.04+1 version table: *** 9.6.7-1.pgdg14.04+1 0 500 http://apt.postgresql.org/pub/repose/apt/ trusty-pgdp/main amd64 package 100 /war/limb/dog/state"
54738120,Build new Rails app Error loading the 'sqlite3' without evidently write verion,"When generate new rails app, and start the server ""rails s"", first massage I got:
  Puma caught this error: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile. (LoadError)
after reload a page:
  ActiveRecord::ConnectionNotEstablished
  No connection pool with 'primary' found.
  def retrieve_connection(spec_name) #:nodoc:
    pool = retrieve_connection_pool(spec_name)
    raise ConnectionNotEstablished, ""No connection pool with '#{spec_name}' found."" unless pool
    pool.connection
  end
I reinstall ruby, rails, bundler, all except rvm
and I don't know what to do
P.S.
this error disappears when I evidently write sqlite3 verion, but it should work fine from a without it!!!
Help!What to do? or maybe reinstall all of it?
",<ruby-on-rails><ruby><sqlite><rvm><bundler>,872,0,5,173,0,1,10,68,3939,0.0,2,3,17,2019-02-17 22:03,2019-02-18 3:02,2019-02-18 3:02,1.0,1.0,Basic,14,"<ruby-on-rails><ruby><sqlite><rvm><bundler>, Build new Rails app Error loading the 'sqlite3' without evidently write verion, When generate new rails app, and start the server ""rails s"", first massage I got:
  Puma caught this error: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile. (LoadError)
after reload a page:
  ActiveRecord::ConnectionNotEstablished
  No connection pool with 'primary' found.
  def retrieve_connection(spec_name) #:nodoc:
    pool = retrieve_connection_pool(spec_name)
    raise ConnectionNotEstablished, ""No connection pool with '#{spec_name}' found."" unless pool
    pool.connection
  end
I reinstall ruby, rails, bundler, all except rvm
and I don't know what to do
P.S.
this error disappears when I evidently write sqlite3 verion, but it should work fine from a without it!!!
Help!What to do? or maybe reinstall all of it?
","<ruby-on-rails><ruby><quite><rum><bundles>, build new rail pp error load 'sqlite3' without evil write version, genet new rail pp, start server ""rail s"", first massage got: pump caught error: error load 'sqlite3' active record adapted. miss gem depend on? can't active sqlite3 (~> 1.3.6), already active sqlite3-1.4.0. make sure depend ad defile. (loaderror) reload page: activerecord::connectionnotestablish connect pool 'primary' found. def retrieve_connection(spec_name) #:nodes: pool = retrieve_connection_pool(spec_name) rays connectionnotestablished, ""no connect pool '#{spec_name}' found."" unless pool pool.connect end rental ruby, rails, bundles, except rum know p.s. error disappear evil write sqlite3 version, work fine without it!!! help!what do? may rental it?"
56526882,PgAdmin Error: Failed to decrypt the saved password,"After updating my pgAdmin version to V4-4.8 I can not login to my database anymore. I'm trying to login on Windows 10.
The error:
Failed to decrypt the saved password.
Error: 'utf-8' codec can't decode byte 0xc3 in position 0: invalid continuation byte
",<postgresql><pgadmin>,253,0,5,654,0,6,10,60,5811,,74,1,17,2019-06-10 12:57,2019-07-23 1:43,2019-07-23 1:43,43.0,43.0,Basic,14,"<postgresql><pgadmin>, PgAdmin Error: Failed to decrypt the saved password, After updating my pgAdmin version to V4-4.8 I can not login to my database anymore. I'm trying to login on Windows 10.
The error:
Failed to decrypt the saved password.
Error: 'utf-8' codec can't decode byte 0xc3 in position 0: invalid continuation byte
","<postgresql><pgadmin>, pgadmin error: fail decrepit save password, update pgadmin version ve-4.8 login database anymore. i'm try login window 10. error: fail decrepit save password. error: 'utf-8' code can't second bite 0xc3 post 0: invalid continue bite"
48780903,"'limit' clause in Oracle SQL ""SQL command not properly ended""","I know that questions related to 'limit' have been asked before here,
and I have already referred to them. My question is somewhat different.
Here's my query:
select id,somecol from sometable where someval=2 order by id desc limit 3
I'm getting an error saying 'SQL command not properly ended'.
How do I resolve this? If you need additional information, feel free to tell me so.
",<sql><oracle><limit>,379,0,1,173,1,1,8,50,46578,0.0,0,2,17,2018-02-14 6:24,2018-02-14 6:37,2018-02-14 6:59,0.0,0.0,Basic,2,"<sql><oracle><limit>, 'limit' clause in Oracle SQL ""SQL command not properly ended"", I know that questions related to 'limit' have been asked before here,
and I have already referred to them. My question is somewhat different.
Here's my query:
select id,somecol from sometable where someval=2 order by id desc limit 3
I'm getting an error saying 'SQL command not properly ended'.
How do I resolve this? If you need additional information, feel free to tell me so.
","<sal><oracle><limit>, 'limit' class oral sal ""sal command properly ended"", know question relate 'limit' ask here, already refer them. question somewhat different. here' query: select id,somehow some someday=2 order id desk limit 3 i'm get error say 'sal command properly ended'. resolve this? need admit information, feel free tell so."
54809012,knex insert multiple rows,"I have a problem inserting many rows into postgres db with knex.
I have dynamic number of rows needed to be inserted. The result i expect is:
insert row four times (four is for an example. I dont know exact number of inserts as it comes dynamically from frontend):
field_id will be diffrent in every row: (1,2,3,4) - i have array of these ID's
id_of_product will be always the same
value will be always diffrent: (req.body[id] that comes from Frontend) - ID in brackets is same value as the field_id from an
array
How i can achieve that? I tried looping it with forEach, but it's async operation so i can't use .then() as it will be called four times
Here's what i tried. i dont know how to set field_id and req.body to take it dynamically.
fields = [1,2,3,4]
Expected result:
knex creates 4 inserts as follows:
field_id: 1,
product_id: some static id
value: frontValue[1]
ETC
knex('metadata').insert(
 [{ field_id: fields, 
    product_id: product_id, 
    value: req.body[fields] 
 }]
)
",<sql><postgresql><knex.js>,989,0,6,273,2,5,13,67,32728,0.0,3,1,17,2019-02-21 14:09,2019-02-21 14:31,2019-02-21 14:31,0.0,0.0,Basic,9,"<sql><postgresql><knex.js>, knex insert multiple rows, I have a problem inserting many rows into postgres db with knex.
I have dynamic number of rows needed to be inserted. The result i expect is:
insert row four times (four is for an example. I dont know exact number of inserts as it comes dynamically from frontend):
field_id will be diffrent in every row: (1,2,3,4) - i have array of these ID's
id_of_product will be always the same
value will be always diffrent: (req.body[id] that comes from Frontend) - ID in brackets is same value as the field_id from an
array
How i can achieve that? I tried looping it with forEach, but it's async operation so i can't use .then() as it will be called four times
Here's what i tried. i dont know how to set field_id and req.body to take it dynamically.
fields = [1,2,3,4]
Expected result:
knex creates 4 inserts as follows:
field_id: 1,
product_id: some static id
value: frontValue[1]
ETC
knex('metadata').insert(
 [{ field_id: fields, 
    product_id: product_id, 
    value: req.body[fields] 
 }]
)
","<sal><postgresql><knew.is>, knew insert multiple rows, problem insert man row poster do knew. dream number row need inserted. result expect is: insert row four time (four example. dont know exact number insert come dream fronted): field_id different every row: (1,2,3,4) - array id' id_of_product away value away different: (red.body[id] come fronted) - id bracket value field_id array achieve that? try loop french, async over can't use .then() call four time here' tried. dont know set field_id red.body take dynamically. field = [1,2,3,4] expect result: knew great 4 insert follows: field_id: 1, product_id: static id value: frontvalue[1] etc knew('metadata').insert( [{ field_id: fields, product_id: product_id, value: red.body[fields] }] )"
51977699,Turn Presto columns to rows via rotation,"This is the desired input and desired output. I'm unfamiliar with the terms used in SQL or Presto and the documentation seems to point to using map_agg but I think the problem here is dynamically creating columns but was curious if this is possible where the a, b, ... columns are known and finite.  
I'd be great to know the proper function for this in SQL or Presto and of course if this is possible. Preferably in a way that doesn't involve manually adding a clause per desired row => column. There must be a way to do this automatically or by providing a list of values to filter rows that get converted to headers (As in how 'a' below gets moved to being a column title) 
table_a: 
id | key | value
 0 |   'a'  |   1
 1 |   'b'  |   2
Then becomes desired:
id | 'a' | 'b' 
 0    1    2
The closest I can get is to use map_agg to get a set of key: values which can be pulled one at a time in the output. However the desired solution would be to not have to explicitly list every key I want outputted in the end and instead explode or roll out all keys of kvs:
with subquery_A as (
  select 
    id,   
    map_agg(A.key, A.value) as ""kvs""
  from A as a
  group by 1
)
select 
  sub_a.id,
  sub_a.kvs['a'],
  sub_a.kvs['b']
from subquery_A as sub_a
",<sql><string><pivot><presto>,1252,0,29,3507,5,32,38,40,25637,0.0,156,2,17,2018-08-23 2:51,2018-08-28 11:19,,5.0,,Basic,9,"<sql><string><pivot><presto>, Turn Presto columns to rows via rotation, This is the desired input and desired output. I'm unfamiliar with the terms used in SQL or Presto and the documentation seems to point to using map_agg but I think the problem here is dynamically creating columns but was curious if this is possible where the a, b, ... columns are known and finite.  
I'd be great to know the proper function for this in SQL or Presto and of course if this is possible. Preferably in a way that doesn't involve manually adding a clause per desired row => column. There must be a way to do this automatically or by providing a list of values to filter rows that get converted to headers (As in how 'a' below gets moved to being a column title) 
table_a: 
id | key | value
 0 |   'a'  |   1
 1 |   'b'  |   2
Then becomes desired:
id | 'a' | 'b' 
 0    1    2
The closest I can get is to use map_agg to get a set of key: values which can be pulled one at a time in the output. However the desired solution would be to not have to explicitly list every key I want outputted in the end and instead explode or roll out all keys of kvs:
with subquery_A as (
  select 
    id,   
    map_agg(A.key, A.value) as ""kvs""
  from A as a
  group by 1
)
select 
  sub_a.id,
  sub_a.kvs['a'],
  sub_a.kvs['b']
from subquery_A as sub_a
","<sal><string><pilot><preston>, turn preston column row via rotation, desire input desire output. i'm unfamiliar term use sal preston document seem point use map_agg think problem dream great column curious possible a, b, ... column known finite. i'd great know proper function sal preston course possible. prefer way involve manual ad class per desire row => column. must way automatic proved list value filter row get convert header (a 'a' get move column title) table: id | key | value 0 | 'a' | 1 1 | 'b' | 2 become desired: id | 'a' | 'b' 0 1 2 closest get use map_agg get set key: value pull one time output. howe desire slut would explicitly list every key want output end instead explode roll key vs: subquery_a ( select id, map_agg(a.key, a.value) ""vs"" group 1 ) select cuba.id, cuba.vs['a'], cuba.vs['b'] subquery_a cuba"
54334202,How to execute a stored procedure against linked server?,"We currently execute a stored procedure against a linked server using:
EXECUTE [LinkedServer].[DatabaseName].[dbo].[MyProcedure]
For example:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
And this works fine; querying through a linked server works fine.
Disabling the deprecated &quot;Remote Access&quot; feature
There is apparently a little known, seldom used, feature known as remote access. Microsoft has very little to say about what this feature is, except to say here:
This configuration option is an obscure SQL Server to SQL Server communication feature that is deprecated, and you probably shouldn't be using it.
 Important
This feature will be removed in the next version of Microsoft SQL Server. Do not use this feature in new development work, and modify applications that currently use this feature as soon as possible. Use sp_addlinkedserver instead.
The remote access option only applies to servers that are added by using sp_addserver, and is included for backward compatibility.
And from the SQL Server 2000 Books Online:
 Note Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
We only ever added linked servers, and have never used this &quot;remote access&quot; feature, and have never added a server using sp_addserver.
We're all good. Right?
Except turning off remote access breaks everything
An auditor has mentioned that we should turn off the remote access feature:
it's a security checkbox on their clipboard
it's deprecated by Microsoft
it's hardly ever used
and we don't use it
Should be fine, right?
Microsoft documents how to turn off this hardly used feature:
Configure the remote access Server Configuration Option
EXEC sp_configure 'remote access', 0 ;  
GO  
RECONFIGURE ;  
GO
Except when we do: everything goes to hell:
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure?
Just to be absolutely sure I'm using a linked server, I:
EXECUTE sp_dropserver @server='screwdriver', @dropLogins='droplogins'
EXECUTE sp_addlinkedserver N'screwdriver', N'SQL Server'
and re-run my procedure call:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure!?
I can confirm that the server is a linked server (and is not a &quot;remote&quot; server) using:
SELECT SrvName, IsRemote 
FROM master..sysservers
WHERE SrvName = 'Screwdriver'
Srvname      IsRemote
-----------  --------
screwdriver  0
Or using the modern objects:
SELECT Name, Is_Linked
FROM sys.servers
WHERE Name = 'Screwdriver'
Name         Is_linked
-----------  --------
screwdriver  1
To Sum Up
We're at the point now where:
I've disabled remote access
Remote access only applies to servers added through sp_addserver
it doesn't apply to servers added through sp_addlinkedserver
I'm accessing a server added through sp_addlinkedserver
Why isn't it working?
Which brings me to my question:
How to execute a stored procedure against a linked server?
And the corollary:
How to not execute a stored procedure against an added (i.e. &quot;remote&quot;) server?
Bonus Chatter
The remote access configuration option, that you adjust using sp_configure, is also exposed through the user interface. The SSMS UI describes the feature incorrectly:
It incorrect phrasing is:
Allow remote connections to this server
It should be phrased:
Allow remote connections to from this server.
The Books Online also document the feature incorrectly:
Allow remote connections to this server
Controls the execution of stored procedures from remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from a remote server.
It should be:
Allow remote connections to from this server
Controls the execution of stored procedures from to remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from to a remote server.
It makes sense that the youngsters at Microsoft these days don't remember what a 20 year old deprecated feature that they've never touched does.
Documentation from BOL 2000
SQL Server 2000 was the last time this feature was documented. Reproduced here for posterity and debugging purposes:
Configuring Remote Servers
A remote server configuration allows a client connected to one instance of Microsoft SQL Server to execute a stored procedure on another instance of SQL Server without establishing another connection. The server to which the client is connected accepts the client request and sends the request to the remote server on behalf of the client. The remote server processes the request and returns any results to the original server, which in turn passes those results to the client.
If you want to set up a server configuration in order to execute stored procedures on another server and do not have existing remote server configurations, use linked servers instead of remote servers. Both stored procedures and distributed queries are allowed against linked servers; however, only stored procedures are allowed against remote servers.
 Note  Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
See also
Executing a Stored Procedure on a Linked Server
SQL Linked server - Remote access error when executing SP
SQL Server Error: Could not execute remote procedure
MSDN Blogs: Unable to execute a remote stored procedure over a linked server (archive.is)
Configure the remote access Server Configuration Option
sp_addlinkedserver (Transact-SQL)
sp_configure (Transact-SQL)
Security Audit requires turning Remote Access off on all SQL 2005 and SQL 2008 Servers
Alternate title: Disabling SQL Server Remote Access breaks stored procedures.
",<sql-server><sql-server-2008-r2>,6489,16,29,248457,256,876,1230,36,60670,0.0,5616,4,17,2019-01-23 19:16,2019-01-24 13:43,2019-01-24 13:43,1.0,1.0,Basic,9,"<sql-server><sql-server-2008-r2>, How to execute a stored procedure against linked server?, We currently execute a stored procedure against a linked server using:
EXECUTE [LinkedServer].[DatabaseName].[dbo].[MyProcedure]
For example:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
And this works fine; querying through a linked server works fine.
Disabling the deprecated &quot;Remote Access&quot; feature
There is apparently a little known, seldom used, feature known as remote access. Microsoft has very little to say about what this feature is, except to say here:
This configuration option is an obscure SQL Server to SQL Server communication feature that is deprecated, and you probably shouldn't be using it.
 Important
This feature will be removed in the next version of Microsoft SQL Server. Do not use this feature in new development work, and modify applications that currently use this feature as soon as possible. Use sp_addlinkedserver instead.
The remote access option only applies to servers that are added by using sp_addserver, and is included for backward compatibility.
And from the SQL Server 2000 Books Online:
 Note Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
We only ever added linked servers, and have never used this &quot;remote access&quot; feature, and have never added a server using sp_addserver.
We're all good. Right?
Except turning off remote access breaks everything
An auditor has mentioned that we should turn off the remote access feature:
it's a security checkbox on their clipboard
it's deprecated by Microsoft
it's hardly ever used
and we don't use it
Should be fine, right?
Microsoft documents how to turn off this hardly used feature:
Configure the remote access Server Configuration Option
EXEC sp_configure 'remote access', 0 ;  
GO  
RECONFIGURE ;  
GO
Except when we do: everything goes to hell:
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure?
Just to be absolutely sure I'm using a linked server, I:
EXECUTE sp_dropserver @server='screwdriver', @dropLogins='droplogins'
EXECUTE sp_addlinkedserver N'screwdriver', N'SQL Server'
and re-run my procedure call:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure!?
I can confirm that the server is a linked server (and is not a &quot;remote&quot; server) using:
SELECT SrvName, IsRemote 
FROM master..sysservers
WHERE SrvName = 'Screwdriver'
Srvname      IsRemote
-----------  --------
screwdriver  0
Or using the modern objects:
SELECT Name, Is_Linked
FROM sys.servers
WHERE Name = 'Screwdriver'
Name         Is_linked
-----------  --------
screwdriver  1
To Sum Up
We're at the point now where:
I've disabled remote access
Remote access only applies to servers added through sp_addserver
it doesn't apply to servers added through sp_addlinkedserver
I'm accessing a server added through sp_addlinkedserver
Why isn't it working?
Which brings me to my question:
How to execute a stored procedure against a linked server?
And the corollary:
How to not execute a stored procedure against an added (i.e. &quot;remote&quot;) server?
Bonus Chatter
The remote access configuration option, that you adjust using sp_configure, is also exposed through the user interface. The SSMS UI describes the feature incorrectly:
It incorrect phrasing is:
Allow remote connections to this server
It should be phrased:
Allow remote connections to from this server.
The Books Online also document the feature incorrectly:
Allow remote connections to this server
Controls the execution of stored procedures from remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from a remote server.
It should be:
Allow remote connections to from this server
Controls the execution of stored procedures from to remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from to a remote server.
It makes sense that the youngsters at Microsoft these days don't remember what a 20 year old deprecated feature that they've never touched does.
Documentation from BOL 2000
SQL Server 2000 was the last time this feature was documented. Reproduced here for posterity and debugging purposes:
Configuring Remote Servers
A remote server configuration allows a client connected to one instance of Microsoft SQL Server to execute a stored procedure on another instance of SQL Server without establishing another connection. The server to which the client is connected accepts the client request and sends the request to the remote server on behalf of the client. The remote server processes the request and returns any results to the original server, which in turn passes those results to the client.
If you want to set up a server configuration in order to execute stored procedures on another server and do not have existing remote server configurations, use linked servers instead of remote servers. Both stored procedures and distributed queries are allowed against linked servers; however, only stored procedures are allowed against remote servers.
 Note  Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
See also
Executing a Stored Procedure on a Linked Server
SQL Linked server - Remote access error when executing SP
SQL Server Error: Could not execute remote procedure
MSDN Blogs: Unable to execute a remote stored procedure over a linked server (archive.is)
Configure the remote access Server Configuration Option
sp_addlinkedserver (Transact-SQL)
sp_configure (Transact-SQL)
Security Audit requires turning Remote Access off on all SQL 2005 and SQL 2008 Servers
Alternate title: Disabling SQL Server Remote Access breaks stored procedures.
","<sal-server><sal-server-2008-re>, execute store procedure link server?, current execute store procedure link server using: execute [linkedserver].[databasename].[do].[procedure] example: execute screwdriver.arm.do.getcustom 619 work fine; query link server work fine. distal degree &quit;remote access&quit; feature appear little known, seldom used, feature known remote access. microsoft little say feature is, except say here: configur option obscure sal server sal server common feature deprecated, probably use it.  import feature remove next version microsoft sal server. use feature new develop work, modify applied current use feature soon possible. use sp_addlinkedserv instead. remote access option apply server ad use sp_addserver, include backward incompatibility. sal server 2000 book online: note support remote server proved backward compact only. new applied must execute store procedure remote instant sal server use link server instead. ever ad link serves, never use &quit;remote access&quit; feature, never ad server use sp_addserver. we'r good. right? except turn remote access break every auditor mention turn remote access feature: secure checkbox cupboard degree microsoft hardly ever use use fine, right? microsoft document turn hardly use feature: configur remote access server configur option even sp_configur 'remote access', 0 ; go reconfigur ; go except do: every go hell: mug 7201, level 17, state 4, procedure getcustomer, line 1 could execute procedure remote server 'screwdriver' sal server configur remote access. ask system administer reconfigur sal server allow remote access. words failure? absolute sure i'm use link server, i: execute sp_dropserv @server='screwdriver', @droplogins='droplogins' execute sp_addlinkedserv n'screwdriver', n'sal server' re-run procedure call: execute screwdriver.arm.do.getcustom 619 mug 7201, level 17, state 4, procedure getcustomer, line 1 could execute procedure remote server 'screwdriver' sal server configur remote access. ask system administer reconfigur sal server allow remote access. words failure!? confirm server link server (and &quit;remote&quit; server) using: select surname, isremot master..sysserv surname = 'screwdriver' surname isremot ----------- -------- screwdriv 0 use modern objects: select name, slink says.serve name = 'screwdriver' name slink ----------- -------- screwdriv 1 sum we'r point where: i'v distal remote access remote access apply server ad sp_addserv apply server ad sp_addlinkedserv i'm access server ad sp_addlinkedserv working? bring question: execute store procedure link server? corollary: execute store procedure ad (i.e. &quit;remote&quit;) server? bone chatter remote access configur option, adjust use sp_configure, also expose user interface. sum i describe feature incorrectly: incorrect phrase is: allow remote connect server phrased: allow remote connect server. book online also document feature incorrectly: allow remote connect server control execute store procedure remote server run instant sal server. select check box effect set sp_configur remote access option 1. clear prevent execute store procedure remote server. be: allow remote connect server control execute store procedure remote server run instant sal server. select check box effect set sp_configur remote access option 1. clear prevent execute store procedure remote server. make sens youngster microsoft day remember 20 year old degree feature they'v never touch does. document boy 2000 sal server 2000 last time feature document. reproduce poster debut purposes: configur remote server remote server configur allow client connect one instant microsoft sal server execute store procedure not instant sal server without establish not connection. server client connect accept client request send request remote server behalf client. remote server process request return result origin server, turn pass result client. want set server configur order execute store procedure not server exist remote server configuration, use link server instead remote serves. store procedure distribute query allow link serves; however, store procedure allow remote serves. note support remote server proved backward compact only. new applied must execute store procedure remote instant sal server use link server instead. see also execute store procedure link server sal link server - remote access error execute s sal server error: could execute remote procedure man blows: unable execute remote store procedure link server (archive.is) configur remote access server configur option sp_addlinkedserv (transact-sal) sp_configur (transact-sal) secure audit require turn remote access sal 2005 sal 2008 server alter title: distal sal server remote access break store procedures."
51768880,Duplicate columns in Oracle query using row limiting clause,"Since Oracle 12c, we can finally use the SQL standard row limiting clause like this:
SELECT * FROM t FETCH FIRST 10 ROWS ONLY
Now, in Oracle 12.1, there was a limitation that is quite annoying when joining tables. It's not possible to have two columns of the same name in the SELECT clause, when using the row limiting clause. E.g. this raises ORA-00918 in Oracle 12.1
SELECT t.id, u.id FROM t, u FETCH FIRST 10 ROWS ONLY
This is a restriction is documented in the manual for all versions 12.1, 12.2, 18.0:
The workaround is obviously to alias the columns
SELECT t.id AS t_id, u.id AS u_id FROM t, u FETCH FIRST 10 ROWS ONLY
Or to resort to ""classic"" pagination using ROWNUM or window functions.
Curiously, though, the original query with ambiguous ID columns runs just fine from Oracle 12.2 onwards. Is this a documentation bug, or an undocumented feature?
",<sql><oracle><pagination>,858,2,6,213305,130,697,1529,48,583,0.0,6816,2,17,2018-08-09 13:52,2018-09-08 6:27,,30.0,,Advanced,35,"<sql><oracle><pagination>, Duplicate columns in Oracle query using row limiting clause, Since Oracle 12c, we can finally use the SQL standard row limiting clause like this:
SELECT * FROM t FETCH FIRST 10 ROWS ONLY
Now, in Oracle 12.1, there was a limitation that is quite annoying when joining tables. It's not possible to have two columns of the same name in the SELECT clause, when using the row limiting clause. E.g. this raises ORA-00918 in Oracle 12.1
SELECT t.id, u.id FROM t, u FETCH FIRST 10 ROWS ONLY
This is a restriction is documented in the manual for all versions 12.1, 12.2, 18.0:
The workaround is obviously to alias the columns
SELECT t.id AS t_id, u.id AS u_id FROM t, u FETCH FIRST 10 ROWS ONLY
Or to resort to ""classic"" pagination using ROWNUM or window functions.
Curiously, though, the original query with ambiguous ID columns runs just fine from Oracle 12.2 onwards. Is this a documentation bug, or an undocumented feature?
","<sal><oracle><agitation>, public column oral query use row limit clause, since oral c, final use sal standard row limit class like this: select * fetch first 10 row now, oral 12.1, limit quit annoy join tables. possible two column name select clause, use row limit clause. e.g. rays or-00918 oral 12.1 select t.id, u.id t, u fetch first 10 row restrict document manual version 12.1, 12.2, 18.0: workaround obvious asia column select t.id this, u.id said t, u fetch first 10 row resort ""classic"" pain use grownup window functions. curiously, though, origin query ambigu id column run fine oral 12.2 onwards. document bug, undone feature?"
52197486,"FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off","It has been working fine for last several months; and suddenly started noticing this error in application,
FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off
pg_hba.conf has,
# IPv4 local connections:
host    all             all             127.0.0.1/32            md5
host    all             all             0.0.0.0/0               md5
postgresql.conf has,
listen_addresses = '*'
Both file have not been touched/changed for many months.
Has anybody faced similar issue in a running environment ?
I have gone through several connection related issues on stoackoverflow; but they all point to one of these two files being misconfigured. Thats not the issue in this case.
The root cause is found and fixed.
This is what happened (for the benefit of those who might encounter such a strange issue)
Three mysterious entires were found in pg_hba.conf, right at the top of the file
These had reject method configured for user postgres, pgsql &amp; pgdbadm
None of our team members added them
Because these were right at the top, even before ""# PostgreSQL Client Authentication Configuration File...."" comment starts, we couldn't notice it.
I am still not sure, how these appeared there
It might be some upgrade issue - but we haven't updated Postgres
It might be a partially successful hacking attempt - still investigating this
But to be on safer side, we have changed server credentials and looking into other hardening methods.
It just might save someone a sleepless night, if such an issue occurs, in a perfectly running environment.
",<postgresql>,1583,0,5,161,1,1,5,35,39425,0.0,3,1,16,2018-09-06 6:06,2018-10-12 10:54,,36.0,,Basic,14,"<postgresql>, FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off, It has been working fine for last several months; and suddenly started noticing this error in application,
FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off
pg_hba.conf has,
# IPv4 local connections:
host    all             all             127.0.0.1/32            md5
host    all             all             0.0.0.0/0               md5
postgresql.conf has,
listen_addresses = '*'
Both file have not been touched/changed for many months.
Has anybody faced similar issue in a running environment ?
I have gone through several connection related issues on stoackoverflow; but they all point to one of these two files being misconfigured. Thats not the issue in this case.
The root cause is found and fixed.
This is what happened (for the benefit of those who might encounter such a strange issue)
Three mysterious entires were found in pg_hba.conf, right at the top of the file
These had reject method configured for user postgres, pgsql &amp; pgdbadm
None of our team members added them
Because these were right at the top, even before ""# PostgreSQL Client Authentication Configuration File...."" comment starts, we couldn't notice it.
I am still not sure, how these appeared there
It might be some upgrade issue - but we haven't updated Postgres
It might be a partially successful hacking attempt - still investigating this
But to be on safer side, we have changed server credentials and looking into other hardening methods.
It just might save someone a sleepless night, if such an issue occurs, in a perfectly running environment.
","<postgresql>, fatal: pg_hba.cone reject connect host ""127.0.0.1"", user ""postures"", database ""proud"", sal off, work fine last never months; suddenly start notice error application, fatal: pg_hba.cone reject connect host ""127.0.0.1"", user ""postures"", database ""proud"", sal pg_hba.cone has, # iv local connections: host 127.0.0.1/32 md host 0.0.0.0/0 md postgresql.cone has, listen_address = '*' file touched/change man months. anybody face similar issue run environs ? gone never connect relate issue stoackoverflow; point one two file misconfigured. that issue case. root cause found fixed. happen (for benefit might count strange issue) three mystery enter found pg_hba.cone, right top file reject method configur user postures, pgsql &amp; pgdbadm none team member ad right top, even ""# postgresql client authentic configur file...."" comment starts, notice it. still sure, appear might upgrade issue - update poster might partial success hack attempt - still investing safer side, change server credenti look garden methods. might save someone sleepless night, issue occurs, perfectly run environment."
55508830,How to upgrade sqlite 3.8.2 to >= 3.8.3,"In a virtual Env with Python 3.7.2, I am trying to run django's python manage.py startap myapp and I get this error:
raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.8.2).
I'm running Ubuntu Trusty 14.04 Server.
How do I upgrade or update my sqlite version to >=3.8.3?
I ran
$ apt list --installed | grep sqlite
libaprutil1-dbd-sqlite3/trusty,now 1.5.3-1 amd64 [installed,automatic]
libdbd-sqlite3/trusty,now 0.9.0-2ubuntu2 amd64 [installed]
libsqlite3-0/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
libsqlite3-dev/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
python-pysqlite2/trusty,now 2.6.3-3 amd64 [installed]
python-pysqlite2-dbg/trusty,now 2.6.3-3 amd64 [installed]
sqlite3/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
and
sudo apt install --only-upgrade libsqlite3-0
Reading package lists... Done
Building dependency tree      
Reading state information... Done
libsqlite3-0 is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.
EDIT:
the settings.py is stock standard:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
",<python><django><sqlite>,1375,0,24,301,1,2,6,80,31448,0.0,1,5,16,2019-04-04 5:59,2019-04-12 16:50,,8.0,,Basic,14,"<python><django><sqlite>, How to upgrade sqlite 3.8.2 to >= 3.8.3, In a virtual Env with Python 3.7.2, I am trying to run django's python manage.py startap myapp and I get this error:
raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.8.2).
I'm running Ubuntu Trusty 14.04 Server.
How do I upgrade or update my sqlite version to >=3.8.3?
I ran
$ apt list --installed | grep sqlite
libaprutil1-dbd-sqlite3/trusty,now 1.5.3-1 amd64 [installed,automatic]
libdbd-sqlite3/trusty,now 0.9.0-2ubuntu2 amd64 [installed]
libsqlite3-0/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
libsqlite3-dev/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
python-pysqlite2/trusty,now 2.6.3-3 amd64 [installed]
python-pysqlite2-dbg/trusty,now 2.6.3-3 amd64 [installed]
sqlite3/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
and
sudo apt install --only-upgrade libsqlite3-0
Reading package lists... Done
Building dependency tree      
Reading state information... Done
libsqlite3-0 is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.
EDIT:
the settings.py is stock standard:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
","<patron><django><quite>, upgrade quite 3.8.2 >= 3.8.3, virtual end patron 3.7.2, try run django' patron manage.i started map get error: rays improperlyconfigured('split 3.8.3 later require (found %s).' % database.sqlite_version) django.core.exceptions.improperlyconfigured: quite 3.8.3 later require (found 3.8.2). i'm run bunt trust 14.04 server. upgrade update quite version >=3.8.3? ran $ apt list --instal | grew quite libaprutil1-did-sqlite3/trusty,now 1.5.3-1 amd64 [installed,automatic] lidded-sqlite3/trusty,now 0.9.0-2ubuntu2 amd64 [installed] libsqlite3-0/trusty-updated,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed] libsqlite3-de/trusty-updated,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed] patron-pysqlite2/trusty,now 2.6.3-3 amd64 [installed] patron-pysqlite2-dog/trusty,now 2.6.3-3 amd64 [installed] sqlite3/trusty-updated,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed] so apt instal --only-upgrade libsqlite3-0 read package lists... done build depend tree read state information... done libsqlite3-0 already newest version. 0 upgrade, 0 newly installed, 0 remove 14 upgrade. edit: settings.i stock standard: database = { 'default': { 'engine': 'django.do.backed.sqlite3', 'name': os.path.join(base_dir, 'do.sqlite3'), } }"
48327065,Why does conversion from DATETIME to DATETIME2 appear to change value?,"I had a stored procedure comparing two dates. From the logic of my application, I expected them to be equal. However, the comparison failed. The reason for this was the fact that one of the values was stored as a DATETIME and had to be CONVERT-ed to a DATETIME2 before being compared to the other DATETIME2. Apparently, this changed its value. I have run this little test:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2='2018-01-18 16:12:25.1130000'
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
Which gave me the following result:
Why is there the difference of 333333ns between these values? I thought that a DATETIME2, as a more precise type, should be able to accurately represent all the values which can be stored in a DATETIME? The documentation of DATETIME2 only says:
  When the conversion is from datetime, the date and time are copied. The fractional precision is extended to 7 digits.
No warnings about the conversion adding or subtracting 333333ns to or from the value! So why does this happen?
I am using SQL Server 2016. 
edit: Strangely, on a different server I am getting a zero difference. Both are SQL Server 2016 but the one where I have the problem has compatibility level set to 130, the one where I don't has it set to 120. Switching between them changes this behaviour.
edit2: DavidG suggested in the comments that the value I am using can be represented as a DATETIME2 but not a DATETIME. So I have modified my test to make sure that the value I am assigning to @DateTime2 is a valid DATETIME value:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2=CONVERT(DATETIME2, @DateTime)
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
This helps a little because the difference is smaller but still not zero:
",<sql><sql-server><t-sql><datetime>,1848,3,16,3879,2,27,40,60,4195,0.0,421,4,16,2018-01-18 17:25,2018-01-18 17:29,2018-01-18 17:43,0.0,0.0,Intermediate,17,"<sql><sql-server><t-sql><datetime>, Why does conversion from DATETIME to DATETIME2 appear to change value?, I had a stored procedure comparing two dates. From the logic of my application, I expected them to be equal. However, the comparison failed. The reason for this was the fact that one of the values was stored as a DATETIME and had to be CONVERT-ed to a DATETIME2 before being compared to the other DATETIME2. Apparently, this changed its value. I have run this little test:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2='2018-01-18 16:12:25.1130000'
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
Which gave me the following result:
Why is there the difference of 333333ns between these values? I thought that a DATETIME2, as a more precise type, should be able to accurately represent all the values which can be stored in a DATETIME? The documentation of DATETIME2 only says:
  When the conversion is from datetime, the date and time are copied. The fractional precision is extended to 7 digits.
No warnings about the conversion adding or subtracting 333333ns to or from the value! So why does this happen?
I am using SQL Server 2016. 
edit: Strangely, on a different server I am getting a zero difference. Both are SQL Server 2016 but the one where I have the problem has compatibility level set to 130, the one where I don't has it set to 120. Switching between them changes this behaviour.
edit2: DavidG suggested in the comments that the value I am using can be represented as a DATETIME2 but not a DATETIME. So I have modified my test to make sure that the value I am assigning to @DateTime2 is a valid DATETIME value:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2=CONVERT(DATETIME2, @DateTime)
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
This helps a little because the difference is smaller but still not zero:
","<sal><sal-server><t-sal><daytime>, covers datetim datetime2 appear change value?, store procedure compare two dates. logic application, expect equal. however, comparison failed. reason fact one value store datetim convert- datetime2 compare datetime2. apparently, change value. run little test: declare @datetim daytime='2018-01-18 16:12:25.113' declare @datetime2 datetime2='2018-01-18 16:12:25.1130000' select @daytime, @datetime2, datediff(nanosecond, @daytime, @datetime2) gave follow result: differ 333333n values? thought datetime2, precise type, all occur repress value store daytime? document datetime2 says: covers daytime, date time copied. fraction precise extend 7 digits. warn covers ad subtracted 333333n value! happen? use sal server 2016. edit: strangely, differ server get zero difference. sal server 2016 one problem compact level set 130, one set 120. switch change behaviour. edit: david suggest comment value use repress datetime2 daytime. modify test make sure value assign @datetime2 valid datetim value: declare @datetim daytime='2018-01-18 16:12:25.113' declare @datetime2 datetime2=convert(datetime2, @daytime) select @daytime, @datetime2, datediff(nanosecond, @daytime, @datetime2) help little differ smaller still zero:"
52731361,SQLAlchemy Group By With Full Child Objects,"Imagine the following Media table:
| site       | show_id | time |
| ---------------------|-------|
| CNN        | 1       | 'a'   |
| ABC        | 2       | 'b'   |
| ABC        | 5       | 'c'   |
| CNN        | 3       | 'd'   |
| NBC        | 4       | 'e'   |
| NBC        | 5       | 'f'   |
--------------------------------
I would like to iterate over query results grouped by show_id and have tried this query:
listings = session.query(Media).filter(Media.site == ""CNN"").group_by(Media.show_id).all()
Here's how I would like to iterate over the results:
for showtimes in listings:
    for show in showtimes:
        print(show.time)
But that query doesn't give me all of the grouped child objects.  What am I missing?
",<python><sqlalchemy>,727,0,13,3820,5,43,56,49,9843,0.0,153,2,16,2018-10-10 1:26,2018-10-12 6:30,2018-10-12 6:30,2.0,2.0,Intermediate,17,"<python><sqlalchemy>, SQLAlchemy Group By With Full Child Objects, Imagine the following Media table:
| site       | show_id | time |
| ---------------------|-------|
| CNN        | 1       | 'a'   |
| ABC        | 2       | 'b'   |
| ABC        | 5       | 'c'   |
| CNN        | 3       | 'd'   |
| NBC        | 4       | 'e'   |
| NBC        | 5       | 'f'   |
--------------------------------
I would like to iterate over query results grouped by show_id and have tried this query:
listings = session.query(Media).filter(Media.site == ""CNN"").group_by(Media.show_id).all()
Here's how I would like to iterate over the results:
for showtimes in listings:
    for show in showtimes:
        print(show.time)
But that query doesn't give me all of the grouped child objects.  What am I missing?
","<patron><sqlalchemy>, sqlalchemi group full child objects, imagine follow media table: | site | showed | time | | ---------------------|-------| | can | 1 | 'a' | | abc | 2 | 'b' | | abc | 5 | 'c' | | can | 3 | 'd' | | bc | 4 | 'e' | | bc | 5 | 'f' | -------------------------------- would like inter query result group showed try query: list = session.query(media).filter(media.sit == ""can"").group_by(media.showed).all() here' would like inter results: showtim listing: show showtimes: print(show.time) query give group child objects. missing?"
62442611,InnoDB initialization warnings,"I am using docker on Windows 10 to build full web stack (php, nginx, mysql 8). Using docker compose.
docker-compose.yml 
Here I am using services to build web applcation. But i will show you only mysql service
version: '3.8'
services:
    db:
        build: services/mysql
        container_name: db
        image: projects/laradock_mysql:latest
        env_file: ../.env
        restart: on-failure
        volumes:
            - ./storage/data:/var/lib/mysql
        ports:
            - 3306:3306
        networks:
            - sites
networks:
    sites:
        driver: bridge
        ipam:
            driver: default
            config:
                - subnet: 10.100.36.0/24
MySQL DockerFile
FROM mysql:8
COPY conf/my.cnf /etc/mysql/conf.d
EXPOSE 3306
MySQL config file
[mysqld]
innodb_flush_method=O_DSYNC
innodb_flush_log_at_trx_commit=0
default-authentication-plugin=mysql_native_password
Output log when docker-compose up
2020-06-18T04:40:52.552867Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-06-18T04:40:55.549460Z 1 [Warning] [MY-012579] [InnoDB] fallocate(15, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.568116Z 1 [Warning] [MY-012579] [InnoDB] fallocate(16, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.582720Z 1 [Warning] [MY-012579] [InnoDB] fallocate(17, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.600203Z 1 [Warning] [MY-012579] [InnoDB] fallocate(18, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.614565Z 1 [Warning] [MY-012579] [InnoDB] fallocate(19, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.630446Z 1 [Warning] [MY-012579] [InnoDB] fallocate(20, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.649691Z 1 [Warning] [MY-012579] [InnoDB] fallocate(21, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.666272Z 1 [Warning] [MY-012579] [InnoDB] fallocate(22, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.682022Z 1 [Warning] [MY-012579] [InnoDB] fallocate(23, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.698763Z 1 [Warning] [MY-012579] [InnoDB] fallocate(24, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.379410Z 1 [Warning] [MY-012579] [InnoDB] fallocate(25, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.384741Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
Thanks for a Help!
",<mysql><docker><alpine-linux>,2771,0,57,161,0,1,3,39,3162,0.0,0,1,16,2020-06-18 4:53,2020-10-06 18:23,,110.0,,Advanced,39,"<mysql><docker><alpine-linux>, InnoDB initialization warnings, I am using docker on Windows 10 to build full web stack (php, nginx, mysql 8). Using docker compose.
docker-compose.yml 
Here I am using services to build web applcation. But i will show you only mysql service
version: '3.8'
services:
    db:
        build: services/mysql
        container_name: db
        image: projects/laradock_mysql:latest
        env_file: ../.env
        restart: on-failure
        volumes:
            - ./storage/data:/var/lib/mysql
        ports:
            - 3306:3306
        networks:
            - sites
networks:
    sites:
        driver: bridge
        ipam:
            driver: default
            config:
                - subnet: 10.100.36.0/24
MySQL DockerFile
FROM mysql:8
COPY conf/my.cnf /etc/mysql/conf.d
EXPOSE 3306
MySQL config file
[mysqld]
innodb_flush_method=O_DSYNC
innodb_flush_log_at_trx_commit=0
default-authentication-plugin=mysql_native_password
Output log when docker-compose up
2020-06-18T04:40:52.552867Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-06-18T04:40:55.549460Z 1 [Warning] [MY-012579] [InnoDB] fallocate(15, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.568116Z 1 [Warning] [MY-012579] [InnoDB] fallocate(16, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.582720Z 1 [Warning] [MY-012579] [InnoDB] fallocate(17, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.600203Z 1 [Warning] [MY-012579] [InnoDB] fallocate(18, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.614565Z 1 [Warning] [MY-012579] [InnoDB] fallocate(19, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.630446Z 1 [Warning] [MY-012579] [InnoDB] fallocate(20, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.649691Z 1 [Warning] [MY-012579] [InnoDB] fallocate(21, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.666272Z 1 [Warning] [MY-012579] [InnoDB] fallocate(22, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.682022Z 1 [Warning] [MY-012579] [InnoDB] fallocate(23, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.698763Z 1 [Warning] [MY-012579] [InnoDB] fallocate(24, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.379410Z 1 [Warning] [MY-012579] [InnoDB] fallocate(25, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.384741Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
Thanks for a Help!
","<myself><doctor><aline-line>, innodb into warnings, use doctor window 10 build full web stick (pp, nine, myself 8). use doctor compose. doctor-compose.you use service build web application. show myself service version: '3.8' services: do: build: services/myself container_name: do image: projects/laradock_mysql:latest env_file: ../.end start: on-failure volumes: - ./storage/data:/war/limb/myself ports: - 3306:3306 network: - site network: sites: driver: bring am: driver: default confirm: - subject: 10.100.36.0/24 myself dockerfil myself:8 copy cone/my.cf /etc/myself/cone.d expose 3306 myself confirm file [myself] innodb_flush_method=o_dsync innodb_flush_log_at_trx_commit=0 default-authentication-plain=mysql_native_password output log doctor-compose 2020-06-18t04:40:52.552867z 1 [system] [my-013576] [innodb] innodb into started. 2020-06-18t04:40:55.549460z 1 [warning] [my-012579] [innodb] allocate(15, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.568116z 1 [warning] [my-012579] [innodb] allocate(16, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.582720z 1 [warning] [my-012579] [innodb] allocate(17, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.600203z 1 [warning] [my-012579] [innodb] allocate(18, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.614565z 1 [warning] [my-012579] [innodb] allocate(19, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.630446z 1 [warning] [my-012579] [innodb] allocate(20, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.649691z 1 [warning] [my-012579] [innodb] allocate(21, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.666272z 1 [warning] [my-012579] [innodb] allocate(22, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.682022z 1 [warning] [my-012579] [innodb] allocate(23, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:55.698763z 1 [warning] [my-012579] [innodb] allocate(24, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:56.379410z 1 [warning] [my-012579] [innodb] allocate(25, falloc_fl_punch_hol | falloc_fl_keep_size, 0, 16384) return error: 22 2020-06-18t04:40:56.384741z 1 [system] [my-013577] [innodb] innodb into ended. thank help!"
53297872,How can I sum multiple columns in a spark dataframe in pyspark?,"I've got a list of column names I want to sum
columns = ['col1','col2','col3']
How can I add the three and put it in a new column ? (in an automatic way, so that I can change the column list and have new results)
Dataframe with result I want:
col1   col2   col3   result
 1      2      3       6
",<python><apache-spark><pyspark><apache-spark-sql>,296,0,3,2093,4,16,38,75,47307,0.0,2055,3,16,2018-11-14 10:21,2018-11-14 10:25,2018-11-14 10:25,0.0,0.0,Basic,10,"<python><apache-spark><pyspark><apache-spark-sql>, How can I sum multiple columns in a spark dataframe in pyspark?, I've got a list of column names I want to sum
columns = ['col1','col2','col3']
How can I add the three and put it in a new column ? (in an automatic way, so that I can change the column list and have new results)
Dataframe with result I want:
col1   col2   col3   result
 1      2      3       6
","<patron><apache-spark><spark><apache-spark-sal>, sum multiple column spark datafram spark?, i'v got list column name want sum column = ['cold','cold','cold'] add three put new column ? (in automatic way, change column list new results) datafram result want: cold cold cold result 1 2 3 6"
62761424,How to create a database diagrams in visual studio code?,"I am trying Visual studio code to code the database but I cannot create the database diagram. Is there a way I can create it just like in SSMS.
Thank you.
",<sql><database><visual-studio-code><rdbms><diagram>,155,1,0,161,2,2,4,50,33892,0.0,208,1,16,2020-07-06 17:31,2020-11-08 3:13,,125.0,,Basic,3,"<sql><database><visual-studio-code><rdbms><diagram>, How to create a database diagrams in visual studio code?, I am trying Visual studio code to code the database but I cannot create the database diagram. Is there a way I can create it just like in SSMS.
Thank you.
","<sal><database><visual-studio-code><rooms><diagram>, great database diagram visual studio code?, try visual studio code code database cannot great database diagram. way great like sums. thank you."
50370683,"Room migration: ""no such table: room_table_modification_log""","Room 1.1.0 version.
I am getting this error on the first run after migration.
It runs well if I close the app and start it again. 
ROOM: Cannot run invalidation tracker. Is the db closed?
java.lang.IllegalStateException: Cannot perform this operation because the connection pool has been closed.
    at android.database.sqlite.SQLiteConnectionPool.throwIfClosedLocked(SQLiteConnectionPool.java:1182)
    at android.database.sqlite.SQLiteConnectionPool.waitForConnection(SQLiteConnectionPool.java:792)
    at android.database.sqlite.SQLiteConnectionPool.acquireConnection(SQLiteConnectionPool.java:518)
Caused By : SQL(query) error or missing database.
    (no such table: room_table_modification_log (code 1): , while compiling: DELETE FROM proposals WHERE hotel_id = ?)
#################################################################
    at android.database.sqlite.SQLiteConnection.nativePrepareStatement(Native Method)
    at android.database.sqlite.SQLiteConnection.acquirePreparedStatement(SQLiteConnection.java:1095)
    at android.database.sqlite.SQLiteConnection.prepare(SQLiteConnection.java:660)
    at android.database.sqlite.SQLiteSession.prepare(SQLiteSession.java:588)
    at android.database.sqlite.SQLiteProgram.&lt;init&gt;(SQLiteProgram.java:59)
    at android.database.sqlite.SQLiteStatement.&lt;init&gt;(SQLiteStatement.java:31)
    at android.database.sqlite.SQLiteDatabase.compileStatement(SQLiteDatabase.java:1417)
    at android.arch.persistence.db.framework.FrameworkSQLiteDatabase.compileStatement(FrameworkSQLiteDatabase.java:64)
    at android.arch.persistence.room.RoomDatabase.compileStatement(RoomDatabase.java:244)
    at android.arch.persistence.room.SharedSQLiteStatement.createNewStatement(SharedSQLiteStatement.java:65)
    at android.arch.persistence.room.SharedSQLiteStatement.getStmt(SharedSQLiteStatement.java:77)
    at android.arch.persistence.room.SharedSQLiteStatement.acquire(SharedSQLiteStatement.java:87)
    at com.hotellook.db.dao.FavoritesDao_Impl.removeProposals(FavoritesDao_Impl.java:723)
The DB initialization code looks like this (Dagger AppModule)
@Provides
@Singleton
fun provideDataBase(app: Application): AppDatabase =
  Room.databaseBuilder(app, AppDatabase::class.java, DATABASE_NAME)
      .addMigrations(MIGRATION_1_2)
      .addMigrations(MIGRATION_2_3)
      .build()
private companion object {
  const val DATABASE_NAME = ""app.db""
  val MIGRATION_1_2 = object : Migration(1, 2) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column0 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column0 TEXT;"")
    }
  }
  val MIGRATION_2_3 = object : Migration(2, 3) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""CREATE TABLE table0 (column1 INTEGER NOT NULL, column2 TEXT NOT NULL, PRIMARY KEY(column1))"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column2 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column3 REAL;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column2 REAL;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column1 INTEGER DEFAULT 0;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column2 TEXT;"")
    }
  } 
The app upgrades from version 2 to 3.
How can I fix it?
",<android><sqlite><database-migration><android-room>,3454,0,52,912,1,8,19,39,5597,,16,3,16,2018-05-16 12:14,2018-05-19 0:29,2018-05-19 0:29,3.0,3.0,Intermediate,31,"<android><sqlite><database-migration><android-room>, Room migration: ""no such table: room_table_modification_log"", Room 1.1.0 version.
I am getting this error on the first run after migration.
It runs well if I close the app and start it again. 
ROOM: Cannot run invalidation tracker. Is the db closed?
java.lang.IllegalStateException: Cannot perform this operation because the connection pool has been closed.
    at android.database.sqlite.SQLiteConnectionPool.throwIfClosedLocked(SQLiteConnectionPool.java:1182)
    at android.database.sqlite.SQLiteConnectionPool.waitForConnection(SQLiteConnectionPool.java:792)
    at android.database.sqlite.SQLiteConnectionPool.acquireConnection(SQLiteConnectionPool.java:518)
Caused By : SQL(query) error or missing database.
    (no such table: room_table_modification_log (code 1): , while compiling: DELETE FROM proposals WHERE hotel_id = ?)
#################################################################
    at android.database.sqlite.SQLiteConnection.nativePrepareStatement(Native Method)
    at android.database.sqlite.SQLiteConnection.acquirePreparedStatement(SQLiteConnection.java:1095)
    at android.database.sqlite.SQLiteConnection.prepare(SQLiteConnection.java:660)
    at android.database.sqlite.SQLiteSession.prepare(SQLiteSession.java:588)
    at android.database.sqlite.SQLiteProgram.&lt;init&gt;(SQLiteProgram.java:59)
    at android.database.sqlite.SQLiteStatement.&lt;init&gt;(SQLiteStatement.java:31)
    at android.database.sqlite.SQLiteDatabase.compileStatement(SQLiteDatabase.java:1417)
    at android.arch.persistence.db.framework.FrameworkSQLiteDatabase.compileStatement(FrameworkSQLiteDatabase.java:64)
    at android.arch.persistence.room.RoomDatabase.compileStatement(RoomDatabase.java:244)
    at android.arch.persistence.room.SharedSQLiteStatement.createNewStatement(SharedSQLiteStatement.java:65)
    at android.arch.persistence.room.SharedSQLiteStatement.getStmt(SharedSQLiteStatement.java:77)
    at android.arch.persistence.room.SharedSQLiteStatement.acquire(SharedSQLiteStatement.java:87)
    at com.hotellook.db.dao.FavoritesDao_Impl.removeProposals(FavoritesDao_Impl.java:723)
The DB initialization code looks like this (Dagger AppModule)
@Provides
@Singleton
fun provideDataBase(app: Application): AppDatabase =
  Room.databaseBuilder(app, AppDatabase::class.java, DATABASE_NAME)
      .addMigrations(MIGRATION_1_2)
      .addMigrations(MIGRATION_2_3)
      .build()
private companion object {
  const val DATABASE_NAME = ""app.db""
  val MIGRATION_1_2 = object : Migration(1, 2) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column0 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column0 TEXT;"")
    }
  }
  val MIGRATION_2_3 = object : Migration(2, 3) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""CREATE TABLE table0 (column1 INTEGER NOT NULL, column2 TEXT NOT NULL, PRIMARY KEY(column1))"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column2 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column3 REAL;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column2 REAL;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column1 INTEGER DEFAULT 0;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column2 TEXT;"")
    }
  } 
The app upgrades from version 2 to 3.
How can I fix it?
","<andros><quite><database-migration><andros-room>, room migration: ""no table: room_table_modification_log"", room 1.1.0 version. get error first run migration. run well close pp start again. room: cannot run invalid trace. do closed? cava.long.illegalstateexception: cannot perform over connect pool closed. andros.database.quite.sqliteconnectionpool.throwifclosedlocked(sqliteconnectionpool.cava:1182) andros.database.quite.sqliteconnectionpool.waitforconnection(sqliteconnectionpool.cava:792) andros.database.quite.sqliteconnectionpool.acquireconnection(sqliteconnectionpool.cava:518) cause : sal(query) error miss database. (no table: room_table_modification_log (code 1): , complying: delete propos hotel_id = ?) ################################################################# andros.database.quite.sqliteconnection.nativepreparestatement(n method) andros.database.quite.sqliteconnection.acquirepreparedstatement(sqliteconnection.cava:1095) andros.database.quite.sqliteconnection.prepare(sqliteconnection.cava:660) andros.database.quite.sqlitesession.prepare(sqlitesession.cava:588) andros.database.quite.sqliteprogram.&it;knit&it;(sqliteprogram.cava:59) andros.database.quite.sqlitestatement.&it;knit&it;(sqlitestatement.cava:31) andros.database.quite.sqlitedatabase.compilestatement(sqlitedatabase.cava:1417) andros.arch.persistence.do.framework.frameworksqlitedatabase.compilestatement(frameworksqlitedatabase.cava:64) andros.arch.persistence.room.roomdatabase.compilestatement(roomdatabase.cava:244) andros.arch.persistence.room.sharedsqlitestatement.createnewstatement(sharedsqlitestatement.cava:65) andros.arch.persistence.room.sharedsqlitestatement.getstmt(sharedsqlitestatement.cava:77) andros.arch.persistence.room.sharedsqlitestatement.acquire(sharedsqlitestatement.cava:87) com.hotellook.do.do.favoritesdao_impl.removeproposals(favoritesdao_impl.cava:723) do into code look like (dagger appmodule) @proved @simpleton fun providedatabase(pp: application): appdatabas = room.databasebuilder(pp, appdatabase::class.cava, database_name) .addmigrations(migration_1_2) .addmigrations(migration_2_3) .build() privat companion object { cost val database_nam = ""pp.do"" val migration_1_2 = object : migration(1, 2) { overdid fun migrate(database: supportsqlitedatabase) { database.execsql(""at table table add column column text;"") database.execsql(""at table table add column column text;"") } } val migration_2_3 = object : migration(2, 3) { overdid fun migrate(database: supportsqlitedatabase) { database.execsql(""or table table (column inter null, column text null, primary key(column))"") database.execsql(""at table table add column column text;"") database.execsql(""at table table add column column text;"") database.execsql(""at table table add column column real;"") database.execsql(""at table table add column column text;"") database.execsql(""at table table add column column real;"") database.execsql(""at table table add column column inter default 0;"") database.execsql(""at table table add column column text;"") } } pp upgrade version 2 3. fix it?"
48454336,SSIS: Code page goes back to 65001,"In an SSIS package that I'm writing, I have a CSV file as a source. On the Connection Manager General page, it has 65001 as the Code page (I was testing something). Unicode is not checked.
The columns map to a SQL Server destination table with varchar (among others) columns. 
  There's an error at the destination: The column ""columnname"" cannot be processed because more than one code page (65001 and 1252) are specified for it.
My SQL columns have to be varchar, not nvarchar due to other applications that use it.
On the Connection Manager General page I then change the Code page to 1252 (ANSI - Latin I) and OK out, but when I open it again it's back to 65001. It doesn't make a difference if (just for test) I check Unicode or not.
As a note, all this started happening after the CSV file and the SQL table had columns added and removed (users, you know.) Before that, I had no issues whatsoever. Yes, I refreshed the OLE DB destination in the Advanced Editor.
This is SQL Server 2012 and whichever version of BIDS and SSIS come with it.
",<sql-server><csv><encoding><ssis><etl>,1045,0,6,2367,15,50,68,75,62382,0.0,393,7,16,2018-01-26 0:56,2018-01-27 11:31,2018-01-27 11:31,1.0,1.0,Intermediate,15,"<sql-server><csv><encoding><ssis><etl>, SSIS: Code page goes back to 65001, In an SSIS package that I'm writing, I have a CSV file as a source. On the Connection Manager General page, it has 65001 as the Code page (I was testing something). Unicode is not checked.
The columns map to a SQL Server destination table with varchar (among others) columns. 
  There's an error at the destination: The column ""columnname"" cannot be processed because more than one code page (65001 and 1252) are specified for it.
My SQL columns have to be varchar, not nvarchar due to other applications that use it.
On the Connection Manager General page I then change the Code page to 1252 (ANSI - Latin I) and OK out, but when I open it again it's back to 65001. It doesn't make a difference if (just for test) I check Unicode or not.
As a note, all this started happening after the CSV file and the SQL table had columns added and removed (users, you know.) Before that, I had no issues whatsoever. Yes, I refreshed the OLE DB destination in the Advanced Editor.
This is SQL Server 2012 and whichever version of BIDS and SSIS come with it.
","<sal-server><is><encoding><suis><etc>, suis: code page go back 65001, si package i'm writing, is file source. connect manage genet page, 65001 code page (i test something). united checked. column map sal server destiny table varchar (among others) columns. there' error destination: column ""columnname"" cannot process one code page (65001 1252) specific it. sal column varchar, nvarchar due applied use it. connect manage genet page change code page 1252 (anti - latin i) ok out, open back 65001. make differ (just test) check united not. note, start happen is file sal table column ad remove (users, know.) that, issue whatsoever. yes, refresh ole do destiny advance editor. sal server 2012 which version bid si come it."
56746192,Table Partitions with Entity Framework Core,"I have a large database that will use partitioned column-store tables in a redesign. Is it possible to specify the partition in the generated sql with Entity Framework Core 2.2?
This is for an Azure SQL hyperscale database with a table which currently contains about 3 billion rows. When using stored procedures to execute the requests performance is excellent but if the partition range is not specified in the query performance is less that optimal. I am hoping to move away from the inline sql we are currently using in the application layer and move to entity framework core. Being able to specify the partition for the tenant is our only blocker at the moment. 
This is an example where clause in the stored proceduure
Select @Range = $PARTITION.TenantRange(@InputTenantId)
Select ..... FROM xxx where $PARTITION.TenantRange(TenantId) = @range
The above query would provide excellent performance but I am hoping I can make the same specification of the partition using entity framework.
",<sql-server><entity-framework-core><data-partitioning>,992,0,3,161,0,1,4,66,2529,0.0,0,0,16,2019-06-25 2:49,,,,,Basic,3,"<sql-server><entity-framework-core><data-partitioning>, Table Partitions with Entity Framework Core, I have a large database that will use partitioned column-store tables in a redesign. Is it possible to specify the partition in the generated sql with Entity Framework Core 2.2?
This is for an Azure SQL hyperscale database with a table which currently contains about 3 billion rows. When using stored procedures to execute the requests performance is excellent but if the partition range is not specified in the query performance is less that optimal. I am hoping to move away from the inline sql we are currently using in the application layer and move to entity framework core. Being able to specify the partition for the tenant is our only blocker at the moment. 
This is an example where clause in the stored proceduure
Select @Range = $PARTITION.TenantRange(@InputTenantId)
Select ..... FROM xxx where $PARTITION.TenantRange(TenantId) = @range
The above query would provide excellent performance but I am hoping I can make the same specification of the partition using entity framework.
","<sal-server><entity-framework-core><data-petitioning>, table partite entity framework core, large database use partite column-story table design. possible specific partite genet sal entity framework core 2.2? azur sal hyperscal database table current contain 3 billion rows. use store procedure execute request perform expel partite rang specific query perform less optical. hope move away indian sal current use applied layer move entity framework core. all specific partite tenant blocked moment. example class store procedure select @rang = $partition.tenantrange(@inputtenantid) select ..... xxx $partition.tenantrange(tenants) = @rang query would proved expel perform hope make specie partite use entity framework."
58786791,Is there a way to directly insert data from a parquet file into PostgreSQL database?,"I'm trying to restore some historic backup files that saved in parquet format, and I want to read from them once and write the data into a PostgreSQL database.
I know that backup files saved using spark, but there is a strict restriction for me that I cant install spark in the DB machine or read the parquet file using spark in a remote device and write it to the database using spark_df.write.jdbc. Everything needs to happen on the DB machine and in the absence of spark and Hadoop only using Postgres and Bash scripting.
my files structure is something like:
foo/
    foo/part-00000-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00001-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00002-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    ..
    ..
I expect to read data and schema from each parquet folder like foo, create a table using that schema and write the data into the shaped table, only using bash and Postgres CLI.
",<bash><postgresql><hdfs><parquet>,987,0,8,410,1,4,16,65,31361,0.0,243,2,16,2019-11-10 8:05,2019-11-10 14:41,2019-11-10 14:41,0.0,0.0,Basic,10,"<bash><postgresql><hdfs><parquet>, Is there a way to directly insert data from a parquet file into PostgreSQL database?, I'm trying to restore some historic backup files that saved in parquet format, and I want to read from them once and write the data into a PostgreSQL database.
I know that backup files saved using spark, but there is a strict restriction for me that I cant install spark in the DB machine or read the parquet file using spark in a remote device and write it to the database using spark_df.write.jdbc. Everything needs to happen on the DB machine and in the absence of spark and Hadoop only using Postgres and Bash scripting.
my files structure is something like:
foo/
    foo/part-00000-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00001-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00002-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    ..
    ..
I expect to read data and schema from each parquet folder like foo, create a table using that schema and write the data into the shaped table, only using bash and Postgres CLI.
","<base><postgresql><hofs><parquet>, way directly insert data parquet file postgresql database?, i'm try restore history back file save parquet format, want read write data postgresql database. know back file save use spark, strict restrict can instal spark do machine read parquet file use spark remote devil write database use spark_df.write.job. every need happen do machine absence spark hadoop use poster base scraping. file structure cometh like: foo/ foo/part-00000-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.sappy.parquet foo/part-00001-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.sappy.parquet foo/part-00002-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.sappy.parquet .. .. expect read data scheme parquet older like foo, great table use scheme write data shape table, use base poster coli."
54026900,Check that a List parameter is null in a Spring data JPA query,"I have a Spring Boot application and use Spring Data JPA to query a MySQL database.
I need to get a list of courses filtered with some parameters.
I usually use the syntax param IS NULL or (/*do something with param*/) so that it ignores the parameter if it is null.
With simple datatypes I have no problems but when it comes to a List of objects I don't know how to check for NULL value. How can I check if the ?3 parameter is NULL in the following query ?
@Query(""SELECT DISTINCT c FROM Course c\n"" +
       ""WHERE c.courseDate &lt; CURRENT_TIME\n"" +
       ""AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\n"" +
       ""AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\n"" +
       ""AND ((?3) IS NULL OR (c.category IN ?3)) "")
List&lt;Course&gt; getCoursesFiltered(Long courseId, String filter, List&lt;Category&gt; categories);
Error is :
  could not extract ResultSet; SQL [n/a]; nested exception is org.hibernate.exception.DataException: could not extract ResultSet[SQL: 1241, 21000]
And in the stack trace I can see :
  Caused by: java.sql.SQLException: Operand should contain 1 column(s)
Indeed generated query would be ... AND ((?3, ?4, ?5) IS NULL OR (c.category IN (?3, ?4, ?5))) if my list contains 3 elements. But IS NULL cannot be applied to multiple elements (query works fine if my list contain only one element).
I have tried size(?3) &lt; 1, length(?3) &lt; 1, (?3) IS EMPTY, etc. but still no luck.
",<sql><jpa><spring-data-jpa><spring-data><hql>,1426,0,15,14705,11,57,84,54,36913,0.0,1335,4,16,2019-01-03 17:17,2019-01-03 17:49,2019-01-04 8:07,0.0,1.0,Basic,10,"<sql><jpa><spring-data-jpa><spring-data><hql>, Check that a List parameter is null in a Spring data JPA query, I have a Spring Boot application and use Spring Data JPA to query a MySQL database.
I need to get a list of courses filtered with some parameters.
I usually use the syntax param IS NULL or (/*do something with param*/) so that it ignores the parameter if it is null.
With simple datatypes I have no problems but when it comes to a List of objects I don't know how to check for NULL value. How can I check if the ?3 parameter is NULL in the following query ?
@Query(""SELECT DISTINCT c FROM Course c\n"" +
       ""WHERE c.courseDate &lt; CURRENT_TIME\n"" +
       ""AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\n"" +
       ""AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\n"" +
       ""AND ((?3) IS NULL OR (c.category IN ?3)) "")
List&lt;Course&gt; getCoursesFiltered(Long courseId, String filter, List&lt;Category&gt; categories);
Error is :
  could not extract ResultSet; SQL [n/a]; nested exception is org.hibernate.exception.DataException: could not extract ResultSet[SQL: 1241, 21000]
And in the stack trace I can see :
  Caused by: java.sql.SQLException: Operand should contain 1 column(s)
Indeed generated query would be ... AND ((?3, ?4, ?5) IS NULL OR (c.category IN (?3, ?4, ?5))) if my list contains 3 elements. But IS NULL cannot be applied to multiple elements (query works fine if my list contain only one element).
I have tried size(?3) &lt; 1, length(?3) &lt; 1, (?3) IS EMPTY, etc. but still no luck.
","<sal><pa><spring-data-pa><spring-data><he>, check list parapet null spring data pa query, spring boot applied use spring data pa query myself database. need get list course filter parameter. usual use santa parma null (/*do cometh parma*/) ignore parapet null. simple datatyp problem come list object know check null value. check ?3 parapet null follow query ? @query(""select distinct c course c\n"" + ""where c.course &it; current_time\n"" + ""and (?1 null (c.id (3, 4, 5)))\n"" + ""and (?2 null (c.till like ?2 c.rescript like ?2))\n"" + ""and ((?3) null (c.category ?3)) "") list&it;course&it; getcoursesfiltered(long course, string filter, list&it;category&it; categories); error : could extract results; sal [n/a]; nest except org.liberate.exception.dataexception: could extract results[sal: 1241, 21000] stick trace see : cause by: cava.sal.sqlexception: operate contain 1 column(s) index genet query would ... ((?3, ?4, ?5) null (c.category (?3, ?4, ?5))) list contain 3 elements. null cannot apply multiple element (query work fine list contain one element). try size(?3) &it; 1, length(?3) &it; 1, (?3) empty, etc. still luck."
58623291,How to restore database from dump or sql file in docker using volume?,"I am trying to run my database in docker which already have some data into it, But when I run it in docker it gives empty .. So how to restore my database in docker PostgreSQL image.
",<postgresql><docker><docker-compose><docker-volume>,183,0,0,171,1,2,7,58,21776,,0,1,16,2019-10-30 10:27,2019-10-30 11:11,2019-10-30 11:11,0.0,0.0,Basic,10,"<postgresql><docker><docker-compose><docker-volume>, How to restore database from dump or sql file in docker using volume?, I am trying to run my database in docker which already have some data into it, But when I run it in docker it gives empty .. So how to restore my database in docker PostgreSQL image.
","<postgresql><doctor><doctor-compose><doctor-volume>, restore database dump sal file doctor use volume?, try run database doctor already data it, run doctor give empty .. restore database doctor postgresql image."
49210401,select 30 random rows where sum amount = x,"I have a table 
items
id int unsigned auto_increment primary key,
name varchar(255)
price DECIMAL(6,2)
I want to get at least 30 random items from this table where the total sum of price equals 500, what's the best approach to accomplish this?
I have seen this solution which looks have a similar issue MySQL Select 3 random rows where sum of three rows is less than value
And I am wondering if there are any other solutions that are easier to implement and/or more efficient 
",<php><mysql>,477,1,4,534,1,5,15,52,1463,0.0,42,7,16,2018-03-10 14:53,2018-03-11 0:47,2018-03-18 23:20,1.0,8.0,Basic,10,"<php><mysql>, select 30 random rows where sum amount = x, I have a table 
items
id int unsigned auto_increment primary key,
name varchar(255)
price DECIMAL(6,2)
I want to get at least 30 random items from this table where the total sum of price equals 500, what's the best approach to accomplish this?
I have seen this solution which looks have a similar issue MySQL Select 3 random rows where sum of three rows is less than value
And I am wondering if there are any other solutions that are easier to implement and/or more efficient 
","<pp><myself>, select 30 random row sum amount = x, table item id in ensign auto_incr primary key, name varchar(255) price denial(6,2) want get least 30 random item table total sum price equal 500, what' best approach accomplish this? seen slut look similar issue myself select 3 random row sum three row less value wonder slut easier implement and/or effect"
60138692,sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation <<table>>,"I've read over 20 different questions with the same problem - and the suggested answers didn't solve my problem. I'm still getting sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation &lt;&lt;table&gt;&gt;
Environment: EC2, debian 8, postgresql, flask, sqlalchemy
my table in postgresql:
Create table Members(
id BIGSERIAL PRIMARY KEY,
joinDate TIMESTAMPTZ DEFAULT Now(),
password TEXT NOT NULL
);
directly in postgresql: INSERT INTO members (password) VALUES('123')  RETURNING id; works perfect
I've granted my user all possible grants
grant all privileges on database &lt;my_db&gt; to &lt;my_user&gt;
grant all privileges on all table in schema public to &lt;my_user&gt;
grant all privileges on all relations in schema public to &lt;my_user&gt;
I've also created a .htaccess file under var/www/html with Header set Access-Control-Allow-Origin ""*""
my table mapped with sqlalchemy member.py
from datetime import datetime
from sqlalchemy import create_engine, Column, String, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from marshmallow import Schema, fields
from sqlalchemy.schema import UniqueConstraint
import sqlalchemy as sa
Engine = create_engine('postgresql://&lt;my_user&gt;:&lt;my_psw&gt;@&lt;my_db_url&gt;/&lt;my_dn_name&gt;')
Session = sessionmaker(bind=Engine)
Base = declarative_base()
class MemberSchema(Schema):
    id = fields.Number()    
    password = fields.Str()
class Member(Base):
    __tablename__ = 'members'
    id = Column(Integer, primary_key=True)
    password = Column(String)
    def __init__(self, password):
        self.password = password
the code receiving the POST and doing the insert in test.py:
from flask import Flask, render_template, request,jsonify
from flask_cors import CORS, cross_origin
import logging
def register_member(password):
    session = Session()
    newMember = Member(password)
    session.add(newMember)
    session.commit()
    return newMember
app = Flask(__name__)
cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
@app.route(""/register"", methods=[""GET"", ""POST""])
@cross_origin()    
def register():
    try:
        formData = request.form
        register_member(formData['password'])
        return jsonify({""message"":""Regsitrated successfully!""})
    except Exception as ex:
        logging.debug(ex)
if __name__ == ""__main__"":
    app.run(host='0.0.0.0', port=1234, debug=True)
executing this code results in exception with these errors
DEBUG:root:(psycopg2.errors.InsufficientPrivilege) permission denied for relation members
[SQL: INSERT INTO members (password) VALUES (%(password)s) RETURNING members.id]
[parameters: {'password': '4444'}]
and
TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.
Any help would be appreciated
",<python><postgresql><sqlalchemy><permissions>,2887,0,71,1798,4,20,38,39,30027,0.0,8,4,16,2020-02-09 16:19,2020-02-19 17:51,,10.0,,Basic,6,"<python><postgresql><sqlalchemy><permissions>, sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation <<table>>, I've read over 20 different questions with the same problem - and the suggested answers didn't solve my problem. I'm still getting sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation &lt;&lt;table&gt;&gt;
Environment: EC2, debian 8, postgresql, flask, sqlalchemy
my table in postgresql:
Create table Members(
id BIGSERIAL PRIMARY KEY,
joinDate TIMESTAMPTZ DEFAULT Now(),
password TEXT NOT NULL
);
directly in postgresql: INSERT INTO members (password) VALUES('123')  RETURNING id; works perfect
I've granted my user all possible grants
grant all privileges on database &lt;my_db&gt; to &lt;my_user&gt;
grant all privileges on all table in schema public to &lt;my_user&gt;
grant all privileges on all relations in schema public to &lt;my_user&gt;
I've also created a .htaccess file under var/www/html with Header set Access-Control-Allow-Origin ""*""
my table mapped with sqlalchemy member.py
from datetime import datetime
from sqlalchemy import create_engine, Column, String, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from marshmallow import Schema, fields
from sqlalchemy.schema import UniqueConstraint
import sqlalchemy as sa
Engine = create_engine('postgresql://&lt;my_user&gt;:&lt;my_psw&gt;@&lt;my_db_url&gt;/&lt;my_dn_name&gt;')
Session = sessionmaker(bind=Engine)
Base = declarative_base()
class MemberSchema(Schema):
    id = fields.Number()    
    password = fields.Str()
class Member(Base):
    __tablename__ = 'members'
    id = Column(Integer, primary_key=True)
    password = Column(String)
    def __init__(self, password):
        self.password = password
the code receiving the POST and doing the insert in test.py:
from flask import Flask, render_template, request,jsonify
from flask_cors import CORS, cross_origin
import logging
def register_member(password):
    session = Session()
    newMember = Member(password)
    session.add(newMember)
    session.commit()
    return newMember
app = Flask(__name__)
cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
@app.route(""/register"", methods=[""GET"", ""POST""])
@cross_origin()    
def register():
    try:
        formData = request.form
        register_member(formData['password'])
        return jsonify({""message"":""Regsitrated successfully!""})
    except Exception as ex:
        logging.debug(ex)
if __name__ == ""__main__"":
    app.run(host='0.0.0.0', port=1234, debug=True)
executing this code results in exception with these errors
DEBUG:root:(psycopg2.errors.InsufficientPrivilege) permission denied for relation members
[SQL: INSERT INTO members (password) VALUES (%(password)s) RETURNING members.id]
[parameters: {'password': '4444'}]
and
TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.
Any help would be appreciated
","<patron><postgresql><sqlalchemy><permission>, sqlalchemi psycopg2.errors.insufficientprivilege: permits den relate <<table>>, i'v read 20 differ question problem - suggest answer sole problem. i'm still get sqlalchemi psycopg2.errors.insufficientprivilege: permits den relate &it;&it;table&it;&it; environment: end, median 8, postgresql, flask, sqlalchemi table postgresql: great table members( id bigger primary key, joint timestamptz default now(), password text null ); directly postgresql: insert member (password) values('123') return id; work perfect i'v grant user possible grant grant privilege database &it;my_db&it; &it;my_user&it; grant privilege table scheme public &it;my_user&it; grant privilege relate scheme public &it;my_user&it; i'v also great .access file war/www/html header set access-control-allow-origin ""*"" table map sqlalchemi member.i datetim import datetim sqlalchemi import create_engine, column, string, inter, datetim sqlalchemy.ext.declare import declarative_bas sqlalchemy.or import sessionmak marshmallow import scheme, field sqlalchemy.scheme import uniqueconstraint import sqlalchemi sa engine = create_engine('postgresql://&it;my_user&it;:&it;my_psw&it;@&it;my_db_url&it;/&it;my_dn_name&it;') session = sessionmaker(bind=engine) base = declarative_base() class memberschema(scheme): id = fields.number() password = fields.sir() class member(base): __tablename__ = 'members' id = column(inter, primary_key=true) password = column(string) def __init__(self, password): self.password = password code receive post insert test.by: flask import flask, render_template, request,jsonifi flask_cor import corps, cross_origin import log def register_member(password): session = session() newmemb = member(password) session.add(remember) session.commit() return newmemb pp = flask(__name__) cor = corps(pp) pp.confirm['cors_headers'] = 'content-type' @pp.route(""/register"", methods=[""get"", ""post""]) @cross_origin() def register(): try: format = request.form register_member(format['password']) return jsonify({""message"":""register successfully!""}) except except ex: logging.debut(ex) __name__ == ""__main__"": pp.run(host='0.0.0.0', port=1234, debut=true) execute code result except error debut:root:(psycopg2.errors.insufficientprivilege) permits den relate member [sal: insert member (password) value (%(password)s) return members.id] [parameter: {'password': '4444'}] typeerror: view function return valid response. function either return none end without return statement. help would appreci"
51627330,SQL to find upper case words from a column,"I have a description column in my table and its values are:
This is a EXAMPLE
This is a TEST
This is a VALUE
I want to display only EXAMPLE, TEST, and VALUE from the description column.
How do I achieve this?
",<sql><oracle>,209,0,3,275,0,2,14,80,6722,0.0,4,7,16,2018-08-01 7:09,2018-08-01 7:43,,0.0,,Basic,10,"<sql><oracle>, SQL to find upper case words from a column, I have a description column in my table and its values are:
This is a EXAMPLE
This is a TEST
This is a VALUE
I want to display only EXAMPLE, TEST, and VALUE from the description column.
How do I achieve this?
","<sal><oracle>, sal find upper case word column, rescript column table value are: example test value want display example, test, value rescript column. achieve this?"
61859247,Cannot connect with SSMS to SQL Server on Docker,"I have followed the official Microsoft documentation and I have installed SQL Server Docker image 
As result I have a SQL Server image running on Docker at the IP address 172.17.0.2
I also can easily connect to it using sqlcmd with my dummy password
The problem is that I cannot connect to it through SSMS:
Login failed for user 'sa'. (Microsoft SQL Server, Error: 18456)
Of course I read other StackOverflow posts before posting this question and I have tried multiple logins: 
localhost,1433
localhost:1433
172.17.0.2,1433
etc...
How can I connect if localhost doesn't work as well as the IP address of the docker image? 
",<sql-server><docker><containers><ssms><sqlcmd>,624,5,1,10484,15,78,123,45,21064,0.0,705,9,16,2020-05-17 21:45,2020-05-20 5:49,,3.0,,Intermediate,20,"<sql-server><docker><containers><ssms><sqlcmd>, Cannot connect with SSMS to SQL Server on Docker, I have followed the official Microsoft documentation and I have installed SQL Server Docker image 
As result I have a SQL Server image running on Docker at the IP address 172.17.0.2
I also can easily connect to it using sqlcmd with my dummy password
The problem is that I cannot connect to it through SSMS:
Login failed for user 'sa'. (Microsoft SQL Server, Error: 18456)
Of course I read other StackOverflow posts before posting this question and I have tried multiple logins: 
localhost,1433
localhost:1433
172.17.0.2,1433
etc...
How can I connect if localhost doesn't work as well as the IP address of the docker image? 
","<sal-server><doctor><container><sums><sqlcmd>, cannot connect sum sal server doctor, follow office microsoft document instal sal server doctor image result sal server image run doctor in address 172.17.0.2 also vasili connect use sqlcmd dummy password problem cannot connect sums: login fail user 'sa'. (microsoft sal server, error: 18456) course read stackoverflow post post question try multiple loins: localhost,1433 localhost:1433 172.17.0.2,1433 etc... connect localhost work well in address doctor image?"
50999358,Xampp MySQL not starting - MYSQL not starting on XAMPP 3.2.1 version,"i installed xampp version 3.2.1 on my laptop and mysql was working fine on it before but suddenly mysql stopped working while apache and others were working.
when i click on start mysql it displays this error
i use windows 10
8:52:32 AM  [mysql]     Status change detected: running
8:52:40 AM  [mysql]     Status change detected: stopped
8:52:40 AM  [mysql]     Error: MySQL shutdown unexpectedly.
8:52:40 AM  [mysql]     This may be due to a blocked port, missing dependencies, 
8:52:40 AM  [mysql]     improper privileges, a crash, or a shutdown by another method.
8:52:40 AM  [mysql]     Press the Logs button to view error logs and check
8:52:40 AM  [mysql]     the Windows Event Viewer for more clues
8:52:40 AM  [mysql]     If you need more help, copy and post this
8:52:40 AM  [mysql]     entire log window on the forums
then i checked the log file and this is what i found there
`
2018-06-23 08:52:32 2354 InnoDB: Warning: Using innodb_additional_mem_pool_size is DEPRECATED. This option may be removed in future releases, together with the option innodb_use_sys_malloc and with the InnoDB's internal memory allocator.
180623  8:52:32 [Note] InnoDB: Using mutexes to ref count buffer pool pages
180623  8:52:32 [Note] InnoDB: The InnoDB memory heap is disabled
180623  8:52:32 [Note] InnoDB: Mutexes and rw_locks use Windows interlocked functions
180623  8:52:32 [Note] InnoDB: Memory barrier is not used
180623  8:52:32 [Note] InnoDB: Compressed tables use zlib 1.2.3
180623  8:52:32 [Note] InnoDB: Not using CPU crc32 instructions
180623  8:52:32 [Note] InnoDB: Initializing buffer pool, size = 16.0M
180623  8:52:32 [Note] InnoDB: Completed initialization of buffer pool
180623  8:52:32 [Note] InnoDB: Highest supported file format is Barracuda.
180623  8:52:32 [Note] InnoDB: 128 rollback segment(s) are active.
180623  8:52:32 [Note] InnoDB: Waiting for purge to start
180623  8:52:33 [Note] InnoDB:  Percona XtraDB (http://www.percona.com) 5.6.22-72.0 started; log sequence number 1600924
180623  8:52:33 [Note] Plugin 'FEEDBACK' is disabled.
180623  8:52:33 [Note] Server socket created on IP: '::'.
180623  8:52:33 [Warning] 'db' entry 'size of datafile is: 1907       should be: 1760
180409 15:17:15 [error] mysql.db: got error: 0 when reading datafile at record: 3
180409 15:17:15 [error] got an error from unknown thread, ha _myisam.cc:952
180409 15:17:15 [Warning] Recove@80409 15:17:15 [Warning] Checking table:   '.\mysql\db'
180409 15:17:15 [ERROR] mysql.db: 1 client is using or hasn't closed the table properly
180409 15:17:15 [ERROR] mysql.db:' had database in mixed case that has been forced to lowercase because lower_case_table_names is set. It will not be possible to remove this privilege using REVOKE.
180623  8:52:33 [ERROR] Missing system table mysql.roles_mapping; please run mysql_upgrade to create it
180623  8:52:33 [ERROR] Incorrect definition of table mysql.event: expected column 'sql_mode' at position 14 to have type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','IGNORE_BAD_TABLE_OPTIONS','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH'), found type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_A
180623  8:52:33 [ERROR] Event Scheduler: An error occurred when initializing system tables. Disabling the Event Scheduler.
180623  8:52:33 [Warning] Failed to load slave replication state from table mysql.gtid_slave_pos: 1146: Table 'mysql.gtid_slave_pos' doesn't exist
180623  8:52:33 [Warning] Neither --relay-log nor --relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!! Please use '--log-basename=#' or '--relay-log=mysql-relay-bin' to avoid this problem.
180623  8:52:33 [Note] Started replication for '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
'
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL thread initialized, starting replication in log 'FIRST' at position 0, relay log '.\mysql-relay-bin-180401@002018@003a51@003a15@0020@005bnote@005d@0020c@003a@005cxampp@005cmysql@005cbin@005cmysqld@002eexe@003a@0020ready@0020for@0020connections@002e@000d.000095' position: 4
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Fatal error: Invalid (empty) username when attempting to connect to the master server. Connection attempt terminated. Internal MariaDB error code: 1593
180623  8:52:33 [ERROR] Failed to open the relay log '.\mysql-relay-bin-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.000036' (relay_log_pos 4)
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread killed while connecting to master
180623  8:52:33 [ERROR] Could not find target log during relay log initialization
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread exiting, read up to log 'FIRST', position 4
180623  8:52:33 [ERROR] Initialized Master_info from 'master-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.info' failed
180623  8:52:33 [Warning] Reading of some Master_info entries failed
180623  8:52:33 [ERROR] Failed to initialize multi master structures
180623  8:52:33 [ERROR] Aborting
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Error reading relay log event: slave SQL thread was killed
180623  8:52:33 [Note] InnoDB: FTS optimize thread exiting.
180623  8:52:33 [Note] InnoDB: Starting shutdown...
180623  8:52:35 [Note] InnoDB: Shutdown completed; log sequence number 1600934
180623  8:52:35 [Note] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete`
",<mysql><xampp>,7304,1,67,161,1,1,3,41,11239,0.0,0,5,16,2018-06-23 8:23,2020-04-22 9:07,,669.0,,Basic,14,"<mysql><xampp>, Xampp MySQL not starting - MYSQL not starting on XAMPP 3.2.1 version, i installed xampp version 3.2.1 on my laptop and mysql was working fine on it before but suddenly mysql stopped working while apache and others were working.
when i click on start mysql it displays this error
i use windows 10
8:52:32 AM  [mysql]     Status change detected: running
8:52:40 AM  [mysql]     Status change detected: stopped
8:52:40 AM  [mysql]     Error: MySQL shutdown unexpectedly.
8:52:40 AM  [mysql]     This may be due to a blocked port, missing dependencies, 
8:52:40 AM  [mysql]     improper privileges, a crash, or a shutdown by another method.
8:52:40 AM  [mysql]     Press the Logs button to view error logs and check
8:52:40 AM  [mysql]     the Windows Event Viewer for more clues
8:52:40 AM  [mysql]     If you need more help, copy and post this
8:52:40 AM  [mysql]     entire log window on the forums
then i checked the log file and this is what i found there
`
2018-06-23 08:52:32 2354 InnoDB: Warning: Using innodb_additional_mem_pool_size is DEPRECATED. This option may be removed in future releases, together with the option innodb_use_sys_malloc and with the InnoDB's internal memory allocator.
180623  8:52:32 [Note] InnoDB: Using mutexes to ref count buffer pool pages
180623  8:52:32 [Note] InnoDB: The InnoDB memory heap is disabled
180623  8:52:32 [Note] InnoDB: Mutexes and rw_locks use Windows interlocked functions
180623  8:52:32 [Note] InnoDB: Memory barrier is not used
180623  8:52:32 [Note] InnoDB: Compressed tables use zlib 1.2.3
180623  8:52:32 [Note] InnoDB: Not using CPU crc32 instructions
180623  8:52:32 [Note] InnoDB: Initializing buffer pool, size = 16.0M
180623  8:52:32 [Note] InnoDB: Completed initialization of buffer pool
180623  8:52:32 [Note] InnoDB: Highest supported file format is Barracuda.
180623  8:52:32 [Note] InnoDB: 128 rollback segment(s) are active.
180623  8:52:32 [Note] InnoDB: Waiting for purge to start
180623  8:52:33 [Note] InnoDB:  Percona XtraDB (http://www.percona.com) 5.6.22-72.0 started; log sequence number 1600924
180623  8:52:33 [Note] Plugin 'FEEDBACK' is disabled.
180623  8:52:33 [Note] Server socket created on IP: '::'.
180623  8:52:33 [Warning] 'db' entry 'size of datafile is: 1907       should be: 1760
180409 15:17:15 [error] mysql.db: got error: 0 when reading datafile at record: 3
180409 15:17:15 [error] got an error from unknown thread, ha _myisam.cc:952
180409 15:17:15 [Warning] Recove@80409 15:17:15 [Warning] Checking table:   '.\mysql\db'
180409 15:17:15 [ERROR] mysql.db: 1 client is using or hasn't closed the table properly
180409 15:17:15 [ERROR] mysql.db:' had database in mixed case that has been forced to lowercase because lower_case_table_names is set. It will not be possible to remove this privilege using REVOKE.
180623  8:52:33 [ERROR] Missing system table mysql.roles_mapping; please run mysql_upgrade to create it
180623  8:52:33 [ERROR] Incorrect definition of table mysql.event: expected column 'sql_mode' at position 14 to have type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','IGNORE_BAD_TABLE_OPTIONS','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH'), found type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_A
180623  8:52:33 [ERROR] Event Scheduler: An error occurred when initializing system tables. Disabling the Event Scheduler.
180623  8:52:33 [Warning] Failed to load slave replication state from table mysql.gtid_slave_pos: 1146: Table 'mysql.gtid_slave_pos' doesn't exist
180623  8:52:33 [Warning] Neither --relay-log nor --relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!! Please use '--log-basename=#' or '--relay-log=mysql-relay-bin' to avoid this problem.
180623  8:52:33 [Note] Started replication for '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
'
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL thread initialized, starting replication in log 'FIRST' at position 0, relay log '.\mysql-relay-bin-180401@002018@003a51@003a15@0020@005bnote@005d@0020c@003a@005cxampp@005cmysql@005cbin@005cmysqld@002eexe@003a@0020ready@0020for@0020connections@002e@000d.000095' position: 4
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Fatal error: Invalid (empty) username when attempting to connect to the master server. Connection attempt terminated. Internal MariaDB error code: 1593
180623  8:52:33 [ERROR] Failed to open the relay log '.\mysql-relay-bin-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.000036' (relay_log_pos 4)
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread killed while connecting to master
180623  8:52:33 [ERROR] Could not find target log during relay log initialization
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread exiting, read up to log 'FIRST', position 4
180623  8:52:33 [ERROR] Initialized Master_info from 'master-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.info' failed
180623  8:52:33 [Warning] Reading of some Master_info entries failed
180623  8:52:33 [ERROR] Failed to initialize multi master structures
180623  8:52:33 [ERROR] Aborting
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Error reading relay log event: slave SQL thread was killed
180623  8:52:33 [Note] InnoDB: FTS optimize thread exiting.
180623  8:52:33 [Note] InnoDB: Starting shutdown...
180623  8:52:35 [Note] InnoDB: Shutdown completed; log sequence number 1600934
180623  8:52:35 [Note] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete`
","<myself><camp>, camp myself start - myself start camp 3.2.1 version, instal camp version 3.2.1 lawton myself work fine suddenly myself stop work apace other working. click start myself display error use window 10 8:52:32 [myself] state change detected: run 8:52:40 [myself] state change detected: stop 8:52:40 [myself] error: myself shutdown unexpectedly. 8:52:40 [myself] may due block port, miss dependencies, 8:52:40 [myself] improve privileges, crash, shutdown not method. 8:52:40 [myself] press log button view error log check 8:52:40 [myself] window event viewer clue 8:52:40 [myself] need help, copy post 8:52:40 [myself] enter log window forum check log file found ` 2018-06-23 08:52:32 2354 innodb: warning: use innodb_additional_mem_pool_s deprecated. option may remove future release, together option innodb_use_sys_malloc innodb' inter memory allocation. 180623 8:52:32 [note] innodb: use mute red count suffer pool page 180623 8:52:32 [note] innodb: innodb memory heap distal 180623 8:52:32 [note] innodb: mute rw_lock use window interlock function 180623 8:52:32 [note] innodb: memory barrier use 180623 8:52:32 [note] innodb: compress table use limb 1.2.3 180623 8:52:32 [note] innodb: use cup crc32 instruct 180623 8:52:32 [note] innodb: into suffer pool, size = 16.am 180623 8:52:32 [note] innodb: complete into suffer pool 180623 8:52:32 [note] innodb: highest support file format barracuda. 180623 8:52:32 [note] innodb: 128 rollback segment(s) active. 180623 8:52:32 [note] innodb: wait pure start 180623 8:52:33 [note] innodb: person trade (http://www.person.com) 5.6.22-72.0 started; log sequence number 1600924 180623 8:52:33 [note] plain 'feedback' disabled. 180623 8:52:33 [note] server socket great in: '::'. 180623 8:52:33 [warning] 'do' entry 'size detail is: 1907 be: 1760 180409 15:17:15 [error] myself.do: got error: 0 read detail record: 3 180409 15:17:15 [error] got error unknown thread, ha _myisam.c:952 180409 15:17:15 [warning] remove@80409 15:17:15 [warning] check table: '.\myself\do' 180409 15:17:15 [error] myself.do: 1 client use close table properly 180409 15:17:15 [error] myself.do:' database mix case for lowers lower_case_table_nam set. possible remove privilege use revoked. 180623 8:52:33 [error] miss system table myself.roles_mapping; pleas run mysql_upgrad great 180623 8:52:33 [error] incorrect definite table myself.event: expect column 'sql_mode' post 14 type set('real_as_float','pipes_as_concat','ansi_quotes','ignore_space','ignore_bad_table_options','only_full_group_by','no_unsigned_subtraction','no_dir_in_create','postgresql','oracle','mssql','by','made','no_key_options','no_table_options','no_field_options','mysql323','mysql40','anti','no_auto_value_on_zero','no_backslash_escapes','strict_trans_tables','strict_all_tables','no_zero_in_date','no_zero_date','invalid_dates','error_for_division_by_zero','traditional','no_auto_create_user','high_not_precedence','no_engine_substitution','pad_char_to_full_length'), found type set('real_as_float','pipes_as_concat','ansi_quotes','ignore_space','not_used','only_full_group_by','no_unsigned_subtraction','no_dir_in_create','postgresql','oracle','mssql','by','made','no_key_options','no_table_options','no_field_options','mysql323','mysql40','anti','no_auto_value_on_zero','no_backslash_escapes','strict_trans_tables','strict 180623 8:52:33 [error] event schedule: error occur into system tables. distal event schedule. 180623 8:52:33 [warning] fail load slave relic state table myself.gtid_slave_pos: 1146: table 'myself.gtid_slave_pos' exist 180623 8:52:33 [warning] neither --relay-log --relay-log-index used; relic may break myself server act slave hostnam changed!! pleas use '--log-basename=#' '--relay-log=myself-relay-bin' avoid problem. 180623 8:52:33 [note] start relic '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ' 180623 8:52:33 [error] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave i/o: unable load relic grid slave state myself.gtid_slave_pos: table 'myself.gtid_slave_pos' exist, inter maria error code: 1146 180623 8:52:33 [note] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave sal thread initialized, start relic log 'first' post 0, relay log '.\myself-relay-bin-180401@002018@003a51@003a15@0020@005bnote@005d@0020c@003a@005cxampp@005cmysql@005cbin@005cmysqld@002eexe@003a@0020ready@0020for@0020connections@002e@000d.000095' position: 4 180623 8:52:33 [error] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave i/o: fatal error: invalid (empty) usernam attempt connect master server. connect attempt terminated. inter maria error code: 1593 180623 8:52:33 [error] fail open relay log '.\myself-relay-bin-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.000036' (relay_log_po 4) 180623 8:52:33 [error] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave sal: unable load relic grid slave state myself.gtid_slave_pos: table 'myself.gtid_slave_pos' exist, inter maria error code: 1146 180623 8:52:33 [note] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave i/o thread kill connect master 180623 8:52:33 [error] could find target log relay log into 180623 8:52:33 [note] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': slave i/o thread existing, read log 'first', post 4 180623 8:52:33 [error] into master_info 'master-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.into' fail 180623 8:52:33 [warning] read master_info entry fail 180623 8:52:33 [error] fail into multi master structure 180623 8:52:33 [error] abort 180623 8:52:33 [note] master '180401 18:51:15 [note] c:\camp\myself\bin\myself.eye: ready connections. ': error read relay log event: slave sal thread kill 180623 8:52:33 [note] innodb: ft optic thread existing. 180623 8:52:33 [note] innodb: start shutdown... 180623 8:52:35 [note] innodb: shutdown completed; log sequence number 1600934 180623 8:52:35 [note] c:\camp\myself\bin\myself.eye: shutdown complete`"
51089952,Conversion failed when converting the ****** value '******' to data type ******,"I'm getting this error from SQL Server on SSMS 17 while running a giant query:
  Conversion failed when converting the ****** value '******' to data type ******
I have never seen this with ****'s before, and google searching seems to come up with nothing. Is there a known cause for why SQL Server would provide this message with asterisks?
",<sql><sql-server>,341,0,0,32923,40,135,186,72,13685,0.0,260,6,16,2018-06-28 19:19,2018-10-12 20:44,,106.0,,Basic,9,"<sql><sql-server>, Conversion failed when converting the ****** value '******' to data type ******, I'm getting this error from SQL Server on SSMS 17 while running a giant query:
  Conversion failed when converting the ****** value '******' to data type ******
I have never seen this with ****'s before, and google searching seems to come up with nothing. Is there a known cause for why SQL Server would provide this message with asterisks?
","<sal><sal-server>, covers fail convert ****** value '******' data type ******, i'm get error sal server sum 17 run giant query: covers fail convert ****** value '******' data type ****** never seen ****' before, good search seem come nothing. known cause sal server would proved message asterisks?"
57955765,Upgrading to MYSQL 8 - MAMP,"I have installed MAMP on my MAC machine and I am trying to upgrade it to use MySQL 8. However, I am not having any luck. I have tried following script but database migration fails. 
Also, sequelPro fails to connect to DB but, phpmyadmin has no issue connecting.
#!/bin/sh
curl -OL https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.17-macos10.14-x86_64.tar.gz
tar xfvz mysql-8.0.*
echo ""stopping mamp""
sudo /Applications/MAMP/bin/stop.sh
sudo killall httpd mysqld
echo ""creating backup""
sudo rsync -arv --progress /Applications/MAMP ~/Desktop/MAMP-Backup
echo ""copy bin""
sudo rsync -arv --progress mysql-8.0.*/bin/* /Applications/MAMP/Library/bin/ --exclude=mysqld_multi --exclude=mysqld_safe 
echo ""copy share""
sudo rsync -arv --progress mysql-8.0.*/share/* /Applications/MAMP/Library/share/
echo ""fixing access (workaround)""
sudo chmod -R o+rw  /Applications/MAMP/db/mysql/
sudo chmod -R o+rw  /Applications/MAMP/tmp/mysql/
#sudo chmod -R o+rw  ""/Library/Application Support/appsolute/MAMP PRO/db/mysql/""
echo ""starting mamp""
sudo /Applications/MAMP/bin/start.sh
echo ""migrate to new version""
/Applications/MAMP/Library/bin/mysql -u root --password=root -h 127.0.0.1
",<mysql><macos><mamp><mysql-8.0>,1174,1,28,503,1,3,14,62,12091,0.0,9,0,16,2019-09-16 11:21,,,,,Basic,14,"<mysql><macos><mamp><mysql-8.0>, Upgrading to MYSQL 8 - MAMP, I have installed MAMP on my MAC machine and I am trying to upgrade it to use MySQL 8. However, I am not having any luck. I have tried following script but database migration fails. 
Also, sequelPro fails to connect to DB but, phpmyadmin has no issue connecting.
#!/bin/sh
curl -OL https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.17-macos10.14-x86_64.tar.gz
tar xfvz mysql-8.0.*
echo ""stopping mamp""
sudo /Applications/MAMP/bin/stop.sh
sudo killall httpd mysqld
echo ""creating backup""
sudo rsync -arv --progress /Applications/MAMP ~/Desktop/MAMP-Backup
echo ""copy bin""
sudo rsync -arv --progress mysql-8.0.*/bin/* /Applications/MAMP/Library/bin/ --exclude=mysqld_multi --exclude=mysqld_safe 
echo ""copy share""
sudo rsync -arv --progress mysql-8.0.*/share/* /Applications/MAMP/Library/share/
echo ""fixing access (workaround)""
sudo chmod -R o+rw  /Applications/MAMP/db/mysql/
sudo chmod -R o+rw  /Applications/MAMP/tmp/mysql/
#sudo chmod -R o+rw  ""/Library/Application Support/appsolute/MAMP PRO/db/mysql/""
echo ""starting mamp""
sudo /Applications/MAMP/bin/start.sh
echo ""migrate to new version""
/Applications/MAMP/Library/bin/mysql -u root --password=root -h 127.0.0.1
","<myself><faces><camp><myself-8.0>, upgrade myself 8 - camp, instal camp mac machine try upgrade use myself 8. however, luck. try follow script database migrate fails. also, sequelpro fail connect do but, phpmyadmin issue connecting. #!/bin/s curl -of http://de.myself.com/get/download/myself-8.0/myself-8.0.17-macos10.14-x86_64.tar.go tar xvi myself-8.0.* echo ""stop camp"" so /applications/camp/bin/stop.s so killed http myself echo ""great back"" so rsync -are --progress /applications/camp ~/desktop/camp-back echo ""copy bin"" so rsync -are --progress myself-8.0.*/bin/* /applications/camp/library/bin/ --exclude=mysqld_multi --exclude=mysqld_saf echo ""copy share"" so rsync -are --progress myself-8.0.*/share/* /applications/camp/library/share/ echo ""fix access (workaround)"" so child -r o+re /applications/camp/do/myself/ so child -r o+re /applications/camp/tm/myself/ #so child -r o+re ""/library/apply support/absolute/camp pro/do/myself/"" echo ""start camp"" so /applications/camp/bin/start.s echo ""migrate new version"" /applications/camp/library/bin/myself -u root --password=root -h 127.0.0.1"
55502546,Find mean of pyspark array<double>,"
In pyspark, I have a variable length array of doubles for which I would like to find the mean. However, the average function requires a single numeric type. 
Is there a way to find the average of an array without exploding the array out? I have several different arrays and I'd like to be able to do something like the following:
df.select(col(""Segment.Points.trajectory_points.longitude""))
DataFrame[longitude: array]
df.select(avg(col(""Segment.Points.trajectory_points.longitude""))).show()
org.apache.spark.sql.AnalysisException: cannot resolve
'avg(Segment.Points.trajectory_points.longitude)' due to data type
mismatch: function average requires numeric types, not
ArrayType(DoubleType,true);;
If I have 3 unique records with the following arrays, I'd like the mean of these values as the output. This would be 3 mean longitude values.  
Input:  
[Row(longitude=[-80.9, -82.9]),
 Row(longitude=[-82.92, -82.93, -82.94, -82.96, -82.92, -82.92]),
 Row(longitude=[-82.93, -82.93])]
Output:  
-81.9,
-82.931,
-82.93
I am using spark version 2.1.3.
Explode Solution:
So I've got this working by exploding, but I was hoping to avoid this step. Here's what I did
from pyspark.sql.functions import col
import pyspark.sql.functions as F
longitude_exp = df.select(
    col(""ID""), 
    F.posexplode(""Segment.Points.trajectory_points.longitude"").alias(""pos"", ""longitude"")
)
longitude_reduced = long_exp.groupBy(""ID"").agg(avg(""longitude""))
This successfully took the mean. However, since I'll be doing this for several columns, I'll have to explode the same DF several different times. I'll keep working through it to find a cleaner way to do this.
",<apache-spark><pyspark><apache-spark-sql>,1641,0,21,329,0,2,11,68,9340,0.0,35,2,16,2019-04-03 19:05,2019-04-03 20:40,2019-04-03 20:40,0.0,0.0,Basic,1,"<apache-spark><pyspark><apache-spark-sql>, Find mean of pyspark array<double>, 
In pyspark, I have a variable length array of doubles for which I would like to find the mean. However, the average function requires a single numeric type. 
Is there a way to find the average of an array without exploding the array out? I have several different arrays and I'd like to be able to do something like the following:
df.select(col(""Segment.Points.trajectory_points.longitude""))
DataFrame[longitude: array]
df.select(avg(col(""Segment.Points.trajectory_points.longitude""))).show()
org.apache.spark.sql.AnalysisException: cannot resolve
'avg(Segment.Points.trajectory_points.longitude)' due to data type
mismatch: function average requires numeric types, not
ArrayType(DoubleType,true);;
If I have 3 unique records with the following arrays, I'd like the mean of these values as the output. This would be 3 mean longitude values.  
Input:  
[Row(longitude=[-80.9, -82.9]),
 Row(longitude=[-82.92, -82.93, -82.94, -82.96, -82.92, -82.92]),
 Row(longitude=[-82.93, -82.93])]
Output:  
-81.9,
-82.931,
-82.93
I am using spark version 2.1.3.
Explode Solution:
So I've got this working by exploding, but I was hoping to avoid this step. Here's what I did
from pyspark.sql.functions import col
import pyspark.sql.functions as F
longitude_exp = df.select(
    col(""ID""), 
    F.posexplode(""Segment.Points.trajectory_points.longitude"").alias(""pos"", ""longitude"")
)
longitude_reduced = long_exp.groupBy(""ID"").agg(avg(""longitude""))
This successfully took the mean. However, since I'll be doing this for several columns, I'll have to explode the same DF several different times. I'll keep working through it to find a cleaner way to do this.
","<apache-spark><spark><apache-spark-sal>, find mean spark array<double>, spark, variable length array doubt would like find mean. however, average function require single number type. way find average array without explode array out? never differ array i'd like all cometh like following: of.select(col(""segment.points.trajectory_points.longitude"")) dataframe[longitude: array] of.select(ave(col(""segment.points.trajectory_points.longitude""))).show() org.apache.spark.sal.analysisexception: cannot resolve 'ave(segment.points.trajectory_points.longitude)' due data type dispatch: function average require number types, arraytype(doubletype,true);; 3 unique record follow array, i'd like mean value output. would 3 mean longitude values. input: [row(longitude=[-80.9, -82.9]), row(longitude=[-82.92, -82.93, -82.94, -82.96, -82.92, -82.92]), row(longitude=[-82.93, -82.93])] output: -81.9, -82.931, -82.93 use spark version 2.1.3. explode solution: i'v got work exploding, hope avoid step. here' spark.sal.fact import col import spark.sal.fact f longitude_exp = of.select( col(""id""), f.posexplode(""segment.points.trajectory_points.longitude"").alias(""pus"", ""longitude"") ) longitude_reduc = long_exp.group(""id"").age(ave(""longitude"")) success took mean. however, since i'll never columns, i'll explode of never differ times. i'll keep work find cleaner way this."
50676867,Executing Named Queries in Athena,"We want to execute a parameterized query in Athena using the javascript sdk by aws.
Seems Athena's named query may be the way to do, but the documentation seems very cryptic to understand how to go about doing this. 
It would be great if someone can help us do the following
What is the recommended way to avoid sql injection in athena?
Create a parameterized query like SELECT c FROM Country c WHERE c.name = :name 
Pass the name parameter's value
Execute this query
",<amazon-web-services><sql-injection><amazon-athena><named-query>,468,1,2,13452,4,61,79,74,7538,0.0,164,1,16,2018-06-04 9:00,2019-02-03 21:02,,244.0,,Basic,3,"<amazon-web-services><sql-injection><amazon-athena><named-query>, Executing Named Queries in Athena, We want to execute a parameterized query in Athena using the javascript sdk by aws.
Seems Athena's named query may be the way to do, but the documentation seems very cryptic to understand how to go about doing this. 
It would be great if someone can help us do the following
What is the recommended way to avoid sql injection in athena?
Create a parameterized query like SELECT c FROM Country c WHERE c.name = :name 
Pass the name parameter's value
Execute this query
","<amazon-web-services><sal-injection><amazon-athens><named-query>, execute name query athens, want execute parameter query athens use javascript sd was. seem athens' name query may way do, document seem cystic understand go this. would great someone help us follow recommend way avoid sal inject athens? great parameter query like select c country c c.name = :name pass name parameter' value execute query"
58351958,psycopg2.ProgrammingError: column cons.consrc does not exist,"I am working on a web app where I needed to update the Postgres(12) database. I am using flask-migrate for the purpose.
I have tried to fix it, but it seems it is a new issue and hence I am not getting much help. I have tried to modify the database from my terminal, that works obviously. This is what I usually type to update my db:
flask db migrate -m ""..""
This is the error message I am receiving at the terminal:
psycopg2.ProgrammingError: column cons.consrc does not exist
LINE 4:                 cons.consrc as src
                        ^
HINT:  Perhaps you meant to reference the column ""cons.conkey"" or the column ""cons.conbin"".
",<python><database><postgresql><flask>,639,0,5,165,0,1,7,40,8014,0.0,23,1,16,2019-10-12 7:36,2019-10-21 22:48,2019-10-21 22:48,9.0,9.0,Basic,9,"<python><database><postgresql><flask>, psycopg2.ProgrammingError: column cons.consrc does not exist, I am working on a web app where I needed to update the Postgres(12) database. I am using flask-migrate for the purpose.
I have tried to fix it, but it seems it is a new issue and hence I am not getting much help. I have tried to modify the database from my terminal, that works obviously. This is what I usually type to update my db:
flask db migrate -m ""..""
This is the error message I am receiving at the terminal:
psycopg2.ProgrammingError: column cons.consrc does not exist
LINE 4:                 cons.consrc as src
                        ^
HINT:  Perhaps you meant to reference the column ""cons.conkey"" or the column ""cons.conbin"".
","<patron><database><postgresql><flask>, psycopg2.programmingerror: column sons.contra exist, work web pp need update postures(12) database. use flask-might purpose. try fix it, seem new issue hence get much help. try modify database terminal, work obviously. usual type update do: flask do migrate -m "".."" error message receive terminal: psycopg2.programmingerror: column sons.contra exist line 4: sons.contra sac ^ hint: perhaps meant refer column ""sons.convey"" column ""sons.corbin""."
59584212,How to grant Refresh permissions to the materialized view to user in POSTGRESQL?,"I am executing sql file on linux by running script.
I can see my queries are getting executed fine but I have following query to refresh view in my testData.sql file that is giving me the error
refresh MATERIALIZED VIEW view_test
Error
psql:/home/test/sql/testData.sql:111: ERROR:  must be owner of relation view_test
I have applied following permissions
grant select,update,delete,insert on view_test to &quot;user123&quot;;
How to grant refresh permissions to the View in POSTGRESQL?
",<postgresql><view><refresh>,486,0,3,565,3,9,21,35,14884,0.0,26,3,16,2020-01-03 19:23,2021-11-17 10:13,,684.0,,Basic,9,"<postgresql><view><refresh>, How to grant Refresh permissions to the materialized view to user in POSTGRESQL?, I am executing sql file on linux by running script.
I can see my queries are getting executed fine but I have following query to refresh view in my testData.sql file that is giving me the error
refresh MATERIALIZED VIEW view_test
Error
psql:/home/test/sql/testData.sql:111: ERROR:  must be owner of relation view_test
I have applied following permissions
grant select,update,delete,insert on view_test to &quot;user123&quot;;
How to grant refresh permissions to the View in POSTGRESQL?
","<postgresql><view><refresh>, grant refresh permits mater view user postgresql?, execute sal file line run script. see query get execute fine follow query refresh view testdata.sal file give error refresh mater view view_test error pool:/home/test/sal/testdata.sal:111: error: must owner relate view_test apply follow permits grant select,update,delete,insert view_test &quit;user123&quit;; grant refresh permits view postgresql?"
52225503,Generate C# class from SQL Server table without Store Procedure,"Is there any way to generate a class for table in SQL Server without adding table in project with ADO.net Entity Framework?
public class Approval
{
        public long Id { get; set; }
        public long? FormId { get; set; }
        public long? DesignationId { get; set; }
        public DateTime? CreatedOn { get; set; }
}
",<c#><sql><asp.net><sql-server><oop>,327,1,7,376,1,3,21,70,43628,0.0,63,3,16,2018-09-07 15:19,2018-10-24 17:08,2019-07-09 15:58,47.0,305.0,Basic,3,"<c#><sql><asp.net><sql-server><oop>, Generate C# class from SQL Server table without Store Procedure, Is there any way to generate a class for table in SQL Server without adding table in project with ADO.net Entity Framework?
public class Approval
{
        public long Id { get; set; }
        public long? FormId { get; set; }
        public long? DesignationId { get; set; }
        public DateTime? CreatedOn { get; set; }
}
","<c#><sal><asp.net><sal-server><top>, genet c# class sal server table without store procedure, way genet class table sal server without ad table project ado.net entity framework? public class approve { public long id { get; set; } public long? formed { get; set; } public long? designation { get; set; } public daytime? created { get; set; } }"
57994758,Why Parquet over some RDBMS like Postgres,"I'm working to build a data architecture for my company. A simple ETL with internal and external data with the aim to build static dashboard and other to search trend.
I try to think about every step of the ETL process one by one and now I'm questioning about the Load part.
I plan to use Spark (LocalExcecutor on dev and a service on Azure for production) so I started to think about using Parquet into a Blob service. I know all the advantage of Parquet over CSV or other storage format and I really love this piece of technology. Most of the articles I read about Spark finish with a df.write.parquet(...).
But I cannot figure it out why can I just start a Postgres and save everything here. I understand that we are not producing 100Go per day of data but I want to build something future proof in a fast growing company that gonna produce exponentially data by the business and by the logs and metrics we start recording more and more.
Any pros/cons by more experienced dev ?
EDIT : What also make me questioning this is this tweet : https://twitter.com/markmadsen/status/1044360179213651968
",<postgresql><apache-spark><parquet>,1097,2,1,2620,6,38,71,78,9357,0.0,682,3,16,2019-09-18 14:09,2019-09-19 7:55,2019-09-19 7:55,1.0,1.0,Intermediate,23,"<postgresql><apache-spark><parquet>, Why Parquet over some RDBMS like Postgres, I'm working to build a data architecture for my company. A simple ETL with internal and external data with the aim to build static dashboard and other to search trend.
I try to think about every step of the ETL process one by one and now I'm questioning about the Load part.
I plan to use Spark (LocalExcecutor on dev and a service on Azure for production) so I started to think about using Parquet into a Blob service. I know all the advantage of Parquet over CSV or other storage format and I really love this piece of technology. Most of the articles I read about Spark finish with a df.write.parquet(...).
But I cannot figure it out why can I just start a Postgres and save everything here. I understand that we are not producing 100Go per day of data but I want to build something future proof in a fast growing company that gonna produce exponentially data by the business and by the logs and metrics we start recording more and more.
Any pros/cons by more experienced dev ?
EDIT : What also make me questioning this is this tweet : https://twitter.com/markmadsen/status/1044360179213651968
","<postgresql><apache-spark><parquet>, parquet room like postures, i'm work build data architecture company. simple etc inter externa data aim build static dashboard search trend. try think every step etc process one one i'm question load part. plan use spark (localexcecutor de service azur production) start think use parquet blow service. know advantage parquet is storage format really love piece technology. article read spark finish of.write.parquet(...). cannot figure start poster save every here. understand produce 100go per day data want build cometh future proof fast grow company donna produce exponenti data busy log merit start record more. pro/con experience de ? edit : also make question sweet : http://twitter.com/markmadsen/status/1044360179213651968"
52360260,SQL Server The column cannot be processed because more than 1 code page (65001 and 1252) are specified for it,"I did a select * on a table. I exported the results and tried to import it and write to another table (I have to do this via SSIS, I cannot do this via SQL Server (i.e. select into) for security permission purposes, but I can do it this way).
How can I fix this error? What exactly does it mean? I tried searching StackOverflow but my search was not very helpful.
I tried to use an OLEDB source step, but my server instance does not appear for some reason, so now I'm trying to export/import out of SSMS.
",<sql-server><t-sql><ssis>,505,0,2,3043,8,48,95,67,46891,0.0,904,3,16,2018-09-17 3:06,2018-09-18 7:16,2018-09-18 7:16,1.0,1.0,Intermediate,18,"<sql-server><t-sql><ssis>, SQL Server The column cannot be processed because more than 1 code page (65001 and 1252) are specified for it, I did a select * on a table. I exported the results and tried to import it and write to another table (I have to do this via SSIS, I cannot do this via SQL Server (i.e. select into) for security permission purposes, but I can do it this way).
How can I fix this error? What exactly does it mean? I tried searching StackOverflow but my search was not very helpful.
I tried to use an OLEDB source step, but my server instance does not appear for some reason, so now I'm trying to export/import out of SSMS.
","<sal-server><t-sal><suis>, sal server column cannot process 1 code page (65001 1252) specific it, select * table. export result try import write not table (i via suis, cannot via sal server (i.e. select into) secure permits purposes, way). fix error? exactly mean? try search stackoverflow search helpful. try use old source step, server instant appear reason, i'm try export/import sums."
52749377,"Google Cloud SQL Postgres, when will PG 10 be available?","I am planning on moving our main project to Postgres 10 at some point.  I like to keep the local dev's database version close to what we are running on prod.
Currently our prod database is on Google Cloud SQL PostgreSQL 9.6.  I have not heard anything from Google on when this managed cloud sql product will offer Postgres 10.x in addition to 9.6.
Does anyone know when Postgres 10 will be a supported option on GCP's managed SQL product?  I would like to start planning for it.
",<postgresql><google-cloud-platform><google-cloud-sql><postgresql-10>,479,0,0,161,0,1,3,38,3227,0.0,0,4,16,2018-10-10 22:01,2018-10-11 6:28,,1.0,,Basic,3,"<postgresql><google-cloud-platform><google-cloud-sql><postgresql-10>, Google Cloud SQL Postgres, when will PG 10 be available?, I am planning on moving our main project to Postgres 10 at some point.  I like to keep the local dev's database version close to what we are running on prod.
Currently our prod database is on Google Cloud SQL PostgreSQL 9.6.  I have not heard anything from Google on when this managed cloud sql product will offer Postgres 10.x in addition to 9.6.
Does anyone know when Postgres 10 will be a supported option on GCP's managed SQL product?  I would like to start planning for it.
","<postgresql><goose-cloud-platform><goose-cloud-sal><postgresql-10>, good cloud sal postures, pg 10 available?, plan move main project poster 10 point. like keep local de' database version close run proud. current proud database good cloud sal postgresql 9.6. heard any good manage cloud sal product offer poster 10.x admit 9.6. anyone know poster 10 support option gap' manage sal product? would like start plan it."
50254277,Connection refused when running Laravel artisan command with Docker,"I'm running Laravel 5.4 in Docker. This is my docker-compose.yml file:
version: '2'
services:
  app:
    container_name: laravel_app
    image: webdevops/php-apache-dev:ubuntu-16.04
    links:
      - mysql
    depends_on:
      - mysql
    ports:
      - 8888:80
    volumes:
      - .:/app
    environment:
      docker: 'true'
      WEB_DOCUMENT_ROOT: '/app/public'
      WEB_NO_CACHE_PATTERN: '\.(.*)$$'
      working_dir: '/app'
  mysql:
    image: mariadb:latest
    ports:
      - 8889:80
    environment:
      MYSQL_ROOT_PASSWORD: 'dev'
      MYSQL_DATABASE: 'dev'
      MYSQL_USER: 'dev'
      MYSQL_PASSWORD: 'dev'
This is the relevant part of my .env file:
DB_CONNECTION=mysql
DB_HOST=mysql
DB_PORT=8889
DB_DATABASE=dev
DB_USERNAME=dev
DB_PASSWORD=dev
I am able to see the Laravel welcome page - that side of things works. But when I run php artisan migrate I get this error:
  SQLSTATE[HY000] [2002] Connection refused (SQL: select * from information_schema.tables where table_schema = dev and table_name = migrations)
I have tried fiddling with the host and port parameters in the .env file.
",<php><mysql><laravel><docker><docker-compose>,1106,0,40,7365,20,69,124,35,28061,0.0,197,6,16,2018-05-09 13:11,2018-05-09 13:29,2018-05-09 13:29,0.0,0.0,Basic,14,"<php><mysql><laravel><docker><docker-compose>, Connection refused when running Laravel artisan command with Docker, I'm running Laravel 5.4 in Docker. This is my docker-compose.yml file:
version: '2'
services:
  app:
    container_name: laravel_app
    image: webdevops/php-apache-dev:ubuntu-16.04
    links:
      - mysql
    depends_on:
      - mysql
    ports:
      - 8888:80
    volumes:
      - .:/app
    environment:
      docker: 'true'
      WEB_DOCUMENT_ROOT: '/app/public'
      WEB_NO_CACHE_PATTERN: '\.(.*)$$'
      working_dir: '/app'
  mysql:
    image: mariadb:latest
    ports:
      - 8889:80
    environment:
      MYSQL_ROOT_PASSWORD: 'dev'
      MYSQL_DATABASE: 'dev'
      MYSQL_USER: 'dev'
      MYSQL_PASSWORD: 'dev'
This is the relevant part of my .env file:
DB_CONNECTION=mysql
DB_HOST=mysql
DB_PORT=8889
DB_DATABASE=dev
DB_USERNAME=dev
DB_PASSWORD=dev
I am able to see the Laravel welcome page - that side of things works. But when I run php artisan migrate I get this error:
  SQLSTATE[HY000] [2002] Connection refused (SQL: select * from information_schema.tables where table_schema = dev and table_name = migrations)
I have tried fiddling with the host and port parameters in the .env file.
","<pp><myself><travel><doctor><doctor-compose>, connect refuse run travel artisans command doctor, i'm run travel 5.4 doctor. doctor-compose.you file: version: '2' services: pp: container_name: laravel_app image: webdevops/pp-apache-de:bunt-16.04 links: - myself depends_on: - myself ports: - 8888:80 volumes: - .:/pp environment: doctor: 'true' web_document_root: '/pp/public' web_no_cache_pattern: '\.(.*)$$' working_dir: '/pp' myself: image: maria:latest ports: - 8889:80 environment: mysql_root_password: 'de' mysql_database: 'de' mysql_user: 'de' mysql_password: 'de' rule part .end file: db_connection=myself db_host=myself duport=8889 db_database=de db_username=de db_password=de all see travel welcome page - side thing works. run pp artisans migrate get error: sqlstate[hy000] [2002] connect refuse (sal: select * information_schema.t table_schema = de table_nam = migrations) try fiddle host port parapet .end file."
48651369,What is the best practice of firestore data structure?,"I'm making a blog app using firebase.
I want to know the best practice of data structure.
As far as I know, there are 2 case.
(I'm using react native)
case 1:
posts
  -postID
   -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
    -favoriteList
      -postID(onlyID)
      -postID(onlyID)
In this case, for example, when we need to get favorite posts.
firebase.firestore().collection(`favorites/${userID}/favoriteList`)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((favorite) =&gt; {
        firebase.firestore().collection(`favorites/`).doc(`${favorite.id}`)
          .get()
          .then((post) =&gt; {
          myPostList.push(post.data())
        });
  });
in this case, we can't order the favorite posts by createdDate. So, need to sort client side. Even if so, we don't use limit() function.
case 2:
posts
  -postID
  -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
     -favoriteList
       -postID
         -title,content,author(userID),createdDate,favoriteCount
       -postID
         -title,content,author(userID),createdDate,favoriteCount
firebase.firestore().collection(`favorites/${userID}/favoriteList`).orderBy('createdDate','desc').limit(30)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((post) =&gt; {
        myPostList.push(post.data())
      });
  });
in this case, When the favorite post is modified by the author,
we have to update all of the favorite posts. (e.g. If 100 users save the post as a favorite, we have to update to 100 data.)
(And I'm not sure we can increment favoritecount by a transaction, exactly same.)
I think if we use firebase.batch(), we can manage it. But I think it seems Inefficient.
It seems that both ways are not perfect. Do you know the best practice of this case?
",<javascript><firebase><react-native><nosql><google-cloud-firestore>,1811,0,40,548,3,11,24,70,12934,0.0,32,2,16,2018-02-06 20:24,2018-03-26 2:19,,48.0,,Advanced,33,"<javascript><firebase><react-native><nosql><google-cloud-firestore>, What is the best practice of firestore data structure?, I'm making a blog app using firebase.
I want to know the best practice of data structure.
As far as I know, there are 2 case.
(I'm using react native)
case 1:
posts
  -postID
   -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
    -favoriteList
      -postID(onlyID)
      -postID(onlyID)
In this case, for example, when we need to get favorite posts.
firebase.firestore().collection(`favorites/${userID}/favoriteList`)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((favorite) =&gt; {
        firebase.firestore().collection(`favorites/`).doc(`${favorite.id}`)
          .get()
          .then((post) =&gt; {
          myPostList.push(post.data())
        });
  });
in this case, we can't order the favorite posts by createdDate. So, need to sort client side. Even if so, we don't use limit() function.
case 2:
posts
  -postID
  -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
     -favoriteList
       -postID
         -title,content,author(userID),createdDate,favoriteCount
       -postID
         -title,content,author(userID),createdDate,favoriteCount
firebase.firestore().collection(`favorites/${userID}/favoriteList`).orderBy('createdDate','desc').limit(30)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((post) =&gt; {
        myPostList.push(post.data())
      });
  });
in this case, When the favorite post is modified by the author,
we have to update all of the favorite posts. (e.g. If 100 users save the post as a favorite, we have to update to 100 data.)
(And I'm not sure we can increment favoritecount by a transaction, exactly same.)
I think if we use firebase.batch(), we can manage it. But I think it seems Inefficient.
It seems that both ways are not perfect. Do you know the best practice of this case?
","<javascript><firebase><react-native><nose><goose-cloud-restore>, best practice director data structure?, i'm make blow pp use firebase. want know best practice data structure. far know, 2 case. (i'm use react native) case 1: post -posted -title,content,author(used),createddate,favoritecount favorite -used -favoritelist -posted(only) -posted(only) case, example, need get favorite posts. firebase.restore().collection(`favorites/${used}/favoritelist`) .get() .then((snapshot) =&it; { snapshot.french((favorite) =&it; { firebase.restore().collection(`favorites/`).do(`${favorite.id}`) .get() .then((post) =&it; { mypostlist.push(post.data()) }); }); case, can't order favorite post createddate. so, need sort client side. even so, use limit() function. case 2: post -posted -title,content,author(used),createddate,favoritecount favorite -used -favoritelist -posted -title,content,author(used),createddate,favoritecount -posted -title,content,author(used),createddate,favoritecount firebase.restore().collection(`favorites/${used}/favoritelist`).orderly('createddate','desk').limit(30) .get() .then((snapshot) =&it; { snapshot.french((post) =&it; { mypostlist.push(post.data()) }); }); case, favorite post modify author, update favorite posts. (e.g. 100 user save post favorite, update 100 data.) (and i'm sure incitement favoritecount transaction, exactly same.) think use firebase.batch(), manage it. think seem efficient. seem way perfect. know best practice case?"
55336035,Is it possible and how create a trigger with Entity Framework Core,"Is it possible to create a SQL trigger with Entity Framework Core.
Maybe by using instruction in 
protected override void OnModelCreating(DbModelBuilder dbModelBuilder)
{
}
Or simply by executing SQL statements in Migration scripts
public override void Up()
{
}
",<sql-server><entity-framework-core><database-trigger>,262,0,6,17827,31,124,203,58,20650,,522,2,16,2019-03-25 10:46,2020-02-18 9:41,2020-02-18 9:41,330.0,330.0,Basic,10,"<sql-server><entity-framework-core><database-trigger>, Is it possible and how create a trigger with Entity Framework Core, Is it possible to create a SQL trigger with Entity Framework Core.
Maybe by using instruction in 
protected override void OnModelCreating(DbModelBuilder dbModelBuilder)
{
}
Or simply by executing SQL statements in Migration scripts
public override void Up()
{
}
","<sal-server><entity-framework-core><database-trigger>, possible great trigger entity framework core, possible great sal trigger entity framework core. may use instruct protect overdid void onmodelcreating(dbmodelbuild dbmodelbuilder) { } simple execute sal statement migrate script public overdid void up() { }"
55956801,Shutting down ExecutorService 'applicationTaskExecutor',"I have a Spring-boot application deployed in docker container within in AWS ECS cluster.
My application stack is => Spring Boot -- JPA -- MySQL RDS
Initially application is deployed and accessible through EC2 public IP.
But after few minutes only application is Shutting down ExecutorService 'applicationTaskExecutor' and restart container again. it is happening constantly in every 3/4 mins.
I am not getting this kind of error in local deployment connecting to same RDS.
Here is my application.properties
server.port=8192
spring.datasource.url=jdbc:mysql://multichannelappdatabase.cvjsdfsdfsdfsfs.us-east-1.rds.amazonaws.com:3306/multichannelappdb
spring.datasource.username=xxxxx
spring.datasource.password=xxxxx
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.jackson.serialization.FAIL_ON_EMPTY_BEANS=false
spring.jpa.database-platform=org.hibernate.dialect.MySQLDialect
spring.jpa.show-sql=true
#spring.jpa.hibernate.ddl-auto=create
spring.datasource.testWhileIdle = true
spring.datasource.timeBetweenEvictionRunsMillis = 10000
spring.datasource.tomcat.testOnBorrow=true 
spring.datasource.tomcat.validationQuery=SELECT 1
and here is my cloudwatch log

16:20:50
2019-05-02 16:20:50.514 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8192 (http) with context path ''

16:20:50
2019-05-02 16:20:50.516 INFO 1 --- [ main] c.api.app.runner.UserManagerApplication : Started UserManagerApplication in 9.338 seconds (JVM running for 10.291)

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.131 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 14 ms

16:23:19
2019-05-02 16:23:19.253 INFO 1 --- [ Thread-4] o.s.s.concurrent.ThreadPoolTaskExecutor : Shutting down ExecutorService 'applicationTaskExecutor'

16:23:19
2019-05-02 16:23:19.254 INFO 1 --- [ Thread-4] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'

16:23:19
2019-05-02 16:23:19.257 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...

16:23:19
2019-05-02 16:23:19.274 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed.
Need suggestion to resolve this issue.
Still getting the same issue .. any solution will be helpful
",<mysql><spring-boot><spring-data-jpa><amazon-rds><amazon-ecs>,2620,0,44,1196,4,14,31,64,26122,,17,4,16,2019-05-02 16:37,2019-06-06 12:58,,35.0,,Intermediate,31,"<mysql><spring-boot><spring-data-jpa><amazon-rds><amazon-ecs>, Shutting down ExecutorService 'applicationTaskExecutor', I have a Spring-boot application deployed in docker container within in AWS ECS cluster.
My application stack is => Spring Boot -- JPA -- MySQL RDS
Initially application is deployed and accessible through EC2 public IP.
But after few minutes only application is Shutting down ExecutorService 'applicationTaskExecutor' and restart container again. it is happening constantly in every 3/4 mins.
I am not getting this kind of error in local deployment connecting to same RDS.
Here is my application.properties
server.port=8192
spring.datasource.url=jdbc:mysql://multichannelappdatabase.cvjsdfsdfsdfsfs.us-east-1.rds.amazonaws.com:3306/multichannelappdb
spring.datasource.username=xxxxx
spring.datasource.password=xxxxx
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.jackson.serialization.FAIL_ON_EMPTY_BEANS=false
spring.jpa.database-platform=org.hibernate.dialect.MySQLDialect
spring.jpa.show-sql=true
#spring.jpa.hibernate.ddl-auto=create
spring.datasource.testWhileIdle = true
spring.datasource.timeBetweenEvictionRunsMillis = 10000
spring.datasource.tomcat.testOnBorrow=true 
spring.datasource.tomcat.validationQuery=SELECT 1
and here is my cloudwatch log

16:20:50
2019-05-02 16:20:50.514 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8192 (http) with context path ''

16:20:50
2019-05-02 16:20:50.516 INFO 1 --- [ main] c.api.app.runner.UserManagerApplication : Started UserManagerApplication in 9.338 seconds (JVM running for 10.291)

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.131 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 14 ms

16:23:19
2019-05-02 16:23:19.253 INFO 1 --- [ Thread-4] o.s.s.concurrent.ThreadPoolTaskExecutor : Shutting down ExecutorService 'applicationTaskExecutor'

16:23:19
2019-05-02 16:23:19.254 INFO 1 --- [ Thread-4] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'

16:23:19
2019-05-02 16:23:19.257 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...

16:23:19
2019-05-02 16:23:19.274 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed.
Need suggestion to resolve this issue.
Still getting the same issue .. any solution will be helpful
","<myself><spring-boot><spring-data-pa><amazon-rd><amazon-es>, shut executorservic 'applicationtaskexecutor', spring-boot applied deploy doctor contain within a c cluster. applied stick => spring boot -- pa -- myself rd into applied deploy access end public in. minute applied shut executorservic 'applicationtaskexecutor' start contain again. happen constantly every 3/4 mind. get kind error local deploy connect rd. application.property server.port=8192 spring.datasource.curl=job:myself://multichannelappdatabase.cvjsdfsdfsdfsfs.us-east-1.rd.amazonaws.com:3306/multichannelappdb spring.datasource.surname=xxxix spring.datasource.password=xxxix spring.datasource.driver-class-name=com.myself.c.job.drive spring.jackson.serialization.fail_on_empty_beans=fall spring.pa.database-platform=org.liberate.dialect.mysqldialect spring.pa.show-sal=true #spring.pa.liberate.del-auto=or spring.datasource.testwhileidl = true spring.datasource.timebetweenevictionrunsmilli = 10000 spring.datasource.combat.testonborrow=true spring.datasource.combat.validationquery=select 1 cloudwatch log  16:20:50 2019-05-02 16:20:50.514 into 1 --- [ main] o.s.b.w.embedded.combat.tomcatwebserv : combat start port(s): 8192 (http) context path ''  16:20:50 2019-05-02 16:20:50.516 into 1 --- [ main] c.apt.pp.runner.usermanagerappl : start usermanagerappl 9.338 second (jem run 10.291)  16:20:57 2019-05-02 16:20:57.117 into 1 --- [no-8192-even-1] o.a.c.c.c.[combat].[localhost].[/] : into spring dispatcherservlet 'dispatcherservlet'  16:20:57 2019-05-02 16:20:57.117 into 1 --- [no-8192-even-1] o.s.web.serve.dispatcherservlet : into serve 'dispatcherservlet'  16:20:57 2019-05-02 16:20:57.131 into 1 --- [no-8192-even-1] o.s.web.serve.dispatcherservlet : complete into 14 ms  16:23:19 2019-05-02 16:23:19.253 into 1 --- [ thread-4] o.s.s.concurrent.threadpooltaskexecutor : shut executorservic 'applicationtaskexecutor'  16:23:19 2019-05-02 16:23:19.254 into 1 --- [ thread-4] j.localcontainerentitymanagerfactorybean : close pa entitymanagerfactori persist unit 'default'  16:23:19 2019-05-02 16:23:19.257 into 1 --- [ thread-4] com.baxter.kari.hikaridatasourc : hikaripool-1 - shutdown initiated...  16:23:19 2019-05-02 16:23:19.274 into 1 --- [ thread-4] com.baxter.kari.hikaridatasourc : hikaripool-1 - shutdown completed. need suggest resolve issue. still get issue .. slut help"
49142684,psql how to drop current database,"Trying to drop the primary database ""foo"", I get the error
  ERROR:  cannot drop the currently open database
Whats the right way to drop the primary psql database? 
",<postgresql>,165,0,0,16955,16,138,148,81,21100,,4078,2,16,2018-03-07 1:24,2018-03-07 1:24,2018-03-07 8:36,0.0,0.0,Basic,10,"<postgresql>, psql how to drop current database, Trying to drop the primary database ""foo"", I get the error
  ERROR:  cannot drop the currently open database
Whats the right way to drop the primary psql database? 
","<postgresql>, pool drop current database, try drop primary database ""foo"", get error error: cannot drop current open database what right way drop primary pool database?"
53720353,"Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""","This happens even when the DB user specified in .env file is different. For the record ""ankitj"" is also the username of my system. I don't understand why this is happening.
Here's the error:
Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""
    at connection.connect.err (/home/ankitj/Desktop/skillbee/node_modules/sequelize/lib/dialects/postgres/connection-manager.js:128:24)
    at Connection.connectingErrorHandler (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/client.js:140:14)
    at Connection.emit (events.js:160:13)
    at Socket.&lt;anonymous&gt; (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/connection.js:124:12)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
",<node.js><postgresql><sequelize.js>,906,0,10,431,2,5,8,77,14356,0.0,27,6,16,2018-12-11 8:45,2019-01-23 21:36,,43.0,,Basic,13,"<node.js><postgresql><sequelize.js>, Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj"", This happens even when the DB user specified in .env file is different. For the record ""ankitj"" is also the username of my system. I don't understand why this is happening.
Here's the error:
Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""
    at connection.connect.err (/home/ankitj/Desktop/skillbee/node_modules/sequelize/lib/dialects/postgres/connection-manager.js:128:24)
    at Connection.connectingErrorHandler (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/client.js:140:14)
    at Connection.emit (events.js:160:13)
    at Socket.&lt;anonymous&gt; (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/connection.js:124:12)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
","<node.is><postgresql><sequelae.is>, unhandl reject sequelizeconnectionerror: password authentic fail user ""anita"", happen even do user specific .end file different. record ""anita"" also usernam system. understand happening. here' error: unhandl reject sequelizeconnectionerror: password authentic fail user ""anita"" connection.connect.err (/home/anita/desktop/skilled/node_modules/sequelae/limb/dialect/postures/connection-manager.is:128:24) connection.connectingerrorhandl (/home/anita/desktop/skilled/node_modules/pg/limb/client.is:140:14) connection.emit (events.is:160:13) socket.&it;anonymous&it; (/home/anita/desktop/skilled/node_modules/pg/limb/connection.is:124:12) socket.emit (events.is:160:13) addchunk (_stream_readable.is:269:12) readableaddchunk (_stream_readable.is:256:11) socket.readable.push (_stream_readable.is:213:10) top.read (net.is:599:20)"
