QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
48962106,add unique constraint in room database to multiple column,"I have one entity in room
@Entity(foreignKeys ={
        @ForeignKey(entity = Label.class, parentColumns = ""_id"", childColumns = ""labelId"", onDelete = CASCADE),
        @ForeignKey(entity = Task.class, parentColumns = ""_id"", childColumns = ""taskId"", onDelete = CASCADE)
})
public class LabelOfTask extends Data{
    @ColumnInfo(name = ""labelId"")
    private Integer labelId;
    @ColumnInfo(name = ""taskId"")
    private Integer taskId;
}
sql syntax of this entity is as below
CREATE TABLE `LabelOfTask` (
    `_id` INTEGER PRIMARY KEY AUTOINCREMENT,
     `labelId` INTEGER,
     `taskId` INTEGER,
     FOREIGN KEY(`labelId`) REFERENCES `Label`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE ,
     FOREIGN KEY(`taskId`) REFERENCES `Task`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE
 );
but what change or annotation I need to add in entity class if I want to append below constraint to the auto generated sql schema of the table
unique (labelId, taskId)
Ultimately I want to make combination of labelId and taskId unique in a table(or entity of room) using room library.
",<android><sqlite><database-design><android-room><android-architecture-components>,1070,0,18,1085,1,9,15,79,50747,0.0,26,5,65,2018-02-24 11:03,2018-02-24 12:16,2018-02-24 12:16,0.0,0.0,Basic,9,"<android><sqlite><database-design><android-room><android-architecture-components>, add unique constraint in room database to multiple column, I have one entity in room
@Entity(foreignKeys ={
        @ForeignKey(entity = Label.class, parentColumns = ""_id"", childColumns = ""labelId"", onDelete = CASCADE),
        @ForeignKey(entity = Task.class, parentColumns = ""_id"", childColumns = ""taskId"", onDelete = CASCADE)
})
public class LabelOfTask extends Data{
    @ColumnInfo(name = ""labelId"")
    private Integer labelId;
    @ColumnInfo(name = ""taskId"")
    private Integer taskId;
}
sql syntax of this entity is as below
CREATE TABLE `LabelOfTask` (
    `_id` INTEGER PRIMARY KEY AUTOINCREMENT,
     `labelId` INTEGER,
     `taskId` INTEGER,
     FOREIGN KEY(`labelId`) REFERENCES `Label`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE ,
     FOREIGN KEY(`taskId`) REFERENCES `Task`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE
 );
but what change or annotation I need to add in entity class if I want to append below constraint to the auto generated sql schema of the table
unique (labelId, taskId)
Ultimately I want to make combination of labelId and taskId unique in a table(or entity of room) using room library.
","<andros><quite><database-design><andros-room><andros-architecture-components>, add unique constraint room database multiple column, one entity room @entity(foreigner ={ @foreigner(went = label.class, parentcolumn = ""did"", childcolumn = ""labels"", omelet = cascade), @foreigner(went = task.class, parentcolumn = ""did"", childcolumn = ""asked"", omelet = cascade) }) public class labeloftask extend data{ @columninfo(am = ""labels"") privat inter labels; @columninfo(am = ""asked"") privat inter asked; } sal santa entity great table `labeloftask` ( `did` inter primary key autoincrement, `labels` inter, `asked` inter, foreign key(`labels`) refer `label`(`did`) update action delete cascade , foreign key(`asked`) refer `task`(`did`) update action delete cascade ); change cannot need add entity class want happened constraint auto genet sal scheme table unique (labels, asked) until want make combine labels asked unique table(or entity room) use room library."
60420940,"How to fix error ""Error: Database is uninitialized and superuser password is not specified.""","Hello i get this error after i run docker-compose build up
But i get this error
postgres_1 | Error: Database is uninitialized and superuser password is not specified.
Here is a snap shot of the error!
And down below is my docker-compose.yml file
version: '3.6'
Server.js file
services: 
  smart-brain-api:
    container_name: backend
    build: ./
    command: npm start
    working_dir: /usr/src/smart-brain-api
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - ./:/usr/src/smart-brain-api
  #PostGres Database
  postgres:
    image: postgres
    ports:
      - &quot;5432:5432&quot;
",<postgresql><docker><docker-compose>,597,1,17,1299,1,12,22,41,74498,0.0,13,7,64,2020-02-26 19:12,2020-02-26 23:54,2020-02-26 23:54,0.0,0.0,Basic,14,"<postgresql><docker><docker-compose>, How to fix error ""Error: Database is uninitialized and superuser password is not specified."", Hello i get this error after i run docker-compose build up
But i get this error
postgres_1 | Error: Database is uninitialized and superuser password is not specified.
Here is a snap shot of the error!
And down below is my docker-compose.yml file
version: '3.6'
Server.js file
services: 
  smart-brain-api:
    container_name: backend
    build: ./
    command: npm start
    working_dir: /usr/src/smart-brain-api
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - ./:/usr/src/smart-brain-api
  #PostGres Database
  postgres:
    image: postgres
    ports:
      - &quot;5432:5432&quot;
","<postgresql><doctor><doctor-compose>, fix error ""error: database uniniti humerus password specified."", hello get error run doctor-compose build get error postgres_1 | error: database uniniti humerus password specified. snap shot error! doctor-compose.you file version: '3.6' server.j file services: smart-brain-apt: container_name: backed build: ./ command: nom start working_dir: /us/sac/smart-brain-apt ports: - &quit;3000:3000&quit; volumes: - ./:/us/sac/smart-brain-apt #poster database postures: image: poster ports: - &quit;5432:5432&quit;"
51294268,"pip install mysqlclient returns ""fatal error C1083: Cannot open file: 'mysql.h': No such file or directory","Here is this issue:
I attempt to install mysqlclient like so
C:\Users\amccommon349&gt;pip install mysqlclient
Collecting mysqlclient
  Using cached https://files.pythonhosted.org/packages/ec/fd/83329b9d3e14f7344d1
cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.gz
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\amccommon349\appdata\local\programs\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\AMCCOM~1\\AppData\\Local\\Temp\\pip-install-qcgo48hf\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\AMCCOM~1\AppData\Local\Temp\pip-record-q4yoftj8\install-record.txt --single-version-externally-managed --compile:
c:\users\amccommon349\appdata\local\programs\python\python36\lib\distutils\dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
warnings.warn(msg)
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.6
copying _mysql_exceptions.py -&gt; build\lib.win-amd64-3.6
creating build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\compat.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\connections.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\converters.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\cursors.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\release.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\times.py -&gt; build\lib.win-amd64-3.6\MySQLdb
creating build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CR.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\ER.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FLAG.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
running build_ext
building '_mysql' extension
creating build\temp.win-amd64-3.6
creating build\temp.win-amd64-3.6\Release
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\bin\HostX86\x64\cl.exe /c/nologo/Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" -Ic:\users\amccommon349\appdata\local\programs\python\python36\include -Ic:\users\amccommon349\appdata\local\programs\python\python36\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\ProgramFiles (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win-amd64-3.6\Release\_mysql.obj /Zl _mysql.c
_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
I made sure I had all of the files needed from visual studios build tools, I downloaded the mysql-python connector, and updated my pip and setup tools. I am a complete beginner to this and would appreciate any input as to how to go about fixing this error.
",<python><mysql><sql><pip>,3995,1,39,645,1,5,8,81,81264,0.0,1,25,63,2018-07-11 21:16,2018-07-11 22:10,2018-07-11 22:10,0.0,0.0,Basic,14,"<python><mysql><sql><pip>, pip install mysqlclient returns ""fatal error C1083: Cannot open file: 'mysql.h': No such file or directory, Here is this issue:
I attempt to install mysqlclient like so
C:\Users\amccommon349&gt;pip install mysqlclient
Collecting mysqlclient
  Using cached https://files.pythonhosted.org/packages/ec/fd/83329b9d3e14f7344d1
cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.gz
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\amccommon349\appdata\local\programs\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\AMCCOM~1\\AppData\\Local\\Temp\\pip-install-qcgo48hf\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\AMCCOM~1\AppData\Local\Temp\pip-record-q4yoftj8\install-record.txt --single-version-externally-managed --compile:
c:\users\amccommon349\appdata\local\programs\python\python36\lib\distutils\dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
warnings.warn(msg)
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.6
copying _mysql_exceptions.py -&gt; build\lib.win-amd64-3.6
creating build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\compat.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\connections.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\converters.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\cursors.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\release.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\times.py -&gt; build\lib.win-amd64-3.6\MySQLdb
creating build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CR.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\ER.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FLAG.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
running build_ext
building '_mysql' extension
creating build\temp.win-amd64-3.6
creating build\temp.win-amd64-3.6\Release
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\bin\HostX86\x64\cl.exe /c/nologo/Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" -Ic:\users\amccommon349\appdata\local\programs\python\python36\include -Ic:\users\amccommon349\appdata\local\programs\python\python36\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\ProgramFiles (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win-amd64-3.6\Release\_mysql.obj /Zl _mysql.c
_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
I made sure I had all of the files needed from visual studios build tools, I downloaded the mysql-python connector, and updated my pip and setup tools. I am a complete beginner to this and would appreciate any input as to how to go about fixing this error.
","<patron><myself><sal><pp>, pp instal mysqlclient return ""fatal error c1083: cannot open file: 'myself.h': file directory, issue: attempt instal mysqlclient like c:\users\amccommon349&it;pp instal mysqlclient collect mysqlclient use each http://files.pythonhosted.org/packages/c/ff/83329b9d3e14f7344d1 cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.go instal collect packages: mysqlclient run set.i instal mysqlclient ... error complete output command c:\users\amccommon349\appdata\local\programs\patron\python36\patron.ex -u -c ""import setuptools, tokenize;__file__='c:\\users\\amccom~1\\appdata\\local\\hemp\\pp-install-qcgo48hf\\mysqlclient\\set.by';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();even(compile(code, __file__, 'even'))"" instal --record c:\users\amccom~1\appdata\local\hemp\pp-record-q4yoftj8\install-record.txt --single-version-externally-manage --compile: c:\users\amccommon349\appdata\local\programs\patron\python36\limb\distutils\list.by:261: userwarning: unknown distribute option: 'long_description_content_type' warnings.warn(mug) run instal run build run build_pi great build great build\limb.win-amd64-3.6 copy _mysql_exceptions.i -&it; build\limb.win-amd64-3.6 great build\limb.win-amd64-3.6\mysqldb copy mysqldb\__init__.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\compact.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\connections.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\converted.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\curious.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\release.i -&it; build\limb.win-amd64-3.6\mysqldb copy mysqldb\times.i -&it; build\limb.win-amd64-3.6\mysqldb great build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\__init__.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\client.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\or.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\er.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\field_type.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\flag.i -&it; build\limb.win-amd64-3.6\mysqldb\cost copy mysqldb\constant\refresh.i -&it; build\limb.win-amd64-3.6\mysqldb\cost run build_ext build '_mysql' extent great build\hemp.win-amd64-3.6 great build\hemp.win-amd64-3.6\release c:\program file (x)\microsoft visual studio\2017\buildtools\ve\tools\move\14.14.26428\bin\hostx86\x\ll.ex /c/nologo/ox /we /go /dndebug /md -dversion_info=(1,3,13,'final',0) -d__version__=1.3.13 ""-in:\program file (x)\myself\myself connection c 6.1\include"" -in:\users\amccommon349\appdata\local\programs\patron\python36\include -in:\users\amccommon349\appdata\local\programs\patron\python36\include ""-in:\program file (x)\microsoft visual studio\2017\buildtools\ve\tools\move\14.14.26428\include"" ""-in:\program file (x)\window kits\netfxsdk\4.6.1\include\up"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\curt"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\shared"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\up"" ""-in:\programfil (x)\window kits\10\include\10.0.17134.0\wirt"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\cppwinrt"" /tc_mysql.c /build\hemp.win-amd64-3.6\release\_mysql.obs /ll _mysql.c _mysql.c(29): fatal error c1083: cannot open include file: 'myself.h': file director error: command 'c:\\program file (x)\\microsoft visual studio\\2017\\buildtools\\ve\\tools\\move\\14.14.26428\\bin\\hostx86\\x\\ll.eye' fail exit state 2 made sure file need visual studio build tools, download myself-patron connection, update pp set tools. complete begin would appreci input go fix error."
51716530,AWS Aurora MySQL serverless: how to connect from MySQL Workbench,"I was trying to use AWS Aurora Serverless for MySQL in my project, but I am impossible to connect to it, though I have the endpoint, username, password.
What I have done:
From AWS console managment, I select RDS > Instances > Aurora > Serverless 
Leave the default settings
Create database
AWS will only create an AWS Cluster
I open MySQL Workbench, and use endpoint, username, password to connect the database
Ressult: 
  Your connection attempt failed for user 'admin' from your host to
  server at xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com:3306: 
  Can't connect to MySQL server on
  'xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com' (60)
Did I make any wrong steps ? Please advice me.
****EDIT****
I tried to create another Aurora database with capacity type: Provisioned. I can connect to the endpoint seamlessly with username and password by MySql workbench. It means that the port 3306 is opened for workbench. 
About the security group: 
",<mysql><amazon-web-services><serverless><amazon-aurora>,964,2,0,2766,6,26,52,70,44243,0.0,17,13,63,2018-08-06 22:37,2018-08-06 22:58,,0.0,,Basic,14,"<mysql><amazon-web-services><serverless><amazon-aurora>, AWS Aurora MySQL serverless: how to connect from MySQL Workbench, I was trying to use AWS Aurora Serverless for MySQL in my project, but I am impossible to connect to it, though I have the endpoint, username, password.
What I have done:
From AWS console managment, I select RDS > Instances > Aurora > Serverless 
Leave the default settings
Create database
AWS will only create an AWS Cluster
I open MySQL Workbench, and use endpoint, username, password to connect the database
Ressult: 
  Your connection attempt failed for user 'admin' from your host to
  server at xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com:3306: 
  Can't connect to MySQL server on
  'xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com' (60)
Did I make any wrong steps ? Please advice me.
****EDIT****
I tried to create another Aurora database with capacity type: Provisioned. I can connect to the endpoint seamlessly with username and password by MySql workbench. It means that the port 3306 is opened for workbench. 
About the security group: 
","<myself><amazon-web-services><serverless><amazon-aroma>, a aroma myself serverless: connect myself workbench, try use a aroma serverless myself project, impose connect it, though endpoint, surname, password. done: a console management, select rd > instant > aroma > serverless leave default set great database a great a cluster open myself workbench, use endpoint, surname, password connect database result: connect attempt fail user 'admit' host server xxxix.cluster-abcdefg1234.e-west-1.rd.amazonaws.com:3306: can't connect myself server 'xxxix.cluster-abcdefg1234.e-west-1.rd.amazonaws.com' (60) make wrong step ? pleas advice me. ****edit**** try great not aroma database cap type: provisioned. connect endpoint seamlessli usernam password myself workbench. mean port 3306 open workbench. secure group:"
57975093,Create a Superuser in postgres,"I'm looking for setup a Rails Environment with Vagrant, for that purpose the box it's been provisioned through bash shell method and includes among others this line:
sudo -u postgres createuser &lt;superuserusername&gt; -s with password '&lt;superuserpassword&gt;'
But I'm getting a configuration error:
createuser: too many command-line arguments (first is &quot;with&quot;)
Can you help me with the correct syntax for create a Superuser with a password. Thanks.
",<sql><database><postgresql><ubuntu><superuser>,464,0,1,1345,2,11,16,35,170650,0.0,1,5,62,2019-09-17 13:16,2019-09-17 13:43,2019-09-18 16:47,0.0,1.0,Basic,9,"<sql><database><postgresql><ubuntu><superuser>, Create a Superuser in postgres, I'm looking for setup a Rails Environment with Vagrant, for that purpose the box it's been provisioned through bash shell method and includes among others this line:
sudo -u postgres createuser &lt;superuserusername&gt; -s with password '&lt;superuserpassword&gt;'
But I'm getting a configuration error:
createuser: too many command-line arguments (first is &quot;with&quot;)
Can you help me with the correct syntax for create a Superuser with a password. Thanks.
","<sal><database><postgresql><bunt><superuser>, great humerus postures, i'm look set rail environs vagrant, purpose box proves base shell method include among other line: so -u poster creates &it;superuserusername&it; -s password '&it;superuserpassword&it;' i'm get configur error: createuser: man command-in argument (first &quit;with&quit;) help correct santa great humerus password. thanks."
58740043,How do I catch a psycopg2.errors.UniqueViolation error in a Python (Flask) app?,"I have a small Python web app (written in Flask) that uses sqlalchemy to persist data to the database.  When I try to insert a duplicate row, an exception is raised, something like this:
(psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""uix_my_column""
I would like to wrap the exception and re-raise my own so I can add my own logging and messaging that is specific to that particular error.  This is what I tried (simplified):
from db import DbApi
from my_exceptions import BadRequest
from psycopg2.errors import UniqueViolation # &lt;-- this does not exist!
class MyClass:
    def __init__(self):
        self.db = DbApi() 
    def create(self, data: dict) -&gt; MyRecord:
        try:
            with self.db.session_local(expire_on_commit=False) as session:
                my_rec = MyRecord(**data)
                session.add(my_rec)
                session.commit()
                session.refresh(my_rec)
                return my_rec
        except UniqueViolation as e:
            raise BadRequest('A duplicate record already exists')
But this fails to trap the error because psycopg2.errors.UniqueViolation isn't actually a class name (!).
In PHP, this would be as easy as catching copy/pasting the classname of the exception, but in Python, this is much more obfuscated.
There was a similar question here, but it didn't deal with this specific use-case and (importantly), it did not clarify how one can identify the root exception class name. 
How does one find out what exception is actually being raised? Why does Python hide this?
",<python><sqlalchemy>,1580,1,20,8967,5,36,51,37,49377,0.0,109,7,61,2019-11-06 23:49,2019-11-07 0:02,,1.0,,Basic,13,"<python><sqlalchemy>, How do I catch a psycopg2.errors.UniqueViolation error in a Python (Flask) app?, I have a small Python web app (written in Flask) that uses sqlalchemy to persist data to the database.  When I try to insert a duplicate row, an exception is raised, something like this:
(psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""uix_my_column""
I would like to wrap the exception and re-raise my own so I can add my own logging and messaging that is specific to that particular error.  This is what I tried (simplified):
from db import DbApi
from my_exceptions import BadRequest
from psycopg2.errors import UniqueViolation # &lt;-- this does not exist!
class MyClass:
    def __init__(self):
        self.db = DbApi() 
    def create(self, data: dict) -&gt; MyRecord:
        try:
            with self.db.session_local(expire_on_commit=False) as session:
                my_rec = MyRecord(**data)
                session.add(my_rec)
                session.commit()
                session.refresh(my_rec)
                return my_rec
        except UniqueViolation as e:
            raise BadRequest('A duplicate record already exists')
But this fails to trap the error because psycopg2.errors.UniqueViolation isn't actually a class name (!).
In PHP, this would be as easy as catching copy/pasting the classname of the exception, but in Python, this is much more obfuscated.
There was a similar question here, but it didn't deal with this specific use-case and (importantly), it did not clarify how one can identify the root exception class name. 
How does one find out what exception is actually being raised? Why does Python hide this?
","<patron><sqlalchemy>, catch psycopg2.errors.uniqueviol error patron (flask) pp?, small patron web pp (written flask) use sqlalchemi persist data database. try insert public row, except raised, cometh like this: (psycopg2.errors.uniqueviolation) public key value violet unique constraint ""uix_my_column"" would like wrap except re-rays add log message specie particular error. try (simplifies): do import dbapi my_except import badrequest psycopg2.error import uniqueviol # &it;-- exist! class class: def __init__(self): self.do = dbapi() def create(self, data: duct) -&it; record: try: self.do.session_local(expire_on_commit=false) session: my_rec = record(**data) session.add(my_rec) session.commit() session.refresh(my_rec) return my_rec except uniqueviol e: rays badrequest('a public record already exists') fail trap error psycopg2.errors.uniqueviol actual class name (!). pp, would east catch copy/past classnam exception, patron, much obfuscated. similar question here, deal specie use-was (importantly), clarify one identify root except class name. one find except actual raised? patron hide this?"
58961043,How to install libpq-fe.h?,"I cannot figure this out for the life of me.
When I pip install django-tenant-schemas it tries to install the dependency psycopg2 which requires the Python headers and gcc. I have all this installed and still keep getting this error!
./psycopg/psycopg.h:35:10: fatal error: libpq-fe.h: No such file or directory
So to install libpq-fe-h I need to sudo apt-get install libpq-dev..
..which returns..
libpq-dev is already the newest version (10.10-0ubuntu0.18.04.1).
Then when I sudo find / libpq-fe.h it doesn't seem to be in my OS.
I am lost at this point. If anyone can help I would highly appreciate it.
",<django><python-3.x><postgresql><pip>,605,0,7,2134,3,13,38,68,36211,0.0,319,4,60,2019-11-20 18:24,2019-11-20 18:46,2019-11-20 18:46,0.0,0.0,Basic,13,"<django><python-3.x><postgresql><pip>, How to install libpq-fe.h?, I cannot figure this out for the life of me.
When I pip install django-tenant-schemas it tries to install the dependency psycopg2 which requires the Python headers and gcc. I have all this installed and still keep getting this error!
./psycopg/psycopg.h:35:10: fatal error: libpq-fe.h: No such file or directory
So to install libpq-fe-h I need to sudo apt-get install libpq-dev..
..which returns..
libpq-dev is already the newest version (10.10-0ubuntu0.18.04.1).
Then when I sudo find / libpq-fe.h it doesn't seem to be in my OS.
I am lost at this point. If anyone can help I would highly appreciate it.
","<django><patron-3.x><postgresql><pp>, instal lips-fe.h?, cannot figure life me. pp instal django-tenant-scheme try instal depend psycopg2 require patron header go. instal still keep get error! ./psycopg/psycopg.h:35:10: fatal error: lips-fe.h: file director instal lips-fe-h need so apt-get instal lips-de.. ..which returns.. lips-de already newest version (10.10-0ubuntu0.18.04.1). so find / lips-fe.h seem os. lost point. anyone help would highly appreci it."
51004516,.NET Core 2.1 Identity get all users with their associated roles,"I'm trying to pull out all my Identity users and their associated roles for a user management admin page. I thought this would be reasonably easy but apparently not. I've tried following the following solution: https://stackoverflow.com/a/43562544/5392786 but it hasn't worked out so far.
Here is what I have so far:
ApplicationUser:
public class ApplicationUser : IdentityUser
{
    public List&lt;IdentityUserRole&lt;string&gt;&gt; Roles { get; set; }
}
DBContext
public class ApplicationDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApplicationDbContext(DbContextOptions&lt;ApplicationDbContext&gt; options)
        : base(options)
    {
    }
}
Startup Identity code
services.AddIdentity&lt;ApplicationUser, IdentityRole&gt;(options =&gt; options.Stores.MaxLengthForKeys = 128)
            .AddEntityFrameworkStores&lt;ApplicationDbContext&gt;()
            .AddDefaultTokenProviders();
Razor Page where I want to display the list:
public class IndexModel : PageModel
{
    private readonly UserManager&lt;ApplicationUser&gt; userManager;
    public IndexModel(UserManager&lt;ApplicationUser&gt; userManager)
    {
        this.userManager = userManager;
    }
    public IEnumerable&lt;ApplicationUser&gt; Users { get; set; }
    public void OnGetAsync()
    {
        this.Users = userManager.Users.Include(u =&gt; u.Roles).ToList();
    }
}
I get the following error when calling userManager.Users.Include(u =&gt; u.Roles).ToList();:
  MySql.Data.MySqlClient.MySqlException: 'Unknown column 'u.Roles.ApplicationUserId' in 'field list''
",<c#><mysql><asp.net-core><entity-framework-core><asp.net-core-identity>,1563,1,31,3834,7,32,56,74,83751,0.0,100,14,59,2018-06-23 19:44,2018-06-23 22:08,2018-06-23 22:08,0.0,0.0,Basic,9,"<c#><mysql><asp.net-core><entity-framework-core><asp.net-core-identity>, .NET Core 2.1 Identity get all users with their associated roles, I'm trying to pull out all my Identity users and their associated roles for a user management admin page. I thought this would be reasonably easy but apparently not. I've tried following the following solution: https://stackoverflow.com/a/43562544/5392786 but it hasn't worked out so far.
Here is what I have so far:
ApplicationUser:
public class ApplicationUser : IdentityUser
{
    public List&lt;IdentityUserRole&lt;string&gt;&gt; Roles { get; set; }
}
DBContext
public class ApplicationDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApplicationDbContext(DbContextOptions&lt;ApplicationDbContext&gt; options)
        : base(options)
    {
    }
}
Startup Identity code
services.AddIdentity&lt;ApplicationUser, IdentityRole&gt;(options =&gt; options.Stores.MaxLengthForKeys = 128)
            .AddEntityFrameworkStores&lt;ApplicationDbContext&gt;()
            .AddDefaultTokenProviders();
Razor Page where I want to display the list:
public class IndexModel : PageModel
{
    private readonly UserManager&lt;ApplicationUser&gt; userManager;
    public IndexModel(UserManager&lt;ApplicationUser&gt; userManager)
    {
        this.userManager = userManager;
    }
    public IEnumerable&lt;ApplicationUser&gt; Users { get; set; }
    public void OnGetAsync()
    {
        this.Users = userManager.Users.Include(u =&gt; u.Roles).ToList();
    }
}
I get the following error when calling userManager.Users.Include(u =&gt; u.Roles).ToList();:
  MySql.Data.MySqlClient.MySqlException: 'Unknown column 'u.Roles.ApplicationUserId' in 'field list''
","<c#><myself><asp.net-core><entity-framework-core><asp.net-core-identity>, .net core 2.1 went get user cassock roles, i'm try pull went user cassock role user manage admit page. thought would reason east appear not. i'v try follow follow solution: http://stackoverflow.com/a/43562544/5392786 work far. far: applicationuser: public class applications : identity { public list&it;identityuserrole&it;string&it;&it; role { get; set; } } context public class applicationdbcontext : identitydbcontext&it;applicationuser&it; { public applicationdbcontext(dbcontextoptions&it;applicationdbcontext&it; option) : base(option) { } } started went code services.addidentity&it;applicationuser, identityrole&it;(opt =&it; option.stores.maxlengthforkey = 128) .addentityframeworkstores&it;applicationdbcontext&it;() .adddefaulttokenproviders(); razor page want display list: public class indexmodel : pagemodel { privat readonli usermanager&it;applicationuser&it; usermanager; public indexmodel(usermanager&it;applicationuser&it; usermanager) { this.usermanag = usermanager; } public innumerable&it;applicationuser&it; user { get; set; } public void ongetasync() { this.us = usermanager.users.include(u =&it; u.roles).moist(); } } get follow error call usermanager.users.include(u =&it; u.roles).moist();: myself.data.mysqlclient.mysqlexception: 'unknown column 'u.roles.applicationuserid' 'field list''"
50646216,Sequelize Where if not null,"Lets say I want to do a select command with
WHERE ID=2134
But if the user does not provide the id then it should just not bother with the WHERE ID (since it is null)
How can I handle this with Sequelize? 
",<javascript><sql><node.js><express>,205,0,1,617,1,5,3,37,116192,0.0,0,5,59,2018-06-01 14:58,2018-06-01 15:20,,0.0,,Basic,10,"<javascript><sql><node.js><express>, Sequelize Where if not null, Lets say I want to do a select command with
WHERE ID=2134
But if the user does not provide the id then it should just not bother with the WHERE ID (since it is null)
How can I handle this with Sequelize? 
","<javascript><sal><node.is><express>, sequel null, let say want select command id=2134 user proved id bother id (since null) hand sequelae?"
51784903,cross-database references are not implemented:,"I am trying to convert SQL inner join query into PostgreSQL inner join query. In this inner join query which tables are using that all tables are not present in one database. we separated tables into two databases i.e. application db and security db 
users and permission table are present in security db 
userrolemapping and department are present in application db
I tried like below but I am getting following error
Error
ERROR:  cross-database references are not implemented: ""Rockefeller_ApplicationDb.public.userrolemapping""
LINE 4:         INNER JOIN ""Rockefeller_ApplicationDb"".public.userro..
SQL Stored Function
SELECT   Department.nDeptID 
    FROM Users INNER JOIN Permission 
         ON Users.nUserID = Permission.nUserID INNER JOIN UserRoleMapping
         ON Users.nUserID = UserRoleMapping.nUserID INNER JOIN Department
         ON Permission.nDeptInst = Department.nInstID
         AND  Department.nInstID = 60
    WHERE     
         Users.nUserID = 3;
PostgreSQL Stored Function
SELECT dep.ndept_id 
        FROM ""Rockefeller_SecurityDb"".public.users as  u 
        INNER JOIN  ""Rockefeller_SecurityDb"".public.permissions p ON u.nuser_id = p.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.userrolemapping as urm ON u.nuser_id = urm.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.department dep ON p.ndept_inst = dep.ninst_id
           AND  dep.ninst_id = 60
                        WHERE     
                            u.nuser_id = 3;
",<postgresql>,1490,0,18,1379,4,29,60,57,147816,0.0,68,5,58,2018-08-10 10:52,2018-08-10 12:43,,0.0,,Basic,10,"<postgresql>, cross-database references are not implemented:, I am trying to convert SQL inner join query into PostgreSQL inner join query. In this inner join query which tables are using that all tables are not present in one database. we separated tables into two databases i.e. application db and security db 
users and permission table are present in security db 
userrolemapping and department are present in application db
I tried like below but I am getting following error
Error
ERROR:  cross-database references are not implemented: ""Rockefeller_ApplicationDb.public.userrolemapping""
LINE 4:         INNER JOIN ""Rockefeller_ApplicationDb"".public.userro..
SQL Stored Function
SELECT   Department.nDeptID 
    FROM Users INNER JOIN Permission 
         ON Users.nUserID = Permission.nUserID INNER JOIN UserRoleMapping
         ON Users.nUserID = UserRoleMapping.nUserID INNER JOIN Department
         ON Permission.nDeptInst = Department.nInstID
         AND  Department.nInstID = 60
    WHERE     
         Users.nUserID = 3;
PostgreSQL Stored Function
SELECT dep.ndept_id 
        FROM ""Rockefeller_SecurityDb"".public.users as  u 
        INNER JOIN  ""Rockefeller_SecurityDb"".public.permissions p ON u.nuser_id = p.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.userrolemapping as urm ON u.nuser_id = urm.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.department dep ON p.ndept_inst = dep.ninst_id
           AND  dep.ninst_id = 60
                        WHERE     
                            u.nuser_id = 3;
","<postgresql>, cross-database refer implements:, try convert sal inner join query postgresql inner join query. inner join query table use table present one database. spear table two database i.e. applied do secure do user permits table present secure do userrolemap depart present applied do try like get follow error error error: cross-database refer implements: ""rockefeller_applicationdb.public.userrolemapping"" line 4: inner join ""rockefeller_applicationdb"".public.user.. sal store function select department.ndeptid user inner join permits users.nuserid = permission.nuserid inner join userrolemap users.nuserid = userrolemapping.nuserid inner join depart permission.ndeptinst = department.ninstid department.ninstid = 60 users.nuserid = 3; postgresql store function select de.ndept_id ""rockefeller_securitydb"".public.us u inner join ""rockefeller_securitydb"".public.permits p u.nuser_id = p.nuser_id inner join ""rockefeller_applicationdb"".public.userrolemap arm u.nuser_id = arm.nuser_id inner join ""rockefeller_applicationdb"".public.depart de p.ndept_inst = de.ninst_id de.ninst_id = 60 u.nuser_id = 3;"
62987154,MySQL won't start - error: su: warning: cannot change directory to /nonexistent: No such file or directory,"New to development &amp; self-teaching (thanks Covid) so this could be sloppy :( sorry...
let me start off by saying I don't care about the data in the database - if it is easier to wipe it and start fresh, I'm good with that (don't know how to do that but I'm ok with  it)
Not sure what caused the issue but one day MySQL wouldn't start. Using service MySQL Restart fixed it... two days later it happened again with this error
sarcasticsnark@LB-HP-LT:~/Projects/FMS$ sudo service mysql start
 * Starting MySQL database server mysqld
su: warning: cannot change directory to /nonexistent: No such file or directory
I've tried a bit of &quot;solutions&quot;
I've tried restarting MySQL
I gave myself file permissions to the mysql files (then attempted to reverse that)
I've moved the MySQL directory (then reversed it - hence the copy of the folder &quot;mysql&quot; named &quot;mysql2&quot; below)
My files now look like this and I'm not sure I got the permissions quite right.
sarcasticsnark@LB-HP-LT:/var/lib$ ls
AccountsService  command-not-found  fwupd            logrotate  mysql          mysql2,   private  systemd                  ucf                  usbutils
PackageKit       dbus               git              man-db     mysql-files    pam       python   tpm                      unattended-upgrades  vim
apt              dhcp               initramfs-tools  mecab      mysql-keyring  plymouth  snapd    ubuntu-advantage         update-manager
boltd            dpkg               landscape        misc       mysql-upgrade  polkit-1  sudo     ubuntu-release-upgrader  update-notifier
sarcasticsnark@LB-HP-LT:/var/lib$ cd mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ ls
'#ib_16384_0.dblwr'   TestingGround_development   binlog.000009   binlog.000013   binlog.000017   client-cert.pem   mysql.ibd            server-cert.pem   undo_002
'#ib_16384_1.dblwr'   TestingGround_test          binlog.000010   binlog.000014   binlog.index    client-key.pem    performance_schema   server-key.pem
'#innodb_temp'        auto.cnf                    binlog.000011   binlog.000015   ca-key.pem      debian-5.7.flag   private_key.pem      sys
 FMS_development      binlog.000008               binlog.000012   binlog.000016   ca.pem          mysql             public_key.pem       undo_001
I've re-initialized MySQL (when not running sudoku it errors the below)
2020-07-20T02:29:41.520132Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 4096 (requested 8161)
2020-07-20T02:29:41.520141Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 1967 (requested 4000)
2020-07-20T02:29:41.520561Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) initializing of server in progress as process 2570
2020-07-20T02:29:41.522888Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting.
2020-07-20T02:29:41.522921Z 0 [ERROR] [MY-010187] [Server] Could not open file '/var/log/mysql/error.log' for error logging: Permission denied
2020-07-20T02:29:41.523139Z 0 [ERROR] [MY-013236] [Server] The designated data directory /var/lib/mysql/ is unusable. You can remove all files that the server added to it.
2020-07-20T02:29:41.523187Z 0 [ERROR] [MY-010119] [Server] Aborting
2020-07-20T02:29:41.523313Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.20-0ubuntu0.20.04.1)  (Ubuntu).
/var/log/mysql - does exist and the permissions for it are:
-rw-r----- 1 mysql adm 62273 Jul 19 19:36 error.log
Here is mysql/error.log
2020-07-20T01:50:07.952988Z mysqld_safe Logging to '/var/log/mysql/error.log'.
2020-07-20T01:50:07.986416Z mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
2020-07-20T01:50:08.000603Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 1024 (requested 8161)
2020-07-20T01:50:08.000610Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 431 (requested 4000)
2020-07-20T01:50:08.262922Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) starting as process 1608
2020-07-20T01:50:08.281623Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-07-20T01:50:08.322464Z 1 [ERROR] [MY-012592] [InnoDB] Operating system error number 2 in a file operation.
2020-07-20T01:50:08.322818Z 1 [ERROR] [MY-012593] [InnoDB] The error means the system cannot find the path specified.
2020-07-20T01:50:08.322947Z 1 [ERROR] [MY-012594] [InnoDB] If you are installing InnoDB, remember that you must create directories yourself, InnoDB does not create them.
2020-07-20T01:50:08.323017Z 1 [ERROR] [MY-012646] [InnoDB] File ./ibdata1: 'open' returned OS error 71. Cannot continue operation
2020-07-20T01:50:08.323105Z 1 [ERROR] [MY-012981] [InnoDB] Cannot continue operation.
2020-07-20T01:50:08.972320Z mysqld_safe mysqld from pid file /var/lib/mysql/LB-HP-LT.pid ended
And the permissions for /var/lib/mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ cd /var/lib
sarcasticsnark@LB-HP-LT:/var/lib$ sudo ls -l mysql
[sudo] password for sarcasticsnark: 
total 58048
-rw-r----- 1 mysql mysql   196608 Jul 19 16:34 '#ib_16384_0.dblwr'
-rw-r----- 1 mysql mysql  8585216 Jul 11 22:54 '#ib_16384_1.dblwr'
drwxr-x--- 2 mysql mysql     4096 Jul 19 16:35 '#innodb_temp'
drwxr-x--- 2 mysql mysql     4096 Jul 15 18:06  FMS_development
drwxr-x--- 2 mysql mysql     4096 Jun 20 09:04  TestingGround_development
drwxr-x--- 2 mysql mysql     4096 Jun 22 20:07  TestingGround_test
-rw-r----- 1 mysql mysql       56 Jun 10 17:43  auto.cnf
-rw-r----- 1 mysql mysql   210461 Jul 15 17:01  binlog.000008
-rw-r----- 1 mysql mysql      179 Jul 15 17:30  binlog.000009
-rw-r----- 1 mysql mysql      156 Jul 15 17:43  binlog.000010
-rw-r----- 1 mysql mysql     2798 Jul 19 15:55  binlog.000011
-rw-r----- 1 mysql mysql      179 Jul 19 15:56  binlog.000012
-rw-r----- 1 mysql mysql      179 Jul 19 16:11  binlog.000013
-rw-r----- 1 mysql mysql      179 Jul 19 16:25  binlog.000014
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000015
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000016
-rw-r----- 1 mysql mysql      179 Jul 19 16:34  binlog.000017
-rw-r----- 1 mysql mysql      160 Jul 19 16:27  binlog.index
-rw------- 1 mysql mysql     1680 Jun 10 17:43  ca-key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  ca.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  client-cert.pem
-rw------- 1 mysql mysql     1680 Jun 10 17:43  client-key.pem
-rw-r--r-- 1 mysql mysql        0 Jun 12 15:54  debian-5.7.flag
drwxr-xr-x 2 mysql mysql     4096 Jun 10 17:43  mysql
-rw-r----- 1 mysql mysql 25165824 Jul 19 16:28  mysql.ibd
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  performance_schema
-rw------- 1 mysql mysql     1680 Jun 10 17:43  private_key.pem
-rw-r--r-- 1 mysql mysql      452 Jun 10 17:43  public_key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  server-cert.pem
-rw------- 1 mysql mysql     1676 Jun 10 17:43  server-key.pem
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  sys
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_001
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_002
",<mysql><linux>,7120,0,72,593,1,4,5,49,61256,0.0,0,1,58,2020-07-20 0:37,2020-07-22 18:12,2020-07-22 18:12,2.0,2.0,Basic,14,"<mysql><linux>, MySQL won't start - error: su: warning: cannot change directory to /nonexistent: No such file or directory, New to development &amp; self-teaching (thanks Covid) so this could be sloppy :( sorry...
let me start off by saying I don't care about the data in the database - if it is easier to wipe it and start fresh, I'm good with that (don't know how to do that but I'm ok with  it)
Not sure what caused the issue but one day MySQL wouldn't start. Using service MySQL Restart fixed it... two days later it happened again with this error
sarcasticsnark@LB-HP-LT:~/Projects/FMS$ sudo service mysql start
 * Starting MySQL database server mysqld
su: warning: cannot change directory to /nonexistent: No such file or directory
I've tried a bit of &quot;solutions&quot;
I've tried restarting MySQL
I gave myself file permissions to the mysql files (then attempted to reverse that)
I've moved the MySQL directory (then reversed it - hence the copy of the folder &quot;mysql&quot; named &quot;mysql2&quot; below)
My files now look like this and I'm not sure I got the permissions quite right.
sarcasticsnark@LB-HP-LT:/var/lib$ ls
AccountsService  command-not-found  fwupd            logrotate  mysql          mysql2,   private  systemd                  ucf                  usbutils
PackageKit       dbus               git              man-db     mysql-files    pam       python   tpm                      unattended-upgrades  vim
apt              dhcp               initramfs-tools  mecab      mysql-keyring  plymouth  snapd    ubuntu-advantage         update-manager
boltd            dpkg               landscape        misc       mysql-upgrade  polkit-1  sudo     ubuntu-release-upgrader  update-notifier
sarcasticsnark@LB-HP-LT:/var/lib$ cd mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ ls
'#ib_16384_0.dblwr'   TestingGround_development   binlog.000009   binlog.000013   binlog.000017   client-cert.pem   mysql.ibd            server-cert.pem   undo_002
'#ib_16384_1.dblwr'   TestingGround_test          binlog.000010   binlog.000014   binlog.index    client-key.pem    performance_schema   server-key.pem
'#innodb_temp'        auto.cnf                    binlog.000011   binlog.000015   ca-key.pem      debian-5.7.flag   private_key.pem      sys
 FMS_development      binlog.000008               binlog.000012   binlog.000016   ca.pem          mysql             public_key.pem       undo_001
I've re-initialized MySQL (when not running sudoku it errors the below)
2020-07-20T02:29:41.520132Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 4096 (requested 8161)
2020-07-20T02:29:41.520141Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 1967 (requested 4000)
2020-07-20T02:29:41.520561Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) initializing of server in progress as process 2570
2020-07-20T02:29:41.522888Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting.
2020-07-20T02:29:41.522921Z 0 [ERROR] [MY-010187] [Server] Could not open file '/var/log/mysql/error.log' for error logging: Permission denied
2020-07-20T02:29:41.523139Z 0 [ERROR] [MY-013236] [Server] The designated data directory /var/lib/mysql/ is unusable. You can remove all files that the server added to it.
2020-07-20T02:29:41.523187Z 0 [ERROR] [MY-010119] [Server] Aborting
2020-07-20T02:29:41.523313Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.20-0ubuntu0.20.04.1)  (Ubuntu).
/var/log/mysql - does exist and the permissions for it are:
-rw-r----- 1 mysql adm 62273 Jul 19 19:36 error.log
Here is mysql/error.log
2020-07-20T01:50:07.952988Z mysqld_safe Logging to '/var/log/mysql/error.log'.
2020-07-20T01:50:07.986416Z mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
2020-07-20T01:50:08.000603Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 1024 (requested 8161)
2020-07-20T01:50:08.000610Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 431 (requested 4000)
2020-07-20T01:50:08.262922Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) starting as process 1608
2020-07-20T01:50:08.281623Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-07-20T01:50:08.322464Z 1 [ERROR] [MY-012592] [InnoDB] Operating system error number 2 in a file operation.
2020-07-20T01:50:08.322818Z 1 [ERROR] [MY-012593] [InnoDB] The error means the system cannot find the path specified.
2020-07-20T01:50:08.322947Z 1 [ERROR] [MY-012594] [InnoDB] If you are installing InnoDB, remember that you must create directories yourself, InnoDB does not create them.
2020-07-20T01:50:08.323017Z 1 [ERROR] [MY-012646] [InnoDB] File ./ibdata1: 'open' returned OS error 71. Cannot continue operation
2020-07-20T01:50:08.323105Z 1 [ERROR] [MY-012981] [InnoDB] Cannot continue operation.
2020-07-20T01:50:08.972320Z mysqld_safe mysqld from pid file /var/lib/mysql/LB-HP-LT.pid ended
And the permissions for /var/lib/mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ cd /var/lib
sarcasticsnark@LB-HP-LT:/var/lib$ sudo ls -l mysql
[sudo] password for sarcasticsnark: 
total 58048
-rw-r----- 1 mysql mysql   196608 Jul 19 16:34 '#ib_16384_0.dblwr'
-rw-r----- 1 mysql mysql  8585216 Jul 11 22:54 '#ib_16384_1.dblwr'
drwxr-x--- 2 mysql mysql     4096 Jul 19 16:35 '#innodb_temp'
drwxr-x--- 2 mysql mysql     4096 Jul 15 18:06  FMS_development
drwxr-x--- 2 mysql mysql     4096 Jun 20 09:04  TestingGround_development
drwxr-x--- 2 mysql mysql     4096 Jun 22 20:07  TestingGround_test
-rw-r----- 1 mysql mysql       56 Jun 10 17:43  auto.cnf
-rw-r----- 1 mysql mysql   210461 Jul 15 17:01  binlog.000008
-rw-r----- 1 mysql mysql      179 Jul 15 17:30  binlog.000009
-rw-r----- 1 mysql mysql      156 Jul 15 17:43  binlog.000010
-rw-r----- 1 mysql mysql     2798 Jul 19 15:55  binlog.000011
-rw-r----- 1 mysql mysql      179 Jul 19 15:56  binlog.000012
-rw-r----- 1 mysql mysql      179 Jul 19 16:11  binlog.000013
-rw-r----- 1 mysql mysql      179 Jul 19 16:25  binlog.000014
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000015
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000016
-rw-r----- 1 mysql mysql      179 Jul 19 16:34  binlog.000017
-rw-r----- 1 mysql mysql      160 Jul 19 16:27  binlog.index
-rw------- 1 mysql mysql     1680 Jun 10 17:43  ca-key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  ca.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  client-cert.pem
-rw------- 1 mysql mysql     1680 Jun 10 17:43  client-key.pem
-rw-r--r-- 1 mysql mysql        0 Jun 12 15:54  debian-5.7.flag
drwxr-xr-x 2 mysql mysql     4096 Jun 10 17:43  mysql
-rw-r----- 1 mysql mysql 25165824 Jul 19 16:28  mysql.ibd
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  performance_schema
-rw------- 1 mysql mysql     1680 Jun 10 17:43  private_key.pem
-rw-r--r-- 1 mysql mysql      452 Jun 10 17:43  public_key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  server-cert.pem
-rw------- 1 mysql mysql     1676 Jun 10 17:43  server-key.pem
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  sys
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_001
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_002
","<myself><line>, myself start - error: s: warning: cannot change director /nonexistent: file directory, new develop &amp; self-teach (thank could) could slope :( sorry... let start say care data database - easier wipe start fresh, i'm good (don't know i'm ok it) sure cause issue one day myself start. use service myself start fix it... two day later happen error sarcasticsnark@lb-he-it:~/projects/ms$ so service myself start * start myself database server myself s: warning: cannot change director /nonexistent: file director i'v try bit &quit;solutions&quit; i'v try start myself gave file permits myself file (then attempt rivers that) i'v move myself director (then rivers - hence copy older &quit;myself&quit; name &quit;myself&quit; below) file look like i'm sure got permits quit right. sarcasticsnark@lb-he-it:/war/limb$ is accountsservic command-not-found found forgot myself myself, privat system cf usbutil packagekit du git man-do myself-fig am patron tm unattended-upgrade him apt deep initramfs-tool mean myself-key plymouth snap bunt-advantage update-manage bold dog landscape miss myself-upgrade pocket-1 so bunt-release-upgrade update-notify sarcasticsnark@lb-he-it:/war/limb$ d myself sarcasticsnark@lb-he-it:/war/limb/myself$ is '#ib_16384_0.blur' testingground_develop billon.000009 billon.000013 billon.000017 client-cent.per myself.bid server-cent.per undo_002 '#ib_16384_1.blur' testingground_test billon.000010 billon.000014 billon.index client-key.per performance_schema server-key.per '#innodb_temp' auto.cf billon.000011 billon.000015 ca-key.per median-5.7.flag private_key.per by fms_develop billon.000008 billon.000012 billon.000016 ca.per myself public_key.per undo_001 i'v re-into myself (when run sudoku error below) 2020-07-20t02:29:41.520132z 0 [warning] [my-010139] [server] change limits: max_open_files: 4096 (request 8161) 2020-07-20t02:29:41.520141z 0 [warning] [my-010142] [server] change limits: table_open_cache: 1967 (request 4000) 2020-07-20t02:29:41.520561z 0 [system] [my-013169] [server] /us/skin/myself (myself 8.0.20-0ubuntu0.20.04.1) into server progress process 2570 2020-07-20t02:29:41.522888z 0 [error] [my-010457] [server] --into specific data director file it. adopting. 2020-07-20t02:29:41.522921z 0 [error] [my-010187] [server] could open file '/war/log/myself/error.log' error logging: permits den 2020-07-20t02:29:41.523139z 0 [error] [my-013236] [server] design data director /war/limb/myself/ unable. remove file server ad it. 2020-07-20t02:29:41.523187z 0 [error] [my-010119] [server] abort 2020-07-20t02:29:41.523313z 0 [system] [my-010910] [server] /us/skin/myself: shutdown complete (myself 8.0.20-0ubuntu0.20.04.1) (bunt). /war/log/myself - exist permits are: -re-r----- 1 myself am 62273 july 19 19:36 error.log myself/error.log 2020-07-20t01:50:07.952988z mysqld_saf log '/war/log/myself/error.log'. 2020-07-20t01:50:07.986416z mysqld_saf start myself demon database /war/limb/myself 2020-07-20t01:50:08.000603z 0 [warning] [my-010139] [server] change limits: max_open_files: 1024 (request 8161) 2020-07-20t01:50:08.000610z 0 [warning] [my-010142] [server] change limits: table_open_cache: 431 (request 4000) 2020-07-20t01:50:08.262922z 0 [system] [my-010116] [server] /us/skin/myself (myself 8.0.20-0ubuntu0.20.04.1) start process 1608 2020-07-20t01:50:08.281623z 1 [system] [my-013576] [innodb] innodb into started. 2020-07-20t01:50:08.322464z 1 [error] [my-012592] [innodb] over system error number 2 file operation. 2020-07-20t01:50:08.322818z 1 [error] [my-012593] [innodb] error mean system cannot find path specified. 2020-07-20t01:50:08.322947z 1 [error] [my-012594] [innodb] instal innodb, remember must great director yourself, innodb great them. 2020-07-20t01:50:08.323017z 1 [error] [my-012646] [innodb] file ./ibdata1: 'open' return os error 71. cannot continue over 2020-07-20t01:50:08.323105z 1 [error] [my-012981] [innodb] cannot continue operation. 2020-07-20t01:50:08.972320z mysqld_saf myself did file /war/limb/myself/lb-he-it.did end permits /war/limb/myself sarcasticsnark@lb-he-it:/war/limb/myself$ d /war/limb sarcasticsnark@lb-he-it:/war/limb$ so is -l myself [so] password sarcasticsnark: total 58048 -re-r----- 1 myself myself 196608 july 19 16:34 '#ib_16384_0.blur' -re-r----- 1 myself myself 8585216 july 11 22:54 '#ib_16384_1.blur' drawer-x--- 2 myself myself 4096 july 19 16:35 '#innodb_temp' drawer-x--- 2 myself myself 4096 july 15 18:06 fms_develop drawer-x--- 2 myself myself 4096 run 20 09:04 testingground_develop drawer-x--- 2 myself myself 4096 run 22 20:07 testingground_test -re-r----- 1 myself myself 56 run 10 17:43 auto.cf -re-r----- 1 myself myself 210461 july 15 17:01 billon.000008 -re-r----- 1 myself myself 179 july 15 17:30 billon.000009 -re-r----- 1 myself myself 156 july 15 17:43 billon.000010 -re-r----- 1 myself myself 2798 july 19 15:55 billon.000011 -re-r----- 1 myself myself 179 july 19 15:56 billon.000012 -re-r----- 1 myself myself 179 july 19 16:11 billon.000013 -re-r----- 1 myself myself 179 july 19 16:25 billon.000014 -re-r----- 1 myself myself 179 july 19 16:27 billon.000015 -re-r----- 1 myself myself 179 july 19 16:27 billon.000016 -re-r----- 1 myself myself 179 july 19 16:34 billon.000017 -re-r----- 1 myself myself 160 july 19 16:27 billon.index -re------- 1 myself myself 1680 run 10 17:43 ca-key.per -re-r--r-- 1 myself myself 1112 run 10 17:43 ca.per -re-r--r-- 1 myself myself 1112 run 10 17:43 client-cent.per -re------- 1 myself myself 1680 run 10 17:43 client-key.per -re-r--r-- 1 myself myself 0 run 12 15:54 median-5.7.flag drawer-or-x 2 myself myself 4096 run 10 17:43 myself -re-r----- 1 myself myself 25165824 july 19 16:28 myself.bid drawer-x--- 2 myself myself 4096 run 10 17:43 performance_schema -re------- 1 myself myself 1680 run 10 17:43 private_key.per -re-r--r-- 1 myself myself 452 run 10 17:43 public_key.per -re-r--r-- 1 myself myself 1112 run 10 17:43 server-cent.per -re------- 1 myself myself 1676 run 10 17:43 server-key.per drawer-x--- 2 myself myself 4096 run 10 17:43 by -re-r----- 1 myself myself 12582912 july 19 16:34 undo_001 -re-r----- 1 myself myself 12582912 july 19 16:34 undo_002"
59432964,Relational Data Model for Double-Entry Accounting,"Assume there is a bank, a large shop, etc, that wants the accounting to be done correctly, for both internal accounts, and keeping track of customer accounts.  Rather than implementing that which satisfies the current simple and narrow requirement, which would a 'home brew': those turn out to be a temporary crutch for the current simple requirement, and difficult or impossible to extend when new requirements come it.
As I understand it, Double-Entry Accounting is a method that is well-established, and serves all Accounting and Audit requirements, including those that are not contemplated at the current moment.  If that is implemented, it would:
eliminate the incremental enhancements that would occur over time, and the expense,
there will not be a need for future enhancement.
I have studied this Answer to another question: Derived account balance vs stored account balance for a simple bank account?, it provides good information, for internal Accounts.  A data model is required, so that one can understand the entities; their interaction; their relations, and @PerformanceDBA has given that.  This model is taken from that Answer:
Whereas that is satisfactory for simple internal accounts, I need to see a data model that provides the full Double-Entry Accounting method. 
The articles are need to be added are Journal; internal vs external Transactions; etc..
Ideally I would like to see what those double entry rows look like in database terms, what the whole process will look like in SQL, which entities are affected in each case, etc.  Cases like:
A Client deposits cash to his account
The Bank charges fees once a month to all Clients accounts (sample batch job),
A Client does some operation over the counter, and the Bank charges a fee (cash withdrawal + withdrawal fee),
Mary sends some money from her account, to John's account, which is in the same bank
Let's just call it System instead of Bank, Bank may be  too complex to model, and let the question be about imaginary system which operates with accounts and assets. Customers perform a set of operations with system (deposits, withdrawals, fee for latter, batch fees), and with each other (transfer).
",<sql><database><database-design><relational-database><accounting>,2179,3,5,982,1,10,16,58,40397,0.0,11,2,57,2019-12-21 1:54,2019-12-24 7:11,2019-12-24 7:11,3.0,3.0,Basic,4,"<sql><database><database-design><relational-database><accounting>, Relational Data Model for Double-Entry Accounting, Assume there is a bank, a large shop, etc, that wants the accounting to be done correctly, for both internal accounts, and keeping track of customer accounts.  Rather than implementing that which satisfies the current simple and narrow requirement, which would a 'home brew': those turn out to be a temporary crutch for the current simple requirement, and difficult or impossible to extend when new requirements come it.
As I understand it, Double-Entry Accounting is a method that is well-established, and serves all Accounting and Audit requirements, including those that are not contemplated at the current moment.  If that is implemented, it would:
eliminate the incremental enhancements that would occur over time, and the expense,
there will not be a need for future enhancement.
I have studied this Answer to another question: Derived account balance vs stored account balance for a simple bank account?, it provides good information, for internal Accounts.  A data model is required, so that one can understand the entities; their interaction; their relations, and @PerformanceDBA has given that.  This model is taken from that Answer:
Whereas that is satisfactory for simple internal accounts, I need to see a data model that provides the full Double-Entry Accounting method. 
The articles are need to be added are Journal; internal vs external Transactions; etc..
Ideally I would like to see what those double entry rows look like in database terms, what the whole process will look like in SQL, which entities are affected in each case, etc.  Cases like:
A Client deposits cash to his account
The Bank charges fees once a month to all Clients accounts (sample batch job),
A Client does some operation over the counter, and the Bank charges a fee (cash withdrawal + withdrawal fee),
Mary sends some money from her account, to John's account, which is in the same bank
Let's just call it System instead of Bank, Bank may be  too complex to model, and let the question be about imaginary system which operates with accounts and assets. Customers perform a set of operations with system (deposits, withdrawals, fee for latter, batch fees), and with each other (transfer).
","<sal><database><database-design><relations-database><accounting>, relate data model double-entry accounting, assume bank, large shop, etc, want account done correctly, inter accounts, keep track custom accounts. rather implement satisfy current simple narrow requirement, would 'home grew': turn temporary crutch current simple requirement, difficult impose extend new require come it. understand it, double-entry account method well-established, serve account audit requirements, include contempt current moment. implements, would: limit incitement enhance would occur time, expense, need future enhancement. study answer not question: derive account balance vs store account balance simple bank account?, proved good information, inter accounts. data model required, one understand entitles; interaction; relations, @performancedba given that. model taken answer: where satisfactory simple inter accounts, need see data model proved full double-entry account method. article need ad journal; inter vs externa transactions; etc.. ideal would like see doubt entry row look like database terms, whole process look like sal, entity affect case, etc. case like: client deposit cash account bank charge fee month client account (sample batch job), client over counter, bank charge fee (cash withdraw + withdraw fee), mary send money account, john' account, bank let' call system instead bank, bank may complex model, let question imaginary system over account asset. custom perform set over system (deposits, withdrawal, fee latter, batch fees), (transfer)."
48225233,"Gem::LoadError: can't activate pg (~> 0.18), already activated pg-1.0.0","I've been doing the Rails tutorial found here and have been successful up to the point of having to migrate the Comments migration using $ rails db:migrate. Prior to this point, I've been able to generate the Article model and migrate the Articles create migration with no issues. In between these two migrations, nothing has changed in my Gemfile, so I'm not sure what it is Bundler is having an issue with. 
Here are the errors, followed by the full command-line output, along with my Gemfile and schema.rb:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0.
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
Full command-line output
xxx:gangelo: ~/dev/rails/test/blog (master*) ☠  rbenv exec rails db:migrate
rails aborted!
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:188:in `rescue in spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:185:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Caused by:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0. Make sure all dependencies are added to Gemfile.
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/postgresql_adapter.rb:2:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:186:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Tasks: TOP =&gt; db:migrate =&gt; db:load_config
(See full trace by running task with --trace)
Gemfile
source 'https://rubygems.org'
ruby '2.3.1'
git_source(:github) do |repo_name|
  repo_name = ""#{repo_name}/#{repo_name}"" unless repo_name.include?(""/"")
  ""https://github.com/#{repo_name}.git""
end
# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'
gem 'rails', '~&gt; 5.1.4'
# Use sqlite3 as the database for Active Record
# gem 'sqlite3'
# Use postgres as the database for Active Record
gem 'pg'
# Use Puma as the app server
gem 'puma', '~&gt; 3.7'
# Use SCSS for stylesheets
gem 'sass-rails', '~&gt; 5.0'
# Use Uglifier as compressor for JavaScript assets
gem 'uglifier', '&gt;= 1.3.0'
# See https://github.com/rails/execjs#readme for more supported runtimes
# gem 'therubyracer', platforms: :ruby
# Use CoffeeScript for .coffee assets and views
gem 'coffee-rails', '~&gt; 4.2'
# Turbolinks makes navigating your web application faster. Read more: https://github.com/turbolinks/turbolinks
gem 'turbolinks', '~&gt; 5'
# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder
gem 'jbuilder', '~&gt; 2.5'
# Use Redis adapter to run Action Cable in production
# gem 'redis', '~&gt; 3.0'
# Use ActiveModel has_secure_password
# gem 'bcrypt', '~&gt; 3.1.7'
# Use Capistrano for deployment
# gem 'capistrano-rails', group: :development
group :development, :test do
  # Call 'byebug' anywhere in the code to stop execution and get a debugger console
  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]
  # Adds support for Capybara system testing and selenium driver
  gem 'capybara', '~&gt; 2.13'
  gem 'selenium-webdriver'
end
# gma - start
group :development, :test do
  gem 'rspec-rails', '~&gt; 3.5', '&gt;= 3.5.2'
  gem 'rspec-activemodel-mocks', '~&gt; 1.0', '&gt;= 1.0.3'
  gem 'shoulda-matchers', '~&gt; 3.1', '&gt;= 3.1.1'
  gem 'factory_bot_rails', '~&gt; 4.8', '&gt;= 4.8.2'
  gem 'ffaker', '~&gt; 2.2'
  # gem 'timecop', '~&gt; 0.8.1'
end
# gma - end
group :development do
  # Access an IRB console on exception pages or by using &lt;%= console %&gt; anywhere in the code.
  gem 'web-console', '&gt;= 3.3.0'
  gem 'listen', '&gt;= 3.0.5', '&lt; 3.2'
  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring
  gem 'spring'
  gem 'spring-watcher-listen', '~&gt; 2.0.0'
end
# Windows does not include zoneinfo files, so bundle the tzinfo-data gem
gem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]
Schema.rb
ActiveRecord::Schema.define(version: 20180110153949) do
  # These are extensions that must be enabled in order to support this database
  enable_extension ""plpgsql""
  create_table ""articles"", force: :cascade do |t|
    t.string ""title""
    t.text ""text""
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
  end
end
Migration file
class CreateComments &lt; ActiveRecord::Migration[5.1]
  def change
    create_table :comments do |t|
      t.string :commenter
      t.text :body
      t.references :article, foreign_key: true
      t.timestamps
    end
  end
end
",<ruby-on-rails><ruby><postgresql><bundler><rails-migrations>,17720,7,211,3044,4,29,44,35,16680,0.0,87,2,57,2018-01-12 11:20,2018-01-14 21:25,2018-01-18 1:54,2.0,6.0,Basic,4,"<ruby-on-rails><ruby><postgresql><bundler><rails-migrations>, Gem::LoadError: can't activate pg (~> 0.18), already activated pg-1.0.0, I've been doing the Rails tutorial found here and have been successful up to the point of having to migrate the Comments migration using $ rails db:migrate. Prior to this point, I've been able to generate the Article model and migrate the Articles create migration with no issues. In between these two migrations, nothing has changed in my Gemfile, so I'm not sure what it is Bundler is having an issue with. 
Here are the errors, followed by the full command-line output, along with my Gemfile and schema.rb:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0.
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
Full command-line output
xxx:gangelo: ~/dev/rails/test/blog (master*) ☠  rbenv exec rails db:migrate
rails aborted!
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:188:in `rescue in spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:185:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Caused by:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0. Make sure all dependencies are added to Gemfile.
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/postgresql_adapter.rb:2:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:186:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Tasks: TOP =&gt; db:migrate =&gt; db:load_config
(See full trace by running task with --trace)
Gemfile
source 'https://rubygems.org'
ruby '2.3.1'
git_source(:github) do |repo_name|
  repo_name = ""#{repo_name}/#{repo_name}"" unless repo_name.include?(""/"")
  ""https://github.com/#{repo_name}.git""
end
# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'
gem 'rails', '~&gt; 5.1.4'
# Use sqlite3 as the database for Active Record
# gem 'sqlite3'
# Use postgres as the database for Active Record
gem 'pg'
# Use Puma as the app server
gem 'puma', '~&gt; 3.7'
# Use SCSS for stylesheets
gem 'sass-rails', '~&gt; 5.0'
# Use Uglifier as compressor for JavaScript assets
gem 'uglifier', '&gt;= 1.3.0'
# See https://github.com/rails/execjs#readme for more supported runtimes
# gem 'therubyracer', platforms: :ruby
# Use CoffeeScript for .coffee assets and views
gem 'coffee-rails', '~&gt; 4.2'
# Turbolinks makes navigating your web application faster. Read more: https://github.com/turbolinks/turbolinks
gem 'turbolinks', '~&gt; 5'
# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder
gem 'jbuilder', '~&gt; 2.5'
# Use Redis adapter to run Action Cable in production
# gem 'redis', '~&gt; 3.0'
# Use ActiveModel has_secure_password
# gem 'bcrypt', '~&gt; 3.1.7'
# Use Capistrano for deployment
# gem 'capistrano-rails', group: :development
group :development, :test do
  # Call 'byebug' anywhere in the code to stop execution and get a debugger console
  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]
  # Adds support for Capybara system testing and selenium driver
  gem 'capybara', '~&gt; 2.13'
  gem 'selenium-webdriver'
end
# gma - start
group :development, :test do
  gem 'rspec-rails', '~&gt; 3.5', '&gt;= 3.5.2'
  gem 'rspec-activemodel-mocks', '~&gt; 1.0', '&gt;= 1.0.3'
  gem 'shoulda-matchers', '~&gt; 3.1', '&gt;= 3.1.1'
  gem 'factory_bot_rails', '~&gt; 4.8', '&gt;= 4.8.2'
  gem 'ffaker', '~&gt; 2.2'
  # gem 'timecop', '~&gt; 0.8.1'
end
# gma - end
group :development do
  # Access an IRB console on exception pages or by using &lt;%= console %&gt; anywhere in the code.
  gem 'web-console', '&gt;= 3.3.0'
  gem 'listen', '&gt;= 3.0.5', '&lt; 3.2'
  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring
  gem 'spring'
  gem 'spring-watcher-listen', '~&gt; 2.0.0'
end
# Windows does not include zoneinfo files, so bundle the tzinfo-data gem
gem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]
Schema.rb
ActiveRecord::Schema.define(version: 20180110153949) do
  # These are extensions that must be enabled in order to support this database
  enable_extension ""plpgsql""
  create_table ""articles"", force: :cascade do |t|
    t.string ""title""
    t.text ""text""
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
  end
end
Migration file
class CreateComments &lt; ActiveRecord::Migration[5.1]
  def change
    create_table :comments do |t|
      t.string :commenter
      t.text :body
      t.references :article, foreign_key: true
      t.timestamps
    end
  end
end
","<ruby-on-rails><ruby><postgresql><bundles><rails-migrations>, gem::loaderror: can't active pg (~> 0.18), already active pg-1.0.0, i'v rail tutor found success point migrate comment migrate use $ rail do:migrate. prior point, i'v all genet article model migrate article great migrate issues. two migrations, not change defile, i'm sure bundles issue with. errors, follow full command-in output, along email scheme.re: gem::loaderror: can't active pg (~&it; 0.18), already active pg-1.0.0. gem::loaderror: specific 'postgresql' database adapted, gem loaded. add `gem 'pg'` email (and ensue version minimum require activerecord). full command-in output xxx:angelo: ~/de/rails/test/blow (master*) ☠ bent even rail do:migrate rail adopted! gem::loaderror: specific 'postgresql' database adapted, gem loaded. add `gem 'pg'` email (and ensue version minimum require activerecord). /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/connection_specification.re:188:in `rescue speck' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/connection_specification.re:185:in `speck' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/abstract/connection_pool.re:880:in `establish_connection' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_handling.re:58:in `establish_connection' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/railtie.re:124:in `block (2 levels) &it;class:railtie&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:69:in `instance_eval' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:69:in `block execute_hook' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:60:in `with_execution_control' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:65:in `execute_hook' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:50:in `block run_load_hooks' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:49:in `each' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:49:in `run_load_hooks' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/base.re:326:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/base.re:25:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:6:in `&it;class:mysqldatabasetasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:3:in `&it;module:tasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:2:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:1:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:74:in `&it;module:databasetasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:35:in `&it;module:tasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:2:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:1:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/railtie.re:34:in `block (3 levels) &it;class:railtie&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands/rake/rake_command.re:21:in `block perform' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands/rake/rake_command.re:18:in `perform' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/command.re:46:in `invoke' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands.re:16:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/bin/rails:9:in `require' /users/angelo/de/rails/test/blow/bin/rails:9:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/rails.re:28:in `load' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/rails.re:28:in `call' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/command.re:7:in `call' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client.re:30:in `run' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/binstub.re:31:in `load' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/binstub.re:31:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/bin/spring:15:in `&it;top (required)&it;' bin/rails:3:in `load' bin/rails:3:in `&it;main&it;' cause by: gem::loaderror: can't active pg (~&it; 0.18), already active pg-1.0.0. make sure depend ad defile. /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/postgresql_adapter.re:2:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/connection_specification.re:186:in `speck' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_adapters/abstract/connection_pool.re:880:in `establish_connection' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/connection_handling.re:58:in `establish_connection' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/railtie.re:124:in `block (2 levels) &it;class:railtie&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:69:in `instance_eval' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:69:in `block execute_hook' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:60:in `with_execution_control' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:65:in `execute_hook' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:50:in `block run_load_hooks' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:49:in `each' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/lazy_load_hooks.re:49:in `run_load_hooks' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/base.re:326:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/base.re:25:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:6:in `&it;class:mysqldatabasetasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:3:in `&it;module:tasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:2:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/mysql_database_tasks.re:1:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:74:in `&it;module:databasetasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:35:in `&it;module:tasks&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:2:in `&it;module:activerecord&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/tasks/database_tasks.re:1:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `block require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:258:in `load_dependency' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activesupport-5.1.4/limb/active_support/dependencies.re:292:in `require' /users/angelo/de/rails/test/blow/vendor/bundle/gems/activerecord-5.1.4/limb/active_record/railtie.re:34:in `block (3 levels) &it;class:railtie&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands/rake/rake_command.re:21:in `block perform' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands/rake/rake_command.re:18:in `perform' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/command.re:46:in `invoke' /users/angelo/de/rails/test/blow/vendor/bundle/gems/realities-5.1.4/limb/rails/commands.re:16:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/bin/rails:9:in `require' /users/angelo/de/rails/test/blow/bin/rails:9:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/rails.re:28:in `load' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/rails.re:28:in `call' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client/command.re:7:in `call' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/client.re:30:in `run' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/binstub.re:31:in `load' /users/angelo/de/rails/test/blow/vendor/bundle/gems/spring-2.0.2/limb/spring/binstub.re:31:in `&it;top (required)&it;' /users/angelo/de/rails/test/blow/bin/spring:15:in `&it;top (required)&it;' bin/rails:3:in `load' bin/rails:3:in `&it;main&it;' tasks: top =&it; do:migrate =&it; do:load_config (see full trace run task --trace) email source 'http://rubygems.org' ruby '2.3.1' git_source(:github) |repo_name| repo_nam = ""#{repo_name}/#{repo_name}"" unless repo_name.include?(""/"") ""http://github.com/#{repo_name}.git"" end # bundle edge rail instead: gem 'rails', github: 'rails/rails' gem 'rails', '~&it; 5.1.4' # use sqlite3 database active record # gem 'sqlite3' # use poster database active record gem 'pg' # use pump pp server gem 'pump', '~&it; 3.7' # use less stylesheet gem 'pass-rails', '~&it; 5.0' # use uglier compression javascript asset gem 'uglier', '&it;= 1.3.0' # see http://github.com/rails/excess#ready support until # gem 'therubyracer', platforms: :ruby # use coffeescript .coffee asset view gem 'coffee-rails', '~&it; 4.2' # turbolink make having web applied faster. read more: http://github.com/turbolinks/turbolink gem 'turbolinks', '~&it; 5' # build son apt ease. read more: http://github.com/rails/build gem 'builder', '~&it; 2.5' # use red adapt run action call product # gem 'red', '~&it; 3.0' # use activemodel has_secure_password # gem 'crept', '~&it; 3.1.7' # use capistrano deploy # gem 'capistrano-rails', group: :develop group :development, :test # call 'byebug' anywhere code stop execute get deluge console gem 'byebug', platforms: [:mr, :mind, :x64_mingw] # add support capybara system test selenium driver gem 'capybara', '~&it; 2.13' gem 'selenium-webdriver' end # ma - start group :development, :test gem 'respect-rails', '~&it; 3.5', '&it;= 3.5.2' gem 'respect-activemodel-locks', '~&it; 1.0', '&it;= 1.0.3' gem 'should-matches', '~&it; 3.1', '&it;= 3.1.1' gem 'factory_bot_rails', '~&it; 4.8', '&it;= 4.8.2' gem 'baker', '~&it; 2.2' # gem 'timecop', '~&it; 0.8.1' end # ma - end group :develop # access rib console except page use &it;%= console %&it; anywhere code. gem 'web-console', '&it;= 3.3.0' gem 'listen', '&it;= 3.0.5', '&it; 3.2' # spring speed develop keep applied run background. read more: http://github.com/rails/sir gem 'spring' gem 'spring-watched-listen', '~&it; 2.0.0' end # window include zoneinfo files, bundle tzinfo-data gem gem 'tzinfo-data', platforms: [:mind, :skin, :x64_mingw, :ruby] scheme.re activerecord::scheme.define(version: 20180110153949) # extent must enable order support database enable_extens ""plpgsql"" greatest ""articles"", force: :cascade |t| t.string ""title"" t.text ""text"" t.datetim ""created_at"", null: fall t.datetim ""updated_at"", null: fall end end migrate file class createcom &it; activerecord::migration[5.1] def change greatest :comment |t| t.string :comment t.text :body t.refer :article, foreign_key: true t.timestamp end end end"
53293349,Azure data studio schema diagram?,"I just recently downloaded Azure Data Studio with SQL Server Express since I'm using Linux .  Is there an entity-relationship diagramming feature, kind of how SQL Server Management Studio has a database diagram feature?  I want to visually see the relationships with tables in a database if possible.
",<sql><database><entity-relationship><diagram><azure-data-studio>,301,0,0,581,1,4,5,68,52584,0.0,1,7,57,2018-11-14 4:48,2019-04-02 8:32,,139.0,,Basic,7,"<sql><database><entity-relationship><diagram><azure-data-studio>, Azure data studio schema diagram?, I just recently downloaded Azure Data Studio with SQL Server Express since I'm using Linux .  Is there an entity-relationship diagramming feature, kind of how SQL Server Management Studio has a database diagram feature?  I want to visually see the relationships with tables in a database if possible.
","<sal><database><entity-relationship><diagram><azure-data-studio>, azur data studio scheme diagram?, recent download azur data studio sal server express since i'm use line . entity-relationship diagram feature, kind sal server manage studio database diagram feature? want visual see relationship table database possible."
50589064,Get unique values using STRING_AGG in SQL Server,"The following query returns the results shown below:
SELECT 
    ProjectID, newID.value
FROM 
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2') 
Results:
ProjectID   value
---------------------
2           Q96NY7-2
2           O95833
2           O95833
2           Q96NY7-2
2           O95833
2           Q96NY7-2
4           Q96NY7-2
4           Q96NY7-2
Using the newly added STRING_AGG function (in SQL Server 2017) as it is shown in the following query I am able to get the result-set below.
SELECT 
    ProjectID,
    STRING_AGG( newID.value, ',') WITHIN GROUP (ORDER BY newID.value) AS 
NewField
FROM
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2')  
GROUP BY 
    ProjectID
ORDER BY 
    ProjectID
Results:
ProjectID   NewField
-------------------------------------------------------------
2           O95833,O95833,O95833,Q96NY7-2,Q96NY7-2,Q96NY7-2
4           Q96NY7-2,Q96NY7-2
I would like my final output to have only unique elements as below:
ProjectID   NewField
-------------------------------
2           O95833, Q96NY7-2
4           Q96NY7-2
Any suggestions about how to get this result? Please feel free to refine/redesign from scratch my query if needed.
",<sql><sql-server><sql-server-2017><string-aggregation>,1341,0,41,1107,1,10,19,63,130478,0.0,57,8,57,2018-05-29 16:33,2018-05-29 16:43,2018-05-29 16:43,0.0,0.0,Basic,10,"<sql><sql-server><sql-server-2017><string-aggregation>, Get unique values using STRING_AGG in SQL Server, The following query returns the results shown below:
SELECT 
    ProjectID, newID.value
FROM 
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2') 
Results:
ProjectID   value
---------------------
2           Q96NY7-2
2           O95833
2           O95833
2           Q96NY7-2
2           O95833
2           Q96NY7-2
4           Q96NY7-2
4           Q96NY7-2
Using the newly added STRING_AGG function (in SQL Server 2017) as it is shown in the following query I am able to get the result-set below.
SELECT 
    ProjectID,
    STRING_AGG( newID.value, ',') WITHIN GROUP (ORDER BY newID.value) AS 
NewField
FROM
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2')  
GROUP BY 
    ProjectID
ORDER BY 
    ProjectID
Results:
ProjectID   NewField
-------------------------------------------------------------
2           O95833,O95833,O95833,Q96NY7-2,Q96NY7-2,Q96NY7-2
4           Q96NY7-2,Q96NY7-2
I would like my final output to have only unique elements as below:
ProjectID   NewField
-------------------------------
2           O95833, Q96NY7-2
4           Q96NY7-2
Any suggestions about how to get this result? Please feel free to refine/redesign from scratch my query if needed.
","<sal><sal-server><sal-server-2017><string-aggregation>, get unique value use string_agg sal server, follow query return result shown below: select projected, new.value [do].[data] with(clock) cross apply string_split([bid],';') new new.value ('o95833', 'q96ny7-2') results: projected value --------------------- 2 q96ny7-2 2 o95833 2 o95833 2 q96ny7-2 2 o95833 2 q96ny7-2 4 q96ny7-2 4 q96ny7-2 use newly ad string_agg function (in sal server 2017) shown follow query all get result-set below. select projected, string_agg( new.value, ',') within group (order new.value) newfield [do].[data] with(clock) cross apply string_split([bid],';') new new.value ('o95833', 'q96ny7-2') group projected order projected results: projected newfield ------------------------------------------------------------- 2 o95833,o95833,o95833,q96ny7-2,q96ny7-2,q96ny7-2 4 q96ny7-2,q96ny7-2 would like final output unique element below: projected newfield ------------------------------- 2 o95833, q96ny7-2 4 q96ny7-2 suggest get result? pleas feel free refine/design scratch query needed."
48466959,Query for list of attribute instead of tuples in SQLAlchemy,"I'm querying for the ids of a model, and get a list of (int,) tuples back instead of a list of ids. Is there a way to query for the attribute directly?
result = session.query(MyModel.id).all()
I realize it's possible to do 
results = [r for (r,) in results]
Is it possible for the query to return that form directly, instead of having to process it myself?
",<python><sqlalchemy>,357,0,3,1018,1,11,26,68,38691,0.0,196,4,54,2018-01-26 17:57,2018-01-26 18:06,2018-01-26 18:06,0.0,0.0,Basic,10,"<python><sqlalchemy>, Query for list of attribute instead of tuples in SQLAlchemy, I'm querying for the ids of a model, and get a list of (int,) tuples back instead of a list of ids. Is there a way to query for the attribute directly?
result = session.query(MyModel.id).all()
I realize it's possible to do 
results = [r for (r,) in results]
Is it possible for the query to return that form directly, instead of having to process it myself?
","<patron><sqlalchemy>, query list attribute instead up sqlalchemy, i'm query id model, get list (in,) up back instead list is. way query attribute directly? result = session.query(model.id).all() realize possible result = [r (r,) results] possible query return form directly, instead process myself?"
53131321,Spring Boot: Jdbc javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify,"I am currently learning more about implementing JDBC and using databases in a Spring Boot webapp, and I encountered the following Stack Trace written in the bottom of the post.
I have created a simple Employee model, and I am trying to execute some database code on the same class which my main() lies in. The model and the main class are the only two java files existing in this whole project. I am trying to implement the following run() code that overrides the one from the interface, CommandLineRunner, but I do not get the logs that should come after log.info(""Part A:""):
log.info(""Part A:"")
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things I noticed:
I noticed that the last line of the log before the stack trace starts comes from: ""Thread-1"" instead of ""main"". I think that means that a thread from somewhere other than the main encountered an error and closed connection before when it should close normally.
Also, I think that because HikariPool closed before ""peer's close_notify"", which I presume that it refers to the normal closure of HikariPool, I am not able to see the final bit of logging that I have kept trying to procure. The final bit of logging that I want to see is the logging of the employee that has become inserted into my database.
The final bit of logging that I want to see should be procured from this line of code:
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things to note:
Because of this line in the log, I thought I would see the employee inserted into my database, but when I queried directly on MySQL Command Line Client, it returned an empty set:
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
I don't understand why a row has been affected when nothing has been inserted into the database.
The stacktrace and logs: (The stacktrace pasted below actually repeats itself several more times but I cut it off for brevity.)
2018-11-03 21:08:32.997  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Starting JdbcTest1Application on KitKat with PID 2408 (C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1\target\classes started by Nano in C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1)
2018-11-03 21:08:33.003  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : No active profile set, falling back to default profiles: default
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Started JdbcTest1Application in 1.024 seconds (JVM running for 1.778)
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Creating tables
2018-11-03 21:08:33.770  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2018-11-03 21:08:34.082  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2018-11-03 21:08:35.135  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Inserting Baggins Hopkins
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Querying for employee
2018-11-03 21:08:36.065  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Part A:
2018-11-03 21:08:36.065  INFO 2408 --- [       Thread-1] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
Sat Nov 03 21:08:36 KST 2018 WARN: Caught while disconnecting...
EXCEPTION STACK TRACE:
** BEGIN NESTED EXCEPTION ** 
javax.net.ssl.SSLException
MESSAGE: closing inbound before receiving peer's close_notify
STACKTRACE:
javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:129)
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:308)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:264)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:255)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:645)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:624)
    at com.mysql.cj.protocol.a.NativeProtocol.quit(NativeProtocol.java:1312)
    at com.mysql.cj.NativeSession.quit(NativeSession.java:182)
    at com.mysql.cj.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:1750)
    at com.mysql.cj.jdbc.ConnectionImpl.close(ConnectionImpl.java:720)
    at com.zaxxer.hikari.pool.PoolBase.quietlyCloseConnection(PoolBase.java:135)
    at com.zaxxer.hikari.pool.HikariPool.lambda$closeConnection$1(HikariPool.java:441)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:834)
** END NESTED EXCEPTION **
The Java Code:
@SpringBootApplication
public class JdbcTest1Application implements CommandLineRunner {
    private static final Logger log = LoggerFactory.getLogger(JdbcTest1Application.class);
    @Autowired
    JdbcTemplate jdbcTemplate;
    public static void main(String[] args) {
        SpringApplication.run(JdbcTest1Application.class, args);
    }
    @Override
    public void run(String... args) throws Exception {
        log.info(""Creating tables"");
        jdbcTemplate.execute(""DROP TABLE IF EXISTS employees"");
        jdbcTemplate.execute(""CREATE TABLE employees (emp_id int, name varchar(100), role varchar(100), status varchar(100))"");
        log.info(""Inserting Baggins Hopkins"");
        int rowsAffected = jdbcTemplate.update(""INSERT INTO EMPLOYEE(EMP_ID, NAME, ROLE, STATUS)""
                + "" VALUES(1,'Baggins Hopkins','thief','WORKING')"");
        log.info(""rows affected: ""+ Integer.toString(rowsAffected));
        log.info(""Querying for employee"");
        String sql = ""SELECT emp_id,name,role,status FROM employees"";
        List&lt;Employee&gt; employees = jdbcTemplate.query(sql,(rs, rowNum)-&gt; 
        new Employee(rs.getInt(""emp_id""), rs.getString(""name""),
                rs.getString(""role""),Status.valueOf(rs.getString(""status""))));
        log.info(""Part A:"");
        employees.forEach(employee -&gt; {log.info(employee.toString());
            log.info(""part a"");});
    }
}
Also just in case this matters, I pasted this code from application.properties:
spring.datasource.url=jdbc:mysql://localhost:3306/employee_database
spring.datasource.username=employee
spring.datasource.password=employee
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
",<mysql><spring-boot><spring-jdbc><jdbctemplate><sslexception>,6948,0,87,815,1,9,19,74,120140,0.0,188,14,54,2018-11-03 12:29,2018-11-04 1:13,2018-11-07 12:55,1.0,4.0,Advanced,32,"<mysql><spring-boot><spring-jdbc><jdbctemplate><sslexception>, Spring Boot: Jdbc javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify, I am currently learning more about implementing JDBC and using databases in a Spring Boot webapp, and I encountered the following Stack Trace written in the bottom of the post.
I have created a simple Employee model, and I am trying to execute some database code on the same class which my main() lies in. The model and the main class are the only two java files existing in this whole project. I am trying to implement the following run() code that overrides the one from the interface, CommandLineRunner, but I do not get the logs that should come after log.info(""Part A:""):
log.info(""Part A:"")
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things I noticed:
I noticed that the last line of the log before the stack trace starts comes from: ""Thread-1"" instead of ""main"". I think that means that a thread from somewhere other than the main encountered an error and closed connection before when it should close normally.
Also, I think that because HikariPool closed before ""peer's close_notify"", which I presume that it refers to the normal closure of HikariPool, I am not able to see the final bit of logging that I have kept trying to procure. The final bit of logging that I want to see is the logging of the employee that has become inserted into my database.
The final bit of logging that I want to see should be procured from this line of code:
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things to note:
Because of this line in the log, I thought I would see the employee inserted into my database, but when I queried directly on MySQL Command Line Client, it returned an empty set:
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
I don't understand why a row has been affected when nothing has been inserted into the database.
The stacktrace and logs: (The stacktrace pasted below actually repeats itself several more times but I cut it off for brevity.)
2018-11-03 21:08:32.997  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Starting JdbcTest1Application on KitKat with PID 2408 (C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1\target\classes started by Nano in C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1)
2018-11-03 21:08:33.003  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : No active profile set, falling back to default profiles: default
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Started JdbcTest1Application in 1.024 seconds (JVM running for 1.778)
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Creating tables
2018-11-03 21:08:33.770  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2018-11-03 21:08:34.082  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2018-11-03 21:08:35.135  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Inserting Baggins Hopkins
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Querying for employee
2018-11-03 21:08:36.065  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Part A:
2018-11-03 21:08:36.065  INFO 2408 --- [       Thread-1] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
Sat Nov 03 21:08:36 KST 2018 WARN: Caught while disconnecting...
EXCEPTION STACK TRACE:
** BEGIN NESTED EXCEPTION ** 
javax.net.ssl.SSLException
MESSAGE: closing inbound before receiving peer's close_notify
STACKTRACE:
javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:129)
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:308)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:264)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:255)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:645)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:624)
    at com.mysql.cj.protocol.a.NativeProtocol.quit(NativeProtocol.java:1312)
    at com.mysql.cj.NativeSession.quit(NativeSession.java:182)
    at com.mysql.cj.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:1750)
    at com.mysql.cj.jdbc.ConnectionImpl.close(ConnectionImpl.java:720)
    at com.zaxxer.hikari.pool.PoolBase.quietlyCloseConnection(PoolBase.java:135)
    at com.zaxxer.hikari.pool.HikariPool.lambda$closeConnection$1(HikariPool.java:441)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:834)
** END NESTED EXCEPTION **
The Java Code:
@SpringBootApplication
public class JdbcTest1Application implements CommandLineRunner {
    private static final Logger log = LoggerFactory.getLogger(JdbcTest1Application.class);
    @Autowired
    JdbcTemplate jdbcTemplate;
    public static void main(String[] args) {
        SpringApplication.run(JdbcTest1Application.class, args);
    }
    @Override
    public void run(String... args) throws Exception {
        log.info(""Creating tables"");
        jdbcTemplate.execute(""DROP TABLE IF EXISTS employees"");
        jdbcTemplate.execute(""CREATE TABLE employees (emp_id int, name varchar(100), role varchar(100), status varchar(100))"");
        log.info(""Inserting Baggins Hopkins"");
        int rowsAffected = jdbcTemplate.update(""INSERT INTO EMPLOYEE(EMP_ID, NAME, ROLE, STATUS)""
                + "" VALUES(1,'Baggins Hopkins','thief','WORKING')"");
        log.info(""rows affected: ""+ Integer.toString(rowsAffected));
        log.info(""Querying for employee"");
        String sql = ""SELECT emp_id,name,role,status FROM employees"";
        List&lt;Employee&gt; employees = jdbcTemplate.query(sql,(rs, rowNum)-&gt; 
        new Employee(rs.getInt(""emp_id""), rs.getString(""name""),
                rs.getString(""role""),Status.valueOf(rs.getString(""status""))));
        log.info(""Part A:"");
        employees.forEach(employee -&gt; {log.info(employee.toString());
            log.info(""part a"");});
    }
}
Also just in case this matters, I pasted this code from application.properties:
spring.datasource.url=jdbc:mysql://localhost:3306/employee_database
spring.datasource.username=employee
spring.datasource.password=employee
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
","<myself><spring-boot><spring-job><jdbctemplate><sslexception>, spring boot: job naval.net.sal.sslexception: close bound receive peer' close_notify, current learn implement job use database spring boot webapp, count follow stick trace written bottom post. great simple employe model, try execute database code class main() lie in. model main class two cava file exist whole project. try implement follow run() code overdid one interface, commandlinerunner, get log come log.into(""part a:""): log.into(""part a:"") employees.french(employe -&it; {log.into(employee.string()); log.into(""part a"");}); --thing noticed: notice last line log stick trace start come from: ""thread-1"" instead ""main"". think mean thread somewhere main count error close connect close normally. also, think hikaripool close ""peer' close_notify"", presume refer normal closer hikaripool, all see final bit log kept try procure. final bit log want see log employe become insert database. final bit log want see procure line code: employees.french(employe -&it; {log.into(employee.string()); log.into(""part a"");}); --thing note: line log, thought would see employe insert database, query directly myself command line client, return empty set: 2018-11-03 21:08:35.362 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : row affected: 1 understand row affect not insert database. stacktrac logs: (the stacktrac past actual repeat never time cut gravity.) 2018-11-03 21:08:32.997 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : start jdbctest1appl mitka did 2408 (c:\users\naso\download\job-test\job-test\target\class start naso c:\users\naso\download\job-test\job-test) 2018-11-03 21:08:33.003 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : active profit set, fall back default profile: default 2018-11-03 21:08:33.770 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : start jdbctest1appl 1.024 second (jem run 1.778) 2018-11-03 21:08:33.770 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : great table 2018-11-03 21:08:33.770 into 2408 --- [ main] com.baxter.kari.hikaridatasourc : hikaripool-1 - starting... 2018-11-03 21:08:34.082 into 2408 --- [ main] com.baxter.kari.hikaridatasourc : hikaripool-1 - start completed. 2018-11-03 21:08:35.135 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : insert bagging hopkins 2018-11-03 21:08:35.362 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : row affected: 1 2018-11-03 21:08:35.362 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : query employe 2018-11-03 21:08:36.065 into 2408 --- [ main] c.j.jdbctest1.jdbctest1appl : part a: 2018-11-03 21:08:36.065 into 2408 --- [ thread-1] com.baxter.kari.hikaridatasourc : hikaripool-1 - shutdown initiated... sat nov 03 21:08:36 st 2018 warn: caught disconcerting... except stick trace: ** begin nest except ** naval.net.sal.sslexcept message: close bound receive peer' close_notifi stacktrace: naval.net.sal.sslexception: close bound receive peer' close_notifi cava.base/sun.security.sal.alert.createsslexception(alert.cava:129) cava.base/sun.security.sal.alert.createsslexception(alert.cava:117) cava.base/sun.security.sal.transportcontext.fatal(transportcontext.cava:308) cava.base/sun.security.sal.transportcontext.fatal(transportcontext.cava:264) cava.base/sun.security.sal.transportcontext.fatal(transportcontext.cava:255) cava.base/sun.security.sal.sslsocketimpl.shutdowninput(sslsocketimpl.cava:645) cava.base/sun.security.sal.sslsocketimpl.shutdowninput(sslsocketimpl.cava:624) com.myself.c.protocol.a.nativeprotocol.quit(nativeprotocol.cava:1312) com.myself.c.nativesession.quit(nativesession.cava:182) com.myself.c.job.connectionimpl.realclose(connectionimpl.cava:1750) com.myself.c.job.connectionimpl.close(connectionimpl.cava:720) com.baxter.kari.pool.poolbase.quietlycloseconnection(poolbase.cava:135) com.baxter.kari.pool.hikaripool.labia$closeconnection$1(hikaripool.cava:441) cava.base/cava.until.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.cava:1128) cava.base/cava.until.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.cava:628) cava.base/cava.long.thread.run(thread.cava:834) ** end nest except ** cava code: @springbootappl public class jdbctest1appl implement commandlinerunn { privat static final longer log = loggerfactory.getlogger(jdbctest1application.class); @autowir jdbctemplat jdbctemplate; public static void main(string[] arms) { springapplication.run(jdbctest1application.class, arms); } @overdid public void run(string... arms) throw except { log.into(""or tables""); jdbctemplate.execute(""drop table exist employees""); jdbctemplate.execute(""or table employe (emptied in, name varchar(100), role varchar(100), state varchar(100))""); log.into(""insert bagging hopkins""); in rowsaffect = jdbctemplate.update(""insert employee(emptied, name, role, status)"" + "" values(1,'bagging hopkins','thief','working')""); log.into(""row affected: ""+ inter.string(rowsaffected)); log.into(""query employee""); string sal = ""select emptied,name,role,state employees""; list&it;employee&it; employe = jdbctemplate.query(sal,(is, grownup)-&it; new employee(is.getting(""emptied""), is.getting(""name""), is.getting(""role""),status.value(is.getting(""status"")))); log.into(""part a:""); employees.french(employe -&it; {log.into(employee.string()); log.into(""part a"");}); } } also case matters, past code application.properties: spring.datasource.curl=job:myself://localhost:3306/employee_databas spring.datasource.surname=employe spring.datasource.password=employe spring.datasource.driver-class-name=com.myself.c.job.drive"
49023821,Nested Join vs Merge Join vs Hash Join in PostgreSQL,"I know how the 
Nested Join
Merge Join
Hash Join  
works and its functionality. 
I wanted to know in which situation these joins are used in Postgres 
",<postgresql><sql-execution-plan>,151,0,0,1224,3,16,34,72,28550,0.0,115,1,54,2018-02-28 7:09,2018-02-28 7:56,,0.0,,Advanced,35,"<postgresql><sql-execution-plan>, Nested Join vs Merge Join vs Hash Join in PostgreSQL, I know how the 
Nested Join
Merge Join
Hash Join  
works and its functionality. 
I wanted to know in which situation these joins are used in Postgres 
","<postgresql><sal-execution-plan>, nest join vs berg join vs has join postgresql, know nest join berg join has join work functionality. want know situate join use poster"
54540928,Why is query with phone = N'1234' slower than phone = '1234'?,"I have a field which is a varchar(20)
When this query is executed, it is fast (Uses index seek):
SELECT * FROM [dbo].[phone] WHERE phone = '5554474477'
But this one is slow (uses index scan).
SELECT * FROM [dbo].[phone] WHERE phone = N'5554474477'
I am guessing that if I change the field to an nvarchar, then it would use the Index Seek.
",<sql><sql-server><query-performance>,339,0,2,34370,40,167,239,73,5925,0.0,838,3,53,2019-02-05 18:35,2019-02-05 18:38,2019-02-05 18:38,0.0,0.0,Intermediate,23,"<sql><sql-server><query-performance>, Why is query with phone = N'1234' slower than phone = '1234'?, I have a field which is a varchar(20)
When this query is executed, it is fast (Uses index seek):
SELECT * FROM [dbo].[phone] WHERE phone = '5554474477'
But this one is slow (uses index scan).
SELECT * FROM [dbo].[phone] WHERE phone = N'5554474477'
I am guessing that if I change the field to an nvarchar, then it would use the Index Seek.
","<sal><sal-server><query-performance>, query phone = n'1234' slower phone = '1234'?, field varchar(20) query executed, fast (use index seek): select * [do].[phone] phone = '5554474477' one slow (use index scan). select * [do].[phone] phone = n'5554474477' guess change field nvarchar, would use index seek."
50476782,Android P - 'SQLite: No Such Table Error' after copying database from assets,"I have a database saved in my apps assets folder and I copy the database using the below code when the app first opens.
inputStream = mContext.getAssets().open(Utils.getDatabaseName());
        if(inputStream != null) {
            int mFileLength = inputStream.available();
            String filePath = mContext.getDatabasePath(Utils.getDatabaseName()).getAbsolutePath();
            // Save the downloaded file
            output = new FileOutputStream(filePath);
            byte data[] = new byte[1024];
            long total = 0;
            int count;
            while ((count = inputStream.read(data)) != -1) {
                total += count;
                if(mFileLength != -1) {
                    // Publish the progress
                    publishProgress((int) (total * 100 / mFileLength));
                }
                output.write(data, 0, count);
            }
            return true;
        }
The above code runs without problem but when you try to query the database you get an SQLite: No such table exception.
This issue only occurs in Android P, all earlier versions of Android work correctly.
Is this a known issue with Android P or has something changed?
",<android><sqlite><android-9.0-pie>,1189,0,24,825,1,9,18,66,21878,0.0,2,14,53,2018-05-22 21:34,2018-05-28 12:25,2018-05-31 18:35,6.0,9.0,Basic,11,"<android><sqlite><android-9.0-pie>, Android P - 'SQLite: No Such Table Error' after copying database from assets, I have a database saved in my apps assets folder and I copy the database using the below code when the app first opens.
inputStream = mContext.getAssets().open(Utils.getDatabaseName());
        if(inputStream != null) {
            int mFileLength = inputStream.available();
            String filePath = mContext.getDatabasePath(Utils.getDatabaseName()).getAbsolutePath();
            // Save the downloaded file
            output = new FileOutputStream(filePath);
            byte data[] = new byte[1024];
            long total = 0;
            int count;
            while ((count = inputStream.read(data)) != -1) {
                total += count;
                if(mFileLength != -1) {
                    // Publish the progress
                    publishProgress((int) (total * 100 / mFileLength));
                }
                output.write(data, 0, count);
            }
            return true;
        }
The above code runs without problem but when you try to query the database you get an SQLite: No such table exception.
This issue only occurs in Android P, all earlier versions of Android work correctly.
Is this a known issue with Android P or has something changed?
","<andros><quite><andros-9.0-pie>, andros p - 'quite: table error' copy database asset, database save pp asset older copy database use code pp first opens. inputstream = context.getassets().open(still.getdatabasename()); if(inputstream != null) { in mfilelength = inputstream.available(); string filepath = context.getdatabasepath(still.getdatabasename()).getabsolutepath(); // save download file output = new fileoutputstream(filepath); bite data[] = new bite[1024]; long total = 0; in count; ((count = inputstream.read(data)) != -1) { total += count; if(mfilelength != -1) { // publish progress publishprogress((in) (total * 100 / mfilelength)); } output.write(data, 0, count); } return true; } code run without problem try query database get quite: table exception. issue occur andros p, earlier version andros work correctly. known issue andros p cometh changed?"
57262748,SQL Server Invalid version: 15 (Microsoft.SqlServer.Smo),"Context: I'm having difficulty modifying a stored procedure in SQL Server 2016. The stored procedure performs parsing of json data within a file. For some reason I'm able to execute the stored procedure and it executes successfully but when I try to modify the stored procedure I get the following message:
Question: Does anyone have any troubleshooting tips? Below is the content of the stored procedure. SQL Server 2016 supports the various functions used including the OPENJSON function.  
USE mattermark_sandbox
GO
CREATE PROCEDURE get_company_data 
AS
IF OBJECT_ID('tempdb..##jsondump') IS NOT NULL DROP TABLE ##jsondump
IF OBJECT_ID('tempdb..##jsonparsed') IS NOT NULL DROP TABLE ##jsonparsed
IF OBJECT_ID('tempdb..##json_loop') IS NOT NULL DROP TABLE ##json_loop
CREATE TABLE ##jsondump (
    [my_json] [nvarchar](max) NULL
) 
-- Create a table to house the parsed content
CREATE TABLE ##jsonparsed (
    [id] [int] NULL,
    [url] [varchar](255) NULL,
    [company_name] [varchar](255) NULL,
    [domain] [varchar](255) NULL
)
-- Clear ##jsondump
TRUNCATE TABLE ##jsondump;
-- Clear ##jsonparsed ( only if you don't want to keep what's already there )
TRUNCATE TABLE ##jsonparsed;
-- Import ( single column ) JSON
--IMPORTANT: Need to be sure the company_data.json file actually exists on the remote server in that directory 
BULK INSERT ##jsondump
FROM 'C:\mattermark_etl_project\company_data.json' -- ( &lt;-- my file, point to your own )
WITH (
    ROWTERMINATOR = '\n'
);
-- Select JSON into ##jsonparsed
SELECT my_json 
INTO ##json_loop
FROM ##jsondump;
--SELECT * FROM ##jsondump;
INSERT INTO ##jsonparsed (
    id, [url], company_name, domain
)
SELECT DISTINCT
    jsn.id, jsn.[url], jsn.company_name, jsn.domain
FROM ##json_loop
OUTER APPLY (
    SELECT * FROM OPENJSON(##json_loop.my_json, '$.companies' )
    WITH (
        id INT '$.id',
        [url] VARCHAR(255) '$.url',
        company_name VARCHAR(255) '$.company_name',
        domain VARCHAR(255) '$.domain'
    )
) AS jsn
DECLARE @bcp_cmd4 VARCHAR(1000);
DECLARE @exe_path4 VARCHAR(200) = 
    ' cd C:\Program Files\Microsoft SQL Server\100\Tools\Binn\ &amp; ';
SET @bcp_cmd4 =  @exe_path4 + 
    ' BCP.EXE ""SELECT ''Company_ID'', ''MatterMark_URL'', ''Company_Name'', ''Domain'' UNION ALL SELECT DISTINCT cast(id as varchar( 12 )) as id, url, company_name, domain FROM ##jsonparsed"" queryout ' +
    ' ""C:\mattermark_etl_project\company_data.txt"" -T -c -q -t0x7c -r\n';
PRINT @bcp_cmd4;
EXEC master..xp_cmdshell @bcp_cmd4,no_output;
SELECT DISTINCT * FROM ##jsonparsed
ORDER BY id ASC;
DROP TABLE ##jsondump 
DROP TABLE ##jsonparsed 
DROP TABLE ##json_loop 
/*
-- To allow advanced options to be changed.  
EXEC sp_configure 'show advanced options', 1;  
GO  
-- To update the currently configured value for advanced options.  
RECONFIGURE;  
GO  
-- To enable the feature.  
EXEC sp_configure 'xp_cmdshell', 1;  
GO  
-- To update the currently configured value for this feature.  
RECONFIGURE;  
GO  
*/
exec xp_cmdshell 'C:\mattermark_etl_project\powershell ""C:\mattermark_etl_project\open_file.ps1""',no_output
",<sql-server><t-sql><open-json>,3092,1,97,1623,4,20,44,57,101322,0.0,292,4,53,2019-07-30 0:16,2019-08-01 0:10,2019-08-01 0:10,2.0,2.0,Basic,11,"<sql-server><t-sql><open-json>, SQL Server Invalid version: 15 (Microsoft.SqlServer.Smo), Context: I'm having difficulty modifying a stored procedure in SQL Server 2016. The stored procedure performs parsing of json data within a file. For some reason I'm able to execute the stored procedure and it executes successfully but when I try to modify the stored procedure I get the following message:
Question: Does anyone have any troubleshooting tips? Below is the content of the stored procedure. SQL Server 2016 supports the various functions used including the OPENJSON function.  
USE mattermark_sandbox
GO
CREATE PROCEDURE get_company_data 
AS
IF OBJECT_ID('tempdb..##jsondump') IS NOT NULL DROP TABLE ##jsondump
IF OBJECT_ID('tempdb..##jsonparsed') IS NOT NULL DROP TABLE ##jsonparsed
IF OBJECT_ID('tempdb..##json_loop') IS NOT NULL DROP TABLE ##json_loop
CREATE TABLE ##jsondump (
    [my_json] [nvarchar](max) NULL
) 
-- Create a table to house the parsed content
CREATE TABLE ##jsonparsed (
    [id] [int] NULL,
    [url] [varchar](255) NULL,
    [company_name] [varchar](255) NULL,
    [domain] [varchar](255) NULL
)
-- Clear ##jsondump
TRUNCATE TABLE ##jsondump;
-- Clear ##jsonparsed ( only if you don't want to keep what's already there )
TRUNCATE TABLE ##jsonparsed;
-- Import ( single column ) JSON
--IMPORTANT: Need to be sure the company_data.json file actually exists on the remote server in that directory 
BULK INSERT ##jsondump
FROM 'C:\mattermark_etl_project\company_data.json' -- ( &lt;-- my file, point to your own )
WITH (
    ROWTERMINATOR = '\n'
);
-- Select JSON into ##jsonparsed
SELECT my_json 
INTO ##json_loop
FROM ##jsondump;
--SELECT * FROM ##jsondump;
INSERT INTO ##jsonparsed (
    id, [url], company_name, domain
)
SELECT DISTINCT
    jsn.id, jsn.[url], jsn.company_name, jsn.domain
FROM ##json_loop
OUTER APPLY (
    SELECT * FROM OPENJSON(##json_loop.my_json, '$.companies' )
    WITH (
        id INT '$.id',
        [url] VARCHAR(255) '$.url',
        company_name VARCHAR(255) '$.company_name',
        domain VARCHAR(255) '$.domain'
    )
) AS jsn
DECLARE @bcp_cmd4 VARCHAR(1000);
DECLARE @exe_path4 VARCHAR(200) = 
    ' cd C:\Program Files\Microsoft SQL Server\100\Tools\Binn\ &amp; ';
SET @bcp_cmd4 =  @exe_path4 + 
    ' BCP.EXE ""SELECT ''Company_ID'', ''MatterMark_URL'', ''Company_Name'', ''Domain'' UNION ALL SELECT DISTINCT cast(id as varchar( 12 )) as id, url, company_name, domain FROM ##jsonparsed"" queryout ' +
    ' ""C:\mattermark_etl_project\company_data.txt"" -T -c -q -t0x7c -r\n';
PRINT @bcp_cmd4;
EXEC master..xp_cmdshell @bcp_cmd4,no_output;
SELECT DISTINCT * FROM ##jsonparsed
ORDER BY id ASC;
DROP TABLE ##jsondump 
DROP TABLE ##jsonparsed 
DROP TABLE ##json_loop 
/*
-- To allow advanced options to be changed.  
EXEC sp_configure 'show advanced options', 1;  
GO  
-- To update the currently configured value for advanced options.  
RECONFIGURE;  
GO  
-- To enable the feature.  
EXEC sp_configure 'xp_cmdshell', 1;  
GO  
-- To update the currently configured value for this feature.  
RECONFIGURE;  
GO  
*/
exec xp_cmdshell 'C:\mattermark_etl_project\powershell ""C:\mattermark_etl_project\open_file.ps1""',no_output
","<sal-server><t-sal><open-son>, sal server invalid version: 15 (microsoft.sqlserver.so), context: i'm difficult modify store procedure sal server 2016. store procedure perform part son data within file. reason i'm all execute store procedure execute success try modify store procedure get follow message: question: anyone troubleshoot tips? content store procedure. sal server 2016 support various function use include openjson function. use mattermark_sandbox go great procedure get_company_data objected('temper..##jsondump') null drop table ##jsondump objected('temper..##jsonparsed') null drop table ##jsonpars objected('temper..##json_loop') null drop table ##json_loop great table ##jsondump ( [my_json] [nvarchar](max) null ) -- great table house part content great table ##jsonpars ( [id] [in] null, [curl] [varchar](255) null, [company_name] [varchar](255) null, [domain] [varchar](255) null ) -- clear ##jsondump truncated table ##jsondump; -- clear ##jsonpars ( want keep what' already ) truncated table ##jsonparsed; -- import ( single column ) son --important: need sure company_data.son file actual exist remote server director bulk insert ##jsondump 'c:\mattermark_etl_project\company_data.son' -- ( &it;-- file, point ) ( rowtermin = '\n' ); -- select son ##jsonpars select my_json ##json_loop ##jsondump; --select * ##jsondump; insert ##jsonpars ( id, [curl], company_name, domain ) select distinct isn.id, isn.[curl], isn.company_name, isn.domain ##json_loop outer apply ( select * openjson(##json_loop.my_json, '$.companies' ) ( id in '$.id', [curl] varchar(255) '$.curl', company_nam varchar(255) '$.company_name', domain varchar(255) '$.domain' ) ) isn declare @bcp_cmd4 varchar(1000); declare @exe_path4 varchar(200) = ' d c:\program files\microsoft sal server\100\tools\inn\ &amp; '; set @bcp_cmd4 = @exe_path4 + ' bc.ex ""select ''company_id'', ''mattermark_url'', ''company_name'', ''domain'' union select distinct cast(id varchar( 12 )) id, curl, company_name, domain ##jsonparsed"" queryout ' + ' ""c:\mattermark_etl_project\company_data.txt"" -t -c -q -toxic -r\n'; print @bcp_cmd4; even master..xp_cmdshel @bcp_cmd4,no_output; select distinct * ##jsonpars order id as; drop table ##jsondump drop table ##jsonpars drop table ##json_loop /* -- allow advance option changed. even sp_configur 'show advance option', 1; go -- update current configur value advance option. reconfigure; go -- enable feature. even sp_configur 'xp_cmdshell', 1; go -- update current configur value feature. reconfigure; go */ even xp_cmdshell 'c:\mattermark_etl_project\powershel ""c:\mattermark_etl_project\open_file.is""',no_output"
58866560,flask_sqlalchemy `pool_pre_ping` only working sometimes,"For testing, I amend the MYSQL (RDS) parameters as follows;
wait_timeout = 40 (default was 28800)
max_allowed_packet = 1GB (max - just to be sure issue not caused by small packets)
net_read_timeout = 10
interactive_timeout unchanged
Then tested my app without pool_pre_ping options set (defaults to False), kept the app inactive for 40 seconds, tried to login, and i get
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: Traceback (most recent call last):
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py&quot;, line 1193, in _execute_context
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     context)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/default.py&quot;, line 507, in do_execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     cursor.execute(statement, parameters)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 206, in execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     res = self._query(query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 312, in _query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     db.query(q)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/connections.py&quot;, line 224, in query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     _mysql.connection.query(self, query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: MySQLdb._exceptions.OperationalError: (2013, 'Lost connection to MySQL server during query')
Added the pool_pre_ping like this (Using flask_sqlalchamy version 2.4.1);
import os
from flask import Flask
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(app, options)
        options[&quot;pool_pre_ping&quot;] = True
#        options[&quot;pool_recycle&quot;] = 30
#        options[&quot;pool_timeout&quot;] = 35
db = SQLAlchemy()
class DevConfig():
    SQLALCHEMY_ENGINE_OPTIONS = {'pool_recycle': 280, 'pool_timeout': 100, 'pool_pre_ping': True} # These configs doesn't get applied in engine configs :/
    DEBUG = True
    # SERVER_NAME = '127.0.0.1:5000'
    SQLALCHEMY_DATABASE_URI = os.getenv('SQLALCHEMY_DATABASE_URI_DEV')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
config = dict(
    dev=DevConfig,
)
app = Flask(__name__, instance_relative_config=True)
app.config.from_object(config['dev'])
# INIT DATABASE
db.init_app(app)
with app.app_context():
    db.create_all()
-----------run.py
app.run(host='127.0.0.1', port=5000)
With this, now the webapp manages to get new connection even after MySQL server has closed the previous connection. It always works fine when I access the database right after its closed by server (tried max 50 seconds after)... but when I keep connection inactive for long time (haven't noted, but ~ &gt;10-15 min), again I see same error.
According to the docs, (especially the section Dealing with disconnects), the pool_pre_ping option should handle this kind of scenario at background rite? Or is there any other timeout variable that I need to change in MySQL server?
",<python><mysql><flask><flask-sqlalchemy><connection-pooling>,3480,1,51,2461,1,30,55,62,8866,0.0,962,2,53,2019-11-14 21:10,2021-07-28 22:32,,622.0,,Basic,9,"<python><mysql><flask><flask-sqlalchemy><connection-pooling>, flask_sqlalchemy `pool_pre_ping` only working sometimes, For testing, I amend the MYSQL (RDS) parameters as follows;
wait_timeout = 40 (default was 28800)
max_allowed_packet = 1GB (max - just to be sure issue not caused by small packets)
net_read_timeout = 10
interactive_timeout unchanged
Then tested my app without pool_pre_ping options set (defaults to False), kept the app inactive for 40 seconds, tried to login, and i get
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: Traceback (most recent call last):
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py&quot;, line 1193, in _execute_context
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     context)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/default.py&quot;, line 507, in do_execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     cursor.execute(statement, parameters)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 206, in execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     res = self._query(query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 312, in _query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     db.query(q)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/connections.py&quot;, line 224, in query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     _mysql.connection.query(self, query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: MySQLdb._exceptions.OperationalError: (2013, 'Lost connection to MySQL server during query')
Added the pool_pre_ping like this (Using flask_sqlalchamy version 2.4.1);
import os
from flask import Flask
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(app, options)
        options[&quot;pool_pre_ping&quot;] = True
#        options[&quot;pool_recycle&quot;] = 30
#        options[&quot;pool_timeout&quot;] = 35
db = SQLAlchemy()
class DevConfig():
    SQLALCHEMY_ENGINE_OPTIONS = {'pool_recycle': 280, 'pool_timeout': 100, 'pool_pre_ping': True} # These configs doesn't get applied in engine configs :/
    DEBUG = True
    # SERVER_NAME = '127.0.0.1:5000'
    SQLALCHEMY_DATABASE_URI = os.getenv('SQLALCHEMY_DATABASE_URI_DEV')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
config = dict(
    dev=DevConfig,
)
app = Flask(__name__, instance_relative_config=True)
app.config.from_object(config['dev'])
# INIT DATABASE
db.init_app(app)
with app.app_context():
    db.create_all()
-----------run.py
app.run(host='127.0.0.1', port=5000)
With this, now the webapp manages to get new connection even after MySQL server has closed the previous connection. It always works fine when I access the database right after its closed by server (tried max 50 seconds after)... but when I keep connection inactive for long time (haven't noted, but ~ &gt;10-15 min), again I see same error.
According to the docs, (especially the section Dealing with disconnects), the pool_pre_ping option should handle this kind of scenario at background rite? Or is there any other timeout variable that I need to change in MySQL server?
","<patron><myself><flask><flask-sqlalchemy><connection-pooling>, flask_sqlalchemi `pool_pre_ping` work sometimes, testing, amend myself (rd) parapet follows; wait_timeout = 40 (default 28800) max_allowed_packet = go (max - sure issue cause small packets) net_read_timeout = 10 interactive_timeout unchanged test pp without pool_pre_p option set (default false), kept pp intact 40 seconds, try login, get nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: traceback (most recent call last): nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: file &quit;/war/www/api_server/vent/limb/python3.6/site-packages/sqlalchemy/engine/base.by&quit;, line 1193, _execute_context nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: context) nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: file &quit;/war/www/api_server/vent/limb/python3.6/site-packages/sqlalchemy/engine/default.by&quit;, line 507, do_execut nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: curses.execute(statement, parameter) nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: file &quit;/war/www/api_server/vent/limb/python3.6/site-packages/mysqldb/curious.by&quit;, line 206, execute nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: re = self.query(query) nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: file &quit;/war/www/api_server/vent/limb/python3.6/site-packages/mysqldb/curious.by&quit;, line 312, query nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: do.query(q) nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: file &quit;/war/www/api_server/vent/limb/python3.6/site-packages/mysqldb/connections.by&quit;, line 224, query nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: _mysql.connection.query(self, query) nov 14 20:05:20 in-172-31-33-52 unicorn[16962]: mysqldb.exceptions.operationalerror: (2013, 'lost connect myself server query') ad pool_pre_p like (use flask_sqlalchami version 2.4.1); import os flask import flask flask_sqlalchemi import sqlalchemi _basesqlalchemi class sqlalchemy(_basesqlalchemy): def apply_pool_defaults(self, pp, option): super(sqlalchemy, self).apply_pool_defaults(pp, option) option[&quit;pool_pre_ping&quit;] = true # option[&quit;pool_recycle&quit;] = 30 # option[&quit;pool_timeout&quit;] = 35 do = sqlalchemy() class devconfig(): sqlalchemy_engine_opt = {'pool_recycle': 280, 'pool_timeout': 100, 'pool_pre_ping': true} # confirm get apply engine confirm :/ debut = true # server_nam = '127.0.0.1:5000' sqlalchemy_database_uri = os.geben('sqlalchemy_database_uri_dev') sqlalchemy_track_modif = fall confirm = duct( de=devconfig, ) pp = flask(__name__, instance_relative_config=true) pp.confirm.from_object(confirm['de']) # knit database do.init_app(pp) pp.app_context(): do.create_all() -----------run.i pp.run(host='127.0.0.1', port=5000) this, webapp manage get new connect even myself server close previous connection. away work fine access database right close server (try max 50 second after)... keep connect intact long time (haven't noted, ~ &it;10-15 min), see error. accord docs, (respect section deal disconnected), pool_pre_p option hand kind scenario background rite? timeout variable need change myself server?"
55674176,django can't find new sqlite version? (SQLite 3.8.3 or later is required (found 3.7.17)),"I've cloned a django project to a Centos 7 vps and I'm trying to run it now, but I get this error when trying to migrate:
$ python manage.py migrate
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.7.17).
When I checked the version for sqlite, it was 3.7.17, so I downloaded the newest version from sqlite website and replaced it with the old one, and now when I version it, it gives:
$ sqlite3 --version
3.27.2 2019-02-25 16:06:06 bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7
Still when I try to migrate the project, I get the exact same message as before which means the newer version is not found. I'm new to linux and would appreciate any help.
",<python><django><sqlite><centos7>,708,0,5,1472,1,18,34,54,109296,0.0,516,14,52,2019-04-14 10:20,2019-04-20 16:08,2019-04-20 16:08,6.0,6.0,Basic,9,"<python><django><sqlite><centos7>, django can't find new sqlite version? (SQLite 3.8.3 or later is required (found 3.7.17)), I've cloned a django project to a Centos 7 vps and I'm trying to run it now, but I get this error when trying to migrate:
$ python manage.py migrate
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.7.17).
When I checked the version for sqlite, it was 3.7.17, so I downloaded the newest version from sqlite website and replaced it with the old one, and now when I version it, it gives:
$ sqlite3 --version
3.27.2 2019-02-25 16:06:06 bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7
Still when I try to migrate the project, I get the exact same message as before which means the newer version is not found. I'm new to linux and would appreciate any help.
","<patron><django><quite><cents>, django can't find new quite version? (quite 3.8.3 later require (found 3.7.17)), i'v alone django project cent 7 up i'm try run now, get error try migrate: $ patron manage.i migrate django.core.exceptions.improperlyconfigured: quite 3.8.3 later require (found 3.7.17). check version quite, 3.7.17, download newest version quite west replace old one, version it, gives: $ sqlite3 --version 3.27.2 2019-02-25 16:06:06 bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7 still try migrate project, get exact message mean newer version found. i'm new line would appreci help."
56824788,How to connect to windows postgres Database from WSL,"I'm running Postgres 11 service on my Windows computer.
How can I connect to this database from WSL?
When I try su - postgres:
postgres@LAPTOP-NQ52TKOG:~$ psql 
psql: could not connect to server: No such file or directory 
        Is the server running locally and accepting
        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
It's trying to connect to a Postgres in WSL. I don't want to run Ubuntu Postgres using:
sudo /etc/init.d/postgresql start
",<postgresql><windows-subsystem-for-linux>,485,0,6,777,2,8,18,46,43606,0.0,12,6,52,2019-06-30 12:19,2019-11-25 3:44,2019-11-25 3:44,148.0,148.0,Basic,9,"<postgresql><windows-subsystem-for-linux>, How to connect to windows postgres Database from WSL, I'm running Postgres 11 service on my Windows computer.
How can I connect to this database from WSL?
When I try su - postgres:
postgres@LAPTOP-NQ52TKOG:~$ psql 
psql: could not connect to server: No such file or directory 
        Is the server running locally and accepting
        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
It's trying to connect to a Postgres in WSL. I don't want to run Ubuntu Postgres using:
sudo /etc/init.d/postgresql start
","<postgresql><windows-subsystem-for-line>, connect window poster database was, i'm run poster 11 service window computer. connect database was? try s - postures: postures@lawton-nq52tkog:~$ pool pool: could connect server: file director server run local accept connect unit domain socket &quit;/war/run/postgresql/.s.pgsql.5432&quit; try connect poster was. want run bunt poster using: so /etc/knit.d/postgresql start"
49389535,Problems with flask and bad request,"I was programming myself a pretty nice api to get some json data from my gameserver to my webspace using json,
but everytime i am sending a request using angular i am getting this:
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request version
(&quot;▒\x9c▒▒{▒'\x12\x99▒▒▒\xadH\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00&quot;)
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9dtTc▒\x93▒4▒M▒▒▒▒▒\x9c▒▒{▒'\x99▒▒▒▒H▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00\x9d\x03\x03▒k,&amp;▒▒ua\x8c\x82\x17\x05▒QwQ$▒0▒▒\x9f▒B1\x98\x19W▒▒▒▒\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00')
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9d▒k,&amp;▒▒ua\x8c\x82▒QwQ$▒0▒▒\x9f▒B1\x98W▒▒▒▒▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00▒\x03\x03)▒▒\x1e\xa0▒\t\r\x14g%▒▒\x17▒▒\x80\x8d}▒F▒▒\x08U▒ġ▒▒\x06▒\x00\x00\x1c▒+▒/▒,▒0▒')
g%▒▒▒▒\x80\x8d}▒F▒U▒ġ▒▒▒▒+▒/▒,▒0▒&quot; HTTPStatus.BAD_REQUEST -
My api
from flask import Flask, jsonify
from flaskext.mysql import MySQL
from flask_cors import CORS, cross_origin
app = Flask(__name__)
CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
cors = CORS(app, resources={r""/punishments"": {""origins"": ""http://localhost:5000"" ""*""}})
mysql = MySQL()
# MySQL configurations
app.config['MYSQL_DATABASE_USER'] = 'test'
app.config['MYSQL_DATABASE_PASSWORD'] = 'Biologie1'
app.config['MYSQL_DATABASE_DB'] = 'test'
app.config['MYSQL_DATABASE_HOST'] = 'localhost'
mysql.init_app(app)
@app.route('/punishments', methods=['GET'])
@cross_origin(origin='localhost:5000',headers=['Content- Type','Authorization'])
def get():
    cur = mysql.connect().cursor()
    cur.execute('''select * from test.punishments''')
    r = [dict((cur.description[i][0], value)
              for i, value in enumerate(row)) for row in cur.fetchall()]
    return jsonify({'punishments' : r})
if __name__ == '__main__':
    app.run()
My client function
export class ApiUserService {
  private _postsURL = ""https://localhost:5000/punishments"";
  constructor(private http: HttpClient) {
  }
  getPosts(): Observable&lt;Punishments[]&gt; {
    let headers = new HttpHeaders();
    headers = headers.set('Content-Type', 'application/json; charset=utf-8');
    return this.http
      .get(this._postsURL,{
        headers: {'Content-Type':'application/json; charset=utf-8'}
      })
      .map((response: Response) =&gt; {
        return &lt;Punishments[]&gt;response.json();
      })
      .catch(this.handleError);
  }
  private handleError(error: Response) {
    return Observable.throw(error.statusText);
  }
}
",<flask><angular5><flask-mysql>,2710,2,55,549,1,5,8,76,61697,0.0,0,6,52,2018-03-20 16:29,2018-08-04 16:02,,137.0,,Basic,9,"<flask><angular5><flask-mysql>, Problems with flask and bad request, I was programming myself a pretty nice api to get some json data from my gameserver to my webspace using json,
but everytime i am sending a request using angular i am getting this:
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request version
(&quot;▒\x9c▒▒{▒'\x12\x99▒▒▒\xadH\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00&quot;)
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9dtTc▒\x93▒4▒M▒▒▒▒▒\x9c▒▒{▒'\x99▒▒▒▒H▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00\x9d\x03\x03▒k,&amp;▒▒ua\x8c\x82\x17\x05▒QwQ$▒0▒▒\x9f▒B1\x98\x19W▒▒▒▒\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00')
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9d▒k,&amp;▒▒ua\x8c\x82▒QwQ$▒0▒▒\x9f▒B1\x98W▒▒▒▒▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00▒\x03\x03)▒▒\x1e\xa0▒\t\r\x14g%▒▒\x17▒▒\x80\x8d}▒F▒▒\x08U▒ġ▒▒\x06▒\x00\x00\x1c▒+▒/▒,▒0▒')
g%▒▒▒▒\x80\x8d}▒F▒U▒ġ▒▒▒▒+▒/▒,▒0▒&quot; HTTPStatus.BAD_REQUEST -
My api
from flask import Flask, jsonify
from flaskext.mysql import MySQL
from flask_cors import CORS, cross_origin
app = Flask(__name__)
CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
cors = CORS(app, resources={r""/punishments"": {""origins"": ""http://localhost:5000"" ""*""}})
mysql = MySQL()
# MySQL configurations
app.config['MYSQL_DATABASE_USER'] = 'test'
app.config['MYSQL_DATABASE_PASSWORD'] = 'Biologie1'
app.config['MYSQL_DATABASE_DB'] = 'test'
app.config['MYSQL_DATABASE_HOST'] = 'localhost'
mysql.init_app(app)
@app.route('/punishments', methods=['GET'])
@cross_origin(origin='localhost:5000',headers=['Content- Type','Authorization'])
def get():
    cur = mysql.connect().cursor()
    cur.execute('''select * from test.punishments''')
    r = [dict((cur.description[i][0], value)
              for i, value in enumerate(row)) for row in cur.fetchall()]
    return jsonify({'punishments' : r})
if __name__ == '__main__':
    app.run()
My client function
export class ApiUserService {
  private _postsURL = ""https://localhost:5000/punishments"";
  constructor(private http: HttpClient) {
  }
  getPosts(): Observable&lt;Punishments[]&gt; {
    let headers = new HttpHeaders();
    headers = headers.set('Content-Type', 'application/json; charset=utf-8');
    return this.http
      .get(this._postsURL,{
        headers: {'Content-Type':'application/json; charset=utf-8'}
      })
      .map((response: Response) =&gt; {
        return &lt;Punishments[]&gt;response.json();
      })
      .catch(this.handleError);
  }
  private handleError(error: Response) {
    return Observable.throw(error.statusText);
  }
}
","<flask><angular><flask-myself>, problem flask bad request, program pretty nice apt get son data gameserv webspac use son, everytim send request use angular get this: 127.0.0.1 - - [20/may/2018 17:07:33] code 400, message bad request version (&quit;▒\c▒▒{▒'\x\x▒▒▒\had\x\x\x▒+▒/▒,▒0▒\x▒\x\x/\x005\x&quit;) 127.0.0.1 - - [20/may/2018 17:07:33] &quit;▒\x9dttc▒\x▒4▒m▒▒▒▒▒\c▒▒{▒'\x▒▒▒▒h▒+▒/▒,▒0▒▒/5&quit; httpstatus.bad_request - 127.0.0.1 - - [20/may/2018 17:07:33] code 400, message bad request santa ('\x\x\x\x▒\x\x\x\and\x\x▒k,&amp;▒▒a\c\x\x\x▒we$▒0▒▒\of▒by\x\x19w▒▒▒▒\x\x\x▒+▒/▒,▒0▒\x▒\x\x/\x005\x') 127.0.0.1 - - [20/may/2018 17:07:33] &quit;▒\and▒k,&amp;▒▒a\c\x▒we$▒0▒▒\of▒by\x98w▒▒▒▒▒+▒/▒,▒0▒▒/5&quit; httpstatus.bad_request - 127.0.0.1 - - [20/may/2018 17:07:33] code 400, message bad request santa ('\x\x\x\x▒\x\x\x▒\x\x)▒▒\the\a▒\t\r\x14g%▒▒\x▒▒\x\and}▒f▒▒\x08u▒ġ▒▒\x▒\x\x\c▒+▒/▒,▒0▒') g%▒▒▒▒\x\and}▒f▒u▒ġ▒▒▒▒+▒/▒,▒0▒&quit; httpstatus.bad_request - apt flask import flask, jsonifi flaskext.myself import myself flask_cor import corps, cross_origin pp = flask(__name__) corps(pp) pp.confirm['cors_headers'] = 'content-type' cor = corps(pp, resources={r""/punishments"": {""origins"": ""http://localhost:5000"" ""*""}}) myself = myself() # myself configur pp.confirm['mysql_database_user'] = 'test' pp.confirm['mysql_database_password'] = 'biologie1' pp.confirm['mysql_database_db'] = 'test' pp.confirm['mysql_database_host'] = 'localhost' myself.init_app(pp) @pp.route('/punishments', methods=['get']) @cross_origin(origin='localhost:5000',leaders=['content- type','authorization']) def get(): our = myself.connect().curses() our.execute('''select * test.punishments''') r = [duct((our.description[i][0], value) i, value enumerate(row)) row our.fetchall()] return jsonify({'punishments' : r}) __name__ == '__main__': pp.run() client function export class apiuserservic { privat _postsurl = ""http://localhost:5000/punishments""; construction(prim http: httpclient) { } outposts(): observance&it;punishments[]&it; { let header = new httpheaders(); header = leaders.set('content-type', 'application/son; charge=utf-8'); return this.http .get(this._postsurl,{ leaders: {'content-type':'application/son; charge=utf-8'} }) .map((response: response) =&it; { return &it;punishments[]&it;response.son(); }) .catch(this.handleerror); } privat handleerror(error: response) { return observance.throw(error.statustext); } }"
61649764,MySQL ERROR 2026 - SSL connection error - Ubuntu 20.04,"I've recently upgraded my local machine OS from Ubuntu 18.04 to 20.04, I'm running my MySQL-server on CentOS (AWS). Post upgrade whenever I'm trying to connect to MySQL server it is throwing SSL connection error. 
$ mysql -u yamcha -h database.yourproject.com -p --port 3309
ERROR 2026 (HY000): SSL connection error: error:1425F102:SSL routines:ssl_choose_client_version:unsupported protocol
But if I pass --ssl-mode=disabled option along with it, I'm able to connect remotely.
$ mysql -u yamcha -h database.yourproject.com -p --port 3309 --ssl-mode=disabled
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 22158946
Server version: 5.7.26 MySQL Community Server (GPL)
Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; 
Queries:  
  How to connect without passing --ssl-mode=disabled  
  How to pass this --ssl-mode=disabled option in my Django application, currently I've defined it as shown below, but I'm still getting the same error.
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'yamcha',
        'USER': 'yamcha',
        'PASSWORD': 'xxxxxxxxxxxxxxx',
        'HOST': 'database.yourproject.com',
        'PORT': '3309',
        'OPTIONS': {'ssl': False},
    }
",<mysql><django><ssl><centos><ubuntu-20.04>,1504,0,31,621,1,5,5,66,83107,0.0,1,7,52,2020-05-07 4:11,2020-05-21 12:07,,14.0,,Basic,9,"<mysql><django><ssl><centos><ubuntu-20.04>, MySQL ERROR 2026 - SSL connection error - Ubuntu 20.04, I've recently upgraded my local machine OS from Ubuntu 18.04 to 20.04, I'm running my MySQL-server on CentOS (AWS). Post upgrade whenever I'm trying to connect to MySQL server it is throwing SSL connection error. 
$ mysql -u yamcha -h database.yourproject.com -p --port 3309
ERROR 2026 (HY000): SSL connection error: error:1425F102:SSL routines:ssl_choose_client_version:unsupported protocol
But if I pass --ssl-mode=disabled option along with it, I'm able to connect remotely.
$ mysql -u yamcha -h database.yourproject.com -p --port 3309 --ssl-mode=disabled
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 22158946
Server version: 5.7.26 MySQL Community Server (GPL)
Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; 
Queries:  
  How to connect without passing --ssl-mode=disabled  
  How to pass this --ssl-mode=disabled option in my Django application, currently I've defined it as shown below, but I'm still getting the same error.
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'yamcha',
        'USER': 'yamcha',
        'PASSWORD': 'xxxxxxxxxxxxxxx',
        'HOST': 'database.yourproject.com',
        'PORT': '3309',
        'OPTIONS': {'ssl': False},
    }
","<myself><django><sal><cents><bunt-20.04>, myself error 2026 - sal connect error - bunt 20.04, i'v recent upgrade local machine os bunt 18.04 20.04, i'm run myself-serve cent (was). post upgrade when i'm try connect myself server throw sal connect error. $ myself -u yacht -h database.yourproject.com -p --port 3309 error 2026 (hy000): sal connect error: error:1425f102:sal routine:ssl_choose_client_version:support protocol pass --sal-mode=dis option along it, i'm all connect remote. $ myself -u yacht -h database.yourproject.com -p --port 3309 --sal-mode=dis welcome myself monitor. command end ; \g. myself connect id 22158946 server version: 5.7.26 myself common server (gal) copyright (c) 2000, 2020, oral and/or affiliated. right reserved. oral resist trademark oral corpora and/or affiliated. name may trademark respect owners. type 'help;' '\h' help. type '\c' clear current input statement. myself&it; queried: connect without pass --sal-mode=dis pass --sal-mode=dis option django application, current i'v define shown below, i'm still get error. database = { 'default': { 'engine': 'django.do.backed.myself', 'name': 'yacht', 'user': 'yacht', 'password': 'xxxxxxxxxxxxxxx', 'host': 'database.yourproject.com', 'port': '3309', 'option': {'sal': false}, }"
49963923,How do I update MySQL 5.7 to the new MySQL 8.0?,"How do I update to MySQL 8.0 from the default version (5.7)? 
It's important for me for it to make an update and not re-install MySQL so all my data won't be corrupt. 
There is not a lot of info regarding this issue since it was only released not long ago with tons of awesome new features!
This is what I have found that seems like it will only update and not destroy my data. I'm not going to proceed until I'm sure. 
",<mysql><ubuntu><ubuntu-server><mysqlupgrade>,420,2,0,5458,14,80,148,67,92302,0.0,1300,3,52,2018-04-22 8:20,2018-11-12 11:39,2018-11-12 11:39,204.0,204.0,Basic,2,"<mysql><ubuntu><ubuntu-server><mysqlupgrade>, How do I update MySQL 5.7 to the new MySQL 8.0?, How do I update to MySQL 8.0 from the default version (5.7)? 
It's important for me for it to make an update and not re-install MySQL so all my data won't be corrupt. 
There is not a lot of info regarding this issue since it was only released not long ago with tons of awesome new features!
This is what I have found that seems like it will only update and not destroy my data. I'm not going to proceed until I'm sure. 
","<myself><bunt><bunt-server><mysqlupgrade>, update myself 5.7 new myself 8.0?, update myself 8.0 default version (5.7)? import make update re-instal myself data corrupt. lot into regard issue since release long ago ton awesom new features! found seem like update destroy data. i'm go proceed i'm sure."
60716482,error: Skipping analyzing 'flask_mysqldb': found module but no type hints or library stubs,"I am using Python 3.6 and flask. I used flask-mysqldb to connect to MySQL, but whenever I try to run mypy on my program I get this error:
Skipping analyzing 'flask_mysqldb': found module but no type hints or
library stubs.
I tried running mypy with the flags ignore-missing-imports or follow-imports=skip. Then I was not getting the error. Why do I get this error?
How can I fix this without adding any additional flags?
",<python><mypy><flask-mysql>,421,0,7,619,1,5,8,53,39785,0.0,10,2,52,2020-03-17 5:00,2020-03-18 1:26,2020-03-18 1:26,1.0,1.0,Basic,3,"<python><mypy><flask-mysql>, error: Skipping analyzing 'flask_mysqldb': found module but no type hints or library stubs, I am using Python 3.6 and flask. I used flask-mysqldb to connect to MySQL, but whenever I try to run mypy on my program I get this error:
Skipping analyzing 'flask_mysqldb': found module but no type hints or
library stubs.
I tried running mypy with the flags ignore-missing-imports or follow-imports=skip. Then I was not getting the error. Why do I get this error?
How can I fix this without adding any additional flags?
","<patron><may><flask-myself>, error: skin analyze 'flask_mysqldb': found model type hint library tubs, use patron 3.6 flask. use flask-mysqldb connect myself, when try run my program get error: skin analyze 'flask_mysqldb': found model type hint library tubs. try run my flag ignore-missing-import follow-imports=skin. get error. get error? fix without ad admit flags?"
49794140,"Connection ""default"" was not found with TypeORM","I use TypeORM with NestJS and I am not able to save properly an entity. 
The connection creation works, postgres is running on 5432 port. Credentials are OK too. 
However when I need to save a resource with entity.save() I got :
Connection ""default"" was not found.
Error
    at new ConnectionNotFoundError (/.../ConnectionNotFoundError.ts:11:22)
I checked the source file of TypeORM ConnectionManager (https://github.com/typeorm/typeorm/blob/master/src/connection/ConnectionManager.ts) but it seems that the first time TypeORM creates connection it attributes ""default"" name if we don't provide one, which is the case for me.
I setup TypeORM with TypeOrmModule as 
TypeOrmModule.forRoot({
      type: config.db.type,
      host: config.db.host,
      port: config.db.port,
      username: config.db.user,
      password: config.db.password,
      database: config.db.database,
      entities: [
        __dirname + '/../../dtos/entities/*.entity.js',
      ]
    })
Of course my constants are correct. Any ideas ?
",<node.js><postgresql><typeorm><nestjs>,1014,2,16,764,1,8,17,64,96987,0.0,86,16,52,2018-04-12 10:25,2018-05-26 12:36,2018-05-26 12:36,44.0,44.0,Basic,9,"<node.js><postgresql><typeorm><nestjs>, Connection ""default"" was not found with TypeORM, I use TypeORM with NestJS and I am not able to save properly an entity. 
The connection creation works, postgres is running on 5432 port. Credentials are OK too. 
However when I need to save a resource with entity.save() I got :
Connection ""default"" was not found.
Error
    at new ConnectionNotFoundError (/.../ConnectionNotFoundError.ts:11:22)
I checked the source file of TypeORM ConnectionManager (https://github.com/typeorm/typeorm/blob/master/src/connection/ConnectionManager.ts) but it seems that the first time TypeORM creates connection it attributes ""default"" name if we don't provide one, which is the case for me.
I setup TypeORM with TypeOrmModule as 
TypeOrmModule.forRoot({
      type: config.db.type,
      host: config.db.host,
      port: config.db.port,
      username: config.db.user,
      password: config.db.password,
      database: config.db.database,
      entities: [
        __dirname + '/../../dtos/entities/*.entity.js',
      ]
    })
Of course my constants are correct. Any ideas ?
","<node.is><postgresql><typeorm><nests>, connect ""default"" found typeorm, use typeorm nest all save properly entity. connect creation works, poster run 5432 port. credenti ok too. howe need save resource entity.save() got : connect ""default"" found. error new connectionnotfounderror (/.../connectionnotfounderror.to:11:22) check source file typeorm connectionmanag (http://github.com/typeorm/typeorm/blow/master/sac/connection/connectionmanager.to) seem first time typeorm great connect attribute ""default"" name proved one, case me. set typeorm typeormmodul typeormmodule.forgot({ type: confirm.do.type, host: confirm.do.host, port: confirm.do.port, surname: confirm.do.user, password: confirm.do.password, database: confirm.do.database, entitles: [ __dirnam + '/../../dots/entitles/*.entity.is', ] }) course constant correct. idea ?"
52610485,How to restart PostgreSQL in Ubuntu 18.04,"How to restart PostgreSQL via ssh console?
When i search this thing on SO I only find: postgres, ubuntu how to restart service on startup? get stuck on clustering after instance reboot
",<postgresql>,185,1,0,6847,6,29,50,74,114513,0.0,1114,4,52,2018-10-02 14:27,2018-10-02 14:27,2018-10-02 14:27,0.0,0.0,Basic,10,"<postgresql>, How to restart PostgreSQL in Ubuntu 18.04, How to restart PostgreSQL via ssh console?
When i search this thing on SO I only find: postgres, ubuntu how to restart service on startup? get stuck on clustering after instance reboot
","<postgresql>, start postgresql bunt 18.04, start postgresql via ash console? search thing find: postures, bunt start service started? get stuck cluster instant report"
50497583,when to disconnect and when to end a pg client or pool,"My stack is node, express and the pg module. I really try to understand by the documentation and some outdated tutorials. I dont know when and how to disconnect and to end a client.
For some routes I decided to use a pool. This is my code
const pool = new pg.Pool({
  user: 'pooluser',host: 'localhost',database: 'mydb',password: 'pooluser',port: 5432});
pool.on('error', (err, client) =&gt; {
  console.log('error ', err);  process.exit(-1);
});
app.get('/', (req, res)=&gt;{
  pool.connect()
    .then(client =&gt; {
      return client.query('select ....')
            .then(resolved =&gt; {
              client.release();
              console.log(resolved.rows);
            })
            .catch(e =&gt; { 
              client.release();
              console.log('error', e);
            })
      pool.end();
    })
});
In the routes of the CMS, I use client instead of pool that has different db privileges than the pool.
const client = new pg.Client({
  user: 'clientuser',host: 'localhost',database: 'mydb',password: 'clientuser',port: 5432});    
client.connect();
const signup = (user) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    getUser(user.email)
    .then(getUserRes =&gt; {
      if (!getUserRes) {
        return resolved(false);
      }            
            client.query('insert into user(username, password) values ($1,$2)',[user.username,user.password])
              .then(queryRes =&gt; {
                client.end();
                resolved(true);
              })
              .catch(queryError =&gt; {
                client.end();
                rejeted('username already used');
              });
    })
    .catch(getUserError =&gt; {
      return rejeted('error');
    });
  }) 
};
const getUser = (username) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    client.query('select username from user WHERE username= $1',[username])
      .then(res =&gt; {
        client.end();
        if (res.rows.length == 0) {
          return resolved(true);
        }
        resolved(false);
      })
      .catch(e =&gt; {
        client.end();
        console.error('error ', e);
      });
  })
}
In this case if I get a username already used and try to re-post with another username, the query of the getUser never starts and the page hangs. If I remove the client.end(); from both functions, it will work. 
I am confused, so please advice on how and when to disconnect and to completely end a pool or a client. Any hint or explanation or tutorial will be appreciated. 
Thank you
",<node.js><postgresql><node-postgres>,2535,0,68,4312,20,69,130,55,56111,0.0,468,4,51,2018-05-23 21:15,2018-05-27 14:26,2018-05-31 18:40,4.0,8.0,Intermediate,31,"<node.js><postgresql><node-postgres>, when to disconnect and when to end a pg client or pool, My stack is node, express and the pg module. I really try to understand by the documentation and some outdated tutorials. I dont know when and how to disconnect and to end a client.
For some routes I decided to use a pool. This is my code
const pool = new pg.Pool({
  user: 'pooluser',host: 'localhost',database: 'mydb',password: 'pooluser',port: 5432});
pool.on('error', (err, client) =&gt; {
  console.log('error ', err);  process.exit(-1);
});
app.get('/', (req, res)=&gt;{
  pool.connect()
    .then(client =&gt; {
      return client.query('select ....')
            .then(resolved =&gt; {
              client.release();
              console.log(resolved.rows);
            })
            .catch(e =&gt; { 
              client.release();
              console.log('error', e);
            })
      pool.end();
    })
});
In the routes of the CMS, I use client instead of pool that has different db privileges than the pool.
const client = new pg.Client({
  user: 'clientuser',host: 'localhost',database: 'mydb',password: 'clientuser',port: 5432});    
client.connect();
const signup = (user) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    getUser(user.email)
    .then(getUserRes =&gt; {
      if (!getUserRes) {
        return resolved(false);
      }            
            client.query('insert into user(username, password) values ($1,$2)',[user.username,user.password])
              .then(queryRes =&gt; {
                client.end();
                resolved(true);
              })
              .catch(queryError =&gt; {
                client.end();
                rejeted('username already used');
              });
    })
    .catch(getUserError =&gt; {
      return rejeted('error');
    });
  }) 
};
const getUser = (username) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    client.query('select username from user WHERE username= $1',[username])
      .then(res =&gt; {
        client.end();
        if (res.rows.length == 0) {
          return resolved(true);
        }
        resolved(false);
      })
      .catch(e =&gt; {
        client.end();
        console.error('error ', e);
      });
  })
}
In this case if I get a username already used and try to re-post with another username, the query of the getUser never starts and the page hangs. If I remove the client.end(); from both functions, it will work. 
I am confused, so please advice on how and when to disconnect and to completely end a pool or a client. Any hint or explanation or tutorial will be appreciated. 
Thank you
","<node.is><postgresql><node-postures>, discontent end pg client pool, stick node, express pg module. really try understand document outset tutorials. dont know discontent end client. rout decide use pool. code cost pool = new pg.pool({ user: 'pooluser',host: 'localhost',database: 'my',password: 'pooluser',port: 5432}); pool.on('error', (err, client) =&it; { console.log('error ', err); process.exit(-1); }); pp.get('/', (red, yes)=&it;{ pool.connect() .then(client =&it; { return client.query('select ....') .then(resolve =&it; { client.release(); console.log(resolved.rows); }) .catch( =&it; { client.release(); console.log('error', e); }) pool.end(); }) }); rout cms, use client instead pool differ do privilege pool. cost client = new pg.client({ user: 'clientuser',host: 'localhost',database: 'my',password: 'clientuser',port: 5432}); client.connect(); cost sign = (user) =&it; { return new promise((resolved, rejected)=&it;{ geiser(user.email) .then(getuserr =&it; { (!getuserres) { return resolved(false); } client.query('insert user(surname, password) value ($1,$2)',[user.surname,user.password]) .then(query =&it; { client.end(); resolved(true); }) .catch(queryerror =&it; { client.end(); rejected('usernam already used'); }); }) .catch(getusererror =&it; { return rejected('error'); }); }) }; cost gets = (surname) =&it; { return new promise((resolved, rejected)=&it;{ client.query('select usernam user surname= $1',[surname]) .then(r =&it; { client.end(); (yes.rows.length == 0) { return resolved(true); } resolved(false); }) .catch( =&it; { client.end(); console.error('error ', e); }); }) } case get usernam already use try re-post not surname, query gets never start page hangs. remove client.end(); functions, work. confused, pleas advice discontent complete end pool client. hint explain tutor appreciated. thank"
50336378,Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER',"I am using MySQL Workbench 8.0. I am trying to dump test data to DB including all the tables, stored procedures and views with data.
When I try to import it's says import finished with one error and the error is 
  Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER'
  Operation failed with exitcode 1
Also after importing if I check the database, only tables have come but there are no stored procedures at all. 
How would one fix this? 
",<mysql><mysql-workbench><data-import>,458,0,0,549,1,5,12,63,100869,0.0,0,9,51,2018-05-14 17:58,2018-06-06 13:31,,23.0,,Basic,14,"<mysql><mysql-workbench><data-import>, Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER', I am using MySQL Workbench 8.0. I am trying to dump test data to DB including all the tables, stored procedures and views with data.
When I try to import it's says import finished with one error and the error is 
  Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER'
  Operation failed with exitcode 1
Also after importing if I check the database, only tables have come but there are no stored procedures at all. 
How would one fix this? 
","<myself><myself-workbench><data-import>, variable 'sql_mode' can't set value 'no_auto_create_user', use myself workbench 8.0. try dump test data do include tables, store procedure view data. try import say import finish one error error variable 'sql_mode' can't set value 'no_auto_create_user' over fail exitcod 1 also import check database, table come store procedure all. would one fix this?"
57316744,Docker SQL bind: An attempt was made to access a socket in a way forbidden by its access permissions,"Error-message when creating container in Docker for SQL-server (with Admin-rights):
  ""… Error response from daemon: driver failed programming external
  connectivity on endpoint SQL19b
  (cc372bb961fb8178c2461d26bf16c4232a62e01c5f48b8fcec273370506cc095):
  Error starting userland proxy: listen tcp 0.0.0.0:1433: bind: An
  attempt was made to access a socket in a way forbidden by its access
  permissions.""
excerpts from Log-file:
    [21:39:17.692][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:17.696][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (3.9929ms)\n""
[21:39:17.699][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:17.742][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; DELETE /v1.40/containers/22810276e261\n""
[21:39:17.758][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; DELETE /v1.40/containers/22810276e261 (16.129ms)\n""
[21:39:17.759][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.866][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:27.869][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (1.6595ms)\n""
[21:39:27.870][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.894][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist\n""
[21:39:27.908][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage
[21:39:27.909][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:27.909][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""Rewrote mount C:\\Docker\\SQL:/sql (volumeDriver=) to /host_mnt/c/Docker/SQL:/sql""
[21:39:28.049][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist (154.5485ms)\n""
[21:39:28.050][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit\n""
[21:39:28.052][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.080][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage
[21:39:28.082][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.060][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start\n""
[21:39:28.088][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage
[21:39:28.088][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage
[21:39:28.089][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.089][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.067][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point type:bind""
[21:39:28.068][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point:/host_mnt/c/Docker/SQL""
[21:39:28.205][Moby              ][Info   ] [ 2254.975742] docker0: port 1(veth69918f7) entered blocking state
[21:39:28.250][Moby              ][Info   ] [ 2255.087127] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.295][Moby              ][Info   ] [ 2255.132041] device veth69918f7 entered promiscuous mode
[21:39:28.354][Moby              ][Info   ] [ 2255.176944] IPv6: ADDRCONF(NETDEV_UP): veth69918f7: link is not ready
[21:39:28.439][GoBackendProcess  ][Info   ] Adding tcp forward from 0.0.0.0:1433 to 172.17.0.2:1433
[21:39:28.560][Moby              ][Info   ] [ 2255.385920] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.616][Moby              ][Info   ] [ 2255.442735] device veth69918f7 left promiscuous mode
[21:39:28.667][Moby              ][Info   ] [ 2255.497549] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.826][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start (767.0192ms)\n""
[21:39:28.829][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.834][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""Cancel connection...""
[21:39:28.836][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit (786.0411ms)\n""
This leads to a container created, but without the port allocated. Therefore cannot start the SQL server.
Edit1: The port 1433 doesn't seem to be used (at least it is not listed under ""netstat -abn"" )
",<sql-server><docker>,5308,1,37,1388,3,12,11,77,40448,0.0,7,6,51,2019-08-01 20:45,2019-08-10 4:07,,9.0,,Advanced,38,"<sql-server><docker>, Docker SQL bind: An attempt was made to access a socket in a way forbidden by its access permissions, Error-message when creating container in Docker for SQL-server (with Admin-rights):
  ""… Error response from daemon: driver failed programming external
  connectivity on endpoint SQL19b
  (cc372bb961fb8178c2461d26bf16c4232a62e01c5f48b8fcec273370506cc095):
  Error starting userland proxy: listen tcp 0.0.0.0:1433: bind: An
  attempt was made to access a socket in a way forbidden by its access
  permissions.""
excerpts from Log-file:
    [21:39:17.692][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:17.696][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (3.9929ms)\n""
[21:39:17.699][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:17.742][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; DELETE /v1.40/containers/22810276e261\n""
[21:39:17.758][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; DELETE /v1.40/containers/22810276e261 (16.129ms)\n""
[21:39:17.759][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.866][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:27.869][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (1.6595ms)\n""
[21:39:27.870][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.894][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist\n""
[21:39:27.908][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage
[21:39:27.909][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:27.909][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""Rewrote mount C:\\Docker\\SQL:/sql (volumeDriver=) to /host_mnt/c/Docker/SQL:/sql""
[21:39:28.049][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist (154.5485ms)\n""
[21:39:28.050][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit\n""
[21:39:28.052][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.080][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage
[21:39:28.082][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.060][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start\n""
[21:39:28.088][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage
[21:39:28.088][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage
[21:39:28.089][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.089][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.067][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point type:bind""
[21:39:28.068][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point:/host_mnt/c/Docker/SQL""
[21:39:28.205][Moby              ][Info   ] [ 2254.975742] docker0: port 1(veth69918f7) entered blocking state
[21:39:28.250][Moby              ][Info   ] [ 2255.087127] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.295][Moby              ][Info   ] [ 2255.132041] device veth69918f7 entered promiscuous mode
[21:39:28.354][Moby              ][Info   ] [ 2255.176944] IPv6: ADDRCONF(NETDEV_UP): veth69918f7: link is not ready
[21:39:28.439][GoBackendProcess  ][Info   ] Adding tcp forward from 0.0.0.0:1433 to 172.17.0.2:1433
[21:39:28.560][Moby              ][Info   ] [ 2255.385920] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.616][Moby              ][Info   ] [ 2255.442735] device veth69918f7 left promiscuous mode
[21:39:28.667][Moby              ][Info   ] [ 2255.497549] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.826][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start (767.0192ms)\n""
[21:39:28.829][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.834][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""Cancel connection...""
[21:39:28.836][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit (786.0411ms)\n""
This leads to a container created, but without the port allocated. Therefore cannot start the SQL server.
Edit1: The port 1433 doesn't seem to be used (at least it is not listed under ""netstat -abn"" )
","<sal-server><doctor>, doctor sal bind: attempt made access socket way forbidden access permission, error-message great contain doctor sal-server (with admit-rights): ""… error response demon: driver fail program externa connect endpoint sql19b (cc372bb961fb8178c2461d26bf16c4232a62e01c5f48b8fcec273370506cc095): error start overland prove: listen top 0.0.0.0:1433: bind: attempt made access socket way forbidden access permission."" except log-file: [21:39:17.692][apiproxi ][into ] time=""2019-08-01t21:39:17+02:00"" mug=""prove &it;&it; head /being\n"" [21:39:17.696][apiproxi ][into ] time=""2019-08-01t21:39:17+02:00"" mug=""prove &it;&it; head /being (3.9929ms)\n"" [21:39:17.699][gobackendprocess ][into ] error closewrit to: pipe closed. [21:39:17.742][apiproxi ][into ] time=""2019-08-01t21:39:17+02:00"" mug=""prove &it;&it; delete /ve.40/container/22810276e261\n"" [21:39:17.758][apiproxi ][into ] time=""2019-08-01t21:39:17+02:00"" mug=""prove &it;&it; delete /ve.40/container/22810276e261 (16.129ms)\n"" [21:39:17.759][gobackendprocess ][into ] error closewrit to: pipe closed. [21:39:27.866][apiproxi ][into ] time=""2019-08-01t21:39:27+02:00"" mug=""prove &it;&it; head /being\n"" [21:39:27.869][apiproxi ][into ] time=""2019-08-01t21:39:27+02:00"" mug=""prove &it;&it; head /being (1.6595ms)\n"" [21:39:27.870][gobackendprocess ][into ] error closewrit to: pipe closed. [21:39:27.894][apiproxi ][into ] time=""2019-08-01t21:39:27+02:00"" mug=""prove &it;&it; post /ve.40/container/create?name=sqllinuxlocalpersist\n"" [21:39:27.908][apirequestlogg ][into ] [db460e2b-7d77-4756-be-665715a9a182] post http://unit/usage [21:39:27.909][apirequestlogg ][into ] [db460e2b-7d77-4756-be-665715a9a182] post http://unit/usage -&it; 200 ok took am [21:39:27.909][apiproxi ][into ] time=""2019-08-01t21:39:27+02:00"" mug=""report mount c:\\doctor\\sal:/sal (volumedriver=) /host_mnt/c/doctor/sal:/sal"" [21:39:28.049][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""prove &it;&it; post /ve.40/container/create?name=sqllinuxlocalpersist (154.5485ms)\n"" [21:39:28.050][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""prove &it;&it; post /ve.40/container/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit\n"" [21:39:28.052][gobackendprocess ][into ] error closewrit to: pipe closed. [21:39:28.080][apirequestlogg ][into ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] post http://unit/usage [21:39:28.082][apirequestlogg ][into ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] post http://unit/usage -&it; 200 ok took am [21:39:28.060][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""prove &it;&it; post /ve.40/container/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start\n"" [21:39:28.088][apirequestlogg ][into ] [89bf69bf-5084-dub-a887-c7acb99bf131] post http://unit/usage [21:39:28.088][apirequestlogg ][into ] [6ca0e28f-bad-4f66-face-43f6d486c8a2] post http://unit/usage [21:39:28.089][apirequestlogg ][into ] [89bf69bf-5084-dub-a887-c7acb99bf131] post http://unit/usage -&it; 200 ok took am [21:39:28.089][apirequestlogg ][into ] [6ca0e28f-bad-4f66-face-43f6d486c8a2] post http://unit/usage -&it; 200 ok took am [21:39:28.067][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""mount point type:bind"" [21:39:28.068][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""mount point:/host_mnt/c/doctor/sal"" [21:39:28.205][mob ][into ] [ 2254.975742] docker0: port 1(veth69918f7) enter block state [21:39:28.250][mob ][into ] [ 2255.087127] docker0: port 1(veth69918f7) enter distal state [21:39:28.295][mob ][into ] [ 2255.132041] devil veth69918f7 enter promised mode [21:39:28.354][mob ][into ] [ 2255.176944] iv: addrconf(netdev_up): veth69918f7: link ready [21:39:28.439][gobackendprocess ][into ] ad top forward 0.0.0.0:1433 172.17.0.2:1433 [21:39:28.560][mob ][into ] [ 2255.385920] docker0: port 1(veth69918f7) enter distal state [21:39:28.616][mob ][into ] [ 2255.442735] devil veth69918f7 left promised mode [21:39:28.667][mob ][into ] [ 2255.497549] docker0: port 1(veth69918f7) enter distal state [21:39:28.826][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""prove &it;&it; post /ve.40/container/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start (767.0192ms)\n"" [21:39:28.829][gobackendprocess ][into ] error closewrit to: pipe closed. [21:39:28.834][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""cancel connection..."" [21:39:28.836][apiproxi ][into ] time=""2019-08-01t21:39:28+02:00"" mug=""prove &it;&it; post /ve.40/container/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit (786.0411ms)\n"" lead contain created, without port allocate. therefore cannot start sal server. edit: port 1433 seem use (at least list ""netstat -an"" )"
64677836,SQLSTATE[HY000]: General error: 1835 Malformed communication packet on LARAVEL,"Suddenly got
SQLSTATE[HY000]: General error: 1835 Malformed communication packet (SQL: select * from tb_users where (username = 121211) limit 1)
on Laravel.
I already checked this: MySQL: ERROR 2027 (HY000): Malformed packet, but it seems a different case.
I've successfully logged in to MySQL after previously login using SSH (using: mysql -u -p).
I've successfully logged in to MySQL directly from a remote PC (using: mysql -h [IP] -u -p).
But my Laravel got the error I mentioned before. Any experience in this?
",<mysql><laravel><mariadb><mariadb-10.3>,515,1,2,845,1,7,8,79,20380,0.0,0,16,50,2020-11-04 9:59,2020-11-04 10:28,,0.0,,Basic,10,"<mysql><laravel><mariadb><mariadb-10.3>, SQLSTATE[HY000]: General error: 1835 Malformed communication packet on LARAVEL, Suddenly got
SQLSTATE[HY000]: General error: 1835 Malformed communication packet (SQL: select * from tb_users where (username = 121211) limit 1)
on Laravel.
I already checked this: MySQL: ERROR 2027 (HY000): Malformed packet, but it seems a different case.
I've successfully logged in to MySQL after previously login using SSH (using: mysql -u -p).
I've successfully logged in to MySQL directly from a remote PC (using: mysql -h [IP] -u -p).
But my Laravel got the error I mentioned before. Any experience in this?
","<myself><travel><maria><maria-10.3>, sqlstate[hy000]: genet error: 1835 malform common packet travel, suddenly got sqlstate[hy000]: genet error: 1835 malform common packet (sal: select * trouser (usernam = 121211) limit 1) travel. already check this: myself: error 2027 (hy000): malform packet, seem differ case. i'v success log myself previous login use ash (using: myself -u -p). i'v success log myself directly remote pp (using: myself -h [in] -u -p). travel got error mention before. expert this?"
48448473,Pyspark convert a standard list to data frame,"The case is really simple, I need to convert a python list into data frame with following code
from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import StringType, IntegerType
schema = StructType([StructField(""value"", IntegerType(), True)])
my_list = [1, 2, 3, 4]
rdd = sc.parallelize(my_list)
df = sqlContext.createDataFrame(rdd, schema)
df.show()
it failed with following error:
    raise TypeError(""StructType can not accept object %r in type %s"" % (obj, type(obj)))
TypeError: StructType can not accept object 1 in type &lt;class 'int'&gt;
",<python><apache-spark><pyspark><apache-spark-sql>,600,0,12,1497,3,17,26,78,146152,0.0,39,2,50,2018-01-25 17:13,2018-01-25 17:25,2018-01-25 21:21,0.0,0.0,Basic,10,"<python><apache-spark><pyspark><apache-spark-sql>, Pyspark convert a standard list to data frame, The case is really simple, I need to convert a python list into data frame with following code
from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import StringType, IntegerType
schema = StructType([StructField(""value"", IntegerType(), True)])
my_list = [1, 2, 3, 4]
rdd = sc.parallelize(my_list)
df = sqlContext.createDataFrame(rdd, schema)
df.show()
it failed with following error:
    raise TypeError(""StructType can not accept object %r in type %s"" % (obj, type(obj)))
TypeError: StructType can not accept object 1 in type &lt;class 'int'&gt;
","<patron><apache-spark><spark><apache-spark-sal>, spark convert standard list data frame, case really simple, need convert patron list data frame follow code spark.sal.type import structtyp spark.sal.type import structfield spark.sal.type import stringtype, integertyp scheme = structtype([structfield(""value"", integertype(), true)]) my_list = [1, 2, 3, 4] red = s.parallelize(my_list) of = sqlcontext.createdataframe(red, scheme) of.show() fail follow error: rays typeerror(""structtyp accept object %r type %s"" % (obs, type(obs))) typeerror: structtyp accept object 1 type &it;class 'in'&it;"
57128891,How repair corrupt xampp 'mysql.user' table?,"I used Xampp yesterday to create some simple Web-based utility tool.
Today I wanted to continue working on it but xampp control panel gave me some weir errors.
This is the MySQL Error Log:
2019-07-20 23:47:13 0 [Note] InnoDB: Uses event mutexes
2019-07-20 23:47:13 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-07-20 23:47:13 0 [Note] InnoDB: Number of pools: 1
2019-07-20 23:47:13 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-07-20 23:47:13 0 [Note] InnoDB: Initializing buffer pool, total size = 16M, instances = 1, chunk size = 16M
2019-07-20 23:47:13 0 [Note] InnoDB: Completed initialization of buffer pool
2019-07-20 23:47:13 0 [Note] InnoDB: Starting crash recovery from checkpoint LSN=1819402
2019-07-20 23:47:14 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-07-20 23:47:14 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
2019-07-20 23:47:14 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-07-20 23:47:14 0 [Note] InnoDB: Setting file 'C:\xampp\mysql\data\ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-07-20 23:47:14 0 [Note] InnoDB: File 'C:\xampp\mysql\data\ibtmp1' size is now 12 MB.
2019-07-20 23:47:14 0 [Note] InnoDB: Waiting for purge to start
2019-07-20 23:47:14 0 [Note] InnoDB: 10.3.16 started; log sequence number 1819411; transaction id 257
2019-07-20 23:47:14 0 [Note] InnoDB: Loading buffer pool(s) from C:\xampp\mysql\data\ib_buffer_pool
2019-07-20 23:47:14 0 [Note] InnoDB: Buffer pool(s) load completed at 190720 23:47:14
2019-07-20 23:47:14 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-07-20 23:47:14 0 [Note] Server socket created on IP: '127.0.0.1'.
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Table '.\mysql\user' is marked as crashed and should be repaired
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Index for table '.\mysql\user' is corrupt; try to repair it
2019-07-20 23:47:14 0 [ERROR] Couldn't repair table: mysql.user
2019-07-20 23:47:14 0 [ERROR] Fatal error: Can't open and lock privilege tables: Index for table 'user' is corrupt; try to repair it
Tried already to repair, but the mySQL Service won't even start, so I'm kinda helpless...
",<mysql><xampp>,2184,0,26,601,1,5,4,69,178564,0.0,0,25,50,2019-07-20 21:58,2019-07-26 2:05,,6.0,,Basic,10,"<mysql><xampp>, How repair corrupt xampp 'mysql.user' table?, I used Xampp yesterday to create some simple Web-based utility tool.
Today I wanted to continue working on it but xampp control panel gave me some weir errors.
This is the MySQL Error Log:
2019-07-20 23:47:13 0 [Note] InnoDB: Uses event mutexes
2019-07-20 23:47:13 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-07-20 23:47:13 0 [Note] InnoDB: Number of pools: 1
2019-07-20 23:47:13 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-07-20 23:47:13 0 [Note] InnoDB: Initializing buffer pool, total size = 16M, instances = 1, chunk size = 16M
2019-07-20 23:47:13 0 [Note] InnoDB: Completed initialization of buffer pool
2019-07-20 23:47:13 0 [Note] InnoDB: Starting crash recovery from checkpoint LSN=1819402
2019-07-20 23:47:14 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-07-20 23:47:14 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
2019-07-20 23:47:14 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-07-20 23:47:14 0 [Note] InnoDB: Setting file 'C:\xampp\mysql\data\ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-07-20 23:47:14 0 [Note] InnoDB: File 'C:\xampp\mysql\data\ibtmp1' size is now 12 MB.
2019-07-20 23:47:14 0 [Note] InnoDB: Waiting for purge to start
2019-07-20 23:47:14 0 [Note] InnoDB: 10.3.16 started; log sequence number 1819411; transaction id 257
2019-07-20 23:47:14 0 [Note] InnoDB: Loading buffer pool(s) from C:\xampp\mysql\data\ib_buffer_pool
2019-07-20 23:47:14 0 [Note] InnoDB: Buffer pool(s) load completed at 190720 23:47:14
2019-07-20 23:47:14 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-07-20 23:47:14 0 [Note] Server socket created on IP: '127.0.0.1'.
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Table '.\mysql\user' is marked as crashed and should be repaired
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Index for table '.\mysql\user' is corrupt; try to repair it
2019-07-20 23:47:14 0 [ERROR] Couldn't repair table: mysql.user
2019-07-20 23:47:14 0 [ERROR] Fatal error: Can't open and lock privilege tables: Index for table 'user' is corrupt; try to repair it
Tried already to repair, but the mySQL Service won't even start, so I'm kinda helpless...
","<myself><camp>, repair corrupt camp 'myself.user' table?, use camp yesterday great simple web-was until tool. today want continue work camp control panel gave weir errors. myself error log: 2019-07-20 23:47:13 0 [note] innodb: use event mute 2019-07-20 23:47:13 0 [note] innodb: compress table use limb 1.2.11 2019-07-20 23:47:13 0 [note] innodb: number pools: 1 2019-07-20 23:47:13 0 [note] innodb: use she crc32 instruct 2019-07-20 23:47:13 0 [note] innodb: into suffer pool, total size = him, instant = 1, chink size = him 2019-07-20 23:47:13 0 [note] innodb: complete into suffer pool 2019-07-20 23:47:13 0 [note] innodb: start crash recovery checkpoint isn=1819402 2019-07-20 23:47:14 0 [note] innodb: 128 128 rollback segment active. 2019-07-20 23:47:14 0 [note] innodb: remove temporary tablespac data file: ""ibtmp1"" 2019-07-20 23:47:14 0 [note] innodb: great share tablespac temporary table 2019-07-20 23:47:14 0 [note] innodb: set file 'c:\camp\myself\data\ibtmp1' size 12 mb. physics write file full; pleas wait ... 2019-07-20 23:47:14 0 [note] innodb: file 'c:\camp\myself\data\ibtmp1' size 12 mb. 2019-07-20 23:47:14 0 [note] innodb: wait pure start 2019-07-20 23:47:14 0 [note] innodb: 10.3.16 started; log sequence number 1819411; transact id 257 2019-07-20 23:47:14 0 [note] innodb: load suffer pool(s) c:\camp\myself\data\ib_buffer_pool 2019-07-20 23:47:14 0 [note] innodb: suffer pool(s) load complete 190720 23:47:14 2019-07-20 23:47:14 0 [note] plain 'feedback' disabled. 2019-07-20 23:47:14 0 [note] server socket great in: '127.0.0.1'. 2019-07-20 23:47:14 0 [error] myself.eye: table '.\myself\user' mark crash repair 2019-07-20 23:47:14 0 [error] myself.eye: index table '.\myself\user' corrupt; try repair 2019-07-20 23:47:14 0 [error] repair table: myself.us 2019-07-20 23:47:14 0 [error] fatal error: can't open lock privilege tables: index table 'user' corrupt; try repair try already repair, myself service even start, i'm kind helpless..."
54187241,EF Core Connection to Azure SQL with Managed Identity,"I am using EF Core to connect to a Azure SQL Database deployed to Azure App Services. I am using an access token (obtained via the Managed Identities) to connect to Azure SQL database.
Here is how I am doing that:
Startup.cs:
public void ConfigureServices(IServiceCollection services)
{
    //code ignored for simplicity
    services.AddDbContext&lt;MyCustomDBContext&gt;();
    services.AddTransient&lt;IDBAuthTokenService, AzureSqlAuthTokenService&gt;();
}
MyCustomDBContext.cs
public partial class MyCustomDBContext : DbContext
{
    public IConfiguration Configuration { get; }
    public IDBAuthTokenService authTokenService { get; set; }
    public CortexContext(IConfiguration configuration, IDBAuthTokenService tokenService, DbContextOptions&lt;MyCustomDBContext&gt; options)
        : base(options)
    {
        Configuration = configuration;
        authTokenService = tokenService;
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        SqlConnection connection = new SqlConnection();
        connection.ConnectionString = Configuration.GetConnectionString(""defaultConnection"");
        connection.AccessToken = authTokenService.GetToken().Result;
        optionsBuilder.UseSqlServer(connection);
    }
}
AzureSqlAuthTokenService.cs
public class AzureSqlAuthTokenService : IDBAuthTokenService
{
    public async Task&lt;string&gt; GetToken()
    {
        AzureServiceTokenProvider provider = new AzureServiceTokenProvider();
        var token = await provider.GetAccessTokenAsync(""https://database.windows.net/"");
        return token;
    }
}
This works fine and I can get data from the database. But I am not sure if this is the right way to do it.
My questions:
Is this a right way to do it or will it have issues with performance?
Do I need to worry about token expiration? I am not caching the token as of now.
Does EF Core has any better way to handle this?
",<c#><entity-framework-core><azure-active-directory><azure-sql-database><ef-core-2.2>,1924,1,38,973,1,13,28,56,18126,0.0,11,6,49,2019-01-14 18:32,2019-01-15 2:54,2019-01-15 2:54,1.0,1.0,Intermediate,18,"<c#><entity-framework-core><azure-active-directory><azure-sql-database><ef-core-2.2>, EF Core Connection to Azure SQL with Managed Identity, I am using EF Core to connect to a Azure SQL Database deployed to Azure App Services. I am using an access token (obtained via the Managed Identities) to connect to Azure SQL database.
Here is how I am doing that:
Startup.cs:
public void ConfigureServices(IServiceCollection services)
{
    //code ignored for simplicity
    services.AddDbContext&lt;MyCustomDBContext&gt;();
    services.AddTransient&lt;IDBAuthTokenService, AzureSqlAuthTokenService&gt;();
}
MyCustomDBContext.cs
public partial class MyCustomDBContext : DbContext
{
    public IConfiguration Configuration { get; }
    public IDBAuthTokenService authTokenService { get; set; }
    public CortexContext(IConfiguration configuration, IDBAuthTokenService tokenService, DbContextOptions&lt;MyCustomDBContext&gt; options)
        : base(options)
    {
        Configuration = configuration;
        authTokenService = tokenService;
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        SqlConnection connection = new SqlConnection();
        connection.ConnectionString = Configuration.GetConnectionString(""defaultConnection"");
        connection.AccessToken = authTokenService.GetToken().Result;
        optionsBuilder.UseSqlServer(connection);
    }
}
AzureSqlAuthTokenService.cs
public class AzureSqlAuthTokenService : IDBAuthTokenService
{
    public async Task&lt;string&gt; GetToken()
    {
        AzureServiceTokenProvider provider = new AzureServiceTokenProvider();
        var token = await provider.GetAccessTokenAsync(""https://database.windows.net/"");
        return token;
    }
}
This works fine and I can get data from the database. But I am not sure if this is the right way to do it.
My questions:
Is this a right way to do it or will it have issues with performance?
Do I need to worry about token expiration? I am not caching the token as of now.
Does EF Core has any better way to handle this?
","<c#><entity-framework-core><azure-active-directory><azure-sal-database><of-core-2.2>, of core connect azur sal manage identity, use of core connect azur sal database deploy azur pp services. use access token (obtain via manage identified) connect azur sal database. that: started.is: public void configureservices(iservicecollect services) { //code ignore simple services.adddbcontext&it;mycustomdbcontext&it;(); services.addtransient&it;idbauthtokenservice, azuresqlauthtokenservice&it;(); } mycustomdbcontext.c public partial class mycustomdbcontext : context { public iconfigur configur { get; } public idbauthtokenservic authtokenservic { get; set; } public cortexcontext(iconfigur configuration, idbauthtokenservic tokenservice, dbcontextoptions&it;mycustomdbcontext&it; option) : base(option) { configur = configuration; authtokenservic = tokenservice; } protect overdid void onconfiguring(dbcontextoptionsbuild optionsbuilder) { sqlconnect connect = new sqlconnection(); connection.connections = configuration.getconnectionstring(""defaultconnection""); connection.accesstoken = authtokenservice.gettoken().result; optionsbuilder.usesqlserver(connection); } } azuresqlauthtokenservice.c public class azuresqlauthtokenservic : idbauthtokenservic { public async task&it;string&it; gettoken() { azureservicetokenprovid proved = new azureservicetokenprovider(); war token = await provider.getaccesstokenasync(""http://database.windows.net/""); return token; } } work fine get data database. sure right way it. questions: right way issue performance? need worry token expiration? each token now. of core better way hand this?"
54504230,"How to fix ""Error executing DDL ""alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq"" via JDBC Statement""","I'm trying to start a Spring Boot project with a MySQL database, but I have some problem with the database. I try to start my application that, and server is running, but Hibernate doesn’t create Tables, etc.
This is my code:
User Entity
 @Entity
   public class User {
      @Id
      @GeneratedValue(strategy = IDENTITY)
      private Long id;
      private String firstName;
      private String lastName;
      private String email;
      private String password;
      private String description;
      private String profile_photo;
      private LocalDate create;
      private LocalDate update;
      @OneToMany(mappedBy = &quot;eventOwner&quot;)
      private List&lt;Event&gt; ownedEvents;
           public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public String getFirstName() {
    return firstName;
}
public void setFirstName(String firstName) {
    this.firstName = firstName;
}
public String getLastName() {
    return lastName;
}
public void setLastName(String lastName) {
    this.lastName = lastName;
}
public String getEmail() {
    return email;
}
public void setEmail(String email) {
    this.email = email;
}
public String getPassword() {
    return password;
}
public void setPassword(String password) {
    this.password = password;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getProfile_photo() {
    return profile_photo;
}
public void setProfile_photo(String profile_photo) {
    this.profile_photo = profile_photo;
}
public LocalDate getCreate() {
    return create;
}
public void setCreate(LocalDate create) {
    this.create = create;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public List&lt;Event&gt; getOwnedEvents() {
    return ownedEvents;
}
public void setOwnedEvents(List&lt;Event&gt; ownedEvents) {
    this.ownedEvents = ownedEvents;
}}
Event Entity
   @Entity
   @Table(name = &quot;events&quot;)
   public class Event {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private Double longitude;
private Double latitude;
private String description;
private String header;
private LocalDate startData;
private LocalDate endData;
private LocalDate creat;
private LocalDate update;
private Filters filters;
@ManyToOne
@JoinColumn(name = &quot;owner_id&quot;)
private User eventOwner;
public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public Double getLongitude() {
    return longitude;
}
public void setLongitude(Double longitude) {
    this.longitude = longitude;
}
public Double getLatitude() {
    return latitude;
}
public void setLatitude(Double latitude) {
    this.latitude = latitude;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getHeader() {
    return header;
}
public void setHeader(String header) {
    this.header = header;
}
public LocalDate getStartData() {
    return startData;
}
public void setStartData(LocalDate startData) {
    this.startData = startData;
}
public LocalDate getEndData() {
    return endData;
}
public void setEndData(LocalDate endData) {
    this.endData = endData;
}
public LocalDate getCreat() {
    return creat;
}
public void setCreat(LocalDate creat) {
    this.creat = creat;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public Filters getFilters() {
    return filters;
}
public void setFilters(Filters filters) {
    this.filters = filters;
}
public User getEventOwner() {
    return eventOwner;
}
public void setEventOwner(User eventOwner) {
    this.eventOwner = eventOwner;
}
}
And these are my properties:
spring.datasource.url= jdbc:mysql://localhost:3306/some_database?
requireSSL=false&amp;useSSL=false
spring.datasource.username= user
spring.datasource.password= passw
logging.level.org.hibernate.SQL= DEBUG
spring.jpa.hibernate.ddl-auto = create-drop
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL55Dialect
This is the error I get:
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq&quot; via
JDBC Statement
and
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;create table events (id bigint not null auto_increment, creat date,
description varchar(255), end_data date, event_type integer, max_age
integer not null, min_age integer not null, open_to_changes bit not null,
pets_allowed bit not null, price_range integer, smoking_allowed bit not
null, header varchar(255), latitude double precision, longitude double
precision, start_data date, update date, owner_id bigint, primary key (id))
engine=InnoDB&quot; via JDBC Statement
at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67) ~[hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:440) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:424) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:315) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:155) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:310) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
How can I fix that?
",<java><mysql><hibernate><spring-boot>,10315,0,262,654,1,6,10,50,132840,0.0,25,22,48,2019-02-03 15:09,2019-02-04 10:16,2019-05-19 10:16,1.0,105.0,Basic,13,"<java><mysql><hibernate><spring-boot>, How to fix ""Error executing DDL ""alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq"" via JDBC Statement"", I'm trying to start a Spring Boot project with a MySQL database, but I have some problem with the database. I try to start my application that, and server is running, but Hibernate doesn’t create Tables, etc.
This is my code:
User Entity
 @Entity
   public class User {
      @Id
      @GeneratedValue(strategy = IDENTITY)
      private Long id;
      private String firstName;
      private String lastName;
      private String email;
      private String password;
      private String description;
      private String profile_photo;
      private LocalDate create;
      private LocalDate update;
      @OneToMany(mappedBy = &quot;eventOwner&quot;)
      private List&lt;Event&gt; ownedEvents;
           public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public String getFirstName() {
    return firstName;
}
public void setFirstName(String firstName) {
    this.firstName = firstName;
}
public String getLastName() {
    return lastName;
}
public void setLastName(String lastName) {
    this.lastName = lastName;
}
public String getEmail() {
    return email;
}
public void setEmail(String email) {
    this.email = email;
}
public String getPassword() {
    return password;
}
public void setPassword(String password) {
    this.password = password;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getProfile_photo() {
    return profile_photo;
}
public void setProfile_photo(String profile_photo) {
    this.profile_photo = profile_photo;
}
public LocalDate getCreate() {
    return create;
}
public void setCreate(LocalDate create) {
    this.create = create;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public List&lt;Event&gt; getOwnedEvents() {
    return ownedEvents;
}
public void setOwnedEvents(List&lt;Event&gt; ownedEvents) {
    this.ownedEvents = ownedEvents;
}}
Event Entity
   @Entity
   @Table(name = &quot;events&quot;)
   public class Event {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private Double longitude;
private Double latitude;
private String description;
private String header;
private LocalDate startData;
private LocalDate endData;
private LocalDate creat;
private LocalDate update;
private Filters filters;
@ManyToOne
@JoinColumn(name = &quot;owner_id&quot;)
private User eventOwner;
public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public Double getLongitude() {
    return longitude;
}
public void setLongitude(Double longitude) {
    this.longitude = longitude;
}
public Double getLatitude() {
    return latitude;
}
public void setLatitude(Double latitude) {
    this.latitude = latitude;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getHeader() {
    return header;
}
public void setHeader(String header) {
    this.header = header;
}
public LocalDate getStartData() {
    return startData;
}
public void setStartData(LocalDate startData) {
    this.startData = startData;
}
public LocalDate getEndData() {
    return endData;
}
public void setEndData(LocalDate endData) {
    this.endData = endData;
}
public LocalDate getCreat() {
    return creat;
}
public void setCreat(LocalDate creat) {
    this.creat = creat;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public Filters getFilters() {
    return filters;
}
public void setFilters(Filters filters) {
    this.filters = filters;
}
public User getEventOwner() {
    return eventOwner;
}
public void setEventOwner(User eventOwner) {
    this.eventOwner = eventOwner;
}
}
And these are my properties:
spring.datasource.url= jdbc:mysql://localhost:3306/some_database?
requireSSL=false&amp;useSSL=false
spring.datasource.username= user
spring.datasource.password= passw
logging.level.org.hibernate.SQL= DEBUG
spring.jpa.hibernate.ddl-auto = create-drop
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL55Dialect
This is the error I get:
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq&quot; via
JDBC Statement
and
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;create table events (id bigint not null auto_increment, creat date,
description varchar(255), end_data date, event_type integer, max_age
integer not null, min_age integer not null, open_to_changes bit not null,
pets_allowed bit not null, price_range integer, smoking_allowed bit not
null, header varchar(255), latitude double precision, longitude double
precision, start_data date, update date, owner_id bigint, primary key (id))
engine=InnoDB&quot; via JDBC Statement
at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67) ~[hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:440) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:424) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:315) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:155) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:310) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
How can I fix that?
","<cava><myself><liberate><spring-boot>, fix ""error execute del ""alter table event drop foreign key fkg0mkvgsqn8584qoql6a2rxheq"" via job statement"", i'm try start spring boot project myself database, problem database. try start applied that, server running, wiberd doesn’t great tables, etc. code: user entity @entity public class user { @id @generatedvalue(strategic = identity) privat long id; privat string firstname; privat string lastname; privat string email; privat string password; privat string description; privat string profile_photo; privat local create; privat local update; @onetomany(mapped = &quit;eventowner&quit;) privat list&it;event&it; ownedevents; public long get() { return id; } public void said(long id) { this.id = id; } public string getfirstname() { return firstname; } public void setfirstname(sir firstname) { this.firstnam = firstname; } public string getlastname() { return lastname; } public void setlastname(sir lastname) { this.lastnam = lastname; } public string getemail() { return email; } public void setemail(sir email) { this.email = email; } public string getpassword() { return password; } public void setpassword(sir password) { this.password = password; } public string getdescription() { return description; } public void setdescription(sir description) { this.rescript = description; } public string getprofile_photo() { return profile_photo; } public void setprofile_photo(sir profile_photo) { this.profile_photo = profile_photo; } public local getcreate() { return create; } public void secrete(local create) { this.great = create; } public local getupdate() { return update; } public void setupdate(local update) { this.up = update; } public list&it;event&it; getownedevents() { return ownedevents; } public void setownedevents(list&it;event&it; ownedevents) { this.owned = ownedevents; }} event entity @entity @table(am = &quit;events&quit;) public class event { @id @generatedvalue(strategic = generationtype.identity) privat long id; privat doubt longitude; privat doubt latitude; privat string description; privat string header; privat local startdata; privat local enemata; privat local great; privat local update; privat filter filters; @manytoon @joincolumn(am = &quit;owner_id&quit;) privat user eventowner; public long get() { return id; } public void said(long id) { this.id = id; } public doubt getlongitude() { return longitude; } public void setlongitude(doubt longitude) { this.longitude = longitude; } public doubt getlatitude() { return latitude; } public void setlatitude(doubt latitude) { this.latitude = latitude; } public string getdescription() { return description; } public void setdescription(sir description) { this.rescript = description; } public string getheader() { return header; } public void setheader(sir header) { this.head = header; } public local getstartdata() { return startdata; } public void setstartdata(local startdata) { this.startdata = startdata; } public local getenddata() { return enemata; } public void setenddata(local enemata) { this.enemata = enemata; } public local retreat() { return great; } public void retreat(local great) { this.great = great; } public local getupdate() { return update; } public void setupdate(local update) { this.up = update; } public filter getfilters() { return filters; } public void setfilters(felt filters) { this.felt = filters; } public user geteventowner() { return eventowner; } public void seteventowner(us eventowner) { this.eventown = eventowner; } } properties: spring.datasource.curl= job:myself://localhost:3306/some_database? requires=false&amp;useful=fall spring.datasource.surname= user spring.datasource.password= pass logging.level.org.liberate.sal= debut spring.pa.liberate.del-auto = create-drop spring.pa.properties.liberate.dialect=org.liberate.dialect.mysql55dialect error get: org.liberate.tool.scheme.spy.commandacceptanceexception: error execute del &quit;at table event drop foreign key fkg0mkvgsqn8584qoql6a2rxheq&quit; via job statement org.liberate.tool.scheme.spy.commandacceptanceexception: error execute del &quit;great table event (id begin null auto_increment, great date, rescript varchar(255), end_data date, event_typ inter, max_ag inter null, mining inter null, open_to_chang bit null, pets_allow bit null, price_rang inter, smoking_allow bit null, header varchar(255), latitude doubt precision, longitude doubt precision, start_data date, update date, owner_id begin, primary key (id)) engine=innodb&quit; via job statement org.liberate.tool.scheme.internal.even.generationtargettodatabase.accept(generationtargettodatabase.cava:67) ~[liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.applysqlstring(schemacreatorimpl.cava:440) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.applysqlstrings(schemacreatorimpl.cava:424) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.createfrommetadata(schemacreatorimpl.cava:315) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.performcreation(schemacreatorimpl.cava:166) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.creation(schemacreatorimpl.cava:135) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.internal.schemacreatorimpl.creation(schemacreatorimpl.cava:121) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.spy.schemamanagementtoolcoordinator.performdatabaseaction(schemamanagementtoolcoordinator.cava:155) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.tool.scheme.spy.schemamanagementtoolcoordinator.process(schemamanagementtoolcoordinator.cava:72) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.internal.sessionfactoryimpl.&it;knit&it;(sessionfactoryimpl.cava:310) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.boot.internal.sessionfactorybuilderimpl.build(sessionfactorybuilderimpl.cava:467) [liberate-core-5.3.7.final.jar:5.3.7.final] org.liberate.pa.boot.internal.entitymanagerfactorybuilderimpl.build(entitymanagerfactorybuilderimpl.cava:939) [liberate-core-5.3.7.final.jar:5.3.7.final] org.springframework.or.pa.vendor.springhibernatejpapersistenceprovider.createcontainerentitymanagerfactory(springhibernatejpapersistenceprovider.cava:57) [spring-or-5.1.4.release.jar:5.1.4.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.createnativeentitymanagerfactory(localcontainerentitymanagerfactorybean.cava:365) [spring-or-5.1.4.release.jar:5.1.4.release] org.springframework.or.pa.abstractentitymanagerfactorybean.buildnativeentitymanagerfactory(abstractentitymanagerfactorybean.cava:390) [spring-or-5.1.4.release.jar:5.1.4.release] org.springframework.or.pa.abstractentitymanagerfactorybean.afterpropertiesset(abstractentitymanagerfactorybean.cava:377) [spring-or-5.1.4.release.jar:5.1.4.release] org.springframework.or.pa.localcontainerentitymanagerfactorybean.afterpropertiesset(localcontainerentitymanagerfactorybean.cava:341) [spring-or-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.invokeinitmethods(abstractautowirecapablebeanfactory.cava:1804) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1741) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:576) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:498) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:320) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) ~[spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:318) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) [spring-beans-5.1.4.release.jar:5.1.4.release] org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1083) ~[spring-context-5.1.4.release.jar:5.1.4.release] org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:853) ~[spring-context-5.1.4.release.jar:5.1.4.release] org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:546) ~[spring-context-5.1.4.release.jar:5.1.4.release] org.springframework.boot.web.serve.context.servletwebserverapplicationcontext.refresh(servletwebserverapplicationcontext.cava:142) ~[spring-boot-2.1.2.release.jar:2.1.2.release] org.springframework.boot.springapplication.refresh(springapplication.cava:775) ~[spring-boot-2.1.2.release.jar:2.1.2.release] org.springframework.boot.springapplication.refreshcontext(springapplication.cava:397) ~[spring-boot-2.1.2.release.jar:2.1.2.release] org.springframework.boot.springapplication.run(springapplication.cava:316) ~[spring-boot-2.1.2.release.jar:2.1.2.release] org.springframework.boot.springapplication.run(springapplication.cava:1260) ~[spring-boot-2.1.2.release.jar:2.1.2.release] fix that?"
63361962,ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access,"I'm trying:
mysql&gt; 
LOAD DATA LOCAL INFILE '/var/tmp/countries.csv' 
INTO TABLE countries 
FIELDS TERMINATED BY ',' 
ENCLOSED BY '&quot;' LINES 
TERMINATED BY '\n' 
IGNORE 1 LINES 
(CountryId,CountryCode,CountryDescription,CountryRegion,LastUpdatedDate,created_by,created_on)
SET created_by = 'DH_INITIAL_LOAD', created_on = current_timestamp();
ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.`
It was working fine, I downloaded pymysql and mysql connector for the python script. I uninstalled and checked still it is not working.
The verion and infile is ON,
 select version() -| 8.0.17
mysql&gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| local_infile  | ON    |
+---------------+-------+
1 row in set (0.00 sec)
",<mysql><load>,846,0,21,1123,1,6,11,37,130807,0.0,4,10,48,2020-08-11 15:58,2020-08-13 5:40,2020-08-13 5:40,2.0,2.0,Basic,14,"<mysql><load>, ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access, I'm trying:
mysql&gt; 
LOAD DATA LOCAL INFILE '/var/tmp/countries.csv' 
INTO TABLE countries 
FIELDS TERMINATED BY ',' 
ENCLOSED BY '&quot;' LINES 
TERMINATED BY '\n' 
IGNORE 1 LINES 
(CountryId,CountryCode,CountryDescription,CountryRegion,LastUpdatedDate,created_by,created_on)
SET created_by = 'DH_INITIAL_LOAD', created_on = current_timestamp();
ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.`
It was working fine, I downloaded pymysql and mysql connector for the python script. I uninstalled and checked still it is not working.
The verion and infile is ON,
 select version() -| 8.0.17
mysql&gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| local_infile  | ON    |
+---------------+-------+
1 row in set (0.00 sec)
","<myself><load>, error 2068 (hy000): load data local until file request reject due restrict access, i'm trying: myself&it; load data local until '/war/tm/countries.is' table country field german ',' enclose '&quit;' line german '\n' ignore 1 line (country,countryside,countrydescription,countryregion,lastupdateddate,created_by,created_on) set created_bi = 'dh_initial_load', created_on = current_timestamp(); error 2068 (hy000): load data local until file request reject due restrict access.` work fine, download pymysql myself connection patron script. instal check still working. version until on, select version() -| 8.0.17 myself&it; show global variable like 'local_infile'; +---------------+-------+ | variable_nam | value | +---------------+-------+ | local_infil | | +---------------+-------+ 1 row set (0.00 see)"
50846722,What is the difference between Postgres DISTINCT vs DISTINCT ON?,"I have a Postgres table created with the following statement. This table is filled by as dump of data from another service.
CREATE TABLE data_table (
    date date DEFAULT NULL,
    dimension1 varchar(64) DEFAULT NULL,
    dimension2 varchar(128) DEFAULT NULL
) TABLESPACE pg_default;
One of the steps in a ETL I'm building is extracting the unique values of dimension1 and inserting them in another intermediary table.
However, during some tests I found out that the 2 commands below do not return the same results. I would expect for both to return the same sum.
The first command returns more results compared with the second (1466 rows vs. 1504.
-- command 1
SELECT DISTINCT count(dimension1)
FROM data_table;
-- command 2    
SELECT count(*)
FROM (SELECT DISTINCT ON (dimension1) dimension1
FROM data_table
GROUP BY dimension1) AS tmp_table;
Any obvious explanations for this? Alternatively to an explanation, is there any suggestion of any check on the data I should do?
EDIT: The following queries both return 1504 (same as the ""simple"" DISTINCT)
SELECT count(*)
FROM data_table WHERE dimension1 IS NOT NULL;
SELECT count(dimension1)
FROM data_table;
Thank you!
",<sql><postgresql>,1169,0,21,611,1,6,12,64,32580,0.0,40,5,47,2018-06-13 21:43,2018-06-13 22:11,2018-06-13 22:19,0.0,0.0,Basic,8,"<sql><postgresql>, What is the difference between Postgres DISTINCT vs DISTINCT ON?, I have a Postgres table created with the following statement. This table is filled by as dump of data from another service.
CREATE TABLE data_table (
    date date DEFAULT NULL,
    dimension1 varchar(64) DEFAULT NULL,
    dimension2 varchar(128) DEFAULT NULL
) TABLESPACE pg_default;
One of the steps in a ETL I'm building is extracting the unique values of dimension1 and inserting them in another intermediary table.
However, during some tests I found out that the 2 commands below do not return the same results. I would expect for both to return the same sum.
The first command returns more results compared with the second (1466 rows vs. 1504.
-- command 1
SELECT DISTINCT count(dimension1)
FROM data_table;
-- command 2    
SELECT count(*)
FROM (SELECT DISTINCT ON (dimension1) dimension1
FROM data_table
GROUP BY dimension1) AS tmp_table;
Any obvious explanations for this? Alternatively to an explanation, is there any suggestion of any check on the data I should do?
EDIT: The following queries both return 1504 (same as the ""simple"" DISTINCT)
SELECT count(*)
FROM data_table WHERE dimension1 IS NOT NULL;
SELECT count(dimension1)
FROM data_table;
Thank you!
","<sal><postgresql>, differ poster distinct vs distinct on?, poster table great follow statement. table fill dump data not service. great table data ( date date default null, dimensions varchar(64) default null, dimensions varchar(128) default null ) tablespac pg_default; one step etc i'm build extract unique value dimensions insert not intermediary table. however, test found 2 command return results. would expect return sum. first command return result compare second (1466 row vs. 1504. -- command 1 select distinct count(dimensions) data_table; -- command 2 select count(*) (select distinct (dimensions) dimensions data group dimensions) tmp_table; obvious explain this? alter explanation, suggest check data do? edit: follow query return 1504 (same ""simple"" distinct) select count(*) data dimensions null; select count(dimensions) data_table; thank you!"
48793257,Laravel: Check with Observer if Column was Changed on Update,"I am using an Observer to watch if a user was updated.
Whenever a user is updated I would like to check if his email has been changed. 
Is something like this possible?
class UserObserver
{
    /**
     * Listen to the User created event.
     *
     * @param  \App\User  $user
     * @return void
     */
    public function updating(User $user)
    {
      // if($user-&gt;hasChangedEmailInThisUpdate()) ?
    }
}
",<php><mysql><laravel-5>,416,1,18,26612,24,163,253,67,55203,0.0,2276,4,45,2018-02-14 17:43,2018-02-14 18:19,2018-02-14 18:19,0.0,0.0,Basic,9,"<php><mysql><laravel-5>, Laravel: Check with Observer if Column was Changed on Update, I am using an Observer to watch if a user was updated.
Whenever a user is updated I would like to check if his email has been changed. 
Is something like this possible?
class UserObserver
{
    /**
     * Listen to the User created event.
     *
     * @param  \App\User  $user
     * @return void
     */
    public function updating(User $user)
    {
      // if($user-&gt;hasChangedEmailInThisUpdate()) ?
    }
}
","<pp><myself><travel-5>, travel: check observe column change update, use observe watch user updated. when user update would like check email changed. cometh like possible? class userobserv { /** * listen user great event. * * @parma \pp\us $user * @return void */ public function dating(us $user) { // if($user-&it;haschangedemailinthisupdate()) ? } }"
55863560,"Method ""join"" and class ""DeepCollectionEquality"" aren't defined","Android Studio is giving me 2 errors using sqflite 2.2.0+3:
The method join isn't defined for the class uploadIntoDb.
Undefined class DeepCollectionEquality.
My code :
import 'package:flutter/material.dart';
import 'dart:async';
import 'package:sqflite/sqflite.dart';
class UploadPage extends StatefulWidget {
  @override
  State&lt;StatefulWidget&gt; createState(){
    return new UploadPageState();
  }
}
class UploadPageState extends State&lt;UploadPage&gt;
    with SingleTickerProviderStateMixin {
  @override
  void initState(){
    super.initState();
  }
  @override
  Widget build(BuildContext context) {
    return null;
  }
  Future&lt;void&gt; uploadIntoDb(String valueToUpload) async{
    // Get a location using getDatabasesPath
    var databasesPath = await getDatabasesPath();
    String path = join(databasesPath, 'poa.db');//FIRST PROBLEM
// open the database
    Database database = await openDatabase(path, version: 1,
        onCreate: (Database db, int version) async {
          // When creating the db, create the table
          await db.execute(
              'CREATE TABLE Test (id INTEGER PRIMARY KEY, name TEXT, value INTEGER, num REAL)');
        });
// Insert some records in a transaction
    await database.transaction((txn) async {
      int id1 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(&quot;some name&quot;, 1234, 456.789)');
      print('inserted1: $id1');
      int id2 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(?, ?, ?)',
          ['another name', 12345678, 3.1416]);
      print('inserted2: $id2');
    });
// Update some record
    int count = await database.rawUpdate(
        'UPDATE Test SET name = ?, VALUE = ? WHERE name = ?',
        ['updated name', '9876', 'some name']);
    print('updated: $count');
// Get the records
    List&lt;Map&gt; list = await database.rawQuery('SELECT * FROM Test');
    List&lt;Map&gt; expectedList = [
      {'name': 'updated name', 'id': 1, 'value': 9876, 'num': 456.789},
      {'name': 'another name', 'id': 2, 'value': 12345678, 'num': 3.1416}
    ];
    print(list);
    print(expectedList);
    assert(const DeepCollectionEquality().equals(list, expectedList));//SECOND PROBLEM
// Count the records
    count = Sqflite
        .firstIntValue(await database.rawQuery('SELECT COUNT(*) FROM Test'));
    assert(count == 2);
// Delete a record
    count = await database
        .rawDelete('DELETE FROM Test WHERE name = ?', ['another name']);
    assert(count == 1);
// Close the database
    await database.close();
  }
}
Yes I included dependency sqflite: ^1.1.0.
",<android><sqlite><join><flutter><sqflite>,2617,1,83,3873,5,27,47,69,19933,0.0,443,1,45,2019-04-26 8:20,2019-04-26 10:30,2019-04-26 10:30,0.0,0.0,Basic,9,"<android><sqlite><join><flutter><sqflite>, Method ""join"" and class ""DeepCollectionEquality"" aren't defined, Android Studio is giving me 2 errors using sqflite 2.2.0+3:
The method join isn't defined for the class uploadIntoDb.
Undefined class DeepCollectionEquality.
My code :
import 'package:flutter/material.dart';
import 'dart:async';
import 'package:sqflite/sqflite.dart';
class UploadPage extends StatefulWidget {
  @override
  State&lt;StatefulWidget&gt; createState(){
    return new UploadPageState();
  }
}
class UploadPageState extends State&lt;UploadPage&gt;
    with SingleTickerProviderStateMixin {
  @override
  void initState(){
    super.initState();
  }
  @override
  Widget build(BuildContext context) {
    return null;
  }
  Future&lt;void&gt; uploadIntoDb(String valueToUpload) async{
    // Get a location using getDatabasesPath
    var databasesPath = await getDatabasesPath();
    String path = join(databasesPath, 'poa.db');//FIRST PROBLEM
// open the database
    Database database = await openDatabase(path, version: 1,
        onCreate: (Database db, int version) async {
          // When creating the db, create the table
          await db.execute(
              'CREATE TABLE Test (id INTEGER PRIMARY KEY, name TEXT, value INTEGER, num REAL)');
        });
// Insert some records in a transaction
    await database.transaction((txn) async {
      int id1 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(&quot;some name&quot;, 1234, 456.789)');
      print('inserted1: $id1');
      int id2 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(?, ?, ?)',
          ['another name', 12345678, 3.1416]);
      print('inserted2: $id2');
    });
// Update some record
    int count = await database.rawUpdate(
        'UPDATE Test SET name = ?, VALUE = ? WHERE name = ?',
        ['updated name', '9876', 'some name']);
    print('updated: $count');
// Get the records
    List&lt;Map&gt; list = await database.rawQuery('SELECT * FROM Test');
    List&lt;Map&gt; expectedList = [
      {'name': 'updated name', 'id': 1, 'value': 9876, 'num': 456.789},
      {'name': 'another name', 'id': 2, 'value': 12345678, 'num': 3.1416}
    ];
    print(list);
    print(expectedList);
    assert(const DeepCollectionEquality().equals(list, expectedList));//SECOND PROBLEM
// Count the records
    count = Sqflite
        .firstIntValue(await database.rawQuery('SELECT COUNT(*) FROM Test'));
    assert(count == 2);
// Delete a record
    count = await database
        .rawDelete('DELETE FROM Test WHERE name = ?', ['another name']);
    assert(count == 1);
// Close the database
    await database.close();
  }
}
Yes I included dependency sqflite: ^1.1.0.
","<andros><quite><join><flutter><sqflite>, method ""join"" class ""deepcollectionequality"" defined, andros studio give 2 error use sqflite 2.2.0+3: method join define class uploadintodb. undefined class deepcollectionequality. code : import 'package:flutter/material.dart'; import 'dart:async'; import 'package:sqflite/sqflite.dart'; class uploadpag extend statefulwidget { @overdid state&it;statefulwidget&it; createstate(){ return new uploadpagestate(); } } class uploadpagest extend state&it;uploadpage&it; singletickerproviderstatemixin { @overdid void initiate(){ super.initiate(); } @overdid wide build(buildcontext context) { return null; } future&it;void&it; uploadintodb(sir valuetoupload) async{ // get local use getdatabasespath war databasespath = await getdatabasespath(); string path = join(databasespath, 'pa.do');//first problem // open database database database = await opendatabase(path, version: 1, increase: (database do, in version) async { // great do, great table await do.execute( 'great table test (id inter primary key, name text, value inter, sum real)'); }); // insert record transact await database.transaction((ten) async { in ida = await ten.rawinsert( 'insert test(name, value, sum) values(&quit;so name&quit;, 1234, 456.789)'); print('inserted: $ida'); in ida = await ten.rawinsert( 'insert test(name, value, sum) values(?, ?, ?)', ['not name', 12345678, 3.1416]); print('inserted: $ida'); }); // update record in count = await database.rawupdate( 'update test set name = ?, value = ? name = ?', ['update name', '9876', 'some name']); print('updated: $count'); // get record list&it;map&it; list = await database.rawquery('select * test'); list&it;map&it; expectedlist = [ {'name': 'update name', 'id': 1, 'value': 9876, 'sum': 456.789}, {'name': 'not name', 'id': 2, 'value': 12345678, 'sum': 3.1416} ]; print(list); print(expectedlist); assert(cost deepcollectionequality().equals(list, expectedlist));//second problem // count record count = sqflite .firstintvalue(await database.rawquery('select count(*) test')); assert(count == 2); // delete record count = await database .rawdelete('delete test name = ?', ['not name']); assert(count == 1); // close database await database.close(); } } ye include depend sqflite: ^1.1.0."
49811955,Unable to install psycopg2 (pip install psycopg2),"I'm using MAC and python version 2.7.14
Collecting psycopg2
  Could not fetch URL https://pypi.python.org/simple/psycopg2/: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:661) - skipping
  Could not find a version that satisfies the requirement psycopg2 (from versions: )
No matching distribution found for psycopg2
",<python><postgresql><pip><psycopg2>,394,1,4,567,1,4,6,72,118872,0.0,1,9,45,2018-04-13 7:54,2018-04-13 8:10,,0.0,,Basic,14,"<python><postgresql><pip><psycopg2>, Unable to install psycopg2 (pip install psycopg2), I'm using MAC and python version 2.7.14
Collecting psycopg2
  Could not fetch URL https://pypi.python.org/simple/psycopg2/: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:661) - skipping
  Could not find a version that satisfies the requirement psycopg2 (from versions: )
No matching distribution found for psycopg2
","<patron><postgresql><pp><psycopg2>, unable instal psycopg2 (pp instal psycopg2), i'm use mac patron version 2.7.14 collect psycopg2 could fetch curl http://pp.patron.org/simple/psycopg2/: problem confirm sal certificate: [sal: tlsv1_alert_protocol_version] tlsv1 alert protocol version (sal.c:661) - skin could find version satisfy require psycopg2 (from versions: ) match distribute found psycopg2"
59985030,Syntax error at: OPTIMIZE_FOR_SEQUENTIAL_KEY,"I created a table in Microsoft SQL Server Management Studio and the table worked fine, no errors while building.
Then i was copying the script to my project in visual studio when the following message showed:
  SQL80001: Incorrect syntax ner 'OPTIMIZE_FOR_SEQUENTIAL_KEY'
I don't know why it happened, but this error was showing on this line of the code:
(PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF  )
Do you guys know why the visual studio is showing that error message? How can I fix it?
",<sql-server><visual-studio>,595,0,1,602,1,5,13,65,92805,0.0,89,3,45,2020-01-30 11:29,2020-01-30 11:48,2020-01-30 11:48,0.0,0.0,Basic,14,"<sql-server><visual-studio>, Syntax error at: OPTIMIZE_FOR_SEQUENTIAL_KEY, I created a table in Microsoft SQL Server Management Studio and the table worked fine, no errors while building.
Then i was copying the script to my project in visual studio when the following message showed:
  SQL80001: Incorrect syntax ner 'OPTIMIZE_FOR_SEQUENTIAL_KEY'
I don't know why it happened, but this error was showing on this line of the code:
(PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF  )
Do you guys know why the visual studio is showing that error message? How can I fix it?
","<sal-server><visual-studio>, santa error at: optimize_for_sequential_key, great table microsoft sal server manage studio table work fine, error building. copy script project visual studio follow message showed: sql80001: incorrect santa her 'optimize_for_sequential_key' know happened, error show line code: (pad_index = off, statistics_norecomput = off, ignore_dup_key = off, allow_row_lock = on, allow_page_lock = on, optimize_for_sequential_key = ) guy know visual studio show error message? fix it?"
48190016,SQL correct way of joining if the other parameter is null,"I have this code and its temporary tables so you can run it.
create table #student
(
    id int identity(1,1),
    firstname varchar(50),
    lastname varchar(50)
)
create table #quiz
(
    id int identity(1,1),
    quiz_name varchar(50)
)
create table #quiz_details
(
    id int identity(1,1),
    quiz_id int,
    student_id int
)
insert into #student(firstname, lastname)
values ('LeBron', 'James'), ('Stephen', 'Curry')
insert into #quiz(quiz_name)
values('NBA 50 Greatest Player Quiz'), ('NBA Top 10 3 point shooters')
insert into #quiz_details(quiz_id, student_id)
values (1, 2), (2, 1)
drop table #student
drop table #quiz
drop table #quiz_details
So as you can see lebron james takes the quiz nba top 10 3 point shooters quiz and stephen curry takes the nba 50 greatest player quiz.
All I want is to get the thing that they didn't take yet for example LeBron hasn't taken the 50 greatest player quiz so what I want is like this.
id   quiz_name                    firstname  lastname
----------------------------------------------------
1    NBA 50 Greatest Player Quiz  NULL       NULL 
I want 2 parameters, the id of lebron and the id of the quiz so that I will know that lebron or stephen hasn't taken it yet, but how would I do that if the value of the student_id is still null?
My attempt:
select
    QD.id,
    Q.quiz_name,
    S.firstname,
    S.lastname
from 
    #quiz_details QD
inner join 
    #quiz Q on Q.id = QD.quiz_id
inner join 
    #student S on S.id = QD.student_id
",<sql><sql-server>,1492,0,48,515,0,3,10,81,1916,0.0,8,5,45,2018-01-10 14:50,2018-01-10 14:56,2018-01-10 14:56,0.0,0.0,Basic,10,"<sql><sql-server>, SQL correct way of joining if the other parameter is null, I have this code and its temporary tables so you can run it.
create table #student
(
    id int identity(1,1),
    firstname varchar(50),
    lastname varchar(50)
)
create table #quiz
(
    id int identity(1,1),
    quiz_name varchar(50)
)
create table #quiz_details
(
    id int identity(1,1),
    quiz_id int,
    student_id int
)
insert into #student(firstname, lastname)
values ('LeBron', 'James'), ('Stephen', 'Curry')
insert into #quiz(quiz_name)
values('NBA 50 Greatest Player Quiz'), ('NBA Top 10 3 point shooters')
insert into #quiz_details(quiz_id, student_id)
values (1, 2), (2, 1)
drop table #student
drop table #quiz
drop table #quiz_details
So as you can see lebron james takes the quiz nba top 10 3 point shooters quiz and stephen curry takes the nba 50 greatest player quiz.
All I want is to get the thing that they didn't take yet for example LeBron hasn't taken the 50 greatest player quiz so what I want is like this.
id   quiz_name                    firstname  lastname
----------------------------------------------------
1    NBA 50 Greatest Player Quiz  NULL       NULL 
I want 2 parameters, the id of lebron and the id of the quiz so that I will know that lebron or stephen hasn't taken it yet, but how would I do that if the value of the student_id is still null?
My attempt:
select
    QD.id,
    Q.quiz_name,
    S.firstname,
    S.lastname
from 
    #quiz_details QD
inner join 
    #quiz Q on Q.id = QD.quiz_id
inner join 
    #student S on S.id = QD.student_id
","<sal><sal-server>, sal correct way join parapet null, code temporary table run it. great table #student ( id in identity(1,1), firstnam varchar(50), lastnam varchar(50) ) great table #quit ( id in identity(1,1), quiz_nam varchar(50) ) great table #quiz_detail ( id in identity(1,1), quiz_id in, student_id in ) insert #student(firstname, lastname) value ('lesion', 'james'), ('stephen', 'carry') insert #quit(quiz_name) values('na 50 greatest player quit'), ('na top 10 3 point shutters') insert #quiz_details(quiz_id, student_id) value (1, 2), (2, 1) drop table #student drop table #quit drop table #quiz_detail see lesion same take quit na top 10 3 point shorter quit stephen burri take na 50 greatest player quit. want get thing take yet example lesion taken 50 greatest player quit want like this. id quiz_nam firstnam lastnam ---------------------------------------------------- 1 na 50 greatest player quit null null want 2 parameter, id lesion id quit know lesion stephen taken yet, would value student_id still null? attempt: select d.id, q.quiz_name, s.firstname, s.lastnam #quiz_detail d inner join #quit q q.id = d.quiz_id inner join #student s.id = d.student_id"
48279481,Multiple tables with same type of objects in Room database,"I'm using Room as the database for the app. I have a scenario where an Object of a certain type needs to be stored in separate tables. As an example, lets take the Object called Book.java 
Now, I want to have two SQL tables:
Books_Read
Books_To_Read 
ignore any naming conventions for SQL DB please - this is just an example
Problem 
Normally, one would just use @Entity(tableName = ""Books_Read"") in the Book.java class and have a DAO class that will use that table name. 
The thing is; how would I then be able to use the same Book.java class to store in the Books_To_Read table? Since I already defined @Entity(tableName = ""Books_Read"") as part of the Book.java class and I see no where to define the Books_To_Read table for the Book.java class 
The only solution I was able to come up with, which seems a little hackery and unnessasery, was to create a new class - let's call it BookToRead.java  that extends Book.java and define @Entity(tableName = ""Books_To_Read"") in the class. 
Question
Is there a better way to do this or is this the expected way to handle it? 
",<android><sql><database><database-design><android-room>,1070,0,15,7241,9,47,99,45,24992,0.0,1779,3,45,2018-01-16 10:50,2018-02-19 10:44,,34.0,,Basic,10,"<android><sql><database><database-design><android-room>, Multiple tables with same type of objects in Room database, I'm using Room as the database for the app. I have a scenario where an Object of a certain type needs to be stored in separate tables. As an example, lets take the Object called Book.java 
Now, I want to have two SQL tables:
Books_Read
Books_To_Read 
ignore any naming conventions for SQL DB please - this is just an example
Problem 
Normally, one would just use @Entity(tableName = ""Books_Read"") in the Book.java class and have a DAO class that will use that table name. 
The thing is; how would I then be able to use the same Book.java class to store in the Books_To_Read table? Since I already defined @Entity(tableName = ""Books_Read"") as part of the Book.java class and I see no where to define the Books_To_Read table for the Book.java class 
The only solution I was able to come up with, which seems a little hackery and unnessasery, was to create a new class - let's call it BookToRead.java  that extends Book.java and define @Entity(tableName = ""Books_To_Read"") in the class. 
Question
Is there a better way to do this or is this the expected way to handle it? 
","<andros><sal><database><database-design><andros-room>, multiple table type object room database, i'm use room database pp. scenario object certain type need store spear tables. example, let take object call book.cava now, want two sal tables: books_read books_to_read ignore name convent sal do pleas - example problem normally, one would use @entity(tablenam = ""books_read"") book.cava class do class use table name. thing is; would all use book.cava class store books_to_read table? since already define @entity(tablenam = ""books_read"") part book.cava class see define books_to_read table book.cava class slut all come with, seem little hawkers unnessasery, great new class - let' call booktoread.cava extend book.cava define @entity(tablenam = ""books_to_read"") class. question better way expect way hand it?"
59156537,Unable to load System.Threading.Tasks.Extensions,"I have a web project build on .net framework 4.5.1. We are trying to added PostgreSQL support for the project. Using Nuget, I have installed 4.0.4 npgsql to the project. Under references, I see the following being added to the project.
Npgsql - 4.0.4.0 - Runtime version v4.0.30319
System.Threading.Tasks.Extensions - 4.2.0.0 - Runtime version v4.0.30319
When I tried run the project and connect and get the data from the database, I am getting the following error saying FileNotFoundException:
    System.TypeInitializationException
      HResult=0x80131534
      Message=The type initializer for 'com.rsol.RConfig' threw an exception.
      Source=RConfig
      StackTrace:
       at com.rsol.RConfig.getInstance() in C:\Workspaces\PS\RConfig\RConfig.cs:line 1113
       at RAdmin.Global.Application_Start(Object sender, EventArgs e) in C:\Workspaces\PS\RAdmin\Global.asax.cs:line 528
    Inner Exception 1:
    TypeInitializationException: The type initializer for 'com.rsol.Db.DbMgr' threw an exception.
    Inner Exception 2:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.1, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
    Inner Exception 3:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
System.Threading.Tasks.Extensions which is installed using Nuget is not getting loaded to the project. When I checked the properties of System.Threading.Tasks.Extensions reference, the dll file exists in the location. I have also tried installing System.Threading.Tasks.Extensions.dll file to assembly using gacutil. I am still getting the same error.
Please let me know if you need any additional information.
Any help is really appreciated.
",<c#><nuget><npgsql>,1963,0,16,1189,2,19,41,72,69195,0.0,28,10,44,2019-12-03 11:54,2019-12-03 12:12,2019-12-03 12:12,0.0,0.0,Basic,14,"<c#><nuget><npgsql>, Unable to load System.Threading.Tasks.Extensions, I have a web project build on .net framework 4.5.1. We are trying to added PostgreSQL support for the project. Using Nuget, I have installed 4.0.4 npgsql to the project. Under references, I see the following being added to the project.
Npgsql - 4.0.4.0 - Runtime version v4.0.30319
System.Threading.Tasks.Extensions - 4.2.0.0 - Runtime version v4.0.30319
When I tried run the project and connect and get the data from the database, I am getting the following error saying FileNotFoundException:
    System.TypeInitializationException
      HResult=0x80131534
      Message=The type initializer for 'com.rsol.RConfig' threw an exception.
      Source=RConfig
      StackTrace:
       at com.rsol.RConfig.getInstance() in C:\Workspaces\PS\RConfig\RConfig.cs:line 1113
       at RAdmin.Global.Application_Start(Object sender, EventArgs e) in C:\Workspaces\PS\RAdmin\Global.asax.cs:line 528
    Inner Exception 1:
    TypeInitializationException: The type initializer for 'com.rsol.Db.DbMgr' threw an exception.
    Inner Exception 2:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.1, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
    Inner Exception 3:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
System.Threading.Tasks.Extensions which is installed using Nuget is not getting loaded to the project. When I checked the properties of System.Threading.Tasks.Extensions reference, the dll file exists in the location. I have also tried installing System.Threading.Tasks.Extensions.dll file to assembly using gacutil. I am still getting the same error.
Please let me know if you need any additional information.
Any help is really appreciated.
","<c#><puget><npgsql>, unable load system.treading.tasks.extensions, web project build .net framework 4.5.1. try ad postgresql support project. use puget, instal 4.0.4 npgsql project. references, see follow ad project. npgsql - 4.0.4.0 - until version ve.0.30319 system.treading.tasks.extent - 4.2.0.0 - until version ve.0.30319 try run project connect get data database, get follow error say filenotfoundexception: system.typeinitializationexcept result=0x80131534 message=th type into 'com.sol.rconfig' threw exception. source=rconfig stacktrace: com.sol.rconfig.getinstance() c:\workspaces\is\rconfig\rconfig.is:in 1113 admit.global.application_start(object tender, eventarg e) c:\workspaces\is\admit\global.as.is:in 528 inner except 1: typeinitializationexception: type into 'com.sol.do.dbmgr' threw exception. inner except 2: filenotfoundexception: could load file assembly 'system.treading.tasks.extensions, version=4.2.0.1, culture=neutral, publickeytoken=cc7b13ffcd2ddd51' one dependencies. system cannot find file specified. inner except 3: filenotfoundexception: could load file assembly 'system.treading.tasks.extensions, version=4.2.0.0, culture=neutral, publickeytoken=cc7b13ffcd2ddd51' one dependencies. system cannot find file specified. system.treading.tasks.extent instal use puget get load project. check property system.treading.tasks.extent reference, all file exist location. also try instal system.treading.tasks.extensions.do file assembly use gacutil. still get error. pleas let know need admit information. help really appreciated."
55457069,"how to fix ""OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly""","Services
My service based on flask + postgresql + gunicorn + supervisor + nginx
When deploying by docker, after running the service, then accessing the api, sometimes it told the error message, and sometimes it workes well.
And the sqlachemy connect database add the parameters 'sslmode:disable'.
File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    Return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    Distilled_params,
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
    Util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 383, in raise_from_cause
    Reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
    Cursor, statement, parameters, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    Cursor.execute(statement, parameters)
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.
Information
Docker for Mac: version: 2.0.0.3 (31259)
macOS: version 10.14.2
Python: version 2.7.15
Recurrence method
When view port information by command
lsof -i:5432
the port 5432 is postgresql database default port，if the outputconsole was
COMMAND    PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 86469 user    4u  IPv6 0xxddd      0t0  TCP *:postgresql (LISTEN)
postgres 86469 user    5u  IPv4 0xxddr      0t0  TCP *:postgresql (LISTEN)
it would display the error message:
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
but if the outputconsolelog show this:
COMMAND     PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 62421 user   26u  IPv4 0xe93      0t0  TCP 192.168.2.7:6435-&gt;192.168.2.7:postgresql (ESTABLISHED)
postgres  86460 user    4u  IPv6 0xed3      0t0  TCP *:postgresql (LISTEN)
postgres  86460 user    5u  IPv4 0xe513      0t0  TCP *:postgresql (LISTEN)
postgres  86856 user   11u  IPv4 0xfe93      0t0  TCP 192.168.2.7:postgresql-&gt;192.168.2.7:6435 (ESTABLISHED)
the situation, the api would work well.
Becauce of Docker for mac?
Refer link https://github.com/docker/for-mac/issues/2442 , the issue can not solve my problem.
Notice a similar problem?
Refer link Python &amp; Sqlalchemy - Connection pattern -&gt; Disconnected from the remote server randomly
also this issue can not solve my problem.
Solution
flask_sqlachemy need the parameter pool_pre_ping
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(self, app, options)
        options[""pool_pre_ping""] = True
db = SQLAlchemy()
",<postgresql><macos><docker><flask>,3340,4,36,613,1,7,15,38,69282,0.0,3,4,44,2019-04-01 14:04,2020-03-10 9:39,,344.0,,Basic,14,"<postgresql><macos><docker><flask>, how to fix ""OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly"", Services
My service based on flask + postgresql + gunicorn + supervisor + nginx
When deploying by docker, after running the service, then accessing the api, sometimes it told the error message, and sometimes it workes well.
And the sqlachemy connect database add the parameters 'sslmode:disable'.
File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    Return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    Distilled_params,
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
    Util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 383, in raise_from_cause
    Reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
    Cursor, statement, parameters, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    Cursor.execute(statement, parameters)
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.
Information
Docker for Mac: version: 2.0.0.3 (31259)
macOS: version 10.14.2
Python: version 2.7.15
Recurrence method
When view port information by command
lsof -i:5432
the port 5432 is postgresql database default port，if the outputconsole was
COMMAND    PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 86469 user    4u  IPv6 0xxddd      0t0  TCP *:postgresql (LISTEN)
postgres 86469 user    5u  IPv4 0xxddr      0t0  TCP *:postgresql (LISTEN)
it would display the error message:
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
but if the outputconsolelog show this:
COMMAND     PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 62421 user   26u  IPv4 0xe93      0t0  TCP 192.168.2.7:6435-&gt;192.168.2.7:postgresql (ESTABLISHED)
postgres  86460 user    4u  IPv6 0xed3      0t0  TCP *:postgresql (LISTEN)
postgres  86460 user    5u  IPv4 0xe513      0t0  TCP *:postgresql (LISTEN)
postgres  86856 user   11u  IPv4 0xfe93      0t0  TCP 192.168.2.7:postgresql-&gt;192.168.2.7:6435 (ESTABLISHED)
the situation, the api would work well.
Becauce of Docker for mac?
Refer link https://github.com/docker/for-mac/issues/2442 , the issue can not solve my problem.
Notice a similar problem?
Refer link Python &amp; Sqlalchemy - Connection pattern -&gt; Disconnected from the remote server randomly
also this issue can not solve my problem.
Solution
flask_sqlachemy need the parameter pool_pre_ping
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(self, app, options)
        options[""pool_pre_ping""] = True
db = SQLAlchemy()
","<postgresql><faces><doctor><flask>, fix ""operationalerror: (psycopg2.operationalerror) server close connect unexpectedly"", service service base flask + postgresql + unicorn + supervisor + nine deploy doctor, run service, access apt, sometime told error message, sometime work well. sqlachemi connect database add parapet 'sslmode:disabled'. file ""/us/local/limb/python2.7/site-packages/sqlalchemy/sal/elements.by"", line 287, _execute_on_connect return connection._execute_clauseelement(self, multiparams, parts) file ""/us/local/limb/python2.7/site-packages/sqlalchemy/engine/base.by"", line 1107, _execute_clauseel distilled_params, file ""/us/local/limb/python2.7/site-packages/sqlalchemy/engine/base.by"", line 1248, _execute_context e, statement, parameter, curses, context file ""/us/local/limb/python2.7/site-packages/sqlalchemy/engine/base.by"", line 1466, _handle_dbapi_except until.raise_from_cause(sqlalchemy_exception, exc_info) file ""/us/local/limb/python2.7/site-packages/sqlalchemy/until/compact.by"", line 383, raise_from_caus raise(type(exception), exception, to=excite, cause=cause) file ""/us/local/limb/python2.7/site-packages/sqlalchemy/engine/base.by"", line 1244, _execute_context curses, statement, parameter, context file ""/us/local/limb/python2.7/site-packages/sqlalchemy/engine/default.by"", line 552, do_execut curses.execute(statement, parameter) operationalerror: (psycopg2.operationalerror) server close connect unexpectedly probably mean server german abnormal process request. inform doctor mac: version: 2.0.0.3 (31259) faces: version 10.14.2 patron: version 2.7.15 recur method view port inform command of -i:5432 port 5432 postgresql database default port，if outputconsol command did user ff type devil size/off node name poster 86469 user u iv 0xxddd to top *:postgresql (listen) poster 86469 user u iv 0xxddr to top *:postgresql (listen) would display error message: operationalerror: (psycopg2.operationalerror) server close connect unexpectedly outputconsolelog show this: command did user ff type devil size/off node name com.dock 62421 user you iv 0xe93 to top 192.168.2.7:6435-&it;192.168.2.7:postgresql (established) poster 86460 user u iv 0xed3 to top *:postgresql (listen) poster 86460 user u iv 0xe513 to top *:postgresql (listen) poster 86856 user you iv 0xfe93 to top 192.168.2.7:postgresql-&it;192.168.2.7:6435 (established) situation, apt would work well. because doctor mac? refer link http://github.com/doctor/for-mac/issues/2442 , issue sole problem. notice similar problem? refer link patron &amp; sqlalchemi - connect pattern -&it; discontent remote server random also issue sole problem. slut flask_sqlachemi need parapet pool_pre_p flask_sqlalchemi import sqlalchemi _basesqlalchemi class sqlalchemy(_basesqlalchemy): def apply_pool_defaults(self, pp, option): super(sqlalchemy, self).apply_pool_defaults(self, pp, option) option[""pool_pre_ping""] = true do = sqlalchemy()"
59720605,Explain vs explain analyze in PostgreSQL,"I understand that explain in postgresql just estimates the cost of a query and explain analyze does the same and also executes a query and gives the actual results.
But I can't figure out in which cases I should use explain and explain analyze.
",<postgresql>,245,0,0,543,1,4,4,59,12591,0.0,0,1,44,2020-01-13 16:36,2020-01-13 16:49,2020-01-13 16:49,0.0,0.0,Intermediate,23,"<postgresql>, Explain vs explain analyze in PostgreSQL, I understand that explain in postgresql just estimates the cost of a query and explain analyze does the same and also executes a query and gives the actual results.
But I can't figure out in which cases I should use explain and explain analyze.
","<postgresql>, explain vs explain analyze postgresql, understand explain postgresql estime cost query explain analyze also execute query give actual results. can't figure case use explain explain analyze."
