QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
57603349,Error:mysqld.service: Start request repeated too quickly. On manjaro,"Yesterday I updated my manjaro. I had many problems since then.
Firstly, I type
systemctl status mysqld.service
to start MySQL, but it errors out with
mysqld.service: Start request repeated too quickly.
I has found many suggestions but they doesn't work.
I already have tried:
Check the permission of the MySQL data directory using the below command. The ownership should be mysql:mysql and the directory permission should be 700.
ls -ld /var/lib/mysql/
Check the permission of databases inside the MySQL data directory using the below command. The ownership should be mysql:mysql for all the files inside that directory.
ls -lh /var/lib/mysql/
Check the listening network TCP ports using the command
netstat -ntlp
Check the MySQL log files for any error using:
cat /var/log/mysql/mysqld.log
Try to start MySQL using
mysqld_safe --defaults-file=/etc/my.cf
My Error:
dong@dong-manjaro  /home/dong   systemctl status mysqld.service                                                                          13:30:33 
● mysqld.service - MySQL Server
   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Thu 2019-08-22 13:30:29 CST; 6s ago
     Docs: man:mysqld(8)
           http://dev.mysql.com/doc/refman/en/using-systemd.html
  Process: 8006 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)
  Process: 8027 ExecStart=/usr/bin/mysqld $MYSQLD_OPTS (code=exited, status=127)
 Main PID: 8027 (code=exited, status=127)
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Service RestartSec=100ms expired, scheduling restart.
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Scheduled restart job, restart counter is at 5.
8月 22 13:30:29 dong-manjaro systemd[1]: Stopped MySQL Server.
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Start request repeated too quickly.**
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Failed with result 'exit-code'.**
8月 22 13:30:29 dong-manjaro systemd[1]: **Failed to start MySQL Server.**
",<mysql><linux><archlinux><manjaro>,2063,1,25,131,1,1,5,73,34795,0.0,3,6,13,2019-08-22 6:39,2020-12-12 8:22,,478.0,,Basic,13,"<mysql><linux><archlinux><manjaro>, Error:mysqld.service: Start request repeated too quickly. On manjaro, Yesterday I updated my manjaro. I had many problems since then.
Firstly, I type
systemctl status mysqld.service
to start MySQL, but it errors out with
mysqld.service: Start request repeated too quickly.
I has found many suggestions but they doesn't work.
I already have tried:
Check the permission of the MySQL data directory using the below command. The ownership should be mysql:mysql and the directory permission should be 700.
ls -ld /var/lib/mysql/
Check the permission of databases inside the MySQL data directory using the below command. The ownership should be mysql:mysql for all the files inside that directory.
ls -lh /var/lib/mysql/
Check the listening network TCP ports using the command
netstat -ntlp
Check the MySQL log files for any error using:
cat /var/log/mysql/mysqld.log
Try to start MySQL using
mysqld_safe --defaults-file=/etc/my.cf
My Error:
dong@dong-manjaro  /home/dong   systemctl status mysqld.service                                                                          13:30:33 
● mysqld.service - MySQL Server
   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Thu 2019-08-22 13:30:29 CST; 6s ago
     Docs: man:mysqld(8)
           http://dev.mysql.com/doc/refman/en/using-systemd.html
  Process: 8006 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)
  Process: 8027 ExecStart=/usr/bin/mysqld $MYSQLD_OPTS (code=exited, status=127)
 Main PID: 8027 (code=exited, status=127)
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Service RestartSec=100ms expired, scheduling restart.
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Scheduled restart job, restart counter is at 5.
8月 22 13:30:29 dong-manjaro systemd[1]: Stopped MySQL Server.
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Start request repeated too quickly.**
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Failed with result 'exit-code'.**
8月 22 13:30:29 dong-manjaro systemd[1]: **Failed to start MySQL Server.**
","<myself><line><archlinux><manjaro>, error:myself.service: start request repeat quickly. manjaro, yesterday update manjaro. man problem since then. firstly, type systemctl state myself.service start myself, error myself.service: start request repeat quickly. found man suggest work. already tried: check permits myself data director use command. ownership myself:myself director permits 700. is -old /war/limb/myself/ check permits database inside myself data director use command. ownership myself:myself file inside directory. is -ll /war/limb/myself/ check listen network top port use command netstat -help check myself log file error using: cat /war/log/myself/myself.log try start myself use mysqld_saf --default-file=/etc/my.cf error: long@long-manjaro  /home/long   systemctl state myself.service  13:30:33 ● myself.service - myself server loaded: load (/us/limb/system/system/myself.service; disabled; vendor present: disabled) active: fail (result: exit-code) since the 2019-08-22 13:30:29 cut; is ago docs: man:myself(8) http://de.myself.com/do/german/en/using-system.html process: 8006 execstartpre=/us/bin/mysqld_pre_systemd (code=excited, status=0/success) process: 8027 execstart=/us/bin/myself $mysqld_opt (code=excited, status=127) main did: 8027 (code=excited, status=127) of 22 13:30:29 long-manjaro system[1]: myself.service: service restarted=100m expired, schedule start. of 22 13:30:29 long-manjaro system[1]: myself.service: schedule start job, start counter 5. of 22 13:30:29 long-manjaro system[1]: stop myself server. of 22 13:30:29 long-manjaro system[1]: **myself.service: start request repeat quickly.** of 22 13:30:29 long-manjaro system[1]: **myself.service: fail result 'exit-code'.** of 22 13:30:29 long-manjaro system[1]: **fail start myself server.**"
62257900,How to list all the stored procedure in AWS RedShift,"I was checking this, but not find the proper one. So I prepared one and sharing that query here.
",<amazon-web-services><plsql><amazon-redshift>,97,0,0,2832,6,40,91,72,16515,0.0,57,2,13,2020-06-08 8:22,2020-06-08 8:23,2020-06-08 8:23,0.0,0.0,Basic,10,"<amazon-web-services><plsql><amazon-redshift>, How to list all the stored procedure in AWS RedShift, I was checking this, but not find the proper one. So I prepared one and sharing that query here.
","<amazon-web-services><plsql><amazon-redshift>, list store procedure a redshift, check this, find proper one. prepare one share query here."
48145384,How to disable only_full_group_by option in Laravel,"I am new to laravel and I am having an issue with DB problem.
I have disabled 'only_full_group_by' sql_mode by editing /etc/mysql/my.cnf file. And I checked sql_mode for both global and session using SELECT @@GLOBAL.sql_mode; and SELECT @@SESSION.sql_mode; and confirmed that sql_mode no longer has only_full_group_by.
However, when I make a request through postman, it gives me the error saying this is incompatible with sql_mode=only_full_group_by.
I am so confused. Why do I get this error even after I changed sql_mode? Am I doing something wrong?
Any suggestion or advice would be appreciated.
Thank you.
SQL using toSql()
select A.* 
from `A` 
inner join `B` on `A`.`id` = `B`.`a_id` 
inner join `C` on `C`.`id` = `B`.`c_id` 
group by `A`.`id` having COUNT(A.id) &gt; 0;
",<php><mysql><laravel-5><mysql-5.7>,777,0,13,1045,3,17,40,47,16047,0.0,114,5,13,2018-01-08 6:38,2018-01-08 6:44,2018-01-08 6:44,0.0,0.0,Basic,10,"<php><mysql><laravel-5><mysql-5.7>, How to disable only_full_group_by option in Laravel, I am new to laravel and I am having an issue with DB problem.
I have disabled 'only_full_group_by' sql_mode by editing /etc/mysql/my.cnf file. And I checked sql_mode for both global and session using SELECT @@GLOBAL.sql_mode; and SELECT @@SESSION.sql_mode; and confirmed that sql_mode no longer has only_full_group_by.
However, when I make a request through postman, it gives me the error saying this is incompatible with sql_mode=only_full_group_by.
I am so confused. Why do I get this error even after I changed sql_mode? Am I doing something wrong?
Any suggestion or advice would be appreciated.
Thank you.
SQL using toSql()
select A.* 
from `A` 
inner join `B` on `A`.`id` = `B`.`a_id` 
inner join `C` on `C`.`id` = `B`.`c_id` 
group by `A`.`id` having COUNT(A.id) &gt; 0;
","<pp><myself><travel-5><myself-5.7>, distal only_full_group_bi option travel, new travel issue do problem. distal 'only_full_group_by' sql_mode edit /etc/myself/my.cf file. check sql_mode global session use select @@global.sql_mode; select @@session.sql_mode; confirm sql_mode longer only_full_group_by. however, make request potman, give error say incompat sql_mode=only_full_group_by. confused. get error even change sql_mode? cometh wrong? suggest advice would appreciated. thank you. sal use total() select a.* `a` inner join `b` `a`.`id` = `b`.`aid` inner join `c` `c`.`id` = `b`.`said` group `a`.`id` count(a.id) &it; 0;"
54446734,Is it possible to issue CREATE statements using sp_executesql with parameters?,"I'm trying to dynamically create triggers, but ran into a confusing issue around using sp_executesql and passing parameters into the dynamic SQL.  The following simple test case works:
DECLARE @tableName sysname = 'MyTable';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT 1
        END';
EXEC sp_executesql @sql
However, I want to be able to use @tableName (and other values) as variables within the script, so I passed it along to the sp_executesql call:
DECLARE @tableName sysname = 'ContentItems';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT @tableName
        END';
EXEC sp_executesql @sql, N'@tableName sysname', @tableName=@tableName
When running the above, I get an error:
  Msg 156, Level 15, State 1, Line 2
  Incorrect syntax near the keyword 'TRIGGER'.
After trying I few things, I've discovered that even if I don't use @tableName in the dynamic SQL at all, I still get this error.  And I also get this error trying to create a PROCEDURE (except, obviously, the message is Incorrect syntax near the keyword 'PROCEDURE'.)
Since the SQL runs fine either directly or when not supplying parameters to sp_executesql, this seems like I'm running into a true limitation in the SQL engine, but I don't see it documented anywhere.  Does anyone know if there is a way to accept to a dynamic CREATE script, or at least have insight into the underlying limitation that's being run into?
Update
I can add a PRINT statement, and get the below SQL, which is valid, and runs successfully (when run directly).  I still get the error if there's nothing dynamic in the SQL (it's just a single string with no concatenation).
CREATE TRIGGER TR_ContentItems ON ContentItems FOR INSERT
    AS
    BEGIN
        PRINT @tableName
    END
I also get the same error whether using sysname or nvarchar(max) for the parameter.
",<sql-server><dynamic-sql><sp-executesql>,2020,0,31,152787,23,148,175,79,1581,0.0,3899,7,13,2019-01-30 17:56,2019-01-30 18:36,,0.0,,Basic,10,"<sql-server><dynamic-sql><sp-executesql>, Is it possible to issue CREATE statements using sp_executesql with parameters?, I'm trying to dynamically create triggers, but ran into a confusing issue around using sp_executesql and passing parameters into the dynamic SQL.  The following simple test case works:
DECLARE @tableName sysname = 'MyTable';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT 1
        END';
EXEC sp_executesql @sql
However, I want to be able to use @tableName (and other values) as variables within the script, so I passed it along to the sp_executesql call:
DECLARE @tableName sysname = 'ContentItems';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT @tableName
        END';
EXEC sp_executesql @sql, N'@tableName sysname', @tableName=@tableName
When running the above, I get an error:
  Msg 156, Level 15, State 1, Line 2
  Incorrect syntax near the keyword 'TRIGGER'.
After trying I few things, I've discovered that even if I don't use @tableName in the dynamic SQL at all, I still get this error.  And I also get this error trying to create a PROCEDURE (except, obviously, the message is Incorrect syntax near the keyword 'PROCEDURE'.)
Since the SQL runs fine either directly or when not supplying parameters to sp_executesql, this seems like I'm running into a true limitation in the SQL engine, but I don't see it documented anywhere.  Does anyone know if there is a way to accept to a dynamic CREATE script, or at least have insight into the underlying limitation that's being run into?
Update
I can add a PRINT statement, and get the below SQL, which is valid, and runs successfully (when run directly).  I still get the error if there's nothing dynamic in the SQL (it's just a single string with no concatenation).
CREATE TRIGGER TR_ContentItems ON ContentItems FOR INSERT
    AS
    BEGIN
        PRINT @tableName
    END
I also get the same error whether using sysname or nvarchar(max) for the parameter.
","<sal-server><dynamic-sal><s-executesql>, possible issue great statement use sp_executesql parameter?, i'm try dream great trigger, ran confuse issue around use sp_executesql pass parapet dream sal. follow simple test case works: declare @tablenam system = 'table'; declare @sal nvarchar(max) = n' great trigger try' + @tablenam + n' ' + @tablenam + n' insert begin print 1 end'; even sp_executesql @sal however, want all use @tablenam (and values) variable within script, pass along sp_executesql call: declare @tablenam system = 'contentitems'; declare @sal nvarchar(max) = n' great trigger try' + @tablenam + n' ' + @tablenam + n' insert begin print @tablenam end'; even sp_executesql @sal, n'@tablenam surname', @tablename=@tablenam run above, get error: mug 156, level 15, state 1, line 2 incorrect santa near eyford 'trigger'. try things, i'v disco even use @tablenam dream sal all, still get error. also get error try great procedure (except, obviously, message incorrect santa near eyford 'procedure'.) since sal run fine either directly supply parapet sp_executesql, seem like i'm run true limit sal engine, see document anywhere. anyone know way accept dream great script, least insight underlip limit that' run into? update add print statement, get sal, valid, run success (when run directly). still get error there' not dream sal (it' single string concatenation). great trigger tr_contentitem contentitem insert begin print @tablenam end also get error whether use system nvarchar(max) parameter."
62802173,Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call,"I have an application based on F#, and I use EF-Core and MySQL (Pomelo.EntityFrameworkCore.MySql).
I have an async method which updates data in DB(MySql)
let updatePlayerAchievementsAsync (logger:ILogger) (ctx:ReportCacheDbContext) (id: int) = async {
  let! account = ctx.AccountCaches.FirstOrDefaultAsync(fun e -&gt; e.AccountId = id) |&gt; Async.AwaitTask
  if account &lt;&gt; null then
    account.State &lt;- &quot;Closed&quot;
    do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
    logger.LogInformation(&quot;Account{0} updated&quot;, id)        
}
when this method comes to the 99th element, the following errors occurred:
|ERROR|System.InvalidOperationException:An exception has been raised that is likely due to a transient failure. Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call. 
---&gt; MySql.Data.MySqlClient.MySqlException (0x80004005): Connect Timeout expired. All pooled connections are in use.
I tried to follow 1st error's recomendation and tried to add EnableRetryOnFailure()
member this.ConfigureServices(services: IServiceCollection) =
    services.AddOptions() |&gt; ignore
    services.AddCors() |&gt; ignore
    services
        .AddDbContext&lt;ApplicationDbContext&gt;(
            fun (service:IServiceProvider) (dbContext:DbContextOptionsBuilder) -&gt;
                dbContext.UseMySql(profile.DbConnectionToAdmin /*HERE*/)|&gt; ignore)
    ...
And I can't find any documentation about this adding options for F# &amp; MySQL, cause all found info written on C#.
Maybe problem in used pools (default max=100) and I wrote next:
...
do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
ctx.Database.CloseConnection()
logger.LogInformation(&quot;Account{0} updated&quot;, id)  
But anyway problem wasn't solved.
This is my new experience in F# and async and I cant understand what I did incorrectly.
Could anyone help me with it?
",<mysql><entity-framework><asynchronous><async-await><f#>,1956,0,26,315,1,3,9,75,48466,,1,3,13,2020-07-08 19:19,2020-11-03 11:34,,118.0,,Basic,10,"<mysql><entity-framework><asynchronous><async-await><f#>, Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call, I have an application based on F#, and I use EF-Core and MySQL (Pomelo.EntityFrameworkCore.MySql).
I have an async method which updates data in DB(MySql)
let updatePlayerAchievementsAsync (logger:ILogger) (ctx:ReportCacheDbContext) (id: int) = async {
  let! account = ctx.AccountCaches.FirstOrDefaultAsync(fun e -&gt; e.AccountId = id) |&gt; Async.AwaitTask
  if account &lt;&gt; null then
    account.State &lt;- &quot;Closed&quot;
    do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
    logger.LogInformation(&quot;Account{0} updated&quot;, id)        
}
when this method comes to the 99th element, the following errors occurred:
|ERROR|System.InvalidOperationException:An exception has been raised that is likely due to a transient failure. Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call. 
---&gt; MySql.Data.MySqlClient.MySqlException (0x80004005): Connect Timeout expired. All pooled connections are in use.
I tried to follow 1st error's recomendation and tried to add EnableRetryOnFailure()
member this.ConfigureServices(services: IServiceCollection) =
    services.AddOptions() |&gt; ignore
    services.AddCors() |&gt; ignore
    services
        .AddDbContext&lt;ApplicationDbContext&gt;(
            fun (service:IServiceProvider) (dbContext:DbContextOptionsBuilder) -&gt;
                dbContext.UseMySql(profile.DbConnectionToAdmin /*HERE*/)|&gt; ignore)
    ...
And I can't find any documentation about this adding options for F# &amp; MySQL, cause all found info written on C#.
Maybe problem in used pools (default max=100) and I wrote next:
...
do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
ctx.Database.CloseConnection()
logger.LogInformation(&quot;Account{0} updated&quot;, id)  
But anyway problem wasn't solved.
This is my new experience in F# and async and I cant understand what I did incorrectly.
Could anyone help me with it?
","<myself><entity-framework><asynchronous><async-await><f#>, consider enable transient error result ad 'enableretryonfailure()' 'usemysql' call, applied base f#, use of-cor myself (homely.entityframeworkcore.myself). async method update data do(myself) let updateplayerachievementsasync (longer:longer) (cox:reportcachedbcontext) (id: in) = async { let! account = cox.accountcaches.firstordefaultasync(fun e -&it; e.accounted = id) |&it; async.awaittask account &it;&it; null account.st &it;- &quit;closed&quit; do! cox.savechangesasync true |&it; async.awaittask |&it; async.ignore longer.loginformation(&quit;account{0} updated&quit;, id) } method come with element, follow error occurred: |error|system.invalidoperationexception:an except rays like due transient failure. consider enable transient error result ad 'enableretryonfailure()' 'usemysql' call. ---&it; myself.data.mysqlclient.mysqlexcept (0x80004005): connect timeout expired. pool connect use. try follow st error' recommend try add enableretryonfailure() member this.configureservices(services: iservicecollection) = services.adoption() |&it; ignore services.adductors() |&it; ignore service .adddbcontext&it;applicationdbcontext&it;( fun (service:iserviceprovider) (context:dbcontextoptionsbuilder) -&it; context.usemysql(profile.dbconnectiontoadmin /*here*/)|&it; ignore) ... can't find document ad option f# &amp; myself, cause found into written c#. may problem use pool (default max=100) wrote next: ... do! cox.savechangesasync true |&it; async.awaittask |&it; async.ignore cox.database.closeconnection() longer.loginformation(&quit;account{0} updated&quit;, id) anyway problem solved. new expert f# async can understand incorrectly. could anyone help it?"
49782240,Can I do case insensitive search with JSON_EXTRACT in MySQL?,"I am running SELECT * FROM mytable WHERE LOWER(JSON_EXTRACT(metadata, ""$.title"")) = 'hello world' with the intent that hello world is data from a user that I will flatten to all lowercase. The actual value in my db is ""Hello World"", but this search comes back empty every time.
If I do a SELECT LOWER(JSON_EXTRACT(metadata, ""$.title"")) FROM mytable, it certainly comes back lowercase as hello world. Not sure what I'm missing here.
Queries to get actual values:
SELECT JSON_EXTRACT(metadata, ""$.title"")  FROM mytable gets me ""Hello World""
SELECT LOWER(JSON_EXTRACT(metadata, ""$.title""))  FROM mytable gets me ""hello world""
Queries trying to find the right row
Gets me value
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""Hello World""
SELECT * FROM mytable WHERE metadata-&gt;""$.title"" = ""Hello World""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""%hello world%""
Gets me nothing
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""hello world""
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") LIKE  ""%hello world%""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"") = ""hello world""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""hello""
So it looks like the result is giving back the value, including quotes. That doesn't appear to be the issue though, given I get a result when I match the case. I am also confused why the % at the start is solving my issue. There is no space between the "" and H. I typed the JSON out myself.
I also updated metadata column straight to {""title"":""Hello World""} by manually typing. MySQL automatically adds a space after colon to make it {""title"": ""Hello World""}, which is fine, but was just sanity checking any spaces.
",<mysql><json>,1733,0,21,8822,14,58,107,69,10018,,151,3,13,2018-04-11 18:35,2018-04-11 20:35,2018-04-11 20:35,0.0,0.0,Basic,10,"<mysql><json>, Can I do case insensitive search with JSON_EXTRACT in MySQL?, I am running SELECT * FROM mytable WHERE LOWER(JSON_EXTRACT(metadata, ""$.title"")) = 'hello world' with the intent that hello world is data from a user that I will flatten to all lowercase. The actual value in my db is ""Hello World"", but this search comes back empty every time.
If I do a SELECT LOWER(JSON_EXTRACT(metadata, ""$.title"")) FROM mytable, it certainly comes back lowercase as hello world. Not sure what I'm missing here.
Queries to get actual values:
SELECT JSON_EXTRACT(metadata, ""$.title"")  FROM mytable gets me ""Hello World""
SELECT LOWER(JSON_EXTRACT(metadata, ""$.title""))  FROM mytable gets me ""hello world""
Queries trying to find the right row
Gets me value
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""Hello World""
SELECT * FROM mytable WHERE metadata-&gt;""$.title"" = ""Hello World""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""%hello world%""
Gets me nothing
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""hello world""
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") LIKE  ""%hello world%""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"") = ""hello world""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""hello""
So it looks like the result is giving back the value, including quotes. That doesn't appear to be the issue though, given I get a result when I match the case. I am also confused why the % at the start is solving my issue. There is no space between the "" and H. I typed the JSON out myself.
I also updated metadata column straight to {""title"":""Hello World""} by manually typing. MySQL automatically adds a space after colon to make it {""title"": ""Hello World""}, which is fine, but was just sanity checking any spaces.
","<myself><son>, case intensity search json_extract myself?, run select * metal lower(json_extract(metadata, ""$.title"")) = 'hello world' intent hello world data user flatten lowercase. actual value do ""hello world"", search come back empty every time. select lower(json_extract(metadata, ""$.title"")) table, certainly come back lowers hello world. sure i'm miss here. query get actual values: select json_extract(metadata, ""$.title"") metal get ""hello world"" select lower(json_extract(metadata, ""$.title"")) metal get ""hello world"" query try find right row get value select * metal json_extract(metadata, ""$.title"") = ""hello world"" select * metal metadata-&it;""$.title"" = ""hello world"" select * tut lower(metadata-&it;""$.title"") like ""%hello world%"" get not select * metal json_extract(metadata, ""$.title"") = ""hello world"" select * metal json_extract(metadata, ""$.title"") like ""%hello world%"" select * tut lower(metadata-&it;""$.title"") = ""hello world"" select * tut lower(metadata-&it;""$.title"") like ""hello"" look like result give back value, include quotes. appear issue though, given get result match case. also confuse % start sole issue. space "" h. type son myself. also update metadata column straight {""title"":""hello world""} manual tying. myself automatic add space colon make {""title"": ""hello world""}, fine, anti check spaces."
51151773,Best practice calling scalar functions with Entity Framework Core (2.1),"I often need to call scalar functions that are defined on a SQL Server from my web applications (ASP.NET Core / EF Core). Since these functions are just simple helper functions and I also use a lot of them I use a general pattern for calling these scalar functions - with the help of the new query types available from EF Core 2.1.
Since I am relatively new to EF Core my question is if this pattern might cause problems and/or if there is a better solution or best practice for calling scalar functions. The solution works and I cannot observe any problems so far but for example I wondered if using the same query type for different functions might lead to unexpected values or weird behaviour due to caching/tracking behaviour, etc. within EF Core - it's more of a gut feeling.
So here's the pattern:
Instead of defining different entity types for every single scalar function I simply define one generic type:
public class PrimitiveDto&lt;T&gt;
{
    public T Value { get; set; }
}
In my context class I register these types for every return type I expect from the scalar functions I want to use - so for all scalar functions returning 'int' the context class would have one additional entry like this:
public virtual DbQuery&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
For EF Core &gt;= 3 it is:
public virtual DbSet&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
In every part of the application where I want to call a scalar function returning 'int' I simply use the same following pattern:
context.BasicIntDto.FromSql(&quot;SELECT &lt;FUNCTION&gt; AS Value&quot;)
By using this pattern I can call any number of functions the same way without defining additional types or extending the context class.
Please let me know if I could run into a trap through this pattern. Thank you very much.
",<c#><sql-server><entity-framework><asp.net-core><entity-framework-core>,1820,0,7,700,0,8,18,54,8974,0.0,305,1,13,2018-07-03 10:03,2019-01-15 16:03,,196.0,,Basic,10,"<c#><sql-server><entity-framework><asp.net-core><entity-framework-core>, Best practice calling scalar functions with Entity Framework Core (2.1), I often need to call scalar functions that are defined on a SQL Server from my web applications (ASP.NET Core / EF Core). Since these functions are just simple helper functions and I also use a lot of them I use a general pattern for calling these scalar functions - with the help of the new query types available from EF Core 2.1.
Since I am relatively new to EF Core my question is if this pattern might cause problems and/or if there is a better solution or best practice for calling scalar functions. The solution works and I cannot observe any problems so far but for example I wondered if using the same query type for different functions might lead to unexpected values or weird behaviour due to caching/tracking behaviour, etc. within EF Core - it's more of a gut feeling.
So here's the pattern:
Instead of defining different entity types for every single scalar function I simply define one generic type:
public class PrimitiveDto&lt;T&gt;
{
    public T Value { get; set; }
}
In my context class I register these types for every return type I expect from the scalar functions I want to use - so for all scalar functions returning 'int' the context class would have one additional entry like this:
public virtual DbQuery&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
For EF Core &gt;= 3 it is:
public virtual DbSet&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
In every part of the application where I want to call a scalar function returning 'int' I simply use the same following pattern:
context.BasicIntDto.FromSql(&quot;SELECT &lt;FUNCTION&gt; AS Value&quot;)
By using this pattern I can call any number of functions the same way without defining additional types or extending the context class.
Please let me know if I could run into a trap through this pattern. Thank you very much.
","<c#><sal-server><entity-framework><asp.net-core><entity-framework-core>, best practice call scala function entity framework core (2.1), often need call scala function define sal server web applied (asp.net core / of core). since function simple helper function also use lot use genet pattern call scala function - help new query type avail of core 2.1. since red new of core question pattern might cause problem and/or better slut best practice call scala functions. slut work cannot observe problem far example wonder use query type differ function might lead expect value weird behaviour due catching/track behaviour, etc. within of core - gut feeling. here' pattern: instead define differ entity type every single scala function simple define one genet type: public class primitivedto&it;t&it; { public value { get; set; } } context class resist type every return type expect scala function want use - scala function return 'in' context class would one admit entry like this: public virtual query&it;primitivedto&it;in&it;&it; basicintdto { get; set; } of core &it;= 3 is: public virtual set&it;primitivedto&it;in&it;&it; basicintdto { get; set; } every part applied want call scala function return 'in' simple use follow pattern: context.basicintdto.fromsql(&quit;select &it;function&it; value&quit;) use pattern call number function way without define admit type extend context class. pleas let know could run trap pattern. thank much."
48557948,Where should I register my DBAL type?,"I am using Doctrine's enum types to track the status of an entity that I am using in a Symfony application. I am using (roughly) the methods described here: 
http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/cookbook/mysql-enums.html
My problem comes when I try to update the database schema. I get the following error:
  [Doctrine\DBAL\DBALException]
  Unknown column type ""EnumStatusType"" requested. Any Doctrine type that you use has to be registered
   with \Doctrine\DBAL\Types\Type::addType(). You can get a list of all the known types with \Doctrin
  e\DBAL\Types\Type::getTypesMap(). If this error occurs during database introspection then you might
   have forgot to register all database types for a Doctrine Type. Use AbstractPlatform#registerDoctr
  ineTypeMapping() or have your custom types implement Type#getMappedDatabaseTypes(). If the type nam
  e is empty you might have a problem with the cache or forgot some mapping information.
This error is very helpful in a way -- as is the documentation -- but both of those resources leave out two pieces of information: In which file should I use addType() to register my new type? 
Two secondary questions: Should I call the addType() method statically, as shown in the examples? If not, how should I retrieve an object in order to call the method non-statically?
",<php><mysql><symfony><doctrine-orm><doctrine>,1342,2,7,1034,5,35,70,70,10124,0.0,843,1,13,2018-02-01 8:03,2018-02-01 8:07,2018-02-01 8:07,0.0,0.0,Basic,3,"<php><mysql><symfony><doctrine-orm><doctrine>, Where should I register my DBAL type?, I am using Doctrine's enum types to track the status of an entity that I am using in a Symfony application. I am using (roughly) the methods described here: 
http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/cookbook/mysql-enums.html
My problem comes when I try to update the database schema. I get the following error:
  [Doctrine\DBAL\DBALException]
  Unknown column type ""EnumStatusType"" requested. Any Doctrine type that you use has to be registered
   with \Doctrine\DBAL\Types\Type::addType(). You can get a list of all the known types with \Doctrin
  e\DBAL\Types\Type::getTypesMap(). If this error occurs during database introspection then you might
   have forgot to register all database types for a Doctrine Type. Use AbstractPlatform#registerDoctr
  ineTypeMapping() or have your custom types implement Type#getMappedDatabaseTypes(). If the type nam
  e is empty you might have a problem with the cache or forgot some mapping information.
This error is very helpful in a way -- as is the documentation -- but both of those resources leave out two pieces of information: In which file should I use addType() to register my new type? 
Two secondary questions: Should I call the addType() method statically, as shown in the examples? If not, how should I retrieve an object in order to call the method non-statically?
","<pp><myself><symfony><doctrine-or><doctrine>, resist deal type?, use doctrine' end type track state entity use symfoni application. use (roughly) method describe here: http://docs.doctrine-project.org/projects/doctrine-or/en/latest/cookbook/myself-enemy.html problem come try update database scheme. get follow error: [doctrine\deal\dbalexception] unknown column type ""enumstatustype"" requested. doctrine type use resist \doctrine\deal\types\type::addtype(). get list known type \doctrine e\deal\types\type::gettypesmap(). error occur database introspect might forgot resist database type doctrine type. use abstractplatform#registerdoctr inetypemapping() custom type implement type#getmappeddatabasetypes(). type am e empty might problem each forgot map information. error help way -- document -- resource leave two piece information: file use addtype() resist new type? two secondary questions: call addtype() method ecstatically, shown examples? not, retrieve object order call method non-ecstatically?"
53694089,ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data,"Problem
I am not able to connect to my Cloud SQL postgres instance with the command line, which has been working previously:
gcloud sql connect &lt;instance_name&gt; --user=&lt;username&gt;
This is the error I'm getting:
ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data.
Version
Running macOS Mojave 10.14 (18A391) with a tethered 4G hotspot via my Samsung Galaxy S8.
$ gcloud --version
Google Cloud SDK 227.0.0
bq 2.0.39
core 2018.11.30
gsutil 4.34
Log
Running the command with the --log-http flag, it returns:
{
 ""error"": {
  ""errors"": [
   {
    ""domain"": ""global"",
    ""reason"": ""invalidRequest"",
    ""message"": ""The incoming request contained invalid data.""
   }
  ],
  ""code"": 400,
  ""message"": ""The incoming request contained invalid data.""
 }
}
Question
Why is this happening and what can I do to fix it?
",<google-cloud-platform><google-cloud-sql>,851,0,21,698,1,6,18,59,11343,0.0,9,3,13,2018-12-09 16:00,2018-12-09 18:07,2018-12-09 18:07,0.0,0.0,Basic,3,"<google-cloud-platform><google-cloud-sql>, ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data, Problem
I am not able to connect to my Cloud SQL postgres instance with the command line, which has been working previously:
gcloud sql connect &lt;instance_name&gt; --user=&lt;username&gt;
This is the error I'm getting:
ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data.
Version
Running macOS Mojave 10.14 (18A391) with a tethered 4G hotspot via my Samsung Galaxy S8.
$ gcloud --version
Google Cloud SDK 227.0.0
bq 2.0.39
core 2018.11.30
gsutil 4.34
Log
Running the command with the --log-http flag, it returns:
{
 ""error"": {
  ""errors"": [
   {
    ""domain"": ""global"",
    ""reason"": ""invalidRequest"",
    ""message"": ""The incoming request contained invalid data.""
   }
  ],
  ""code"": 400,
  ""message"": ""The incoming request contained invalid data.""
 }
}
Question
Why is this happening and what can I do to fix it?
","<goose-cloud-platform><goose-cloud-sal>, error: (cloud.sal.connect) httperror 400: income request contain invalid data, problem all connect cloud sal poster instant command line, work previously: cloud sal connect &it;instance_name&it; --user=&it;surname&it; error i'm getting: error: (cloud.sal.connect) httperror 400: income request contain invalid data. version run mack moral 10.14 (18a391) ether g hotspot via samson galaxy s. $ cloud --version good cloud sd 227.0.0 by 2.0.39 core 2018.11.30 gsutil 4.34 log run command --log-http flag, returns: { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""invalidrequest"", ""message"": ""the income request contain invalid data."" } ], ""code"": 400, ""message"": ""the income request contain invalid data."" } } question happen fix it?"
48837393,Scala doobie fragment with generic type parameter,"I am trying to abstract inserting objects of different types into sql tables of similar structure. Here's what I'm trying to do:
class TableAccess[A : Meta](table: String) {
  def insert(key: String, a: A): ConnectionIO[Unit] = {
    (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
  }
}
But I get this compile error:
[error] diverging implicit expansion for type doobie.util.param.Param[A]
[error] starting with method fromMeta in object Param
[error]     (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
All I can find in the documentation is:
  doobie allows you to interpolate values of any type (and options
  thereof) with an Meta instance, which includes...
But it seems that is not enough in this case; what's the right typeclass/imports/conversions I need?
",<sql><scala><doobie>,865,0,8,18284,2,37,59,58,1581,0.0,334,3,13,2018-02-17 2:47,2018-03-18 8:42,2020-01-06 19:10,29.0,688.0,Basic,5,"<sql><scala><doobie>, Scala doobie fragment with generic type parameter, I am trying to abstract inserting objects of different types into sql tables of similar structure. Here's what I'm trying to do:
class TableAccess[A : Meta](table: String) {
  def insert(key: String, a: A): ConnectionIO[Unit] = {
    (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
  }
}
But I get this compile error:
[error] diverging implicit expansion for type doobie.util.param.Param[A]
[error] starting with method fromMeta in object Param
[error]     (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
All I can find in the documentation is:
  doobie allows you to interpolate values of any type (and options
  thereof) with an Meta instance, which includes...
But it seems that is not enough in this case; what's the right typeclass/imports/conversions I need?
","<sal><scala><double>, scala door fragment genet type parameter, try abstract insert object differ type sal table similar structure. here' i'm try do: class tableaccess[a : met](table: string) { def insert(key: string, a: a): connection[unit] = { (fr""insert "" ++ fragment.cost(table) ++ fr"" value ($key, $a);"").update.run.map(_ =&it; ()) } } get compel error: [error] divert implicit expanse type double.until.parma.parma[a] [error] start method frommeta object parma [error] (fr""insert "" ++ fragment.cost(table) ++ fr"" value ($key, $a);"").update.run.map(_ =&it; ()) find document is: door allow internal value type (and option thereof) met instance, includes... seem enough case; what' right typeclass/imports/covers need?"
57987355,Remove sensitive information from environment variables in postgres docker container,"I want to make a postgres database image but don't want to expose password and username which are stored as environment variable when produced using docker-compose.yml file. Basically, I don't want anyone to exec into the container and find out the variables.
One way is to use docker-secrets, but I don't want to to use docker swarm because my containers would be running on a single host.
my docker-compose file -
    version: ""3""
    services:
       db:
         image: postgres:10.0-alpine
      environment:
         POSTGRES_USER: 'user'
         POSTGRES_PASSWORD: 'pass'
         POSTGRES_DB: 'db'
Things I have tried -
1) unset the environment variable at the end of entrypoint-entrypoint.sh 
        for f in /docker-entrypoint-initdb.d/*; do
            case ""$f"" in
            *.sh)     echo ""$0: running $f""; . ""$f"" ;;
            *.sql)    echo ""$0: running $f""; ""${psql[@]}"" -f ""$f""; echo ;;
            *.sql.gz) echo ""$0: running $f""; gunzip -c ""$f"" | ""${psql[@]}""; echo ;;
            *)        echo ""$0: ignoring $f"" ;;
            esac
            echo
        done
        unset POSTGRES_USER
nothing happened though. :(
2) init.sql inside docker-entrypoint-initdb.d, to create db, user and pass without using env.
I shared the volume, as - 
```
   volumes:
       - ./docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
```
and, on my host, inside docker-entrypoint-initdb.d, I saved an init.sql as -
CREATE DATABASE docker_db;CREATE USER docker_user with encrypted password 'pass';GRANT ALL PRIVILEGES ON DATABASE docker_db TO docker_user;
I moved inside the running container and this file was there but, no user or database was created as mentioned in the file.
I have been stuck on this for past two days, any help is much appreciated.
",<postgresql><docker><docker-compose><dockerfile><docker-swarm>,1767,0,23,608,1,5,16,43,8922,0.0,83,5,13,2019-09-18 7:32,2019-09-18 7:38,2019-09-19 6:48,0.0,1.0,Basic,14,"<postgresql><docker><docker-compose><dockerfile><docker-swarm>, Remove sensitive information from environment variables in postgres docker container, I want to make a postgres database image but don't want to expose password and username which are stored as environment variable when produced using docker-compose.yml file. Basically, I don't want anyone to exec into the container and find out the variables.
One way is to use docker-secrets, but I don't want to to use docker swarm because my containers would be running on a single host.
my docker-compose file -
    version: ""3""
    services:
       db:
         image: postgres:10.0-alpine
      environment:
         POSTGRES_USER: 'user'
         POSTGRES_PASSWORD: 'pass'
         POSTGRES_DB: 'db'
Things I have tried -
1) unset the environment variable at the end of entrypoint-entrypoint.sh 
        for f in /docker-entrypoint-initdb.d/*; do
            case ""$f"" in
            *.sh)     echo ""$0: running $f""; . ""$f"" ;;
            *.sql)    echo ""$0: running $f""; ""${psql[@]}"" -f ""$f""; echo ;;
            *.sql.gz) echo ""$0: running $f""; gunzip -c ""$f"" | ""${psql[@]}""; echo ;;
            *)        echo ""$0: ignoring $f"" ;;
            esac
            echo
        done
        unset POSTGRES_USER
nothing happened though. :(
2) init.sql inside docker-entrypoint-initdb.d, to create db, user and pass without using env.
I shared the volume, as - 
```
   volumes:
       - ./docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
```
and, on my host, inside docker-entrypoint-initdb.d, I saved an init.sql as -
CREATE DATABASE docker_db;CREATE USER docker_user with encrypted password 'pass';GRANT ALL PRIVILEGES ON DATABASE docker_db TO docker_user;
I moved inside the running container and this file was there but, no user or database was created as mentioned in the file.
I have been stuck on this for past two days, any help is much appreciated.
","<postgresql><doctor><doctor-compose><dockerfile><doctor-swarm>, remove sent inform environs variable poster doctor container, want make poster database image want expose password usernam store environs variable produce use doctor-compose.you file. basically, want anyone even contain find variable. one way use doctor-secrets, want use doctor swarm contain would run single host. doctor-compose file - version: ""3"" services: do: image: postures:10.0-again environment: postgres_user: 'user' postgres_password: 'pass' postgres_db: 'do' thing try - 1) onset environs variable end entrypoint-entrypoint.s f /doctor-entrypoint-initdb.d/*; case ""$f"" *.s) echo ""$0: run $f""; . ""$f"" ;; *.sal) echo ""$0: run $f""; ""${pool[@]}"" -f ""$f""; echo ;; *.sal.go) echo ""$0: run $f""; gunzip -c ""$f"" | ""${pool[@]}""; echo ;; *) echo ""$0: ignore $f"" ;; sac echo done onset postgres_us not happen though. :( 2) knit.sal inside doctor-entrypoint-initdb.d, great do, user pass without use end. share volume, - ``` volumes: - ./doctor-entrypoint-initdb.d:/doctor-entrypoint-initdb.d ``` and, host, inside doctor-entrypoint-initdb.d, save knit.sal - great database docker_db;or user docker_us encrypt password 'pass';grant privilege database docker_db docker_user; move inside run contain file but, user database great mention file. stuck past two days, help much appreciated."
49391212,PostgreSQL: compare jsons,"As known, at the moment PostgreSQL has no method to compare two json values. The comparison like json = json doesn't work. But what about casting json to text before?
Then
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""b""}')::json::text
returns true
while
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""d""}')::json::text
returns false
I tried several variants with more complex objects and it works as expected.
Are there any gotchas in this solution?
UPDATE:
The compatibility with v9.3 is needed
",<json><postgresql>,528,0,9,351,1,3,15,59,38607,0.0,4,3,13,2018-03-20 17:59,2018-03-20 19:51,2018-03-20 19:51,0.0,0.0,Basic,2,"<json><postgresql>, PostgreSQL: compare jsons, As known, at the moment PostgreSQL has no method to compare two json values. The comparison like json = json doesn't work. But what about casting json to text before?
Then
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""b""}')::json::text
returns true
while
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""d""}')::json::text
returns false
I tried several variants with more complex objects and it works as expected.
Are there any gotchas in this solution?
UPDATE:
The compatibility with v9.3 is needed
","<son><postgresql>, postgresql: compare sons, known, moment postgresql method compare two son values. comparison like son = son work. cast son text before? select ('{""x"":""a"", ""y"":""b""}')::son::text = ('{""x"":""a"", ""y"":""b""}')::son::text return true select ('{""x"":""a"", ""y"":""b""}')::son::text = ('{""x"":""a"", ""y"":""d""}')::son::text return fall try never variant complex object work expected. notch solution? update: compact ve.3 need"
48554917,Getting Sequelize.js library to work on Amazon Lambda,"So I'm trying to run a lambda on amazon and narrowed down the error finally by testing the lambda in amazons testing console.
The error I got is this.
{
  ""errorMessage"": ""Please install mysql2 package manually"",
  ""errorType"": ""Error"",
  ""stackTrace"": [
    ""new MysqlDialect (/var/task/node_modules/sequelize/lib/dialects/mysql/index.js:14:30)"",
    ""new Sequelize (/var/task/node_modules/sequelize/lib/sequelize.js:234:20)"",
    ""Object.exports.getSequelizeConnection (/var/task/src/twilio/twilio.js:858:20)"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:679:25)"",
    ""__webpack_require__ (/var/task/src/twilio/twilio.js:20:30)"",
    ""/var/task/src/twilio/twilio.js:63:18"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:66:10)"",
    ""Module._compile (module.js:570:32)"",
    ""Object.Module._extensions..js (module.js:579:10)"",
    ""Module.load (module.js:487:32)"",
    ""tryModuleLoad (module.js:446:12)"",
    ""Function.Module._load (module.js:438:3)"",
    ""Module.require (module.js:497:17)"",
    ""require (internal/module.js:20:19)""
  ]
}
Easy enough, so I have to install mysql2.  So I added it to my package.json file.
{
  ""name"": ""test-api"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""handler.js"",
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 0""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""devDependencies"": {
    ""aws-sdk"": ""^2.153.0"",
    ""babel-core"": ""^6.26.0"",
    ""babel-loader"": ""^7.1.2"",
    ""babel-plugin-transform-runtime"": ""^6.23.0"",
    ""babel-preset-es2015"": ""^6.24.1"",
    ""babel-preset-stage-3"": ""^6.24.1"",
    ""serverless-domain-manager"": ""^1.1.20"",
    ""serverless-dynamodb-autoscaling"": ""^0.6.2"",
    ""serverless-webpack"": ""^4.0.0"",
    ""webpack"": ""^3.8.1"",
    ""webpack-node-externals"": ""^1.6.0""
  },
  ""dependencies"": {
    ""babel-runtime"": ""^6.26.0"",
    ""mailgun-js"": ""^0.13.1"",
    ""minimist"": ""^1.2.0"",
    ""mysql"": ""^2.15.0"",
    ""mysql2"": ""^1.5.1"",
    ""qs"": ""^6.5.1"",
    ""sequelize"": ""^4.31.2"",
    ""serverless"": ""^1.26.0"",
    ""serverless-plugin-scripts"": ""^1.0.2"",
    ""twilio"": ""^3.10.0"",
    ""uuid"": ""^3.1.0""
  }
}
I noticed when I do sls deploy however, it seems to only be packaging some of the modules?
Serverless: Package lock found - Using locked versions
Serverless: Packing external modules: babel-runtime@^6.26.0, twilio@^3.10.0, qs@^6.5.1, mailgun-js@^0.13.1, sequelize@^4.31.2, minimi
st@^1.2.0, uuid@^3.1.0
Serverless: Packaging service...
Serverless: Uploading CloudFormation file to S3...
Serverless: Uploading artifacts...
Serverless: Validating template...
Serverless: Updating Stack...
Serverless: Checking Stack update progress...
................................
Serverless: Stack update finished...
I think this is why it's not working.  In short, how do I get mysql2 library to be packaged correctly with serverless so my lambda function will work with the sequelize library?
Please note that when I test locally my code works fine.
My serverless file is below
service: testapi
# Use serverless-webpack plugin to transpile ES6/ES7
plugins:
  - serverless-webpack
  - serverless-plugin-scripts
  # - serverless-domain-manager
custom:
  #Define the Stage or default to Staging.
  stage: ${opt:stage, self:provider.stage}
  webpackIncludeModules: true
  #Define Databases Here
  databaseName: ""${self:service}-${self:custom.stage}""
  #Define Bucket Names Here
  uploadBucket: ""${self:service}-uploads-${self:custom.stage}""
  #Custom Script setup
  scripts:
    hooks:
      #Script below will run schema changes to the database as neccesary and update according to stage.
      'deploy:finalize':  node database-schema-update.js --stage ${self:custom.stage}
  #Domain Setup
  # customDomain:
  #    basePath: ""/""
  #    domainName: ""api-${self:custom.stage}.test.com""
  #    stage: ""${self:custom.stage}""
  #    certificateName: ""*.test.com""
  #    createRoute53Record: true
provider:
  name: aws
  runtime: nodejs6.10
  stage: staging
  region: us-east-1
  environment:
    DOMAIN_NAME: ""api-${self:custom.stage}.test.com""
    DATABASE_NAME: ${self:custom.databaseName}
    DATABASE_USERNAME: ${env:RDS_USERNAME}
    DATABASE_PASSWORD: ${env:RDS_PASSWORD}
    UPLOAD_BUCKET: ${self:custom.uploadBucket}
    TWILIO_ACCOUNT_SID: """"
    TWILIO_AUTH_TOKEN: """"
    USER_POOL_ID: """"
    APP_CLIENT_ID: """"
    REGION: ""us-east-1""
    IDENTITY_POOL_ID: """"
    RACKSPACE_API_KEY: """"
  #Below controls permissions for lambda functions.
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:UpdateTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource: ""arn:aws:dynamodb:us-east-1:*:*""
functions:
  create_visit:
    handler: src/visits/create.main
    events:
      - http:
          path: visits
          method: post
          cors: true
          authorizer: aws_iam
  get_visit:
    handler: src/visits/get.main
    events:
      - http:
          path: visits/{id}
          method: get
          cors: true
          authorizer: aws_iam
  list_visit:
    handler: src/visits/list.main
    events:
      - http:
          path: visits
          method: get
          cors: true
          authorizer: aws_iam
  update_visit:
    handler: src/visits/update.main
    events:
      - http:
          path: visits/{id}
          method: put
          cors: true
          authorizer: aws_iam
  delete_visit:
    handler: src/visits/delete.main
    events:
      - http:
          path: visits/{id}
          method: delete
          cors: true
          authorizer: aws_iam
  twilio_send_text_message:
    handler: src/twilio/twilio.send_text_message
    events:
      - http:
          path: twilio/sendtextmessage
          method: post
          cors: true
          authorizer: aws_iam
  #This function handles incoming calls and where to route it to.
  twilio_incoming_call:
    handler: src/twilio/twilio.incoming_calls
    events:
      - http:
          path: twilio/calls
          method: post
  twilio_failure:
    handler: src/twilio/twilio.twilio_failure
    events:
      - http:
          path: twilio/failure
          method: post
  twilio_statuschange:
    handler: src/twilio/twilio.statuschange
    events:
      - http:
          path: twilio/statuschange
          method: post
  twilio_incoming_message:
    handler: src/twilio/twilio.incoming_message
    events:
      - http:
          path: twilio/messages
          method: post
  twilio_whisper:
    handler: src/twilio/twilio.whisper
    events:
      - http:
          path: twilio/whisper
          method: post
      - http:
          path: twilio/whisper
          method: get
  twilio_start_call:
    handler: src/twilio/twilio.start_call
    events:
      - http:
          path: twilio/startcall
          method: post
      - http:
          path: twilio/startcall
          method: get
resources:
  Resources:
    uploadBucket:
       Type: AWS::S3::Bucket
       Properties:
         BucketName: ${self:custom.uploadBucket}
    RDSDatabase:
      Type: AWS::RDS::DBInstance
      Properties:
        Engine : mysql
        MasterUsername: ${env:RDS_USERNAME}
        MasterUserPassword: ${env:RDS_PASSWORD}
        DBInstanceClass : db.t2.micro
        AllocatedStorage: '5'
        PubliclyAccessible: true
        #TODO: The Value of Stage is also available as a TAG automatically which I may use to replace this manually being put here..
        Tags:
          -
            Key: ""Name""
            Value: ${self:custom.databaseName}
      DeletionPolicy: Snapshot
    DNSRecordSet:
      Type: AWS::Route53::RecordSet
      Properties:
        HostedZoneName: test.com.
        Name: database-${self:custom.stage}.test.com
        Type: CNAME
        TTL: '300'
        ResourceRecords:
        - {""Fn::GetAtt"": [""RDSDatabase"",""Endpoint.Address""]}
      DependsOn: RDSDatabase
UPDATE:: So I confirmed that running sls package --stage dev seems to create this in the zip folder that would eventually upload to AWS.  This confirms that serverless is not creating the package correctly with the mysql2 reference for some reason? Why is this?
webpack config file as requested
const slsw = require(""serverless-webpack"");
const nodeExternals = require(""webpack-node-externals"");
module.exports = {
  entry: slsw.lib.entries,
  target: ""node"",
  // Since 'aws-sdk' is not compatible with webpack,
  // we exclude all node dependencies
  externals: [nodeExternals()],
  // Run babel on all .js files and skip those in node_modules
  module: {
    rules: [
      {
        test: /\.js$/,
        loader: ""babel-loader"",
        include: __dirname,
        exclude: /node_modules/
      }
    ]
  }
};
",<amazon-web-services><aws-lambda><amazon-rds><serverless-framework><node-mysql2>,8812,1,276,8781,12,86,154,77,9087,0.0,2051,2,13,2018-02-01 3:33,2018-02-01 8:02,2018-02-01 8:02,0.0,0.0,Basic,6,"<amazon-web-services><aws-lambda><amazon-rds><serverless-framework><node-mysql2>, Getting Sequelize.js library to work on Amazon Lambda, So I'm trying to run a lambda on amazon and narrowed down the error finally by testing the lambda in amazons testing console.
The error I got is this.
{
  ""errorMessage"": ""Please install mysql2 package manually"",
  ""errorType"": ""Error"",
  ""stackTrace"": [
    ""new MysqlDialect (/var/task/node_modules/sequelize/lib/dialects/mysql/index.js:14:30)"",
    ""new Sequelize (/var/task/node_modules/sequelize/lib/sequelize.js:234:20)"",
    ""Object.exports.getSequelizeConnection (/var/task/src/twilio/twilio.js:858:20)"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:679:25)"",
    ""__webpack_require__ (/var/task/src/twilio/twilio.js:20:30)"",
    ""/var/task/src/twilio/twilio.js:63:18"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:66:10)"",
    ""Module._compile (module.js:570:32)"",
    ""Object.Module._extensions..js (module.js:579:10)"",
    ""Module.load (module.js:487:32)"",
    ""tryModuleLoad (module.js:446:12)"",
    ""Function.Module._load (module.js:438:3)"",
    ""Module.require (module.js:497:17)"",
    ""require (internal/module.js:20:19)""
  ]
}
Easy enough, so I have to install mysql2.  So I added it to my package.json file.
{
  ""name"": ""test-api"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""handler.js"",
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 0""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""devDependencies"": {
    ""aws-sdk"": ""^2.153.0"",
    ""babel-core"": ""^6.26.0"",
    ""babel-loader"": ""^7.1.2"",
    ""babel-plugin-transform-runtime"": ""^6.23.0"",
    ""babel-preset-es2015"": ""^6.24.1"",
    ""babel-preset-stage-3"": ""^6.24.1"",
    ""serverless-domain-manager"": ""^1.1.20"",
    ""serverless-dynamodb-autoscaling"": ""^0.6.2"",
    ""serverless-webpack"": ""^4.0.0"",
    ""webpack"": ""^3.8.1"",
    ""webpack-node-externals"": ""^1.6.0""
  },
  ""dependencies"": {
    ""babel-runtime"": ""^6.26.0"",
    ""mailgun-js"": ""^0.13.1"",
    ""minimist"": ""^1.2.0"",
    ""mysql"": ""^2.15.0"",
    ""mysql2"": ""^1.5.1"",
    ""qs"": ""^6.5.1"",
    ""sequelize"": ""^4.31.2"",
    ""serverless"": ""^1.26.0"",
    ""serverless-plugin-scripts"": ""^1.0.2"",
    ""twilio"": ""^3.10.0"",
    ""uuid"": ""^3.1.0""
  }
}
I noticed when I do sls deploy however, it seems to only be packaging some of the modules?
Serverless: Package lock found - Using locked versions
Serverless: Packing external modules: babel-runtime@^6.26.0, twilio@^3.10.0, qs@^6.5.1, mailgun-js@^0.13.1, sequelize@^4.31.2, minimi
st@^1.2.0, uuid@^3.1.0
Serverless: Packaging service...
Serverless: Uploading CloudFormation file to S3...
Serverless: Uploading artifacts...
Serverless: Validating template...
Serverless: Updating Stack...
Serverless: Checking Stack update progress...
................................
Serverless: Stack update finished...
I think this is why it's not working.  In short, how do I get mysql2 library to be packaged correctly with serverless so my lambda function will work with the sequelize library?
Please note that when I test locally my code works fine.
My serverless file is below
service: testapi
# Use serverless-webpack plugin to transpile ES6/ES7
plugins:
  - serverless-webpack
  - serverless-plugin-scripts
  # - serverless-domain-manager
custom:
  #Define the Stage or default to Staging.
  stage: ${opt:stage, self:provider.stage}
  webpackIncludeModules: true
  #Define Databases Here
  databaseName: ""${self:service}-${self:custom.stage}""
  #Define Bucket Names Here
  uploadBucket: ""${self:service}-uploads-${self:custom.stage}""
  #Custom Script setup
  scripts:
    hooks:
      #Script below will run schema changes to the database as neccesary and update according to stage.
      'deploy:finalize':  node database-schema-update.js --stage ${self:custom.stage}
  #Domain Setup
  # customDomain:
  #    basePath: ""/""
  #    domainName: ""api-${self:custom.stage}.test.com""
  #    stage: ""${self:custom.stage}""
  #    certificateName: ""*.test.com""
  #    createRoute53Record: true
provider:
  name: aws
  runtime: nodejs6.10
  stage: staging
  region: us-east-1
  environment:
    DOMAIN_NAME: ""api-${self:custom.stage}.test.com""
    DATABASE_NAME: ${self:custom.databaseName}
    DATABASE_USERNAME: ${env:RDS_USERNAME}
    DATABASE_PASSWORD: ${env:RDS_PASSWORD}
    UPLOAD_BUCKET: ${self:custom.uploadBucket}
    TWILIO_ACCOUNT_SID: """"
    TWILIO_AUTH_TOKEN: """"
    USER_POOL_ID: """"
    APP_CLIENT_ID: """"
    REGION: ""us-east-1""
    IDENTITY_POOL_ID: """"
    RACKSPACE_API_KEY: """"
  #Below controls permissions for lambda functions.
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:UpdateTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource: ""arn:aws:dynamodb:us-east-1:*:*""
functions:
  create_visit:
    handler: src/visits/create.main
    events:
      - http:
          path: visits
          method: post
          cors: true
          authorizer: aws_iam
  get_visit:
    handler: src/visits/get.main
    events:
      - http:
          path: visits/{id}
          method: get
          cors: true
          authorizer: aws_iam
  list_visit:
    handler: src/visits/list.main
    events:
      - http:
          path: visits
          method: get
          cors: true
          authorizer: aws_iam
  update_visit:
    handler: src/visits/update.main
    events:
      - http:
          path: visits/{id}
          method: put
          cors: true
          authorizer: aws_iam
  delete_visit:
    handler: src/visits/delete.main
    events:
      - http:
          path: visits/{id}
          method: delete
          cors: true
          authorizer: aws_iam
  twilio_send_text_message:
    handler: src/twilio/twilio.send_text_message
    events:
      - http:
          path: twilio/sendtextmessage
          method: post
          cors: true
          authorizer: aws_iam
  #This function handles incoming calls and where to route it to.
  twilio_incoming_call:
    handler: src/twilio/twilio.incoming_calls
    events:
      - http:
          path: twilio/calls
          method: post
  twilio_failure:
    handler: src/twilio/twilio.twilio_failure
    events:
      - http:
          path: twilio/failure
          method: post
  twilio_statuschange:
    handler: src/twilio/twilio.statuschange
    events:
      - http:
          path: twilio/statuschange
          method: post
  twilio_incoming_message:
    handler: src/twilio/twilio.incoming_message
    events:
      - http:
          path: twilio/messages
          method: post
  twilio_whisper:
    handler: src/twilio/twilio.whisper
    events:
      - http:
          path: twilio/whisper
          method: post
      - http:
          path: twilio/whisper
          method: get
  twilio_start_call:
    handler: src/twilio/twilio.start_call
    events:
      - http:
          path: twilio/startcall
          method: post
      - http:
          path: twilio/startcall
          method: get
resources:
  Resources:
    uploadBucket:
       Type: AWS::S3::Bucket
       Properties:
         BucketName: ${self:custom.uploadBucket}
    RDSDatabase:
      Type: AWS::RDS::DBInstance
      Properties:
        Engine : mysql
        MasterUsername: ${env:RDS_USERNAME}
        MasterUserPassword: ${env:RDS_PASSWORD}
        DBInstanceClass : db.t2.micro
        AllocatedStorage: '5'
        PubliclyAccessible: true
        #TODO: The Value of Stage is also available as a TAG automatically which I may use to replace this manually being put here..
        Tags:
          -
            Key: ""Name""
            Value: ${self:custom.databaseName}
      DeletionPolicy: Snapshot
    DNSRecordSet:
      Type: AWS::Route53::RecordSet
      Properties:
        HostedZoneName: test.com.
        Name: database-${self:custom.stage}.test.com
        Type: CNAME
        TTL: '300'
        ResourceRecords:
        - {""Fn::GetAtt"": [""RDSDatabase"",""Endpoint.Address""]}
      DependsOn: RDSDatabase
UPDATE:: So I confirmed that running sls package --stage dev seems to create this in the zip folder that would eventually upload to AWS.  This confirms that serverless is not creating the package correctly with the mysql2 reference for some reason? Why is this?
webpack config file as requested
const slsw = require(""serverless-webpack"");
const nodeExternals = require(""webpack-node-externals"");
module.exports = {
  entry: slsw.lib.entries,
  target: ""node"",
  // Since 'aws-sdk' is not compatible with webpack,
  // we exclude all node dependencies
  externals: [nodeExternals()],
  // Run babel on all .js files and skip those in node_modules
  module: {
    rules: [
      {
        test: /\.js$/,
        loader: ""babel-loader"",
        include: __dirname,
        exclude: /node_modules/
      }
    ]
  }
};
","<amazon-web-services><was-labia><amazon-rd><serverless-framework><node-myself>, get sequelae.j library work amazon labia, i'm try run labia amazon narrow error final test labia amazon test console. error got this. { ""errormessage"": ""pleas instal myself package mentally"", ""errortype"": ""error"", ""stacktrace"": [ ""new mysqldialect (/war/task/node_modules/sequelae/limb/dialect/myself/index.is:14:30)"", ""new sequel (/war/task/node_modules/sequelae/limb/sequelae.is:234:20)"", ""object.exports.getsequelizeconnect (/war/task/sac/ilio/ilio.is:858:20)"", ""object.&it;anonymous&it; (/war/task/sac/ilio/ilio.is:679:25)"", ""__webpack_require__ (/war/task/sac/ilio/ilio.is:20:30)"", ""/war/task/sac/ilio/ilio.is:63:18"", ""object.&it;anonymous&it; (/war/task/sac/ilio/ilio.is:66:10)"", ""module.compel (module.is:570:32)"", ""object.module.extensions..j (module.is:579:10)"", ""module.load (module.is:487:32)"", ""trymoduleload (module.is:446:12)"", ""function.module.load (module.is:438:3)"", ""module.require (module.is:497:17)"", ""require (internal/module.is:20:19)"" ] } east enough, instal myself. ad package.son file. { ""name"": ""test-apt"", ""version"": ""1.0.0"", ""description"": """", ""main"": ""handle.is"", ""script"": { ""test"": ""echo \""error: test specified\"" &amp;&amp; exit 0"" }, ""keywords"": [], ""author"": """", ""license"": ""is"", ""devdependencies"": { ""was-sd"": ""^2.153.0"", ""babel-core"": ""^6.26.0"", ""babel-leader"": ""^7.1.2"", ""babel-plain-transform-auntie"": ""^6.23.0"", ""babel-present-es2015"": ""^6.24.1"", ""babel-present-stage-3"": ""^6.24.1"", ""serverless-domain-manager"": ""^1.1.20"", ""serverless-dynamodb-autoscaling"": ""^0.6.2"", ""serverless-webpack"": ""^4.0.0"", ""webpack"": ""^3.8.1"", ""webpack-node-externals"": ""^1.6.0"" }, ""dependencies"": { ""babel-auntie"": ""^6.26.0"", ""malign-is"": ""^0.13.1"", ""minimise"": ""^1.2.0"", ""myself"": ""^2.15.0"", ""myself"": ""^1.5.1"", ""is"": ""^6.5.1"", ""sequelae"": ""^4.31.2"", ""serverless"": ""^1.26.0"", ""serverless-plain-script"": ""^1.0.2"", ""ilio"": ""^3.10.0"", ""quid"": ""^3.1.0"" } } notice s deploy however, seem package nodules? serverless: package lock found - use lock version serverless: pack externa nodules: babel-auntie@^6.26.0, ilio@^3.10.0, is@^6.5.1, malign-is@^0.13.1, sequelae@^4.31.2, minims st@^1.2.0, quid@^3.1.0 serverless: package service... serverless: unload cloudform file s... serverless: unload artifacts... serverless: valid temple... serverless: update stick... serverless: check stick update progress... ................................ serverless: stick update finished... think working. short, get myself library package correctly serverless labia function work sequel library? pleas note test local code work fine. serverless file service: tetani # use serverless-webpack plain tranquil est/est plains: - serverless-webpack - serverless-plain-script # - serverless-domain-manage custom: #define stage default staying. stage: ${opt:stage, self:provider.stage} webpackincludemodules: true #define database databasename: ""${self:service}-${self:custom.stage}"" #define bucket name uploadbucket: ""${self:service}-uplands-${self:custom.stage}"" #custom script set script: hooks: #script run scheme change database neccesari update accord stage. 'deploy:finalize': node database-scheme-update.j --stage ${self:custom.stage} #domain set # customdomain: # basepath: ""/"" # domainname: ""apt-${self:custom.stage}.test.com"" # stage: ""${self:custom.stage}"" # certificatename: ""*.test.com"" # createroute53record: true provider: name: a auntie: nodes.10 stage: stage region: us-east-1 environment: domain_name: ""apt-${self:custom.stage}.test.com"" database_name: ${self:custom.databasename} database_username: ${end:rds_username} database_password: ${end:rds_password} upload_bucket: ${self:custom.uploadbucket} twilio_account_sid: """" twilio_auth_token: """" user_pool_id: """" app_client_id: """" region: ""us-east-1"" identity_pool_id: """" rackspace_api_key: """" #below control permits labia functions. iamrolestatements: - effect: allow action: - dynamodb:described - dynamodb:updated - dynamodb:query - dynamodb:scan - dynamodb:petite - dynamodb:puttee - dynamodb:updateitem - dynamodb:deleteitem resource: ""are:was:dynamodb:us-east-1:*:*"" functions: create_visit: handle: sac/visits/create.main events: - http: path: visit method: post corps: true authorized: aws_iam get_visit: handle: sac/visits/get.main events: - http: path: visits/{id} method: get corps: true authorized: aws_iam list_visit: handle: sac/visits/list.main events: - http: path: visit method: get corps: true authorized: aws_iam update_visit: handle: sac/visits/update.main events: - http: path: visits/{id} method: put corps: true authorized: aws_iam delete_visit: handle: sac/visits/delete.main events: - http: path: visits/{id} method: delete corps: true authorized: aws_iam twilio_send_text_message: handle: sac/ilio/ilio.send_text_messag events: - http: path: ilio/sendtextmessag method: post corps: true authorized: aws_iam #the function hand income call rout to. twilio_incoming_call: handle: sac/ilio/ilio.incoming_cal events: - http: path: ilio/cal method: post twilio_failure: handle: sac/ilio/ilio.twilio_failur events: - http: path: ilio/failure method: post twilio_statuschange: handle: sac/ilio/ilio.statuschang events: - http: path: ilio/statuschang method: post twilio_incoming_message: handle: sac/ilio/ilio.incoming_messag events: - http: path: ilio/message method: post twilio_whisper: handle: sac/ilio/ilio.whip events: - http: path: ilio/whip method: post - http: path: ilio/whip method: get twilio_start_call: handle: sac/ilio/ilio.start_cal events: - http: path: ilio/startcal method: post - http: path: ilio/startcal method: get resources: resources: uploadbucket: type: was::s::bucket properties: bucketname: ${self:custom.uploadbucket} rdsdatabase: type: was::rd::dint properties: engine : myself masterusername: ${end:rds_username} masteruserpassword: ${end:rds_password} dbinstanceclass : do.to.micro allocatedstorage: '5' publiclyaccessible: true #too: value stage also avail tag automatic may use replace manual put here.. tags: - key: ""name"" value: ${self:custom.databasename} deletionpolicy: snapshot dnsrecordset: type: was::route::recorded properties: hostedzonename: test.com. name: database-${self:custom.stage}.test.com type: came til: '300' resourcerecords: - {""fn::etats"": [""rdsdatabase"",""endpoint.address""]} depends: rdsdatabas update:: confirm run s package --stage de seem great zip older would events unload was. confirm serverless great package correctly myself refer reason? this? webpack confirm file request cost slow = require(""serverless-webpack""); cost nodeextern = require(""webpack-node-externals""); module.export = { entry: slow.limb.entries, target: ""node"", // since 'was-sd' compact webpack, // exclude node depend externals: [nodeexternals()], // run babel .j file skin node_modul module: { rules: [ { test: /\.is$/, leader: ""babel-leader"", include: __dirname, exclude: /node_modules/ } ] } };"
48462011,Alembic migration: How to set server_onupdate in Alembic's alter_column function,"I'm trying to change a table column in PostgreSQL using Alembic but I don't know how to perform the needed update to apply the SQLAlchemy's server_onupdate property.
The column is:
changed = Column(ArrowType(timezone=True), server_default=utcnow(), primary_key=True)
I'm using the Arrowtype column type from SQLAlchemy_utils package (this is not a problem).
My intention is to create something like this:
changed = Column(ArrowType(timezone=True), **server_onupdate=utcnow()**, primary_key=True)
But using the Alembic function: alter_column
In the documentation there are only references to the server_default property but nothing about server_onupdate
Is there a way to achieve this?
Thanks
",<python><postgresql><sqlalchemy><alembic>,692,0,2,305,0,1,8,45,1575,0.0,46,1,13,2018-01-26 13:10,2023-02-22 20:59,,1853.0,,Basic,3,"<python><postgresql><sqlalchemy><alembic>, Alembic migration: How to set server_onupdate in Alembic's alter_column function, I'm trying to change a table column in PostgreSQL using Alembic but I don't know how to perform the needed update to apply the SQLAlchemy's server_onupdate property.
The column is:
changed = Column(ArrowType(timezone=True), server_default=utcnow(), primary_key=True)
I'm using the Arrowtype column type from SQLAlchemy_utils package (this is not a problem).
My intention is to create something like this:
changed = Column(ArrowType(timezone=True), **server_onupdate=utcnow()**, primary_key=True)
But using the Alembic function: alter_column
In the documentation there are only references to the server_default property but nothing about server_onupdate
Is there a way to achieve this?
Thanks
","<patron><postgresql><sqlalchemy><alembic>, limb migration: set server_onupd alembic' alter_column function, i'm try change table column postgresql use limb know perform need update apply sqlalchemy' server_onupd property. column is: change = column(arrowtype(timezone=true), server_default=utcnow(), primary_key=true) i'm use arrowtyp column type sqlalchemy_util package (the problem). intent great cometh like this: change = column(arrowtype(timezone=true), **server_onupdate=utcnow()**, primary_key=true) use limb function: alter_column document refer server_default property not server_onupd way achieve this? thank"
57795044,spring data JPA - mysql - findById() empty unless findAll() called before,"I'm struggling with this strange error: the findById() method of a CrudRepository returns Optional.empty, unless findAll() is called before when using mysql.
e.g.
User
@Entity
public class User {
    @Id
    @GeneratedValue
    private UUID id;
    public UUID getId() {
        return id;
    }
}
UserRepository
public interface UserRepository extends CrudRepository&lt;User, UUID&gt; { }
UserService
@Service
public class UserService {
    @Autowired
    private UserRepository userRepository;
    @Transactional
    public UUID create() {
        final User user = new User();
        userRepository.save(user);
        return user.getId();
    }
    @Transactional
    public User find(@PathVariable UUID userId) {
        // userRepository.findAll(); TODO without this functin call, Optinoal.empty is returned by the repo
        return userRepository.findById(userId).orElseThrow(() -&gt; new IllegalArgumentException(String.format(""missing user:%s"", userId)));
    }
}
UserApp
@SpringBootApplication
public class UserApp {
    private static final Logger LOG = LoggerFactory.getLogger(UserApp.class);
    @Autowired
    private UserService userService;
    @EventListener
    public void onApplicationEvent(ContextRefreshedEvent event) {
        final UUID userId = userService.create();
        final User user = userService.find(userId);
        LOG.info(""found user: {}"", user.getId());
    }
    public static void main(String[] args) {
        SpringApplication.run(UserApp.class, args);
    }
}
application.properties
spring.datasource.url=jdbc:mysql://localhost:3306/db_test
spring.datasource.username=springuser
spring.datasource.password=ThePassword
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.show-sql=true
spring.jpa.database=mysql
Why does the findAll() method call change the result of findById()?
Edit: Hibernate logs with findAll:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_ from user user0_
Without:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_0_ from user user0_ where user0_.id=?
",<mysql><spring><spring-data-jpa><spring-data>,2337,0,74,6382,14,56,96,57,3551,0.0,68,4,13,2019-09-04 19:55,2019-09-25 22:01,,21.0,,Basic,9,"<mysql><spring><spring-data-jpa><spring-data>, spring data JPA - mysql - findById() empty unless findAll() called before, I'm struggling with this strange error: the findById() method of a CrudRepository returns Optional.empty, unless findAll() is called before when using mysql.
e.g.
User
@Entity
public class User {
    @Id
    @GeneratedValue
    private UUID id;
    public UUID getId() {
        return id;
    }
}
UserRepository
public interface UserRepository extends CrudRepository&lt;User, UUID&gt; { }
UserService
@Service
public class UserService {
    @Autowired
    private UserRepository userRepository;
    @Transactional
    public UUID create() {
        final User user = new User();
        userRepository.save(user);
        return user.getId();
    }
    @Transactional
    public User find(@PathVariable UUID userId) {
        // userRepository.findAll(); TODO without this functin call, Optinoal.empty is returned by the repo
        return userRepository.findById(userId).orElseThrow(() -&gt; new IllegalArgumentException(String.format(""missing user:%s"", userId)));
    }
}
UserApp
@SpringBootApplication
public class UserApp {
    private static final Logger LOG = LoggerFactory.getLogger(UserApp.class);
    @Autowired
    private UserService userService;
    @EventListener
    public void onApplicationEvent(ContextRefreshedEvent event) {
        final UUID userId = userService.create();
        final User user = userService.find(userId);
        LOG.info(""found user: {}"", user.getId());
    }
    public static void main(String[] args) {
        SpringApplication.run(UserApp.class, args);
    }
}
application.properties
spring.datasource.url=jdbc:mysql://localhost:3306/db_test
spring.datasource.username=springuser
spring.datasource.password=ThePassword
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.show-sql=true
spring.jpa.database=mysql
Why does the findAll() method call change the result of findById()?
Edit: Hibernate logs with findAll:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_ from user user0_
Without:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_0_ from user user0_ where user0_.id=?
","<myself><spring><spring-data-pa><spring-data>, spring data pa - myself - findbyid() empty unless finally() call before, i'm struggle strange error: findbyid() method crudrepositori return optional.empty, unless finally() call use myself. e.g. user @entity public class user { @id @generatedvalu privat quid id; public quid get() { return id; } } userrepositori public interface userrepositori extend crudrepository&it;user, quid&it; { } userservic @service public class userservic { @autowir privat userrepositori userrepository; @transact public quid create() { final user user = new user(); userrepository.save(user); return user.get(); } @transact public user find(@pathvari quid used) { // userrepository.finally(); too without function call, optional.empty return rep return userrepository.findbyid(used).orelsethrow(() -&it; new illegalargumentexception(string.format(""miss user:%s"", used))); } } userapp @springbootappl public class userapp { privat static final longer log = loggerfactory.getlogger(userapp.class); @autowir privat userservic userservice; @eventlisten public void onapplicationevent(contextrefreshedev event) { final quid used = userservice.create(); final user user = userservice.find(used); log.into(""found user: {}"", user.get()); } public static void main(string[] arms) { springapplication.run(userapp.class, arms); } } application.property spring.datasource.curl=job:myself://localhost:3306/detest spring.datasource.surname=springs spring.datasource.password=thepassword spring.pa.liberate.del-auto=create-drop spring.pa.show-sal=true spring.pa.database=myself finally() method call change result findbyid()? edit: wiberd log finally: liberate: drop table exist user liberate: great table user (id binary(255) null, primary key (id)) engine=myisam liberate: insert user (id) value (?) liberate: select user.id id1_0_ user user without: liberate: drop table exist user liberate: great table user (id binary(255) null, primary key (id)) engine=myisam liberate: insert user (id) value (?) liberate: select user.id id1_0_0_ user user user.id=?"
48748070,SQLAlchemy connection via proxy,"I need to connect to existing database from SQLAlchemy via proxy.
        self.DB = {
            'drivername': 'oracle',
            'host': url,
            'port': port,
            'username': username,
            'password': password,
            'database': dbname
        }
        _engine = create_engine(URL(**self.DB))
        self.connection = _engine.connect()
I'm getting:
cx_Oracle.DatabaseError: ORA-12170: TNS:Connect timeout occurred
And I'm pretty sure I just need proxy because of my company policy. I couldn't find any tips in documentation how can I create connection via proxy.
",<python><proxy><sqlalchemy>,601,0,12,1271,0,14,35,66,2051,0.0,206,0,13,2018-02-12 13:51,,,,,Basic,3,"<python><proxy><sqlalchemy>, SQLAlchemy connection via proxy, I need to connect to existing database from SQLAlchemy via proxy.
        self.DB = {
            'drivername': 'oracle',
            'host': url,
            'port': port,
            'username': username,
            'password': password,
            'database': dbname
        }
        _engine = create_engine(URL(**self.DB))
        self.connection = _engine.connect()
I'm getting:
cx_Oracle.DatabaseError: ORA-12170: TNS:Connect timeout occurred
And I'm pretty sure I just need proxy because of my company policy. I couldn't find any tips in documentation how can I create connection via proxy.
","<patron><prove><sqlalchemy>, sqlalchemi connect via prove, need connect exist database sqlalchemi via prove. self.do = { 'drivername': 'oracle', 'host': curl, 'port': port, 'surname': surname, 'password': password, 'database': name } begin = create_engine(curl(**self.do)) self.connect = engine.connect() i'm getting: cx_oracle.databaseerror: or-12170: tens:connect timeout occur i'm pretty sure need prove company policy. find tip document great connect via prove."
54845280,How to interpret mysqldump output?,"My intent is to extract the triggers, functions, and stored procedures from a database, edit them, and add them to another database.
Below is a partial output from mysqldump.  I understand how the database is updated with the DROP, CREATE, andINSERT INTO statements, but don't understand the triggers.  I expected the following:
CREATE TRIGGER users_BINS BEFORE INSERT ON users
FOR EACH ROW
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
What does /*!50003 mean?  I thought it was some comment which would mean the CREATE for the trigger isn't present, but I must be misinterpreting the output. 
 How should one interpret a mysqldump output?
mysqldump -u username-ppassword --routines mydb
--
-- Table structure for table `users`
--
DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `users` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `idPublic` int(11) NOT NULL,
  `accountsId` int(11) NOT NULL,
  `firstname` varchar(45) NOT NULL,
  `lastname` varchar(45) NOT NULL,
  `email` varchar(45) NOT NULL,
  `username` varchar(45) NOT NULL,
  `password` char(255) NOT NULL COMMENT 'Password currently uses bcrypt and only requires 60 characters, but may change over time.',
  `tsCreated` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp(),
  `osTicketId` int(11) NOT NULL,
  `phone` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uniqueEmail` (`accountsId`,`email`),
  UNIQUE KEY `uniqueUsername` (`accountsId`,`username`),
  KEY `fk_users_accounts1_idx` (`accountsId`),
  CONSTRAINT `fk_users_accounts1` FOREIGN KEY (`accountsId`) REFERENCES `accounts` (`id`) ON DELETE CASCADE ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=35 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;
--
-- Dumping data for table `users`
--
LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES (xxx
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;
/*!50003 SET @saved_cs_client      = @@character_set_client */ ;
/*!50003 SET @saved_cs_results     = @@character_set_results */ ;
/*!50003 SET @saved_col_connection = @@collation_connection */ ;
/*!50003 SET character_set_client  = utf8 */ ;
/*!50003 SET character_set_results = utf8 */ ;
/*!50003 SET collation_connection  = utf8_general_ci */ ;
/*!50003 SET @saved_sql_mode       = @@sql_mode */ ;
/*!50003 SET sql_mode              = 'STRICT_TRANS_TABLES,STRICT_ALL_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,TRADITIONAL,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' */ ;
DELIMITER ;;
/*!50003 CREATE*/ /*!50017 DEFINER=`michael`@`12.34.56.78`*/ /*!50003 TRIGGER `users_BINS` BEFORE INSERT ON `users` FOR EACH ROW
BEGIN
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
END */;;
DELIMITER ;
/*!50003 SET sql_mode              = @saved_sql_mode */ ;
/*!50003 SET character_set_client  = @saved_cs_client */ ;
/*!50003 SET character_set_results = @saved_cs_results */ ;
/*!50003 SET collation_connection  = @saved_col_connection */ ;
",<mysql><triggers>,3622,0,72,25165,68,221,392,74,1604,0.0,1099,1,13,2019-02-23 19:21,2019-03-05 3:57,2019-03-05 3:57,10.0,10.0,Basic,5,"<mysql><triggers>, How to interpret mysqldump output?, My intent is to extract the triggers, functions, and stored procedures from a database, edit them, and add them to another database.
Below is a partial output from mysqldump.  I understand how the database is updated with the DROP, CREATE, andINSERT INTO statements, but don't understand the triggers.  I expected the following:
CREATE TRIGGER users_BINS BEFORE INSERT ON users
FOR EACH ROW
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
What does /*!50003 mean?  I thought it was some comment which would mean the CREATE for the trigger isn't present, but I must be misinterpreting the output. 
 How should one interpret a mysqldump output?
mysqldump -u username-ppassword --routines mydb
--
-- Table structure for table `users`
--
DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `users` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `idPublic` int(11) NOT NULL,
  `accountsId` int(11) NOT NULL,
  `firstname` varchar(45) NOT NULL,
  `lastname` varchar(45) NOT NULL,
  `email` varchar(45) NOT NULL,
  `username` varchar(45) NOT NULL,
  `password` char(255) NOT NULL COMMENT 'Password currently uses bcrypt and only requires 60 characters, but may change over time.',
  `tsCreated` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp(),
  `osTicketId` int(11) NOT NULL,
  `phone` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uniqueEmail` (`accountsId`,`email`),
  UNIQUE KEY `uniqueUsername` (`accountsId`,`username`),
  KEY `fk_users_accounts1_idx` (`accountsId`),
  CONSTRAINT `fk_users_accounts1` FOREIGN KEY (`accountsId`) REFERENCES `accounts` (`id`) ON DELETE CASCADE ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=35 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;
--
-- Dumping data for table `users`
--
LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES (xxx
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;
/*!50003 SET @saved_cs_client      = @@character_set_client */ ;
/*!50003 SET @saved_cs_results     = @@character_set_results */ ;
/*!50003 SET @saved_col_connection = @@collation_connection */ ;
/*!50003 SET character_set_client  = utf8 */ ;
/*!50003 SET character_set_results = utf8 */ ;
/*!50003 SET collation_connection  = utf8_general_ci */ ;
/*!50003 SET @saved_sql_mode       = @@sql_mode */ ;
/*!50003 SET sql_mode              = 'STRICT_TRANS_TABLES,STRICT_ALL_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,TRADITIONAL,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' */ ;
DELIMITER ;;
/*!50003 CREATE*/ /*!50017 DEFINER=`michael`@`12.34.56.78`*/ /*!50003 TRIGGER `users_BINS` BEFORE INSERT ON `users` FOR EACH ROW
BEGIN
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
END */;;
DELIMITER ;
/*!50003 SET sql_mode              = @saved_sql_mode */ ;
/*!50003 SET character_set_client  = @saved_cs_client */ ;
/*!50003 SET character_set_results = @saved_cs_results */ ;
/*!50003 SET collation_connection  = @saved_col_connection */ ;
","<myself><trigger>, interpret mysqldump output?, intent extract trigger, functions, store procedure database, edit them, add not database. partial output mysqldump. understand database update drop, create, andinsert statements, understand trigger. expect following: great trigger users_bin insert user row if(full(new.public, 0) = 0) insert _inc_account (type, accounts, public) value (""users"",new.accounts,1) public key update idpubl = idpubl + 1; set new.public=(select idpubl _inc_account accounts=new.accounts type=""users""); end if; /*!50003 mean? thought comment would mean great trigger present, must misinterpret output. one interpret mysqldump output? mysqldump -u surname-password --routine my -- -- table structure table `users` -- drop table exist `users`; /*!40101 set @saved_cs_cli = @@character_set_cli */; /*!40101 set character_set_cli = utf */; great table `users` ( `id` in(11) null auto_increment, `public` in(11) null, `accounts` in(11) null, `firstname` varchar(45) null, `lastname` varchar(45) null, `email` varchar(45) null, `surname` varchar(45) null, `password` chair(255) null comment 'password current use crept require 60 characters, may change time.', `treated` timestamp null default current_timestamp() update current_timestamp(), `osticketid` in(11) null, `phone` varchar(45) default null, primary key (`id`), unique key `uniqueemail` (`accounts`,`email`), unique key `uniqueusername` (`accounts`,`surname`), key `fk_users_accounts1_idx` (`accounts`), constraint `fk_users_accounts1` foreign key (`accounts`) refer `accounts` (`id`) delete cascade update action ) engine=innodb auto_increment=35 default charge=utf; /*!40101 set character_set_cli = @saved_cs_cli */; -- -- dump data table `users` -- lock table `users` write; /*!40000 alter table `users` distal key */; insert `users` value (xxx /*!40000 alter table `users` enable key */; clock tables; /*!50003 set @saved_cs_cli = @@character_set_cli */ ; /*!50003 set @saved_cs_result = @@character_set_result */ ; /*!50003 set @saved_col_connect = @@collation_connect */ ; /*!50003 set character_set_cli = utf */ ; /*!50003 set character_set_result = utf */ ; /*!50003 set collation_connect = utf8_general_ci */ ; /*!50003 set @saved_sql_mod = @@sql_mode */ ; /*!50003 set sql_mode = 'strict_trans_tables,strict_all_tables,no_zero_in_date,no_zero_date,allow_invalid_dates,error_for_division_by_zero,traditional,no_auto_create_user,no_engine_substitution' */ ; delight ;; /*!50003 create*/ /*!50017 defined=`michael`@`12.34.56.78`*/ /*!50003 trigger `users_bins` insert `users` row begin if(full(new.public, 0) = 0) insert _inc_account (type, accounts, public) value (""users"",new.accounts,1) public key update idpubl = idpubl + 1; set new.public=(select idpubl _inc_account accounts=new.accounts type=""users""); end if; end */;; delight ; /*!50003 set sql_mode = @saved_sql_mod */ ; /*!50003 set character_set_cli = @saved_cs_cli */ ; /*!50003 set character_set_result = @saved_cs_result */ ; /*!50003 set collation_connect = @saved_col_connect */ ;"
58249954,JSON stringify and PostgreSQL bigint compliance,"I am trying to add BigInt support within my library, and ran into an issue with JSON.stringify.
The nature of the library permits not to worry about type ambiguity and de-serialization, as everything that's serialized goes into the server, and never needs any de-serialization.
I initially came up with the following simplified approach, just to counteract Node.js throwing TypeError: Do not know how to serialize a BigInt at me:
// Does JSON.stringify, with support for BigInt:
function toJson(data) {
    return JSON.stringify(data, (_, v) =&gt; typeof v === 'bigint' ? v.toString() : v);
}
But since it converts each BigInt into a string, each value ends up wrapped into double quotes.
Is there any work-around, perhaps some trick within Node.js formatting utilities, to produce a result from JSON.stringify where each BigInt would be formatted as an open value? This is what PostgreSQL understands and supports, and so I'm looking for a way to generate JSON with BigInt that's compliant with PostgreSQL.
Example
const obj = {
    value: 123n
};
console.log(toJson(obj));
// This is what I'm getting: {""value"":""123""}
// This is what I want: {""value"":123}
Obviously, I cannot just convert BigInt into number, as I would be losing information then. And rewriting the entire JSON.stringify for this probably would be too complicated.
UPDATE
At this point I have reviewed and played with several polyfills, like these ones:
polyfill-1
polyfill-2
But they all seem like an awkward solution, to bring in so much code, and then modify for BigInt support. I am hoping to find something more elegant.
",<node.js><postgresql><stringify><bigint>,1595,3,23,24653,15,116,141,43,15274,,1325,3,13,2019-10-05 15:54,2019-10-06 0:11,2019-10-06 0:11,1.0,1.0,Basic,9,"<node.js><postgresql><stringify><bigint>, JSON stringify and PostgreSQL bigint compliance, I am trying to add BigInt support within my library, and ran into an issue with JSON.stringify.
The nature of the library permits not to worry about type ambiguity and de-serialization, as everything that's serialized goes into the server, and never needs any de-serialization.
I initially came up with the following simplified approach, just to counteract Node.js throwing TypeError: Do not know how to serialize a BigInt at me:
// Does JSON.stringify, with support for BigInt:
function toJson(data) {
    return JSON.stringify(data, (_, v) =&gt; typeof v === 'bigint' ? v.toString() : v);
}
But since it converts each BigInt into a string, each value ends up wrapped into double quotes.
Is there any work-around, perhaps some trick within Node.js formatting utilities, to produce a result from JSON.stringify where each BigInt would be formatted as an open value? This is what PostgreSQL understands and supports, and so I'm looking for a way to generate JSON with BigInt that's compliant with PostgreSQL.
Example
const obj = {
    value: 123n
};
console.log(toJson(obj));
// This is what I'm getting: {""value"":""123""}
// This is what I want: {""value"":123}
Obviously, I cannot just convert BigInt into number, as I would be losing information then. And rewriting the entire JSON.stringify for this probably would be too complicated.
UPDATE
At this point I have reviewed and played with several polyfills, like these ones:
polyfill-1
polyfill-2
But they all seem like an awkward solution, to bring in so much code, and then modify for BigInt support. I am hoping to find something more elegant.
","<node.is><postgresql><stringify><begin>, son stringifi postgresql begin compliance, try add begin support within library, ran issue son.stringify. nature library permit worry type ambigu de-serialization, every that' aerial go server, never need de-serialization. into came follow simplify approach, counteract node.j throw typeerror: know aerial begin me: // son.stringify, support begin: function poison(data) { return son.stringify(data, (_, v) =&it; type v === 'begin' ? v.string() : v); } since convert begin string, value end wrap doubt quotes. work-around, perhaps trick within node.j format utilities, produce result son.stringifi begin would format open value? postgresql understand supports, i'm look way genet son begin that' complaint postgresql. example cost obs = { value: 123n }; console.log(poison(obs)); // i'm getting: {""value"":""123""} // want: {""value"":123} obviously, cannot convert begin number, would lose inform then. regret enter son.stringifi probably would complicated. update point review play never polyfills, like ones: polyfill-1 polyfill-2 seem like awkward solution, bring much code, modify begin support. hope find cometh elegant."
53405317,Postgres changeset with column TEXT not working with Liquibase 3.6.2 and Postgres 9.6,"I am working with the new Spring Boot 2.1.0 version.  In Spring Boot 2.1.0, Liquibase was updated from 3.5.5 to 3.6.2.  I've noticed several things in my change sets are no long working.  
-- test_table.sql
CREATE TABLE test_table (
   id             SERIAL PRIMARY KEY,
   --Works fine as TEXT or VARCHAR with Liquibase 3.5 which is bundled with Spring Boot version 2.0.6.RELEASE
   --Will only work as VARCHAR with Liquibase 3.6.2 which is bundled with Spring Boot version 2.1.0.RELEASE and above
   worksheet_data TEXT
);
-- test_table.csv
id,worksheet_data
1,fff
-- Liquibase Changeset
    &lt;changeSet id=""DATA_01"" author=""me"" runOnChange=""false""&gt;
    &lt;loadData
            file=""${basedir}/sql/data/test_table.csv""
            tableName=""test_table""/&gt;
    &lt;/changeSet&gt;
This will not work.  I am presented with this odd stacktrace.  It complains it can't find liquibase/changelog/fff which I'm not referencing at all in the changeset.  The ""fff"" coincidentally matches the data value in table_test.csv.    
    org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: Invocation of init method failed; nested exception is liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
     Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1745) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:307) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at net.migov.amar.MiAmarApiApplication.main(MiAmarApiApplication.java:33) [classes/:na]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    Caused by: liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
         Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:78) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:202) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:179) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:353) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:305) ~[liquibase-core-3.6.2.jar:na]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        ... 23 common frames omitted
    Caused by: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:191) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.attachParams(ExecutablePreparedStatementBase.java:110) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.BatchDmlExecutablePreparedStatement.attachParams(BatchDmlExecutablePreparedStatement.java:51) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.execute(ExecutablePreparedStatementBase.java:81) ~[liquibase-core-3.6.2.jar:na]
        at liquib
ase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:115) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600) ~[liquibase-core-3.6.2.jar:na]
    ... 31 common frames omitted
Caused by: java.io.FileNotFoundException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:195) ~[spring-core-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at liquibase.integration.spring.SpringLiquibase$SpringResourceOpener.getResourcesAsStream(SpringLiquibase.java:556) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.getResourceAsStream(ExecutablePreparedStatementBase.java:281) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.toCharacterStream(ExecutablePreparedStatementBase.java:241) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:184) ~[liquibase-core-3.6.2.jar:na]
    ... 38 common frames omitted
If I change TEXT to VARCHAR it works.  From my understanding these column types are the same in postgres, so I can work around this.  However, this is frustrating, and I don't see this new behavior documented.  From this link 3.6.2 is advertised as a ""drop in"" change (http://www.liquibase.org/2018/04/liquibase-3-6-0-released.html).
I would like to use the new features of Spring Boot 2.1.0, but I cannot specify liquibase 3.5.5 in my build because Spring Boot will complain about incompatible versions.  This is just one issue I'm seeing with changesets that worked in 3.5.5.  Maybe the folks at Spring should consider rolling back the version of liquibase.
Any advice on this matter would be greatly appreciated.  Thanks.
UPDATED
I have created a sample Spring Boot project to demonstrate this:  https://github.com/pcalouche/postgres-liquibase-text
",<postgresql><liquibase>,9651,4,74,1615,2,17,19,47,5159,0.0,153,1,13,2018-11-21 4:34,2018-12-27 10:26,2018-12-27 10:26,36.0,36.0,Advanced,32,"<postgresql><liquibase>, Postgres changeset with column TEXT not working with Liquibase 3.6.2 and Postgres 9.6, I am working with the new Spring Boot 2.1.0 version.  In Spring Boot 2.1.0, Liquibase was updated from 3.5.5 to 3.6.2.  I've noticed several things in my change sets are no long working.  
-- test_table.sql
CREATE TABLE test_table (
   id             SERIAL PRIMARY KEY,
   --Works fine as TEXT or VARCHAR with Liquibase 3.5 which is bundled with Spring Boot version 2.0.6.RELEASE
   --Will only work as VARCHAR with Liquibase 3.6.2 which is bundled with Spring Boot version 2.1.0.RELEASE and above
   worksheet_data TEXT
);
-- test_table.csv
id,worksheet_data
1,fff
-- Liquibase Changeset
    &lt;changeSet id=""DATA_01"" author=""me"" runOnChange=""false""&gt;
    &lt;loadData
            file=""${basedir}/sql/data/test_table.csv""
            tableName=""test_table""/&gt;
    &lt;/changeSet&gt;
This will not work.  I am presented with this odd stacktrace.  It complains it can't find liquibase/changelog/fff which I'm not referencing at all in the changeset.  The ""fff"" coincidentally matches the data value in table_test.csv.    
    org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: Invocation of init method failed; nested exception is liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
     Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1745) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:307) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at net.migov.amar.MiAmarApiApplication.main(MiAmarApiApplication.java:33) [classes/:na]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    Caused by: liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
         Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:78) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:202) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:179) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:353) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:305) ~[liquibase-core-3.6.2.jar:na]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        ... 23 common frames omitted
    Caused by: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:191) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.attachParams(ExecutablePreparedStatementBase.java:110) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.BatchDmlExecutablePreparedStatement.attachParams(BatchDmlExecutablePreparedStatement.java:51) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.execute(ExecutablePreparedStatementBase.java:81) ~[liquibase-core-3.6.2.jar:na]
        at liquib
ase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:115) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600) ~[liquibase-core-3.6.2.jar:na]
    ... 31 common frames omitted
Caused by: java.io.FileNotFoundException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:195) ~[spring-core-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at liquibase.integration.spring.SpringLiquibase$SpringResourceOpener.getResourcesAsStream(SpringLiquibase.java:556) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.getResourceAsStream(ExecutablePreparedStatementBase.java:281) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.toCharacterStream(ExecutablePreparedStatementBase.java:241) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:184) ~[liquibase-core-3.6.2.jar:na]
    ... 38 common frames omitted
If I change TEXT to VARCHAR it works.  From my understanding these column types are the same in postgres, so I can work around this.  However, this is frustrating, and I don't see this new behavior documented.  From this link 3.6.2 is advertised as a ""drop in"" change (http://www.liquibase.org/2018/04/liquibase-3-6-0-released.html).
I would like to use the new features of Spring Boot 2.1.0, but I cannot specify liquibase 3.5.5 in my build because Spring Boot will complain about incompatible versions.  This is just one issue I'm seeing with changesets that worked in 3.5.5.  Maybe the folks at Spring should consider rolling back the version of liquibase.
Any advice on this matter would be greatly appreciated.  Thanks.
UPDATED
I have created a sample Spring Boot project to demonstrate this:  https://github.com/pcalouche/postgres-liquibase-text
","<postgresql><liquibase>, poster changes column text work liquibas 3.6.2 poster 9.6, work new spring boot 2.1.0 version. spring boot 2.1.0, liquibas update 3.5.5 3.6.2. i'v notice never thing change set long working. -- test_table.sal great table test_tabl ( id aerial primary key, --work fine text varchar liquibas 3.5 bundle spring boot version 2.0.6.release --will work varchar liquibas 3.6.2 bundle spring boot version 2.1.0.release worksheet_data text ); -- test_table.is id,worksheet_data 1,off -- liquibas changes &it;changes id=""data_01"" author=""me"" runonchange=""false""&it; &it;loaddata file=""${based}/sal/data/test_table.is"" tablename=""test_table""/&it; &it;/changes&it; work. present odd stacktrace. complain can't find liquibase/changelog/off i'm reference changes. ""off"" coincident match data value table_test.is. org.springframework.beans.factory.beancreationexception: error great bean name 'liquibase' define class path resource [org/springframework/boot/autoconfigure/liquibase/liquibaseautoconfiguration$liquibaseconfiguration.class]: into knit method failed; nest except liquibase.exception.migrationfailedexception: migrate fail change set liquibase/changelog/data_nonprod.all::data_nonprod_02::scott_winters: reason: liquibase.exception.databaseexception: class path resource [liquibase/changelog/off] cannot resolve curl exist org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1745) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.docreatebean(abstractautowirecapablebeanfactory.cava:576) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.createbean(abstractautowirecapablebeanfactory.cava:498) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractbeanfactory.labia$dogetbean$0(abstractbeanfactory.cava:320) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.defaultsingletonbeanregistry.getsingleton(defaultsingletonbeanregistry.cava:222) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:318) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractbeanfactory.dogetbean(abstractbeanfactory.cava:307) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractbeanfactory.geben(abstractbeanfactory.cava:199) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.context.support.abstractapplicationcontext.geben(abstractapplicationcontext.cava:1083) ~[spring-context-5.1.3.release.jar:5.1.3.release] org.springframework.context.support.abstractapplicationcontext.finishbeanfactoryinitialization(abstractapplicationcontext.cava:853) ~[spring-context-5.1.3.release.jar:5.1.3.release] org.springframework.context.support.abstractapplicationcontext.refresh(abstractapplicationcontext.cava:546) ~[spring-context-5.1.3.release.jar:5.1.3.release] org.springframework.boot.web.serve.context.servletwebserverapplicationcontext.refresh(servletwebserverapplicationcontext.cava:142) ~[spring-boot-2.1.1.release.jar:2.1.1.release] org.springframework.boot.springapplication.refresh(springapplication.cava:775) [spring-boot-2.1.1.release.jar:2.1.1.release] org.springframework.boot.springapplication.refreshcontext(springapplication.cava:397) [spring-boot-2.1.1.release.jar:2.1.1.release] org.springframework.boot.springapplication.run(springapplication.cava:316) [spring-boot-2.1.1.release.jar:2.1.1.release] org.springframework.boot.springapplication.run(springapplication.cava:1260) [spring-boot-2.1.1.release.jar:2.1.1.release] org.springframework.boot.springapplication.run(springapplication.cava:1248) [spring-boot-2.1.1.release.jar:2.1.1.release] net.might.afar.miamarapiapplication.main(miamarapiapplication.cava:33) [classes/:na] sun.reflect.nativemethodaccessorimpl.invoked(n method) ~[na:1.8.0_181] sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.cava:62) ~[na:1.8.0_181] sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.cava:43) ~[na:1.8.0_181] cava.long.reflect.method.invoke(method.cava:498) ~[na:1.8.0_181] org.springframework.boot.devtools.start.restartlauncher.run(restartlauncher.cava:49) [spring-boot-devtools-2.1.1.release.jar:2.1.1.release] cause by: liquibase.exception.migrationfailedexception: migrate fail change set liquibase/changelog/data_nonprod.all::data_nonprod_02::scott_winters: reason: liquibase.exception.databaseexception: class path resource [liquibase/changelog/off] cannot resolve curl exist liquibase.changelog.changes.execute(changes.cava:637) ~[liquibase-core-3.6.2.jar:na] liquibase.changelog.visitor.updatevisitor.visit(updatevisitor.cava:53) ~[liquibase-core-3.6.2.jar:na] liquibase.changelog.changelogiterator.run(changelogiterator.cava:78) ~[liquibase-core-3.6.2.jar:na] liquibase.liquibase.update(liquibase.cava:202) ~[liquibase-core-3.6.2.jar:na] liquibase.liquibase.update(liquibase.cava:179) ~[liquibase-core-3.6.2.jar:na] liquibase.integration.spring.springliquibase.performupdate(springliquibase.cava:353) ~[liquibase-core-3.6.2.jar:na] liquibase.integration.spring.springliquibase.afterpropertiesset(springliquibase.cava:305) ~[liquibase-core-3.6.2.jar:na] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.invokeinitmethods(abstractautowirecapablebeanfactory.cava:1804) ~[spring-beans-5.1.3.release.jar:5.1.3.release] org.springframework.beans.factory.support.abstractautowirecapablebeanfactory.initializebean(abstractautowirecapablebeanfactory.cava:1741) ~[spring-beans-5.1.3.release.jar:5.1.3.release] ... 23 common frame omit cause by: liquibase.exception.databaseexception: class path resource [liquibase/changelog/off] cannot resolve curl exist liquibase.statement.executablepreparedstatementbase.applycolumnparameter(executablepreparedstatementbase.cava:191) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.executablepreparedstatementbase.attachparams(executablepreparedstatementbase.cava:110) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.batchdmlexecutablepreparedstatement.attachparams(batchdmlexecutablepreparedstatement.cava:51) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.executablepreparedstatementbase.execute(executablepreparedstatementbase.cava:81) ~[liquibase-core-3.6.2.jar:na] liquid as.executor.jem.jdbcexecutor.execute(jdbcexecutor.cava:115) ~[liquibase-core-3.6.2.jar:na] liquibase.database.abstractjdbcdatabase.execute(abstractjdbcdatabase.cava:1229) ~[liquibase-core-3.6.2.jar:na] liquibase.database.abstractjdbcdatabase.executestatements(abstractjdbcdatabase.cava:1211) ~[liquibase-core-3.6.2.jar:na] liquibase.changelog.changes.execute(changes.cava:600) ~[liquibase-core-3.6.2.jar:na] ... 31 common frame omit cause by: cava.to.filenotfoundexception: class path resource [liquibase/changelog/off] cannot resolve curl exist org.springframework.core.to.classpathresource.return(classpathresource.cava:195) ~[spring-core-5.1.3.release.jar:5.1.3.release] liquibase.integration.spring.springliquibase$springresourceopener.getresourcesasstream(springliquibase.cava:556) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.executablepreparedstatementbase.getresourceasstream(executablepreparedstatementbase.cava:281) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.executablepreparedstatementbase.tocharacterstream(executablepreparedstatementbase.cava:241) ~[liquibase-core-3.6.2.jar:na] liquibase.statement.executablepreparedstatementbase.applycolumnparameter(executablepreparedstatementbase.cava:184) ~[liquibase-core-3.6.2.jar:na] ... 38 common frame omit change text varchar works. understand column type postures, work around this. however, frustration, see new behavior document. link 3.6.2 advertise ""drop in"" change (http://www.liquibase.org/2018/04/liquibase-3-6-0-released.html). would like use new feature spring boot 2.1.0, cannot specific liquibas 3.5.5 build spring boot complain incompat versions. one issue i'm see changes work 3.5.5. may folk spring consider roll back version liquibase. advice matter would greatly appreciated. thanks. update great sample spring boot project demonstr this: http://github.com/pcalouche/postures-liquibase-text"
52485862,How to use HASHBYTES function in SQL Server for multiple columns,"I have a requirement wherein I have to create hashvalue which consist of all columns of a table. With Checksum this can be done easily, but Checksum is not recommended as per Microsoft: 
  If at least one of the values in the expression list changes, the list checksum will probably change. However, this is not guaranteed. Therefore, to detect whether values have changed, we recommend the use of CHECKSUM only if your application can tolerate an occasional missed change. Otherwise, consider using HashBytes instead. With a specified MD5 hash algorithm, the probability that HashBytes will return the same result, for two different inputs, is much lower compared to CHECKSUM.
HASHBYTES accepts only 2 parameters (algorithm type, column)
Now the problem is even though HASHBYTES is more reliable compared to checksum but there doesn't seem to be an easy way to create it on multiple columns.
An example in the checksum,
create table dbo.chksum_demo1
(
    id int not null,
    name varchar(25),
    address varchar(250),
    HashValue as Checksum (id,name,address)
    CONSTRAINT PK_chksum_demo1 PRIMARY KEY (Id)
)
How can we do the above using Hashbytes instead of checksum? 
",<sql-server>,1178,0,13,813,3,15,32,54,38627,0.0,42,4,13,2018-09-24 19:12,2018-09-24 19:40,,0.0,,Basic,2,"<sql-server>, How to use HASHBYTES function in SQL Server for multiple columns, I have a requirement wherein I have to create hashvalue which consist of all columns of a table. With Checksum this can be done easily, but Checksum is not recommended as per Microsoft: 
  If at least one of the values in the expression list changes, the list checksum will probably change. However, this is not guaranteed. Therefore, to detect whether values have changed, we recommend the use of CHECKSUM only if your application can tolerate an occasional missed change. Otherwise, consider using HashBytes instead. With a specified MD5 hash algorithm, the probability that HashBytes will return the same result, for two different inputs, is much lower compared to CHECKSUM.
HASHBYTES accepts only 2 parameters (algorithm type, column)
Now the problem is even though HASHBYTES is more reliable compared to checksum but there doesn't seem to be an easy way to create it on multiple columns.
An example in the checksum,
create table dbo.chksum_demo1
(
    id int not null,
    name varchar(25),
    address varchar(250),
    HashValue as Checksum (id,name,address)
    CONSTRAINT PK_chksum_demo1 PRIMARY KEY (Id)
)
How can we do the above using Hashbytes instead of checksum? 
","<sal-server>, use hashbyt function sal server multiple columns, require wherein great hashvalu consist column table. checks done easily, checks recommend per microsoft: least one value express list changes, list checks probably change. however, guaranteed. therefore, detect whether value changed, recommend use checks applied tyler occasion miss change. otherwise, consider use hashbyt instead. specific md has algorithm, probably hashbyt return result, two differ input, much lower compare checks. hashbyt accept 2 parapet (algorithm type, column) problem even though hashbyt reliable compare checks seem east way great multiple columns. example checks, great table do.chksum_demo1 ( id in null, name varchar(25), address varchar(250), hashvalu checks (id,name,address) constraint pk_chksum_demo1 primary key (id) ) use hashbyt instead checks?"
48409610,Share a dict with multiple Python scripts,"I'd like a unique dict (key/value) database to be accessible from multiple Python scripts running at the same time.
If script1.py updates d[2839], then script2.py should see the modified value when querying d[2839] a few seconds after.
I thought about using SQLite but it seems that concurrent write/read from multiple processes is not SQLite's strength (let's say script1.py has just modified d[2839], how would script2.py's SQLite connection know it has to reload this specific part of the database?)
I also thought about locking the file when I want to flush the modifications (but it's rather tricky to do), and use json.dump to serialize, then trying to detect the modifications, use json.load to reload if any modification, etc. ... oh no I'm reinventing the wheel, and reinventing a particularly inefficient key/value database!
redis looked like a solution but it does not officially support Windows, the same applies for leveldb.
multiple scripts might want to write at exactly the same time (even if this is a very rare event), is there a way to let the DB system handle this (thanks to a locking parameter? it seems that by default SQLite can't do this because ""SQLite supports an unlimited number of simultaneous readers, but it will only allow one writer at any instant in time."")
What would be a Pythonic solution for this?
Note: I'm on Windows, and the dict should have maximum 1M items (key and value both integers).
",<python><sqlite><dictionary><key-value-store>,1432,5,10,42385,103,394,707,54,3096,0.0,3760,8,13,2018-01-23 19:42,2018-01-23 20:09,2018-01-27 8:19,0.0,4.0,Intermediate,22,"<python><sqlite><dictionary><key-value-store>, Share a dict with multiple Python scripts, I'd like a unique dict (key/value) database to be accessible from multiple Python scripts running at the same time.
If script1.py updates d[2839], then script2.py should see the modified value when querying d[2839] a few seconds after.
I thought about using SQLite but it seems that concurrent write/read from multiple processes is not SQLite's strength (let's say script1.py has just modified d[2839], how would script2.py's SQLite connection know it has to reload this specific part of the database?)
I also thought about locking the file when I want to flush the modifications (but it's rather tricky to do), and use json.dump to serialize, then trying to detect the modifications, use json.load to reload if any modification, etc. ... oh no I'm reinventing the wheel, and reinventing a particularly inefficient key/value database!
redis looked like a solution but it does not officially support Windows, the same applies for leveldb.
multiple scripts might want to write at exactly the same time (even if this is a very rare event), is there a way to let the DB system handle this (thanks to a locking parameter? it seems that by default SQLite can't do this because ""SQLite supports an unlimited number of simultaneous readers, but it will only allow one writer at any instant in time."")
What would be a Pythonic solution for this?
Note: I'm on Windows, and the dict should have maximum 1M items (key and value both integers).
","<patron><quite><dictionary><key-value-store>, share duct multiple patron script, i'd like unique duct (key/value) database access multiple patron script run time. script.i update d[2839], script.i see modify value query d[2839] second after. thought use quite seem concur write/read multiple process quite' strength (let' say script.i modify d[2839], would script.by' quite connect know reload specie part database?) also thought lock file want flush modify (but rather trick do), use son.dump serialize, try detect modifications, use son.load reload modification, etc. ... oh i'm invent wheel, invent particularly ineffici key/value database! red look like slut office support windows, apply level. multiple script might want write exactly time (even rare event), way let do system hand (thank lock parameter? seem default quite can't ""quite support limit number simulate readers, allow one writer instant time."") would patron slut this? note: i'm windows, duct maximum am item (key value integers)."
50310979,How to use SQLAlchemy's `one_or_none` query method?,"I'm trying to use the one_or_none query method to retrieve a record from my database but when I pass in a kwargs like I normally would with the filter_by method, it says it doesn't expect that keyword.
I tried going through the doc, but there's not description of the method's argument or an example.
",<python><sqlalchemy>,301,1,2,1679,6,18,31,50,20017,0.0,12,1,13,2018-05-12 21:54,2018-05-12 22:58,,0.0,,Basic,3,"<python><sqlalchemy>, How to use SQLAlchemy's `one_or_none` query method?, I'm trying to use the one_or_none query method to retrieve a record from my database but when I pass in a kwargs like I normally would with the filter_by method, it says it doesn't expect that keyword.
I tried going through the doc, but there's not description of the method's argument or an example.
","<patron><sqlalchemy>, use sqlalchemy' `one_or_none` query method?, i'm try use one_or_non query method retrieve record database pass war like normal would filter_bi method, say expect eyford. try go do, there' rescript method' argument example."
48777206,DROP TABLE IF EXISTS not working on Azure SQL Data Warehouse,"I used the SQL Server management studio to generate script against Azure Data Warehouse. I selected Edition Azure Data Warehouse, it generates below the script to drop table if it exists and create table. However, the script cannot pass validation. Please see below for the error message.
DROP TABLE IF EXISTS Table1
GO
Error message:  
  Parse error at line: 2, column: 12: Incorrect syntax near 'IF'.
",<azure-sql-database><azure-synapse>,403,0,2,317,1,6,18,77,17584,0.0,24,4,13,2018-02-13 23:12,2018-02-14 9:18,,1.0,,Basic,3,"<azure-sql-database><azure-synapse>, DROP TABLE IF EXISTS not working on Azure SQL Data Warehouse, I used the SQL Server management studio to generate script against Azure Data Warehouse. I selected Edition Azure Data Warehouse, it generates below the script to drop table if it exists and create table. However, the script cannot pass validation. Please see below for the error message.
DROP TABLE IF EXISTS Table1
GO
Error message:  
  Parse error at line: 2, column: 12: Incorrect syntax near 'IF'.
","<azure-sal-database><azure-synapse>, drop table exist work azur sal data warehouse, use sal server manage studio genet script azur data warehouse. select edit azur data warehouse, genet script drop table exist great table. however, script cannot pass variation. pleas see error message. drop table exist table go error message: part error line: 2, column: 12: incorrect santa near 'if'."
53473804,Build sqlproj on Azure DevOps,"I'm trying to use Azure DevOps Pipelines to build my .NET Core 2.1 solution from GitHub.  It includes a SQL project that has a TargetFrameworkVersion of v4.6.2.  This project always fails to build.
Build FAILED.
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj : warning NU1503: Skipping restore for project '/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj'. The project file may be invalid or missing targets required for restore. [/home/vsts/work/1/s/MySolution/MySolution.sln]
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj(57,3): error MSB4019: The imported project ""/usr/share/dotnet/sdk/2.1.403/Microsoft/VisualStudio/v15.0/SSDT/Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
1 Warning(s)
1 Error(s)
How do I reference or include those targets for the build server?  It builds fine in VS2017.  I've spent more than a day hunting and cannot find any information on this problem.
",<build><azure-devops><azure-pipelines><sqlproj>,1039,0,7,12416,9,45,81,36,8624,,1275,3,13,2018-11-26 1:47,2018-11-27 5:24,2018-11-27 5:24,1.0,1.0,Intermediate,22,"<build><azure-devops><azure-pipelines><sqlproj>, Build sqlproj on Azure DevOps, I'm trying to use Azure DevOps Pipelines to build my .NET Core 2.1 solution from GitHub.  It includes a SQL project that has a TargetFrameworkVersion of v4.6.2.  This project always fails to build.
Build FAILED.
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj : warning NU1503: Skipping restore for project '/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj'. The project file may be invalid or missing targets required for restore. [/home/vsts/work/1/s/MySolution/MySolution.sln]
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj(57,3): error MSB4019: The imported project ""/usr/share/dotnet/sdk/2.1.403/Microsoft/VisualStudio/v15.0/SSDT/Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
1 Warning(s)
1 Error(s)
How do I reference or include those targets for the build server?  It builds fine in VS2017.  I've spent more than a day hunting and cannot find any information on this problem.
","<build><azure-drops><azure-pipelines><sqlproj>, build sqlproj azur drops, i'm try use azur deep pipelin build .net core 2.1 slut github. include sal project targetframeworkvers ve.6.2. project away fail build. build failed. /home/vests/work/1/s/solution/database/database.sqlproj : warn nu1503: skin restore project '/home/vests/work/1/s/solution/database/database.sqlproj'. project file may invalid miss target require restore. [/home/vests/work/1/s/solution/solution.son] /home/vests/work/1/s/solution/database/database.sqlproj(57,3): error msb4019: import project ""/us/share/done/sd/2.1.403/microsoft/visualstudio/ve.0/side/microsoft.data.tools.scheme.sqltasks.target"" found. confirm path &it;import&it; declare correct, file exist disk. 1 warning(s) 1 error(s) refer include target build server? build fine vs2017. i'v spent day hunt cannot find inform problem."
55566459,"C# Dynamic Linq: Implement ""Like"" in The Where Clause","So I want to make a general sorter for my data. I have this code to get data from the database which will extract the data only which contains value.
using System.Linq.Dynamic;
public static IQueryable&lt;object&gt; SortList(string searchString, Type modelType, 
    IQueryable&lt;object&gt; model)
{
    ....
    string toStringPredicate = type == typeof(string) ? propertyName + 
        "".Contains(@0)"" : propertyName + "".ToString().Contains(@0)"";
    model = model.Where(propertyName + "" != NULL AND "" + toStringPredicate, value);
}
The model is this:
public class ManageSubscriberItems
{
    public int? UserId { get; set; }
    public string Email { get; set; }
    public Guid SubscriberId { get; set; }
}
When I call:
models = (IQueryable&lt;ManageSubscriberItems&gt;)EcommerceCMS.Helpers.FilterHelper
    .SortList(searchString, typeof(ManageSubscriberItems), models);
if(models.Any())
It throws this error:
  ""LINQ to Entities does not recognize the method 'System.String
  ToString()' method, and this method cannot be translated into a store
  expression.""
EDIT
I found the problem, but I still cannot fix it. So if the property is not string, it will throw an error when calling .ToString().Contains(). 
model = model.Where(propertyName + "" != NULL AND "" + propertyName + 
    "".ToString().Contains(@0)"", value);
What I want is to implement LIKE in the query. Can anyone help me?
",<c#><sql-server><linq><dynamic><dynamic-linq>,1393,0,27,1953,2,23,64,38,6249,0.0,60,4,13,2019-04-08 4:53,2019-04-12 6:36,,4.0,,Basic,2,"<c#><sql-server><linq><dynamic><dynamic-linq>, C# Dynamic Linq: Implement ""Like"" in The Where Clause, So I want to make a general sorter for my data. I have this code to get data from the database which will extract the data only which contains value.
using System.Linq.Dynamic;
public static IQueryable&lt;object&gt; SortList(string searchString, Type modelType, 
    IQueryable&lt;object&gt; model)
{
    ....
    string toStringPredicate = type == typeof(string) ? propertyName + 
        "".Contains(@0)"" : propertyName + "".ToString().Contains(@0)"";
    model = model.Where(propertyName + "" != NULL AND "" + toStringPredicate, value);
}
The model is this:
public class ManageSubscriberItems
{
    public int? UserId { get; set; }
    public string Email { get; set; }
    public Guid SubscriberId { get; set; }
}
When I call:
models = (IQueryable&lt;ManageSubscriberItems&gt;)EcommerceCMS.Helpers.FilterHelper
    .SortList(searchString, typeof(ManageSubscriberItems), models);
if(models.Any())
It throws this error:
  ""LINQ to Entities does not recognize the method 'System.String
  ToString()' method, and this method cannot be translated into a store
  expression.""
EDIT
I found the problem, but I still cannot fix it. So if the property is not string, it will throw an error when calling .ToString().Contains(). 
model = model.Where(propertyName + "" != NULL AND "" + propertyName + 
    "".ToString().Contains(@0)"", value);
What I want is to implement LIKE in the query. Can anyone help me?
","<c#><sal-server><line><dynamic><dynamic-line>, c# dream line: implement ""like"" clause, want make genet sorter data. code get data database extract data contain value. use system.line.dynamic; public static iqueryable&it;object&it; sortlist(sir searchstring, type modeltype, iqueryable&it;object&it; model) { .... string tostringpred = type == type(string) ? propertynam + "".contains(@0)"" : propertynam + "".string().contains(@0)""; model = model.where(propertynam + "" != null "" + tostringpredicate, value); } model this: public class managesubscriberitem { public in? used { get; set; } public string email { get; set; } public guide subscribed { get; set; } } call: model = (iqueryable&it;managesubscriberitems&it;)ecommercecms.helper.filterhelp .sortlist(searchstring, type(managesubscriberitems), models); if(models.any()) throw error: ""line entity record method 'system.sir string()' method, method cannot translate store expression."" edit found problem, still cannot fix it. property string, throw error call .string().contains(). model = model.where(propertynam + "" != null "" + propertynam + "".string().contains(@0)"", value); want implement like query. anyone help me?"
53257270,SQL over clause - dividing partition into numbered sub-partitions,"I have a challenge, that I've come across at multiple occasions but never been able to find an efficient solution to. Imagine I have a large table with data regarding e.g. bank accounts and their possible revolving moves from debit to credit:
AccountId DebitCredit AsOfDate
--------- ----------- ----------
aaa       d           2018-11-01
aaa       d           2018-11-02
aaa       c           2018-11-03
aaa       c           2018-11-04
aaa       c           2018-11-05
bbb       d           2018-11-02
ccc       c           2018-11-01
ccc       d           2018-11-02
ccc       d           2018-11-03
ccc       c           2018-11-04
ccc       d           2018-11-05
ccc       c           2018-11-06
In the example above I would like to assign sub-partition numbers to the combination of AccountId and DebitCredit where the partition number is incremented each time DebitCredit shifts. In other words in the example above I would like this result:
AccountId DebitCredit AsOfDate   PartNo
--------- ----------- ---------- ------
aaa       d           2018-11-01      1
aaa       d           2018-11-02      1
aaa       c           2018-11-03      2
aaa       c           2018-11-04      2
aaa       c           2018-11-05      2
bbb       d           2018-11-02      1
ccc       c           2018-11-01      1
ccc       d           2018-11-02      2
ccc       d           2018-11-03      2
ccc       c           2018-11-04      3
ccc       d           2018-11-05      4
ccc       c           2018-11-06      5
I cannot really figure out how to do it quickly and efficiently. The operation has to be done daily on a tables with millions of rows.
In this example it is guaranteed that we will have consecutive rows for all accounts. However, of course the customer might open an account the 15th in the month and/or close his account the 26th.
The challenge is to be solved on an MSSQL 2016 server, but a solution that would work on 2012 (and maybe even 2008r2) would be nice.
As you can imagine there's no way of telling whether there will only be debit or credit rows or whether the account will be revolving each day.
",<sql><sql-server><t-sql><sql-server-2016><ranking-functions>,2120,0,30,133,0,0,6,56,973,0.0,2,3,13,2018-11-12 7:02,2018-11-12 7:17,2018-11-12 8:17,0.0,0.0,Intermediate,17,"<sql><sql-server><t-sql><sql-server-2016><ranking-functions>, SQL over clause - dividing partition into numbered sub-partitions, I have a challenge, that I've come across at multiple occasions but never been able to find an efficient solution to. Imagine I have a large table with data regarding e.g. bank accounts and their possible revolving moves from debit to credit:
AccountId DebitCredit AsOfDate
--------- ----------- ----------
aaa       d           2018-11-01
aaa       d           2018-11-02
aaa       c           2018-11-03
aaa       c           2018-11-04
aaa       c           2018-11-05
bbb       d           2018-11-02
ccc       c           2018-11-01
ccc       d           2018-11-02
ccc       d           2018-11-03
ccc       c           2018-11-04
ccc       d           2018-11-05
ccc       c           2018-11-06
In the example above I would like to assign sub-partition numbers to the combination of AccountId and DebitCredit where the partition number is incremented each time DebitCredit shifts. In other words in the example above I would like this result:
AccountId DebitCredit AsOfDate   PartNo
--------- ----------- ---------- ------
aaa       d           2018-11-01      1
aaa       d           2018-11-02      1
aaa       c           2018-11-03      2
aaa       c           2018-11-04      2
aaa       c           2018-11-05      2
bbb       d           2018-11-02      1
ccc       c           2018-11-01      1
ccc       d           2018-11-02      2
ccc       d           2018-11-03      2
ccc       c           2018-11-04      3
ccc       d           2018-11-05      4
ccc       c           2018-11-06      5
I cannot really figure out how to do it quickly and efficiently. The operation has to be done daily on a tables with millions of rows.
In this example it is guaranteed that we will have consecutive rows for all accounts. However, of course the customer might open an account the 15th in the month and/or close his account the 26th.
The challenge is to be solved on an MSSQL 2016 server, but a solution that would work on 2012 (and maybe even 2008r2) would be nice.
As you can imagine there's no way of telling whether there will only be debit or credit rows or whether the account will be revolving each day.
","<sal><sal-server><t-sal><sal-server-2016><banking-functions>, sal class - livid partite number sub-partitions, challenge, i'v come across multiple occur never all find effect slut to. imagine large table data regard e.g. bank account possible revolt move debit credit: accounted debitcredit asked --------- ----------- ---------- ana 2018-11-01 ana 2018-11-02 ana c 2018-11-03 ana c 2018-11-04 ana c 2018-11-05 bob 2018-11-02 can c 2018-11-01 can 2018-11-02 can 2018-11-03 can c 2018-11-04 can 2018-11-05 can c 2018-11-06 example would like assign sub-partite number combine accounted debitcredit partite number incitement time debitcredit shifts. word example would like result: accounted debitcredit asked part --------- ----------- ---------- ------ ana 2018-11-01 1 ana 2018-11-02 1 ana c 2018-11-03 2 ana c 2018-11-04 2 ana c 2018-11-05 2 bob 2018-11-02 1 can c 2018-11-01 1 can 2018-11-02 2 can 2018-11-03 2 can c 2018-11-04 3 can 2018-11-05 4 can c 2018-11-06 5 cannot really figure quickly efficiently. over done daily table million rows. example guarantee consent row accounts. however, course custom might open account with month and/or close account with. challenge sole mssql 2016 server, slut would work 2012 (and may even 2008r2) would nice. imagine there' way tell whether debit credit row whether account revolt day."
50073853,Cannot upgrade server earlier than 5.7 to 8.0 in centos. server start fail,"I try to configure Mysql server on centos 7.4. After installing Mysql 8.0 to my system, systemctl restart mysqld failed.
See the error log /var/log/mysqld.log.
  [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.11) starting as process 320
  [ERROR] [MY-013168] [InnoDB] Cannot upgrade server earlier than 5.7 to 8.0
  [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.[ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
  [ERROR] [MY-010119] [Server] Aborting
  [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
",<mysql><centos>,619,0,0,139,0,1,4,78,6343,0.0,0,1,13,2018-04-28 7:02,2019-05-01 22:31,,368.0,,Basic,14,"<mysql><centos>, Cannot upgrade server earlier than 5.7 to 8.0 in centos. server start fail, I try to configure Mysql server on centos 7.4. After installing Mysql 8.0 to my system, systemctl restart mysqld failed.
See the error log /var/log/mysqld.log.
  [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.11) starting as process 320
  [ERROR] [MY-013168] [InnoDB] Cannot upgrade server earlier than 5.7 to 8.0
  [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.[ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
  [ERROR] [MY-010119] [Server] Aborting
  [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
","<myself><cents>, cannot upgrade server earlier 5.7 8.0 cents. server start fail, try configur myself server cent 7.4. instal myself 8.0 system, systemctl start myself failed. see error log /war/log/myself.log. [system] [my-010116] [server] /us/skin/myself (myself 8.0.11) start process 320 [error] [my-013168] [innodb] cannot upgrade server earlier 5.7 8.0 [error] [my-011013] [server] fail into did storage engine.[error] [my-010020] [server] data dictionary into failed. [error] [my-010119] [server] abort [system] [my-010910] [server] /us/skin/myself: shutdown complete (myself 8.0.11) myself common server - gal."
52038200,"Can not persist data model's field into database, but can retrieve it","I have a problem when trying to persist a data model class into a database. I have a class like this: 
class DataModelClass{
    //some more field etc.
    @Column(name = ""number1"", nullable = true)
    private Integer number1;
    @Column(name = ""number2"", nullable = true)
    private Integer number2;
    public DataModelClass(){}
    (...)
    public Integer getNumber2() {
        return number2;
    }
    public void setNumber2( Integer number2 ) {
        this.number2= number2;
    }
}
The second field was added after first one. When to persist object created with this class via:
em.persist(dataModelClass);
A new row in database is created, but only with first field added. The second one is empty. When I am debugging the object dataModelClass has every field set with some integer value.
When I am adding a value for number2 through pgAdmin, and then retrieving this row with java code via:
DataModelClass dmc = em.find(DataModelClass.class, 1);
Than dmc.getNumber2() is not empty/null.
Anyone have any ideas what is wrong?
[Edit]
Maybe it will help a little more, On data model (DataModelClass) class i got this annotation:
@Entity
@Table(name = ""custom_table"",
       uniqueConstraints=@UniqueConstraint(name=""UK_example_foreign_id"", columnNames={""example_foreign_id""})
)
@SequenceGenerator(name = DataModelClass.SEQ_NAME, sequenceName = DataModelClass.SEQ_NAME, allocationSize = 1)
Obviously this field exist in my class
",<java><postgresql><hibernate><jpa>,1438,0,31,2218,4,17,43,64,568,,80,2,13,2018-08-27 11:22,2018-09-04 11:17,2018-09-04 11:39,8.0,8.0,Basic,9,"<java><postgresql><hibernate><jpa>, Can not persist data model's field into database, but can retrieve it, I have a problem when trying to persist a data model class into a database. I have a class like this: 
class DataModelClass{
    //some more field etc.
    @Column(name = ""number1"", nullable = true)
    private Integer number1;
    @Column(name = ""number2"", nullable = true)
    private Integer number2;
    public DataModelClass(){}
    (...)
    public Integer getNumber2() {
        return number2;
    }
    public void setNumber2( Integer number2 ) {
        this.number2= number2;
    }
}
The second field was added after first one. When to persist object created with this class via:
em.persist(dataModelClass);
A new row in database is created, but only with first field added. The second one is empty. When I am debugging the object dataModelClass has every field set with some integer value.
When I am adding a value for number2 through pgAdmin, and then retrieving this row with java code via:
DataModelClass dmc = em.find(DataModelClass.class, 1);
Than dmc.getNumber2() is not empty/null.
Anyone have any ideas what is wrong?
[Edit]
Maybe it will help a little more, On data model (DataModelClass) class i got this annotation:
@Entity
@Table(name = ""custom_table"",
       uniqueConstraints=@UniqueConstraint(name=""UK_example_foreign_id"", columnNames={""example_foreign_id""})
)
@SequenceGenerator(name = DataModelClass.SEQ_NAME, sequenceName = DataModelClass.SEQ_NAME, allocationSize = 1)
Obviously this field exist in my class
","<cava><postgresql><liberate><pa>, persist data model' field database, retrieve it, problem try persist data model class database. class like this: class datamodelclass{ //some field etc. @column(am = ""number"", nullabl = true) privat inter number; @column(am = ""number"", nullabl = true) privat inter number; public datamodelclass(){} (...) public inter getnumber2() { return number; } public void setnumber2( inter number ) { this.number= number; } } second field ad first one. persist object great class via: em.persist(datamodelclass); new row database created, first field added. second one empty. debut object datamodelclass every field set inter value. ad value number pgadmin, retrieve row cava code via: datamodelclass duc = em.find(datamodelclass.class, 1); duc.getnumber2() empty/null. anyone idea wrong? [edit] may help little more, data model (datamodelclass) class got annexation: @entity @table(am = ""custom_table"", uniqueconstraints=@uniqueconstraint(name=""uk_example_foreign_id"", columnnames={""example_foreign_id""}) ) @sequencegenerator(am = datamodelclass.seq_name, sequencenam = datamodelclass.seq_name, allocation = 1) obvious field exist class"
64354878,Convert UTF-8 varbinary(max) to varchar(max),"I have a varbinary(max) column with UTF-8-encoded text that has been compressed. I would like to decompress this data and work with it in T-SQL as a varchar(max) using the UTF-8 capabilities of SQL Server.
I'm looking for a way of specifying the encoding when converting from varbinary(max) to varchar(max). The only way I've managed to do that is by creating a table variable with a column with a UTF-8 collation and inserting the varbinary data into it.
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
SELECT SUBSTRING(Decompressed, 4, DATALENGTH(Decompressed) - 3) WithoutBOM
FROM
    (SELECT DECOMPRESS(RawResource) AS Decompressed FROM Resource) t
I'm wondering if there is a more elegant and efficient approach that does not involve inserting into a table variable.
UPDATE:
Boiling this down to a simple example that doesn't deal with byte order marks or compression:
I have the string &quot;Hello 😊&quot; UTF-8 encoded without a BOM stored in variable @utf8Binary
DECLARE @utf8Binary varbinary(max) = 0x48656C6C6F20F09F988A
Now I try to assign that into various char-based variables and print the result:
DECLARE @brokenVarChar varchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenVarChar = ' + @brokenVarChar
DECLARE @brokenNVarChar nvarchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenNVarChar = ' +  @brokenNVarChar 
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
select @utf8Binary
DECLARE @working nvarchar(max)
Select TOP 1 @working = Res from @rv
print '@working = ' + @working
The results of this are:
@brokenVarChar = Hello ðŸ˜Š
@brokenNVarChar = Hello ðŸ˜Š
@working = Hello 😊
So I am able to get the binary result properly decoded using this indirect method, but I am wondering if there is a more straightforward (and likely efficient) approach.
",<sql-server><t-sql>,1891,0,30,148,0,1,6,55,4092,0.0,5,4,13,2020-10-14 13:54,2020-10-14 16:00,2020-10-14 16:00,0.0,0.0,Basic,9,"<sql-server><t-sql>, Convert UTF-8 varbinary(max) to varchar(max), I have a varbinary(max) column with UTF-8-encoded text that has been compressed. I would like to decompress this data and work with it in T-SQL as a varchar(max) using the UTF-8 capabilities of SQL Server.
I'm looking for a way of specifying the encoding when converting from varbinary(max) to varchar(max). The only way I've managed to do that is by creating a table variable with a column with a UTF-8 collation and inserting the varbinary data into it.
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
SELECT SUBSTRING(Decompressed, 4, DATALENGTH(Decompressed) - 3) WithoutBOM
FROM
    (SELECT DECOMPRESS(RawResource) AS Decompressed FROM Resource) t
I'm wondering if there is a more elegant and efficient approach that does not involve inserting into a table variable.
UPDATE:
Boiling this down to a simple example that doesn't deal with byte order marks or compression:
I have the string &quot;Hello 😊&quot; UTF-8 encoded without a BOM stored in variable @utf8Binary
DECLARE @utf8Binary varbinary(max) = 0x48656C6C6F20F09F988A
Now I try to assign that into various char-based variables and print the result:
DECLARE @brokenVarChar varchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenVarChar = ' + @brokenVarChar
DECLARE @brokenNVarChar nvarchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenNVarChar = ' +  @brokenNVarChar 
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
select @utf8Binary
DECLARE @working nvarchar(max)
Select TOP 1 @working = Res from @rv
print '@working = ' + @working
The results of this are:
@brokenVarChar = Hello ðŸ˜Š
@brokenNVarChar = Hello ðŸ˜Š
@working = Hello 😊
So I am able to get the binary result properly decoded using this indirect method, but I am wondering if there is a more straightforward (and likely efficient) approach.
","<sal-server><t-sal>, convert utf-8 varbinary(max) varchar(max), varbinary(max) column utf-8-end text compressed. would like compress data work t-sal varchar(max) use utf-8 capable sal server. i'm look way specific end convert varbinary(max) varchar(max). way i'v manage great table variable column utf-8 collar insert varbinari data it. declare @re table( re varchar(max) collar latin1_general_100_ci_as_sc_utf8 ) insert @re select subsiding(compressed, 4, datalength(compressed) - 3) withoutbom (select compress(rawresource) compress resource) i'm wonder leg effect approach involve insert table variable. update: boil simple example deal bite order mark compression: string &quit;hello 😊&quit; utf-8 end without boy store variable @utf8binari declare @utf8binari varbinary(max) = 0x48656c6c6f20f09f988a try assign various chair-was variable print result: declare @brokenvarchar varchar(max) = convert(varchar(max), @utf8binary) print '@brokenvarchar = ' + @brokenvarchar declare @brokennvarchar nvarchar(max) = convert(varchar(max), @utf8binary) print '@brokennvarchar = ' + @brokennvarchar declare @re table( re varchar(max) collar latin1_general_100_ci_as_sc_utf8 ) insert @re select @utf8binari declare @work nvarchar(max) select top 1 @work = re @re print '@work = ' + @work result are: @brokenvarchar = hello of˜š @brokennvarchar = hello of˜š @work = hello 😊 all get binary result properly second use indirect method, wonder straightforward (and like efficient) approach."
54946697,psycopg2 - Inserting list of dictionaries into PosgreSQL database. Too many executions?,"I am inserting a list of dictionaries to a PostgreSQL database. The list will be growing quickly and the number of dict values (columns) is around 30. The simplified data:
projects = [
{'name': 'project alpha', 'code': 12, 'active': True},
{'name': 'project beta', 'code': 25, 'active': True},
{'name': 'project charlie', 'code': 46, 'active': False}
]
Inserting the data into the PostgreSQL database with the following code does work (as in this answer), but I am worried about executing too many queries.
for project in projects:
    columns = project.keys()
    values = project.values()
    query = &quot;&quot;&quot;INSERT INTO projects (%s) VALUES %s;&quot;&quot;&quot;
    # print(cursor.mogrify(query, (AsIs(','.join(project.keys())), tuple(project.values()))))
    cursor.execute(query, (AsIs(','.join(columns)), tuple(values)))
conn.commit()
Is there a better practice? Thank you so much in advance for your help!
",<python><postgresql><psycopg2>,924,1,16,2570,3,20,40,53,8627,0.0,1616,3,13,2019-03-01 14:32,2019-03-01 15:13,2019-03-01 17:47,0.0,0.0,Basic,9,"<python><postgresql><psycopg2>, psycopg2 - Inserting list of dictionaries into PosgreSQL database. Too many executions?, I am inserting a list of dictionaries to a PostgreSQL database. The list will be growing quickly and the number of dict values (columns) is around 30. The simplified data:
projects = [
{'name': 'project alpha', 'code': 12, 'active': True},
{'name': 'project beta', 'code': 25, 'active': True},
{'name': 'project charlie', 'code': 46, 'active': False}
]
Inserting the data into the PostgreSQL database with the following code does work (as in this answer), but I am worried about executing too many queries.
for project in projects:
    columns = project.keys()
    values = project.values()
    query = &quot;&quot;&quot;INSERT INTO projects (%s) VALUES %s;&quot;&quot;&quot;
    # print(cursor.mogrify(query, (AsIs(','.join(project.keys())), tuple(project.values()))))
    cursor.execute(query, (AsIs(','.join(columns)), tuple(values)))
conn.commit()
Is there a better practice? Thank you so much in advance for your help!
","<patron><postgresql><psycopg2>, psycopg2 - insert list dictionary posgresql database. man executions?, insert list dictionary postgresql database. list grow quickly number duct value (columns) around 30. simplify data: project = [ {'name': 'project alpha', 'code': 12, 'active': true}, {'name': 'project beta', 'code': 25, 'active': true}, {'name': 'project charlie', 'code': 46, 'active': false} ] insert data postgresql database follow code work (a answer), worry execute man queried. project projects: column = project.keys() value = project.values() query = &quit;&quit;&quit;insert project (%s) value %s;&quit;&quit;&quit; # print(curses.modify(query, (basis(','.join(project.keys())), table(project.values())))) curses.execute(query, (basis(','.join(columns)), table(values))) corn.commit() better practice? thank much advance help!"
50799157,Use a CTE to UPDATE or DELETE in MySQL,"The new version of MySQL, 8.0, now supports Common Table Expressions.
According to the manual:
A WITH clause is permitted at the beginning of SELECT, UPDATE, and DELETE statements:
WITH ... SELECT ...
WITH ... UPDATE ...
WITH ... DELETE ...
So, I thought, given the following table:
ID lastName firstName
----------------------
1  Smith    Pat
2  Smith    Pat
3  Smith    Bob
I can use the following query:
WITH ToDelete AS 
(
   SELECT ID,
          ROW_NUMBER() OVER (PARTITION BY lastName, firstName ORDER BY ID) AS rn
   FROM mytable
)   
DELETE FROM ToDelete;
in order to delete duplicates from the table, just like I could do in SQL Server.
It turns out I was wrong. When I try to execute the DELETE stament from MySQL Workbench I get the error:
Error Code: 1146. Table 'todelete' doesn't exist
I also get an error message when I try to do an UPDATE using the CTE.
So, my question is, how could one use a WITH clause in the context of an UPDATE or DELETE statement in MySQL (as cited in the manual of version 8.0)?
",<mysql><sql-update><common-table-expression><sql-delete><mysql-8.0>,1021,2,20,71571,9,63,98,50,13195,0.0,4808,2,13,2018-06-11 13:41,2018-06-11 13:48,2018-06-11 13:48,0.0,0.0,Basic,9,"<mysql><sql-update><common-table-expression><sql-delete><mysql-8.0>, Use a CTE to UPDATE or DELETE in MySQL, The new version of MySQL, 8.0, now supports Common Table Expressions.
According to the manual:
A WITH clause is permitted at the beginning of SELECT, UPDATE, and DELETE statements:
WITH ... SELECT ...
WITH ... UPDATE ...
WITH ... DELETE ...
So, I thought, given the following table:
ID lastName firstName
----------------------
1  Smith    Pat
2  Smith    Pat
3  Smith    Bob
I can use the following query:
WITH ToDelete AS 
(
   SELECT ID,
          ROW_NUMBER() OVER (PARTITION BY lastName, firstName ORDER BY ID) AS rn
   FROM mytable
)   
DELETE FROM ToDelete;
in order to delete duplicates from the table, just like I could do in SQL Server.
It turns out I was wrong. When I try to execute the DELETE stament from MySQL Workbench I get the error:
Error Code: 1146. Table 'todelete' doesn't exist
I also get an error message when I try to do an UPDATE using the CTE.
So, my question is, how could one use a WITH clause in the context of an UPDATE or DELETE statement in MySQL (as cited in the manual of version 8.0)?
","<myself><sal-update><common-table-expression><sal-delete><myself-8.0>, use ate update delete myself, new version myself, 8.0, support common table expressions. accord manual: class permit begin select, update, delete statements: ... select ... ... update ... ... delete ... so, thought, given follow table: id lastnam firstnam ---------------------- 1 smith pat 2 smith pat 3 smith bob use follow query: toilet ( select id, row_number() (partite lastname, firstnam order id) in metal ) delete delete; order delete public table, like could sal server. turn wrong. try execute delete statement myself workbench get error: error code: 1146. table 'delete' exist also get error message try update use ate. so, question is, could one use class context update delete statement myself (a cite manual version 8.0)?"
52375300,Count(*) vs Count(id) speed,"I know they return different results (the first counts nulls, the latter not). That's not my question. Just imagine a case where I don't care (either because there are no nulls, or because there are only a few and I only want a general sense of the number of rows in the database).
My question is about the following (presumable) contradiction:
Here  one of the highest rep users in the SQL tag says 
  Your use of COUNT(*) or COUNT(column) should be based on the
  desired output only.
On the other hand, here is a 47 times upvoted comment saying 
  ... if you have a non-nullable column such as ID, then count(ID) will
  significantly improve performance over count(*).
The two seem to contradict each other. So can someone please explain to me why is whatever the correct one correct?
",<sql><sql-server>,788,2,2,26768,38,141,295,50,14150,0.0,2952,2,13,2018-09-17 20:33,2018-09-17 20:41,,0.0,,Intermediate,23,"<sql><sql-server>, Count(*) vs Count(id) speed, I know they return different results (the first counts nulls, the latter not). That's not my question. Just imagine a case where I don't care (either because there are no nulls, or because there are only a few and I only want a general sense of the number of rows in the database).
My question is about the following (presumable) contradiction:
Here  one of the highest rep users in the SQL tag says 
  Your use of COUNT(*) or COUNT(column) should be based on the
  desired output only.
On the other hand, here is a 47 times upvoted comment saying 
  ... if you have a non-nullable column such as ID, then count(ID) will
  significantly improve performance over count(*).
The two seem to contradict each other. So can someone please explain to me why is whatever the correct one correct?
","<sal><sal-server>, count(*) vs count(id) speed, know return differ result (the first count null, latter not). that' question. imagine case care (either null, want genet sens number row database). question follow (presumably) contradiction: one highest rep user sal tag say use count(*) count(column) base desire output only. hand, 47 time upon comment say ... non-null column id, count(id) significantly improve perform count(*). two seem contradict other. someone pleas explain what correct one correct?"
59283754,STRING_AGG with line break,"DROP TABLE IF EXISTS items;
CREATE TABLE items (item varchar(20));
INSERT INTO items VALUES ('apple'),('raspberry');
SELECT STRING_AGG(item, CHAR(13)) AS item_list FROM items;
How do I get a line break between items ?
",<sql><sql-server><t-sql><string-aggregation>,218,1,4,9734,4,17,28,50,16293,,245,4,13,2019-12-11 10:33,2019-12-11 11:02,2019-12-11 11:40,0.0,0.0,Basic,2,"<sql><sql-server><t-sql><string-aggregation>, STRING_AGG with line break, DROP TABLE IF EXISTS items;
CREATE TABLE items (item varchar(20));
INSERT INTO items VALUES ('apple'),('raspberry');
SELECT STRING_AGG(item, CHAR(13)) AS item_list FROM items;
How do I get a line break between items ?
","<sal><sal-server><t-sal><string-aggregation>, string_agg line break, drop table exist items; great table item (item varchar(20)); insert item value ('apple'),('raspberry'); select string_agg(item, chair(13)) item_list items; get line break item ?"
61576670,Databases in psql Don't Show up in PgAdmin4,"I installed Postgres 
and followed the instruction. I create a database and logged in by the master password but I don't find the database even the + mark is not shown in the servers. can anyone help, please?
",<postgresql><pgadmin>,209,1,0,151,1,2,8,62,15936,0.0,3,3,13,2020-05-03 15:04,2020-06-28 5:22,,56.0,,Basic,14,"<postgresql><pgadmin>, Databases in psql Don't Show up in PgAdmin4, I installed Postgres 
and followed the instruction. I create a database and logged in by the master password but I don't find the database even the + mark is not shown in the servers. can anyone help, please?
","<postgresql><pgadmin>, database pool show pgadmin4, instal poster follow instruction. great database log master password find database even + mark shown serves. anyone help, please?"
52302676,"Hibernate entity query for finding the most recent, semi-unique row, in a single table","I have a Hibernate database with a single table that looks like:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     4            Tape        09-10-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
     7           Pencil       09-08-2018         Allen           Supplies
And I want to return only the newest purchases, based on some other limitations. For example:
List&lt;Purchase&gt; getNewestPurchasesFor(Array&lt;String&gt; productNames, Array&lt;String&gt; purchaserNames) { ... }
Could be called using:
List&lt;Purchase&gt; purchases = getNewestPurchasesFor([""Notebook"", ""Pencil""], [""Bob"", ""Steve""]);
In English, ""Give me the newest purchases, for either a Notebook or Pencil, by either Bob or Steve.""
And would provide:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME
-----------------------------------------------------------
     1          Notebook      09-07-2018          Bob            
     3           Pencil       09-06-2018          Bob            
     5           Pencil       09-09-2018         Steve           
So it's like a ""distinct"" lookup on multiple columns, or a ""limit"" based on some post-sorted combined-column unique key, but all the examples I've found show using the SELECT DISTINCT(PRODUCT_NAME, PURCHASER_NAME) to obtain those columns only, whereas I need to use the format:
from Purchases as entity where ... 
So that the model types are returned with relationships intact.
Currently, my query returns me all of the old purchases as well:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
Which, for repeat purchases, causes quite the performance drop.
Are there any special keywords I should be using to accomplish this? Query languages and SQL-fu are not my strong suits.
Edit:
Note that I'm currently using the Criteria API, and would like to continue doing so.
Criteria criteria = session.createCriteria(Purchase.class);
criteria.addOrder(Order.desc(""purchaseDate""));
// Product names
Criterion purchaseNameCriterion = Restrictions.or(productNames.stream().map(name -&gt; Restrictions.eq(""productName"", name)).toArray(Criterion[]::new));
// Purchaser
Criterion purchaserCriterion = Restrictions.or(purchaserNames.stream().map(name -&gt; Restrictions.eq(""purchaser"", name)).toArray(Criterion[]::new));
// Bundle the two together
criteria.add(Restrictions.and(purchaseNameCriterion, purchaserCriterion));
criteria.list(); // Gives the above results
If I try to use a distinct Projection, I get an error:
ProjectionList projections = Projections.projectionList();
projections.add(Projections.property(""productName""));
projections.add(Projections.property(""purchaser""));
criteria.setProjection(Projections.distinct(projections));
Results in:
17:08:39 ERROR Order by expression ""THIS_.PURCHASE_DATE"" must be in the result list in this case; SQL statement:
Because, as mentioned above, adding a projection/distinct column set seems to indicate to Hibernate that I want those columns as a result/return value, when what I want is to simply limit the returned model objects based on unique column values.
",<java><sql><hibernate>,3998,0,41,31434,32,138,234,47,1029,0.0,899,10,13,2018-09-12 20:06,2018-09-12 20:56,,0.0,,Advanced,32,"<java><sql><hibernate>, Hibernate entity query for finding the most recent, semi-unique row, in a single table, I have a Hibernate database with a single table that looks like:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     4            Tape        09-10-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
     7           Pencil       09-08-2018         Allen           Supplies
And I want to return only the newest purchases, based on some other limitations. For example:
List&lt;Purchase&gt; getNewestPurchasesFor(Array&lt;String&gt; productNames, Array&lt;String&gt; purchaserNames) { ... }
Could be called using:
List&lt;Purchase&gt; purchases = getNewestPurchasesFor([""Notebook"", ""Pencil""], [""Bob"", ""Steve""]);
In English, ""Give me the newest purchases, for either a Notebook or Pencil, by either Bob or Steve.""
And would provide:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME
-----------------------------------------------------------
     1          Notebook      09-07-2018          Bob            
     3           Pencil       09-06-2018          Bob            
     5           Pencil       09-09-2018         Steve           
So it's like a ""distinct"" lookup on multiple columns, or a ""limit"" based on some post-sorted combined-column unique key, but all the examples I've found show using the SELECT DISTINCT(PRODUCT_NAME, PURCHASER_NAME) to obtain those columns only, whereas I need to use the format:
from Purchases as entity where ... 
So that the model types are returned with relationships intact.
Currently, my query returns me all of the old purchases as well:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
Which, for repeat purchases, causes quite the performance drop.
Are there any special keywords I should be using to accomplish this? Query languages and SQL-fu are not my strong suits.
Edit:
Note that I'm currently using the Criteria API, and would like to continue doing so.
Criteria criteria = session.createCriteria(Purchase.class);
criteria.addOrder(Order.desc(""purchaseDate""));
// Product names
Criterion purchaseNameCriterion = Restrictions.or(productNames.stream().map(name -&gt; Restrictions.eq(""productName"", name)).toArray(Criterion[]::new));
// Purchaser
Criterion purchaserCriterion = Restrictions.or(purchaserNames.stream().map(name -&gt; Restrictions.eq(""purchaser"", name)).toArray(Criterion[]::new));
// Bundle the two together
criteria.add(Restrictions.and(purchaseNameCriterion, purchaserCriterion));
criteria.list(); // Gives the above results
If I try to use a distinct Projection, I get an error:
ProjectionList projections = Projections.projectionList();
projections.add(Projections.property(""productName""));
projections.add(Projections.property(""purchaser""));
criteria.setProjection(Projections.distinct(projections));
Results in:
17:08:39 ERROR Order by expression ""THIS_.PURCHASE_DATE"" must be in the result list in this case; SQL statement:
Because, as mentioned above, adding a projection/distinct column set seems to indicate to Hibernate that I want those columns as a result/return value, when what I want is to simply limit the returned model objects based on unique column values.
","<cava><sal><liberate>, wiberd entity query find recent, semi-unique row, single table, wiberd database single table look like: purchased | product_nam | purchased | purchaser_nam | product_categori ------------------------------------------------------------------------------ 1 notebook 09-07-2018 bob supply 2 notebook 09-06-2018 bob supply 3 pencil 09-06-2018 bob supply 4 tape 09-10-2018 bob supply 5 pencil 09-09-2018 steve supply 6 pencil 09-06-2018 steve supply 7 pencil 09-08-2018 allen supply want return newest purchases, base limitations. example: list&it;purchase&it; getnewestpurchasesfor(array&it;string&it; productnames, array&it;string&it; purchasernames) { ... } could call using: list&it;purchase&it; purchase = getnewestpurchasesfor([""notebook"", ""pencil""], [""bob"", ""steve""]); english, ""give newest purchases, either notebook pencil, either bob steve."" would provide: purchased | product_nam | purchased | purchaser_nam ----------------------------------------------------------- 1 notebook 09-07-2018 bob 3 pencil 09-06-2018 bob 5 pencil 09-09-2018 steve like ""distinct"" lockup multiple columns, ""limit"" base post-sort combined-column unique key, example i'v found show use select distinct(product_name, purchaser_name) obtain column only, where need use format: purchase entity ... model type return relationship intact. currently, query return old purchase well: purchased | product_nam | purchased | purchaser_nam | product_categori ------------------------------------------------------------------------------ 1 notebook 09-07-2018 bob supply 2 notebook 09-06-2018 bob supply 3 pencil 09-06-2018 bob supply 5 pencil 09-09-2018 steve supply 6 pencil 09-06-2018 steve supply which, repeat purchases, cause quit perform drop. special eyford use accomplish this? query language sal-ff strong suits. edit: note i'm current use criterion apt, would like continue so. criterion criterion = session.createcriteria(purchase.class); criterion.adorer(order.desk(""purchasedate"")); // product name criterion purchasenamecriterion = restrictions.or(productnames.stream().map(am -&it; restrictions.e(""productname"", name)).array(criterion[]::new)); // purchase criterion purchasercriterion = restrictions.or(purchasernames.stream().map(am -&it; restrictions.e(""purchaser"", name)).array(criterion[]::new)); // bundle two together criterion.add(restrictions.and(purchasenamecriterion, purchasercriterion)); criterion.list(); // give result try use distinct projection, get error: protectionist project = projections.protectionist(); projections.add(projections.property(""productname"")); projections.add(projections.property(""purchaser"")); criterion.setprojection(projections.distinct(projections)); result in: 17:08:39 error order express ""this.purchase_date"" must result list case; sal statement: because, mention above, ad projection/distinct column set seem india wiberd want column result/return value, want simple limit return model object base unique column values."
54479941,How to init MySql Database in Docker Compose,"Scenario:
I developed a microservice in Spring which uses a mysql 8 database.
This db has to be initalized (create a Database, some tables and data).
On my host machine I initialized the database with data.sql and schema.sql script.
The Problem is, that I have to set:
spring.datasource.initialization-mode=always
for the first execute. This initializes my db the way I want to.
For future runs I have to comment this command. Very ugly soltion but I could not find a better one and I got no reponse right now to this question.
I thought for testing it is ok but I definetly have to improve that.
Currently I want to run my service with docker by a docker compose.
Expected:
This is the docker-compose file. Fairly simple. I'm totally new in the world of docker and so I want to go on step by step.
version: '3' 
services:usermanagement-service:
build:
  ./UserManagementService
restart:
  on-failure
ports:
  - ""7778:7778""
links:
  - mysqldb
depends_on:
  - mysqldb   mysqldb:
build:
  ./CustomMySql
volumes:
  - ./mysql-data:/var/lib/mysql
restart:
  on-failure
environment:
  MYSQL_ROOT_PASSWORD: root
  MYSQL_DATABASE: userdb
  MYSQL_USER: testuser
  MYSQL_PASSWORD: testuser
expose:
  - ""3600""
I was expecting, that my database gets initialized with a user and that in the first run my microservice initializes the db with data.
So before the next start of compose I have to comment the command and rebuild the image. (I know , ugly)
Problem:
So besides this ugly solution I run into runtime problems. 
On docker-compose up my Microservice is faster than the init of the database. So it tries to call the db what results in en error.
Because of the restart on failure the microservice comes up again.
Now it works because the init of the db has finished.
Solution:
I searched the www and for it seems like a known problem which might be solved within a wait-for-it.sh file. This has to be included with COPY in the Dockerfile.
So I'm no expert but I am searching for a good solution to either:
init database from within spring und make my service wait till mysql is ready
or init the database from withn my container via a volume on the first run and of course solve this init problem.
I don't know what is best practice here I and I would be very thankful for some help how to build this up.
",<mysql><spring-boot><docker><docker-compose><init>,2298,1,29,2102,3,30,53,57,21435,0.0,490,1,13,2019-02-01 12:52,2019-02-01 13:38,2019-02-01 13:38,0.0,0.0,Advanced,32,"<mysql><spring-boot><docker><docker-compose><init>, How to init MySql Database in Docker Compose, Scenario:
I developed a microservice in Spring which uses a mysql 8 database.
This db has to be initalized (create a Database, some tables and data).
On my host machine I initialized the database with data.sql and schema.sql script.
The Problem is, that I have to set:
spring.datasource.initialization-mode=always
for the first execute. This initializes my db the way I want to.
For future runs I have to comment this command. Very ugly soltion but I could not find a better one and I got no reponse right now to this question.
I thought for testing it is ok but I definetly have to improve that.
Currently I want to run my service with docker by a docker compose.
Expected:
This is the docker-compose file. Fairly simple. I'm totally new in the world of docker and so I want to go on step by step.
version: '3' 
services:usermanagement-service:
build:
  ./UserManagementService
restart:
  on-failure
ports:
  - ""7778:7778""
links:
  - mysqldb
depends_on:
  - mysqldb   mysqldb:
build:
  ./CustomMySql
volumes:
  - ./mysql-data:/var/lib/mysql
restart:
  on-failure
environment:
  MYSQL_ROOT_PASSWORD: root
  MYSQL_DATABASE: userdb
  MYSQL_USER: testuser
  MYSQL_PASSWORD: testuser
expose:
  - ""3600""
I was expecting, that my database gets initialized with a user and that in the first run my microservice initializes the db with data.
So before the next start of compose I have to comment the command and rebuild the image. (I know , ugly)
Problem:
So besides this ugly solution I run into runtime problems. 
On docker-compose up my Microservice is faster than the init of the database. So it tries to call the db what results in en error.
Because of the restart on failure the microservice comes up again.
Now it works because the init of the db has finished.
Solution:
I searched the www and for it seems like a known problem which might be solved within a wait-for-it.sh file. This has to be included with COPY in the Dockerfile.
So I'm no expert but I am searching for a good solution to either:
init database from within spring und make my service wait till mysql is ready
or init the database from withn my container via a volume on the first run and of course solve this init problem.
I don't know what is best practice here I and I would be very thankful for some help how to build this up.
","<myself><spring-boot><doctor><doctor-compose><knit>, knit myself database doctor compose, scenario: develop microservic spring use myself 8 database. do knit (great database, table data). host machine into database data.sal scheme.sal script. problem is, set: spring.datasource.initialization-mode=away first execute. into do way want to. future run comment command. ugly solution could find better one got report right question. thought test ok definetli improve that. current want run service doctor doctor compose. expected: doctor-compose file. fairly simple. i'm total new world doctor want go step step. version: '3' services:usermanagement-service: build: ./usermanagementservic start: on-failure ports: - ""7778:7778"" links: - mysqldb depends_on: - mysqldb mysqldb: build: ./custommysql volumes: - ./myself-data:/war/limb/myself start: on-failure environment: mysql_root_password: root mysql_database: used mysql_user: tests mysql_password: tests expose: - ""3600"" expecting, database get into user first run microservic into do data. next start compose comment command rebuild image. (i know , ugly) problem: beside ugly slut run until problems. doctor-compose microservic faster knit database. try call do result en error. start failure microservic come again. work knit do finished. solution: search www seem like known problem might sole within wait-for-it.s file. include copy dockerfile. i'm expert search good slut either: knit database within spring und make service wait till myself ready knit database with contain via volume first run course sole knit problem. know best practice would thank help build up."
48382457,MYSQL JSON column change array order after saving,"I am using JSON column type in MySQL database table. When I try to save JSON values in table column, the JSON array automatically re-order(shuffle) 
I have following JSON:
{""TIMER_HEADER"": [{""XX!TIMERHDR"": ""XXTIMERHDR"", ""VER"": "" 7"", ""REL"": "" 0"", ""COMPANYNAME"": ""XXX"", ""IMPORTEDBEFORE"": ""N"", ""FROMTIMER"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""XX!HDR"": ""XXHDR"", ""PROD"": ""Qics for Wnows"", ""VER"": ""Version 6.0"", ""REL"": ""Release R"", ""IIFVER"": ""1"", ""DATE"": ""2018-01-20"", ""TIME"": ""1516520267"",   ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
After saving in JSON-column of MySql table, this becomes:
{""TIMER_HEADER"": [{""REL"": "" 0"", ""VER"": "" 7"", ""FROMTIMER"": ""N"", ""COMPANYNAME"": ""XXX"", ""XX!TIMERHDR"": ""XXTIMERHDR"", ""IMPORTEDBEFORE"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""REL"": ""Release R"", ""VER"": ""Version 6.0"", ""DATE"": ""2018-01-20"", ""PROD"": ""Qics for Wnows"", ""TIME"": ""1516520267"", ""IIFVER"": ""1"", ""XX!HDR"": ""XXHDR"", ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
I need the same order as I have original, because there is an validation on 3rd party.
Please help. Thanks.
",<mysql><json><mysql-json>,1292,0,2,1211,0,19,32,70,7443,0.0,45,2,13,2018-01-22 13:15,2018-01-22 13:31,,0.0,,Basic,9,"<mysql><json><mysql-json>, MYSQL JSON column change array order after saving, I am using JSON column type in MySQL database table. When I try to save JSON values in table column, the JSON array automatically re-order(shuffle) 
I have following JSON:
{""TIMER_HEADER"": [{""XX!TIMERHDR"": ""XXTIMERHDR"", ""VER"": "" 7"", ""REL"": "" 0"", ""COMPANYNAME"": ""XXX"", ""IMPORTEDBEFORE"": ""N"", ""FROMTIMER"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""XX!HDR"": ""XXHDR"", ""PROD"": ""Qics for Wnows"", ""VER"": ""Version 6.0"", ""REL"": ""Release R"", ""IIFVER"": ""1"", ""DATE"": ""2018-01-20"", ""TIME"": ""1516520267"",   ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
After saving in JSON-column of MySql table, this becomes:
{""TIMER_HEADER"": [{""REL"": "" 0"", ""VER"": "" 7"", ""FROMTIMER"": ""N"", ""COMPANYNAME"": ""XXX"", ""XX!TIMERHDR"": ""XXTIMERHDR"", ""IMPORTEDBEFORE"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""REL"": ""Release R"", ""VER"": ""Version 6.0"", ""DATE"": ""2018-01-20"", ""PROD"": ""Qics for Wnows"", ""TIME"": ""1516520267"", ""IIFVER"": ""1"", ""XX!HDR"": ""XXHDR"", ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
I need the same order as I have original, because there is an validation on 3rd party.
Please help. Thanks.
","<myself><son><myself-son>, myself son column change array order saving, use son column type myself database table. try save son value table column, son array automatic re-order(scuffle) follow son: {""timer_header"": [{""xx!timerhdr"": ""xxtimerhdr"", ""ver"": "" 7"", ""red"": "" 0"", ""companyname"": ""xxx"", ""importedbefore"": ""n"", ""frontier"": ""n"", ""companycreatetime"": ""12423426""}, {""xx!her"": ""xxhdr"", ""proud"": ""tic knows"", ""ver"": ""version 6.0"", ""red"": ""release r"", ""silver"": ""1"", ""date"": ""2018-01-20"", ""time"": ""1516520267"", ""account"": ""n"", ""accntntsplittime"": ""0""}], ""column_header"": [""!impact"", ""date"", ""job"", ""hemp"", ""item"", ""item"", ""duration"", ""pro"", ""note"", ""billingstatus""]} save son-column myself table, becomes: {""timer_header"": [{""red"": "" 0"", ""ver"": "" 7"", ""frontier"": ""n"", ""companyname"": ""xxx"", ""xx!timerhdr"": ""xxtimerhdr"", ""importedbefore"": ""n"", ""companycreatetime"": ""12423426""}, {""red"": ""release r"", ""ver"": ""version 6.0"", ""date"": ""2018-01-20"", ""proud"": ""tic knows"", ""time"": ""1516520267"", ""silver"": ""1"", ""xx!her"": ""xxhdr"", ""account"": ""n"", ""accntntsplittime"": ""0""}], ""column_header"": [""!impact"", ""date"", ""job"", ""hemp"", ""item"", ""item"", ""duration"", ""pro"", ""note"", ""billingstatus""]} need order original, valid rd party. pleas help. thanks."
57165310,Create a DATETIME column in SQLite FLutter database?,"I create a TABLE with some columns and everything's ok until I tried to insert a new DATETIME column:
_onCreate(Database db, int version) async {
await db.
execute(""CREATE TABLE $TABLE ($ID INTEGER PRIMARY KEY, $NAME TEXT, $COLOUR 
TEXT, $BREED TEXT, $BIRTH DATETIME)"");
}
My Model Class to store is:
class Pet {
 int id;
 String name;
 DateTime birth;
 String breed;
 String colour;
 Pet({this.id, this.name, this.birth, this.colour, this.breed});
 }
In the controller, I tried to store a new instance of Pet, instantiating a new variable DateTime _date = new DateTime.now(); and saving all in
Pet newPet = Pet(
      id: null,
      name: _name,
      colour: _colour,
      breed: _breed,
      birth: _date
  );
But when I insert in the database I receive:
  Unhandled Exception: Invalid argument: Instance of 'DateTime'
",<sqlite><datetime><flutter><instance>,825,0,24,523,3,7,18,54,25011,0.0,22,1,13,2019-07-23 13:30,2019-07-23 13:33,2019-07-23 13:33,0.0,0.0,Basic,3,"<sqlite><datetime><flutter><instance>, Create a DATETIME column in SQLite FLutter database?, I create a TABLE with some columns and everything's ok until I tried to insert a new DATETIME column:
_onCreate(Database db, int version) async {
await db.
execute(""CREATE TABLE $TABLE ($ID INTEGER PRIMARY KEY, $NAME TEXT, $COLOUR 
TEXT, $BREED TEXT, $BIRTH DATETIME)"");
}
My Model Class to store is:
class Pet {
 int id;
 String name;
 DateTime birth;
 String breed;
 String colour;
 Pet({this.id, this.name, this.birth, this.colour, this.breed});
 }
In the controller, I tried to store a new instance of Pet, instantiating a new variable DateTime _date = new DateTime.now(); and saving all in
Pet newPet = Pet(
      id: null,
      name: _name,
      colour: _colour,
      breed: _breed,
      birth: _date
  );
But when I insert in the database I receive:
  Unhandled Exception: Invalid argument: Instance of 'DateTime'
","<quite><daytime><flutter><instance>, great datetim column quite flutter database?, great table column everything' ok try insert new datetim column: concrete(database do, in version) async { await do. execute(""or table $table ($id inter primary key, $name text, $colour text, $breed text, $birth daytime)""); } model class store is: class pet { in id; string name; datetim birth; string breed; string colour; pet({this.id, this.name, this.birth, this.colour, this.breed}); } controller, try store new instant pet, instant new variable datetim date = new daytime.now(); save pet newport = pet( id: null, name: name, colour: colour, breed: breed, birth: date ); insert database receive: unhandl exception: invalid argument: instant 'daytime'"
61144337,How to get insert id after save to database in CodeIgniter 4,"I'm using Codeigniter 4. 
And inserting new data like this,
$data = [
        'username' =&gt; 'darth',
        'email'    =&gt; 'd.vader@theempire.com'
];
$userModel-&gt;save($data);
Which is mentioned here: CodeIgniter’s Model reference
It's doing the insertion. 
But I haven't found any reference about to get the inserted id after insertion.
Please help! Thanks in advance. 
",<php><mysql><codeigniter-4>,379,1,6,636,1,10,22,47,49589,0.0,42,10,13,2020-04-10 16:19,2020-04-11 9:32,2020-04-11 9:32,1.0,1.0,Basic,3,"<php><mysql><codeigniter-4>, How to get insert id after save to database in CodeIgniter 4, I'm using Codeigniter 4. 
And inserting new data like this,
$data = [
        'username' =&gt; 'darth',
        'email'    =&gt; 'd.vader@theempire.com'
];
$userModel-&gt;save($data);
Which is mentioned here: CodeIgniter’s Model reference
It's doing the insertion. 
But I haven't found any reference about to get the inserted id after insertion.
Please help! Thanks in advance. 
","<pp><myself><codeigniter-4>, get insert id save database codeignit 4, i'm use codeignit 4. insert new data like this, $data = [ 'surname' =&it; 'earth', 'email' =&it; 'd.made@theempire.com' ]; $usermodel-&it;save($data); mention here: codeigniter’ model refer insertion. found refer get insert id insertion. pleas help! thank advance."
51619861,Sql left join on left key with null values,"I have a question about join with key with null value. 
Suppose I have a table t, which is going to be on left side. (id is primary key and sub_id is the key to join with the right table.)
    id sub_id value
    1  3       23
    2  3       234
    3  2       245
    4  1       12
    5  null    948
    6  2       45
    7  null    12
and I have another table m which is on right side. (t.sub_id = m.id)
    id feature
    1  9       
    2  8       
    3  2       
    4  1       
    5  4    
    6  2       
    7  null
Now I want to use 
    select * from t left join m on t.sub_id = m.id   
What result will it return? Is Null value in left key influence the result? I want all null left key rows not to shown in my result.
Thank you!  
",<sql><null><left-join>,746,0,24,2707,5,18,30,79,85700,0.0,34,4,13,2018-07-31 18:27,2018-07-31 18:30,2018-07-31 18:30,0.0,0.0,Basic,10,"<sql><null><left-join>, Sql left join on left key with null values, I have a question about join with key with null value. 
Suppose I have a table t, which is going to be on left side. (id is primary key and sub_id is the key to join with the right table.)
    id sub_id value
    1  3       23
    2  3       234
    3  2       245
    4  1       12
    5  null    948
    6  2       45
    7  null    12
and I have another table m which is on right side. (t.sub_id = m.id)
    id feature
    1  9       
    2  8       
    3  2       
    4  1       
    5  4    
    6  2       
    7  null
Now I want to use 
    select * from t left join m on t.sub_id = m.id   
What result will it return? Is Null value in left key influence the result? I want all null left key rows not to shown in my result.
Thank you!  
","<sal><null><left-join>, sal left join left key null values, question join key null value. suppose table t, go left side. (id primary key submit key join right table.) id submit value 1 3 23 2 3 234 3 2 245 4 1 12 5 null 948 6 2 45 7 null 12 not table right side. (t.submit = m.id) id feature 1 9 2 8 3 2 4 1 5 4 6 2 7 null want use select * left join t.submit = m.id result return? null value left key influence result? want null left key row shown result. thank you!"
60421158,Would it be possible to have multiple database connection pools in rails to switch between?,"A little background
I have been using the Apartment gem for running a multi-tenancy app for years. Now recently the need to scale the database out into separate hosts has arrived, the db server simply can't keep up any more (both reads and writes are getting too too much) - and yes, I scaled the hardware to the max (dedicated hardware, 64 cores, 12 Nvm-e drives in raid 10, 384Gb ram etc.). 
I was considering doing this per-tenant (1 tenant = 1 database connection config / pool) as that would be a ""simple"" and efficient way to get up to number-of-tenants-times more capacity without doing loads of application code changes. 
Now, I am running rails 4.2 atm., soon upgrading to 5.2. I can see that rails 6 adds support for a per-model connection definitions, however that is not really what I need, as I have a completely mirrored database schema for each of my 20 tenants. Typically I switch ""database"" per request (in middleware) or per background job (sidekiq middleware), however this is currently trivial and handled ny the Apartment gem, as it just sets the search_path in Postgresql and does not really change the actual connection. When switching to a per-tenant hosting strategy I will need to switch the entire connection per request.
Questions:
I understand that I could do an ActiveRecord::Base.establish_connection(config) per request / background job - however, as I also understand, that triggers an entirely new database connection handshake to be made and a new db pool to spawn in rails - right? I guess that would be a performance suicide to make that kind of overhead on every single request to my application.
I am therefore wondering if anyone can see the option with rails of e.g. pre-establishing multiple (total of 20) database connections/pools from the beginning (e.g. on boot of the application), and then just switch between those pools per request? So that he db connections are already made and ready to be used.
Is all this just a poor poor idea, and should I instead look for a different approach? E.g. 1 app instance = one specific connection to one specific tenant. Or something else. 
",<ruby-on-rails><ruby><postgresql><multi-tenant>,2125,1,3,8613,11,59,118,44,2129,0.0,228,3,13,2020-02-26 19:28,2020-02-27 16:52,2020-03-03 5:06,1.0,6.0,Intermediate,21,"<ruby-on-rails><ruby><postgresql><multi-tenant>, Would it be possible to have multiple database connection pools in rails to switch between?, A little background
I have been using the Apartment gem for running a multi-tenancy app for years. Now recently the need to scale the database out into separate hosts has arrived, the db server simply can't keep up any more (both reads and writes are getting too too much) - and yes, I scaled the hardware to the max (dedicated hardware, 64 cores, 12 Nvm-e drives in raid 10, 384Gb ram etc.). 
I was considering doing this per-tenant (1 tenant = 1 database connection config / pool) as that would be a ""simple"" and efficient way to get up to number-of-tenants-times more capacity without doing loads of application code changes. 
Now, I am running rails 4.2 atm., soon upgrading to 5.2. I can see that rails 6 adds support for a per-model connection definitions, however that is not really what I need, as I have a completely mirrored database schema for each of my 20 tenants. Typically I switch ""database"" per request (in middleware) or per background job (sidekiq middleware), however this is currently trivial and handled ny the Apartment gem, as it just sets the search_path in Postgresql and does not really change the actual connection. When switching to a per-tenant hosting strategy I will need to switch the entire connection per request.
Questions:
I understand that I could do an ActiveRecord::Base.establish_connection(config) per request / background job - however, as I also understand, that triggers an entirely new database connection handshake to be made and a new db pool to spawn in rails - right? I guess that would be a performance suicide to make that kind of overhead on every single request to my application.
I am therefore wondering if anyone can see the option with rails of e.g. pre-establishing multiple (total of 20) database connections/pools from the beginning (e.g. on boot of the application), and then just switch between those pools per request? So that he db connections are already made and ready to be used.
Is all this just a poor poor idea, and should I instead look for a different approach? E.g. 1 app instance = one specific connection to one specific tenant. Or something else. 
","<ruby-on-rails><ruby><postgresql><multi-tenant>, would possible multiple database connect pool rail switch between?, little background use apart gem run multi-ten pp years. recent need scale database spear host arrived, do server simple can't keep (both read write get much) - yes, scale hardware max (devil hardware, 64 comes, 12 nom-e drive raid 10, 384gb ram etc.). consider per-ten (1 tenant = 1 database connect confirm / pool) would ""simple"" effect way get number-of-tenants-tim cap without load applied code changes. now, run rail 4.2 at., soon upgrade 5.2. see rail 6 add support per-model connect definitions, howe really need, complete mirror database scheme 20 tenants. topic switch ""database"" per request (in middleware) per background job (sidekiq middleware), howe current trivial hand ny apart gem, set search_path postgresql really change actual connection. switch per-ten host strategic need switch enter connect per request. questions: understand could activerecord::base.establish_connection(confirm) per request / background job - however, also understand, trigger enter new database connect handshak made new do pool spain rail - right? guess would perform suicide make kind overhead every single request application. therefore wonder anyone see option rail e.g. pre-establish multiple (total 20) database connections/pool begin (e.g. boot application), switch pool per request? do connect already made ready used. poor poor idea, instead look differ approach? e.g. 1 pp instant = one specie connect one specie tenant. cometh else."
53226642,SQLite3 database is Locked in Azure,"I have a Flask server Running on Azure provided by Azure App services with sqlite3 as a database. I am unable to update sqlite3 as it is showing that database is locked 
    2018-11-09T13:21:53.854367947Z [2018-11-09 13:21:53,835] ERROR in app: Exception on /borrow [POST]
    2018-11-09T13:21:53.854407246Z Traceback (most recent call last):
    2018-11-09T13:21:53.854413046Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 2292, in wsgi_app
    2018-11-09T13:21:53.854417846Z     response = self.full_dispatch_request()
    2018-11-09T13:21:53.854422246Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    2018-11-09T13:21:53.854427146Z     rv = self.handle_user_exception(e)
    2018-11-09T13:21:53.854431646Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1718, in handle_user_exception
    2018-11-09T13:21:53.854436146Z     reraise(exc_type, exc_value, tb)
    2018-11-09T13:21:53.854440346Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/_compat.py"", line 35, in reraise
    2018-11-09T13:21:53.854444746Z     raise value
    2018-11-09T13:21:53.854448846Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    2018-11-09T13:21:53.854453246Z     rv = self.dispatch_request()
    2018-11-09T13:21:53.854457546Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1799, in dispatch_request
    2018-11-09T13:21:53.854461846Z     return self.view_functions[rule.endpoint](**req.view_args)
    2018-11-09T13:21:53.854466046Z   File ""/home/site/wwwroot/application.py"", line 282, in borrow
    2018-11-09T13:21:53.854480146Z     cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    2018-11-09T13:21:53.854963942Z sqlite3.OperationalError: database is locked
Here is the route - 
@app.route('/borrow',methods=[""POST""])
def borrow():
    # import pdb; pdb.set_trace()
    body = request.get_json()
    user_id = body[""userid""]
    bookid = body[""bookid""]
    conn = sqlite3.connect(""database.db"")
    cursor = conn.cursor()
    date = datetime.now()
    expiry_date = date + timedelta(days=30)
    cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    # conn.commit()
    cursor.execute(""INSERT INTO borrowed (issuedate,returndate,memberid,bookid) VALUES (?,?,?,?)"",(""xxx"",""xxx"",user_id,bookid,))
    conn.commit()
    cursor.close()
    conn.close()
    return json.dumps({""status"":200,""conn"":""working with datess update""})
I tried checking the database integrity using pragma. There was no integrity loss. So I don't know what might be causing that error. Any help is Appreciated :)
",<sql><azure><sqlite>,2796,0,35,363,0,2,12,67,6387,0.0,146,6,13,2018-11-09 13:29,2019-01-05 10:20,,57.0,,Basic,9,"<sql><azure><sqlite>, SQLite3 database is Locked in Azure, I have a Flask server Running on Azure provided by Azure App services with sqlite3 as a database. I am unable to update sqlite3 as it is showing that database is locked 
    2018-11-09T13:21:53.854367947Z [2018-11-09 13:21:53,835] ERROR in app: Exception on /borrow [POST]
    2018-11-09T13:21:53.854407246Z Traceback (most recent call last):
    2018-11-09T13:21:53.854413046Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 2292, in wsgi_app
    2018-11-09T13:21:53.854417846Z     response = self.full_dispatch_request()
    2018-11-09T13:21:53.854422246Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    2018-11-09T13:21:53.854427146Z     rv = self.handle_user_exception(e)
    2018-11-09T13:21:53.854431646Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1718, in handle_user_exception
    2018-11-09T13:21:53.854436146Z     reraise(exc_type, exc_value, tb)
    2018-11-09T13:21:53.854440346Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/_compat.py"", line 35, in reraise
    2018-11-09T13:21:53.854444746Z     raise value
    2018-11-09T13:21:53.854448846Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    2018-11-09T13:21:53.854453246Z     rv = self.dispatch_request()
    2018-11-09T13:21:53.854457546Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1799, in dispatch_request
    2018-11-09T13:21:53.854461846Z     return self.view_functions[rule.endpoint](**req.view_args)
    2018-11-09T13:21:53.854466046Z   File ""/home/site/wwwroot/application.py"", line 282, in borrow
    2018-11-09T13:21:53.854480146Z     cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    2018-11-09T13:21:53.854963942Z sqlite3.OperationalError: database is locked
Here is the route - 
@app.route('/borrow',methods=[""POST""])
def borrow():
    # import pdb; pdb.set_trace()
    body = request.get_json()
    user_id = body[""userid""]
    bookid = body[""bookid""]
    conn = sqlite3.connect(""database.db"")
    cursor = conn.cursor()
    date = datetime.now()
    expiry_date = date + timedelta(days=30)
    cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    # conn.commit()
    cursor.execute(""INSERT INTO borrowed (issuedate,returndate,memberid,bookid) VALUES (?,?,?,?)"",(""xxx"",""xxx"",user_id,bookid,))
    conn.commit()
    cursor.close()
    conn.close()
    return json.dumps({""status"":200,""conn"":""working with datess update""})
I tried checking the database integrity using pragma. There was no integrity loss. So I don't know what might be causing that error. Any help is Appreciated :)
","<sal><azure><quite>, sqlite3 database lock azure, flask server run azur proved azur pp service sqlite3 database. unable update sqlite3 show database lock 2018-11-09t13:21:53.854367947z [2018-11-09 13:21:53,835] error pp: except /borrow [post] 2018-11-09t13:21:53.854407246z traceback (most recent call last): 2018-11-09t13:21:53.854413046z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/pp.by"", line 2292, wsgi_app 2018-11-09t13:21:53.854417846z response = self.full_dispatch_request() 2018-11-09t13:21:53.854422246z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/pp.by"", line 1815, full_dispatch_request 2018-11-09t13:21:53.854427146z re = self.handle_user_exception(e) 2018-11-09t13:21:53.854431646z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/pp.by"", line 1718, handle_user_except 2018-11-09t13:21:53.854436146z raise(exc_type, exc_value, to) 2018-11-09t13:21:53.854440346z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/compact.by"", line 35, remain 2018-11-09t13:21:53.854444746z rays value 2018-11-09t13:21:53.854448846z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/pp.by"", line 1813, full_dispatch_request 2018-11-09t13:21:53.854453246z re = self.dispatch_request() 2018-11-09t13:21:53.854457546z file ""/home/site/wwwroot/attend/limb/python3.7/site-packages/flask/pp.by"", line 1799, dispatch_request 2018-11-09t13:21:53.854461846z return self.view_functions[rule.endpoint](**red.view_args) 2018-11-09t13:21:53.854466046z file ""/home/site/wwwroot/application.by"", line 282, borrow 2018-11-09t13:21:53.854480146z curses.execute(""up book set stock = stock - 1 booked = ?"",(booked,)) 2018-11-09t13:21:53.854963942z sqlite3.operationalerror: database lock rout - @pp.route('/borrow',methods=[""post""]) def borrow(): # import pub; pub.set_trace() body = request.get_json() user_id = body[""used""] booked = body[""booked""] corn = sqlite3.connect(""database.do"") curses = corn.curses() date = daytime.now() expired = date + timedelta(days=30) curses.execute(""up book set stock = stock - 1 booked = ?"",(booked,)) # corn.commit() curses.execute(""insert borrow (issuedate,returndate,members,booked) value (?,?,?,?)"",(""xxx"",""xxx"",user_id,booked,)) corn.commit() curses.close() corn.close() return son.dumps({""status"":200,""corn"":""work dates update""}) try check database inter use trauma. inter loss. know might cause error. help appreci :)"
53783736,What is the difference between Azure SQL Database Elastic Pools and Azure SQL Database Managed Instance?,"Azure SQL Database has two similar flavors - Managed Instance and Elastic pools. Both flavors enables placing multiple databases that share the same resources and in both cases can be changed cpu/storage for entire group of database within the instance/pool. What is the difference between them?
",<azure><azure-sql-database><azure-sql-managed-instance>,296,0,0,13452,4,40,56,55,19771,0.0,21,1,13,2018-12-14 16:41,2018-12-14 16:41,2018-12-14 16:41,0.0,0.0,Basic,8,"<azure><azure-sql-database><azure-sql-managed-instance>, What is the difference between Azure SQL Database Elastic Pools and Azure SQL Database Managed Instance?, Azure SQL Database has two similar flavors - Managed Instance and Elastic pools. Both flavors enables placing multiple databases that share the same resources and in both cases can be changed cpu/storage for entire group of database within the instance/pool. What is the difference between them?
","<azure><azure-sal-database><azure-sal-managed-instance>, differ azur sal database last pool azur sal database manage instance?, azur sal database two similar flavor - manage instant last pools. flavor enable place multiple database share resource case change cup/storage enter group database within instance/pool. differ them?"
63256680,Adding an array of integers as a data type in a Gorm Model,"I am trying to save an array of numbers in a single postgresql field using Gorm.
The array needs to be a list with between 2 &amp; 13 numbers: [1, 2, 3, 5, 8, 13, 21, 40, 1000]
Everything was working when saving a single int64. When I tried changing the model to account for an array of int64's it gives me the following error:
&quot;panic: invalid sql type  (slice) for postgres&quot;
my Gorm model is:
type Game struct {
    gorm.Model
    GameCode    string
    GameName    string
    DeckType    []int64
    GameEndDate string
}
Update based on answer from @pacuna. I tried the suggested code and I get a similar error.
&quot;panic: invalid sql type Int64Array (slice) for postgres&quot;
Here is the full code block:
package main
import (
    &quot;fmt&quot;
    &quot;github.com/jinzhu/gorm&quot;
    _ &quot;github.com/jinzhu/gorm/dialects/postgres&quot;
    pq &quot;github.com/lib/pq&quot;
)
var db *gorm.DB
// Test -- Model for Game table
type Test struct {
    gorm.Model                                           
    GameCode    string                                      
    GameName    string                                      
    DeckType    pq.Int64Array    
    GameEndDate string   
}
func main() {
    db, err := gorm.Open(&quot;postgres&quot;, &quot;host=localhost port=5432 user=fullstack dbname=scratch_game sslmode=disable&quot;)
    if err != nil {
        fmt.Println(err.Error())
        panic(&quot;Failed to connect to database...&quot;)
    }
    defer db.Close()
    dt := []int64{1, 2, 3}
    db.AutoMigrate(&amp;Test{})
    fmt.Println(&quot;Table Created&quot;)
    db.Create(&amp;Test{GameCode: &quot;xxx&quot;, GameName: &quot;xxx&quot;, DeckType: pq.Int64Array(dt), GameEndDate: &quot;xxx&quot;})
    fmt.Println(&quot;Record Added&quot;)
}
",<postgresql><go><go-gorm>,1783,0,47,192,1,1,12,54,21620,0.0,5,1,13,2020-08-05 0:12,2020-08-05 1:04,2020-08-05 1:04,0.0,0.0,Basic,8,"<postgresql><go><go-gorm>, Adding an array of integers as a data type in a Gorm Model, I am trying to save an array of numbers in a single postgresql field using Gorm.
The array needs to be a list with between 2 &amp; 13 numbers: [1, 2, 3, 5, 8, 13, 21, 40, 1000]
Everything was working when saving a single int64. When I tried changing the model to account for an array of int64's it gives me the following error:
&quot;panic: invalid sql type  (slice) for postgres&quot;
my Gorm model is:
type Game struct {
    gorm.Model
    GameCode    string
    GameName    string
    DeckType    []int64
    GameEndDate string
}
Update based on answer from @pacuna. I tried the suggested code and I get a similar error.
&quot;panic: invalid sql type Int64Array (slice) for postgres&quot;
Here is the full code block:
package main
import (
    &quot;fmt&quot;
    &quot;github.com/jinzhu/gorm&quot;
    _ &quot;github.com/jinzhu/gorm/dialects/postgres&quot;
    pq &quot;github.com/lib/pq&quot;
)
var db *gorm.DB
// Test -- Model for Game table
type Test struct {
    gorm.Model                                           
    GameCode    string                                      
    GameName    string                                      
    DeckType    pq.Int64Array    
    GameEndDate string   
}
func main() {
    db, err := gorm.Open(&quot;postgres&quot;, &quot;host=localhost port=5432 user=fullstack dbname=scratch_game sslmode=disable&quot;)
    if err != nil {
        fmt.Println(err.Error())
        panic(&quot;Failed to connect to database...&quot;)
    }
    defer db.Close()
    dt := []int64{1, 2, 3}
    db.AutoMigrate(&amp;Test{})
    fmt.Println(&quot;Table Created&quot;)
    db.Create(&amp;Test{GameCode: &quot;xxx&quot;, GameName: &quot;xxx&quot;, DeckType: pq.Int64Array(dt), GameEndDate: &quot;xxx&quot;})
    fmt.Println(&quot;Record Added&quot;)
}
","<postgresql><go><go-form>, ad array inter data type form model, try save array number single postgresql field use form. array need list 2 &amp; 13 numbers: [1, 2, 3, 5, 8, 13, 21, 40, 1000] every work save single into. try change model account array into' give follow error: &quit;panic: invalid sal type (slice) postures&quit; form model is: type game struck { form.model gamecod string gamenam string desktop []into gameendd string } update base answer @pacing. try suggest code get similar error. &quit;panic: invalid sal type int64array (slice) postures&quit; full code block: package main import ( &quit;fat&quit; &quit;github.com/jinzhu/form&quit; _ &quit;github.com/jinzhu/form/dialect/postures&quit; pp &quit;github.com/limb/pp&quit; ) war do *form.do // test -- model game table type test struck { form.model gamecod string gamenam string desktop pp.int64array gameendd string } fun main() { do, err := form.open(&quit;postures&quit;, &quit;host=localhost port=5432 user=fullstack name=scratch_gam sslmode=disabled&quit;) err != nail { fat.print(err.error()) panic(&quit;fail connect database...&quit;) } defer do.close() it := []into{1, 2, 3} do.automigrate(&amp;test{}) fat.print(&quit;t created&quit;) do.create(&amp;test{gamecode: &quit;xxx&quit;, gamename: &quit;xxx&quit;, decktype: pp.int64array(it), gameenddate: &quit;xxx&quit;}) fat.print(&quit;record added&quit;) }"
48243934,Mocking Postgresql now() function for testing,"I have the following stack
Node/Express backend
Postgresql 10 database
Mocha for testing
Sinon for mocking
I have written a bunch of end-to-end tests to test all my webservices. The problem is that some of them are time dependent (as in ""give me the modified records of the last X seconds""). 
sinon is pretty good at mocking all the time/dated related stuff in Node, however I have a modified field in my Postgresql tables that is populated with a trigger:
CREATE FUNCTION update_modified_column()
  RETURNS TRIGGER AS $$
BEGIN
  NEW.modified = now();
  RETURN NEW;
END;
$$ LANGUAGE 'plpgsql';
The problem of course is that sinon can't override that now() function.
Any idea on how I could solve this? The problem is not setting a specific date at the start of the test, but advancing time faster than real-time (in one of my tests I want to change some stuff in the database, advance the 'current time' with one day, change some more stuff in the database and do webservice calls to see the result).
I can figure out a few solutions myself, but they all involve changing the application code and making it less elegant. I don't think your application code should be impacted by the fact that you want to test it.
",<node.js><postgresql><mocha.js><sinon>,1214,0,11,6044,6,43,70,72,4163,,692,4,13,2018-01-13 20:18,2018-01-13 22:01,,0.0,,Basic,10,"<node.js><postgresql><mocha.js><sinon>, Mocking Postgresql now() function for testing, I have the following stack
Node/Express backend
Postgresql 10 database
Mocha for testing
Sinon for mocking
I have written a bunch of end-to-end tests to test all my webservices. The problem is that some of them are time dependent (as in ""give me the modified records of the last X seconds""). 
sinon is pretty good at mocking all the time/dated related stuff in Node, however I have a modified field in my Postgresql tables that is populated with a trigger:
CREATE FUNCTION update_modified_column()
  RETURNS TRIGGER AS $$
BEGIN
  NEW.modified = now();
  RETURN NEW;
END;
$$ LANGUAGE 'plpgsql';
The problem of course is that sinon can't override that now() function.
Any idea on how I could solve this? The problem is not setting a specific date at the start of the test, but advancing time faster than real-time (in one of my tests I want to change some stuff in the database, advance the 'current time' with one day, change some more stuff in the database and do webservice calls to see the result).
I can figure out a few solutions myself, but they all involve changing the application code and making it less elegant. I don't think your application code should be impacted by the fact that you want to test it.
","<node.is><postgresql><much.is><simon>, mock postgresql now() function testing, follow stick node/express backed postgresql 10 database much test simon mock written bunch end-to-end test test webservices. problem time depend (a ""give modify record last x seconds""). simon pretty good mock time/d relate stuff node, howe modify field postgresql table soul trigger: great function update_modified_column() return trigger $$ begin new.modify = now(); return new; end; $$ language 'plpgsql'; problem course simon can't overdid now() function. idea could sole this? problem set specie date start test, advance time faster real-tim (in one test want change stuff database, advance 'current time' one day, change stuff database webservic call see result). figure slut myself, involve change applied code make less elegant. think applied code impact fact want test it."
53295322,PostgreSQL statement timeout,"PostgreSQL Version: 9.3
We have online system which gets transnational data (approximately 15000 records per day).  
We have table partitioning on date &amp; time and have a PostgreSQL function to load the incoming request into the table.
Sometimes we see the error message
  ERROR: 57014: canceling statement due to statement timeout
The client sends the request again after some time if not successful and on second try it gets recorded successfully.  It seems this has to be something with table locks but I am not sure.
",<postgresql>,524,0,0,131,1,1,5,76,38728,0.0,0,2,13,2018-11-14 7:50,2018-11-14 11:33,,0.0,,Basic,9,"<postgresql>, PostgreSQL statement timeout, PostgreSQL Version: 9.3
We have online system which gets transnational data (approximately 15000 records per day).  
We have table partitioning on date &amp; time and have a PostgreSQL function to load the incoming request into the table.
Sometimes we see the error message
  ERROR: 57014: canceling statement due to statement timeout
The client sends the request again after some time if not successful and on second try it gets recorded successfully.  It seems this has to be something with table locks but I am not sure.
","<postgresql>, postgresql statement timeout, postgresql version: 9.3 online system get transmit data (approxim 15000 record per day). table partite date &amp; time postgresql function load income request table. sometime see error message error: 57014: cancel statement due statement timeout client send request time success second try get record successfully. seem cometh table lock sure."
52066085,System.Linq.Expressions exception thrown when using FirstOrDefault in .Net Core 2.1,"I am receiving ~300+ exceptions that are spammed in my server output labeled:
Exception thrown: 'System.ArgumentException' in System.Linq.Expressions.dll
The query I am using is as follows:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
Eventually the exceptions stop generating, it shows a large query in the output window, and everything continues as normal.
If I change the query to the following I do not experience the exception:
IQueryable&lt;Account&gt; account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     Where(a =&gt; a.FacebookUserID == facebookUserID);
However, if I call anything such as First, FirstOrDefault, Single, etc on the IQueryable&lt;Account&gt; variable the exceptions start up again and then stop after ~300.
These exceptions are stalling user logins upwards of 30 seconds or more. The duration of exceptions grows with the amount of data being returned from the database.
I use the Account object by passing it around the server to perform varying maintenance tasks on it and then eventually sending the object client-side where I have it being deserialized into the client-side version of the Account class.
Does anyone know what could be causing these internal exceptions and how I might be able to eliminate or mitigate them? 
Here is my output log:
Below is the exception message:
The AccountStatistics isn't listed in the query above because there are about 20 some includes and I shorthanded the include list for brevity.
Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountStatistics]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountStatistics]'
There is no inner exception.
I double checked my database and I have an entry for the user and all of their fields are filled with valid data.
Account Class (Edited for brevity)
public class Account
    {
        [Key]
        public int ID { get; set; }
        public DateTime CreationDate { get; set; }
        public AccountCurrency Currency { get; set; }
        public AccountProgression Progression { get; set; }
        public AccountSettings Settings { get; set; }
        public AccountStatistics Statistics { get; set; }
        public ICollection&lt;AccountFriendEntry&gt; Friends { get; set; }
        public ICollection&lt;AccountUnlockedGameEntry&gt; Unlocks{ get; set; }
    }
Account Statistics class
public class AccountStatistics
{
    [Key]
    public int AccountID { get; set; }
    public int LoginCount { get; set; }
    public DateTime LastLoginTime { get; set; }
    public DateTime LastActivityTime { get; set; }
}
Edit
Keys for the Account Statistics table
   migrationBuilder.CreateTable(
            name: ""AccountStatistics"",
            columns: table =&gt; new
            {
                AccountID = table.Column&lt;int&gt;(nullable: false),
                LoginCount = table.Column&lt;int&gt;(nullable: false),
                LastLoginTime = table.Column&lt;DateTime&gt;(nullable: false),
                CreationDate = table.Column&lt;DateTime&gt;(nullable: false)
            },
            constraints: table =&gt;
            {
                table.PrimaryKey(""PK_AccountStatistics"", x =&gt; x.AccountID);
                table.ForeignKey(
                    name: ""FK_AccountStatistics_Accounts_AccountID"",
                    column: x =&gt; x.AccountID,
                    principalTable: ""Accounts"",
                    principalColumn: ""ID"",
                    onDelete: ReferentialAction.Cascade);
            });
Edit 9001
After doing some testing I've realized the exception only occurs when chaining includes.
This will cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
This will NOT cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
It does not matter if its currency and unlock, friends and currency, settings, and statistics. Any combination of includes (2 or more) causes the exception to happen.
Edit 9002
Here are my results of the following query:
var acct = _accountContext.Account
     .Where(a =&gt; a.FacebookUserID == facebookUserID)
     .Select(x =&gt; new { Account = x, x.Currency, x.Settings }).ToList();
Exception:
System.ArgumentException: 'Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountSettings]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountSettings]''
I feel like this is treating AccountSettings as a collection when it's a single field reference.
Edit Final:
I never found a fix for this issue. I re-created all the tables and such in another environment and it works just fine. Not a very ideal solution to blow away all tables, classes, and migrations, but it's the only thing that fixed the issue.
",<c#><sql-server><entity-framework>,5685,2,74,301,0,2,10,60,5118,0.0,0,4,13,2018-08-28 20:56,2018-11-01 10:31,,65.0,,Basic,10,"<c#><sql-server><entity-framework>, System.Linq.Expressions exception thrown when using FirstOrDefault in .Net Core 2.1, I am receiving ~300+ exceptions that are spammed in my server output labeled:
Exception thrown: 'System.ArgumentException' in System.Linq.Expressions.dll
The query I am using is as follows:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
Eventually the exceptions stop generating, it shows a large query in the output window, and everything continues as normal.
If I change the query to the following I do not experience the exception:
IQueryable&lt;Account&gt; account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     Where(a =&gt; a.FacebookUserID == facebookUserID);
However, if I call anything such as First, FirstOrDefault, Single, etc on the IQueryable&lt;Account&gt; variable the exceptions start up again and then stop after ~300.
These exceptions are stalling user logins upwards of 30 seconds or more. The duration of exceptions grows with the amount of data being returned from the database.
I use the Account object by passing it around the server to perform varying maintenance tasks on it and then eventually sending the object client-side where I have it being deserialized into the client-side version of the Account class.
Does anyone know what could be causing these internal exceptions and how I might be able to eliminate or mitigate them? 
Here is my output log:
Below is the exception message:
The AccountStatistics isn't listed in the query above because there are about 20 some includes and I shorthanded the include list for brevity.
Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountStatistics]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountStatistics]'
There is no inner exception.
I double checked my database and I have an entry for the user and all of their fields are filled with valid data.
Account Class (Edited for brevity)
public class Account
    {
        [Key]
        public int ID { get; set; }
        public DateTime CreationDate { get; set; }
        public AccountCurrency Currency { get; set; }
        public AccountProgression Progression { get; set; }
        public AccountSettings Settings { get; set; }
        public AccountStatistics Statistics { get; set; }
        public ICollection&lt;AccountFriendEntry&gt; Friends { get; set; }
        public ICollection&lt;AccountUnlockedGameEntry&gt; Unlocks{ get; set; }
    }
Account Statistics class
public class AccountStatistics
{
    [Key]
    public int AccountID { get; set; }
    public int LoginCount { get; set; }
    public DateTime LastLoginTime { get; set; }
    public DateTime LastActivityTime { get; set; }
}
Edit
Keys for the Account Statistics table
   migrationBuilder.CreateTable(
            name: ""AccountStatistics"",
            columns: table =&gt; new
            {
                AccountID = table.Column&lt;int&gt;(nullable: false),
                LoginCount = table.Column&lt;int&gt;(nullable: false),
                LastLoginTime = table.Column&lt;DateTime&gt;(nullable: false),
                CreationDate = table.Column&lt;DateTime&gt;(nullable: false)
            },
            constraints: table =&gt;
            {
                table.PrimaryKey(""PK_AccountStatistics"", x =&gt; x.AccountID);
                table.ForeignKey(
                    name: ""FK_AccountStatistics_Accounts_AccountID"",
                    column: x =&gt; x.AccountID,
                    principalTable: ""Accounts"",
                    principalColumn: ""ID"",
                    onDelete: ReferentialAction.Cascade);
            });
Edit 9001
After doing some testing I've realized the exception only occurs when chaining includes.
This will cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
This will NOT cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
It does not matter if its currency and unlock, friends and currency, settings, and statistics. Any combination of includes (2 or more) causes the exception to happen.
Edit 9002
Here are my results of the following query:
var acct = _accountContext.Account
     .Where(a =&gt; a.FacebookUserID == facebookUserID)
     .Select(x =&gt; new { Account = x, x.Currency, x.Settings }).ToList();
Exception:
System.ArgumentException: 'Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountSettings]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountSettings]''
I feel like this is treating AccountSettings as a collection when it's a single field reference.
Edit Final:
I never found a fix for this issue. I re-created all the tables and such in another environment and it works just fine. Not a very ideal solution to blow away all tables, classes, and migrations, but it's the only thing that fixed the issue.
","<c#><sal-server><entity-framework>, system.line.express except thrown use firstordefault .net core 2.1, receive ~300+ except spasm server output labelled: except thrown: 'system.argumentexception' system.line.expressions.do query use follows: account account = _accountcontext.account. include(i =&it; i.currency). include(i =&it; i.unlocked). include(i =&it; i.settings). include(i =&it; i.friends). firstordefault(a =&it; a.facebookuserid == facebookuserid); events except stop generation, show large query output window, every continue normal. change query follow expert exception: iqueryable&it;account&it; account = _accountcontext.account. include(i =&it; i.currency). include(i =&it; i.unlocked). include(i =&it; i.settings). include(i =&it; i.friends). where(a =&it; a.facebookuserid == facebookuserid); however, call any first, firstordefault, single, etc iqueryable&it;account&it; variable except start stop ~300. except stall user login upward 30 second more. murat except grow amount data return database. use account object pass around server perform vary maintain task events send object client-said desert client-said version account class. anyone know could cause inter except might all limit piti them? output log: except message: accountstatist list query 20 include shorthand include list gravity. field 'microsoft.entityframeworkcore.query.entityquerymodelvisitor+transparentidentifier`2[project.models.account,system.collections.genetic.innumerable`1[project.models.accountstatistics]].inner' define type 'microsoft.entityframeworkcore.query.entityquerymodelvisitor+transparentidentifier`2[project.models.account,project.models.accountstatistics]' inner exception. doubt check database entry user field fill valid data. account class (edit gravity) public class account { [key] public in id { get; set; } public datetim creation { get; set; } public accountcurr current { get; set; } public accountprogress progress { get; set; } public accounts set { get; set; } public accountstatist states { get; set; } public collection&it;accountfriendentry&it; friend { get; set; } public collection&it;accountunlockedgameentry&it; unlocked{ get; set; } } account states class public class accountstatist { [key] public in accounted { get; set; } public in logincount { get; set; } public datetim lastlogintim { get; set; } public datetim lastactivitytim { get; set; } } edit key account states table migrationbuilder.createtable( name: ""accountstatistics"", columns: table =&it; new { accounted = table.column&it;in&it;(syllable: false), logincount = table.column&it;in&it;(syllable: false), lastlogintim = table.column&it;daytime&it;(syllable: false), creation = table.column&it;daytime&it;(syllable: false) }, constraint: table =&it; { table.primarykey(""pk_accountstatistics"", x =&it; x.accounted); table.foreigner( name: ""fk_accountstatistics_accounts_accountid"", column: x =&it; x.accounted, principaltable: ""accounts"", principalcolumn: ""id"", delete: referentialaction.cascade); }); edit 9001 test i'v realize except occur chain includes. cause exception: account account = _accountcontext.account. include(i =&it; i.currency). include(i =&it; i.unlocked). include(i =&it; i.settings). include(i =&it; i.friends). firstordefault(a =&it; a.facebookuserid == facebookuserid); cause exception: account account = _accountcontext.account. include(i =&it; i.currency). firstordefault(a =&it; a.facebookuserid == facebookuserid); matter current clock, friend currency, settings, statistics. combine include (2 more) cause except happen. edit 9002 result follow query: war act = _accountcontext.account .where(a =&it; a.facebookuserid == facebookuserid) .select(x =&it; new { account = x, x.currency, x.set }).moist(); exception: system.argumentexception: 'field 'microsoft.entityframeworkcore.query.entityquerymodelvisitor+transparentidentifier`2[project.models.account,system.collections.genetic.innumerable`1[project.models.accountsettings]].inner' define type 'microsoft.entityframeworkcore.query.entityquerymodelvisitor+transparentidentifier`2[project.models.account,project.models.accountsettings]'' feel like treat accounts collect single field reference. edit final: never found fix issue. re-great table not environs work fine. ideal slut blow away tables, classes, migrations, thing fix issue."
51632735,JDBC result set retrieve LocalDateTime,"I run a simple query to retrieve a row from a MySQL database.
I get ResultSet and I need to retrieve a LocalDateTime object from it.
My DB table.
CREATE TABLE `some_entity` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `title` varchar(45) NOT NULL,
  `text` varchar(255) DEFAULT NULL,
  `created_date_time` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `id_UNIQUE` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
I need to retrieve some entity by id.
String SELECT = ""SELECT ID, TITLE, TEXT, CREATED_DATE_TIME FROM some_entity WHERE some_entity.id = ?"";
PreparedStatement selectPreparedStatement = connection.prepareStatement(SELECT);
try {
    selectPreparedStatement.setLong(1, id);
    ResultSet resultSet = selectPreparedStatement.executeQuery();
    if (resultSet.next()) {
        Long foundId = resultSet.getLong(1);
        String title = resultSet.getString(2);
        String text = resultSet.getString(3);
        LocalDateTime createdDateTime = null;// How do I retrieve it???
    }
} catch (SQLException e) {
    throw new RuntimeException(""Failed to retrieve some entity by id."", e);
}
",<java><mysql><jdbc><java-time>,1123,0,23,12375,15,79,114,50,22758,0.0,3438,1,13,2018-08-01 11:59,2018-08-01 12:05,2018-08-01 12:05,0.0,0.0,Advanced,35,"<java><mysql><jdbc><java-time>, JDBC result set retrieve LocalDateTime, I run a simple query to retrieve a row from a MySQL database.
I get ResultSet and I need to retrieve a LocalDateTime object from it.
My DB table.
CREATE TABLE `some_entity` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `title` varchar(45) NOT NULL,
  `text` varchar(255) DEFAULT NULL,
  `created_date_time` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `id_UNIQUE` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
I need to retrieve some entity by id.
String SELECT = ""SELECT ID, TITLE, TEXT, CREATED_DATE_TIME FROM some_entity WHERE some_entity.id = ?"";
PreparedStatement selectPreparedStatement = connection.prepareStatement(SELECT);
try {
    selectPreparedStatement.setLong(1, id);
    ResultSet resultSet = selectPreparedStatement.executeQuery();
    if (resultSet.next()) {
        Long foundId = resultSet.getLong(1);
        String title = resultSet.getString(2);
        String text = resultSet.getString(3);
        LocalDateTime createdDateTime = null;// How do I retrieve it???
    }
} catch (SQLException e) {
    throw new RuntimeException(""Failed to retrieve some entity by id."", e);
}
","<cava><myself><job><cava-time>, job result set retrieve localdatetime, run simple query retrieve row myself database. get results need retrieve localdatetim object it. do table. great table `some_entity` ( `id` begin(20) null auto_increment, `title` varchar(45) null, `text` varchar(255) default null, `created_date_time` datetim null, primary key (`id`), unique key `id_unique` (`id`) ) engine=innodb auto_increment=2 default charge=utf; need retrieve entity id. string select = ""select id, title, text, created_date_tim some_ent some_entity.id = ?""; preparedstat selectpreparedstat = connection.preparestatement(select); try { selectpreparedstatement.strong(1, id); results results = selectpreparedstatement.executequery(); (results.next()) { long founded = results.getting(1); string till = results.getting(2); string text = results.getting(3); localdatetim createddatetim = null;// retrieve it??? } } catch (sqlexcept e) { throw new runtimeexception(""fail retrieve entity id."", e); }"
52043874,Limiting maximum size of dataframe partition,"When I write out a dataframe to, say, csv, a .csv file is created for each partition.  Suppose I want to limit the max size of each file to, say, 1 MB.  I could do the write multiple times and increase the argument to repartition each time.  Is there a way I can calculate ahead of time what argument to use for repartition to ensure the max size of each file is less than some specified size.
I imagine there might be pathological cases where all the data ends up on one partition. So make the weaker assumption that we only want to ensure that the average file size is less than some specified amount, say 1 MB. 
",<scala><apache-spark><apache-spark-sql>,615,0,0,8646,33,119,206,72,14117,0.0,723,2,13,2018-08-27 16:59,2018-08-30 19:29,2018-08-30 19:29,3.0,3.0,Intermediate,22,"<scala><apache-spark><apache-spark-sql>, Limiting maximum size of dataframe partition, When I write out a dataframe to, say, csv, a .csv file is created for each partition.  Suppose I want to limit the max size of each file to, say, 1 MB.  I could do the write multiple times and increase the argument to repartition each time.  Is there a way I can calculate ahead of time what argument to use for repartition to ensure the max size of each file is less than some specified size.
I imagine there might be pathological cases where all the data ends up on one partition. So make the weaker assumption that we only want to ensure that the average file size is less than some specified amount, say 1 MB. 
","<scala><apache-spark><apache-spark-sal>, limit maximum size datafram partition, write datafram to, say, is, .is file great partition. suppose want limit max size file to, say, 1 mb. could write multiple time increase argument repartee time. way call ahead time argument use repartee ensue max size file less specific size. imagine might pathology case data end one partition. make weaker assume want ensue average file size less specific amount, say 1 mb."
