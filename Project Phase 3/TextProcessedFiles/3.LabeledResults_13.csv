QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
49286687,"Managing database connections in Node.js, best practices?","I'm building an Node application which will query simple and more complex (multiple joins) queries. I'm looking for suggestions on how I should manage the mySQL connections. 
I have the following elements: 
server.js : Express
router1.js (fictive name) : Express Router middleware
router2.js (fictive name) : Express Router middleware
    //this is router1
    router.get('/', function (req, res){
    connection.connect(function(Err){...});
      connection.query('SELECT* FROM table WHERE id = ""blah""', function(err,results,fields){
        console.log(results);
      });
      ...
    connection.end();
    })
Should I connect to mysql everytime '/router1/' is requested, like in this example, or it's better to leave one connection open one at start up? As: connection.connect(); outside of:  router.get('/',function(req,res){
...
}); ?
",<mysql><node.js><database>,842,0,0,277,1,4,8,48,17538,,76,4,16,2018-03-14 19:59,2019-11-03 19:31,,599.0,,Intermediate,22
48974135,`Error: file is not a database` in SQLite,"I have created a database named database.db.
When I create a table in the database, I get the error Error: file is not a database as shown below:
nehal@nehal-Inspiron-5559:~/Desktop/UAV$ sqlite3 database.db 
SQLite version 3.20.1 2017-08-24 16:21:36
Enter &quot;.help&quot; for usage hints.
sqlite&gt; CREATE TABLE users(
   ...&gt; password varchar(10),
   ...&gt; email text,
   ...&gt; name text
   ...&gt; );
Error: file is not a database
How do I resolve the error?
",<database><sqlite>,471,0,11,407,2,5,11,54,48664,0.0,0,5,16,2018-02-25 13:44,2018-02-25 16:58,2018-02-25 16:58,0.0,0.0,Basic,9
56319638,EntityFrameworkCore SQLite in-memory db tables are not created,"For integration tests I am using an EntityFrameworkCore SQLite in-memory db and creating its schema as per Microsoft docs, but when I attempt to seed data an exception is thrown that tables do not exist.
The mouse-over docs for DbContext.Database.EnsureCreated(); :
  Ensure that the database for the context exists.  If it exists, no
  action is taken.  If it does not exist then the database and all its
  schema are created.  If the database exists, then no action is made to
  ensure it is compatible with the model for this context.
I've read that an EntityFrameworkCore in-memory db only exists as long as an open connection exists, and so I tried explicitly creating a var connection = new SqliteConnection(""DataSource=:memory:""); instance and wrapping the below code in a using(connection) {} block and passing the connection instance options.UseSqlite(connection);, but DbContext.Database.EnsureCreated(); still doesn't create any db-objects
public class CustomWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;Startup&gt;
{
    protected override IWebHostBuilder CreateWebHostBuilder()
    {
        return WebHost.CreateDefaultBuilder()
            .UseStartup&lt;Startup&gt;();
    }
    protected override void ConfigureWebHost(IWebHostBuilder builder)
    {
      using (var connection = new SqliteConnection(""DataSource=MySharedInMemoryDb;mode=memory;cache=shared""))
      {
          connection.Open();
          builder.ConfigureServices(services =&gt;
          {
              var serviceProvider = new ServiceCollection()
                  .AddEntityFrameworkSqlite()
                  .BuildServiceProvider();
              services.AddDbContext&lt;MyDbContext&gt;(options =&gt;
              {
                  options.UseSqlite(connection);
                  options.UseInternalServiceProvider(serviceProvider);
              });
              var contextServiceProvider = services.BuildServiceProvider();
              // we need a scope to obtain a reference to the database contexts
              using (var scope = contextServiceProvider.CreateScope())
              {
                  var scopedProvider = scope.ServiceProvider;
                  var logger = scopedProvider.GetRequiredService&lt;ILogger&lt;CustomWebApplicationFactory&lt;TStartup&gt;&gt;&gt;();
                  using (var myDb = scopedProvider.GetRequiredService&lt;MyDbContext&gt;())
                  {
                      // DEBUG CODE
                      // this returns script to create db objects as expected
                      // proving that MyDbContext is setup correctly
                      var script = myDb.Database.GenerateCreateScript();
                      // DEBUG CODE
                      // this does not create the db objects ( tables etc )
                      // this is not as expected and contrary to ms docs
                      var result = myDb.Database.EnsureCreated();
                      try
                      {
                          SeedData.PopulateTestData(myDb);
                      }
                      catch (Exception e)
                      {
                          // exception is thrown that tables don't exist
                          logger.LogError(e, $""SeedData.PopulateTestData(myDb) threw exception=[{e.Message}]"");
                      }
                  }
              }
          });
        }
        builder.UseContentRoot(""."");
        base.ConfigureWebHost(builder);
    }
Please note that in this post I'm only asking the question why doesn't DbContext.Database.EnsureCreated(); create the schema as expected. I'm not presenting the above code as a general pattern for running integration-tests.
",<c#><sqlite><integration-testing><in-memory-database><entity-framework-core-2.2>,3702,0,71,8295,17,61,91,80,15929,0.0,96,1,16,2019-05-27 4:20,2019-05-29 20:07,,2.0,,Advanced,32
54325397,How to connect MySQL database to ReactJS app?,"I have build a Todo App with create-react-app. The store I'm using is based on Local Storage(JS attribute of object window). Now I created a MySQL databases and want to connect to that database, so the state will show the values from database, and will be updated through actions. 
I've tried to connect to db and output values through 'node' console using db.js. It works.
const mysql = require('mysql');
const con = mysql.createConnection({
  host: ""localhost"",
  user: ""root"",
  password: ""root"",
  database: 'root'
});
con.connect(function(err) {
  if (err) throw err;
  con.query(""SELECT * FROM tasks"", function (err, result, fields) {
    if (err) throw err;
    console.log(result);
  });
});
Is it possible to connect the state of app to database using this script?
",<javascript><mysql><reactjs>,774,0,16,175,1,1,9,76,98021,0.0,3,2,16,2019-01-23 10:43,2019-01-23 10:45,2019-01-23 10:45,0.0,0.0,Basic,3
50477550,Lookup against MYSQL TEXT type column,"My table/model has TEXT type column, and when filtering for the records on the model itself, the AR where produces the correct SQL and returns correct results, here is what I mean :
MyNamespace::MyValue.where(value: 'Good Quality')
Produces this SQL :
SELECT `my_namespace_my_values`.* 
FROM `my_namespace_my_values` 
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Take another example where I m joining MyNamespace::MyValue and filtering on the same value column but from the other model (has relation on the model to my_values). See this (query #2) :
OtherModel.joins(:my_values).where(my_values: { value: 'Good Quality' })
This does not produce correct query, this filters on the value column as if it was a String column and not Text, therefore producing incorrect results like so (only pasting relevant where) :
WHERE my_namespace_my_values`.`value` = 'Good Quality'
Now I can get past this by doing LIKE inside my AR where, which will produce the correct result but slightly different query. This is what I mean :
OtherModel.joins(:my_values).where('my_values.value LIKE ?, '%Good Quality%')
Finally arriving to my questions. What is this and how it's being generated for where on the model (for text column type)?
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Maybe most important question what is the difference in terms of performance using :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
and this :
(my_namespace_my_values.value LIKE '%Good Quality%')
and more importantly how do I get my query with joins (query #2) produce where like this :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
",<mysql><ruby-on-rails><activerecord>,1670,0,18,22672,36,134,182,63,578,0.0,1642,4,16,2018-05-22 22:58,2018-05-25 13:44,2018-05-30 14:54,3.0,8.0,Basic,2
52109026,Firestore schema versioning and backward compatibility with android app to prevent crashes,"Firestore a NOSQL is a Document oriented database. Now how to manage versioning of the data as I use it with Firebase SDK and Android applications?
For e.g. let's say I have a JSON schema that I launch with my 1.0.0version of my android app. Later 1.0.1 comes up where I have to add some extra fields for the newer documents. Since I changed the structure to have additional information, it only applies to new documents.
Therefore, using this logic, I can see my Android application must be able to deal with all versions of JSON tree if used against this project I create in the firebase console with Firestore. But this can be very painful right, that I have carry the deadweight of backward compatibility endlessly? Is there a way to have some sort of version like in protobuf or something the android app can send to firestore server side so that automatically we can do something to prevent crashes on the android app when it sees new fields? 
See also this thread, the kind of problem the engineer has posted. You can end up with this kind of problem as new fields get discovered in your JSON tree by the android app
Add new field or change the structure on all Firestore documents
Any suggestions for how we should go about this?
In node.js architecture we handle this with default-> v1.1/update or 
default-> v1.0/update, that way we can manage the routes.
But for android+firebase SKD-> talking to Firestore NOSQL, how do I manage the versioning of the json schema. 
",<android><firebase><nosql><google-cloud-firestore><versioning>,1477,1,0,597,0,7,18,43,1602,0.0,0,1,16,2018-08-31 5:35,2020-04-08 9:21,,586.0,,Advanced,32
54332942,spark windowing function VS group by performance issue,"I have a dataframe done like
| id | date      |  KPI_1 | ... | KPI_n
| 1  |2012-12-12 |   0.1  | ... |  0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46
| 4  |2012-12-14 |   0.2  | ... |  0.45 
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
we have 
X identifiers
a row for every identifier for a given date
n KPIs
I have to calculate some derived KPI for every row, and this KPI depends on the previous values of every ID.
Let's say my derived KPI is a diff, it would be:
| id | date      |  KPI_1 | ... | KPI_n | KPI_1_diff | KPI_n_diff
| 1  |2012-12-12 |   0.1  | ... |  0.5  |   0.1      | 0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4  |   0.2      |0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 |   0.66     | 0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46 |   0.2-0.1  | 0.46 - 0.66
| 4  |2012-12-13 |   0.2  | ... |  0.45  ...
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
Now: what I would do is:
val groupedDF = myDF.groupBy(""id"").agg(
    collect_list(struct(col(""date"",col(""KPI_1""))).as(""wrapped_KPI_1""),
    collect_list(struct(col(""date"",col(""KPI_2""))).as(""wrapped_KPI_2"")
    // up until nth KPI
)
I would get aggregated data such as:
[(""2012-12-12"",0.1),(""2012-12-12"",0.2) ...
Then I would sort these wrapped data, unwrap and map over these aggregated result with some UDF and produce the output (compute diffs and other statistics).
Another approach is to use the window functions such as:
val window = Window.partitionBy(col(""id"")).orderBy(col(""date"")).rowsBetween(Window.unboundedPreceding,0L) 
and do :
val windowedDF = df.select (
  col(""id""),
  col(""date""),
  col(""KPI_1""),
  collect_list(struct(col(""date""),col(""KPI_1""))).over(window),  
  collect_list(struct(col(""date""),col(""KPI_2""))).over(window)
   )   
This way I get:
[(""2012-12-12"",0.1)]
[(""2012-12-12"",0.1), (""2012-12-13"",0.1)]
...
That look nicer to process, but I suspect that repeating the window would produce unnecessary grouping and sorting for every KPI.
So here are the questions:
I'd rather go for the grouping approach?
Would I go for the window? If so what is the most efficient approach to do it?
",<apache-spark><apache-spark-sql>,2156,0,33,851,1,10,17,38,21302,0.0,348,2,16,2019-01-23 17:49,2019-01-24 7:28,2019-01-24 7:28,1.0,1.0,Intermediate,23
54301624,[ERR_HTTP_HEADERS_SENT]: Cannot set headers after they are sent to the client,"I'm working with PostgreSQL and NodeJS with its ""PG Module"". 
CRUD works but sometimes doesn't update automatically the views when i save or delete some item. this is my code and I think that the error is here but i cannot find it, i tried everything :'( 
Error Message:
const controller = {};
const { Pool } = require('pg');
var connectionString = 'postgres://me:system@localhost/recipebookdb';
const pool = new Pool({
    connectionString: connectionString,
})
controller.list = (request, response) =&gt; {
    pool.query('SELECT * FROM recipes', (err, result) =&gt; {
        if (err) {
            return next(err);
        }
           return response.render('recipes', { data: result.rows });
    });
};
controller.save = (req, res) =&gt; {
    pool.query('INSERT INTO recipes(name, ingredients, directions) VALUES ($1, $2, $3)',
        [req.body.name, req.body.ingredients, req.body.directions]);
    return res.redirect('/');
};
controller.delete = (req, res) =&gt; {
    pool.query('DELETE FROM RECIPES WHERE ID = $1', [req.params.id]);
    return res.redirect('/');
}
module.exports = controller;
PD: CRUD works but sometimes appears that error.
",<javascript><node.js><postgresql>,1157,1,29,256,1,3,16,63,71152,0.0,40,3,16,2019-01-22 5:13,2019-01-22 5:24,2019-01-22 5:52,0.0,0.0,Basic,13
53934294,How to count null values in postgresql?,"select distinct ""column"" from table;
output:
    column
1     0.0
2     [null]
3     1.0
But when I try to count the null values
select count(""column"") from train where ""column"" is NULL;
Gives output 0 (zero)
Can you suggest where it's going wrong?
",<sql><postgresql><count><null>,249,0,6,7771,25,82,130,53,25494,0.0,121,6,16,2018-12-26 16:01,2018-12-26 16:03,2018-12-26 16:03,0.0,0.0,Basic,9
56977456,varchar is missing on pgadmin4 UI,"I am very new on Postgresql. I have installed pgadmin4 on my Mac OS X. I try to create a very basic table with a column which has type varchar but it doesn't seem to have ti on the list of the UI.
Am I missing something?
",<postgresql><macos><pgadmin>,221,1,1,2309,7,31,63,81,14545,0.0,3517,1,16,2019-07-10 19:31,2019-07-16 14:55,2019-07-16 14:55,6.0,6.0,Intermediate,15
53526505,Granting Full SQL Server Permissions for a Database,"How can I give a user (who was created WITHOUT LOGIN) full control over a contained database without having to specify that database's name, like GRANT CONTROL ON DATABASE::DatabaseName TO UserName, but without using a database name?  I figure it'll include GRANT ALTER ANY SCHEMA TO UserName, but I'm not sure what else I'd need to grant, or if there's a better way.  Thanks.
",<sql-server><permissions><roles>,377,0,0,657,1,6,17,54,38229,0.0,13,2,16,2018-11-28 19:10,2018-11-28 19:35,2018-11-28 19:35,0.0,0.0,Basic,5
56897416,HikariPool-1 - Exception during pool initialization,"I'm deploying Spring-Boot4 project and I have got an error called HikariPool-1 - Exception during pool initialization.
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.3.10.Final}
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
2019-07-05 11:29:43.792  INFO 5084 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.4.Final}
2019-07-05 11:29:43.877  INFO 5084 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2019-07-05 11:29:44.964 ERROR 5084 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: database ""test2"" does not exist
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2559) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.&lt;init&gt;(QueryExecutorImpl.java:133) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:250) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.makeConnection(Driver.java:454) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.connect(Driver.java:256) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) [HikariCP-3.2.0.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) ~[classes/:na]
2019-07-05 11:29:44.980  WARN 5084 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : FATAL: database ""test2"" does not exist
2019-07-05 11:29:44.980  WARN 5084 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
2019-07-05 11:29:44.980  INFO 5084 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2019-07-05 11:29:45.011  INFO 5084 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2019-07-05 11:29:45.027 ERROR 5084 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) [classes/:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:275) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    ... 16 common frames omitted
Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.determineDialect(DialectFactoryImpl.java:100) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.buildDialect(DialectFactoryImpl.java:54) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:137) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    ... 33 common frames omitted
Process finished with exit code 1
I can't understand what does it means. I have already searched for the error in StackOverflow. There are some questions about HikariPool but none of them are working for me.
My application.properties is as follows,
spring.datasource.url=jdbc:postgresql://localhost/test2
spring.datasource.username=postgres
spring.datasource.password=root
spring.jpa.generate-ddl=true
My pom.xml dependancies are,
&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
",<spring><postgresql><spring-boot><hikaricp>,19330,0,147,3977,11,43,64,78,127318,0.0,2342,6,16,2019-07-05 6:12,2019-07-05 6:29,2019-07-05 13:50,0.0,0.0,Basic,9
49095292,Exclude primary key attributes from a sequelize query,"I have a sequelize query from multiple tables inner joined together. I need to group them by on a nested include model but the sequelize query throws the primary key every time, even if I mention the attributes as: attributes:[].
However attributes:[] is working for nested include models.
",<mysql><node.js><sequelize.js>,290,0,4,195,1,1,10,69,33371,0.0,26,4,16,2018-03-04 12:16,2018-03-04 18:12,,0.0,,Basic,10
58206572,AWS RDS Mysql kill transaction by ID (without thread_id),"I'm currently having an issue with some long running transactions that are holding locks to a single row in a table that I'm being unable to kill.
Innodb_trx holds the following information - special attention to thread_id = null;
select * from information_schema.innodb_trx\G
*************************** 2. row ***************************
                    trx_id: 153261728
                 trx_state: RUNNING
               trx_started: 2019-10-02 10:05:42
     trx_requested_lock_id: NULL
          trx_wait_started: NULL
                trx_weight: 5
       trx_mysql_thread_id: 0
                 trx_query: NULL
       trx_operation_state: NULL
         trx_tables_in_use: 0
         trx_tables_locked: 2
          trx_lock_structs: 3
     trx_lock_memory_bytes: 1136
           trx_rows_locked: 1
         trx_rows_modified: 2
   trx_concurrency_tickets: 0
       trx_isolation_level: READ COMMITTED
         trx_unique_checks: 1
    trx_foreign_key_checks: 1
trx_last_foreign_key_error: NULL
 trx_adaptive_hash_latched: 0
 trx_adaptive_hash_timeout: 0
          trx_is_read_only: 0
trx_autocommit_non_locking: 0
and
show engine innodb status\G;
------------------
---TRANSACTION 153261728, ACTIVE (PREPARED) 27716 sec recovered trx
3 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 2
---TRANSACTION 96697370, ACTIVE (PREPARED) 988082 sec recovered trx
0 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 3
Is there as way to kill/stop/rollback a transaction like this?
",<mysql><amazon-rds>,1510,0,32,168,0,0,6,49,1641,0.0,1,1,16,2019-10-02 17:39,2022-10-07 1:53,,1101.0,,Basic,10
52647648,Remove last n chars of a varchar column,"I wanted to know if there's a ""compact"" way of deleting last n chars from a column in PostGreSQL.
I have a column born as a DATE formatted like this: yyyy-MM-dd.
I altered the table to make this column a varchar, replaced all of the unnecessary dashes but I can't think about a quick and reliable way to delete the last two chars since I'd like the varchar to show only yyyyMM.
With ""quick and reliable"" I mean something that won't ask me to use a temp table.
Thanks in advance
",<postgresql>,478,0,0,665,4,10,33,53,14820,0.0,20,2,16,2018-10-04 13:12,2018-10-04 13:48,2018-10-04 13:48,0.0,0.0,Basic,2
54844651,I dont know how Postgresql created user on my mac,"Two days ago, i started learning postgresql. Most tutorials I followed online were either old or the codes wont just work on my mac. I followed a lot of tutorials that did a lot of totally different things.
When i switched on my system today. I noticed Postgresql created a user on my mac. I don't know what this is or maybe i used the wrong CLI code.
When I tried viewing the user, I saw this
should I delete this user or it has a function?
",<postgresql><macos><terminal>,442,2,0,423,2,5,13,40,20634,0.0,11,2,16,2019-02-23 18:20,2019-02-23 20:04,2019-02-23 20:04,0.0,0.0,Basic,14
50093776,"SQLITE, Create a temp table then select from it","just wondering how i can create a temp table and then select from it further down the script.
Example. 
CREATE TEMPORARY TABLE TEMP_TABLE1 AS 
Select 
L.ID,
SUM(L.cost)/2 as Costs,
from Table1 L
JOIN Table2 C on L.ID = C.ID
Where C.name  = 'mike'
Group by L.ID
Select 
Count(L.ID)
from Table1 L
JOIN TEMP_TABLE1 TT1 on L.ID = TT1.ID;
Where L.ID not in (TT1) 
And Sum(L.Cost) &gt; TT1.Costs
Ideally I want to have a temp table then use it later in the script to reference from.
Any help would be great!
",<sqlite>,502,0,17,181,1,1,6,42,52575,,1,2,16,2018-04-30 4:03,2018-04-30 4:58,,0.0,,Basic,10
53057786,"Woocommerce mySQL Query - List All Orders, Users and Purchased Items","I have a fully working mySQL query which pulls all of the orders, users, addresses and items purchased from Woocommerce, however it only lists the products individually, and I would like to add the quantity for each product displayed.
Currently shows 'Items Ordered'
Running Shoes
Walking Shoes
Where it should show 'Items Ordered'
3 x Running Shoes
4 x Walking Shoes
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
I believe Woocommerce stores the QTY in the following table / entries:
wp_woocommerce_order_itemmeta
  order_item_id
    SELECT wp_woocommerce_order_itemmeta.meta_value
    WHERE wp_woocommerce_order_itemmeta.meta_value = '_qty' and wp_woocommerce_order_itemmeta.order_item_id =
And I need to join it in somehow into this section:
( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
Thanks in advance.
UPDATED WITH ANSWER FROM: Lucek
Thanks Lucek, absolutely perfect.
I've combined the complete query in case anyone else wants to copy it.
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( SELECT GROUP_CONCAT(CONCAT(m.meta_value, ' x ', i.order_item_name) separator '&lt;/br&gt;' )
    FROM wp_woocommerce_order_items i
    JOIN wp_woocommerce_order_itemmeta m ON i.order_item_id = m.order_item_id AND meta_key = '_qty'
    WHERE i.order_id = p.ID AND i.order_item_type = 'line_item') AS 'Items Ordered',
  MAX( CASE WHEN pm.meta_key = 'post_excerpt'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'User Comments'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
",<mysql><wordpress><woocommerce><product>,4903,0,65,173,0,1,9,46,9943,0.0,0,1,16,2018-10-30 5:09,2018-10-31 8:27,2018-10-31 8:27,1.0,1.0,Basic,10
48102013,SQL Server INSERT INTO with WHERE clause,"I'm trying to insert some mock payment info into a dev database with this query:
INSERT
    INTO
        Payments(Amount)
    VALUES(12.33)
WHERE
    Payments.CustomerID = '145300';
How can adjust this to execute? I also tried something like this: 
IF NOT EXISTS(
    SELECT
        1
    FROM
        Payments
    WHERE
        Payments.CustomerID = '145300' 
) INSERT 
    INTO
        Payments(Amount)
    VALUES(12.33);
",<sql><sql-server><database><t-sql><insert>,424,0,17,1406,4,21,57,44,100145,0.0,208,8,16,2018-01-04 19:13,2018-01-04 19:18,2018-01-04 19:19,0.0,0.0,Basic,3
50607120,CosmosDb count distinct elements,"Is there a direct function to count distinct elements in a CosmosDb query?
This is the default count:
SELECT value count(c.id) FROM c
And distinct works without count:
SELECT distinct c.id FROM c
But this returns a Bad Request - Syntax Error:
  SELECT value count(distinct c.id) FROM c
How would count and distinct work together?
",<sql><azure><azure-cosmosdb>,330,0,5,2660,7,35,51,58,32338,0.0,126,7,16,2018-05-30 14:21,2018-05-31 9:08,2019-01-04 0:32,1.0,219.0,Basic,1
55406735,Can a Postgres Commit Exist in Procedure that has an Exception Block?,"I am having a difficult time understanding transactions in Postgres. I have a procedure that may encounter an exception. There are parts of the procedure where I might want to commit my work so-far so that it won't be rolled back if an exceptions ensues.
I want to have an exception handling block at the end of the procedure where I catch the exception and insert the information from the exception into a logging table.
I have boiled the problem down to a simple procedure, below, which fails on PostgreSQL 11.2 with
2D000 cannot commit while a subtransaction is active
PL/pgSQL function x_transaction_try() line 6 at COMMIT
    drop procedure if exists x_transaction_try;
    create or replace procedure x_transaction_try()
        language plpgsql
    as $$
    declare
    begin
         raise notice 'A';
         -- TODO A: do some insert or update that I want to commit no matter what
         commit;
         raise notice 'B';
         -- TODO B: do something else that might raise an exception, without rolling
         -- back the work that we did in ""TODO A"".
    exception when others then
      declare
        my_ex_state text;
        my_ex_message text;
        my_ex_detail text;
        my_ex_hint text;
        my_ex_ctx text;
      begin
          raise notice 'C';
          GET STACKED DIAGNOSTICS
            my_ex_state   = RETURNED_SQLSTATE,
            my_ex_message = MESSAGE_TEXT,
            my_ex_detail  = PG_EXCEPTION_DETAIL,
            my_ex_hint    = PG_EXCEPTION_HINT,
            my_ex_ctx     = PG_EXCEPTION_CONTEXT
          ;
          raise notice '% % % % %', my_ex_state, my_ex_message, my_ex_detail, my_ex_hint, my_ex_ctx;
          -- TODO C: insert this exception information in a logging table and commit
      end;
    end;
    $$;
    call x_transaction_try();
Why doesn't this stored procedure work? Why is it that we never see the output of raise notice 'B' and instead we go into the exception block? Is it possible to do what I have described above with a Postgres 11 stored procedure?
Edit: This is a complete code sample. Paste the above complete code sample (including both the create procedure and call statements) into a sql file and run it in a Postgres 11.2 database to repro. The desired output would be for the function to print A then B, but instead it prints A then C along with the exception information.
Also notice that if you comment out all of the exception handling block such that the function does not catch exceptions at all, then the function will output 'A' then 'B' without an exception occurring. This is why I titled the question the way that I did 'Can a Postgres Commit Exist in Procedure that has an Exception Block?'
",<postgresql><stored-procedures><plpgsql><postgresql-11>,2701,0,44,2155,2,29,60,71,16087,0.0,579,2,16,2019-03-28 20:56,2019-03-29 8:18,2019-03-29 19:46,1.0,1.0,Advanced,32
58936116,PyCharm warns about unexpected arguments for SQLAlchemy User model,"I'm working with Flask-SQLAlchemy in PyCharm. When I try to create instances of my User model by passing keyword arguments to the model, PyCharm highlights the arguments with an ""Unexpected argument(s)"" warning. When I create instances of other models, I don't get this warning. Why am I getting this error for my User model?
class User(db.Model, UserMixin):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String, unique=True, nullable=False)
new_user = User(username=""test"")
In the above example username=""test"" is highlighted as a warning.
",<python><sqlalchemy><pycharm><flask-login>,572,1,8,355,0,4,11,81,4193,0.0,16,3,16,2019-11-19 14:17,2019-11-19 15:39,2019-11-19 15:39,0.0,0.0,Basic,6
59089991,Ruby 2.6.5 and PostgreSQL pg-gem segmentation fault,"From the console I cannot do any operation that touches the database. I get a Segmentation fault.
.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56: [BUG] Segmentation fault at 0x0000000000000110
ruby 2.6.5p114 (2019-10-01 revision 67812) [x86_64-darwin18]
It is literally any operation that might need the database, including MyModel.new.
-- Control frame information -----------------------------------------------
c:0071 p:---- s:0406 e:000405 CFUNC  :initialize
c:0070 p:---- s:0403 e:000402 CFUNC  :new
c:0069 p:0016 s:0398 e:000397 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56
c:0068 p:0107 s:0393 e:000392 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/activerecord-6.0.1/lib/active_record/connection_adapters/postgres
I have uninstalled and reinstalled the pg gem. And rebuilt the database. And restarted PostgreSQL.
I have seen other people reporting the problem when running under Puma, but my configuration works under Puma, fails under console!
Edit for clarity:
Yes, using bundler.
Starting the rails console either with rails c or bundle exec rails c has the same effect (segfault) with same stack trace.
Gemfile.lock has pg (1.1.4)
I re-bundled, specifying a bundle path. The stack trace now has that bundle path, so I guess by default bundler was using the rbenv path.
",<ruby><postgresql><ruby-on-rails-6>,1364,0,10,4718,4,34,52,66,7933,0.0,103,2,16,2019-11-28 13:28,2019-12-04 10:42,2019-12-04 10:42,6.0,6.0,Advanced,32
65035103,Sqldelight database schema not generated,"I have a KMM project and want to use SqlDelight library, but when I build the project  database schema not generated and table entities also.
actual class DatabaseDriverFactory(private val context: Context) {
    actual fun createDriver(): SqlDriver {
               //Unresolved reference: CoreDb
        return AndroidSqliteDriver(CoreDb.Schema, context, &quot;test.db&quot;)
    }
}
I have defined sqldelight folder inside my shared module and also created folder for feature generated kotlin classes as it is configured in gradle.build.kts and also have one *.sq file inside sqldelight folder
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
When I run generateSqlDelightInterface task I just see those log
&gt; Task :core:generateAndroidDebugCoreDbInterface NO-SOURCE
&gt; Task :core:generateAndroidReleaseCoreDbInterface NO-SOURCE
&gt; Task :core:generateIosMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataCommonMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateSqlDelightInterface UP-TO-DATE
can't register checkAndroidModules
BUILD SUCCESSFUL in 311ms
1:40:36 PM: Task execution finished 'generateSqlDelightInterface'.
Here is my full build.gradle.kts
import org.jetbrains.kotlin.gradle.plugin.mpp.KotlinNativeTarget
plugins {
  kotlin(&quot;multiplatform&quot;)
  kotlin(&quot;plugin.serialization&quot;)
  id(&quot;com.android.library&quot;)
  id(&quot;kotlin-android-extensions&quot;)
  id(&quot;koin&quot;)
  id(&quot;com.squareup.sqldelight&quot;)
}
repositories {
  gradlePluginPortal()
  google()
  jcenter()
  mavenCentral()
  maven {
    url = uri(&quot;https://dl.bintray.com/kotlin/kotlin-eap&quot;)
  }
  maven {
    url = uri(&quot;https://dl.bintray.com/ekito/koin&quot;)
  }
}
kotlin {
  android()
  val iOSTarget: (String, KotlinNativeTarget.() -&gt; Unit) -&gt; KotlinNativeTarget =
    if (System.getenv(&quot;SDK_NAME&quot;)?.startsWith(&quot;iphoneos&quot;) == true)
      ::iosArm64
    else
      ::iosX64
  iOSTarget(&quot;ios&quot;) {
    binaries {
      framework {
        baseName = &quot;core&quot;
      }
    }
  }
  val coroutinesVersion = &quot;1.3.9-native-mt&quot;
  val ktor_version = &quot;1.4.2&quot;
  val serializationVersion = &quot;1.0.0-RC&quot;
  val koin_version = &quot;3.0.0-alpha-4&quot;
  val sqlDelight = &quot;1.4.4&quot;
  sourceSets {
    val commonMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-core:$ktor_version&quot;)
        implementation(&quot;io.ktor:ktor-client-serialization:$ktor_version&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$coroutinesVersion&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-serialization-core:$serializationVersion&quot;)
        implementation(&quot;com.squareup.sqldelight:runtime:$sqlDelight&quot;)
        // SqlDelight extension
        implementation(&quot;com.squareup.sqldelight:coroutines-extensions:$sqlDelight&quot;)
        // Koin for Kotlin
        implementation(&quot;org.koin:koin-core:$koin_version&quot;)
        //shared preferences
        implementation(&quot;com.russhwolf:multiplatform-settings:0.6.3&quot;)
      }
    }
    val commonTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-common&quot;))
        implementation(kotlin(&quot;test-annotations-common&quot;))
      }
    }
    val androidMain by getting {
      dependencies {
        implementation(&quot;androidx.core:core-ktx:1.3.2&quot;)
        implementation(&quot;io.ktor:ktor-client-android:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:android-driver:$sqlDelight&quot;)
      }
    }
    val androidTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-junit&quot;))
        implementation(&quot;junit:junit:4.12&quot;)
      }
    }
    val iosMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-ios:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:native-driver:$sqlDelight&quot;)
      }
    }
    val iosTest by getting
  }
}
android {
  compileSdkVersion(29)
  sourceSets[&quot;main&quot;].manifest.srcFile(&quot;src/androidMain/AndroidManifest.xml&quot;)
  defaultConfig {
    minSdkVersion(23)
    targetSdkVersion(29)
    versionCode = 1
    versionName = &quot;1.0&quot;
  }
  buildTypes {
    getByName(&quot;release&quot;) {
      isMinifyEnabled = false
    }
  }
}
val packForXcode by tasks.creating(Sync::class) {
  group = &quot;build&quot;
  val mode = System.getenv(&quot;CONFIGURATION&quot;) ?: &quot;DEBUG&quot;
  val sdkName = System.getenv(&quot;SDK_NAME&quot;) ?: &quot;iphonesimulator&quot;
  val targetName = &quot;ios&quot; + if (sdkName.startsWith(&quot;iphoneos&quot;)) &quot;Arm64&quot; else &quot;X64&quot;
  val framework =
    kotlin.targets.getByName&lt;KotlinNativeTarget&gt;(&quot;ios&quot;).binaries.getFramework(mode)
  inputs.property(&quot;mode&quot;, mode)
  dependsOn(framework.linkTask)
  val targetDir = File(buildDir, &quot;xcode-frameworks&quot;)
  from({ framework.outputDirectory })
  into(targetDir)
}
tasks.getByName(&quot;build&quot;).dependsOn(packForXcode)
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
and for top level build.gradle
classpath &quot;com.squareup.sqldelight:gradle-plugin:$sqlDelight&quot;
Update
My project folder structure
root
  app
    src
      ...
  core //(kmm shared module)
    androidMain
        com.example.app.core
            database
    commonMain
        com.example.app.core
            database
            repository
            ...
            sqldelight
    iosMain
        com.example.app.core
            database
",<android><gradle><kotlin-multiplatform><sqldelight>,6024,2,185,5287,7,38,64,77,6964,0.0,751,5,16,2020-11-27 9:45,2020-11-27 11:16,2020-11-27 11:16,0.0,0.0,Intermediate,23
48685606,Import-Module : The specified module 'SqlServer' was not loaded because no valid module file was found in any module directory,"Well, hello there!
I'm working on a Script to get the Sql Job History and need to use the ""SqlServer"" Module. It's installed but I can't import it due to the Error Message above. When I got to the Modules Path, the Folder ""SqlServer"" exists and isn't empty. Don't get the problem here.
Thanks in advance!
",<sql-server><powershell><module>,305,0,0,169,1,1,4,45,26433,0.0,0,2,16,2018-02-08 12:31,2019-07-18 19:17,,525.0,,Basic,14
50493398,"MySQL connector error ""The server time zone value Central European Time""","My problem
MySQL connector ""The server time zone value Central European Time"" is unrecognized or represents more than one time zone.
The project
Small web Project with:
JavaEE, Tomcat 8.5, MySQL, Maven
My attempt
Maven -> change MySQL-connector form 6.x to 5.1.39 (no change)
Edit context.xml URL change
Connection in context.xml
URL=""jdbc: mysql://127.0.0.1:3306/rk_tu_lager?useLegacyDatetimeCode=false;serverTimezone=CEST;useSSL=false;
Error:
  Caused by:
  com.mysql.cj.core.exceptions.InvalidConnectionAttributeException:  The
  server time zone value 'Mitteleurop?ische Sommerzeit' is unrecognized
  or  represents more than one time zone. You must configure either the
  server or JDBC driver (via the serverTimezone configuration property)
  to use a more specifc time zone value if you want to utilize time zone
  support.
",<java><mysql><jdbc>,831,0,0,574,1,4,15,61,43737,0.0,195,4,16,2018-05-23 16:35,2018-05-24 15:01,2018-05-24 15:01,1.0,1.0,Basic,14
49425985,Inserting NULL as default in SQLAlchemy?,"I have the following column in SQLAlchemy:
name = Column(String(32), nullable=False)
I want that if no value is passed onto an insertion, it should enter a default value of either completely blank or if not possible, then default as NULL.
Should I define the column as :
name = Column(String(32), nullable=False, default=NULL)
Because in python NULL object is actually None
Any suggestions?
",<python><sqlalchemy><models>,391,0,4,1795,4,25,49,49,21170,0.0,84,2,16,2018-03-22 10:16,2018-03-22 13:53,,0.0,,Basic,14
50783515,Pyspark dataframe OrderBy list of columns,"I am trying to use OrderBy function in pyspark dataframe before I write into csv but I am not sure to use OrderBy functions if I have a list of columns.
Code:
Cols = ['col1','col2','col3']
df = df.OrderBy(cols,ascending=False)
",<python-3.x><apache-spark><pyspark><apache-spark-sql><sql-order-by>,227,0,2,967,3,10,23,78,46502,0.0,0,1,16,2018-06-10 12:07,2018-06-10 12:26,2018-06-10 12:26,0.0,0.0,Basic,10
57082613,AWS RDS Performance Insights - view full queries,"I'm using a MySQL-Server from AWS RDS. I would like to inspect the queries made by the app to optimize them. My problem is, that almost every query is longer than 1024 chars (which is the max-size, as stated here).
So I cannot identify the query by the first 1024 chars, as thats only the SELECT-Part - the interesting parts WHERE, ORDER, and so on are truncated. Since the app uses an ORM-System, I cannot change the queries to shorten them. 
Already tried to increase the option performance_schema_max_digest_length in the parameter-group to 4096, but that has no effect (no change can be seen in the options directly on the server).
What can I do?
",<mysql><amazon-web-services>,651,2,0,817,1,7,25,60,6673,,48,2,15,2019-07-17 19:03,2019-07-17 20:15,2022-06-22 17:06,0.0,1071.0,Intermediate,23
53847917,"PostgreSQL throws ""column is of type jsonb but expression is of type bytea"" with JPA and Hibernate","This is my entity class that is mapped to a table in postgres (9.4)
I am trying to store metadata as jsonb type in the database
@Entity
@Table(name = “room_categories”)
@TypeDef(name = “jsonb”, typeClass = JsonBinaryType.class)
public class RoomCategory extends AbstractEntity implements Serializable {
    private String name;
    private String code;
    @Type(type = ""jsonb"")
    @Column(columnDefinition = ""json"")
    private Metadata metadata;
}
This is the metadata class:
public class Metadata implements Serializable {
    private String field1;
    private String field2;
}
I have used following migration file to add jsonb column:
databaseChangeLog:
– changeSet:
id: addColumn_metadata-room_categories
author: arihant
changes:
– addColumn:
schemaName: public
tableName: room_categories
columns:
– column:
name: metadata
type: jsonb
I am getting this error while creating the record in postgres:
ERROR: column “metadata” is of type jsonb but expression is of type bytea
Hint: You will need to rewrite or cast the expression.
This is the request body i am trying to persist in db:
  {
    “name”: “Test102”,
    “code”: “Code102”,
    “metadata”: {
    “field1”: “field11”,
    “field2”: “field12”
    }
    }
Please help how to convert bytea type to jsonb in java spring boot app
",<java><postgresql><hibernate><jpa><jsonb>,1289,0,44,151,1,1,3,78,33884,,0,3,15,2018-12-19 9:13,2018-12-19 9:24,,0.0,,Intermediate,19
53657509,"Do I need ""transactionScope.Complete();""?","As far as I understand, the ""correct"" way to use a TransactionScope is to always call transactionScope.Complete(); before exiting the using block. Like this:
using (TransactionScope transactionScope = new TransactionScope(TransactionScopeOption.Required, new TransactionOptions { IsolationLevel = IsolationLevel.ReadUncommitted }))
{
    //...
    //I'm using this as a NOLOCK-alternative in Linq2sql.
    transactionScope.Complete();
}
However, I've seen that the code works without it, and even the answer I've learnt to use it from   omits it. So my question is, must it be used or not? 
",<c#><sql><.net><sql-server><linq-to-sql>,591,1,9,26768,38,141,295,44,8010,0.0,2952,3,15,2018-12-06 18:21,2018-12-06 18:50,2018-12-10 0:30,0.0,4.0,Intermediate,31
52465530,"Sequelize connection timeout while using Serverless Aurora, looking for a way to increase timeout duration or retry connection","I'm having an issue at the moment where I'm trying to make use of a Serverless Aurora database as part of my application.
The problem is essentially that when the database is cold, time to establish a connection can be greater than 30 seconds (due to db spinup) - This seems to be longer than the default timeout in Sequelize (using mysql), and as far as I can see I can't find any other way to increase this timeout or perhaps I need some way of re-attempting a connection?
Here's my current config:
const sequelize = new Sequelize(DATABASE, DB_USER, DB_PASSWORD, {
    host: DB_ENDPOINT,
    dialect: ""mysql"",
    operatorsAliases: false,
    pool: {
      max: 2,
      min: 0,
      acquire: 120000, // This needs to be fairly high to account for a 
      serverless db spinup
      idle: 120000,
      evict: 120000
    }
});
A couple of extra points:
Once the database is warm then everything works perfectly.
Keeping the database ""hot"", while it would technically work, kind of defeats the point of having it as a serverless db (Cost reasons).
I'm open to simply having my client re-try the API call in the event the timeout is a connection error.
Here's the logs in case they help at all.
{
""name"": ""SequelizeConnectionError"",
""parent"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
},
""original"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
}
}
",<mysql><node.js><amazon-rds><serverless><amazon-aurora>,1462,0,28,459,1,3,10,73,18139,0.0,2,1,15,2018-09-23 11:10,2018-09-23 12:03,2018-09-23 12:03,0.0,0.0,Intermediate,23
64933345,BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate on ASP.NET core 2.2,"I'm using aspnetboilerplate solution developed with ASP.NET core 2.2 .
The backend is deployed on azure and it uses the SQL server provided.
Sometimes, when the backend has a lot of requests to handle, it logs this exception:
ERROR 2020-11-20 12:28:21,968 [85   ] Mvc.ExceptionHandling.AbpExceptionFilter - Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
Microsoft.AspNetCore.Server.Kestrel.Core.BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
I tried to solve this problem adding this code to my Program.cs
 namespace WorkFlowManager.Web.Host.Startup
    {
        public class Program
        {
            public static void Main(string[] args)
            {
                var host = new WebHostBuilder()
                    .UseKestrel(options =&gt;
                    {
                        options.Limits.MinResponseDataRate = null;
                    });
                BuildWebHost(args).Run();
            }
            public static IWebHost BuildWebHost(string[] args)
            {
                return WebHost.CreateDefaultBuilder(args)
                    .UseStartup&lt;Startup&gt;()
                    .Build();
            }
        }
    }
But the problem is not solved.
",<c#><sql-server><azure-devops><aspnetboilerplate><asp.net-core-2.2>,1321,0,23,685,4,14,25,46,20300,0.0,15,3,15,2020-11-20 16:38,2020-11-20 18:57,2020-11-20 18:57,0.0,0.0,Intermediate,23
62340498,Open database files (.db) using python,"I have a data base file .db in SQLite3 format and I was attempting to open it to look at the data inside it. Below is my attempt to code using python.
    import sqlite3
    # Create a SQL connection to our SQLite database
    con = sqlite3.connect(dbfile)
    cur = con.cursor()
    # The result of a ""cursor.execute"" can be iterated over by row
    for row in cur.execute(""SELECT * FROM ""):
    print(row)
    # Be sure to close the connection
    con.close()
For the line (""SELECT * FROM "") , I understand that you have to put in the header of the table after the word ""FROM"", however, since I can't even open up the file in the first place, I have no idea what header to put. Hence how can I code such that I can open up the data base file to read its contents?
",<python><sqlite>,766,0,14,199,1,1,7,65,49292,0.0,28,3,15,2020-06-12 8:31,2020-06-12 10:01,2020-06-12 10:01,0.0,0.0,Basic,6
56821546,Can't connect to db from docker container,"I have MySQL server installed on a server and dockerized project, The DB is not publicly accessible, only from inside the server. It's used by local non dockerized apps too. 
I want to connect to it from inside the docker with it remaining not publicly accessible, I tried 172.17.0.1 but I get connection refused.
the current bind_address is 127.0.0.1, What do you suggest the bind_address would be ??
",<mysql><docker>,402,0,0,365,2,3,10,77,52171,0.0,5,2,15,2019-06-29 23:48,2019-06-30 0:48,2019-06-30 0:48,1.0,1.0,Basic,14
53397531,"How to use scram-sha-256 in Postgres 10 in Debian? Getting ""FATAL: password authentication failed""","I edited pg_hba.conf:
sudo su postgres
nano /etc/postgresql/10/main/pg_hba.conf
and added this line:
local   all             username                               scram-sha-256
and changed all md5 to scram-sha-256 in that file.
As the postgres user, I created a new user with superuser rights:
sudo su postgres
psql
CREATE USER username WITH SUPERUSER PASSWORD 'password';
Then I restarted Postgres:
/etc/init.d/postgresql restart
and tried to login with pgAdmin4 where I changed the username under the database's Connection properties. But neither that nor psql -U username testdb &lt; ./testdb.sql work as I'm getting:
  FATAL: password authentication failed for user ""username""
So how can I get Postgres working with scram-sha-256 on my Debian9/KDE machine? It worked earlier when I left all the md5 in pg_hba.conf as they were.
",<postgresql><authentication><postgresql-10><sasl-scram>,833,0,15,771,2,8,27,67,18581,0.0,47,2,15,2018-11-20 16:36,2018-11-20 21:12,2018-11-20 21:12,0.0,0.0,Basic,14
50505042,MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.Util.getInstance(Util.java:381)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3558)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3490)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:919)
    at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:3996)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1284)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2137)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:776)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:352)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:284)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriverManager(DriverManagerDataSource.java:154)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriver(DriverManagerDataSource.java:145)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnectionFromDriver(AbstractDriverBasedDataSource.java:205)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnection(AbstractDriverBasedDataSource.java:169)
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122)
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180)
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43)
I get this error even after updating mysql password. I read in some posts that mysql has a new caching mechanism and all clients need to run the following which I already did but still the same error:
 update user set authentication_string='test' where user='root';
 flush privileges   
",<java><mysql><spring><tomcat>,3344,0,35,237,2,6,13,57,90098,0.0,3,8,15,2018-05-24 8:55,2018-05-24 11:03,,0.0,,Basic,14
54357532,What is the execution order of the PARTITION BY clause compared to other SQL clauses?,"I cannot find any source mentioning execution order for Partition By window functions in SQL.
Is it in the same order as Group By?
For example table like:
Select *, row_number() over (Partition by Name) 
from NPtable 
Where Name = 'Peter'
I understand if Where gets executed first, it will only look at Name = 'Peter', then execute window function that just aggregates this particular person instead of entire table aggregation, which is much more efficient.
But when the query is:
Select top 1 *, row_number() over (Partition by Name order by Date) 
from NPtable 
Where Date &gt; '2018-01-02 00:00:00'
Doesn't the window function need to be executed against the entire table first then applies the Date&gt; condition otherwise the result is wrong?
",<sql><sql-server>,749,2,11,643,1,5,15,51,10947,0.0,32,3,15,2019-01-25 0:48,2019-01-25 2:36,2019-01-25 2:36,0.0,0.0,Basic,5
50761433,How to optimize count and order by query in millions of row,"Needed help in optimizing order by and count query, I have tables having millions (approx 3 millions) rows.
I have to join 4 tables and fetch the records, When i run the simple query it takes only millisecond to complete but as I try to count or order by having left join table it get stuck for unlimited of time.
Please see the cases below.
DB Server Configuration:
CPU Number of virtual cores: 4
Memory(RAM): 16 GiB
Network Performance: High
Rows in each table:
tbl_customers -  #Rows: 20 million.
tbl_customers_address -  #Row 25 million.
tbl_shop_setting - #Rows 50k
aio_customer_tracking - #Rows 5k
Tables Schema:
CREATE TABLE `tbl_customers` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(225) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `accepts_marketing` TINYINT(1) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_order_id` BIGINT(20) NULL DEFAULT NULL,
    `total_spent` DECIMAL(12,2) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `verified_email` TINYINT(4) NULL DEFAULT NULL,
    `updated_at` DATETIME NULL DEFAULT NULL,
    `created_at` DATETIME NULL DEFAULT NULL,
    `date_updated` DATETIME NULL DEFAULT NULL,
    `date_created` DATETIME NULL DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE INDEX `shopify_customer_id_unique` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `shop_id` (`shop_id`)
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_customers_address` (
    `id` BIGINT(20) NOT NULL AUTO_INCREMENT,
    `customer_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_address_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_customer_id` BIGINT(20) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL,
    `last_name` VARCHAR(50) NULL DEFAULT NULL,
    `company` VARCHAR(50) NULL DEFAULT NULL,
    `address1` VARCHAR(250) NULL DEFAULT NULL,
    `address2` VARCHAR(250) NULL DEFAULT NULL,
    `city` VARCHAR(50) NULL DEFAULT NULL,
    `province` VARCHAR(50) NULL DEFAULT NULL,
    `country` VARCHAR(50) NULL DEFAULT NULL,
    `zip` VARCHAR(15) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL,
    `name` VARCHAR(50) NULL DEFAULT NULL,
    `province_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_name` VARCHAR(50) NULL DEFAULT NULL,
    `longitude` VARCHAR(250) NULL DEFAULT NULL,
    `latitude` VARCHAR(250) NULL DEFAULT NULL,
    `default` TINYINT(1) NULL DEFAULT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `customer_id` (`customer_id`),
    INDEX `shopify_address_id` (`shopify_address_id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_shop_setting` (
    `id` INT(11) NOT NULL AUTO_INCREMENT,   
    `shop_name` VARCHAR(300) NOT NULL COLLATE 'latin1_swedish_ci',
     PRIMARY KEY (`id`),
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `aio_customer_tracking` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(255) NULL DEFAULT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `domain` VARCHAR(255) NULL DEFAULT NULL,
    `web_session_count` INT(11) NOT NULL,
    `last_seen_date` DATETIME NULL DEFAULT NULL,
    `last_contact_date` DATETIME NULL DEFAULT NULL,
    `last_email_open` DATETIME NULL DEFAULT NULL,
    `created_date` DATETIME NOT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id_shop_id` (`shopify_customer_id`, `shop_id`),
    INDEX `last_seen_date` (`last_seen_date`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
Query Cases Running and Not Running:
1. Running:  Below query fetch the records by joining all the 4 tables, It takes only 0.300 ms.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
2. Not running: Simply when try to get the count of these row stuk the query, I waited 10 min but still running.
SELECT 
     COUNT(DISTINCT c.shopify_customer_id)   -- what makes #2 different
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
3. Not running: In the #1 query we simply put the 1 Order by clause and it get stuck, I waited 10 min but still running. I study query optimization some article and tried by indexing, Right Join etc.. but still not working.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
  ORDER BY `t`.`last_seen_date`    -- what makes #3 different
LIMIT 20
EXPLAIN QUERY #1:
EXPLAIN QUERY #2:
EXPLAIN QUERY #3:
Any suggestion to optimize the query, table structure are welcome.
WHAT I'M TRYING TO DO:
tbl_customers table contains the customer info, tbl_customer_address table contains the addresses of the customers(one customer may have multiple address), And aio_customer_tracking table contains visiting records of the customer last_seen_date is the visiting date.
Now, simply I want to fetch and count the customers, with their one of the address, and visiting info. Also, I may order by any of the column from these 3 tables, In my example i am ordering by last_seen_date (the default order). Hope this explanation helps to understand what i am trying to do. 
",<mysql><database><database-administration>,7174,3,129,688,3,13,35,57,4231,0.0,12,4,15,2018-06-08 12:44,2018-06-11 4:51,,3.0,,Intermediate,23
53663954,Trouble connecting to postgres from outside Kubernetes cluster,"I've launched a postgresql server in minikube, and I'm having difficulty connecting to it from outside the cluster.
Update
It turned out my cluster was suffering from unrelated problems, causing all sorts of broken behavior. I ended up nuking the whole cluster and vm and starting from scratch. Now I've got working. I changed the deployment to a statefulset, though I think it could work either way.
Setup and test:
kubectl --context=minikube create -f postgres-statefulset.yaml
kubectl --context=minikube create -f postgres-service.yaml
url=$(minikube service postgres --url --format={{.IP}}:{{.Port}})
psql --host=${url%:*} --port=${url#*:} --username=postgres --dbname=postgres \
     --command='SELECT refobjid FROM pg_depend LIMIT 1'
Password for user postgres:
 refobjid
----------
     1247
postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  selector:
    app: postgres
  type: NodePort
  ports:
    - name: postgres
      port: 5432
      targetPort: 5432
      protocol: TCP
postgres-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
      role: service
  serviceName: postgres
  template:
    metadata:
      labels:
        app: postgres
        role: service
    spec:
      containers:
        - name: postgres
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
              name: postgres
              protocol: TCP
Original question
I created a deployment running one container (postgres-container) and a NodePort (postgres-service). I can connect to postgresql from within the pod itself:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- psql --port=5432 --username=postgres --dbname=postgres
But I can't connect through the service.
$ minikube service --url postgres-service
http://192.168.99.100:32254
$ psql --host=192.168.99.100 --port=32254 --username=postgres --dbname=postgres
psql: could not connect to server: Connection refused
        Is the server running on host ""192.168.99.100"" and accepting
        TCP/IP connections on port 32254?
I think postgres is correctly configured to accept remote TCP connections:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- tail /var/lib/postgresql/data/pg_hba.conf
host    all             all             127.0.0.1/32            trust
...
host all all all md5
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- grep listen_addresses /var/lib/postgresql/data/postgresql.conf
listen_addresses = '*'
My service definition looks like:
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres-container
  type: NodePort
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
And the deployment is:
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-container
  template:
    metadata:
      labels:
        app: postgres-container
    spec:
      containers:
        - name: postgres-container
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
The resulting service configuration:
$ kubectl --context=minikube get service postgres-service -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-12-07T05:29:22Z
  name: postgres-service
  namespace: default
  resourceVersion: ""194827""
  selfLink: /api/v1/namespaces/default/services/postgres-service
  uid: 0da6bc36-f9e1-11e8-84ea-080027a52f02
spec:
  clusterIP: 10.109.120.251
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32254
    port: 5432
    protocol: TCP
    targetPort: 5432
  selector:
    app: postgres-container
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
I can connect if I use port-forward, but I'd like to use the nodePort instead. What am I missing?
",<postgresql><kubernetes>,4501,1,143,6082,9,43,67,74,30418,0.0,135,2,15,2018-12-07 5:47,2018-12-08 8:13,,1.0,,Intermediate,23
50547724,How to resolve the error: SQL authentication method unknown in Laravel-MySql,"I using docker and I have a container of Laravel Framework 5.5.25 and other with mysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL). in my configuration of docker compose I have this: 
version: ""2""
services:
    mysql:
    image: mysql
        ports:
            - ""3307:3306""
        command: --sql_mode=""""
So, when Laravel try to connect to MySql I have this error: 
  SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from
",<php><docker><docker-compose><laravel-5.5><mysql-8.0>,497,0,7,351,2,3,12,46,21493,0.0,25,5,15,2018-05-26 23:11,2018-06-27 7:00,,32.0,,Basic,14
48630408,SSIS Script Tasks losing code,"I have a very strange issue happening that is causing Script Task code to clear out. I have been able to test on 2-3 different machines. We are running SSDT 15.4 preview. The steps to reproduce were as follows.
Create a script task inside of a foreach loop container. 
Create a comment in the script task. 
Change or add a variable mapping in the foreach. 
Save package. 
Close the package.
Open the package.
Open the script task and the comment will have vanished.
As my last attempt for success,I have upgraded to 15.5.1 and the problem still exists.
",<sql-server><ssis><etl><sql-server-data-tools><script-task>,553,0,0,349,0,4,12,40,3756,0.0,6,3,15,2018-02-05 19:50,2018-03-27 21:04,,50.0,,Basic,14
48087980,Annotate QuerySet with first value of ordered related model,"I have a QuerySet of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM.
Background
Let's say that I have two related models: Book, and BlogPost, each with a foreign key to an Author:
class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()
class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing.
Working solution in PostgreSQL 9.6
WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
Translating to Django's ORM
While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further
books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now.
Attempted solution
I'd first built a Q object to filter down to mystery books published after the blog post.
published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
From here, I attempted to piece together django.db.models.Min and additional F objects to acheive my desired results, but with no success.
Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.
",<python><django><postgresql><django-queryset><django-1.10>,2289,0,39,16645,14,66,76,55,2258,,2247,2,15,2018-01-04 2:47,2018-01-11 9:29,2018-01-11 9:29,7.0,7.0,Intermediate,16
51227958,SQLAlchemy - check if query found any results,"How can I check if a query found any results?
result = db.engine.execute(sql, id=foo)
// check if result has rows ...
for row in result:
  ...
Tried this:
if result is None:
    print(""reuslt is None!"")
and checked length:
print(""result len"", len(result))
",<sqlalchemy><flask-sqlalchemy>,256,0,7,11791,26,86,143,68,26581,0.0,880,6,15,2018-07-08 0:31,2018-07-09 5:48,,1.0,,Basic,9
48315442,Error while exploding a struct column in Spark,"I have a dataframe whose schema looks like this:
event: struct (nullable = true)
|    | event_category: string (nullable = true)
|    | event_name: string (nullable = true)
|    | properties: struct (nullable = true)
|    |    | ErrorCode: string (nullable = true)
|    |    | ErrorDescription: string (nullable = true)
I am trying to explode the struct column properties using the following code: 
df_json.withColumn(""event_properties"", explode($""event.properties""))
But it is throwing the following exception: 
cannot resolve 'explode(`event`.`properties`)' due to data type mismatch: 
input to function explode should be array or map type, 
not StructType(StructField(IDFA,StringType,true),
How to explode the column properties?
",<scala><apache-spark><pyspark><apache-spark-sql>,732,0,13,463,1,8,18,43,30529,0.0,45,3,15,2018-01-18 6:55,2018-01-18 8:12,2018-01-18 9:48,0.0,0.0,Basic,2
51083593,Parent count based on pairing of multiple children,"In the below example, I'm trying to count the number of drinks I can make based on the availability of ingredients per bar location that I have.
To further clarify, as seen in the below example: based on the figures highlighted in the chart below; I know that I can only make 1 Margarita on 6/30/2018 (in either DC or FL if I ship the supplies to the location).
Sample of data table
Please use the below code to enter the relevant data above:
    CREATE TABLE #drinks 
    (
        a_date      DATE,
        loc         NVARCHAR(2),
        parent      NVARCHAR(20),
        line_num    INT,
        child       NVARCHAR(20),
        avail_amt   INT
    );
INSERT INTO #drinks VALUES ('6/26/2018','CA','Long Island','1','Vodka','7');
INSERT INTO #drinks VALUES ('6/27/2018','CA','Long Island','2','Gin','5');
INSERT INTO #drinks VALUES ('6/28/2018','CA','Long Island','3','Rum','26');
INSERT INTO #drinks VALUES ('6/26/2018','DC','Long Island','1','Vodka','15');
INSERT INTO #drinks VALUES ('6/27/2018','DC','Long Island','2','Gin','18');
INSERT INTO #drinks VALUES ('6/28/2018','DC','Long Island','3','Rum','5');
INSERT INTO #drinks VALUES ('6/26/2018','FL','Long Island','1','Vodka','34');
INSERT INTO #drinks VALUES ('6/27/2018','FL','Long Island','2','Gin','14');
INSERT INTO #drinks VALUES ('6/28/2018','FL','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Margarita','1','Tequila','6');
INSERT INTO #drinks VALUES ('7/1/2018','DC','Margarita','2','Triple Sec','3');
INSERT INTO #drinks VALUES ('6/29/2018','FL','Margarita','1','Tequila','1');
INSERT INTO #drinks VALUES ('6/30/2018','FL','Margarita','2','Triple Sec','0');
INSERT INTO #drinks VALUES ('7/2/2018','CA','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/8/2018','CA','Cuba Libre','2','Coke','5');
INSERT INTO #drinks VALUES ('7/13/2018','CA','Cuba Libre','3','Lime','14');
INSERT INTO #drinks VALUES ('7/5/2018','DC','Cuba Libre','1','Rum','0');
INSERT INTO #drinks VALUES ('7/19/2018','DC','Cuba Libre','2','Coke','12');
INSERT INTO #drinks VALUES ('7/31/2018','DC','Cuba Libre','3','Lime','9');
INSERT INTO #drinks VALUES ('7/2/2018','FL','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/19/2018','FL','Cuba Libre','2','Coke','3');
INSERT INTO #drinks VALUES ('7/17/2018','FL','Cuba Libre','3','Lime','2');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('7/7/2018','FL','Cosmopolitan','5','Triple Sec','7');
The expected results are as follows:
Please note, as seen in the expected results, children are interchangeable. For example, on 7/7/2018 Triple Sec arrived for the drink cosmopolitan; however because the child is also rum, it changes the availability of Margaritas for FL.
Also not the update to the DC region for Cuba Libre's on both 06/30 and 06/31.
Please take into consideration that parts are interchangeable and also that each time a new item arrives it makes available any item previously now.
Lastly - It would be awesome if I could add another column that shows kit availability regardless of location based only on availability of the child. For Ex. If there is a child #3 in DC and none in FL they FL can assume that they have enough inventory to make drink based on inventory in another location!
",<sql><sql-server><parent-child><cross-apply><outer-apply>,3294,2,34,165,0,0,6,37,375,0.0,0,4,15,2018-06-28 13:00,2018-06-29 18:15,,1.0,,Advanced,32
49370562,Comma separating numbers without advance knowledge of number of digits in Postgres,"In Postgres 9.5:
select to_char(111, 'FM9,999'); -- gives 111
select to_char(1111, 'FM9,999'); -- gives 1,111
select to_char(11111, 'FM9,999'); -- gives #,### !!!
How can I format my numbers with commas without advance knowledge/assumption of maximum number of possible digits preferably by using built-in stuff like above?
",<postgresql>,324,0,3,7848,5,44,48,37,14202,0.0,2543,2,15,2018-03-19 19:03,2018-03-19 19:08,2018-03-19 19:08,0.0,0.0,Basic,1
52243652,GCP SQL Postgres problem with privileges : can't run a query with postgres user with generated symfony db,"I am struggling to solve this problem with Google Cloud Platform's Cloud SQL component. My tech stack consists of hosting my application in a Google Kubernetes Engine (GKE) deployment, using the Cloud SQL proxy sidecar to connect to the database within the pods. The backend is a Symfony project.
I follow these steps to create and populate the database (without success):
Create Cloud SQL Postgres instance
Add proxy to k8s container to connect to the Cloud SQL instance with all credentials, as described in the GCP documentation
Enter my Symfony (phpfpm) pod and run the command php bin/console doctrine:schema:update --force to update the schema. The queries are executed in the database, so the schema is created and so on.
I try to open the database from the SQL console connection within GCP with the postgres user and try to execute a simple select * from foo; query. The response is Insufficient privilege: 7 ERROR:  permission denied for relation
How can I query the data in the database with the postgres user?
EDIT :
I have this situation about users :
     Role name     |                         Attributes                         |           Member of           
-------------------+------------------------------------------------------------+-------------------------------
 acando14          | Create role, Create DB                                     | {cloudsqlsuperuser,proxyuser}
 cloudsqladmin     | Superuser, Create role, Create DB, Replication, Bypass RLS | {}
 cloudsqlagent     | Create role, Create DB                                     | {cloudsqlsuperuser}
 cloudsqlreplica   | Replication                                                | {}
 cloudsqlsuperuser | Create role, Create DB                                     | {}
 postgres          | Create role, Create DB                                     | {cloudsqlsuperuser,acando14}
 proxyuser         | Create role, Create DB                                     | {cloudsqlsuperuser}
And I have this situation in the tables :
              List of relations
 Schema |      Name       | Type  |  Owner   
--------+-----------------+-------+----------
 public | article         | table | acando14
If I use postgres user logged in my db symfony works :
symfony =&gt; select * from article;
 id | model_id | code | size 
----+----------+------+------
(0 rows)
But if I use the server to execute the code the response is :
  SQLSTATE[42501]: Insufficient privilege: 7 ERROR: permission denied
  for relation employee at PDOException(code: 42501): SQLSTATE[42501]:
  Insufficient privilege: 7 ERROR: permission denied for relation .. at
And another problem is that I didn't generate all the tables with the command but I have to generate it executing all the queries, so strange...
Thank you, regards
",<php><postgresql><symfony><google-cloud-platform><google-cloud-sql>,2784,1,24,562,2,7,21,58,14934,0.0,16,1,15,2018-09-09 10:39,2018-09-09 12:47,2018-09-09 12:47,0.0,0.0,Basic,3
49945649,MySQL Error: Authentication plugin 'caching_sha2_password' cannot be loaded,"I just installed MySQL Ver 14.14 Distrib 5.7.22 with Homebrew on my macOS v10.13.4.
I ran the command:
brew install mysql 
After the installation completed, as directed by Homebrew, I ran the command:
mysql_secure_installation
and was returned the error: Error: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found   
I tried a few things like changing default_authentication_plugin to mysql_native_password in the my.cnf file but it still throws the same error. 
Next I tried running:
mysql_upgrade -u root
and I was thrown the same error again mysql_upgrade: Got error: 2059: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found while connecting to the MySQL server
Upgrade process encountered error and will not continue.
Any help is appreciated.
",<mysql><macos><macos-high-sierra>,962,0,6,587,2,6,15,75,29574,0.0,242,4,15,2018-04-20 16:01,2018-04-20 16:57,2018-04-21 12:50,0.0,1.0,Basic,9
