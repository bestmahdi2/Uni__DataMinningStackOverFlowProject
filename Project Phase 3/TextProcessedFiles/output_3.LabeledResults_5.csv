QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition,MergedText,ProcessedText
51534758,Visual Studio Code SQL Syntax Highlighting in .py Files,"I am switching over from atom to VSCode and finding it to be a way better experience for (mostly) python.
One thing I can't seem to work out is that the python syntax highlighting on atom recognised SQL in strings and highlighted it.
I can't seem to find an extension for VSCode to do the same thing.
Does one exist or is there a way to get this highlighting in VSCode?
",<python><sql><visual-studio-code><syntax-highlighting>,370,2,0,553,1,5,17,39,10424,0.0,27,4,29,2018-07-26 8:50,2019-03-26 23:27,,243.0,,Intermediate,20,"<python><sql><visual-studio-code><syntax-highlighting>, Visual Studio Code SQL Syntax Highlighting in .py Files, I am switching over from atom to VSCode and finding it to be a way better experience for (mostly) python.
One thing I can't seem to work out is that the python syntax highlighting on atom recognised SQL in strings and highlighted it.
I can't seem to find an extension for VSCode to do the same thing.
Does one exist or is there a way to get this highlighting in VSCode?
","<patron><sal><visual-studio-code><santa-highlighting>, visual studio code sal santa highlight .i files, switch atom score find way better expert (mostly) patron. one thing can't seem work patron santa highlight atom recognise sal string highlight it. can't seem find extent score thing. one exist way get highlight score?"
53471882,"MySQL Workbench reports ""is not valid at this position for this server version"" error","For the following SQL query:
SELECT COUNT (distinct first_name) from actor;
I receive the following error message:
""SELECT"" is not valid at this position for this server version, expecting: '(', WITH
I am a total newbie at SQL. How do I resolve this error?
I put the exact same line at another PC with the exact same schema and it worked fine.
",<mysql><mysql-workbench>,344,0,2,292,1,3,6,45,150690,0.0,1,4,28,2018-11-25 20:54,2018-11-25 21:11,2018-11-25 21:11,0.0,0.0,Basic,9,"<mysql><mysql-workbench>, MySQL Workbench reports ""is not valid at this position for this server version"" error, For the following SQL query:
SELECT COUNT (distinct first_name) from actor;
I receive the following error message:
""SELECT"" is not valid at this position for this server version, expecting: '(', WITH
I am a total newbie at SQL. How do I resolve this error?
I put the exact same line at another PC with the exact same schema and it worked fine.
","<myself><myself-workbench>, myself workbench report ""i valid post server version"" error, follow sal query: select count (distinct first_name) actor; receive follow error message: ""select"" valid post server version, expecting: '(', total newby sal. resolve error? put exact line not pp exact scheme work fine."
59607616,When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?,"Most applications I have programmed do not use MultipleActiveResultSets=True, but I have seen the option being enabled in a couple of them and in a few tutorials.
This SO question deals with the same topic, but it is very old and I believe that things have changed much in the mean time.
OP argues about executing some non-queries, while performing an ExecuteReader. In this case I believe it to be a bad design since it might be replaced with some batch-style operation, perhaps a stored procedure to minimize the number of round-trips.
When using Entity Framework with ASP.NET Core and receiving an exception related to the data context executing already something in the scope, I treat it as a bug and not thinking about enabling MARS.
Reading this MS Docs article I see that one should pay attention to various aspects such as options (ANSI_NULLS, DATE_FORMAT, LANGUAGE, TEXTSIZE), security context, current database, state variables (@@ERROR, @@ROWCOUNT, @@FETCH_STATUS, @@IDENTITY) when working with MARS enabled.
Also, 10+ years mean much more capable servers being able to hold much more connections if this is really needed (caching should help reduce this need).
So I am wondering if I ever have to consider enabling MARS when working with modern ASP.NET Core applications (3.0+).
Question: When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?
Edit to address feedback
I am not interested in an exhaustive analysis, but a couple of appropriate contexts to justify using MARS or not.
A typical example in ASP.NET Core applications is to have database context as scoped (get a database connection from the connection pool per request, make changes, usually one transaction per request/scope). So far, I have treated errors related to multiple queries per connection as my own fault to avoid MARS, but I did so without understanding actually why.
",<sql-server><asp.net-core><entity-framework-core><sql-server-mars>,1906,2,10,22299,19,146,170,40,12139,0.0,7185,1,28,2020-01-06 6:30,2021-04-10 17:19,2021-04-10 17:19,460.0,460.0,Basic,9,"<sql-server><asp.net-core><entity-framework-core><sql-server-mars>, When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?, Most applications I have programmed do not use MultipleActiveResultSets=True, but I have seen the option being enabled in a couple of them and in a few tutorials.
This SO question deals with the same topic, but it is very old and I believe that things have changed much in the mean time.
OP argues about executing some non-queries, while performing an ExecuteReader. In this case I believe it to be a bad design since it might be replaced with some batch-style operation, perhaps a stored procedure to minimize the number of round-trips.
When using Entity Framework with ASP.NET Core and receiving an exception related to the data context executing already something in the scope, I treat it as a bug and not thinking about enabling MARS.
Reading this MS Docs article I see that one should pay attention to various aspects such as options (ANSI_NULLS, DATE_FORMAT, LANGUAGE, TEXTSIZE), security context, current database, state variables (@@ERROR, @@ROWCOUNT, @@FETCH_STATUS, @@IDENTITY) when working with MARS enabled.
Also, 10+ years mean much more capable servers being able to hold much more connections if this is really needed (caching should help reduce this need).
So I am wondering if I ever have to consider enabling MARS when working with modern ASP.NET Core applications (3.0+).
Question: When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?
Edit to address feedback
I am not interested in an exhaustive analysis, but a couple of appropriate contexts to justify using MARS or not.
A typical example in ASP.NET Core applications is to have database context as scoped (get a database connection from the connection pool per request, make changes, usually one transaction per request/scope). So far, I have treated errors related to multiple queries per connection as my own fault to avoid MARS, but I did so without understanding actually why.
","<sal-server><asp.net-core><entity-framework-core><sal-server-mars>, use multipleactiveresultsets=true work asp.net core 3.0 sal server 2019+?, applied program use multipleactiveresultsets=true, seen option enable couple tutorials. question deal topic, old believe thing change much mean time. op argue execute non-queried, perform executereader. case believe bad design since might replace batch-style operation, perhaps store procedure minims number round-trips. use entity framework asp.net core receive except relate data context execute already cometh scope, treat bug think enable mars. read ms do article see one pay attend various aspect option (ansi_nulls, date_format, language, textile), secure context, current database, state variable (@@error, @@rowcount, @@fetch_status, @@identity) work may enabled. also, 10+ year mean much capable server all hold much connect really need (each help reduce need). wonder ever consider enable may work modern asp.net core applied (3.0+). question: use multipleactiveresultsets=true work asp.net core 3.0 sal server 2019+? edit address feedback interest exhaust analysis, couple appropri context justify use may not. topic example asp.net core applied database context scope (get database connect connect pool per request, make changes, usual one transact per request/scope). far, treat error relate multiple query per connect fault avoid mars, without understand actual why."
52802521,How can I get time.Time in Golang protobuf v3 struct?,"I'm using the google time package github.com/golang/protobuf/ptypes/timestamp in protobuf message file  now.
google.protobuf.Timestamp UpdateTime = 9;
But the UpdateTime property becomes a pointer *timestamp.Timestamp in golang struct after protoc compiling, it's not a time.Time and I can't save these property into Mysql timestamp column.
What can I do?
",<mysql><go><timestamp><protocol-buffers>,356,0,5,427,1,5,6,59,44394,0.0,0,2,28,2018-10-14 12:14,2018-10-14 14:53,,0.0,,Basic,9,"<mysql><go><timestamp><protocol-buffers>, How can I get time.Time in Golang protobuf v3 struct?, I'm using the google time package github.com/golang/protobuf/ptypes/timestamp in protobuf message file  now.
google.protobuf.Timestamp UpdateTime = 9;
But the UpdateTime property becomes a pointer *timestamp.Timestamp in golang struct after protoc compiling, it's not a time.Time and I can't save these property into Mysql timestamp column.
What can I do?
","<myself><go><timestamp><protocol-suffers>, get time.tim going protobuf ve struck?, i'm use good time package github.com/going/protobuf/types/timestamp protobuf message file now. goose.protobuf.timestamp updatetim = 9; updatetim property become pointer *timestamp.timestamp going struck proto complying, time.tim can't save property myself timestamp column. do?"
53623048,Restore database in docker container,"Getting an error below when restoring a AdventureWorks2017 database within a docker container.
Running SQL Server 2019 CTP 2.0 (mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Both backup and target data volume are persisted. 
No problems creating new database. 
Checked the paths and they are correct. Do not have any problems when restoring using 2017-latest docker image.
Anybody else have this issue with 2019-CTP2, workarounds?
  Msg 3634, Level 16, State 1, Line 7 The operating system returned the
  error '2(The system cannot find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017' cannot be restored to
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3634, Level 16, State 1,
  Line 7 The operating system returned the error '2(The system cannot
  find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017_log' cannot be restored to
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3119, Level 16, State 1,
  Line 7 Problems were identified while planning for the RESTORE
  statement. Previous messages provide details. Msg 3013, Level 16,
  State 1, Line 7 RESTORE DATABASE is terminating abnormally.
to create container.
$datapath = ""D:\Foo"";
$logpath = ""D:\Foo"";
$backuppath = ""D:\Foo"";
$pass = "":-)""
$ct = (docker run -e ""ACCEPT_EULA=Y"" -e ""SA_PASSWORD=$pass"" `
    -e ""MSSQL_PID=Developer"" -p 2017:1433 `
    -e ""MSSQL_TCP_PORT=1433"" `
    -v ${datapath}:/var/opt/mssql/data `
    -v ${logpath}:/var/opt/mssql/log `
    -v ${backuppath}:/var/opt/mssql/backup `
    -e ""MSSQL_BACKUP_DIR=/var/opt/mssql/backup"" `
    -e ""MSSQL_DATA_DIR=/var/opt/mssql/data"" ` 
    -e ""MSSQL_LOG_DIR=/var/opt/mssql/log"" `
    -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Restore command.
RESTORE DATABASE [AdventureWorks2017] FROM  DISK = N'/var/opt/mssql/backup/AdventureWorks2017.bak' 
WITH  FILE = 1,  
MOVE N'AdventureWorks2017' TO N'/var/opt/mssql/data/AdventureWorks2017.mdf',  
MOVE N'AdventureWorks2017_log' TO N'/var/opt/mssql/log/AdventureWorks2017_log.ldf', 
NOUNLOAD,  STATS = 1 
",<sql-server><docker><sql-server-2019>,2430,0,20,1142,1,8,14,50,15652,0.0,52,7,28,2018-12-04 23:41,2018-12-05 5:40,2018-12-14 1:39,1.0,10.0,Basic,14,"<sql-server><docker><sql-server-2019>, Restore database in docker container, Getting an error below when restoring a AdventureWorks2017 database within a docker container.
Running SQL Server 2019 CTP 2.0 (mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Both backup and target data volume are persisted. 
No problems creating new database. 
Checked the paths and they are correct. Do not have any problems when restoring using 2017-latest docker image.
Anybody else have this issue with 2019-CTP2, workarounds?
  Msg 3634, Level 16, State 1, Line 7 The operating system returned the
  error '2(The system cannot find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017' cannot be restored to
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3634, Level 16, State 1,
  Line 7 The operating system returned the error '2(The system cannot
  find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017_log' cannot be restored to
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3119, Level 16, State 1,
  Line 7 Problems were identified while planning for the RESTORE
  statement. Previous messages provide details. Msg 3013, Level 16,
  State 1, Line 7 RESTORE DATABASE is terminating abnormally.
to create container.
$datapath = ""D:\Foo"";
$logpath = ""D:\Foo"";
$backuppath = ""D:\Foo"";
$pass = "":-)""
$ct = (docker run -e ""ACCEPT_EULA=Y"" -e ""SA_PASSWORD=$pass"" `
    -e ""MSSQL_PID=Developer"" -p 2017:1433 `
    -e ""MSSQL_TCP_PORT=1433"" `
    -v ${datapath}:/var/opt/mssql/data `
    -v ${logpath}:/var/opt/mssql/log `
    -v ${backuppath}:/var/opt/mssql/backup `
    -e ""MSSQL_BACKUP_DIR=/var/opt/mssql/backup"" `
    -e ""MSSQL_DATA_DIR=/var/opt/mssql/data"" ` 
    -e ""MSSQL_LOG_DIR=/var/opt/mssql/log"" `
    -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Restore command.
RESTORE DATABASE [AdventureWorks2017] FROM  DISK = N'/var/opt/mssql/backup/AdventureWorks2017.bak' 
WITH  FILE = 1,  
MOVE N'AdventureWorks2017' TO N'/var/opt/mssql/data/AdventureWorks2017.mdf',  
MOVE N'AdventureWorks2017_log' TO N'/var/opt/mssql/log/AdventureWorks2017_log.ldf', 
NOUNLOAD,  STATS = 1 
","<sal-server><doctor><sal-server-2019>, restore database doctor container, get error restore adventureworks2017 database within doctor container. run sal server 2019 cap 2.0 (mr.microsoft.com/mssql/server:next-cap.0-bunt) back target data volume persisted. problem great new database. check path correct. problem restore use 2017-latest doctor image. anybody else issue 2019-cap, workarounds? mug 3634, level 16, state 1, line 7 over system return error '2(the system cannot find file specified.)' attempt 'restorecontainer::validatetargetforcreation' '/war/opt/mssql/data/adventureworks2017.md'. mug 3156, level 16, state 5, line 7 file 'adventureworks2017' cannot restore '/war/opt/mssql/data/adventureworks2017.md'. use move identify valid local file. mug 3634, level 16, state 1, line 7 over system return error '2(the system cannot find file specified.)' attempt 'restorecontainer::validatetargetforcreation' '/war/opt/mssql/log/adventureworks2017_log.of'. mug 3156, level 16, state 5, line 7 file 'adventureworks2017_log' cannot restore '/war/opt/mssql/log/adventureworks2017_log.of'. use move identify valid local file. mug 3119, level 16, state 1, line 7 problem identify plan restore statement. previous message proved details. mug 3013, level 16, state 1, line 7 restore database german abnormally. great container. $datapath = ""d:\foo""; $loath = ""d:\foo""; $backuppath = ""d:\foo""; $pass = "":-)"" $it = (doctor run -e ""accept_eula=y"" -e ""sa_password=$pass"" ` -e ""mssql_pid=developer"" -p 2017:1433 ` -e ""mssql_tcp_port=1433"" ` -v ${datapath}:/war/opt/mssql/data ` -v ${loath}:/war/opt/mssql/log ` -v ${backuppath}:/war/opt/mssql/back ` -e ""mssql_backup_dir=/war/opt/mssql/back"" ` -e ""mssql_data_dir=/war/opt/mssql/data"" ` -e ""mssql_log_dir=/war/opt/mssql/log"" ` -d mr.microsoft.com/mssql/server:next-cap.0-bunt) restore command. restore database [adventureworks2017] disk = n'/war/opt/mssql/back/adventureworks2017.back' file = 1, move n'adventureworks2017' n'/war/opt/mssql/data/adventureworks2017.md', move n'adventureworks2017_log' n'/war/opt/mssql/log/adventureworks2017_log.of', download, state = 1"
51117503,"Python 3.7, Failed building wheel for MySql-Python","I am new to python and I am trying django framework that involves some MySql and ran into this error when try to do pip install mysqlclient and down the lines of cmd messages I got this.
   Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.7
    copying _mysql_exceptions.py -&gt; build\lib.win32-3.7
    creating build\lib.win32-3.7\MySQLdb
    copying MySQLdb\__init__.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\compat.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\connections.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\converters.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\cursors.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\release.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\times.py -&gt; build\lib.win32-3.7\MySQLdb
    creating build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\__init__.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CR.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\ER.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FLAG.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    running build_ext
    building '_mysql' extension
    creating build\temp.win32-3.7
    creating build\temp.win32-3.7\Release
    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\bin\HostX86\x86\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win32-3.7\Release\_mysql.obj /Zl
    _mysql.c
    _mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe' failed with exit status 2
&gt; 
&gt; 
&gt; Command ""c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient"" failed with error code 1 in C:\Users\RONANL~1\AppData\Local\Temp\pip-install-pkbqy3t3\mysqlclient\
anyone knows how to fix this ?
",<python><mysql><django><python-3.x><mysql-python>,4457,0,40,389,1,4,13,59,90537,0.0,27,13,28,2018-06-30 18:27,2018-10-04 11:24,,96.0,,Basic,14,"<python><mysql><django><python-3.x><mysql-python>, Python 3.7, Failed building wheel for MySql-Python, I am new to python and I am trying django framework that involves some MySql and ran into this error when try to do pip install mysqlclient and down the lines of cmd messages I got this.
   Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.7
    copying _mysql_exceptions.py -&gt; build\lib.win32-3.7
    creating build\lib.win32-3.7\MySQLdb
    copying MySQLdb\__init__.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\compat.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\connections.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\converters.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\cursors.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\release.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\times.py -&gt; build\lib.win32-3.7\MySQLdb
    creating build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\__init__.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CR.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\ER.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FLAG.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    running build_ext
    building '_mysql' extension
    creating build\temp.win32-3.7
    creating build\temp.win32-3.7\Release
    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\bin\HostX86\x86\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win32-3.7\Release\_mysql.obj /Zl
    _mysql.c
    _mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe' failed with exit status 2
&gt; 
&gt; 
&gt; Command ""c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient"" failed with error code 1 in C:\Users\RONANL~1\AppData\Local\Temp\pip-install-pkbqy3t3\mysqlclient\
anyone knows how to fix this ?
","<patron><myself><django><patron-3.x><myself-patron>, patron 3.7, fail build wheel myself-patron, new patron try django framework involve myself ran error try pp instal mysqlclient line cod message got this. fail build wheel mysqlclient run set.i clean mysqlclient fail build mysqlclient instal collect packages: mysqlclient run set.i instal mysqlclient ... error complete output command c:\users\royal~1\ends\by\script\patron.ex -u -c ""import setuptools, tokenize;__file__='c:\\users\\royal~1\\appdata\\local\\hemp\\pp-install-pkbqy3t3\\mysqlclient\\set.by';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();even(compile(code, __file__, 'even'))"" instal --record c:\users\royal~1\appdata\local\hemp\pp-record-moxwf7lu\install-record.txt --single-version-externally-manage --compel --install-head c:\users\royal~1\ends\by\include\site\python3.7\mysqlclient: run instal run build run build_pi great build great build\limb.wine-3.7 copy _mysql_exceptions.i -&it; build\limb.wine-3.7 great build\limb.wine-3.7\mysqldb copy mysqldb\__init__.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\compact.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\connections.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\converted.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\curious.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\release.i -&it; build\limb.wine-3.7\mysqldb copy mysqldb\times.i -&it; build\limb.wine-3.7\mysqldb great build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\__init__.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\client.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\or.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\er.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\field_type.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\flag.i -&it; build\limb.wine-3.7\mysqldb\cost copy mysqldb\constant\refresh.i -&it; build\limb.wine-3.7\mysqldb\cost run build_ext build '_mysql' extent great build\hemp.wine-3.7 great build\hemp.wine-3.7\release c:\program file (x)\microsoft visual studio\2017\community\ve\tools\move\14.14.26428\bin\hostx86\x\ll.ex /c /nologo /ox /we /go /dndebug /md -dversion_info=(1,3,13,'final',0) -d__version__=1.3.13 ""-in:\program file (x)\myself\myself connection c 6.1\include"" ""-in:\users\roman line\appdata\local\programs\patron\python37-32\include"" ""-in:\users\roman line\appdata\local\programs\patron\python37-32\include"" ""-in:\program file (x)\microsoft visual studio\2017\community\ve\tools\move\14.14.26428\atomic\include"" ""-in:\program file (x)\microsoft visual studio\2017\community\ve\tools\move\14.14.26428\include"" ""-in:\program file (x)\window kits\netfxsdk\4.6.1\include\up"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\curt"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\shared"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\up"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\wirt"" ""-in:\program file (x)\window kits\10\include\10.0.17134.0\cppwinrt"" /tc_mysql.c /build\hemp.wine-3.7\release\_mysql.obs /ll _mysql.c _mysql.c(29): fatal error c1083: cannot open include file: 'myself.h': file director error: command 'c:\\program file (x)\\microsoft visual studio\\2017\\community\\ve\\tools\\move\\14.14.26428\\bin\\hostx86\\x\\ll.eye' fail exit state 2 &it; &it; &it; command ""c:\users\royal~1\ends\by\script\patron.ex -u -c ""import setuptools, tokenize;__file__='c:\\users\\royal~1\\appdata\\local\\hemp\\pp-install-pkbqy3t3\\mysqlclient\\set.by';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();even(compile(code, __file__, 'even'))"" instal --record c:\users\royal~1\appdata\local\hemp\pp-record-moxwf7lu\install-record.txt --single-version-externally-manage --compel --install-head c:\users\royal~1\ends\by\include\site\python3.7\mysqlclient"" fail error code 1 c:\users\royal~1\appdata\local\hemp\pp-install-pkbqy3t3\mysqlclient\ anyone know fix ?"
49228926,Get city name either do not start with vowels or do not end with vowels,"Query the list of CITY names from STATION that either do not start with vowels or do not end with vowels. Your result cannot contain duplicates.
Input Format
The STATION table is described as follows:
I write the below query, but it's not working for me. Any suggestion?
select distinct city
from station
where city regexp '^[^aeiou].*[^aeiou]$'; 
",<sql>,348,1,3,407,1,4,5,65,171270,0.0,0,50,28,2018-03-12 5:55,2018-03-12 6:07,2018-03-12 6:07,0.0,0.0,Basic,10,"<sql>, Get city name either do not start with vowels or do not end with vowels, Query the list of CITY names from STATION that either do not start with vowels or do not end with vowels. Your result cannot contain duplicates.
Input Format
The STATION table is described as follows:
I write the below query, but it's not working for me. Any suggestion?
select distinct city
from station
where city regexp '^[^aeiou].*[^aeiou]$'; 
","<sal>, get city name either start towel end bowels, query list city name station either start towel end bowels. result cannot contain duplicates. input format station table describe follows: write query, work me. suggestion? select distinct city station city regent '^[^adieu].*[^adieu]$';"
54690701,Is there a way to ensure WHERE clause happens after DISTINCT?,"Imagine you have a table comments in your database.
The comment table has the columns, id, text, show, comment_id_no.
If a user enters a comment, it inserts a row into the database
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
If a user wants to update that comment it inserts a new row into the db
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
| 2  | 1              | hey  | true | 1/1/2001    |
Notice it keeps the same comment_id_no. This is so we will be able to see the history of a comment.
Now the user decides that they no longer want to display their comment
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
This hides the comment from the end users. 
Now a second comment is made (not an update of the first)
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
| 4  | 2              | new  | true  | 1/1/2003    |
What I would like to be able to do is select all the latest versions of unique commend_id_no, where show is equal to true. However, I do not want the query to return id=2.
Steps the query needs to take...
select all the most recent, distinct comment_id_nos. (should return id=3 and id=4)
select where show = true (should only return id=4)
  Note: I am actually writing this query in elixir using ecto and would like to be able to do this without using the subquery function. If anyone can answer this in sql I can convert the answer myself. If anyone knows how to answer this in elixir then also feel free to answer. 
",<sql><postgresql><elixir><distinct><where-clause>,2091,0,31,1624,1,17,28,42,2481,0.0,344,5,28,2019-02-14 12:43,2019-02-14 12:45,2019-02-14 12:58,0.0,0.0,Basic,10,"<sql><postgresql><elixir><distinct><where-clause>, Is there a way to ensure WHERE clause happens after DISTINCT?, Imagine you have a table comments in your database.
The comment table has the columns, id, text, show, comment_id_no.
If a user enters a comment, it inserts a row into the database
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
If a user wants to update that comment it inserts a new row into the db
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
| 2  | 1              | hey  | true | 1/1/2001    |
Notice it keeps the same comment_id_no. This is so we will be able to see the history of a comment.
Now the user decides that they no longer want to display their comment
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
This hides the comment from the end users. 
Now a second comment is made (not an update of the first)
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
| 4  | 2              | new  | true  | 1/1/2003    |
What I would like to be able to do is select all the latest versions of unique commend_id_no, where show is equal to true. However, I do not want the query to return id=2.
Steps the query needs to take...
select all the most recent, distinct comment_id_nos. (should return id=3 and id=4)
select where show = true (should only return id=4)
  Note: I am actually writing this query in elixir using ecto and would like to be able to do this without using the subquery function. If anyone can answer this in sql I can convert the answer myself. If anyone knows how to answer this in elixir then also feel free to answer. 
","<sal><postgresql><elicit><distinct><where-clause>, way ensue class happen distinct?, imagine table comment database. comment table columns, id, text, show, comment_id_no. user enter comment, insert row database | id | comment_id_no | text | show | inserted_at | | -- | -------------- | ---- | ---- | ----------- | | 1 | 1 | hi | true | 1/1/2000 | user want update comment insert new row do | id | comment_id_no | text | show | inserted_at | | -- | -------------- | ---- | ---- | ----------- | | 1 | 1 | hi | true | 1/1/2000 | | 2 | 1 | hey | true | 1/1/2001 | notice keep comment_id_no. all see history comment. user decide longer want display comment | id | comment_id_no | text | show | inserted_at | | -- | -------------- | ---- | ----- | ----------- | | 1 | 1 | hi | true | 1/1/2000 | | 2 | 1 | hey | true | 1/1/2001 | | 3 | 1 | hey | fall | 1/1/2002 | hide comment end users. second comment made (not update first) | id | comment_id_no | text | show | inserted_at | | -- | -------------- | ---- | ----- | ----------- | | 1 | 1 | hi | true | 1/1/2000 | | 2 | 1 | hey | true | 1/1/2001 | | 3 | 1 | hey | fall | 1/1/2002 | | 4 | 2 | new | true | 1/1/2003 | would like all select latest version unique commend_id_no, show equal true. however, want query return id=2. step query need take... select recent, distinct comment_id_nos. (should return id=3 id=4) select show = true (should return id=4) note: actual write query elicit use echo would like all without use subqueri function. anyone answer sal convert answer myself. anyone know answer elicit also feel free answer."
51278467,"MySQL collation: utf8mb4_unicode_ci vs ""utf8mb4 - default collation""","Please help me to understand the differences between the collations listed in MySQL Workbench:
utf8mb4_unicode_ci vs utf8mb4 - default collation
p.s. Everyone is recommending using utf8mb4_unicode_ci. If this is so popular why it is not default? What differs it from the default?
I use MySQL 5.7.21.
",<mysql><mysql-workbench><collation>,300,1,3,38596,26,175,195,35,25212,0.0,2983,1,28,2018-07-11 6:19,2018-07-11 15:18,,0.0,,Intermediate,19,"<mysql><mysql-workbench><collation>, MySQL collation: utf8mb4_unicode_ci vs ""utf8mb4 - default collation"", Please help me to understand the differences between the collations listed in MySQL Workbench:
utf8mb4_unicode_ci vs utf8mb4 - default collation
p.s. Everyone is recommending using utf8mb4_unicode_ci. If this is so popular why it is not default? What differs it from the default?
I use MySQL 5.7.21.
","<myself><myself-workbench><collection>, myself collection: utf8mb4_unicode_ci vs ""utf8mb4 - default collection"", pleas help understand differ collar list myself workbench: utf8mb4_unicode_ci vs utf8mb4 - default collar p.s. everyone recommend use utf8mb4_unicode_ci. popular default? differ default? use myself 5.7.21."
51959944,"SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1","I am getting below exception only in android 9, after reinstalling everything looks good,
Exception:
android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1...
Code:
Cursor cursor = database.query(......);
    if(cursor == null || cursor.getCount() &lt; 0) { //Here is the error
        Log.d(""Error"", ""count : null"");
        return """";
    }
Edited:
java.lang.RuntimeException: An error occurred while executing doInBackground()
at android.os.AsyncTask$3.done(AsyncTask.java:354)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:383)
at java.util.concurrent.FutureTask.setException(FutureTask.java:252)
at java.util.concurrent.FutureTask.run(FutureTask.java:271)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
at java.lang.Thread.run(Thread.java:764)
Caused by: android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1
at android.database.sqlite.SQLiteConnection.nativeExecuteForCursorWindow(Native Method)
at android.database.sqlite.SQLiteConnection.executeForCursorWindow(SQLiteConnection.java:859)
at android.database.sqlite.SQLiteSession.executeForCursorWindow(SQLiteSession.java:836)
at android.database.sqlite.SQLiteQuery.fillWindow(SQLiteQuery.java:62)
at android.database.sqlite.SQLiteCursor.fillWindow(SQLiteCursor.java:149)
at android.database.sqlite.SQLiteCursor.getCount(SQLiteCursor.java:137)
Thanks in advance guys
",<android><sqlite><android-database>,1586,0,22,322,1,3,10,74,30636,,9,4,28,2018-08-22 4:35,2019-04-24 10:48,,245.0,,Intermediate,24,"<android><sqlite><android-database>, SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1, I am getting below exception only in android 9, after reinstalling everything looks good,
Exception:
android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1...
Code:
Cursor cursor = database.query(......);
    if(cursor == null || cursor.getCount() &lt; 0) { //Here is the error
        Log.d(""Error"", ""count : null"");
        return """";
    }
Edited:
java.lang.RuntimeException: An error occurred while executing doInBackground()
at android.os.AsyncTask$3.done(AsyncTask.java:354)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:383)
at java.util.concurrent.FutureTask.setException(FutureTask.java:252)
at java.util.concurrent.FutureTask.run(FutureTask.java:271)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
at java.lang.Thread.run(Thread.java:764)
Caused by: android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1
at android.database.sqlite.SQLiteConnection.nativeExecuteForCursorWindow(Native Method)
at android.database.sqlite.SQLiteConnection.executeForCursorWindow(SQLiteConnection.java:859)
at android.database.sqlite.SQLiteSession.executeForCursorWindow(SQLiteSession.java:836)
at android.database.sqlite.SQLiteQuery.fillWindow(SQLiteQuery.java:62)
at android.database.sqlite.SQLiteCursor.fillWindow(SQLiteCursor.java:149)
at android.database.sqlite.SQLiteCursor.getCount(SQLiteCursor.java:137)
Thanks in advance guys
","<andros><quite><andros-database>, sqliteblobtoobigexception: row big fit cursorwindow requiredpos=0, totalrows=1, get except andros 9, rental every look good, exception: andros.database.quite.sqliteblobtoobigexception: row big fit cursorwindow requiredpos=0, totalrows=1... code: curses curses = database.query(......); if(curses == null || curses.getcount() &it; 0) { //here error log.d(""error"", ""count : null""); return """"; } edited: cava.long.runtimeexception: error occur execute doinbackground() andros.os.asynctask$3.done(asynctask.cava:354) cava.until.concurrent.futuretask.finishcompletion(futuretask.cava:383) cava.until.concurrent.futuretask.setexception(futuretask.cava:252) cava.until.concurrent.futuretask.run(futuretask.cava:271) cava.until.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.cava:1167) cava.until.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.cava:641) cava.long.thread.run(thread.cava:764) cause by: andros.database.quite.sqliteblobtoobigexception: row big fit cursorwindow requiredpos=0, totalrows=1 andros.database.quite.sqliteconnection.nativeexecuteforcursorwindow(n method) andros.database.quite.sqliteconnection.executeforcursorwindow(sqliteconnection.cava:859) andros.database.quite.sqlitesession.executeforcursorwindow(sqlitesession.cava:836) andros.database.quite.sqlitequery.fillwindow(sqlitequery.cava:62) andros.database.quite.sqlitecursor.fillwindow(sqlitecursor.cava:149) andros.database.quite.sqlitecursor.getcount(sqlitecursor.cava:137) thank advance guy"
50641841,MySQL Docker Container INFILE/ INTO OUTFILE statement on MacOS System,"I hava a Java program and a mysql Docker container (image: mysql:5.7.20).
My MacOs is High Sierra 10.13.4.
The problem in short
Using Docker on MacOS (10.13.4.). Inside a docker container (image: mysql:5.7.20) mostly the queries (executed from a java program)
LOAD DATA INFILE ...
SELECT ... INTO OUTFILE ...
are working fine, but sometimes the java program throws the exceptions:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
btw. the file exists and the permissions should be fine (see longer version)
The longer version
The process is the following:
a .csv file gets created
this .csv file is copied into a directory, which is mounted for the docker container
docker-compose volumes section: - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
then MySQL reads this .csv file into a table:
LOAD DATA INFILE '.csv-file' REPLACE INTO TABLE 'my-table';
then some stuff on that database happens
after that MySQL writes an .csv output file
SELECTtbl.sku,tbl.deleted,tbl.data_source_valuesINTO OUTFILE 'output.csv' FIELDS TERMINATED BY '|' ENCLOSED BY '""' ESCAPED BY '""' FROM (SELECT ...
This project has some java integration-tests for this process. These tests are mostly green, but sometimes they fail with:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
The docker-compose file looks like:
version: '3'
  services:
    datahub_db:
      image: ""mysql:5.7.20""
      restart: always
      environment:
        - MYSQL_ROOT_PASSWORD=${DATAHUB_DB_ROOT_PASSWORD}
        - MYSQL_DATABASE=${DATAHUB_DB_DATABASE}
      volumes:
        - ""datahub_db:/var/lib/mysql""
        - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
        - ""./data/akeneo/import:/var/lib/mysql-files/akeneo/import""
      ports:
        - ""${DATAHUB_DB_PORT}:3306""
...
volumes:
  datahub_db:
The Log from that Docker database container shows the following (but sometimes, this happened when all test are green, too)
datahub_db_1  | 2018-06-01T10:04:33.937646Z 144 [Note] Aborted connection 144 to db: 'datahub_test' user: 'root' host: '172.18.0.1' (Got an error reading communication packets)
The .csv file inside the datahub container, shows the following, fo ls -lha
root@e02e2074fb6b:/var/lib/mysql- 
files/datahub/import/test/products/kaw# ls -lha
total 4.0K
drwxr-xr-x 3 root root  96 Jun  1 09:36 .
drwxr-xr-x 3 root root  96 Jun  1 09:36 ..
-rw-r--r-- 1 root root 378 Jun  1 06:47 deactivated_product_merged_bub.csv
I think there is no problem, that this file belongs to root because mostly this file can get read by MySQL. When I change to user mysql via su mysql inside the Docker container, I get the following:
$ ls -al
total 4
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 .
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 ..
-rw-r--r-- 1 mysql mysql 378 Jun  1 06:47 deactivated_product_merged_bub.csv
Now some strange stuff happened. 
with root user, i could make a cat deactivated_product_merged_bub.csv
with mysql user i couldn't i got: 
Output:
$ cat deactivated_product_merge_bub.csv
cat: deactivated_product_merge_bub.csv: No such file or directory
I made a stat deactivated_product_merged_bub.csv as MySQL user and suddenly I could make a cat on that file (as you see I chmod 777 to that file to get the cat working - but it didn't work).
stat as root
Output:
root@e02e2074fb6b:/var/lib/mysql-files/datahub/import/test/products/kaw# stat 
deactivated_product_merged_bub.csv 
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2018-06-01 09:23:38.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
stat as mysql user
Output:
$ stat deactivated_product_merged_bub.csv
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (  999/   mysql)   Gid: (  999/   mysql)
Access: 2018-06-01 09:32:25.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
Question
Does anyone knows, what happened here or has a hint for what I could search to dig deeper?
My speculation is that it's because using Docker with MacOs and the mounted volume.
",<java><mysql><docker><docker-compose>,4985,0,73,680,0,9,21,65,3956,0.0,63,3,28,2018-06-01 10:51,2018-10-17 20:55,,138.0,,Intermediate,24,"<java><mysql><docker><docker-compose>, MySQL Docker Container INFILE/ INTO OUTFILE statement on MacOS System, I hava a Java program and a mysql Docker container (image: mysql:5.7.20).
My MacOs is High Sierra 10.13.4.
The problem in short
Using Docker on MacOS (10.13.4.). Inside a docker container (image: mysql:5.7.20) mostly the queries (executed from a java program)
LOAD DATA INFILE ...
SELECT ... INTO OUTFILE ...
are working fine, but sometimes the java program throws the exceptions:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
btw. the file exists and the permissions should be fine (see longer version)
The longer version
The process is the following:
a .csv file gets created
this .csv file is copied into a directory, which is mounted for the docker container
docker-compose volumes section: - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
then MySQL reads this .csv file into a table:
LOAD DATA INFILE '.csv-file' REPLACE INTO TABLE 'my-table';
then some stuff on that database happens
after that MySQL writes an .csv output file
SELECTtbl.sku,tbl.deleted,tbl.data_source_valuesINTO OUTFILE 'output.csv' FIELDS TERMINATED BY '|' ENCLOSED BY '""' ESCAPED BY '""' FROM (SELECT ...
This project has some java integration-tests for this process. These tests are mostly green, but sometimes they fail with:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
The docker-compose file looks like:
version: '3'
  services:
    datahub_db:
      image: ""mysql:5.7.20""
      restart: always
      environment:
        - MYSQL_ROOT_PASSWORD=${DATAHUB_DB_ROOT_PASSWORD}
        - MYSQL_DATABASE=${DATAHUB_DB_DATABASE}
      volumes:
        - ""datahub_db:/var/lib/mysql""
        - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
        - ""./data/akeneo/import:/var/lib/mysql-files/akeneo/import""
      ports:
        - ""${DATAHUB_DB_PORT}:3306""
...
volumes:
  datahub_db:
The Log from that Docker database container shows the following (but sometimes, this happened when all test are green, too)
datahub_db_1  | 2018-06-01T10:04:33.937646Z 144 [Note] Aborted connection 144 to db: 'datahub_test' user: 'root' host: '172.18.0.1' (Got an error reading communication packets)
The .csv file inside the datahub container, shows the following, fo ls -lha
root@e02e2074fb6b:/var/lib/mysql- 
files/datahub/import/test/products/kaw# ls -lha
total 4.0K
drwxr-xr-x 3 root root  96 Jun  1 09:36 .
drwxr-xr-x 3 root root  96 Jun  1 09:36 ..
-rw-r--r-- 1 root root 378 Jun  1 06:47 deactivated_product_merged_bub.csv
I think there is no problem, that this file belongs to root because mostly this file can get read by MySQL. When I change to user mysql via su mysql inside the Docker container, I get the following:
$ ls -al
total 4
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 .
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 ..
-rw-r--r-- 1 mysql mysql 378 Jun  1 06:47 deactivated_product_merged_bub.csv
Now some strange stuff happened. 
with root user, i could make a cat deactivated_product_merged_bub.csv
with mysql user i couldn't i got: 
Output:
$ cat deactivated_product_merge_bub.csv
cat: deactivated_product_merge_bub.csv: No such file or directory
I made a stat deactivated_product_merged_bub.csv as MySQL user and suddenly I could make a cat on that file (as you see I chmod 777 to that file to get the cat working - but it didn't work).
stat as root
Output:
root@e02e2074fb6b:/var/lib/mysql-files/datahub/import/test/products/kaw# stat 
deactivated_product_merged_bub.csv 
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2018-06-01 09:23:38.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
stat as mysql user
Output:
$ stat deactivated_product_merged_bub.csv
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (  999/   mysql)   Gid: (  999/   mysql)
Access: 2018-06-01 09:32:25.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
Question
Does anyone knows, what happened here or has a hint for what I could search to dig deeper?
My speculation is that it's because using Docker with MacOs and the mounted volume.
","<cava><myself><doctor><doctor-compose>, myself doctor contain inside/ outfit statement mack system, have cava program myself doctor contain (image: myself:5.7.20). mack high pierre 10.13.4. problem short use doctor mack (10.13.4.). inside doctor contain (image: myself:5.7.20) mostly query (execute cava program) load data until ... select ... outfit ... work fine, sometime cava program throw exceptions: sqlexception: can't create/writ file ‘…’ (erode: 2 - file directory) sqlexception: can't get state ‘…’ erode: 2 - file directory) sqlexception: myself server run --secure-file-prim option cannot execute statement sqlexception: file ‘…’ found (erode: 2 - file directory) bow. file exist permits fine (see longer version) longer version process following: .is file get great .is file copy directory, mount doctor contain doctor-compose volume section: - ""./data/datahub/import:/war/limb/myself-files/datahub/import"" myself read .is file table: load data until '.is-file' replace table 'my-table'; stuff database happen myself write .is output file selecttbl.sky,til.delete,til.data_source_valuesinto outfit 'output.is' field german '|' enclose '""' escape '""' (select ... project cava integration-test process. test mostly green, sometime fail with: sqlexception: can't create/writ file ‘…’ (erode: 2 - file directory) sqlexception: can't get state ‘…’ erode: 2 - file directory) sqlexception: myself server run --secure-file-prim option cannot execute statement sqlexception: file ‘…’ found (erode: 2 - file directory) doctor-compose file look like: version: '3' services: datahub_db: image: ""myself:5.7.20"" start: away environment: - mysql_root_password=${datahub_db_root_password} - mysql_database=${datahub_db_database} volumes: - ""datahub_db:/war/limb/myself"" - ""./data/datahub/import:/war/limb/myself-files/datahub/import"" - ""./data/wakened/import:/war/limb/myself-files/wakened/import"" ports: - ""${datahub_db_port}:3306"" ... volumes: datahub_db: log doctor database contain show follow (but sometimes, happen test green, too) datahub_db_1 | 2018-06-01t10:04:33.937646z 144 [note] abort connect 144 do: 'datahub_test' user: 'root' host: '172.18.0.1' (got error read common packets) .is file inside datahub container, show following, fo is -ha root@e02e2074fb6b:/war/limb/myself- files/datahub/import/test/products/saw# is -ha total 4.k drawer-or-x 3 root root 96 run 1 09:36 . drawer-or-x 3 root root 96 run 1 09:36 .. -re-r--r-- 1 root root 378 run 1 06:47 deactivated_product_merged_bub.is think problem, file belong root mostly file get read myself. change user myself via s myself inside doctor container, get following: $ is -al total 4 drawer-or-x 3 myself myself 96 run 1 09:36 . drawer-or-x 3 myself myself 96 run 1 09:36 .. -re-r--r-- 1 myself myself 378 run 1 06:47 deactivated_product_merged_bub.is strange stuff happened. root user, could make cat deactivated_product_merged_bub.is myself user got: output: $ cat deactivated_product_merge_bub.is cat: deactivated_product_merge_bub.is: file director made state deactivated_product_merged_bub.is myself user suddenly could make cat file (a see child 777 file get cat work - work). state root output: root@e02e2074fb6b:/war/limb/myself-files/datahub/import/test/products/saw# state deactivated_product_merged_bub.is file: 'deactivated_product_merged_bub.is' size: 378 blocks: 8 to block: 4194304 regular file device: of/and node: 4112125 links: 1 access: (0777/-rwxrwxrwx) did: ( 0/ root) did: ( 0/ root) access: 2018-06-01 09:23:38.000000000 +0000 modify: 2018-06-01 06:47:44.000000000 +0000 change: 2018-06-01 09:04:53.000000000 +0000 birth: - state myself user output: $ state deactivated_product_merged_bub.is file: 'deactivated_product_merged_bub.is' size: 378 blocks: 8 to block: 4194304 regular file device: of/and node: 4112125 links: 1 access: (0777/-rwxrwxrwx) did: ( 999/ myself) did: ( 999/ myself) access: 2018-06-01 09:32:25.000000000 +0000 modify: 2018-06-01 06:47:44.000000000 +0000 change: 2018-06-01 09:04:53.000000000 +0000 birth: - question anyone knows, happen hint could search dig deeper? special use doctor mack mount volume."
52697734,Postgresql | remaining connection slots are reserved for non-replication superuser connections,"I am getting an error ""remaining connection slots are reserved for non-replication superuser connections"" at one of PostgreSQL instances.
However, when I run below query from superuser to check available connections, I found that enough connections are available. But still getting the same error.
select max_conn,used,res_for_super,max_conn-used-res_for_super 
res_for_normal 
from 
  (select count(*) used from pg_stat_activity) t1,
  (select setting::int res_for_super from pg_settings where 
name='superuser_reserved_connections') t2,
  (select setting::int max_conn from pg_settings where name='max_connections') t3
Output
I searched this error and everyone is suggesting to increase the max connections like below link.
Heroku &quot;psql: FATAL: remaining connection slots are reserved for non-replication superuser connections&quot;
EDIT
I restarted the server and after some time used connections were almost 210 but i was able to connect to the server from a normal user. 
",<postgresql><postgresql-9.6>,982,2,7,1636,2,22,44,73,19593,0.0,105,2,28,2018-10-08 7:54,2021-01-03 18:35,,818.0,,Basic,6,"<postgresql><postgresql-9.6>, Postgresql | remaining connection slots are reserved for non-replication superuser connections, I am getting an error ""remaining connection slots are reserved for non-replication superuser connections"" at one of PostgreSQL instances.
However, when I run below query from superuser to check available connections, I found that enough connections are available. But still getting the same error.
select max_conn,used,res_for_super,max_conn-used-res_for_super 
res_for_normal 
from 
  (select count(*) used from pg_stat_activity) t1,
  (select setting::int res_for_super from pg_settings where 
name='superuser_reserved_connections') t2,
  (select setting::int max_conn from pg_settings where name='max_connections') t3
Output
I searched this error and everyone is suggesting to increase the max connections like below link.
Heroku &quot;psql: FATAL: remaining connection slots are reserved for non-replication superuser connections&quot;
EDIT
I restarted the server and after some time used connections were almost 210 but i was able to connect to the server from a normal user. 
","<postgresql><postgresql-9.6>, postgresql | remain connect slot reserve non-reply humerus connections, get error ""remain connect slot reserve non-reply humerus connections"" one postgresql instances. however, run query humerus check avail connections, found enough connect available. still get error. select max_conn,used,res_for_super,max_conn-used-res_for_sup res_for_norm (select count(*) use pg_stat_activity) to, (select setting::in res_for_sup pg_set name='superuser_reserved_connections') to, (select setting::in max_conn pg_set name='max_connections') to output search error everyone suggest increase max connect like link. hero &quit;pool: fatal: remain connect slot reserve non-reply humerus connections&quit; edit start server time use connect almost 210 all connect server normal user."
53730591,Sqlite DB Locked on Azure Dotnet Core Entity Framework,"I have a simple asp.net core web app (v2.1), that I deployed to a B1 (and I tried B2) Azure App Service on Linux. When I call dbContext.SaveChanges(), after adding one very simple entity, the request takes about 30 seconds before throwing the following error:
Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
Here is the code. _dbContext is injected with scoped lifetime.
public async Task&lt;IActionResult&gt; SignIn([Bind(""Email,Password,RedirectUrl"")] SignInModel model) {
    if (ModelState.IsValid) {
        var user = _dbContext.Users.Include(u =&gt; u.Claims).FirstOrDefault(u =&gt; u.UserName == model.Email);
        ...
        user.LastLogin = DateTimeOffset.Now;
        await _dbContext.SaveChangesAsync();
        ...
        return Redirect(String.IsNullOrWhiteSpace(model.RedirectUrl) ? ""/"" : model.RedirectUrl); 
    }
    else {
        return View(model);
    }
}
During the 30 seconds, I see, via SSH, that a journal file exists beside by SQLite DB file. It's eventually deleted.
UPDATE: Here are the logs. You can see, that after a single update call, a lock exception is thrown exactly 30 seconds later. 30 seconds is the SQL command timeout. I'm watching the file system using a remote SSH shell, and the journal file is there for ~30 seconds. It's like the combination of the network share used by the app service, and the SQLite file locking logic, is broken or very slow.
2018-12-20T15:06:27.660624755Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[1]
2018-12-20T15:06:27.660656156Z       Request starting HTTP/1.1 POST http://insecuresite.azurewebsites.net/Account/SignIn application/x-www-form-urlencoded 56
2018-12-20T15:06:27.660797960Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660875561Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660885462Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.660890662Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.661837484Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.661856585Z       Route matched with {action = ""SignIn"", controller = ""Account""}. Executing action InsecureSite.Controllers.AccountController.SignIn (InsecureSite)
2018-12-20T15:06:27.662465200Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.662478400Z       Executing action method InsecureSite.Controllers.AccountController.SignIn (InsecureSite) with arguments (InsecureSite.Models.SignInModel) - Validation state: Valid
2018-12-20T15:06:27.667736726Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Infrastructure[10403]
2018-12-20T15:06:27.667751427Z       Entity Framework Core 2.1.4-rtm-31024 initialized 'AppDbContext' using provider 'Microsoft.EntityFrameworkCore.Sqlite' with options: None
2018-12-20T15:06:27.716864407Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.716886507Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.716892507Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.776374136Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.776410837Z       Executed DbCommand (59ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.776514640Z       SELECT ""u"".""UserId"", ""u"".""FirstName"", ""u"".""LastLogin"", ""u"".""LastName"", ""u"".""PasswordHash"", ""u"".""UserName""
2018-12-20T15:06:27.776526140Z       FROM ""Users"" AS ""u""
2018-12-20T15:06:27.776531140Z       WHERE ""u"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.776536040Z       ORDER BY ""u"".""UserId""
2018-12-20T15:06:27.776540740Z       LIMIT 1
2018-12-20T15:06:27.778553489Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.778567689Z       Executed DbCommand (1ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.778840096Z       SELECT ""u.Claims"".""UserClaimId"", ""u.Claims"".""Claim"", ""u.Claims"".""UserId"", ""u.Claims"".""Value""
2018-12-20T15:06:27.778852696Z       FROM ""UserClaims"" AS ""u.Claims""
2018-12-20T15:06:27.778857796Z       INNER JOIN (
2018-12-20T15:06:27.778862596Z           SELECT ""u0"".""UserId""
2018-12-20T15:06:27.778869696Z           FROM ""Users"" AS ""u0""
2018-12-20T15:06:27.778874396Z           WHERE ""u0"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.778879897Z           ORDER BY ""u0"".""UserId""
2018-12-20T15:06:27.780228429Z           LIMIT 1
2018-12-20T15:06:27.780242129Z       ) AS ""t"" ON ""u.Claims"".""UserId"" = ""t"".""UserId""
2018-12-20T15:06:27.780247829Z       ORDER BY ""t"".""UserId""
2018-12-20T15:06:27.789636555Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.789651955Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.789657656Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.794111763Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.794126763Z       Executed DbCommand (4ms) [Parameters=[@p1='?', @p0='?'], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.794132363Z       UPDATE ""Users"" SET ""LastLogin"" = @p0
2018-12-20T15:06:27.794280267Z       WHERE ""UserId"" = @p1;
2018-12-20T15:06:27.794298667Z       SELECT changes();
2018-12-20T15:06:57.833069471Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Transaction[20205]
2018-12-20T15:06:57.833107571Z       An error occurred using a transaction.
2018-12-20T15:06:57.833113572Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.833118772Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.833123772Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.833128672Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.833133672Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.833138672Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.833143372Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.853805669Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Update[10000]
2018-12-20T15:06:57.853833569Z       An exception occurred in the database while saving changes for context type 'InsecureSite.Data.AppDbContext'.
2018-12-20T15:06:57.853928072Z       Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.853938272Z          at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.853943572Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854041474Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854051475Z          at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854056675Z          at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854137377Z          at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854146577Z          at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854208178Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854283180Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854292080Z          at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854299081Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.854366282Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.854384483Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854389683Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854455384Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854463685Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854468185Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854529686Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854652489Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854673890Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854748391Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.858109772Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.858123973Z       Executed action InsecureSite.Controllers.AccountController.SignIn (InsecureSite) in 30193.6715ms
2018-12-20T15:06:57.860885139Z [41m[30mfail[39m[22m[49m: Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware[1]
2018-12-20T15:06:57.860899939Z       An unhandled exception has occurred while executing the request.
2018-12-20T15:06:57.860905239Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.861009242Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.861018942Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.861023842Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.861120545Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.861130145Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.861134745Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.861237547Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.861311249Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.861320149Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861392851Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861401851Z    at InsecureSite.Controllers.AccountController.SignIn(SignInModel model) in /home/site/repository/InsecureSite/Controllers/AccountController.cs:line 57
2018-12-20T15:06:57.861463253Z    at Microsoft.AspNetCore.Mvc.Internal.ActionMethodExecutor.TaskOfIActionResultExecutor.Execute(IActionResultTypeMapper mapper, ObjectMethodExecutor executor, Object controller, Object[] arguments)
2018-12-20T15:06:57.861472053Z    at System.Threading.Tasks.ValueTask`1.get_Result()
2018-12-20T15:06:57.861541855Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()
2018-12-20T15:06:57.861550055Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()
2018-12-20T15:06:57.861554655Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)
2018-12-20T15:06:57.861629257Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861639057Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()
2018-12-20T15:06:57.861718659Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextResourceFilter()
2018-12-20T15:06:57.861727059Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Rethrow(ResourceExecutedContext context)
2018-12-20T15:06:57.861791861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861800861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeFilterPipelineAsync()
2018-12-20T15:06:57.861805561Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeAsync()
2018-12-20T15:06:57.861810161Z    at Microsoft.AspNetCore.Builder.RouterMiddleware.Invoke(HttpContext httpContext)
2018-12-20T15:06:57.861880363Z    at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861888363Z    at Microsoft.AspNetCore.StaticFiles.StaticFileMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861948164Z    at Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.862056667Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.862066767Z       Route matched with {action = ""Error"", controller = ""Home""}. Executing action InsecureSite.Controllers.HomeController.Error (InsecureSite)
2018-12-20T15:06:57.867899207Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.867914108Z       Executing action method InsecureSite.Controllers.HomeController.Error (InsecureSite) - Validation state: Valid
2018-12-20T15:06:57.868025910Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.868045011Z       Executed action method InsecureSite.Controllers.HomeController.Error (InsecureSite), returned result Microsoft.AspNetCore.Mvc.ViewResult in 0.0771ms.
2018-12-20T15:06:57.868147613Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[1]
2018-12-20T15:06:57.868157914Z       Executing ViewResult, running view Error.
2018-12-20T15:06:57.869182938Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[4]
2018-12-20T15:06:57.869196139Z       Executed ViewResult - view Error executed in 7.5623ms.
2018-12-20T15:06:57.869201439Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.869206339Z       Executed action InsecureSite.Controllers.HomeController.Error (InsecureSite) in 7.9125ms
2018-12-20T15:06:57.869222639Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[2]
2018-12-20T15:06:57.869228039Z       Request finished in 30208.5835ms 500 text/html; charset=utf-8
2018-12-20T15:08:44  No new trace in the past 1 min(s).
There is not any other requests hitting this web app. I am the only one making requests.
Controllers can read data from the DB file without errors. I see that a couple queries right before the failed write take 3ms and 6ms.
I saw there was an issue with locking and multiple threads, but it is fixed, and I'm on a later version from the fix (2.1.4)
",<linux><azure><docker><sqlite><entity-framework-core>,16441,2,143,3407,1,23,26,45,3526,0.0,152,4,28,2018-12-11 18:51,2018-12-20 8:19,2022-07-27 10:12,9.0,1324.0,Intermediate,23,"<linux><azure><docker><sqlite><entity-framework-core>, Sqlite DB Locked on Azure Dotnet Core Entity Framework, I have a simple asp.net core web app (v2.1), that I deployed to a B1 (and I tried B2) Azure App Service on Linux. When I call dbContext.SaveChanges(), after adding one very simple entity, the request takes about 30 seconds before throwing the following error:
Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
Here is the code. _dbContext is injected with scoped lifetime.
public async Task&lt;IActionResult&gt; SignIn([Bind(""Email,Password,RedirectUrl"")] SignInModel model) {
    if (ModelState.IsValid) {
        var user = _dbContext.Users.Include(u =&gt; u.Claims).FirstOrDefault(u =&gt; u.UserName == model.Email);
        ...
        user.LastLogin = DateTimeOffset.Now;
        await _dbContext.SaveChangesAsync();
        ...
        return Redirect(String.IsNullOrWhiteSpace(model.RedirectUrl) ? ""/"" : model.RedirectUrl); 
    }
    else {
        return View(model);
    }
}
During the 30 seconds, I see, via SSH, that a journal file exists beside by SQLite DB file. It's eventually deleted.
UPDATE: Here are the logs. You can see, that after a single update call, a lock exception is thrown exactly 30 seconds later. 30 seconds is the SQL command timeout. I'm watching the file system using a remote SSH shell, and the journal file is there for ~30 seconds. It's like the combination of the network share used by the app service, and the SQLite file locking logic, is broken or very slow.
2018-12-20T15:06:27.660624755Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[1]
2018-12-20T15:06:27.660656156Z       Request starting HTTP/1.1 POST http://insecuresite.azurewebsites.net/Account/SignIn application/x-www-form-urlencoded 56
2018-12-20T15:06:27.660797960Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660875561Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660885462Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.660890662Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.661837484Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.661856585Z       Route matched with {action = ""SignIn"", controller = ""Account""}. Executing action InsecureSite.Controllers.AccountController.SignIn (InsecureSite)
2018-12-20T15:06:27.662465200Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.662478400Z       Executing action method InsecureSite.Controllers.AccountController.SignIn (InsecureSite) with arguments (InsecureSite.Models.SignInModel) - Validation state: Valid
2018-12-20T15:06:27.667736726Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Infrastructure[10403]
2018-12-20T15:06:27.667751427Z       Entity Framework Core 2.1.4-rtm-31024 initialized 'AppDbContext' using provider 'Microsoft.EntityFrameworkCore.Sqlite' with options: None
2018-12-20T15:06:27.716864407Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.716886507Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.716892507Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.776374136Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.776410837Z       Executed DbCommand (59ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.776514640Z       SELECT ""u"".""UserId"", ""u"".""FirstName"", ""u"".""LastLogin"", ""u"".""LastName"", ""u"".""PasswordHash"", ""u"".""UserName""
2018-12-20T15:06:27.776526140Z       FROM ""Users"" AS ""u""
2018-12-20T15:06:27.776531140Z       WHERE ""u"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.776536040Z       ORDER BY ""u"".""UserId""
2018-12-20T15:06:27.776540740Z       LIMIT 1
2018-12-20T15:06:27.778553489Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.778567689Z       Executed DbCommand (1ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.778840096Z       SELECT ""u.Claims"".""UserClaimId"", ""u.Claims"".""Claim"", ""u.Claims"".""UserId"", ""u.Claims"".""Value""
2018-12-20T15:06:27.778852696Z       FROM ""UserClaims"" AS ""u.Claims""
2018-12-20T15:06:27.778857796Z       INNER JOIN (
2018-12-20T15:06:27.778862596Z           SELECT ""u0"".""UserId""
2018-12-20T15:06:27.778869696Z           FROM ""Users"" AS ""u0""
2018-12-20T15:06:27.778874396Z           WHERE ""u0"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.778879897Z           ORDER BY ""u0"".""UserId""
2018-12-20T15:06:27.780228429Z           LIMIT 1
2018-12-20T15:06:27.780242129Z       ) AS ""t"" ON ""u.Claims"".""UserId"" = ""t"".""UserId""
2018-12-20T15:06:27.780247829Z       ORDER BY ""t"".""UserId""
2018-12-20T15:06:27.789636555Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.789651955Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.789657656Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.794111763Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.794126763Z       Executed DbCommand (4ms) [Parameters=[@p1='?', @p0='?'], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.794132363Z       UPDATE ""Users"" SET ""LastLogin"" = @p0
2018-12-20T15:06:27.794280267Z       WHERE ""UserId"" = @p1;
2018-12-20T15:06:27.794298667Z       SELECT changes();
2018-12-20T15:06:57.833069471Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Transaction[20205]
2018-12-20T15:06:57.833107571Z       An error occurred using a transaction.
2018-12-20T15:06:57.833113572Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.833118772Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.833123772Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.833128672Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.833133672Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.833138672Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.833143372Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.853805669Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Update[10000]
2018-12-20T15:06:57.853833569Z       An exception occurred in the database while saving changes for context type 'InsecureSite.Data.AppDbContext'.
2018-12-20T15:06:57.853928072Z       Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.853938272Z          at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.853943572Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854041474Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854051475Z          at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854056675Z          at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854137377Z          at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854146577Z          at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854208178Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854283180Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854292080Z          at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854299081Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.854366282Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.854384483Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854389683Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854455384Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854463685Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854468185Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854529686Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854652489Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854673890Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854748391Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.858109772Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.858123973Z       Executed action InsecureSite.Controllers.AccountController.SignIn (InsecureSite) in 30193.6715ms
2018-12-20T15:06:57.860885139Z [41m[30mfail[39m[22m[49m: Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware[1]
2018-12-20T15:06:57.860899939Z       An unhandled exception has occurred while executing the request.
2018-12-20T15:06:57.860905239Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.861009242Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.861018942Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.861023842Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.861120545Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.861130145Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.861134745Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.861237547Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.861311249Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.861320149Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861392851Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861401851Z    at InsecureSite.Controllers.AccountController.SignIn(SignInModel model) in /home/site/repository/InsecureSite/Controllers/AccountController.cs:line 57
2018-12-20T15:06:57.861463253Z    at Microsoft.AspNetCore.Mvc.Internal.ActionMethodExecutor.TaskOfIActionResultExecutor.Execute(IActionResultTypeMapper mapper, ObjectMethodExecutor executor, Object controller, Object[] arguments)
2018-12-20T15:06:57.861472053Z    at System.Threading.Tasks.ValueTask`1.get_Result()
2018-12-20T15:06:57.861541855Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()
2018-12-20T15:06:57.861550055Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()
2018-12-20T15:06:57.861554655Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)
2018-12-20T15:06:57.861629257Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861639057Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()
2018-12-20T15:06:57.861718659Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextResourceFilter()
2018-12-20T15:06:57.861727059Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Rethrow(ResourceExecutedContext context)
2018-12-20T15:06:57.861791861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861800861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeFilterPipelineAsync()
2018-12-20T15:06:57.861805561Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeAsync()
2018-12-20T15:06:57.861810161Z    at Microsoft.AspNetCore.Builder.RouterMiddleware.Invoke(HttpContext httpContext)
2018-12-20T15:06:57.861880363Z    at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861888363Z    at Microsoft.AspNetCore.StaticFiles.StaticFileMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861948164Z    at Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.862056667Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.862066767Z       Route matched with {action = ""Error"", controller = ""Home""}. Executing action InsecureSite.Controllers.HomeController.Error (InsecureSite)
2018-12-20T15:06:57.867899207Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.867914108Z       Executing action method InsecureSite.Controllers.HomeController.Error (InsecureSite) - Validation state: Valid
2018-12-20T15:06:57.868025910Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.868045011Z       Executed action method InsecureSite.Controllers.HomeController.Error (InsecureSite), returned result Microsoft.AspNetCore.Mvc.ViewResult in 0.0771ms.
2018-12-20T15:06:57.868147613Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[1]
2018-12-20T15:06:57.868157914Z       Executing ViewResult, running view Error.
2018-12-20T15:06:57.869182938Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[4]
2018-12-20T15:06:57.869196139Z       Executed ViewResult - view Error executed in 7.5623ms.
2018-12-20T15:06:57.869201439Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.869206339Z       Executed action InsecureSite.Controllers.HomeController.Error (InsecureSite) in 7.9125ms
2018-12-20T15:06:57.869222639Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[2]
2018-12-20T15:06:57.869228039Z       Request finished in 30208.5835ms 500 text/html; charset=utf-8
2018-12-20T15:08:44  No new trace in the past 1 min(s).
There is not any other requests hitting this web app. I am the only one making requests.
Controllers can read data from the DB file without errors. I see that a couple queries right before the failed write take 3ms and 6ms.
I saw there was an issue with locking and multiple threads, but it is fixed, and I'm on a later version from the fix (2.1.4)
","<line><azure><doctor><quite><entity-framework-core>, quite do lock azur done core entity framework, simple asp.net core web pp (ve.1), deploy by (and try by) azur pp service line. call context.savechanges(), ad one simple entity, request take 30 second throw follow error: microsoft.data.quite.sqliteexcept (0x80004005): quite error 5: 'database locked'. code. _dbcontext inject scope lifetime. public async task&it;iactionresult&it; signing([bind(""email,password,redirecturl"")] signinmodel model) { (modelstate.invalid) { war user = _dbcontext.users.include(u =&it; u.claims).firstordefault(u =&it; u.usernam == model.email); ... user.lastlogin = datetimeoffset.now; await _dbcontext.savechangesasync(); ... return direct(string.isnullorwhitespace(model.redirecturl) ? ""/"" : model.redirecturl); } else { return view(model); } } 30 seconds, see, via ash, journal file exist beside quite do file. events delete. update: logs. see, single update call, lock except thrown exactly 30 second later. 30 second sal command timeout. i'm watch file system use remote ash shell, journal file ~30 seconds. like combine network share use pp service, quite file lock logic, broken slow. 2018-12-20t15:06:27.660624755z [him[32minfo[him[him[him: microsoft.aspnetcore.costing.internal.webhost[1] 2018-12-20t15:06:27.660656156z request start http/1.1 post http://insecuresite.azurewebsites.net/account/signing application/x-www-form-urlencod 56 2018-12-20t15:06:27.660797960z [15:06:27 dog] insecuresite.middleware.mycookieauthhandler:construction called. 2018-12-20t15:06:27.660875561z [15:06:27 dog] insecuresite.middleware.mycookieauthhandler:construction called. 2018-12-20t15:06:27.660885462z [15:06:27 dog] insecuresite.middleware.mycookieauthhandler:handleauthenticateasync called. 2018-12-20t15:06:27.660890662z [15:06:27 dog] insecuresite.middleware.mycookieauthhandler:handleauthenticateasync called. 2018-12-20t15:06:27.661837484z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[1] 2018-12-20t15:06:27.661856585z rout match {action = ""signing"", control = ""account""}. execute action insecuresite.controller.accountcontroller.signing (insecuresite) 2018-12-20t15:06:27.662465200z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[1] 2018-12-20t15:06:27.662478400z execute action method insecuresite.controller.accountcontroller.signing (insecuresite) argument (insecuresite.models.signinmodel) - valid state: valid 2018-12-20t15:06:27.667736726z [him[32minfo[him[him[him: microsoft.entityframeworkcore.infrastructure[10403] 2018-12-20t15:06:27.667751427z entity framework core 2.1.4-tm-31024 into 'appdbcontext' use proved 'microsoft.entityframeworkcore.quite' option: none 2018-12-20t15:06:27.716864407z [him[32minfo[him[him[him: microsoft.entityframeworkcore.database.command[20101] 2018-12-20t15:06:27.716886507z execute command (ms) [parameter=[], commandtype='text', commandtimeout='30'] 2018-12-20t15:06:27.716892507z trauma foreign_keys=on; 2018-12-20t15:06:27.776374136z [him[32minfo[him[him[him: microsoft.entityframeworkcore.database.command[20101] 2018-12-20t15:06:27.776410837z execute command (arms) [parameter=[@__model_email_0='?' (size = 18)], commandtype='text', commandtimeout='30'] 2018-12-20t15:06:27.776514640z select ""u"".""used"", ""u"".""firstname"", ""u"".""lastlogin"", ""u"".""lastname"", ""u"".""passwordhash"", ""u"".""surname"" 2018-12-20t15:06:27.776526140z ""users"" ""u"" 2018-12-20t15:06:27.776531140z ""u"".""surname"" = @__model_email_0 2018-12-20t15:06:27.776536040z order ""u"".""used"" 2018-12-20t15:06:27.776540740z limit 1 2018-12-20t15:06:27.778553489z [him[32minfo[him[him[him: microsoft.entityframeworkcore.database.command[20101] 2018-12-20t15:06:27.778567689z execute command (ms) [parameter=[@__model_email_0='?' (size = 18)], commandtype='text', commandtimeout='30'] 2018-12-20t15:06:27.778840096z select ""u.claims"".""userclaimid"", ""u.claims"".""claim"", ""u.claims"".""used"", ""u.claims"".""value"" 2018-12-20t15:06:27.778852696z ""userclaims"" ""u.claims"" 2018-12-20t15:06:27.778857796z inner join ( 2018-12-20t15:06:27.778862596z select ""up"".""used"" 2018-12-20t15:06:27.778869696z ""users"" ""up"" 2018-12-20t15:06:27.778874396z ""up"".""surname"" = @__model_email_0 2018-12-20t15:06:27.778879897z order ""up"".""used"" 2018-12-20t15:06:27.780228429z limit 1 2018-12-20t15:06:27.780242129z ) ""t"" ""u.claims"".""used"" = ""t"".""used"" 2018-12-20t15:06:27.780247829z order ""t"".""used"" 2018-12-20t15:06:27.789636555z [him[32minfo[him[him[him: microsoft.entityframeworkcore.database.command[20101] 2018-12-20t15:06:27.789651955z execute command (ms) [parameter=[], commandtype='text', commandtimeout='30'] 2018-12-20t15:06:27.789657656z trauma foreign_keys=on; 2018-12-20t15:06:27.794111763z [him[32minfo[him[him[him: microsoft.entityframeworkcore.database.command[20101] 2018-12-20t15:06:27.794126763z execute command (ms) [parameter=[@pp='?', @pp='?'], commandtype='text', commandtimeout='30'] 2018-12-20t15:06:27.794132363z update ""users"" set ""lastlogin"" = @pp 2018-12-20t15:06:27.794280267z ""used"" = @pp; 2018-12-20t15:06:27.794298667z select changes(); 2018-12-20t15:06:57.833069471z [him[30mfail[him[him[him: microsoft.entityframeworkcore.database.transaction[20205] 2018-12-20t15:06:57.833107571z error occur use transaction. 2018-12-20t15:06:57.833113572z microsoft.data.quite.sqliteexcept (0x80004005): quite error 5: 'database locked'. 2018-12-20t15:06:57.833118772z microsoft.data.quite.sqliteexception.throwexceptionforrc(into re, sqlite3 do) 2018-12-20t15:06:57.833123772z microsoft.data.quite.sqlitecommand.executereader(commandbehavior behavior) 2018-12-20t15:06:57.833128672z microsoft.data.quite.sqlitecommand.executenonquery() 2018-12-20t15:06:57.833133672z microsoft.data.quite.sqliteconnectionextensions.executenonquery(sqliteconnect connection, string commandtext) 2018-12-20t15:06:57.833138672z microsoft.data.quite.sqlitetransaction.commit() 2018-12-20t15:06:57.833143372z microsoft.entityframeworkcore.storage.relationaltransaction.commit() 2018-12-20t15:06:57.853805669z [him[30mfail[him[him[him: microsoft.entityframeworkcore.update[10000] 2018-12-20t15:06:57.853833569z except occur database save change context type 'insecuresite.data.appdbcontext'. 2018-12-20t15:06:57.853928072z microsoft.data.quite.sqliteexcept (0x80004005): quite error 5: 'database locked'. 2018-12-20t15:06:57.853938272z microsoft.data.quite.sqliteexception.throwexceptionforrc(into re, sqlite3 do) 2018-12-20t15:06:57.853943572z microsoft.data.quite.sqlitecommand.executereader(commandbehavior behavior) 2018-12-20t15:06:57.854041474z microsoft.data.quite.sqlitecommand.executenonquery() 2018-12-20t15:06:57.854051475z microsoft.data.quite.sqliteconnectionextensions.executenonquery(sqliteconnect connection, string commandtext) 2018-12-20t15:06:57.854056675z microsoft.data.quite.sqlitetransaction.commit() 2018-12-20t15:06:57.854137377z microsoft.entityframeworkcore.storage.relationaltransaction.commit() 2018-12-20t15:06:57.854146577z microsoft.entityframeworkcore.update.internal.batchexecutor.executeasync(context _, valuetuple`2 parameter, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854208178z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(ireadonlylist`1 entriestosave, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854283180z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854292080z microsoft.entityframeworkcore.context.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854299081z microsoft.data.quite.sqliteexcept (0x80004005): quite error 5: 'database locked'. 2018-12-20t15:06:57.854366282z microsoft.data.quite.sqliteexception.throwexceptionforrc(into re, sqlite3 do) 2018-12-20t15:06:57.854384483z microsoft.data.quite.sqlitecommand.executereader(commandbehavior behavior) 2018-12-20t15:06:57.854389683z microsoft.data.quite.sqlitecommand.executenonquery() 2018-12-20t15:06:57.854455384z microsoft.data.quite.sqliteconnectionextensions.executenonquery(sqliteconnect connection, string commandtext) 2018-12-20t15:06:57.854463685z microsoft.data.quite.sqlitetransaction.commit() 2018-12-20t15:06:57.854468185z microsoft.entityframeworkcore.storage.relationaltransaction.commit() 2018-12-20t15:06:57.854529686z microsoft.entityframeworkcore.update.internal.batchexecutor.executeasync(context _, valuetuple`2 parameter, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854652489z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(ireadonlylist`1 entriestosave, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854673890z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.854748391z microsoft.entityframeworkcore.context.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.858109772z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[2] 2018-12-20t15:06:57.858123973z execute action insecuresite.controller.accountcontroller.signing (insecuresite) 30193.6715m 2018-12-20t15:06:57.860885139z [him[30mfail[him[him[him: microsoft.aspnetcore.diagnostic.exceptionhandlermiddleware[1] 2018-12-20t15:06:57.860899939z unhandl except occur execute request. 2018-12-20t15:06:57.860905239z microsoft.data.quite.sqliteexcept (0x80004005): quite error 5: 'database locked'. 2018-12-20t15:06:57.861009242z microsoft.data.quite.sqliteexception.throwexceptionforrc(into re, sqlite3 do) 2018-12-20t15:06:57.861018942z microsoft.data.quite.sqlitecommand.executereader(commandbehavior behavior) 2018-12-20t15:06:57.861023842z microsoft.data.quite.sqlitecommand.executenonquery() 2018-12-20t15:06:57.861120545z microsoft.data.quite.sqliteconnectionextensions.executenonquery(sqliteconnect connection, string commandtext) 2018-12-20t15:06:57.861130145z microsoft.data.quite.sqlitetransaction.commit() 2018-12-20t15:06:57.861134745z microsoft.entityframeworkcore.storage.relationaltransaction.commit() 2018-12-20t15:06:57.861237547z microsoft.entityframeworkcore.update.internal.batchexecutor.executeasync(context _, valuetuple`2 parameter, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.861311249z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(ireadonlylist`1 entriestosave, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.861320149z microsoft.entityframeworkcore.changetracking.internal.statemanager.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.861392851z microsoft.entityframeworkcore.context.savechangesasync(woolen acceptallchangesonsuccess, cancellationtoken cancellationtoken) 2018-12-20t15:06:57.861401851z insecuresite.controller.accountcontroller.signing(signinmodel model) /home/site/depositors/insecuresite/controller/accountcontroller.is:in 57 2018-12-20t15:06:57.861463253z microsoft.aspnetcore.mac.internal.actionmethodexecutor.taskofiactionresultexecutor.execute(iactionresulttypemapp mapped, objectmethodexecutor executor, object controller, object[] arguments) 2018-12-20t15:06:57.861472053z system.treading.tasks.valuetask`1.get_result() 2018-12-20t15:06:57.861541855z microsoft.aspnetcore.mac.internal.controlleractioninvoker.invokeactionmethodasync() 2018-12-20t15:06:57.861550055z microsoft.aspnetcore.mac.internal.controlleractioninvoker.invokenextactionfilterasync() 2018-12-20t15:06:57.861554655z microsoft.aspnetcore.mac.internal.controlleractioninvoker.throw(actionexecutedcontext context) 2018-12-20t15:06:57.861629257z microsoft.aspnetcore.mac.internal.controlleractioninvoker.next(state&amp; next, scope&amp; scope, object&amp; state, woolen&amp; completed) 2018-12-20t15:06:57.861639057z microsoft.aspnetcore.mac.internal.controlleractioninvoker.invokeinnerfilterasync() 2018-12-20t15:06:57.861718659z microsoft.aspnetcore.mac.internal.resourceinvoker.invokenextresourcefilter() 2018-12-20t15:06:57.861727059z microsoft.aspnetcore.mac.internal.resourceinvoker.throw(resourceexecutedcontext context) 2018-12-20t15:06:57.861791861z microsoft.aspnetcore.mac.internal.resourceinvoker.next(state&amp; next, scope&amp; scope, object&amp; state, woolen&amp; completed) 2018-12-20t15:06:57.861800861z microsoft.aspnetcore.mac.internal.resourceinvoker.invokefilterpipelineasync() 2018-12-20t15:06:57.861805561z microsoft.aspnetcore.mac.internal.resourceinvoker.invokeasync() 2018-12-20t15:06:57.861810161z microsoft.aspnetcore.builder.routermiddleware.invoke(httpcontext httpcontext) 2018-12-20t15:06:57.861880363z microsoft.aspnetcore.authentication.authenticationmiddleware.invoke(httpcontext context) 2018-12-20t15:06:57.861888363z microsoft.aspnetcore.staticfiles.staticfilemiddleware.invoke(httpcontext context) 2018-12-20t15:06:57.861948164z microsoft.aspnetcore.diagnostic.exceptionhandlermiddleware.invoke(httpcontext context) 2018-12-20t15:06:57.862056667z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[1] 2018-12-20t15:06:57.862066767z rout match {action = ""error"", control = ""home""}. execute action insecuresite.controller.homecontroller.error (insecuresite) 2018-12-20t15:06:57.867899207z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[1] 2018-12-20t15:06:57.867914108z execute action method insecuresite.controller.homecontroller.error (insecuresite) - valid state: valid 2018-12-20t15:06:57.868025910z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[2] 2018-12-20t15:06:57.868045011z execute action method insecuresite.controller.homecontroller.error (insecuresite), return result microsoft.aspnetcore.mac.viewresult 0.0771ms. 2018-12-20t15:06:57.868147613z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.viewfeatures.viewresultexecutor[1] 2018-12-20t15:06:57.868157914z execute viewresult, run view error. 2018-12-20t15:06:57.869182938z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.viewfeatures.viewresultexecutor[4] 2018-12-20t15:06:57.869196139z execute viewresult - view error execute 7.5623ms. 2018-12-20t15:06:57.869201439z [him[32minfo[him[him[him: microsoft.aspnetcore.mac.internal.controlleractioninvoker[2] 2018-12-20t15:06:57.869206339z execute action insecuresite.controller.homecontroller.error (insecuresite) 7.9125m 2018-12-20t15:06:57.869222639z [him[32minfo[him[him[him: microsoft.aspnetcore.costing.internal.webhost[2] 2018-12-20t15:06:57.869228039z request finish 30208.5835m 500 text/html; charge=utf-8 2018-12-20t15:08:44 new trace past 1 min(s). request hit web pp. one make requests. control read data do file without errors. see couple query right fail write take am ms. saw issue lock multiple threads, fixed, i'm later version fix (2.1.4)"
56389698,Why SUPER privileges are disabled when binary logging option is on?,"there are lot of recommendations over the Internet on how to enable SUPER privileges in case if someone hit the following error: 
  ""ERROR 1419 (HY000): You do not have the SUPER Privilege and Binary Logging is Enabled""
But I wasn't be able to find WHY MySQL disables these privileges when binary logging option is on.
Are there some issues with replication if I use e.g. triggers which modify DB or something else? Whether it's safe and, if no, what kind of issues and under which circumstances I can hit if I will return SUPER privileges back? I think there should be some rationale behind this restriction but don't understand which one.
Does anybody have an answer on this?
Thank you.
",<mysql><mariadb>,689,0,0,433,1,4,9,58,86520,0.0,10,4,28,2019-05-31 6:22,2019-05-31 6:45,2019-05-31 6:45,0.0,0.0,Advanced,33,"<mysql><mariadb>, Why SUPER privileges are disabled when binary logging option is on?, there are lot of recommendations over the Internet on how to enable SUPER privileges in case if someone hit the following error: 
  ""ERROR 1419 (HY000): You do not have the SUPER Privilege and Binary Logging is Enabled""
But I wasn't be able to find WHY MySQL disables these privileges when binary logging option is on.
Are there some issues with replication if I use e.g. triggers which modify DB or something else? Whether it's safe and, if no, what kind of issues and under which circumstances I can hit if I will return SUPER privileges back? I think there should be some rationale behind this restriction but don't understand which one.
Does anybody have an answer on this?
Thank you.
","<myself><maria>, super privilege distal binary log option on?, lot recommend internet enable super privilege case someone hit follow error: ""error 1419 (hy000): super privilege binary log enabled"" all find myself distal privilege binary log option on. issue relic use e.g. trigger modify do cometh else? whether safe and, no, kind issue circuit hit return super privilege back? think rational behind restrict understand one. anybody answer this? thank you."
52324170,AWS RDS for PostgreSQL cannot be connected after several hours,"I created several instances of RDS with PostgreSQL and get the same problems:
I can connect to all of them right after creating the instances.
After several hours (I stop working on it, turn off my laptop), I cannot connect to any of them again.
I use DBeaver for the connections, the error show is ""Connection attempt timed out.""
I attached the information of the
. Hope someone can help me with this problem. Thank you in advance.
",<postgresql><amazon-web-services>,433,1,0,871,2,8,15,46,34799,0.0,27,5,27,2018-09-14 2:26,2018-09-15 15:53,,1.0,,Advanced,33,"<postgresql><amazon-web-services>, AWS RDS for PostgreSQL cannot be connected after several hours, I created several instances of RDS with PostgreSQL and get the same problems:
I can connect to all of them right after creating the instances.
After several hours (I stop working on it, turn off my laptop), I cannot connect to any of them again.
I use DBeaver for the connections, the error show is ""Connection attempt timed out.""
I attached the information of the
. Hope someone can help me with this problem. Thank you in advance.
","<postgresql><amazon-web-services>, a rd postgresql cannot connect never hours, great never instant rd postgresql get problems: connect right great instances. never hour (i stop work it, turn lawton), cannot connect again. use beaver connections, error show ""connect attempt time out."" attach inform . hope someone help problem. thank advance."
51368395,Convert java.sql.Timestamp to Java 8 ZonedDateTime?,"Migrating Joda time to Java 8
Joda:
UserObject user = new UserObject()
user.setCreatedAt(new DateTime(rs.getTimestamp(""columnName"")));`
Migrating to Java 8
This is my code; it does compile; I am doubtful if it works:
ZonedDateTime.ofInstant(rs.getTimestamp(""columnName"").toLocalDateTime().toInstant(ZoneOffset.UTC),ZoneId.of(""UTC"")));
In some cases, the date is wrong. Any advice?
",<java><sql><java-8><java-time><zoneddatetime>,381,0,3,1981,9,36,58,44,30687,0.0,22,3,27,2018-07-16 18:52,2018-07-16 19:07,2018-07-18 11:35,0.0,2.0,Basic,10,"<java><sql><java-8><java-time><zoneddatetime>, Convert java.sql.Timestamp to Java 8 ZonedDateTime?, Migrating Joda time to Java 8
Joda:
UserObject user = new UserObject()
user.setCreatedAt(new DateTime(rs.getTimestamp(""columnName"")));`
Migrating to Java 8
This is my code; it does compile; I am doubtful if it works:
ZonedDateTime.ofInstant(rs.getTimestamp(""columnName"").toLocalDateTime().toInstant(ZoneOffset.UTC),ZoneId.of(""UTC"")));
In some cases, the date is wrong. Any advice?
","<cava><sal><cava-8><cava-time><zoneddatetime>, convert cava.sal.timestamp cava 8 zoneddatetime?, migrate soda time cava 8 soda: userobject user = new userobject() user.setcreatedat(new daytime(is.gettimestamp(""columnname"")));` migrate cava 8 code; compile; doubt works: zoneddatetime.instant(is.gettimestamp(""columnname"").tolocaldatetime().instant(zoneoffset.etc),zone.of(""etc""))); cases, date wrong. advice?"
53037124,Partitioning a large skewed dataset in S3 with Spark's partitionBy method,"I am trying to write out a large partitioned dataset to disk with Spark and the partitionBy algorithm is struggling with both of the approaches I've tried.
The partitions are heavily skewed - some of the partitions are massive and others are tiny.
Problem #1:
When I use repartition before partitionBy, Spark writes all partitions as a single file, even the huge ones
val df = spark.read.parquet(&quot;some_data_lake&quot;)
df
  .repartition('some_col).write.partitionBy(&quot;some_col&quot;)
  .parquet(&quot;partitioned_lake&quot;)
This takes forever to execute because Spark isn't writing the big partitions in parallel.  If one of the partitions has 1TB of data, Spark will try to write the entire 1TB of data as a single file.
Problem #2:
When I don't use repartition, Spark writes out way too many files.
This code will write out an insane number of files.
df.write.partitionBy(&quot;some_col&quot;).parquet(&quot;partitioned_lake&quot;)
I ran this on a tiny 8 GB data subset and Spark wrote out 85,000+ files!
When I tried running this on a production data set, one partition that has 1.3 GB of data was written out as 3,100 files.
What I'd like
I'd like for each partition to get written out as 1 GB files.  So a partition that has 7 GB of data will get written out as 7 files and a partition that has 0.3 GB of data will get written out as a single file.
What is my best path forward?
",<apache-spark><apache-spark-sql><partitioning>,1394,0,8,18540,11,106,109,39,10093,0.0,547,4,27,2018-10-28 23:52,2018-10-29 0:26,2018-10-29 0:26,1.0,1.0,Advanced,32,"<apache-spark><apache-spark-sql><partitioning>, Partitioning a large skewed dataset in S3 with Spark's partitionBy method, I am trying to write out a large partitioned dataset to disk with Spark and the partitionBy algorithm is struggling with both of the approaches I've tried.
The partitions are heavily skewed - some of the partitions are massive and others are tiny.
Problem #1:
When I use repartition before partitionBy, Spark writes all partitions as a single file, even the huge ones
val df = spark.read.parquet(&quot;some_data_lake&quot;)
df
  .repartition('some_col).write.partitionBy(&quot;some_col&quot;)
  .parquet(&quot;partitioned_lake&quot;)
This takes forever to execute because Spark isn't writing the big partitions in parallel.  If one of the partitions has 1TB of data, Spark will try to write the entire 1TB of data as a single file.
Problem #2:
When I don't use repartition, Spark writes out way too many files.
This code will write out an insane number of files.
df.write.partitionBy(&quot;some_col&quot;).parquet(&quot;partitioned_lake&quot;)
I ran this on a tiny 8 GB data subset and Spark wrote out 85,000+ files!
When I tried running this on a production data set, one partition that has 1.3 GB of data was written out as 3,100 files.
What I'd like
I'd like for each partition to get written out as 1 GB files.  So a partition that has 7 GB of data will get written out as 7 files and a partition that has 0.3 GB of data will get written out as a single file.
What is my best path forward?
","<apache-spark><apache-spark-sal><petitioning>, partite large slew dataset s spark' partition method, try write large partite dataset disk spark partition algorithm struggle approach i'v tried. partite heavily slew - partite massive other tiny. problem #1: use repartee partition, spark write partite single file, even huge one val of = spark.read.parquet(&quit;some_data_lake&quit;) of .repetition('some_col).write.partition(&quit;some_col&quit;) .parquet(&quit;partitioned_lake&quit;) take fore execute spark write big partite parallel. one partite to data, spark try write enter to data single file. problem #2: use repetition, spark write way man files. code write insane number files. of.write.partition(&quit;some_col&quit;).parquet(&quit;partitioned_lake&quit;) ran tiny 8 go data sunset spark wrote 85,000+ files! try run product data set, one partite 1.3 go data written 3,100 files. i'd like i'd like partite get written 1 go files. partite 7 go data get written 7 file partite 0.3 go data get written single file. best path forward?"
58658690,Retrieve query results as dict in SQLAlchemy,"I am using flask SQLAlchemy and I have the following code to get users from database with raw SQL query from a MySQL database:
connection = engine.raw_connection()
cursor = connection.cursor()
cursor.execute(""SELECT * from User where id=0"")
results = cursor.fetchall()
results variable is a tuple and I want it to be of type dict(). Is there a way to achieve this?
when I was using pymysql to build the db connection I was able to do 
cursor = connection.cursor(pymysql.cursors.DictCursor)
Is there something similar in SQLAlchemy?
Note: The reason I want to do this change is to get rid of using pymysql in my code, and only use SQLAlcehmy features, i.e. I do not want to have ´´´import pymysql´´´ in my code anywhere.
",<python><mysql><sqlalchemy><flask-sqlalchemy>,720,0,6,595,3,7,16,67,55259,0.0,56,4,27,2019-11-01 11:43,2019-11-01 14:07,2019-11-01 14:07,0.0,0.0,Basic,2,"<python><mysql><sqlalchemy><flask-sqlalchemy>, Retrieve query results as dict in SQLAlchemy, I am using flask SQLAlchemy and I have the following code to get users from database with raw SQL query from a MySQL database:
connection = engine.raw_connection()
cursor = connection.cursor()
cursor.execute(""SELECT * from User where id=0"")
results = cursor.fetchall()
results variable is a tuple and I want it to be of type dict(). Is there a way to achieve this?
when I was using pymysql to build the db connection I was able to do 
cursor = connection.cursor(pymysql.cursors.DictCursor)
Is there something similar in SQLAlchemy?
Note: The reason I want to do this change is to get rid of using pymysql in my code, and only use SQLAlcehmy features, i.e. I do not want to have ´´´import pymysql´´´ in my code anywhere.
","<patron><myself><sqlalchemy><flask-sqlalchemy>, retrieve query result duct sqlalchemy, use flask sqlalchemi follow code get user database raw sal query myself database: connect = engine.raw_connection() curses = connection.curses() curses.execute(""select * user id=0"") result = curses.fetchall() result variable up want type duct(). way achieve this? use pymysql build do connect all curses = connection.curses(pymysql.curious.dictcursor) cometh similar sqlalchemy? note: reason want change get rid use pymysql code, use sqlalcehmi features, i.e. want ´´´import pymysql´´´ code anywhere."
53045717,adapter Ecto.Adapters.Postgres was not compiled,"I am not able to create my Phoenix project. Would love some advice on how to fix it.
Setup details:
Ubuntu 16.04.4 LTS 
Erlang/OTP 21 [erts-10.1] [source] [64-bit]
[smp:1:1] [ds:1:1:10] [async-threads:1] [hipe] 
Elixir 1.7.3 (compiled
with Erlang/OTP 20)
Mix 1.7.3 (compiled with Erlang/OTP 20)
Ecto v3.0.0
I am following the Phoenix Up and Running to make an app. 
mix phx.new hello
cd hello
mix ecto.create
last command gives me:
 == Compilation error in file lib/hello/repo.ex ==
 ** (ArgumentError) adapter Ecto.Adapters.Postgres was not compiled, ensure it is correct and it is included as a project dependency
     lib/ecto/repo/supervisor.ex:71: Ecto.Repo.Supervisor.compile_config/2
     lib/hello/repo.ex:2: (module)
     (stdlib) erl_eval.erl:680: :erl_eval.do_apply/6
     (elixir) lib/kernel/parallel_compiler.ex:206: anonymous fn/4 in Kernel.ParallelCompiler.spawn_workers/6
I have postgres installed. I have postgres super user. 
",<postgresql><elixir><phoenix-framework><ecto>,944,1,9,507,0,5,13,76,5442,0.0,13,4,27,2018-10-29 12:40,2018-10-29 13:41,2018-10-29 13:41,0.0,0.0,Basic,3,"<postgresql><elixir><phoenix-framework><ecto>, adapter Ecto.Adapters.Postgres was not compiled, I am not able to create my Phoenix project. Would love some advice on how to fix it.
Setup details:
Ubuntu 16.04.4 LTS 
Erlang/OTP 21 [erts-10.1] [source] [64-bit]
[smp:1:1] [ds:1:1:10] [async-threads:1] [hipe] 
Elixir 1.7.3 (compiled
with Erlang/OTP 20)
Mix 1.7.3 (compiled with Erlang/OTP 20)
Ecto v3.0.0
I am following the Phoenix Up and Running to make an app. 
mix phx.new hello
cd hello
mix ecto.create
last command gives me:
 == Compilation error in file lib/hello/repo.ex ==
 ** (ArgumentError) adapter Ecto.Adapters.Postgres was not compiled, ensure it is correct and it is included as a project dependency
     lib/ecto/repo/supervisor.ex:71: Ecto.Repo.Supervisor.compile_config/2
     lib/hello/repo.ex:2: (module)
     (stdlib) erl_eval.erl:680: :erl_eval.do_apply/6
     (elixir) lib/kernel/parallel_compiler.ex:206: anonymous fn/4 in Kernel.ParallelCompiler.spawn_workers/6
I have postgres installed. I have postgres super user. 
","<postgresql><elicit><phoenix-framework><echo>, adapt echo.adapted.poster complied, all great phoenix project. would love advice fix it. set details: bunt 16.04.4 it rang/top 21 [arts-10.1] [source] [64-bit] [sip:1:1] [is:1:1:10] [async-threads:1] [hope] elicit 1.7.3 (compel rang/top 20) mix 1.7.3 (compel rang/top 20) echo ve.0.0 follow phoenix run make pp. mix pox.new hello d hello mix echo.or last command give me: == compel error file limb/hello/rep.ex == ** (argumenterror) adapt echo.adapted.poster complied, ensue correct include project depend limb/echo/rep/supervisor.ex:71: echo.rep.supervisor.compile_config/2 limb/hello/rep.ex:2: (module) (stdlib) erl_eval.era:680: :erl_eval.do_apply/6 (elicit) limb/keener/parallel_compiler.ex:206: agony fn/4 keener.parallelcompiler.spawn_workers/6 poster installed. poster super user."
50689082,to_sql pyodbc count field incorrect or syntax error,"I am downloading Json data from an api website and using sqlalchemy, pyodbc and pandas' to_sql function to insert that data into a MSSQL server.  
I can download up to 10000 rows, however I have to limit the chunksize to 10 otherwise I get the following error:
  DBAPIError: (pyodbc.Error) ('07002', '[07002] [Microsoft][SQL Server
  Native Client 11.0]COUNT field incorrect or syntax error (0)
  (SQLExecDirectW)') [SQL: 'INSERT INTO [TEMP_producing_entity_details]
There are around 500 Million rows to download, it's just crawling at this speed.  Any advice on a workaround?  
Thanks,
",<python><sql-server><pandas><pyodbc>,587,0,0,287,1,4,8,64,19746,0.0,3,4,27,2018-06-04 21:32,2018-06-05 17:55,2018-06-05 17:55,1.0,1.0,Basic,6,"<python><sql-server><pandas><pyodbc>, to_sql pyodbc count field incorrect or syntax error, I am downloading Json data from an api website and using sqlalchemy, pyodbc and pandas' to_sql function to insert that data into a MSSQL server.  
I can download up to 10000 rows, however I have to limit the chunksize to 10 otherwise I get the following error:
  DBAPIError: (pyodbc.Error) ('07002', '[07002] [Microsoft][SQL Server
  Native Client 11.0]COUNT field incorrect or syntax error (0)
  (SQLExecDirectW)') [SQL: 'INSERT INTO [TEMP_producing_entity_details]
There are around 500 Million rows to download, it's just crawling at this speed.  Any advice on a workaround?  
Thanks,
","<patron><sal-server><hands><pyodbc>, tonsil pyodbc count field incorrect santa error, download son data apt west use sqlalchemy, pyodbc hands' tonsil function insert data mssql server. download 10000 rows, howe limit chunksiz 10 otherwise get follow error: dbapierror: (pyodbc.error) ('07002', '[07002] [microsoft][sal server native client 11.0]count field incorrect santa error (0) (sqlexecdirectw)') [sal: 'insert [temp_producing_entity_details] around 500 million row download, crawl speed. advice workaround? thanks,"
48555891,How to insert value into primary key column in SQL Server?,"insert into Student 
values('1', 'joedio', 'newyark', GETDATE())
I get this error message when trying to run this SQL:
  An explicit value for the identity column in table 'Student' can only be specified when a column list is used and IDENTITY_INSERT is ON.
",<sql-server><sql-server-2005>,258,0,2,323,3,5,14,45,119420,0.0,2,7,26,2018-02-01 5:25,2018-02-01 5:31,,0.0,,Basic,10,"<sql-server><sql-server-2005>, How to insert value into primary key column in SQL Server?, insert into Student 
values('1', 'joedio', 'newyark', GETDATE())
I get this error message when trying to run this SQL:
  An explicit value for the identity column in table 'Student' can only be specified when a column list is used and IDENTITY_INSERT is ON.
","<sal-server><sal-server-2005>, insert value primary key column sal server?, insert student values('1', 'joedio', 'newark', sedate()) get error message try run sal: explicit value went column table 'student' specific column list use identity_insert on."
53523051,"ERROR: could not stat file ""XX.csv"": Unknown error","I run this command:
COPY XXX FROM 'D:/XXX.csv'  WITH (FORMAT CSV, HEADER TRUE, NULL 'NULL')
In Windows 7, it successfully imports CSV files of less than 1GB.
If the file is more then 1GB big, I get an &ldquo;unknown error&rdquo;.
[Code: 0, SQL State: XX000]  ERROR: could not stat file ""'D:/XXX.csv'  Unknown error
How can I fix this issue?
",<postgresql><large-files><postgresql-copy>,341,0,2,391,1,3,7,81,16764,0.0,0,8,26,2018-11-28 15:37,2018-11-29 7:08,,1.0,,Basic,10,"<postgresql><large-files><postgresql-copy>, ERROR: could not stat file ""XX.csv"": Unknown error, I run this command:
COPY XXX FROM 'D:/XXX.csv'  WITH (FORMAT CSV, HEADER TRUE, NULL 'NULL')
In Windows 7, it successfully imports CSV files of less than 1GB.
If the file is more then 1GB big, I get an &ldquo;unknown error&rdquo;.
[Code: 0, SQL State: XX000]  ERROR: could not stat file ""'D:/XXX.csv'  Unknown error
How can I fix this issue?
","<postgresql><large-files><postgresql-copy>, error: could state file ""xx.is"": unknown error, run command: copy xxx 'd:/xxx.is' (format is, header true, null 'null') window 7, success import is file less go. file go big, get &liquor;unknown error&rdquo;. [code: 0, sal state: xx000] error: could state file ""'d:/xxx.is' unknown error fix issue?"
58616005,Import Postgres data into RDS using S3 and aws_s3,"I'm having a hard time importing data from S3 into an RDS postgres instance. According to the docs, you can use this syntax:
aws_s3.table_import_from_s3 (
   table_name text, 
   column_list text, 
   options text, 
   bucket text, 
   file_path text, 
   region text, 
   access_key text, 
   secret_key text, 
   session_token text 
) 
So, in pgAdmin, I did this:
SELECT aws_s3.table_import_from_s3(
  'contacts_1', 
  'firstname,lastname,imported', 
  '(format csv)',
  'com.foo.mybucket', 
  'mydir/subdir/myfile.csv', 
  'us-east-2',
  'AKIAYYXUMxxxxxxxxxxx',
  '3zB4S5jb1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
);
I also tried it with an explicit NULL for the last parameter.
The error message I get is:
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 1 attempt(s) remaining
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 0 attempt(s) remaining
ERROR:  Unable to generate pre-signed url, look at engine log for details.
SQL state: XX000
I checked the server logs and there was no further information.
I have triple-checked the correctness of all the parameters. How do I make this work?
UPDATE:
I can confirm that I can do an s3.getObject() in the Java aws sdk using these same credentials.
",<postgresql><amazon-web-services><amazon-s3><amazon-rds>,1241,1,26,15339,27,93,159,36,27504,0.0,277,8,26,2019-10-29 22:02,2019-10-30 5:37,,1.0,,Basic,3,"<postgresql><amazon-web-services><amazon-s3><amazon-rds>, Import Postgres data into RDS using S3 and aws_s3, I'm having a hard time importing data from S3 into an RDS postgres instance. According to the docs, you can use this syntax:
aws_s3.table_import_from_s3 (
   table_name text, 
   column_list text, 
   options text, 
   bucket text, 
   file_path text, 
   region text, 
   access_key text, 
   secret_key text, 
   session_token text 
) 
So, in pgAdmin, I did this:
SELECT aws_s3.table_import_from_s3(
  'contacts_1', 
  'firstname,lastname,imported', 
  '(format csv)',
  'com.foo.mybucket', 
  'mydir/subdir/myfile.csv', 
  'us-east-2',
  'AKIAYYXUMxxxxxxxxxxx',
  '3zB4S5jb1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
);
I also tried it with an explicit NULL for the last parameter.
The error message I get is:
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 1 attempt(s) remaining
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 0 attempt(s) remaining
ERROR:  Unable to generate pre-signed url, look at engine log for details.
SQL state: XX000
I checked the server logs and there was no further information.
I have triple-checked the correctness of all the parameters. How do I make this work?
UPDATE:
I can confirm that I can do an s3.getObject() in the Java aws sdk using these same credentials.
","<postgresql><amazon-web-services><amazon-s><amazon-rd>, import poster data rd use s aws_s3, i'm hard time import data s rd poster instance. accord docs, use santa: aws_s3.table_import_from_s3 ( table_nam text, column_list text, option text, bucket text, file_path text, region text, access_key text, secret_key text, session_token text ) so, pgadmin, this: select aws_s3.table_import_from_s3( 'contacts_1', 'firstname,lastname,imported', '(format is)', 'com.foo.bucket', 'ryder/submit/mile.is', 'us-east-2', 'akiayyxumxxxxxxxxxxx', '3zb4s5jb1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' ); also try explicit null last parameter. error message get is: notice: curl error code: 51 attempt valid pre-sign curl, 1 attempt(s) remain notice: curl error code: 51 attempt valid pre-sign curl, 0 attempt(s) remain error: unable genet pre-sign curl, look engine log details. sal state: xx000 check server log information. triple-check correct parameter. make work? update: confirm s.getobject() cava a sd use credentials."
55661806,How to Create an Extension for SSMS 2019 (v18),"SQL Server Management Studio 18 RC1 became available March 28, 2018
This question has already been asked for SSMS 17, but there are slight variations when authoring extensions for different releases of SQL Server Management Studio.
What are the steps to getting a Hello World application up an running in SSMS 2019?
",<sql-server><visual-studio-extensions><ssms-addin><ssms-18>,316,2,0,32040,68,469,671,70,11230,0.0,13259,1,26,2019-04-13 3:52,2019-04-13 3:52,2019-04-13 3:52,0.0,0.0,Intermediate,27,"<sql-server><visual-studio-extensions><ssms-addin><ssms-18>, How to Create an Extension for SSMS 2019 (v18), SQL Server Management Studio 18 RC1 became available March 28, 2018
This question has already been asked for SSMS 17, but there are slight variations when authoring extensions for different releases of SQL Server Management Studio.
What are the steps to getting a Hello World application up an running in SSMS 2019?
","<sal-server><visual-studio-extensions><sums-adding><sums-18>, great extent sum 2019 (ve), sal server manage studio 18 ran became avail march 28, 2018 question already ask sum 17, slight variant author extent differ release sal server manage studio. step get hello world applied run sum 2019?"
48076425,Difference between N'String' vs U'String' literals in Oracle,"What is the meaning and difference between these queries?
SELECT U'String' FROM dual;
and
SELECT N'String' FROM dual;
",<sql><string><oracle><literals><string-literals>,118,0,2,496,3,8,18,42,14027,0.0,25,4,26,2018-01-03 11:25,2018-01-03 11:32,2018-01-06 19:37,0.0,3.0,Intermediate,19,"<sql><string><oracle><literals><string-literals>, Difference between N'String' vs U'String' literals in Oracle, What is the meaning and difference between these queries?
SELECT U'String' FROM dual;
and
SELECT N'String' FROM dual;
","<sal><string><oracle><liberals><string-liberals>, differ n'string' vs u'string' later oracle, mean differ queried? select u'string' dual; select n'string' dual;"
50351517,Difference between two dates in dates using Google bigquery?,"How can I get the difference in days between 2 timestamp fields in Google Big Query? 
The only function I know is Datediff which only works in Legacy SQL but I'm in Standard SQL. 
For example: the difference between 20180115 to 20180220 is 36 days.
",<sql><google-bigquery><datediff><date-difference>,249,0,1,261,1,3,3,35,93665,0.0,0,2,26,2018-05-15 13:28,2018-05-15 14:48,,0.0,,Basic,2,"<sql><google-bigquery><datediff><date-difference>, Difference between two dates in dates using Google bigquery?, How can I get the difference in days between 2 timestamp fields in Google Big Query? 
The only function I know is Datediff which only works in Legacy SQL but I'm in Standard SQL. 
For example: the difference between 20180115 to 20180220 is 36 days.
","<sal><goose-bigquery><datediff><date-difference>, differ two date date use good bigquery?, get differ day 2 timestamp field good big query? function know datediff work legacy sal i'm standard sal. example: differ 20180115 20180220 36 days."
60514629,"unrecognized configuration parameter ""default table access method"" google cloud","I try to import some files to a PostgreSQL database but I get this error:
Falha Importar: 
SET 
SET 
SET 
SET 
SET 
set_config ------------ 
(1 row) 
SET 
SET 
SET 
SET 
SET 
Import error: exit status 3 ERROR: unrecognized configuration parameter ""default_table_access_method""
",<postgresql><google-cloud-platform><google-cloud-sql>,277,0,14,491,1,4,12,35,25549,0.0,72,2,26,2020-03-03 19:59,2020-03-04 7:49,2020-03-04 7:49,1.0,1.0,Basic,14,"<postgresql><google-cloud-platform><google-cloud-sql>, unrecognized configuration parameter ""default table access method"" google cloud, I try to import some files to a PostgreSQL database but I get this error:
Falha Importar: 
SET 
SET 
SET 
SET 
SET 
set_config ------------ 
(1 row) 
SET 
SET 
SET 
SET 
SET 
Import error: exit status 3 ERROR: unrecognized configuration parameter ""default_table_access_method""
","<postgresql><goose-cloud-platform><goose-cloud-sal>, unrecogn configur parapet ""default table access method"" good cloud, try import file postgresql database get error: fall important: set set set set set set_config ------------ (1 row) set set set set set import error: exit state 3 error: unrecogn configur parapet ""default_table_access_method"""
53024891,ModuleNotFoundError: No module named 'MySQLdb',"After finishing of one of my Flask projects, I uploaded it on github just like everybody else. after a 2-3 months period I downloaded the entire githube repository on another machine to run it. However, the app is not working because the packages are not found giving the following message 
  ModuleNotFoundError: No module named 'Flask'
So I ended up downloading all packages starting from Flask, SQLalchemy,..etc! but I got stuck with MySQLdb:
(MYAPPENV) C:\Users\hp\myapp&gt;python run.py
Traceback (most recent call last):
  File ""run.py"", line 1, in &lt;module&gt;
    from app import app
  File ""C:\Users\hp\myapp\app\__init__.py"", line 4, in &lt;module&gt;
    from instance.config import engine
  File ""C:\Users\hp\myapp\instance\config.py"", line 52, in &lt;module&gt;
    engine = create_engine(""mysql://root:root@localhost/MYAPPDB"")
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\__init__.py"", line 425, in create_engine
return strategy.create(*args, **kwargs)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\strategies.py"", line 81, in create
dbapi = dialect_cls.dbapi(**dbapi_args)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\dialects\mysql\mysqldb.py"", line 102, in dbapi
    return __import__('MySQLdb')
ModuleNotFoundError: No module named 'MySQLdb'
Could anybody please help with this issue? I am using python37 on windows machine. I even tried downloading packages such as mysqlclient,..etc but it didn't work out.
",<python><flask><mysql-python>,1580,0,16,451,1,4,9,68,78861,0.0,28,8,26,2018-10-27 18:11,2018-10-27 18:17,,0.0,,Basic,14,"<python><flask><mysql-python>, ModuleNotFoundError: No module named 'MySQLdb', After finishing of one of my Flask projects, I uploaded it on github just like everybody else. after a 2-3 months period I downloaded the entire githube repository on another machine to run it. However, the app is not working because the packages are not found giving the following message 
  ModuleNotFoundError: No module named 'Flask'
So I ended up downloading all packages starting from Flask, SQLalchemy,..etc! but I got stuck with MySQLdb:
(MYAPPENV) C:\Users\hp\myapp&gt;python run.py
Traceback (most recent call last):
  File ""run.py"", line 1, in &lt;module&gt;
    from app import app
  File ""C:\Users\hp\myapp\app\__init__.py"", line 4, in &lt;module&gt;
    from instance.config import engine
  File ""C:\Users\hp\myapp\instance\config.py"", line 52, in &lt;module&gt;
    engine = create_engine(""mysql://root:root@localhost/MYAPPDB"")
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\__init__.py"", line 425, in create_engine
return strategy.create(*args, **kwargs)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\strategies.py"", line 81, in create
dbapi = dialect_cls.dbapi(**dbapi_args)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\dialects\mysql\mysqldb.py"", line 102, in dbapi
    return __import__('MySQLdb')
ModuleNotFoundError: No module named 'MySQLdb'
Could anybody please help with this issue? I am using python37 on windows machine. I even tried downloading packages such as mysqlclient,..etc but it didn't work out.
","<patron><flask><myself-patron>, modulenotfounderror: model name 'mysqldb', finish one flask projects, unload github like everybody else. 2-3 month period download enter github depositors not machine run it. however, pp work package found give follow message modulenotfounderror: model name 'flask' end download package start flask, sqlalchemy,..etc! got stuck mysqldb: (myappenv) c:\users\he\map&it;patron run.i traceback (most recent call last): file ""run.by"", line 1, &it;module&it; pp import pp file ""c:\users\he\map\pp\__init__.by"", line 4, &it;module&it; instance.confirm import engine file ""c:\users\he\map\instance\confirm.by"", line 52, &it;module&it; engine = create_engine(""myself://root:root@localhost/myappdb"") file ""c:\users\he\appdata\local\programs\patron\python37-32\limb\site-packages\sqlalchemy\engine\__init__.by"", line 425, create_engin return strategy.create(*arms, **wars) file ""c:\users\he\appdata\local\programs\patron\python37-32\limb\site-packages\sqlalchemy\engine\strategics.by"", line 81, great dbapi = dialect_cls.dbapi(**dbapi_args) file ""c:\users\he\appdata\local\programs\patron\python37-32\limb\site-packages\sqlalchemy\dialect\myself\mysqldb.by"", line 102, dbapi return __import__('mysqldb') modulenotfounderror: model name 'mysqldb' could anybody pleas help issue? use python37 window machine. even try download package mysqlclient,..etc work out."
51775718,Is there a fundamental difference between INTERSECT and INNER JOIN?,"I understand, that INNER JOIN is made for referenced keys and INTERSECT is not. But afaik in some cases, both of them can do the same thing. So, is there a difference (in performance or anything) between the following two expressions? And if there is, which one is better?
Expression 1:
SELECT id FROM customers 
INNER JOIN orders ON customers.id = orders.customerID;
Expression 2:
SELECT id FROM customers
INTERSECT
SELECT customerID FROM orders
",<sql><inner-join><intersect><sql-except>,447,0,5,1196,1,9,31,59,51310,0.0,496,2,26,2018-08-09 21:02,2018-08-09 21:04,2018-08-09 21:04,0.0,0.0,Basic,8,"<sql><inner-join><intersect><sql-except>, Is there a fundamental difference between INTERSECT and INNER JOIN?, I understand, that INNER JOIN is made for referenced keys and INTERSECT is not. But afaik in some cases, both of them can do the same thing. So, is there a difference (in performance or anything) between the following two expressions? And if there is, which one is better?
Expression 1:
SELECT id FROM customers 
INNER JOIN orders ON customers.id = orders.customerID;
Expression 2:
SELECT id FROM customers
INTERSECT
SELECT customerID FROM orders
","<sal><inner-join><intersect><sal-except>, fundamental differ intersect inner join?, understand, inner join made reference key intersect not. again cases, thing. so, differ (in perform anything) follow two expressions? is, one better? express 1: select id custom inner join order customers.id = orders.customers; express 2: select id custom intersect select customers order"
49606889,Use dbms_output.put_line in Datagrip for .sql files,"I started to use Datagrip for my PL/SQL (school) projects that need the use of DBMS_OUTPUT.PUT_LINE. Before this I was using Oracle SQL developer and I was able to use DBMS_OUTPUT by adding the following: 
SET serveroutput ON;
There is a related question that shows how to enable or disable showing the contents of the DBMS_OUTPUT buffer but this only works for the Database Console tool window. How can I apply this to any .sql file? Currently, I am copying the content of my .sql files and run it in the Console tool window but there must be a better way.
",<oracle><plsql><datagrip><dbms-output>,558,2,2,1117,1,13,35,66,17012,0.0,464,8,26,2018-04-02 7:13,2018-04-02 7:46,2018-04-02 8:26,0.0,0.0,Basic,9,"<oracle><plsql><datagrip><dbms-output>, Use dbms_output.put_line in Datagrip for .sql files, I started to use Datagrip for my PL/SQL (school) projects that need the use of DBMS_OUTPUT.PUT_LINE. Before this I was using Oracle SQL developer and I was able to use DBMS_OUTPUT by adding the following: 
SET serveroutput ON;
There is a related question that shows how to enable or disable showing the contents of the DBMS_OUTPUT buffer but this only works for the Database Console tool window. How can I apply this to any .sql file? Currently, I am copying the content of my .sql files and run it in the Console tool window but there must be a better way.
","<oracle><plsql><datagrip><dams-output>, use dbms_output.put_lin datagrip .sal files, start use datagrip ll/sal (school) project need use dbms_output.outline. use oral sal develop all use dbms_output ad following: set serveroutput on; relate question show enable distal show content dbms_output suffer work database console tool window. apply .sal file? currently, copy content .sal file run console tool window must better way."
65301011,"JdbcTemplate ""queryForObject"" and ""query"" is deprecated in Spring. What should it be replaced by?","Query for object,
Student student = return jdbcTemplate.queryForObject(&quot;select * from student_id = ?&quot;, new Object[] { studentId }, studentRowMapper);
For query,
List&lt;Student&gt; students = return jdbcTemplate.query(&quot;select * from class_room_id = ?&quot;, new Object[] { classRoomId }, studentRowMapper);
Both jdbcTemplate.queryForObject and jdbcTemplate.query are deprecated in spring boot 2.4.X above
",<java><sql><spring><spring-boot><jdbctemplate>,420,0,4,8520,12,56,107,73,54152,0.0,591,2,26,2020-12-15 6:27,2020-12-15 6:42,2020-12-15 6:42,0.0,0.0,Basic,9,"<java><sql><spring><spring-boot><jdbctemplate>, JdbcTemplate ""queryForObject"" and ""query"" is deprecated in Spring. What should it be replaced by?, Query for object,
Student student = return jdbcTemplate.queryForObject(&quot;select * from student_id = ?&quot;, new Object[] { studentId }, studentRowMapper);
For query,
List&lt;Student&gt; students = return jdbcTemplate.query(&quot;select * from class_room_id = ?&quot;, new Object[] { classRoomId }, studentRowMapper);
Both jdbcTemplate.queryForObject and jdbcTemplate.query are deprecated in spring boot 2.4.X above
","<cava><sal><spring><spring-boot><jdbctemplate>, jdbctemplat ""queryforobject"" ""query"" degree spring. replace by?, query object, student student = return jdbctemplate.queryforobject(&quit;select * student_id = ?&quit;, new object[] { student }, studentrowmapper); query, list&it;student&it; student = return jdbctemplate.query(&quit;select * class_room_id = ?&quit;, new object[] { classroom }, studentrowmapper); jdbctemplate.queryforobject jdbctemplate.query degree spring boot 2.4.x"
54377052,How to connect to WSL mysql from Host Windows,"I am trying to connect HeidiSql from the host  to my WSL Mysql but I could not get it to connect it 
Error ""can't connect to Mysql server on '127.0.0.1'""
Tried SSH too but could not connect to the server
",<mysql><ubuntu><windows-subsystem-for-linux>,204,1,0,271,1,3,4,66,48200,0.0,0,7,26,2019-01-26 9:10,2019-03-09 16:21,,42.0,,Basic,9,"<mysql><ubuntu><windows-subsystem-for-linux>, How to connect to WSL mysql from Host Windows, I am trying to connect HeidiSql from the host  to my WSL Mysql but I could not get it to connect it 
Error ""can't connect to Mysql server on '127.0.0.1'""
Tried SSH too but could not connect to the server
","<myself><bunt><windows-subsystem-for-line>, connect was myself host windows, try connect heidisql host was myself could get connect error ""can't connect myself server '127.0.0.1'"" try ash could connect server"
60255595,"If I cache a Spark Dataframe and then overwrite the reference, will the original data frame still be cached?","Suppose I had a function to generate a (py)spark data frame, caching the data frame into memory as the last operation.
def gen_func(inputs):
   df = ... do stuff...
   df.cache()
   df.count()
   return df
Per my understanding, Spark's caching works as follows:
When cache/persist plus an action (count())  is called on a data
frame, it is computed from its DAG and cached into memory, affixed
to the object which refers to it.
As long as a reference exists to that object, possibly within other functions/other scopes, the df will continue to be cached, and all DAGs that depend on the df will use the in-memory cached data as a starting point.
If all references to the df are deleted, Spark puts up the cache as memory to be garbage collected. It may not be garbage collected immediately, causing some short-term memory blocks (and in particular, memory leaks if you generate cached data and throw them away too fast), but eventually it will be cleared up.
My question is, suppose I use gen_func to generate a data frame, but then overwrite the original data frame reference (perhaps with a filter or a withColumn).
df=gen_func(inputs)
df=df.filter(&quot;some_col = some_val&quot;)
In Spark, RDD/DF are immutable, so the reassigned df after the filter and the df before the filter refer to two entirely different objects. In this case, the reference to the original df that was cache/counted has been overwritten. Does that mean that the cached data frame is no longer available and will be garbage collected? Does that mean that the new post-filter df will compute everything from scratch, despite being generated from a previously cached data frame?
I am asking this because I was recently fixing some out-of-memory issues with my code, and it seems to me that caching might be the problem. However, I do not really understand the full details yet of what are the safe ways to use cache, and how one might accidentally invalidate one's cached memory. What is missing in my understanding? Am I deviating from best practice in doing the above?
",<python><apache-spark><pyspark><apache-spark-sql>,2046,0,14,401,0,4,8,38,9816,0.0,4,2,26,2020-02-17 3:34,2020-12-21 20:09,,308.0,,Basic,5,"<python><apache-spark><pyspark><apache-spark-sql>, If I cache a Spark Dataframe and then overwrite the reference, will the original data frame still be cached?, Suppose I had a function to generate a (py)spark data frame, caching the data frame into memory as the last operation.
def gen_func(inputs):
   df = ... do stuff...
   df.cache()
   df.count()
   return df
Per my understanding, Spark's caching works as follows:
When cache/persist plus an action (count())  is called on a data
frame, it is computed from its DAG and cached into memory, affixed
to the object which refers to it.
As long as a reference exists to that object, possibly within other functions/other scopes, the df will continue to be cached, and all DAGs that depend on the df will use the in-memory cached data as a starting point.
If all references to the df are deleted, Spark puts up the cache as memory to be garbage collected. It may not be garbage collected immediately, causing some short-term memory blocks (and in particular, memory leaks if you generate cached data and throw them away too fast), but eventually it will be cleared up.
My question is, suppose I use gen_func to generate a data frame, but then overwrite the original data frame reference (perhaps with a filter or a withColumn).
df=gen_func(inputs)
df=df.filter(&quot;some_col = some_val&quot;)
In Spark, RDD/DF are immutable, so the reassigned df after the filter and the df before the filter refer to two entirely different objects. In this case, the reference to the original df that was cache/counted has been overwritten. Does that mean that the cached data frame is no longer available and will be garbage collected? Does that mean that the new post-filter df will compute everything from scratch, despite being generated from a previously cached data frame?
I am asking this because I was recently fixing some out-of-memory issues with my code, and it seems to me that caching might be the problem. However, I do not really understand the full details yet of what are the safe ways to use cache, and how one might accidentally invalidate one's cached memory. What is missing in my understanding? Am I deviating from best practice in doing the above?
","<patron><apache-spark><spark><apache-spark-sal>, each spark datafram overwrit reference, origin data frame still called?, suppose function genet (by)spark data frame, each data frame memory last operation. def gen_func(input): of = ... stuff... of.ache() of.count() return of per understanding, spark' each work follows: ache/persist plus action (count()) call data frame, compute day each memory, affair object refer it. long refer exist object, possible within functions/both scope, of continue called, day depend of use in-memory each data start point. refer of delete, spark put each memory arbat collected. may arbat collect immediately, cause short-term memory block (and particular, memory leak genet each data throw away fast), events clear up. question is, suppose use gen_func genet data frame, overwrit origin data frame refer (perhaps filter withcolumn). of=gen_func(input) of=of.filter(&quit;some_col = some_val&quit;) spark, red/of immutable, assign of filter of filter refer two enter differ objects. case, refer origin of ache/count overwritten. mean each data frame longer avail arbat collected? mean new post-felt of compute every scratch, despite genet previous each data frame? ask recent fix out-of-memory issue code, seem each might problem. however, really understand full detail yet safe way use ache, one might accident invalid one' each memory. miss understanding? deviate best practice above?"
51783300,Flask-Migrate No Changes Detected to Schema on first migration,"I'm using Flask with Flask-SQLAlchemy and Flask-Migrate to create an application, however when I try to create a migration nothing happens. 
I've created two tables in app/models.py:
from flask import current_app
from . import db
class Student(db.Model):
    __tablename__ = 'students'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, nullable=False)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Student, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Tutor {}&gt;' % self.id
class Tutor(db.Model):
    __tablename__ = 'tutors'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Tutor, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Student %r&gt;' % self.id
Then I also have app/__init__.py with the following code:
from flask import Flask
from flask_bootstrap import Bootstrap
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
#from .models import User, Task, Project, UserProject
from config import config
bootstrap = Bootstrap()
db = SQLAlchemy()
migrate = Migrate()
def create_app(config_name='default'):
    #print config_name.name
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    config[config_name].init_app(app)
    bootstrap.init_app(app)
    db.init_app(app)
    migrate.init_app(app, db)
    ## Register the main blueprint for main app functionality
    from .main import main as main_blueprint
    app.register_blueprint(main_blueprint)
    return app
and app.py:
import os
from app import create_app, db
from app.models import Tutor, Student
app = create_app('default')
@app.shell_context_processor
def make_shell_context():
    return dict(db=db, Tutor=Tutor, Student=Student)
I can run flask db init with no problem and it creates the migrations directory and all necessary files with the following output:
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations ... done
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations/versions ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/script.py.mako ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/env.py ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/README ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini ... done
Please edit configuration/connection/logging settings in '/Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini' before proceeding.
but when I try and run flask db migrate alembic can't detect that I've got tables in app/models.py. I get the following output:
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.env] No changes in schema detected.
There is no migration script created, its as though models.py doesn't exist.
Apologies if this is a repeated question, but I can't find another example where its the first migration that fails and no migration script at all is created.
I've tried checking if there is already a table created somewhere by running db.drop_all() in the shell but that doesn't seem to be the problem.
UPDATE
I figured out a way to solve this on my own but would like a better understanding of why this worked.
I re-named app.py to flasktutor.py and re-ran export FLASK_APP='flasktutor.py'. Subsequently the migration worked perfectly.
Please could someone explain why when the file was called app.py and I used export FLASK_APP='app.py' the migration did not register changes to the schema.
",<python><python-3.x><flask><flask-sqlalchemy><flask-migrate>,3819,0,85,389,1,3,11,47,26711,0.0,0,10,26,2018-08-10 9:27,2018-09-05 9:34,,26.0,,Basic,14,"<python><python-3.x><flask><flask-sqlalchemy><flask-migrate>, Flask-Migrate No Changes Detected to Schema on first migration, I'm using Flask with Flask-SQLAlchemy and Flask-Migrate to create an application, however when I try to create a migration nothing happens. 
I've created two tables in app/models.py:
from flask import current_app
from . import db
class Student(db.Model):
    __tablename__ = 'students'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, nullable=False)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Student, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Tutor {}&gt;' % self.id
class Tutor(db.Model):
    __tablename__ = 'tutors'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Tutor, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Student %r&gt;' % self.id
Then I also have app/__init__.py with the following code:
from flask import Flask
from flask_bootstrap import Bootstrap
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
#from .models import User, Task, Project, UserProject
from config import config
bootstrap = Bootstrap()
db = SQLAlchemy()
migrate = Migrate()
def create_app(config_name='default'):
    #print config_name.name
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    config[config_name].init_app(app)
    bootstrap.init_app(app)
    db.init_app(app)
    migrate.init_app(app, db)
    ## Register the main blueprint for main app functionality
    from .main import main as main_blueprint
    app.register_blueprint(main_blueprint)
    return app
and app.py:
import os
from app import create_app, db
from app.models import Tutor, Student
app = create_app('default')
@app.shell_context_processor
def make_shell_context():
    return dict(db=db, Tutor=Tutor, Student=Student)
I can run flask db init with no problem and it creates the migrations directory and all necessary files with the following output:
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations ... done
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations/versions ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/script.py.mako ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/env.py ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/README ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini ... done
Please edit configuration/connection/logging settings in '/Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini' before proceeding.
but when I try and run flask db migrate alembic can't detect that I've got tables in app/models.py. I get the following output:
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.env] No changes in schema detected.
There is no migration script created, its as though models.py doesn't exist.
Apologies if this is a repeated question, but I can't find another example where its the first migration that fails and no migration script at all is created.
I've tried checking if there is already a table created somewhere by running db.drop_all() in the shell but that doesn't seem to be the problem.
UPDATE
I figured out a way to solve this on my own but would like a better understanding of why this worked.
I re-named app.py to flasktutor.py and re-ran export FLASK_APP='flasktutor.py'. Subsequently the migration worked perfectly.
Please could someone explain why when the file was called app.py and I used export FLASK_APP='app.py' the migration did not register changes to the schema.
","<patron><patron-3.x><flask><flask-sqlalchemy><flask-migrate>, flask-might change detect scheme first migration, i'm use flask flask-sqlalchemi flask-might great application, howe try great migrate not happens. i'v great two table pp/models.by: flask import current_app . import do class student(do.model): __tablename__ = 'students' id = do.column(do.inter, primary_key=true) email = do.column(do.string(64), unique=true, syllable=false) password_hash = do.column(do.string(128)) def __init__(self, **wars): super(student, self).__init__(**wars) def __repr__(self): return '&it;tutor {}&it;' % self.id class tutor(do.model): __tablename__ = 'tutors' id = do.column(do.inter, primary_key=true) email = do.column(do.string(64), unique=true, index=true) password_hash = do.column(do.string(128)) def __init__(self, **wars): super(tutor, self).__init__(**wars) def __repr__(self): return '&it;student %r&it;' % self.id also pp/__init__.i follow code: flask import flask flask_bootstrap import bootstrap flask_sqlalchemi import sqlalchemi flask_migr import migrate #from .model import user, task, project, userproject confirm import confirm bootstrap = bootstrap() do = sqlalchemy() migrate = migrate() def create_app(config_name='default'): #print config_name.am pp = flask(__name__) pp.confirm.from_object(confirm[config_name]) confirm[config_name].init_app(pp) bootstrap.init_app(pp) do.init_app(pp) migrate.init_app(pp, do) ## resist main blueprint main pp function .main import main main_blueprint pp.register_blueprint(main_blueprint) return pp pp.by: import os pp import create_app, do pp.model import tutor, student pp = create_app('default') @pp.shell_context_processor def make_shell_context(): return duct(do=do, tutor=tutor, student=student) run flask do knit problem great migrate director necessary file follow output: great director /users/famine/projects/flask/flask-tutoring/might ... done great director /users/famine/projects/flask/flask-tutoring/migrations/very ... done genet /users/famine/projects/flask/flask-tutoring/migrations/script.by.make ... done genet /users/famine/projects/flask/flask-tutoring/migrations/end.i ... done genet /users/famine/projects/flask/flask-tutoring/migrations/ready ... done genet /users/famine/projects/flask/flask-tutoring/migrations/alembic.in ... done pleas edit configuration/connection/log set '/users/famine/projects/flask/flask-tutoring/migrations/alembic.in' proceeding. try run flask do migrate limb can't detect i'v got table pp/models.by. get follow output: into [alembic.auntie.migration] context imply sqliteimpl. into [alembic.auntie.migration] assume non-transact del. into [alembic.end] change scheme detected. migrate script created, though models.i exist. apology repeat question, can't find not example first migrate fail migrate script created. i'v try check already table great somewhere run do.drop_all() shell seem problem. update figure way sole would like better understand worked. re-am pp.i flasktutor.i re-ran export flask_app='flasktutor.by'. subsequ migrate work perfectly. pleas could someone explain file call pp.i use export flask_app='pp.by' migrate resist change scheme."
50505893,Updating .NET framework resulting in SQL timeouts,"We have an app which targets .NET 4.5.1, and this has remained unchanged.
However when we upgraded the .NET framework on the server from 4.5.1 -> 4.7.1, we started experiencing SQL timeouts several hours afterwards (the app target remained at 4.5.1).
""Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.""
Other servers which had the same treatment produced the issue also, so we looked for a breaking change in .NET, and found this article: https://blogs.msdn.microsoft.com/dataaccesstechnologies/2016/05/07/connection-timeout-issue-with-net-framework-4-6-1-transparentnetworkipresolution/
That article quotes a different exception type, but might be somewhat related. However I'd be stunned if our DNS lookup took longer than 500ms. Also I'd expect to see far more cases of this connection string config reported and used.
Our app is high traffic, but we're confident we're not leaking connections as this has never been an issues for years until we updated the .NET framework. 
We're going to try applying this fix (and wait >24 hours to see the results), but is there anything else we could have missed? We're not confident this is the solution.
EDIT: Even after rolling .NET back to 4.5.1 and restart all servers, we're still seeing the problem. Nothing else has changed in the codebase, but we've yet to roll back a registry change which enabled 'SchUseStrongCrypto' - if that could be the cause?
",<c#><sql><.net><connection-pooling>,1538,2,1,4373,2,45,75,72,3032,0.0,472,1,26,2018-05-24 9:35,2018-06-02 20:54,,9.0,,Intermediate,23,"<c#><sql><.net><connection-pooling>, Updating .NET framework resulting in SQL timeouts, We have an app which targets .NET 4.5.1, and this has remained unchanged.
However when we upgraded the .NET framework on the server from 4.5.1 -> 4.7.1, we started experiencing SQL timeouts several hours afterwards (the app target remained at 4.5.1).
""Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.""
Other servers which had the same treatment produced the issue also, so we looked for a breaking change in .NET, and found this article: https://blogs.msdn.microsoft.com/dataaccesstechnologies/2016/05/07/connection-timeout-issue-with-net-framework-4-6-1-transparentnetworkipresolution/
That article quotes a different exception type, but might be somewhat related. However I'd be stunned if our DNS lookup took longer than 500ms. Also I'd expect to see far more cases of this connection string config reported and used.
Our app is high traffic, but we're confident we're not leaking connections as this has never been an issues for years until we updated the .NET framework. 
We're going to try applying this fix (and wait >24 hours to see the results), but is there anything else we could have missed? We're not confident this is the solution.
EDIT: Even after rolling .NET back to 4.5.1 and restart all servers, we're still seeing the problem. Nothing else has changed in the codebase, but we've yet to roll back a registry change which enabled 'SchUseStrongCrypto' - if that could be the cause?
","<c#><sal><.net><connection-pooling>, update .net framework result sal timeouts, pp target .net 4.5.1, remain unchanged. howe upgrade .net framework server 4.5.1 -> 4.7.1, start experience sal timeout never hour afterward (the pp target remain 4.5.1). ""timeout expired. timeout period leaps prior obtain connect pool. may occur pool connect use max pool size reached."" server treatment produce issue also, look break change .net, found article: http://blows.man.microsoft.com/dataaccesstechnologies/2016/05/07/connection-timeout-issue-with-net-framework-4-6-1-transparentnetworkipresolution/ article quit differ except type, might somewhat related. howe i'd sun in lockup took longer 500ms. also i'd expect see far case connect string confirm report used. pp high traffic, we'r confide we'r leak connect never issue year update .net framework. we'r go try apply fix (and wait >24 hour see results), any else could missed? we'r confide solution. edit: even roll .net back 4.5.1 start serves, we'r still see problem. not else change codebase, we'v yet roll back registry change enable 'schusestrongcrypto' - could cause?"
51242938,"Spring Boot Application gets stuck on ""Hikari-Pool-1 - Starting...""","I'm trying to run Spring Boot application connected to PostgreSQL database. However, when it comes to Hikari connection pool initializing, it just gets stuck and nothing goes on. HikariPool-1 - Starting... appears in logs and then nothing happens. 
Logs:
2018-07-09 15:32:48.475  INFO 21920 --- [           main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068: startup date [Mon Jul 09 15:32:48 ALMT 2018]; root of context hierarchy
2018-07-09 15:32:48.736  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'configurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.0.3.RELEASE)
2018-07-09 15:32:49.207  INFO 21920 --- [           main] o.paperplane.todoapp.TodoappApplication  : No active profile set, falling back to default profiles: default
2018-07-09 15:32:49.217  INFO 21920 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@1b410b60: startup date [Mon Jul 09 15:32:49 ALMT 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068
2018-07-09 15:32:49.763  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Dbcp2; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Dbcp2.class]] with [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.046  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=refresh; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=false; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]] with [Root bean: class [org.springframework.aop.scope.ScopedProxyFactoryBean]; scope=; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null; defined in BeanDefinition defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.212  INFO 21920 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=2203ab9b-5bb0-34f2-b496-2cbda1e334a2
2018-07-09 15:32:50.342  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$ff61cd29] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.380  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration' of type [org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration$$EnhancerBySpringCGLIB$$980f9563] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.389  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'objectPostProcessor' of type [org.springframework.security.config.annotation.configuration.AutowireBeanFactoryObjectPostProcessor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.393  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.access.expression.method.DefaultMethodSecurityExpressionHandler@309028af' of type [org.springframework.security.oauth2.provider.expression.OAuth2MethodSecurityExpressionHandler] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.397  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration' of type [org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration$$EnhancerBySpringCGLIB$$bce43815] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.404  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'methodSecurityMetadataSource' of type [org.springframework.security.access.method.DelegatingMethodSecurityMetadataSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.424  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.692  INFO 21920 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2018-07-09 15:32:50.710  INFO 21920 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2018-07-09 15:32:50.710  INFO 21920 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.31
2018-07-09 15:32:50.714  INFO 21920 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [C:\Program Files\Java\jdk1.8.0_151\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Intel\Shared Files\cpp\bin\Intel64;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\7-Zip;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio\;D:\node_js\;C:\Program Files\Git\cmd;C:\WINDOWS\System32\OpenSSH\;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\GitHubDesktop\bin;C:\Users\User\AppData\Roaming\npm;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;;.]
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1600 ms
2018-07-09 15:32:51.784  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] .s.DelegatingFilterProxyRegistrationBean : Mapping filter: 'springSecurityFilterChain' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
2018-07-09 15:32:51.885  INFO 21920 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
My pom.xml:
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;org.paperplane&lt;/groupId&gt;
    &lt;artifactId&gt;todoapp&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;todoapp&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.3.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Finchley.RELEASE&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
My apllication.properties:
spring.datasource.url=jdbc:postgresql://127.0.0.1:55491/TodoAppDatabase
spring.datasource.username=admin
spring.datasource.password=root
spring.datasource.hikari.connection-timeout=10000
spring.datasource.hikari.driver-class-name=org.postgresql.Driver
spring.datasource.hikari.maximum-pool-size=100
spring.data.jpa.repositories.enabled=true
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto = update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQL94Dialect
spring.jpa.properties.hibernate.cache.use_query_cache=true
spring.jpa.properties.hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
How can I overcome this issue?
",<spring><postgresql><spring-boot>,15840,4,153,252,1,3,5,46,52392,,2,10,26,2018-07-09 10:06,2018-11-14 13:30,,128.0,,Intermediate,23,"<spring><postgresql><spring-boot>, Spring Boot Application gets stuck on ""Hikari-Pool-1 - Starting..."", I'm trying to run Spring Boot application connected to PostgreSQL database. However, when it comes to Hikari connection pool initializing, it just gets stuck and nothing goes on. HikariPool-1 - Starting... appears in logs and then nothing happens. 
Logs:
2018-07-09 15:32:48.475  INFO 21920 --- [           main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068: startup date [Mon Jul 09 15:32:48 ALMT 2018]; root of context hierarchy
2018-07-09 15:32:48.736  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'configurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.0.3.RELEASE)
2018-07-09 15:32:49.207  INFO 21920 --- [           main] o.paperplane.todoapp.TodoappApplication  : No active profile set, falling back to default profiles: default
2018-07-09 15:32:49.217  INFO 21920 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@1b410b60: startup date [Mon Jul 09 15:32:49 ALMT 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068
2018-07-09 15:32:49.763  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Dbcp2; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Dbcp2.class]] with [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.046  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=refresh; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=false; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]] with [Root bean: class [org.springframework.aop.scope.ScopedProxyFactoryBean]; scope=; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null; defined in BeanDefinition defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.212  INFO 21920 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=2203ab9b-5bb0-34f2-b496-2cbda1e334a2
2018-07-09 15:32:50.342  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$ff61cd29] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.380  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration' of type [org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration$$EnhancerBySpringCGLIB$$980f9563] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.389  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'objectPostProcessor' of type [org.springframework.security.config.annotation.configuration.AutowireBeanFactoryObjectPostProcessor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.393  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.access.expression.method.DefaultMethodSecurityExpressionHandler@309028af' of type [org.springframework.security.oauth2.provider.expression.OAuth2MethodSecurityExpressionHandler] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.397  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration' of type [org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration$$EnhancerBySpringCGLIB$$bce43815] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.404  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'methodSecurityMetadataSource' of type [org.springframework.security.access.method.DelegatingMethodSecurityMetadataSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.424  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.692  INFO 21920 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2018-07-09 15:32:50.710  INFO 21920 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2018-07-09 15:32:50.710  INFO 21920 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.31
2018-07-09 15:32:50.714  INFO 21920 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [C:\Program Files\Java\jdk1.8.0_151\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Intel\Shared Files\cpp\bin\Intel64;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\7-Zip;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio\;D:\node_js\;C:\Program Files\Git\cmd;C:\WINDOWS\System32\OpenSSH\;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\GitHubDesktop\bin;C:\Users\User\AppData\Roaming\npm;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;;.]
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1600 ms
2018-07-09 15:32:51.784  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] .s.DelegatingFilterProxyRegistrationBean : Mapping filter: 'springSecurityFilterChain' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
2018-07-09 15:32:51.885  INFO 21920 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
My pom.xml:
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;org.paperplane&lt;/groupId&gt;
    &lt;artifactId&gt;todoapp&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;todoapp&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.3.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Finchley.RELEASE&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
My apllication.properties:
spring.datasource.url=jdbc:postgresql://127.0.0.1:55491/TodoAppDatabase
spring.datasource.username=admin
spring.datasource.password=root
spring.datasource.hikari.connection-timeout=10000
spring.datasource.hikari.driver-class-name=org.postgresql.Driver
spring.datasource.hikari.maximum-pool-size=100
spring.data.jpa.repositories.enabled=true
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto = update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQL94Dialect
spring.jpa.properties.hibernate.cache.use_query_cache=true
spring.jpa.properties.hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
How can I overcome this issue?
","<spring><postgresql><spring-boot>, spring boot applied get stuck ""kari-pool-1 - starting..."", i'm try run spring boot applied connect postgresql database. however, come kari connect pool initializing, get stuck not go on. hikaripool-1 - starting... appear log not happens. logs: 2018-07-09 15:32:48.475 into 21920 --- [ main] s.c.a.annotationconfigapplicationcontext : refresh org.springframework.context.annexation.annotationconfigapplicationcontext@68999068: started date [mon july 09 15:32:48 alms 2018]; root context hierarchy 2018-07-09 15:32:48.736 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'configurationpropertiesrebinderautoconfiguration' type [org.springframework.cloud.autoconfigure.configurationpropertiesrebinderautoconfiguration$$enhancerbyspringcglib$$1b7bd026] fig get process beanpostprocessor (for example: fig auto-proving) . ____ _ of _ _ /\\ / ___'_ of _ _(_)_ of of _ \ \ \ \ ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .of|_| |_|_| |_\of, | / / / / =========|_|==============|___/=/_/_/_/ :: spring boot :: (ve.0.3.release) 2018-07-09 15:32:49.207 into 21920 --- [ main] o.paperplane.todoapp.todoappappl : active profit set, fall back default profile: default 2018-07-09 15:32:49.217 into 21920 --- [ main] configservletwebserverapplicationcontext : refresh org.springframework.boot.web.serve.context.annotationconfigservletwebserverapplicationcontext@1b410b60: started date [mon july 09 15:32:49 alms 2018]; parent: org.springframework.context.annexation.annotationconfigapplicationcontext@68999068 2018-07-09 15:32:49.763 into 21920 --- [ main] o.s.b.f.s.defaultlistablebeanfactori : overdid bean definite bean 'datasource' differ definition: replace [root bean: class [null]; scope=; abstract=false; lazyinit=false; autowiremode=3; dependencycheck=0; autowirecandidate=true; primary=false; factorybeanname=org.springframework.boot.autoconfigure.job.datasourceconfiguration$dbcp2; factorymethodname=datasource; initmethodname=null; destroymethodname=(inferred); define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$dbcp2.class]] [root bean: class [null]; scope=; abstract=false; lazyinit=false; autowiremode=3; dependencycheck=0; autowirecandidate=true; primary=false; factorybeanname=org.springframework.boot.autoconfigure.job.datasourceconfiguration$kari; factorymethodname=datasource; initmethodname=null; destroymethodname=(inferred); define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]] 2018-07-09 15:32:50.046 into 21920 --- [ main] o.s.b.f.s.defaultlistablebeanfactori : overdid bean definite bean 'datasource' differ definition: replace [root bean: class [null]; scope=refresh; abstract=false; lazyinit=false; autowiremode=3; dependencycheck=0; autowirecandidate=false; primary=false; factorybeanname=org.springframework.boot.autoconfigure.job.datasourceconfiguration$kari; factorymethodname=datasource; initmethodname=null; destroymethodname=(inferred); define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]] [root bean: class [org.springframework.top.scope.scopedproxyfactorybean]; scope=; abstract=false; lazyinit=false; autowiremode=0; dependencycheck=0; autowirecandidate=true; primary=false; factorybeanname=null; factorymethodname=null; initmethodname=null; destroymethodname=null; define beandefinit define class path resource [org/springframework/boot/autoconfigure/job/datasourceconfiguration$kari.class]] 2018-07-09 15:32:50.212 into 21920 --- [ main] o.s.cloud.context.scope.genericscop : beanfactori id=2203ab9b-abbe-34f2-b496-2cbda1e334a2 2018-07-09 15:32:50.342 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'org.springframework.transaction.annexation.proxytransactionmanagementconfiguration' type [org.springframework.transaction.annexation.proxytransactionmanagementconfiguration$$enhancerbyspringcglib$$ff61cd29] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.380 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'org.springframework.security.confirm.annexation.configuration.objectpostprocessorconfiguration' type [org.springframework.security.confirm.annexation.configuration.objectpostprocessorconfiguration$$enhancerbyspringcglib$$980f9563] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.389 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'objectpostprocessor' type [org.springframework.security.confirm.annexation.configuration.autowirebeanfactoryobjectpostprocessor] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.393 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'org.springframework.security.access.expression.method.defaultmethodsecurityexpressionhandler@309028af' type [org.springframework.security.oath.provider.expression.oauth2methodsecurityexpressionhandler] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.397 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'org.springframework.security.confirm.annexation.method.configuration.globalmethodsecurityconfiguration' type [org.springframework.security.confirm.annexation.method.configuration.globalmethodsecurityconfiguration$$enhancerbyspringcglib$$bce43815] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.404 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'methodsecuritymetadatasource' type [org.springframework.security.access.method.delegatingmethodsecuritymetadatasource] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.424 into 21920 --- [ main] trationdelegate$beanpostprocessorcheck : bean 'org.springframework.cloud.autoconfigure.configurationpropertiesrebinderautoconfiguration' type [org.springframework.cloud.autoconfigure.configurationpropertiesrebinderautoconfiguration$$enhancerbyspringcglib$$1b7bd026] fig get process beanpostprocessor (for example: fig auto-proving) 2018-07-09 15:32:50.692 into 21920 --- [ main] o.s.b.w.embedded.combat.tomcatwebserv : combat into port(s): 8080 (http) 2018-07-09 15:32:50.710 into 21920 --- [ main] o.apache.carolina.core.standardservic : start service [combat] 2018-07-09 15:32:50.710 into 21920 --- [ main] org.apache.carolina.core.standardengin : start serve engine: apace combat/8.5.31 2018-07-09 15:32:50.714 into 21920 --- [out-startstop-1] o.a.carolina.core.aprlifecyclelisten : air base apace combat native library allow optic perform product environs found cava.library.path: [c:\program files\cava\joke.8.0_151\bin;c:\windows\sun\cava\bin;c:\windows\system;c:\windows;c:\program file (x)\common files\oracle\cava\javapath;c:\program file (x)\common files\inter\scar files\pp\bin\intel64;c:\programdata\oracle\cava\javapath;c:\program file (x)\inter\ill client\;c:\program files\inter\ill client\;c:\windows\system;c:\windows;c:\windows\system\be;c:\windows\system\windowspowershell\ve.0\;c:\program file (x)\india corporation\phase\common;c:\program file (x)\inter\inter(r) manage engine components\day;c:\program files\inter\inter(r) manage engine components\day;c:\program file (x)\inter\inter(r) manage engine components\it;c:\program files\inter\inter(r) manage engine components\it;c:\program files\7-zip;c:\windows\system;c:\windows;c:\windows\system\be;c:\windows\system\windowspowershell\ve.0\;c:\program files\microsoft sal server\coli sd\duc\130\tools\inn\;c:\program file (x)\microsoft sal server\140\tools\inn\;c:\program files\microsoft sal server\140\tools\inn\;c:\program files\microsoft sal server\140\its\inn\;c:\program file (x)\microsoft sal server\coli sd\duc\130\tools\inn\;c:\program file (x)\microsoft sal server\140\its\inn\;c:\program file (x)\microsoft sal server\140\tools\inn\managementstudio\;d:\nodes\;c:\program files\git\cod;c:\windows\system\opens\;c:\users\user\appdata\local\microsoft\windowsapps;c:\users\user\appdata\local\githubdesktop\bin;c:\users\user\appdata\roaming\nom;%userprofile%\appdata\local\microsoft\windowsapps;;.] 2018-07-09 15:32:50.817 into 21920 --- [out-startstop-1] o.a.c.c.c.[combat].[localhost].[/] : into spring ebbed webapplicationcontext 2018-07-09 15:32:50.817 into 21920 --- [out-startstop-1] o.s.web.context.contextload : root webapplicationcontext: into complete 1600 ms 2018-07-09 15:32:51.784 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'characterencodingfilter' to: [/*] 2018-07-09 15:32:51.785 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'hiddenhttpmethodfilter' to: [/*] 2018-07-09 15:32:51.785 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'httpputformcontentfilter' to: [/*] 2018-07-09 15:32:51.785 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'requestcontextfilter' to: [/*] 2018-07-09 15:32:51.785 into 21920 --- [out-startstop-1] .s.delegatingfilterproxyregistrationbean : map filter: 'springsecurityfilterchain' to: [/*] 2018-07-09 15:32:51.785 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'httptracefilter' to: [/*] 2018-07-09 15:32:51.786 into 21920 --- [out-startstop-1] o.s.b.w.serve.filterregistrationbean : map filter: 'webmvcmetricsfilter' to: [/*] 2018-07-09 15:32:51.786 into 21920 --- [out-startstop-1] o.s.b.w.serve.servletregistrationbean : serve dispatcherservlet map [/] 2018-07-09 15:32:51.885 into 21920 --- [ main] com.baxter.kari.hikaridatasourc : hikaripool-1 - starting... pot.all: &it;?all version=""1.0"" encoding=""utf-8""?&it; &it;project omens=""http://haven.apache.org/pot/4.0.0"" omens:xvi=""http://www.we.org/2001/xmlschema-instance"" xvi:schemalocation=""http://haven.apache.org/pot/4.0.0 http://haven.apache.org/sd/haven-4.0.0.sd""&it; &it;modelversion&it;4.0.0&it;/modelversion&it; &it;grouped&it;org.paperplane&it;/grouped&it; &it;artifactid&it;todoapp&it;/artifactid&it; &it;version&it;0.0.1-snapshot&it;/version&it; &it;packing&it;jar&it;/packing&it; &it;name&it;todoapp&it;/name&it; &it;description&it;domo project spring boot&it;/description&it; &it;parent&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-parent&it;/artifactid&it; &it;version&it;2.0.3.release&it;/version&it; &it;relativepath/&it; &it;!-- lockup parent depositors --&it; &it;/parent&it; &it;properties&it; &it;project.build.sourceencoding&it;utf-8&it;/project.build.sourceencoding&it; &it;project.reporting.outputencoding&it;utf-8&it;/project.reporting.outputencoding&it; &it;cava.version&it;1.8&it;/cava.version&it; &it;spring-cloud.version&it;finchley.release&it;/spring-cloud.version&it; &it;/properties&it; &it;dependencies&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-data-pa&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-security&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-web&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.cloud&it;/grouped&it; &it;artifactid&it;spring-cloud-started-oath&it;/artifactid&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.fasterxml.jackson.core&it;/grouped&it; &it;artifactid&it;jackson-databind&it;/artifactid&it; &it;version&it;2.9.3&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;com.fasterxml.jackson.core&it;/grouped&it; &it;artifactid&it;jackson-innovations&it;/artifactid&it; &it;version&it;2.9.3&it;/version&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.postgresql&it;/grouped&it; &it;artifactid&it;postgresql&it;/artifactid&it; &it;scope&it;auntie&it;/scope&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.projectlombok&it;/grouped&it; &it;artifactid&it;look&it;/artifactid&it; &it;optional&it;true&it;/optional&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-started-test&it;/artifactid&it; &it;scope&it;test&it;/scope&it; &it;/dependency&it; &it;dependency&it; &it;grouped&it;org.springframework.security&it;/grouped&it; &it;artifactid&it;spring-security-test&it;/artifactid&it; &it;scope&it;test&it;/scope&it; &it;/dependency&it; &it;/dependencies&it; &it;dependencymanagement&it; &it;dependencies&it; &it;dependency&it; &it;grouped&it;org.springframework.cloud&it;/grouped&it; &it;artifactid&it;spring-cloud-dependencies&it;/artifactid&it; &it;version&it;${spring-cloud.version}&it;/version&it; &it;type&it;pot&it;/type&it; &it;scope&it;import&it;/scope&it; &it;/dependency&it; &it;/dependencies&it; &it;/dependencymanagement&it; &it;build&it; &it;plains&it; &it;plain&it; &it;grouped&it;org.springframework.boot&it;/grouped&it; &it;artifactid&it;spring-boot-haven-plain&it;/artifactid&it; &it;/plain&it; &it;/plains&it; &it;/build&it; &it;/project&it; application.properties: spring.datasource.curl=job:postgresql://127.0.0.1:55491/todoappdatabas spring.datasource.surname=admit spring.datasource.password=root spring.datasource.kari.connection-timeout=10000 spring.datasource.kari.driver-class-name=org.postgresql.drive spring.datasource.kari.maximum-pool-size=100 spring.data.pa.depositaries.enabled=true spring.pa.show-sal=true spring.pa.liberate.del-auto = update spring.pa.properties.liberate.dialect=org.liberate.dialect.postgresql94dialect spring.pa.properties.liberate.ache.use_query_cache=true spring.pa.properties.liberate.connection.provider_class=com.baxter.kari.liberate.hikariconnectionprovid overcome issue?"
48280776,NHibernate HQL Generator to support SQL Server 2016 temporal tables,"I am trying to implement basic support for SQL Server 2016 temporal tables in NHibernate 4.x. The idea is to alter SQL statement from  
SELECT * FROM Table t0
to  
SELECT * FROM Table FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00' t0
You can find more info about temporal tables in SQL Server 2016 here
Unfortunately, I've not found any way to insert FOR FOR SYSTEM_TIME AS OF '...' statement between table name and its alias. I'm not sure if custom dialects supports this. The only working solution I have for now is to append FOR SYSTEM_TIME statement within extra WHERE and my output SQL looks like this
SELECT * FROM Table t0 WHERE FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00'=1
To do so, I have implemented generator and dialect as follows:
public static class AuditableExtensions
{
    public static bool AsOf(this IAuditable entity, DateTime date)
    {
        return true;
    }
    public static IQueryable&lt;T&gt; Query&lt;T&gt;(this ISession session, DateTime asOf) where T : IAuditable
    {
        return session.Query&lt;T&gt;().Where(x =&gt; x.AsOf(asOf));
    }
}
public class ForSystemTimeGenerator : BaseHqlGeneratorForMethod
{
    public static readonly string ForSystemTimeAsOfString = ""FOR SYSTEM_TIME AS OF"";
    public ForSystemTimeGenerator()
    {
        SupportedMethods = new[]
        {
            ReflectionHelper.GetMethod(() =&gt; AuditableExtensions.AsOf(null, DateTime.MinValue))
        };
    }
    public override HqlTreeNode BuildHql(MethodInfo method, Expression targetObject, 
        ReadOnlyCollection&lt;Expression&gt; arguments,
        HqlTreeBuilder treeBuilder, 
        IHqlExpressionVisitor visitor)
    {
        return treeBuilder.BooleanMethodCall(nameof(AuditableExtensions.AsOf), new[]
        {
            visitor.Visit(arguments[1]).AsExpression()
        });
    }
}
public class MsSql2016Dialect : MsSql2012Dialect
{
    public MsSql2016Dialect()
    {
        RegisterFunction(nameof(AuditableExtensions.AsOf), new SQLFunctionTemplate(
            NHibernateUtil.Boolean, 
            $""{ForSystemTimeGenerator.ForSystemTimeAsOfString} ?1?2=1""));
    }
}
Can anyone provide any better approach or samples I could use to move forward and insert FOR SYSTEM_TIME AS OF statement between table name and its alias? At this moment the only solution I can see is to alter SQL in OnPrepareStatement in SessionInterceptor but I believe there is some better approach...
",<c#><sql-server><nhibernate><sql-server-2016><temporal-tables>,2420,1,55,762,0,6,25,56,1022,0.0,54,3,26,2018-01-16 12:00,2018-05-10 5:47,,114.0,,Intermediate,18,"<c#><sql-server><nhibernate><sql-server-2016><temporal-tables>, NHibernate HQL Generator to support SQL Server 2016 temporal tables, I am trying to implement basic support for SQL Server 2016 temporal tables in NHibernate 4.x. The idea is to alter SQL statement from  
SELECT * FROM Table t0
to  
SELECT * FROM Table FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00' t0
You can find more info about temporal tables in SQL Server 2016 here
Unfortunately, I've not found any way to insert FOR FOR SYSTEM_TIME AS OF '...' statement between table name and its alias. I'm not sure if custom dialects supports this. The only working solution I have for now is to append FOR SYSTEM_TIME statement within extra WHERE and my output SQL looks like this
SELECT * FROM Table t0 WHERE FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00'=1
To do so, I have implemented generator and dialect as follows:
public static class AuditableExtensions
{
    public static bool AsOf(this IAuditable entity, DateTime date)
    {
        return true;
    }
    public static IQueryable&lt;T&gt; Query&lt;T&gt;(this ISession session, DateTime asOf) where T : IAuditable
    {
        return session.Query&lt;T&gt;().Where(x =&gt; x.AsOf(asOf));
    }
}
public class ForSystemTimeGenerator : BaseHqlGeneratorForMethod
{
    public static readonly string ForSystemTimeAsOfString = ""FOR SYSTEM_TIME AS OF"";
    public ForSystemTimeGenerator()
    {
        SupportedMethods = new[]
        {
            ReflectionHelper.GetMethod(() =&gt; AuditableExtensions.AsOf(null, DateTime.MinValue))
        };
    }
    public override HqlTreeNode BuildHql(MethodInfo method, Expression targetObject, 
        ReadOnlyCollection&lt;Expression&gt; arguments,
        HqlTreeBuilder treeBuilder, 
        IHqlExpressionVisitor visitor)
    {
        return treeBuilder.BooleanMethodCall(nameof(AuditableExtensions.AsOf), new[]
        {
            visitor.Visit(arguments[1]).AsExpression()
        });
    }
}
public class MsSql2016Dialect : MsSql2012Dialect
{
    public MsSql2016Dialect()
    {
        RegisterFunction(nameof(AuditableExtensions.AsOf), new SQLFunctionTemplate(
            NHibernateUtil.Boolean, 
            $""{ForSystemTimeGenerator.ForSystemTimeAsOfString} ?1?2=1""));
    }
}
Can anyone provide any better approach or samples I could use to move forward and insert FOR SYSTEM_TIME AS OF statement between table name and its alias? At this moment the only solution I can see is to alter SQL in OnPrepareStatement in SessionInterceptor but I believe there is some better approach...
","<c#><sal-server><nhibernate><sal-server-2016><temporal-tables>, nhibern he genet support sal server 2016 temper tables, try implement basic support sal server 2016 temper table nhibern 4.x. idea alter sal statement select * table to select * table systematic '2018-01-16 00:00:00' to find into temper table sal server 2016 unfortunately, i'v found way insert systematic '...' statement table name alias. i'm sure custom dialect support this. work slut happened systematic statement within extra output sal look like select * table to systematic '2018-01-16 00:00:00'=1 so, implement genet dialect follows: public static class auditableextens { public static book of(the audit entity, datetim date) { return true; } public static iqueryable&it;t&it; query&it;t&it;(the less session, datetim of) : audit { return session.query&it;t&it;().where(x =&it; x.of(of)); } } public class forsystemtimegener : basehqlgeneratorformethod { public static readonli string forsystemtimeasofstr = ""for systematic of""; public forsystemtimegenerator() { supportedmethod = new[] { reflectionhelper.getmethod(() =&it; auditableextensions.of(null, daytime.minvalue)) }; } public overdid hqltreenod buildhql(methodinfo method, express targetobject, readonlycollection&it;expression&it; arguments, hqltreebuild treebuilder, ihqlexpressionvisitor visitor) { return treebuilder.booleanmethodcall(name(auditableextensions.of), new[] { visitor.visit(arguments[1]).expression() }); } } public class mssql2016dialect : mssql2012dialect { public mssql2016dialect() { registerfunction(name(auditableextensions.of), new sqlfunctiontemplate( nhibernateutil.woolen, $""{forsystemtimegenerator.forsystemtimeasofstring} ?1?2=1"")); } } anyone proved better approach sample could use move forward insert systematic statement table name alias? moment slut see alter sal onpreparestat sessioninterceptor believe better approach..."
48777993,How do I add a column to a nested struct in a PySpark dataframe?,"I have a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
I'd like to add columns within the state struct, that is, create a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
 |    |-- a: integer (nullable = true)
I tried
df.withColumn('state.a', val).printSchema()
# root
#  |-- state: struct (nullable = true)
#  |    |-- fld: integer (nullable = true)
#  |-- state.a: integer (nullable = true)
",<dataframe><apache-spark><pyspark><struct><apache-spark-sql>,522,0,13,2007,4,23,39,57,55595,0.0,12,7,26,2018-02-14 0:43,2018-02-14 15:26,2018-02-14 15:26,0.0,0.0,Basic,2,"<dataframe><apache-spark><pyspark><struct><apache-spark-sql>, How do I add a column to a nested struct in a PySpark dataframe?, I have a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
I'd like to add columns within the state struct, that is, create a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
 |    |-- a: integer (nullable = true)
I tried
df.withColumn('state.a', val).printSchema()
# root
#  |-- state: struct (nullable = true)
#  |    |-- fld: integer (nullable = true)
#  |-- state.a: integer (nullable = true)
","<dataframe><apache-spark><spark><struck><apache-spark-sal>, add column nest struck spark dataframe?, datafram scheme like root |-- state: struck (nullabl = true) | |-- old: inter (nullabl = true) i'd like add column within state struck, is, great datafram scheme like root |-- state: struck (nullabl = true) | |-- old: inter (nullabl = true) | |-- a: inter (nullabl = true) try of.withcolumn('state.a', val).printschema() # root # |-- state: struck (nullabl = true) # | |-- old: inter (nullabl = true) # |-- state.a: inter (nullabl = true)"
48243734,Is there a way to have table name automatically added to Eloquent query methods?,"I'm developing an app on Laravel 5.5 and I'm facing an issue with a specific query scope. I have the following table structure (some fields omitted):
orders
---------
id
parent_id
status
The parent_id column references the id from the same table. I have this query scope to filter records that don't have any children:
public function scopeNoChildren(Builder $query): Builder
{
    return $query-&gt;select('orders.*')
        -&gt;leftJoin('orders AS children', function ($join) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;where('children.status', self::STATUS_COMPLETED);
        })
        -&gt;where('children.id', null);
}
This scope works fine when used alone. However, if I try to combine it with any another condition, it throws an SQL exception:
Order::where('status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
Leads to this:
  SQLSTATE[23000]: Integrity constraint violation: 1052 Column 'status' in where clause is ambiguous
I found two ways to avoid that error:
Solution #1: Prefix all other conditions with the table name
Doing something like this works:
Order::where('orders.status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
But I don't think this is a good approach since it's not clear the table name is required in case other dev or even myself try to use that scope again in the future. They'll probably end up figuring that out, but it doesn't seem a good practice.
Solution #2: Use a subquery
I can keep the ambiguous columns apart in a subquery. Still, in this case and as the table grows, the performance will degrade.
This is the strategy I'm using, though. Because it doesn't require any change to other scopes and conditions. At least not in the way I'm applying it right now.
public function scopeNoChildren(Builder $query): Builder
{
    $subQueryChildren = self::select('id', 'parent_id')
        -&gt;completed();
    $sqlChildren = DB::raw(sprintf(
        '(%s) AS children',
        $subQueryChildren-&gt;toSql()
    ));
    return $query-&gt;select('orders.*')
        -&gt;leftJoin($sqlChildren, function ($join) use ($subQueryChildren) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;addBinding($subQueryChildren-&gt;getBindings());
         })-&gt;where('children.id', null);
}
The perfect solution
I think that having the ability to use queries without prefixing with table name without relying on subqueries would be the perfect solution.
That's why I'm asking: Is there a way to have table name automatically added to Eloquent query methods?
",<php><mysql><laravel><eloquent>,2608,0,37,3774,6,40,63,41,4259,0.0,2480,3,25,2018-01-13 19:52,2018-06-02 19:56,2018-06-04 14:57,140.0,142.0,Intermediate,20,"<php><mysql><laravel><eloquent>, Is there a way to have table name automatically added to Eloquent query methods?, I'm developing an app on Laravel 5.5 and I'm facing an issue with a specific query scope. I have the following table structure (some fields omitted):
orders
---------
id
parent_id
status
The parent_id column references the id from the same table. I have this query scope to filter records that don't have any children:
public function scopeNoChildren(Builder $query): Builder
{
    return $query-&gt;select('orders.*')
        -&gt;leftJoin('orders AS children', function ($join) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;where('children.status', self::STATUS_COMPLETED);
        })
        -&gt;where('children.id', null);
}
This scope works fine when used alone. However, if I try to combine it with any another condition, it throws an SQL exception:
Order::where('status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
Leads to this:
  SQLSTATE[23000]: Integrity constraint violation: 1052 Column 'status' in where clause is ambiguous
I found two ways to avoid that error:
Solution #1: Prefix all other conditions with the table name
Doing something like this works:
Order::where('orders.status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
But I don't think this is a good approach since it's not clear the table name is required in case other dev or even myself try to use that scope again in the future. They'll probably end up figuring that out, but it doesn't seem a good practice.
Solution #2: Use a subquery
I can keep the ambiguous columns apart in a subquery. Still, in this case and as the table grows, the performance will degrade.
This is the strategy I'm using, though. Because it doesn't require any change to other scopes and conditions. At least not in the way I'm applying it right now.
public function scopeNoChildren(Builder $query): Builder
{
    $subQueryChildren = self::select('id', 'parent_id')
        -&gt;completed();
    $sqlChildren = DB::raw(sprintf(
        '(%s) AS children',
        $subQueryChildren-&gt;toSql()
    ));
    return $query-&gt;select('orders.*')
        -&gt;leftJoin($sqlChildren, function ($join) use ($subQueryChildren) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;addBinding($subQueryChildren-&gt;getBindings());
         })-&gt;where('children.id', null);
}
The perfect solution
I think that having the ability to use queries without prefixing with table name without relying on subqueries would be the perfect solution.
That's why I'm asking: Is there a way to have table name automatically added to Eloquent query methods?
","<pp><myself><travel><eloquent>, way table name automatic ad elope query methods?, i'm develop pp travel 5.5 i'm face issue specie query scope. follow table structure (some field omitted): order --------- id parent_id state parent_id column refer id table. query scope filter record children: public function scopenochildren(build $query): builder { return $query-&it;select('orders.*') -&it;leftjoin('ord children', function ($join) { $join-&it;on('orders.id', '=', 'children.parent_id') -&it;where('children.status', self::status_completed); }) -&it;where('children.id', null); } scope work fine use alone. however, try combine not condition, throw sal exception: order::where('status', order::status_completed) -&it;children() -&it;get(); lead this: sqlstate[23000]: inter constraint violation: 1052 column 'status' class ambigu found two way avoid error: slut #1: prefix conduct table name cometh like works: order::where('orders.status', order::status_completed) -&it;children() -&it;get(); think good approach since clear table name require case de even try use scope future. they'll probably end figure out, seem good practice. slut #2: use subqueri keep ambigu column apart subquery. still, case table grows, perform degraded. strategic i'm using, though. require change scope conditions. least way i'm apply right now. public function scopenochildren(build $query): builder { $subquerychildren = self::select('id', 'parent_id') -&it;completed(); $sqlchildren = do::raw(spring( '(%s) children', $subquerychildren-&it;total() )); return $query-&it;select('orders.*') -&it;leftjoin($sqlchildren, function ($join) use ($subquerychildren) { $join-&it;on('orders.id', '=', 'children.parent_id') -&it;addbinding($subquerychildren-&it;getbindings()); })-&it;where('children.id', null); } perfect slut think bail use query without prefix table name without rely subqueri would perfect solution. that' i'm asking: way table name automatic ad elope query methods?"
55693241,How to securely connect to Cloud SQL from Cloud Run?,"How do I connect to the database on Cloud SQL without having to add my credentials file inside the container?
",<google-cloud-sql><google-cloud-run>,110,0,0,992,1,9,17,80,13914,0.0,3,3,25,2019-04-15 16:08,2019-04-28 16:25,2019-04-28 16:25,13.0,13.0,Intermediate,20,"<google-cloud-sql><google-cloud-run>, How to securely connect to Cloud SQL from Cloud Run?, How do I connect to the database on Cloud SQL without having to add my credentials file inside the container?
","<goose-cloud-sal><goose-cloud-run>, secure connect cloud sal cloud run?, connect database cloud sal without add credenti file inside container?"
56155514,Azure Cosmos DB SQL API UPDATE statement - don't replace whole document,"Can I write an UPDATE statement for Azure Cosmos DB? The SQL API supports queries, but what about updates?
In particular, I am looking to update documents without having to retrieve the whole document. I have the ID for the document and I know the exact path I want to update within the document. For example, say that my document is
{
  ""id"": ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"",
  ""name"": ""Fido"",
  ""weight"": 35,
  ""breed"": ""pomeranian"",
  ""diet"": {
    ""scoops"": 3,
    ""timesPerDay"": 2
  }
}
and I want to update diet.timesPerDay to 1 where the ID is ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"". Can I do that using the Azure SQL API without completely replacing the document?
",<azure><azure-cosmosdb><azure-cosmosdb-sqlapi>,679,0,13,6482,7,43,95,46,43375,0.0,1337,5,25,2019-05-15 18:23,2019-05-15 18:25,2022-03-10 12:07,0.0,1030.0,Basic,3,"<azure><azure-cosmosdb><azure-cosmosdb-sqlapi>, Azure Cosmos DB SQL API UPDATE statement - don't replace whole document, Can I write an UPDATE statement for Azure Cosmos DB? The SQL API supports queries, but what about updates?
In particular, I am looking to update documents without having to retrieve the whole document. I have the ID for the document and I know the exact path I want to update within the document. For example, say that my document is
{
  ""id"": ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"",
  ""name"": ""Fido"",
  ""weight"": 35,
  ""breed"": ""pomeranian"",
  ""diet"": {
    ""scoops"": 3,
    ""timesPerDay"": 2
  }
}
and I want to update diet.timesPerDay to 1 where the ID is ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"". Can I do that using the Azure SQL API without completely replacing the document?
","<azure><azure-cosmosdb><azure-cosmosdb-slap>, azur costo do sal apt update statement - replace whole document, write update statement azur costo do? sal apt support queried, updated? particular, look update document without retrieve whole document. id document know exact path want update within document. example, say document { ""id"": ""9e576f8b-146f-self-a68f-14ef816e0e50"", ""name"": ""did"", ""weight"": 35, ""breed"": ""pomerania"", ""diet"": { ""scoop"": 3, ""timesperday"": 2 } } want update diet.timesperday 1 id ""9e576f8b-146f-self-a68f-14ef816e0e50"". use azur sal apt without complete replace document?"
48270374,Invalid datetime format: 1366 Incorrect string value,"I'm getting this error:
  SQLSTATE[22007]: Invalid datetime format: 1366 Incorrect string value: '\xBD Inch...' for column 'column-name' at row 1
My database, table, and column have the format utf8mb4_unicode_ci also column-name is type text and NULL.
This is the value of the column-name
  [column-name] => Some text before 11 ▒ and other text after, and after.
However I wait that laravel adds quotes to column's values, because the values are separated by commas (,). It should be as follow:
  [column-name] => 'Some text before 11 ▒ and other text after, and after.'
See below the Schema
    Schema::create('mws_orders', function (Blueprint $table) {
        $table-&gt;string('custom-id');
        $table-&gt;string('name');
        $table-&gt;string('description')-&gt;nullable();
        $table-&gt;string('comment')-&gt;nullable();
        $table-&gt;integer('count')-&gt;nullable();
        $table-&gt;text('column-name')-&gt;nullable();
        $table-&gt;timestamps();
        $table-&gt;primary('custom-id');
    });
I have been looking for on google but not any solution, yet.
Anyone has an idea how to solve this issue?
I'm using Laravel 5.5 and MariaDB 10.2.11.
",<php><mysql><laravel><mariadb><laravel-5.5>,1177,0,11,1492,6,32,58,43,39927,0.0,206,6,25,2018-01-15 20:40,2018-01-15 21:27,2018-01-17 21:09,0.0,2.0,Basic,14,"<php><mysql><laravel><mariadb><laravel-5.5>, Invalid datetime format: 1366 Incorrect string value, I'm getting this error:
  SQLSTATE[22007]: Invalid datetime format: 1366 Incorrect string value: '\xBD Inch...' for column 'column-name' at row 1
My database, table, and column have the format utf8mb4_unicode_ci also column-name is type text and NULL.
This is the value of the column-name
  [column-name] => Some text before 11 ▒ and other text after, and after.
However I wait that laravel adds quotes to column's values, because the values are separated by commas (,). It should be as follow:
  [column-name] => 'Some text before 11 ▒ and other text after, and after.'
See below the Schema
    Schema::create('mws_orders', function (Blueprint $table) {
        $table-&gt;string('custom-id');
        $table-&gt;string('name');
        $table-&gt;string('description')-&gt;nullable();
        $table-&gt;string('comment')-&gt;nullable();
        $table-&gt;integer('count')-&gt;nullable();
        $table-&gt;text('column-name')-&gt;nullable();
        $table-&gt;timestamps();
        $table-&gt;primary('custom-id');
    });
I have been looking for on google but not any solution, yet.
Anyone has an idea how to solve this issue?
I'm using Laravel 5.5 and MariaDB 10.2.11.
","<pp><myself><travel><maria><travel-5.5>, invalid datetim format: 1366 incorrect string value, i'm get error: sqlstate[22007]: invalid datetim format: 1366 incorrect string value: '\and inch...' column 'column-name' row 1 database, table, column format utf8mb4_unicode_ci also column-am type text null. value column-am [column-name] => text 11 ▒ text after, after. howe wait travel add quit column' values, value spear comma (,). follow: [column-name] => 'some text 11 ▒ text after, after.' see scheme scheme::create('mws_orders', function (blueprint $table) { $table-&it;string('custom-id'); $table-&it;string('name'); $table-&it;string('description')-&it;syllable(); $table-&it;string('comment')-&it;syllable(); $table-&it;inter('count')-&it;syllable(); $table-&it;text('column-name')-&it;syllable(); $table-&it;timestamps(); $table-&it;primary('custom-id'); }); look good solution, yet. anyone idea sole issue? i'm use travel 5.5 maria 10.2.11."
54316131,How to create multiple tables in a database in sqflite?,"Im building and app with flutter that uses SQLite database. I have created first table using this piece of code:
 void _createDb(Database db, int newVersion) async {
    await db.execute('''CREATE TABLE cards (id_card INTEGER PRIMARY KEY, 
         color TEXT, type TEXT, rarity TEXT, name TEXT UNIQUE, goldCost INTEGER,
         manaCost INTEGER, armor INTEGER, attack INTEGER, health INTEGER, description TEXT)''');
}
Table gets created and I can access it without problems.
Unfortunately I cannot include more than 1 table that i just created. I tried adding another SQL CREATE TABLE clause in the same method, and repeating method db.execute with a different SQL clause just in the next line.
I'm mimicing code from this tutorial: https://www.youtube.com/watch?v=xke5_yGL0uk
How to add another table within the same database?
",<database><flutter><sqlite><dart><sqflite>,830,2,6,471,2,6,15,52,41385,0.0,4,7,25,2019-01-22 20:40,2019-02-04 8:15,,13.0,,Intermediate,15,"<database><flutter><sqlite><dart><sqflite>, How to create multiple tables in a database in sqflite?, Im building and app with flutter that uses SQLite database. I have created first table using this piece of code:
 void _createDb(Database db, int newVersion) async {
    await db.execute('''CREATE TABLE cards (id_card INTEGER PRIMARY KEY, 
         color TEXT, type TEXT, rarity TEXT, name TEXT UNIQUE, goldCost INTEGER,
         manaCost INTEGER, armor INTEGER, attack INTEGER, health INTEGER, description TEXT)''');
}
Table gets created and I can access it without problems.
Unfortunately I cannot include more than 1 table that i just created. I tried adding another SQL CREATE TABLE clause in the same method, and repeating method db.execute with a different SQL clause just in the next line.
I'm mimicing code from this tutorial: https://www.youtube.com/watch?v=xke5_yGL0uk
How to add another table within the same database?
","<database><flutter><quite><dart><sqflite>, great multiple table database sqflite?, in build pp flutter use quite database. great first table use piece code: void created(database do, in perversion) async { await do.execute('''or table card (discard inter primary key, color text, type text, rarity text, name text unique, goldcost inter, manacost inter, armor inter, attack inter, health inter, rescript text)'''); } table get great access without problems. unfortun cannot include 1 table created. try ad not sal great table class method, repeat method do.execute differ sal class next line. i'm mimi code tutoring: http://www.couture.com/watch?v=xke5_ygl0uk add not table within database?"
64350794,"typeORM: ""message"": ""Data type \""Object\"" in \""..."" is not supported by \""postgres\"" database.""","Given the following entity definition:
@Entity()
export class User extends BaseEntity {
  @Column({ nullable: true })
  name!: string | null;
  @Column()
  age!: number;
}
The following error appears:
typeORM:   &quot;message&quot;: &quot;Data type \&quot;Object\&quot; in \&quot;User.name&quot; is not supported by \&quot;postgres\&quot; database.&quot;
...
name: 'DataTypeNotSupportedError',
  message:
   'Data type &quot;Object&quot; in &quot;User.name&quot; is not supported by &quot;postgres&quot; database.' }
When looking at the build, I see that the metadata that's emitted by TS addresses it as object:
__decorate([
    typeorm_1.Column({ nullable: true }),
    __metadata(&quot;design:type&quot;, Object)
], User.prototype, &quot;name&quot;, void 0);
What am I doing wrong?
",<node.js><typescript><postgresql><typeorm><reflect-metadata>,785,0,19,6484,3,28,39,63,15355,0.0,50,2,25,2020-10-14 9:54,2020-10-14 9:54,,0.0,,Intermediate,15,"<node.js><typescript><postgresql><typeorm><reflect-metadata>, typeORM: ""message"": ""Data type \""Object\"" in \""..."" is not supported by \""postgres\"" database."", Given the following entity definition:
@Entity()
export class User extends BaseEntity {
  @Column({ nullable: true })
  name!: string | null;
  @Column()
  age!: number;
}
The following error appears:
typeORM:   &quot;message&quot;: &quot;Data type \&quot;Object\&quot; in \&quot;User.name&quot; is not supported by \&quot;postgres\&quot; database.&quot;
...
name: 'DataTypeNotSupportedError',
  message:
   'Data type &quot;Object&quot; in &quot;User.name&quot; is not supported by &quot;postgres&quot; database.' }
When looking at the build, I see that the metadata that's emitted by TS addresses it as object:
__decorate([
    typeorm_1.Column({ nullable: true }),
    __metadata(&quot;design:type&quot;, Object)
], User.prototype, &quot;name&quot;, void 0);
What am I doing wrong?
","<node.is><typescript><postgresql><typeorm><reflect-metadata>, typeorm: ""message"": ""data type \""object\"" \""..."" support \""postures\"" database."", given follow entity definition: @entity() export class user extend basement { @column({ syllable: true }) name!: string | null; @column() age!: number; } follow error appears: typeorm: &quit;message&quit;: &quit;data type \&quit;object\&quit; \&quit;user.name&quit; support \&quit;postures\&quit; database.&quit; ... name: 'datatypenotsupportederror', message: 'data type &quit;object&quit; &quit;user.name&quit; support &quit;postures&quit; database.' } look build, see metadata that' emit to address object: decorate([ typeorm_1.column({ syllable: true }), __metadata(&quit;design:type&quit;, object) ], user.prototype, &quit;name&quit;, void 0); wrong?"
49509615,How do I use parameters in VBA in the different contexts in Microsoft Access?,"I've read a lot about SQL injection, and using parameters, from sources like bobby-tables.com. However, I'm working with a complex application in Access, that has a lot of dynamic SQL with string concatenation in all sorts of places.
It has the following things I want to change, and add parameters to, to avoid errors and allow me to handle names with single quotes, like Jack O'Connel.
It uses:
DoCmd.RunSQL to execute SQL commands
DAO recordsets
ADODB recordsets
Forms and reports, opened with DoCmd.OpenForm and DoCmd.OpenReport, using string concatenation in the WhereCondition argument
Domain aggregates like DLookUp that use string concatenation
The queries are mostly structured like this:
DoCmd.RunSQL ""INSERT INTO Table1(Field1) SELECT Field1 FROM Table2 WHERE ID = "" &amp; Me.SomeTextbox
What are my options to use parameters for these different kinds of queries?
This question is intended as a resource, for the frequent how do I use parameters comment on various posts
",<sql><vba><ms-access>,982,1,6,31848,13,42,67,68,13793,0.0,513,2,25,2018-03-27 9:47,2018-03-27 9:47,2018-03-27 9:47,0.0,0.0,Basic,3,"<sql><vba><ms-access>, How do I use parameters in VBA in the different contexts in Microsoft Access?, I've read a lot about SQL injection, and using parameters, from sources like bobby-tables.com. However, I'm working with a complex application in Access, that has a lot of dynamic SQL with string concatenation in all sorts of places.
It has the following things I want to change, and add parameters to, to avoid errors and allow me to handle names with single quotes, like Jack O'Connel.
It uses:
DoCmd.RunSQL to execute SQL commands
DAO recordsets
ADODB recordsets
Forms and reports, opened with DoCmd.OpenForm and DoCmd.OpenReport, using string concatenation in the WhereCondition argument
Domain aggregates like DLookUp that use string concatenation
The queries are mostly structured like this:
DoCmd.RunSQL ""INSERT INTO Table1(Field1) SELECT Field1 FROM Table2 WHERE ID = "" &amp; Me.SomeTextbox
What are my options to use parameters for these different kinds of queries?
This question is intended as a resource, for the frequent how do I use parameters comment on various posts
","<sal><va><ms-access>, use parapet va differ context microsoft access?, i'v read lot sal injection, use parameter, source like bobby-tables.com. however, i'm work complex applied access, lot dream sal string concave sort places. follow thing want change, add parapet to, avoid error allow hand name single quotes, like jack o'colonel. uses: domo.runs execute sal command do recorded add recorded form reports, open domo.perform domo.openreport, use string concave wherecondit argument domain agree like lockup use string concave query mostly structure like this: domo.runs ""insert table(field) select field table id = "" &amp; me.sometextbox option use parapet differ kind queried? question intend resource, frequent use parapet comment various post"
64906396,"fetch API always returns {""_U"": 0, ""_V"": 0, ""_W"": null, ""_X"": null}","The below code always return the below wired object
{&quot;_U&quot;: 0, &quot;_V&quot;: 0, &quot;_W&quot;: null, &quot;_X&quot;: null}
as response.
Here is my code
    getData = () =&gt; {
        fetch('http://192.168.64.1:3000/getAll',{
            method: 'GET',
            headers: {
                Accept: 'application/json',
                'Content-Type': 'application/json'
            }
        })
        .then((response) =&gt; {
            console.log('Response:')
            console.log(response.json())
            console.info('=================================')
        })
        .catch(err =&gt; console.error(err));
    } 
    componentDidMount(){
        this.getData();
    }
I am using node, express, Mysql as backend and react-native frontend
my backend code is here
app.get('/getAll',(req,res) =&gt; {
    console.log('getAll method Called');
    con.query('select * from dummy',(err,results,fields) =&gt; {
        if(err) throw err;
        console.log('Response');
        console.log(results);
        res.send(results);
    });
});
The above code gives correct output in console but fetch API is not.
i cant find solution for the my problem. Thanks in advance.
",<mysql><node.js><reactjs><react-native><fetch>,1194,1,29,321,1,3,7,52,28228,0.0,24,4,25,2020-11-19 6:24,2020-11-19 6:42,2020-11-19 6:42,0.0,0.0,Basic,1,"<mysql><node.js><reactjs><react-native><fetch>, fetch API always returns {""_U"": 0, ""_V"": 0, ""_W"": null, ""_X"": null}, The below code always return the below wired object
{&quot;_U&quot;: 0, &quot;_V&quot;: 0, &quot;_W&quot;: null, &quot;_X&quot;: null}
as response.
Here is my code
    getData = () =&gt; {
        fetch('http://192.168.64.1:3000/getAll',{
            method: 'GET',
            headers: {
                Accept: 'application/json',
                'Content-Type': 'application/json'
            }
        })
        .then((response) =&gt; {
            console.log('Response:')
            console.log(response.json())
            console.info('=================================')
        })
        .catch(err =&gt; console.error(err));
    } 
    componentDidMount(){
        this.getData();
    }
I am using node, express, Mysql as backend and react-native frontend
my backend code is here
app.get('/getAll',(req,res) =&gt; {
    console.log('getAll method Called');
    con.query('select * from dummy',(err,results,fields) =&gt; {
        if(err) throw err;
        console.log('Response');
        console.log(results);
        res.send(results);
    });
});
The above code gives correct output in console but fetch API is not.
i cant find solution for the my problem. Thanks in advance.
","<myself><node.is><reacts><react-native><fetch>, fetch apt away return {""u"": 0, ""iv"": 0, ""w"": null, ""x"": null}, code away return wire object {&quit;u&quit;: 0, &quit;iv&quit;: 0, &quit;w&quit;: null, &quit;x&quit;: null} response. code getdata = () =&it; { fetch('http://192.168.64.1:3000/tall',{ method: 'get', leaders: { accept: 'application/son', 'content-type': 'application/son' } }) .then((response) =&it; { console.log('response:') console.log(response.son()) console.into('=================================') }) .catch(err =&it; console.error(err)); } componentdidmount(){ this.getdata(); } use node, express, myself backed react-n fronted backed code pp.get('/tall',(red,yes) =&it; { console.log('metal method called'); con.query('select * dummy',(err,results,fields) =&it; { if(err) throw err; console.log('response'); console.log(results); yes.send(results); }); }); code give correct output console fetch apt not. can find slut problem. thank advance."
52142645,How to improve SQLite insert performance in Python 3.6?,"Background
I would like to insert 1-million records to SQLite using Python. I tried a number of ways to improve it but it is still not so satisfied. The database load file to memory using 0.23 second (search pass below) but SQLite 1.77 second to load and insert to file.
Environment
Intel Core i7-7700 @ 3.6GHz
16GB RAM
Micron 1100 256GB SSD, Windows 10 x64
Python 3.6.5 Minconda
sqlite3.version 2.6.0  
GenerateData.py
I generate the 1 million test input data with the same format as my real data.
import time
start_time = time.time()
with open('input.ssv', 'w') as out:
    symbols = ['AUDUSD','EURUSD','GBPUSD','NZDUSD','USDCAD','USDCHF','USDJPY','USDCNY','USDHKD']
    lines = []
    for i in range(0,1*1000*1000):
        q1, r1, q2, r2 = i//100000, i%100000, (i+1)//100000, (i+1)%100000
        line = '{} {}.{:05d} {}.{:05d}'.format(symbols[i%len(symbols)], q1, r1, q2, r2)
        lines.append(line)
    out.write('\n'.join(lines))
print(time.time()-start_time, i)
input.ssv
The test data looks like this.
AUDUSD 0.00000 0.00001
EURUSD 0.00001 0.00002
GBPUSD 0.00002 0.00003
NZDUSD 0.00003 0.00004
USDCAD 0.00004 0.00005
...
USDCHF 9.99995 9.99996
USDJPY 9.99996 9.99997
USDCNY 9.99997 9.99998
USDHKD 9.99998 9.99999
AUDUSD 9.99999 10.00000
// total 1 million of lines, taken 1.38 second for Python code to generate to disk
Windows correctly shows 23,999,999 bytes file size.
Baseline Code InsertData.py
import time
class Timer:
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed)) 
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        infile.read()
Basic I/O
with open('input.ssv', 'r') as infile:
    infile.read()
  Imported in 0.13 seconds or 7.6 M per second
It tests the read speed.
with open('input.ssv', 'r') as infile:
    with open('output.ssv', 'w') as outfile:
        outfile.write(infile.read()) // insert here
  Imported in 0.26 seconds or 3.84 M per second
It tests the read and write speed without parsing anything
with open('input.ssv', 'r') as infile:
    lines = infile.read().splitlines()
    for line in lines:
        pass # do insert here
  Imported in 0.23 seconds or 4.32 M per second
When I parse the data line by line, it achieves a very high output.
This gives us a sense about how fast the IO and string processing operations on my testing machine.
1.   Write File
outfile.write(line)
  Imported in 0.52 seconds or 1.93 M per second
2.   Split to floats to string
tokens = line.split()
sym, bid, ask = tokens[0], float(tokens[1]), float(tokens[2])
outfile.write('{} {:.5f} {%.5f}\n'.format(sym, bid, ask)) // real insert here
  Imported in 2.25 seconds or 445 K per second
3.   Insert Statement with autocommit
conn = sqlite3.connect('example.db', isolation_level=None)
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  When isolation_level = None (autocommit), program takes many hours to complete (I could not wait for such a long hours)
Note the output database file size is 32,325,632 bytes, which is 32MB. It is bigger than the input file ssv file size of 23MB by 10MB.
4.   Insert Statement with BEGIN (DEFERRED)
conn = sqlite3.connect('example.db', isolation_level=’DEFERRED’) # default
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  Imported in 7.50 seconds or 133,296 per second
This is the same as writing BEGIN, BEGIN TRANSACTION or BEGIN DEFERRED TRANSACTION, not BEGIN IMMEDIATE nor BEGIN EXCLUSIVE.
5.   Insert by Prepared Statement
Using the transaction above gives a satisfactory results but it should be noted that using Python’s string operations is undesired because it is subjected to SQL injection. Moreover using string is slow compared to parameter substitution.
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(sym,bid,ask)])
  Imported in 2.31 seconds or 432,124 per second
6.   Turn off Synchronous
Power failure corrupts the database file when synchronous is not set to EXTRA nor FULL before data reaches the physical disk surface. When we can ensure the power and OS is healthy, we can turn synchronous to OFF so that it doe not synchronized after data handed to OS layer.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
  Imported in 2.25 seconds or 444,247 per second
7.   Turn off journal and so no rollback nor atomic commit
In some applications the rollback function of a database is not required, for example a time series data insertion. When we can ensure the power and OS is healthy, we can turn journal_mode to off so that rollback journal is disabled completely and it disables the atomic commit and rollback capabilities.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
c.execute('''PRAGMA journal_mode = OFF''')
  Imported in 2.22 seconds or 450,653 per second
8.   Using in-memory database
In some applications writing data back to disks is not required, such as applications providing queried data to web applications.
conn = sqlite3.connect("":memory:"")
  Imported in 2.17 seconds or 460,405 per second
9.   Faster Python code in the loop
We should consider to save every bit of computation inside an intensive loop, such as avoiding assignment to variable and string operations.
9a.  Avoid assignment to variable
tokens = line.split()
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(tokens[0], float(tokens[1]), float(tokens[2]))])
  Imported in 2.10 seconds or 475,964 per second
9b.  Avoid string.split()
When we can treat the space separated data as fixed width format, we can directly indicate the distance between each data to the head of data.
It means line.split()[1] becomes line[7:14]
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], float(line[7:14]), float(line[15:]))])
  Imported in 1.94 seconds or 514,661 per second
9c.  Avoid float() to ?
When we are using executemany() with ? placeholder, we don’t need to turn the string into float beforehand.
executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
  Imported in 1.59 seconds or 630,520 per second
10.  The fastest full functioned and robust code so far
import time
class Timer:    
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed))
import sqlite3
conn = sqlite3.connect('example.db')
c = conn.cursor()
c.execute('''DROP TABLE IF EXISTS stocks''')
c.execute('''CREATE TABLE IF NOT EXISTS stocks
             (sym text, bid real, ask real)''')
c.execute('''PRAGMA synchronous = EXTRA''')
c.execute('''PRAGMA journal_mode = WAL''')
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        lines = infile.read().splitlines()
        for line in lines:
            c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
        conn.commit()
        conn.close()
  Imported in 1.77 seconds or 564,611 per second
Possible to get faster?
I have a 23MB file with 1 million records composing of a piece of text as symbol name and 2 floating point number as bid and ask. When you search pass above, the test result shows a 4.32 M inserts per second to plain file. When I insert to a robust SQLite database, it drops to 0.564 M inserts per second. What else you may think of to make it even faster in SQLite? What if not SQLite but other database system?
",<python><performance><sql-insert>,7738,0,104,641,1,8,10,37,12793,0.0,186,3,25,2018-09-03 2:51,2019-10-24 2:05,,416.0,,Intermediate,23,"<python><performance><sql-insert>, How to improve SQLite insert performance in Python 3.6?, Background
I would like to insert 1-million records to SQLite using Python. I tried a number of ways to improve it but it is still not so satisfied. The database load file to memory using 0.23 second (search pass below) but SQLite 1.77 second to load and insert to file.
Environment
Intel Core i7-7700 @ 3.6GHz
16GB RAM
Micron 1100 256GB SSD, Windows 10 x64
Python 3.6.5 Minconda
sqlite3.version 2.6.0  
GenerateData.py
I generate the 1 million test input data with the same format as my real data.
import time
start_time = time.time()
with open('input.ssv', 'w') as out:
    symbols = ['AUDUSD','EURUSD','GBPUSD','NZDUSD','USDCAD','USDCHF','USDJPY','USDCNY','USDHKD']
    lines = []
    for i in range(0,1*1000*1000):
        q1, r1, q2, r2 = i//100000, i%100000, (i+1)//100000, (i+1)%100000
        line = '{} {}.{:05d} {}.{:05d}'.format(symbols[i%len(symbols)], q1, r1, q2, r2)
        lines.append(line)
    out.write('\n'.join(lines))
print(time.time()-start_time, i)
input.ssv
The test data looks like this.
AUDUSD 0.00000 0.00001
EURUSD 0.00001 0.00002
GBPUSD 0.00002 0.00003
NZDUSD 0.00003 0.00004
USDCAD 0.00004 0.00005
...
USDCHF 9.99995 9.99996
USDJPY 9.99996 9.99997
USDCNY 9.99997 9.99998
USDHKD 9.99998 9.99999
AUDUSD 9.99999 10.00000
// total 1 million of lines, taken 1.38 second for Python code to generate to disk
Windows correctly shows 23,999,999 bytes file size.
Baseline Code InsertData.py
import time
class Timer:
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed)) 
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        infile.read()
Basic I/O
with open('input.ssv', 'r') as infile:
    infile.read()
  Imported in 0.13 seconds or 7.6 M per second
It tests the read speed.
with open('input.ssv', 'r') as infile:
    with open('output.ssv', 'w') as outfile:
        outfile.write(infile.read()) // insert here
  Imported in 0.26 seconds or 3.84 M per second
It tests the read and write speed without parsing anything
with open('input.ssv', 'r') as infile:
    lines = infile.read().splitlines()
    for line in lines:
        pass # do insert here
  Imported in 0.23 seconds or 4.32 M per second
When I parse the data line by line, it achieves a very high output.
This gives us a sense about how fast the IO and string processing operations on my testing machine.
1.   Write File
outfile.write(line)
  Imported in 0.52 seconds or 1.93 M per second
2.   Split to floats to string
tokens = line.split()
sym, bid, ask = tokens[0], float(tokens[1]), float(tokens[2])
outfile.write('{} {:.5f} {%.5f}\n'.format(sym, bid, ask)) // real insert here
  Imported in 2.25 seconds or 445 K per second
3.   Insert Statement with autocommit
conn = sqlite3.connect('example.db', isolation_level=None)
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  When isolation_level = None (autocommit), program takes many hours to complete (I could not wait for such a long hours)
Note the output database file size is 32,325,632 bytes, which is 32MB. It is bigger than the input file ssv file size of 23MB by 10MB.
4.   Insert Statement with BEGIN (DEFERRED)
conn = sqlite3.connect('example.db', isolation_level=’DEFERRED’) # default
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  Imported in 7.50 seconds or 133,296 per second
This is the same as writing BEGIN, BEGIN TRANSACTION or BEGIN DEFERRED TRANSACTION, not BEGIN IMMEDIATE nor BEGIN EXCLUSIVE.
5.   Insert by Prepared Statement
Using the transaction above gives a satisfactory results but it should be noted that using Python’s string operations is undesired because it is subjected to SQL injection. Moreover using string is slow compared to parameter substitution.
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(sym,bid,ask)])
  Imported in 2.31 seconds or 432,124 per second
6.   Turn off Synchronous
Power failure corrupts the database file when synchronous is not set to EXTRA nor FULL before data reaches the physical disk surface. When we can ensure the power and OS is healthy, we can turn synchronous to OFF so that it doe not synchronized after data handed to OS layer.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
  Imported in 2.25 seconds or 444,247 per second
7.   Turn off journal and so no rollback nor atomic commit
In some applications the rollback function of a database is not required, for example a time series data insertion. When we can ensure the power and OS is healthy, we can turn journal_mode to off so that rollback journal is disabled completely and it disables the atomic commit and rollback capabilities.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
c.execute('''PRAGMA journal_mode = OFF''')
  Imported in 2.22 seconds or 450,653 per second
8.   Using in-memory database
In some applications writing data back to disks is not required, such as applications providing queried data to web applications.
conn = sqlite3.connect("":memory:"")
  Imported in 2.17 seconds or 460,405 per second
9.   Faster Python code in the loop
We should consider to save every bit of computation inside an intensive loop, such as avoiding assignment to variable and string operations.
9a.  Avoid assignment to variable
tokens = line.split()
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(tokens[0], float(tokens[1]), float(tokens[2]))])
  Imported in 2.10 seconds or 475,964 per second
9b.  Avoid string.split()
When we can treat the space separated data as fixed width format, we can directly indicate the distance between each data to the head of data.
It means line.split()[1] becomes line[7:14]
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], float(line[7:14]), float(line[15:]))])
  Imported in 1.94 seconds or 514,661 per second
9c.  Avoid float() to ?
When we are using executemany() with ? placeholder, we don’t need to turn the string into float beforehand.
executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
  Imported in 1.59 seconds or 630,520 per second
10.  The fastest full functioned and robust code so far
import time
class Timer:    
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed))
import sqlite3
conn = sqlite3.connect('example.db')
c = conn.cursor()
c.execute('''DROP TABLE IF EXISTS stocks''')
c.execute('''CREATE TABLE IF NOT EXISTS stocks
             (sym text, bid real, ask real)''')
c.execute('''PRAGMA synchronous = EXTRA''')
c.execute('''PRAGMA journal_mode = WAL''')
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        lines = infile.read().splitlines()
        for line in lines:
            c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
        conn.commit()
        conn.close()
  Imported in 1.77 seconds or 564,611 per second
Possible to get faster?
I have a 23MB file with 1 million records composing of a piece of text as symbol name and 2 floating point number as bid and ask. When you search pass above, the test result shows a 4.32 M inserts per second to plain file. When I insert to a robust SQLite database, it drops to 0.564 M inserts per second. What else you may think of to make it even faster in SQLite? What if not SQLite but other database system?
","<patron><performance><sal-insert>, improve quite insert perform patron 3.6?, background would like insert 1-million record quite use patron. try number way improve still satisfied. database load file memory use 0.23 second (search pass below) quite 1.77 second load insert file. environs inter core in-7700 @ 3.ugh 16gb ram micro 1100 256gb sad, window 10 x patron 3.6.5 minconda sqlite3.very 2.6.0 generatedata.i genet 1 million test input data format real data. import time start_tim = time.time() open('input.sov', 'w') out: symbol = ['august','eurusd','gbpusd','nidus','usdcad','usdchf','usdjpy','usdcny','usdhkd'] line = [] range(0,1*1000*1000): qu, re, qu, re = i//100000, i%100000, (i+1)//100000, (i+1)%100000 line = '{} {}.{:and} {}.{:and}'.format(symbols[i%len(symbols)], qu, re, qu, re) lines.happened(line) out.write('\n'.join(lines)) print(time.time()-start_time, i) input.sov test data look like this. august 0.00000 0.00001 eurusd 0.00001 0.00002 gbpusd 0.00002 0.00003 nidus 0.00003 0.00004 usdcad 0.00004 0.00005 ... usdchf 9.99995 9.99996 usdjpi 9.99996 9.99997 usdcni 9.99997 9.99998 usdhkd 9.99998 9.99999 august 9.99999 10.00000 // total 1 million lines, taken 1.38 second patron code genet disk window correctly show 23,999,999 bite file size. vaselin code insertdata.i import time class time: def __enter__(self): self.start = time.time() return self def __exit__(self, *arms): leaps = time.time()-self.start print('import {:.of} second {:.of} per second'.format(elapsed, 1*1000*1000/elapsed)) time() t: open('input.sov', 'r') inside: inside.read() basic i/o open('input.sov', 'r') inside: inside.read() import 0.13 second 7.6 per second test read speed. open('input.sov', 'r') inside: open('output.sov', 'w') outside: outside.write(inside.read()) // insert import 0.26 second 3.84 per second test read write speed without part any open('input.sov', 'r') inside: line = inside.read().splitlines() line lines: pass # insert import 0.23 second 4.32 per second part data line line, achieve high output. give us sens fast to string process over test machine. 1. write file outside.write(line) import 0.52 second 1.93 per second 2. split float string token = line.split() sum, bid, ask = tokens[0], float(tokens[1]), float(tokens[2]) outside.write('{} {:.of} {%.of}\n'.format(sum, bid, ask)) // real insert import 2.25 second 445 k per second 3. insert statement autocommit corn = sqlite3.connect('example.do', isolation_level=none) c.execute(""insert stock value ('{}',{:.of},{:.of})"".format(sum,bid,ask)) isolation_level = none (autocommit), program take man hour complete (i could wait long hours) note output database file size 32,325,632 bites, limb. bigger input file sov file size limb limb. 4. insert statement begin (deferred) corn = sqlite3.connect('example.do', isolation_level=’deferred’) # default c.execute(""insert stock value ('{}',{:.of},{:.of})"".format(sum,bid,ask)) import 7.50 second 133,296 per second write begin, begin transact begin defer transaction, begin dimmed begin exclusive. 5. insert prepare statement use transact give satisfactory result note use patron’ string over under subject sal injection. more use string slow compare parapet substitution. c.executemany(""insert stock value (?,?,?)"", [(sum,bid,ask)]) import 2.31 second 432,124 per second 6. turn synchron power failure corrupt database file synchron set extra full data reach physics disk surface. ensue power os healthy, turn synchron doe synchron data hand os layer. corn = sqlite3.connect('example.do', isolation_level='deferred') c = corn.curses() c.execute('''trauma synchron = off''') import 2.25 second 444,247 per second 7. turn journal rollback atom commit applied rollback function database required, example time peri data insertion. ensue power os healthy, turn journal_mod rollback journal distal complete distal atom commit rollback capabilities. corn = sqlite3.connect('example.do', isolation_level='deferred') c = corn.curses() c.execute('''trauma synchron = off''') c.execute('''trauma journal_mod = off''') import 2.22 second 450,653 per second 8. use in-memory database applied write data back disk required, applied proved query data web applications. corn = sqlite3.connect("":memory:"") import 2.17 second 460,405 per second 9. faster patron code loop consider save every bit compute inside intense loop, avoid assign variable string operations. a. avoid assign variable token = line.split() c.executemany(""insert stock value (?,?,?)"", [(tokens[0], float(tokens[1]), float(tokens[2]))]) import 2.10 second 475,964 per second b. avoid string.split() treat space spear data fix width format, directly india distance data head data. mean line.split()[1] become line[7:14] c.executemany(""insert stock value (?,?,?)"", [(line[0:6], float(line[7:14]), float(line[15:]))]) import 1.94 second 514,661 per second c. avoid float() ? use executemany() ? slaveholder, don’t need turn string float beforehand. executemany(""insert stock value (?,?,?)"", [(line[0:6], line[7:14], line[15:])]) import 1.59 second 630,520 per second 10. fattest full function robust code far import time class time: def __enter__(self): self.start = time.time() return self def __exit__(self, *arms): leaps = time.time()-self.start print('import {:.of} second {:.of} per second'.format(elapsed, 1*1000*1000/elapsed)) import sqlite3 corn = sqlite3.connect('example.do') c = corn.curses() c.execute('''drop table exist stocks''') c.execute('''or table exist stock (sum text, bid real, ask real)''') c.execute('''trauma synchron = extra''') c.execute('''trauma journal_mod = was''') time() t: open('input.sov', 'r') inside: line = inside.read().splitlines() line lines: c.executemany(""insert stock value (?,?,?)"", [(line[0:6], line[7:14], line[15:])]) corn.commit() corn.close() import 1.77 second 564,611 per second possible get faster? limb file 1 million record compose piece text symbol name 2 float point number bid ask. search pass above, test result show 4.32 insert per second plain file. insert robust quite database, drop 0.564 insert per second. else may think make even faster quite? quite database system?"
52287553,How to create a copy of a dataframe in pyspark?,"I have a dataframe from which I need to create a new dataframe with a small change in the schema by doing the following operation.
&gt;&gt;&gt; X = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])
&gt;&gt;&gt; schema_new = X.schema.add('id_col', LongType(), False)
&gt;&gt;&gt; _X = X.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)
The problem is that in the above operation, the schema of X gets changed inplace. So when I print X.columns I get 
&gt;&gt;&gt; X.columns
['a', 'b', 'id_col']
but the values in X are still the same
&gt;&gt;&gt; X.show()
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  3|  4|
+---+---+
To avoid changing the schema of X, I tried creating a copy of X using three ways 
- using copy and deepcopy methods from the copy module
- simply using _X = X
The copy methods failed and returned a 
RecursionError: maximum recursion depth exceeded
The assignment method also doesn't work
&gt;&gt;&gt; _X = X
&gt;&gt;&gt; id(_X) == id(X)
True
Since their id are the same, creating a duplicate dataframe doesn't really help here and the operations done on _X reflect in X.
So my question really is two fold
how to change the schema outplace (that is without making any changes to X)?
and more importantly, how to create a duplicate of a pyspark dataframe?
Note:
This question is a followup to this post
",<python><apache-spark><pyspark><apache-spark-sql>,1338,1,30,7707,15,70,110,65,95019,0.0,2100,6,25,2018-09-12 4:35,2018-09-12 6:55,2018-09-12 6:56,0.0,0.0,Basic,9,"<python><apache-spark><pyspark><apache-spark-sql>, How to create a copy of a dataframe in pyspark?, I have a dataframe from which I need to create a new dataframe with a small change in the schema by doing the following operation.
&gt;&gt;&gt; X = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])
&gt;&gt;&gt; schema_new = X.schema.add('id_col', LongType(), False)
&gt;&gt;&gt; _X = X.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)
The problem is that in the above operation, the schema of X gets changed inplace. So when I print X.columns I get 
&gt;&gt;&gt; X.columns
['a', 'b', 'id_col']
but the values in X are still the same
&gt;&gt;&gt; X.show()
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  3|  4|
+---+---+
To avoid changing the schema of X, I tried creating a copy of X using three ways 
- using copy and deepcopy methods from the copy module
- simply using _X = X
The copy methods failed and returned a 
RecursionError: maximum recursion depth exceeded
The assignment method also doesn't work
&gt;&gt;&gt; _X = X
&gt;&gt;&gt; id(_X) == id(X)
True
Since their id are the same, creating a duplicate dataframe doesn't really help here and the operations done on _X reflect in X.
So my question really is two fold
how to change the schema outplace (that is without making any changes to X)?
and more importantly, how to create a duplicate of a pyspark dataframe?
Note:
This question is a followup to this post
","<patron><apache-spark><spark><apache-spark-sal>, great copy datafram spark?, datafram need great new datafram small change scheme follow operation. &it;&it;&it; x = spark.createdataframe([[1,2], [3,4]], ['a', 'b']) &it;&it;&it; schema_new = x.scheme.add('idol', longtype(), false) &it;&it;&it; x = x.red.zipwithindex().map(labia l: list(l[0]) + [l[1]]).of(schema_new) problem operation, scheme x get change place. print x.column get &it;&it;&it; x.column ['a', 'b', 'idol'] value x still &it;&it;&it; x.show() +---+---+ | a| b| +---+---+ | 1| 2| | 3| 4| +---+---+ avoid change scheme x, try great copy x use three way - use copy deepcopi method copy model - simple use x = x copy method fail return recursionerror: maximum recurs depth exceed assign method also work &it;&it;&it; x = x &it;&it;&it; id(x) == id(x) true since id same, great public datafram really help over done x reflect x. question really two fold change scheme outlaw (that without make change x)? importantly, great public spark dataframe? note: question followed post"
51087037,SQL Server json truncated (even when using NVARCHAR(max) ),"DECLARE @result NVARCHAR(max);
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
SELECT @result;
This returns a json string of ~43000 characters, with some results truncated.
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
This returns a json string of ~2000 characters. Is there any way to prevent any truncation? Even when dealing with some bigdata and the string is millions and millions of characters?
",<sql><json><sql-server>,466,0,8,583,1,8,14,66,31442,0.0,3,10,25,2018-06-28 15:51,2018-07-23 22:17,,25.0,,Basic,9,"<sql><json><sql-server>, SQL Server json truncated (even when using NVARCHAR(max) ), DECLARE @result NVARCHAR(max);
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
SELECT @result;
This returns a json string of ~43000 characters, with some results truncated.
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
This returns a json string of ~2000 characters. Is there any way to prevent any truncation? Even when dealing with some bigdata and the string is millions and millions of characters?
","<sal><son><sal-server>, sal server son truncated (even use nvarchar(max) ), declare @result nvarchar(max); set @result = (select * table son auto, root('data')) select @result; return son string ~43000 characters, result truncated. set @result = (select * table son auto, root('data')) return son string ~2000 characters. way prevent truncation? even deal bigdata string million million characters?"
