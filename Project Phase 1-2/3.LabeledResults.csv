QuestionId,QuestionTitle,QuestionBody,QuestionTags,QuestionBodyLength,URLImageCount,LOC,UserReputation,UserGoldBadges,UserSilverBadges,UserBronzeBadges,QuestionAcceptRate,QuestionViewCount,QuestionFavoriteCount,UserUpVoteCount,QuestionAnswersCount,QuestionScore,QuestionCreationDate,FirstAnswerCreationDate,AcceptedAnswerCreationDate,FirstAnswerIntervalDays,AcceptedAnswerIntervalDays,QuestionLabel,QuestionLabelDefinition
50093144,MySQL 8.0 - Client does not support authentication protocol requested by server; consider upgrading MySQL client,"I can't make a simple connection to the server for some reason. I install the newest MySQL Community 8.0 database along with Node.JS with default settings.
This is my node.js code
    var mysql = require('mysql');
    var con = mysql.createConnection({
      host: &quot;localhost&quot;,
      user: &quot;root&quot;,
      password: &quot;password&quot;,
      insecureAuth : true
    });
    con.connect(function(err) {
      if (err) throw err;
      console.log(&quot;Connected!&quot;);
    });
Below is the error found in Command Prompt:
C:\Users\mysql-test&gt;node app.js
    C:\Users\mysql-test\node_modules\mysql\lib\protocol\Parse
    r.js:80
            throw err; // Rethrow non-MySQL errors
            ^
Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at Handshake.Sequence._packetToError (C:\Users\mysql-
test\node_modules\mysql\lib\protocol\sequences\Sequence.js:52:14)
    at Handshake.ErrorPacket (C:\Users\mysql-test\node_mo
dules\mysql\lib\protocol\sequences\Handshake.js:130:18)
    at Protocol._parsePacket (C:\Users\mysql-test\node_mo
dules\mysql\lib\protocol\Protocol.js:279:23)
    at Parser.write (C:\Users\mysql-test\node_modules\mys
ql\lib\protocol\Parser.js:76:12)
    at Protocol.write (C:\Users\mysql-test\node_modules\m
ysql\lib\protocol\Protocol.js:39:16)
    at Socket.&lt;anonymous&gt; (C:\Users\mysql-test\node_modul
es\mysql\lib\Connection.js:103:28)
    at Socket.emit (events.js:159:13)
    at addChunk (_stream_readable.js:265:12)
    at readableAddChunk (_stream_readable.js:252:11)
    at Socket.Readable.push (_stream_readable.js:209:10)
    --------------------
    at Protocol._enqueue (C:\Users\mysql-test\node_module
s\mysql\lib\protocol\Protocol.js:145:48)
    at Protocol.handshake (C:\Users\mysql-test\node_modul
es\mysql\lib\protocol\Protocol.js:52:23)
    at Connection.connect (C:\Users\mysql-test\node_modul
es\mysql\lib\Connection.js:130:18)
    at Object.&lt;anonymous&gt; (C:\Users\mysql-test\server.js:
11:5)
at Module._compile (module.js:660:30)
at Object.Module._extensions..js (module.js:671:10)
at Module.load (module.js:573:32)
at tryModuleLoad (module.js:513:12)
at Function.Module._load (module.js:505:3)
at Function.Module.runMain (module.js:701:10)
I've read up on some things such as:
https://dev.mysql.com/doc/refman/5.5/en/old-client.html
https://github.com/mysqljs/mysql/issues/1507
But I am still not sure how to fix my problem.
",<mysql><node.js>,2492,4,51,7655,3,10,14,64,1015467,0,7,35,755,2018-04-30 2:04,2018-05-02 9:58,2018-05-02 9:58,2,2,Advanced,32
49194719,Authentication plugin 'caching_sha2_password' cannot be loaded,"I am connecting MySQL - 8.0 with MySQL Workbench and getting the below error:
  Authentication plugin 'caching_sha2_password' cannot be loaded:
  dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image
  not found
I have tried with other client tool as well.
Any solution for this?
",<mysql><mysql-workbench><mysql-8.0>,297,0,0,17755,9,53,82,61,1043189,0,715,38,671,2018-03-09 13:19,2018-03-12 5:08,2018-03-12 5:08,3,3,Advanced,32
50379839,Connection Java - MySQL : Public Key Retrieval is not allowed,"I try to connect MySQL database with Java using connector 8.0.11.   Everything seems to be OK, but I get this exception:
Exception in thread &quot;main&quot; java.sql.SQLNonTransientConnectionException: Public Key Retrieval is not allowed at
     com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:108) at 
     com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:95) at
     com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122) at     
     com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:862) at 
     com.mysql.cj.jdbc.ConnectionImpl.(ConnectionImpl.java:444) at
     com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:230) at
     com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:226) at
     com.mysql.cj.jdbc.MysqlDataSource.getConnection(MysqlDataSource.java:438) at
     com.mysql.cj.jdbc.MysqlDataSource.getConnection(MysqlDataSource.java:146) at
     com.mysql.cj.jdbc.MysqlDataSource.getConnection(MysqlDataSource.java:119) at
     ConnectionManager.getConnection(ConnectionManager.java:28) at
     Main.main(Main.java:8)
Here is my Connection Manager class:
public class ConnectionManager {
    public static final String serverTimeZone = &quot;UTC&quot;;
    public static final String serverName = &quot;localhost&quot;;
    public static final String databaseName =&quot;biblioteka&quot;;
    public static final int portNumber = 3306;
    public static final String user = &quot;anyroot&quot;;
    public static final String password = &quot;anyroot&quot;;
    public static Connection getConnection() throws SQLException {
        MysqlDataSource dataSource = new MysqlDataSource();
        dataSource.setUseSSL( false );
        dataSource.setServerTimezone( serverTimeZone );
        dataSource.setServerName( serverName );
        dataSource.setDatabaseName( databaseName );
        dataSource.setPortNumber( portNumber );
        dataSource.setUser( user );
        dataSource.setPassword( password );
        return dataSource.getConnection();
    }
}
",<java><mysql><exception><connector>,2115,0,37,4667,3,9,10,73,670288,0,17,25,450,2018-05-16 21:02,2018-05-20 19:57,2018-05-20 19:57,4,4,Intermediate,31
65456814,"Docker (Apple Silicon/M1 Preview) MySQL ""no matching manifest for linux/arm64/v8 in the manifest list entries""","I'm running the latest build of the Docker Apple Silicon Preview. I created the tutorial container/images and it works fine. When I went to create a custom YAML file and run docker-compose I get the following error when pulling mysql:
ERROR: no matching manifest for linux/arm64/v8 in the manifest list entries
Here is a snippet from my YAMl file:
version: '3'
services:
  # Database
  db:
    image: mysql-server:5.7
    volumes:
      - db_data:/var/lib/mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: pass
      MYSQL_DATABASE: wp
      MYSQL_USER: wp
      MYSQL_PASSWORD: wp
    networks:
      - wpsite 
I've tried :latest and :8 which result in the same error. It pulls phpmyadmin and wordpress fine.
",<mysql><docker><apple-silicon>,730,1,16,5470,4,31,52,54,405092,0,34,24,445,2020-12-26 13:20,2021-01-06 9:04,2021-01-06 9:04,11,11,Advanced,37
50177216,How to grant all privileges to root user in MySQL 8.0,"Tried
mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;
Getting
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that
  corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'root' WITH GRANT OPTION' at line 1.
Note: The same is working when tried in previous versions.
Also tried
mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;
Getting
  ERROR 1410 (42000): You are not allowed to create a user with GRANT
MySQL (8.0.11.0) username/password is root/root.
",<mysql><mysql-error-1064><mysql-8.0>,584,0,2,2820,3,8,8,53,720528,0,7,24,282,2018-05-04 14:23,2018-05-06 8:17,,2,,Basic,9
50690076,phpMyAdmin - Error > Incorrect format parameter?,"I have a WordPress production website.
I've exported the database by the following commands: select database &gt; export &gt; custom &gt; select all tables &gt; select .zip compression &gt; 'Go'
I've downloaded the file which is example.sql.zip but when I upload to my localhost I get this error: phpMyAdmin - Error &gt; Incorrect format parameter
I've tried to export with other formats and I get the same error.
I've tried with other SQL Databases and it exports/ imports just fine.
What could it be? A corrupt database or other?
Thanks
",<php><mysql><database><wordpress><phpmyadmin>,539,0,2,5235,7,21,35,63,474164,0,9,22,272,2018-06-04 23:15,2018-06-07 16:58,2018-06-07 16:58,3,3,Basic,14
52364415,PHP with MySQL 8.0+ error: The server requested authentication method unknown to the client,"I'm running MySQL version 8 on PHP 7.0.
I'm getting the following error when I try to connect to my database from PHP:
  Connect Error: SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client
PHP might show this error
  Warning: mysqli_connect(): The server requested authentication method unknown to the client [caching_sha2_password] in D:\xampp\htdocs\reg\server.php on line 10
How can I fix this problem?
",<php><mysql><mysql-8.0>,441,0,0,2311,2,12,16,67,420053,0,1,8,228,2018-09-17 9:17,2018-09-17 9:18,,0,,Basic,3
55038942,"FATAL: password authentication failed for user ""postgres"" (postgresql 11 with pgAdmin 4)","I recently installed Postgresql 11, during the installation, there's no step to put password and username for Postgres. Now in pgAdmin 4, I wanted to connect the database to server and it's asking me to input password, and I haven't put any in the first place.
Any one knows what's going on?
",<postgresql><pgadmin-4><postgresql-11>,292,0,0,1849,2,8,9,71,589784,0,0,18,183,2019-03-07 8:12,2019-03-07 8:41,2019-03-07 8:41,0,0,Basic,13
50557234,Authentication plugin 'caching_sha2_password' is not supported,"I am trying to connect to a MySQL server with python connector. I created a new user lcherukuri with the authentication plugin mysql_native_password.
But I got the error
  mysql.connector.errors.NotSupportedError: Authentication plugin 'caching_sha2_password' is not supported
Can someone help me? 
import mysql.connector
cnx = mysql.connector.connect(user='lcherukuri', password='password',
                              host='127.0.0.1',
                              database='test')
cnx.close()
",<python><mysql>,499,1,8,4649,13,43,75,42,307207,0,215,29,172,2018-05-27 22:53,2018-05-27 23:05,2018-05-27 23:05,0,0,Advanced,44
50026939,php mysqli_connect: authentication method unknown to the client [caching_sha2_password],"I am using php mysqli_connect for login to a MySQL database (all on localhost)
&lt;?php
//DEFINE ('DB_USER', 'user2');
//DEFINE ('DB_PASSWORD', 'pass2');
DEFINE ('DB_USER', 'user1');
DEFINE ('DB_PASSWORD', 'pass1');
DEFINE ('DB_HOST', '127.0.0.1');
DEFINE ('DB_NAME', 'dbname');
$dbc = mysqli_connect(DB_HOST, DB_USER, DB_PASSWORD, DB_NAME);
if(!$dbc){
    die('error connecting to database');    
}
?&gt;
this is the mysql.user table:
MySQL Server ini File:
[mysqld]
# The default authentication plugin to be used when connecting to the server
default_authentication_plugin=caching_sha2_password
#default_authentication_plugin=mysql_native_password
with caching_sha2_password in the MySQL Server ini file, it's not possible at all to login with user1 or user2;
  error: mysqli_connect(): The server requested authentication method unknown to the client [caching_sha2_password] in...
with mysql_native_password in the MySQL Server ini file, it's possible to login with user1, but with user2, same error;
how can I login using caching_sha2_password on the mySql Server?
",<php><mysql><hash><sha>,1069,1,22,2568,2,16,20,51,310409,0,91,18,162,2018-04-25 16:15,2018-04-25 16:54,2018-04-25 16:54,0,0,Advanced,44
61523447,Skipping acquire of configured file 'main/binary-i386/Packages',"Good afternoon, please tell me what I'm doing wrong. I just installed the Linux Ubuntu on my computer and still don’t understand anything about it. I tried to install PostreSQL and pgAdmin. I installed on this video tutorial https://www.youtube.com/watch?v=Vdzb7JTPnGk I get this error.
Text of Error: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'http://apt.postgresql.org/pub/repos/apt focal-pgdg InRelease' doesn't support architecture 'i386'
Tell me please how to fix it.
My version of ubuntu: Ubuntu 20.04 LTS
",<linux><postgresql><ubuntu><installation>,548,3,3,1601,2,6,3,50,108093,0,0,4,160,2020-04-30 12:31,2020-05-01 20:08,,1,,Basic,14
55300370,PostgreSQL: serial vs identity,"To have an integer auto-numbering primary key on a table, you can use SERIAL
But I noticed the table information_schema.columns has a number of identity_ fields, and indeed, you could create a column with a GENERATED specifier...
What's the difference? Were they introduced with different PostgreSQL versions? Is one preferred over the other?
",<postgresql>,343,2,4,36202,11,45,69,71,79782,0,1709,1,150,2019-03-22 13:10,2019-03-22 13:31,2019-03-22 13:31,0,0,Intermediate,19
57879150,How can I solve error gypgyp ERR!ERR! find VSfind VS msvs_version not set from command line or npm config?,"I want to run this project : https://github.com/adonis-china/adonis-adminify
When I run npm install, there exist error : 
&gt; sqlite3@3.1.13 install C:\laragon\www\adonis-admin\node_modules\sqlite3
&gt; node-pre-gyp install --fallback-to-build
node-pre-gyp ERR! Tried to download(403): https://mapbox-node-binary.s3.amazonaws.com/sqlite3/v3.1.13/node-v64-win32-x64.tar.gz
node-pre-gyp ERR! Pre-built binaries not found for sqlite3@3.1.13 and node@10.15.0 (node-v64 ABI) (falling back to source compile with node-gyp)
node-pre-gyp ERR! Tried to download(undefined): https://mapbox-node-binary.s3.amazonaws.com/sqlite3/v3.1.13/node-v64-win32-x64.tar.gz
node-pre-gyp ERR! Pre-built binaries not found for sqlite3@3.1.13 and node@10.15.0 (node-v64 ABI) (falling back to source compile with node-gyp)
gyp ERR! gypfind VS
 gyp ERR!ERR!  find VSfind VS
 msvs_version not set from command line or npm config
gypgyp  ERR!ERR!  find VSfind VS msvs_version not set from command line or npm config
 VCINSTALLDIR not set, not running in VS Command Prompt
gyp gypERR! ERR!  find VSfind VS VCINSTALLDIR not set, not running in VS Command Prompt
gyp checking VS2019 (16.2.29230.47) found at:
 gypERR!  find VSERR! checking VS2019 (16.2.29230.47) found at:
gyp  find VS ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional""
ERR!gyp find VS ERR! ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional""
 gypfind VS ERR! - ""Visual Studio C++ core features"" missing
gyp  ERR!find VS  - ""Visual Studio C++ core features"" missing
find VSgyp could not find a version of Visual Studio 2017 or newer to use
 gypERR!  ERR!find VS  could not find a version of Visual Studio 2017 or newer to use
find VS looking for Visual Studio 2015
gyp gyp ERR!ERR!  find VSfind VS looking for Visual Studio 2015
 - not found
gyp gyp ERR!ERR!  find VSfind VS - not found
 not looking for VS2013 as it is only supported up to Node.js 8
gyp ERR!gyp  ERR!find VS find VS not looking for VS2013 as it is only supported up to Node.js 8
gyp gypERR!  ERR!find VS
 gypfind VS  **************************************************************
gypERR!  ERR!find VS find VS **************************************************************
 You need to install the latest version of Visual Studio
gypgyp ERR!  ERR!find VS find VS You need to install the latest version of Visual Studio
 including the ""Desktop development with C++"" workload.
gypgyp  ERR!ERR! find VS find VS including the ""Desktop development with C++"" workload.
 For more information consult the documentation at:
gyp ERR!gyp find VS  For more information consult the documentation at:
ERR!gyp  ERR! find VSfind VS https://github.com/nodejs/node-gyp#on-windows
 https://github.com/nodejs/node-gyp#on-windows
gyp gyp ERR!ERR! find VS  **************************************************************
find VSgyp **************************************************************
 gypERR! find VS
ERR! find VS
gyp gypERR!  ERR!configure error
 configure errorgyp
 ERR! stackgyp Error: Could not find any Visual Studio installation to use
 gypERR!  stackERR! Error: Could not find any Visual Studio installation to use
 stackgyp      at VisualStudioFinder.fail (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)
ERR!gyp stack      at VisualStudioFinder.fail (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:121:47)
ERR! gypstack      at findVisualStudio2013 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16)
gypERR!  ERR!stack      at findVisualStudio2013 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:74:16)
stack     at VisualStudioFinder.findVisualStudio2013 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:344:14)
gypgyp ERR! stack     at findVisualStudio2015 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14)
 gypERR!  ERR! stackstack     at regSearchKeys (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:365:16)
     at VisualStudioFinder.findVisualStudio2013 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:344:14)
gyp gypERR!  ERR!stack stack     at regGetValue (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7)
     at findVisualStudio2015 (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:70:14)
gypgyp  ERR!ERR!  stackstack     at C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16
gyp     at regSearchKeys (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\find-visualstudio.js:365:16)
 gyp ERR! ERR!stack     at ChildProcess.exithandler (child_process.js:301:5)
 gypstack      at regGetValue (C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\util.js:54:7)
ERR! gypstack     at ChildProcess.emit (events.js:182:13)
 gypERR! ERR!  stack     at C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\util.js:33:16
stack     at maybeClose (internal/child_process.js:962:16)
gyp gypERR! ERR!  System Windows_NT 10.0.17134
stackgyp     at ChildProcess.exithandler (child_process.js:301:5)
 ERR!gyp  commandERR! ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\Chelsea\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\laragon\\www\\adonis-admin\\node_modules\\sqlite3\\lib\\binding\\node-v64-win32-x64\\node_sqlite3.node"" ""--module_name=node_sqlite3"" ""--module_path=C:\\laragon\\www\\adonis-admin\\node_modules\\sqlite3\\lib\\binding\\node-v64-win32-x64"" ""--python=C:\\Users\\Chelsea\\.windows-build-tools\\python27\\python.exe""
 gypstack     at ChildProcess.emit (events.js:182:13)
 gypERR!  ERR!cwd C:\laragon\www\adonis-admin\node_modules\sqlite3
 gypstack ERR!     at maybeClose (internal/child_process.js:962:16)
 node -v v10.15.0
gypgyp  ERR!ERR!  Systemnode-gyp -v Windows_NT 10.0.17134
 v5.0.3
gypgyp  ERR!ERR! command  ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\Chelsea\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=C:\\laragon\\www\\adonis-admin\\node_modules\\sqlite3\\lib\\binding\\node-v64-win32-x64\\node_sqlite3.node"" ""--module_name=node_sqlite3"" ""--module_path=C:\\laragon\\www\\adonis-admin\\node_modules\\sqlite3\\lib\\binding\\node-v64-win32-x64"" ""--python=C:\\Users\\Chelsea\\.windows-build-tools\\python27\\python.exe""
not okgyp
 ERR! cwd C:\laragon\www\adonis-admin\node_modules\sqlite3
gyp ERR! node -v v10.15.0
gyp ERR! node-gyp -v v5.0.3
gyp ERR! not ok
node-pre-gyp ERR! build error
node-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=C:\laragon\www\adonis-admin\node_modules\sqlite3\lib\binding\node-v64-win32-x64\node_sqlite3.node --module_name=node_sqlite3 --module_path=C:\laragon\www\adonis-admin\node_modules\sqlite3\lib\binding\node-v64-win32-x64 --python=C:\Users\Chelsea\.windows-build-tools\python27\python.exe' (1)
node-pre-gyp ERR! stack     at ChildProcess.&lt;anonymous&gt; (C:\laragon\www\adonis-admin\node_modules\sqlite3\node_modules\node-pre-gyp\lib\util\compile.js:83:29)
node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:182:13)
node-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:962:16)
node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:251:5)
node-pre-gyp ERR! System Windows_NT 10.0.17134
node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\laragon\\www\\adonis-admin\\node_modules\\sqlite3\\node_modules\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""
node-pre-gyp ERR! cwd C:\laragon\www\adonis-admin\node_modules\sqlite3
node-pre-gyp ERR! node -v v10.15.0
node-pre-gyp ERR! node-pre-gyp -v v0.6.38
node-pre-gyp ERR! not ok
Failed to execute 'C:\Program Files\nodejs\node.exe C:\Users\Chelsea\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=C:\laragon\www\adonis-admin\node_modules\sqlite3\lib\binding\node-v64-win32-x64\node_sqlite3.node --module_name=node_sqlite3 --module_path=C:\laragon\www\adonis-admin\node_modules\sqlite3\lib\binding\node-v64-win32-x64 --python=C:\Users\Chelsea\.windows-build-tools\python27\python.exe' (1)
npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules\fsevents):
npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""win32"",""arch"":""x64""})
npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! sqlite3@3.1.13 install: `node-pre-gyp install --fallback-to-build`
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the sqlite3@3.1.13 install script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\Chelsea\AppData\Roaming\npm-cache\_logs\2019-09-10T22_53_41_072Z-debug.log
How can I solve the error?
",<node.js><sqlite><vue.js><npm><adonis.js>,9526,4,106,12524,73,249,449,65,296271,0,463,28,144,2019-09-10 22:59,2020-01-23 16:12,,135,,Intermediate,30
48218065,Objects created in a thread can only be used in that same thread,"I can't find the problem:
@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm(request.form)
    if request.method=='POST' and form.validate():
        name =  form.name.data 
        email = form.email.data
        username = form.username.data
        password = sha256_crypt.encrypt(str(form.password.data))
        c.execute(&quot;INSERT INTO users(name,email,username,password) 
        VALUES(?,?,?,?)&quot;, (name, email, username, password))
        conn.commit
        conn.close()
Error:
File &quot;C:\Users\app.py&quot;, line 59, in register c.execute(&quot;INSERT INTO
users(name,email,username,password) VALUES(?,?,?,?)&quot;, (name, email,
username, password))  ProgrammingError: SQLite objects created in a
thread can only be used in that   same thread.The object was created
in thread id 23508 and this is thread id   22640
Does this mean I can't use the name, email username &amp; password in an HTML file? How do I solve this?
",<python><mysql><sqlite><flask>,981,0,15,1293,2,7,5,52,143516,0,0,11,128,2018-01-12 0:52,2018-01-12 1:17,2018-01-12 1:17,0,0,Basic,6
58461178,"How to fix ""ERROR: column c.relhasoids does not exist"" in Postgres?","I’m trying to CREATE TABLE command in Postgresql. 
After creating a table, if I punch in TABLE table name, it works.
But I punch in \d table name, I keep getting an error below.
ERROR:  column c.relhasoids does not exist
LINE 1: ...riggers, c.relrowsecurity, c.relforcerowsecurity, c.relhasoi...
I attempted DROP DATABASE table name recreated a database and recreated a table again several times. But it didn't work.
Any suggestions would be appreciated! Thank you.
",<postgresql>,466,0,2,1393,2,7,8,71,116287,0,5,15,126,2019-10-19 6:39,2019-10-19 7:19,,0,,Basic,9
57665645,Server returns invalid timezone. Go to Advanced tab and set servertimezone property manually,"My Intelij IDE wont connect to my database server of MySQL Workbench, it specifies it as a timezone problem and wants me to go to advanced tab and set serverTimezone property manually.
I tried looking if there where a similar problem but i cant seem to find it.
",<mysql><sql><database><intellij-idea><mysql-workbench>,262,0,0,1240,2,6,7,81,88473,0,54,5,113,2019-08-26 22:29,2019-08-27 2:27,2019-08-27 2:27,1,1,Basic,14
54513450,A strange operation problem in SQL Server: -100/-100*10 = 0,"
If you execute SELECT -100/-100*10 the result is 0.
If you execute SELECT (-100/-100)*10 the result is 10.
If you execute SELECT -100/(-100*10) the result is 0.
If you execute SELECT 100/100*10 the result is 10.
BOL states:
  When two operators in an expression have the same operator precedence level, they are evaluated left to right based on their position in the expression.
And
Level   Operators
  1     ~ (Bitwise NOT)
  2     * (Multiplication), / (Division), % (Modulus)
  3     + (Positive), - (Negative), + (Addition), + (Concatenation), - (Subtraction), &amp; (Bitwise AND), ^ (Bitwise Exclusive OR), | (Bitwise OR)
Is BOL wrong, or am I missing something? It seems the - is throwing the (expected) precedence off.
",<sql><sql-server><t-sql><operator-precedence>,727,1,13,839,1,6,7,61,8047,0,1,3,105,2019-02-04 9:43,2019-02-04 11:02,,0,,Basic,2
60864367,"#1030 - Got error 176 ""Read page with wrong checksum"" from storage engine Aria","Created a new database but can't create new user account due to this error.
Does anyone know how to fix this? I can't find any solution to fix this.
1030 - Got error 176 &quot;Read page with wrong checksum&quot; from storage engine Aria
",<mysql><phpmyadmin><mariadb>,237,1,0,1003,2,8,7,44,156659,0,4,6,100,2020-03-26 9:36,2020-04-30 8:52,2020-05-17 8:56,35,52,Advanced,37
52032739,Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver',"This is the Warning im getting in console, Im confused with this warning:
Loading class `com.mysql.jdbc.Driver'. 
This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'.
The driver is automatically registered via the SPI and manual loading 
of the driver class is generally unnecessary.
",<mysql><jdbc>,303,0,4,1021,2,7,7,70,278424,0,0,26,100,2018-08-27 4:52,2018-09-19 12:23,,23,,Advanced,38
59993844,ERROR: Loading local data is disabled - this must be enabled on both the client and server sides,"I don't understand the responses that others have provided to similar questions except for the most obvious ones, such as the one below:
mysql&gt; SET GLOBAL local_infile=1;
Query OK, 0 rows affected (0.00 sec)
mysql&gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| local_infile  | ON    |
+---------------+-------+
1 row in set (0.01 sec)
By this I mean the exact code was provided. I'd greatly appreciate if someone could walk me through, step by step, what I need to do to enable local data on the ""client"" side and ""server"" side. It seems like I've enabled local data on the client side, but I don't know what instructions I need to give my computer to enable the ""server side"". I'm not tech savvy at all, and I just want to be able to get to the point where the data has been uploaded into MySQL workbench.
ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides
CREATE TABLE toys (
uniq_id VARCHAR(1000),
product_name VARCHAR(1000),
manufacturer VARCHAR(1000),
price VARCHAR(1000),
number_available_in_stock VARCHAR (1000),
number_of_reviews INT,
number_of_answered_questions INT,
average_review_rating VARCHAR(1000),
amazon_category_and_sub_category VARCHAR(1000),
customers_who_bought_this_item_also_bought VARCHAR(1000),
description VARCHAR(1000),
product_information VARCHAR(1000),
product_description VARCHAR(1000),
items_customers_buy_after_viewing_this_item VARCHAR(1000),
customer_questions_and_answers VARCHAR(1000),
customer_reviews VARCHAR(1000),
sellers VARCHAR(1000)
);
LOAD DATA LOCAL INFILE ‘/Users/BruddaDave/Desktop/amazonsample.csv’ INTO TABLE toys
FIELDS TERMINATED BY ‘,’
LINES TERMINATED BY ‘\n’
IGNORE 1 LINES
(uniq_id, product_name, manufacturer, price, number_available_in_stock, number_of_reviews, number_of_answered_questions, average_review_rating, amazon_category_and_sub_category, customers_who_bought_this_item_also_bought, description, product_information, product_description, items_customers_buy_after_viewing_this_item, customer_questions_and_answers, customer_reviews, sellers)
;
I just want to be able to import a .csv file into MySQL using the command line shell.
",<mysql><client><local>,2238,0,38,1121,1,6,4,38,257829,0,0,15,99,2020-01-30 20:15,2020-02-02 15:45,,3,,Basic,3
51335298,Concepts of backref and back_populate in SQLalchemy?,"Can anyone explain the concepts of these two ideas and how they relate to making relationships between tables? I can't really seem to find anything that explains it clearly and the documentation feels like there's too much jargon to understand in easy concepts. For instance, in this example of a one to many relationship in the documentation:
class Parent(Base):
    __tablename__ = 'parent'
    id = Column(Integer, primary_key=True)
    children = relationship(""Child"", back_populates=""parent"")
class Child(Base):
    __tablename__ = 'child'
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey('parent.id'))
    parent = relationship(""Parent"", back_populates=""children"")
Why does the relationship() go inside the parent class while ForeignKey goes inside the child class? And what does having back_populates exactly do to one another? Does having the placement of which class the relationship() function exist in matter?
",<python><database><sqlalchemy>,958,0,14,1009,2,10,11,68,34417,0,0,2,97,2018-07-14 4:44,2020-01-26 17:15,,561,,Intermediate,27
52423595,mysqldump: Couldn't execute. Unknown table 'column_statistics' in information_schema,"I want to dump my database, even after following correct syntax it thows me following error.  
Syntax I used : 
mysqldump -uroot -p omnichannel_store_india &gt; omnichannel_store_india.sql
Throws errors :
  mysqldump: Couldn't execute
   'SELECT COLUMN_NAME, JSON_EXTRACT(HISTOGRAM, '$.""number-of-buckets-specified""') FROM information_schema.COLUMN_STATISTICS WHERE SCHEMA_NAME = 'omnichannel_store_india' AND TABLE_NAME = 'consignment_items';': 
Unknown table 'column_statistics' in information_schema (1109)
",<mysql>,510,0,1,14737,6,67,65,68,61214,0,64,3,93,2018-09-20 11:00,2018-09-20 11:00,2018-09-20 11:00,0,0,Advanced,32
50691977,How to reset the root password in MySQL 8.0.11?,"I have actually lost my root password and I need to change it.  I follow these steps :
Step # 1: Stop the MySQL server process.
Step # 2: Start the MySQL (mysqld) server/daemon process with the
--skip-grant-tables option so that it will not prompt for a password.
Step # 3: Connect to the MySQL server as the root user.
that we can found on these website : https://www.howtoforge.com/setting-changing-resetting-mysql-root-passwords#recover-mysql-root-password
mysql&gt; use mysql;
mysql&gt; SET PASSWORD FOR 'root'@'localhost' = PASSWORD(""TOOR"");
mysql&gt; flush privileges;
mysql&gt; quit
First error, so I tried :
mysql&gt; use mysql;
mysql&gt; update user set password=PASSWORD(""TOOR"") where User='root';
mysql&gt; flush privileges;
mysql&gt; quit
Always the same error said : 
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that
corresponds to your MySQL server version for the right syntax to use near '(""TOO
R"") WHERE User='root'' at line 1
How can I resolve this?
",<mysql><passwords><root>,999,2,11,1479,1,12,16,68,155526,0,42,8,93,2018-06-05 4:20,2018-07-05 8:29,2018-09-30 16:29,30,117,Basic,9
48406228,Room - Select query with IN condition?,"Is it possible to use SQLite's IN condition with Room?
I'm trying to select a list of items from my database where the value of a certain column (in this case a TEXT column) matches any one of a set of filter values. That's pretty easily done in SQL and SQLite, by my knowledge, just by adding an IN condition to your SELECT statement (see here). However, I can't seem to make it work with Room.
I keep getting this error:
Error:(70, 25) error: no viable alternative at input 'SELECT * FROM Table WHERE column IN :filterValues'
(where the input to the DAO @Query-annotated method is called filterValues)
I have tried three different methods now:
Passing the argument as a List&lt;String&gt;
Passing the argument as a String[]
And lastly passing the argument as simply a String, but formatted as (value_1, value_2, ..., value_n)
The last one in particular should work easily, as it will (or at least, it should) directly translate to SELECT * FROM Table WHERE column IN (value_1, value_2, ..., value_n), which is the exact way you would manually write out the SELECT if you were just accessing the database directly.
",<android><sqlite><select><android-room><sql-in>,1116,1,13,3025,2,15,22,76,48798,0,29,3,89,2018-01-23 16:14,2018-01-23 16:14,2018-01-23 16:14,0,0,Basic,9
54340470,TRIM is not a recognized built-in function name,"For the following code:
DECLARE @ss varchar(60)
  SET @ss = 'admin'
  select TRIM(@ss)
I've got an error: 
  'TRIM' is not a recognized built-in function name
",<sql><sql-server><t-sql>,159,0,4,923,1,7,10,57,118870,0,3,3,89,2019-01-24 6:21,2019-01-24 6:26,2019-01-24 6:26,0,0,Basic,1
49948350,phpMyAdmin on MySQL 8.0,"UPDATE
Newer versions of phpMyAdmin solved this issue. I've successfully tested with phpMyAdmin 5.0.1
I have installed the MySQL 8.0 server and phpMyAdmin, but when I try to access it from the browser the following errors occur:
#2054 - The server requested authentication method unknown to the client
mysqli_real_connect(): The server requested authentication method unknown to the client [caching_sha2_password]
mysqli_real_connect(): (HY000/2054): The server requested authentication method unknown to the client
I imagine it must have something to do with the strong passwords implemented and the relative freshness of the MySQL release.
But I  know nothing of the most advanced driver and connection configuration.
Has someone faced the same problem and solved it? :D
",<mysql><phpmyadmin><database-connection><mysql-8.0>,773,0,3,1690,1,14,29,69,195495,0,943,18,84,2018-04-20 19:15,2018-04-20 21:46,2018-05-20 16:59,0,30,Basic,14
51442639,Why is 199.96 - 0 = 200 in SQL?,"I have some clients getting weird bills. I was able to isolate the core problem:
SELECT 199.96 - (0.0 * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * CAST(199.96 AS DECIMAL(19, 4)))) -- 200 what the?
SELECT 199.96 - (0.0 * FLOOR(1.0 * CAST(199.96 AS DECIMAL(19, 4)))) -- 199.96
SELECT 199.96 - (0.0 * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * 199.96)) -- 199.96
SELECT 199.96 - (CAST(0.0 AS DECIMAL(19, 4)) * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * CAST(199.96 AS DECIMAL(19, 4)))) -- 199.96
SELECT 199.96 - (CAST(0.0 AS DECIMAL(19, 4)) * FLOOR(1.0 * CAST(199.96 AS DECIMAL(19, 4))))                         -- 199.96
SELECT 199.96 - (CAST(0.0 AS DECIMAL(19, 4)) * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * 199.96))                         -- 199.96
-- It gets weirder...
SELECT (0 * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * CAST(199.96 AS DECIMAL(19, 4)))) -- 0
SELECT (0 * FLOOR(1.0 * CAST(199.96 AS DECIMAL(19, 4))))                         -- 0
SELECT (0 * FLOOR(CAST(1.0 AS DECIMAL(19, 4)) * 199.96))                         -- 0
-- so... ... 199.06 - 0 equals 200... ... right???
SELECT 199.96 - 0 -- 199.96 ...NO....
Has anyone a clue, what the heck is happening here? I mean, it has certainly something to do with the decimal datatype, but I can't really wrap my head around it...
There was a lot of confusion about of what datatype the number literals were, so I decided to show the real line:
PS.SharePrice - (CAST((@InstallmentCount - 1) AS DECIMAL(19, 4)) * CAST(FLOOR(@InstallmentPercent * PS.SharePrice) AS DECIMAL(19, 4))))
PS.SharePrice DECIMAL(19, 4)
@InstallmentCount INT
@InstallmentPercent DECIMAL(19, 4)
I made sure that the result of each operation having an operand of a type different than DECIMAL(19, 4) is cast explicitly before applying it to the outer context.
Nevertheless, the result remains 200.00.
I have now created a boiled down sample you guys can execute on your computer.
DECLARE @InstallmentIndex INT = 1
DECLARE @InstallmentCount INT = 1
DECLARE @InstallmentPercent DECIMAL(19, 4) = 1.0
DECLARE @PS TABLE (SharePrice DECIMAL(19, 4))
INSERT INTO @PS (SharePrice) VALUES (599.96)
-- 2000
SELECT
  IIF(@InstallmentIndex &lt; @InstallmentCount,
  FLOOR(@InstallmentPercent * PS.SharePrice),
  1999.96)
FROM @PS PS
-- 2000
SELECT
  IIF(@InstallmentIndex &lt; @InstallmentCount,
  FLOOR(@InstallmentPercent * CAST(599.96 AS DECIMAL(19, 4))),
  1999.96)
FROM @PS PS
-- 1996.96
SELECT
  IIF(@InstallmentIndex &lt; @InstallmentCount,
  FLOOR(@InstallmentPercent * 599.96),
  1999.96)
FROM @PS PS
-- Funny enough - with this sample explicitly converting EVERYTHING to DECIMAL(19, 4) - it still doesn't work...
-- 2000
SELECT
  IIF(@InstallmentIndex &lt; @InstallmentCount,
  FLOOR(@InstallmentPercent * CAST(199.96 AS DECIMAL(19, 4))),
  CAST(1999.96 AS DECIMAL(19, 4)))
FROM @PS PS
Now I've got something...
-- 2000
SELECT
  IIF(1 = 2,
  FLOOR(CAST(1.0 AS decimal(19, 4)) * CAST(199.96 AS DECIMAL(19, 4))),
  CAST(1999.96 AS DECIMAL(19, 4)))
-- 1999.9600
SELECT
  IIF(1 = 2,
  CAST(FLOOR(CAST(1.0 AS decimal(19, 4)) * CAST(199.96 AS DECIMAL(19, 4))) AS INT),
  CAST(1999.96 AS DECIMAL(19, 4)))
What the hell - floor is supposed to return an integer anyway. What's going on here? :-D
I think I now managed to really boil it down to the very essence :-D
-- 1.96
SELECT IIF(1 = 2,
  CAST(1.0 AS DECIMAL (36, 0)),
  CAST(1.96 AS DECIMAL(19, 4))
)
-- 2.0
SELECT IIF(1 = 2,
  CAST(1.0 AS DECIMAL (37, 0)),
  CAST(1.96 AS DECIMAL(19, 4))
)
-- 2
SELECT IIF(1 = 2,
  CAST(1.0 AS DECIMAL (38, 0)),
  CAST(1.96 AS DECIMAL(19, 4))
)
",<sql-server><t-sql><precision><sql-server-2016><sqldatatypes>,3520,0,86,1493,0,14,26,45,6128,0,18,2,83,2018-07-20 12:32,2018-07-20 13:05,2018-07-20 13:05,0,0,Basic,2
48427185,How to make good reproducible Apache Spark examples,"I've been spending a fair amount of time reading through some questions with the pyspark and spark-dataframe tags and very often I find that posters don't provide enough information to truly understand their question. I usually comment asking them to post an MCVE but sometimes getting them to show some sample input/output data is like pulling teeth.
Perhaps part of the problem is that people just don't know how to easily create an MCVE for spark-dataframes. I think it would be useful to have a spark-dataframe version of this pandas question as a guide that can be linked.
So how does one go about creating a good, reproducible example?
",<dataframe><apache-spark><pyspark><apache-spark-sql>,642,2,0,41968,17,109,152,50,9088,0,3145,4,83,2018-01-24 16:24,2018-01-24 16:24,2018-01-24 16:24,0,0,Intermediate,31
50645402,MySQL: SyntaxError: Unexpected identifier,"I just installed MySQL on my computer and when I try to create a database from the MySQL shell, I get this error:
MySQL  JS &gt; CREATE DATABASE databasename;
SyntaxError: Unexpected identifier
Does anyone know why this is happening? Is there a problem with the installation of MySQL?
",<mysql><sql>,285,0,2,2164,4,24,42,81,101109,0,417,7,82,2018-06-01 14:14,2018-06-01 14:18,2018-06-01 14:18,0,0,Basic,1
57265913,Error: TCP Provider: Error code 0x2746. During the Sql setup in linux through terminal,"I am trying to setup the ms-sql server in my linux by following the documentation 
https://learn.microsoft.com/pl-pl/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-2017
The SQL server status is Active (Running).
I am getting the following error while executing the command 
sqlcmd -S localhost -U SA -P '&lt;YourPassword&gt;'
Error:
  Sqlcmd: Error: Microsoft ODBC Driver 17 for SQL Server : TCP Provider:
  Error code 0x2746. Sqlcmd: Error: Microsoft ODBC Driver 17 for SQL
  Server : Client unable to establish connection.
I also tried by giving the command 
sqlcmd -S 127.0.0.1 -U SA -P '&lt;YourPassword&gt;' 
But the same error is displayed. When I tried the wrong password it also displays the same error.
",<sql-server><linux><tcpclient>,725,2,2,821,1,6,3,43,102076,0,0,21,82,2019-07-30 6:54,2019-07-30 9:26,,0,,Intermediate,15
56108144,Using S3 as a database vs. database (e.g. MongoDB),"Due to simple setup and low costs I am considering using AWS S3 bucket instead of a NoSQL database to save simple user settings as a JSON (around 30 documents).
I researched the following disadvantages of not using a database which are not relevant for my use case:
Listing of buckets/files will cost you money.
No updates - you cannot update a file, just replace it.
No indexes.
Versioning will cost you $$.
No search
No transactions
No query API (SQL or NoSQL)
Are there any other disavantages of using a S3 bucket instead of a database?
",<mongodb><amazon-web-services><amazon-s3><nosql>,540,0,0,2978,6,26,47,80,63603,0,1087,2,80,2019-05-13 8:09,2019-05-13 8:50,2019-05-13 8:52,0,0,Intermediate,18
50746465,How do I call SQLitePCL.Batteries.Init().?,"I am attempting to create an SQLite database for my application and have come across this error. 
  System.Exception: 'You need to call SQLitePCL.raw.SetProvider().  If
  you are using a bundle package, this is done by calling
  SQLitePCL.Batteries.Init().'
I created a simple console app the run the exact same code for creation, with no issues. The code looks like this!
using (var dataContext = new SampleDBContext())
{
    dataContext.Accounts.Add(new Account() { AccountName = name, AccountBalance = balance });
}
public class SampleDBContext : DbContext
{
    private static bool _created = false;
    public SampleDBContext()
    {
        if (!_created)
        {
            _created = true;
            Database.EnsureDeleted();
            Database.EnsureCreated();
        }
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionbuilder)
    {
        optionbuilder.UseSqlite(@""Data Source=""Source folder""\Database.db"");
    }
    public DbSet&lt;Account&gt; Accounts { get; set; }
}
Can anyone shed any light on the issue? I installed the same Nuget Packages on both projects, the only difference between the two is the Data Source and the POCO classes I used for the database.
Thanks.
Edit
My program currently consists of a Console application that references a .Net Framework Class Library. The Console application simple has a constructor that looks like this:
public Program()
{   
    using (var db = new FinancialContext())
    {
        db.Accounts.Add(new Account() { AccountName = ""RBS"", AccountBalance=20 });
    }
}
The Class Library has a FinancialContext as Follows:
public class FinancialContext : DbContext
{
    public DbSet&lt;Account&gt; Accounts { get; set; }
    public FinancialContext()
    {
      # Database.EnsureDeleted();
        Database.EnsureCreated();
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionbuilder)
    {
        optionbuilder.UseSqlite(@""Data Source=""Some Source Folder""\Database.db"");
    }
}
The Above error is shown at the # symbol point, is there a problem with the way I am coding? I would really like to know what the issue is so I can fix it properly rather than apply a 'fix'. Also I tried the suggestion in the comments, but putting the code line SQLitePCL.raw.SetProvider(new SQLitePCL.SQLite3Provider_e_sqlite3()); in the Console Application gave the error SQLitePCL is not in the current context, which leaves me thinking I am missing a reference?
",<c#><entity-framework><sqlite>,2470,0,53,2850,2,17,34,65,57172,0,50,8,80,2018-06-07 16:48,2018-06-08 10:17,2018-07-28 9:52,1,51,Basic,9
58812324,PostgreSQL(Full Text Search) vs ElasticSearch,"Hi I am doing some research before I implement search feature into my service. 
I'm currently using PostgreSQL as my main storage. I could definitely use PostgreSQL's built-in Full-Text-Search but the problem is that I have data scattered around several tables. 
My service is an e-commerce website. So if a customer searches ""good apple laptop"", I need to join Brand table, post table and review table(1 post is a combination of several reviews + short summary) to fully search all posts. If I were to use elasticsearch, I could insert complete posts by preprocessing.
From my research, some people said PostgreSQL's FTS and elasticsearch have similar performance and some people said elasticsearch is faster. Which would be better solution for my case?
Thanks in advance
",<postgresql><elasticsearch><full-text-search>,773,0,3,1413,3,18,22,78,47451,0,5,3,79,2019-11-12 5:02,2019-11-12 5:22,2019-11-12 5:22,0,0,Intermediate,21
56751565,pq: could not resize shared memory segment. No space left on device,"I have on a dashboard, a number of panels (numbering around 6)to display data points chart making queries to dockerised instance of PostgreSQL database.
Panels were working fine until very recently, some stop working and report an error like this:
  pq: could not resize shared memory segment ""/PostgreSQL.2058389254"" to 12615680 bytes: No space left on device
Any idea why this? how to work around solving this. Docker container runs on remote host accessed via ssh.
EDIT
Disk space:
$df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda1       197G  140G   48G  75% /
devtmpfs        1.4G     0  1.4G   0% /dev
tmpfs           1.4G  4.0K  1.4G   1% /dev/shm
tmpfs           1.4G  138M  1.3G  10% /run
tmpfs           1.4G     0  1.4G   0% /sys/fs/cgroup
/dev/dm-16       10G   49M   10G   1% /var/lib/docker/devicemapper/mnt/a0f3c5ab84aa06d5b2db00c4324dd6bf7141500ff4c83e23e9aba7c7268bcad4
/dev/dm-1        10G  526M  9.5G   6% /var/lib/docker/devicemapper/mnt/8623a774d736ed3dc0d2db89b7d07cae85c3d1bcafc245180eec4ffd738f93a5
shm              64M     0   64M   0% /var/lib/docker/containers/260552ebcdf2bf0961329108d3d975110f8ada0a41325f5e7dd81b8ddad9d18b/mounts/shm
/dev/dm-4        10G  266M  9.8G   3% /var/lib/docker/devicemapper/mnt/6f873e62607e7cac4c4b658c72874c787b90290f74d1159eca81af61cb467cfb
shm              64M   50M   15M  78% /var/lib/docker/containers/84c66d9fb5b6ae023d051766f4d35ced87a519a1fee68ca5c89d61ff87cf1e5a/mounts/shm
/dev/dm-2        10G  383M  9.7G   4% /var/lib/docker/devicemapper/mnt/cb3df1ae654ed78802c2e5bd7a51a1b0bdd562855a7c7803750b80b33f5c206e
shm              64M     0   64M   0% /var/lib/docker/containers/22ba2ae2b6859c24623703dcb596527d64257d2d61de53f4d88e00a8e2335211/mounts/shm
/dev/dm-3        10G   99M  9.9G   1% /var/lib/docker/devicemapper/mnt/492a19fc8f3e254c4e5cc691c3300b5fee9d1a849422673bf0c19b4b2d1db571
shm              64M     0   64M   0% /var/lib/docker/containers/39abe855a9b107d4921807332309517697f024b2d169ebc5f409436208f766d0/mounts/shm
/dev/dm-7        10G  276M  9.8G   3% /var/lib/docker/devicemapper/mnt/55c6a6c17c892d149c1cc91fbf42b98f1340ffa30a1da508e3526af7060f3ce2
shm              64M     0   64M   0% /var/lib/docker/containers/bf2e7254cd7e2c6000da61875343580ec6ff5cbf40c017a398ba7479af5720ec/mounts/shm
/dev/dm-8        10G  803M  9.3G   8% /var/lib/docker/devicemapper/mnt/4e51f48d630041316edd925f1e20d3d575fce4bf19ef39a62756b768460d1a3a
shm              64M     0   64M   0% /var/lib/docker/containers/72d4ae743de490ed580ec9265ddf8e6b90e3a9d2c69bd74050e744c8e262b342/mounts/shm
/dev/dm-6        10G   10G   20K 100% /var/lib/docker/devicemapper/mnt/3dcddaee736017082fedb0996e42b4c7b00fe7b850d9a12c81ef1399fa00dfa5
shm              64M     0   64M   0% /var/lib/docker/containers/9f2bf4e2736d5128d6c240bb10da977183676c081ee07789bee60d978222b938/mounts/shm
/dev/dm-5        10G  325M  9.7G   4% /var/lib/docker/devicemapper/mnt/65a2bf48cbbfe42f0c235493981e62b90363b4be0a2f3aa0530bbc0b5b29dbe3
shm              64M     0   64M   0% /var/lib/docker/containers/e53d5ababfdefc5c8faf65a4b2d635e2543b5a807b65a4f3cd8553b4d7ef2d06/mounts/shm
/dev/dm-9        10G  1.2G  8.9G  12% /var/lib/docker/devicemapper/mnt/3216c48346c3702a5cd2f62a4737cc39666983b8079b481ab714cdb488400b08
shm              64M     0   64M   0% /var/lib/docker/containers/5cd0774a742f54c7c4fe3d4c1307fc93c3c097a861cde5f611a0fa9b454af3dd/mounts/shm
/dev/dm-10       10G  146M  9.9G   2% /var/lib/docker/devicemapper/mnt/6a98acd1428ae670e8f1da62cb8973653c8b11d1c98a8bf8be78f59d2ddba062
shm              64M     0   64M   0% /var/lib/docker/containers/a878042353f6a605167e7f9496683701fd2889f62ba1d6c0dc39c58bc03a8209/mounts/shm
tmpfs           285M     0  285M   0% /run/user/0
EDIT-2
$df -ih
Filesystem     Inodes IUsed IFree IUse% Mounted on
/dev/vda1         13M  101K   13M    1% /
devtmpfs         354K   394  353K    1% /dev
tmpfs            356K     2  356K    1% /dev/shm
tmpfs            356K   693  356K    1% /run
tmpfs            356K    16  356K    1% /sys/fs/cgroup
/dev/dm-16        10M  2.3K   10M    1% /var/lib/docker/devicemapper/mnt/a0f3c5ab84aa06d5b2db00c4324dd6bf7141500ff4c83e23e9aba7c7268bcad4
/dev/dm-1         10M   19K   10M    1% /var/lib/docker/devicemapper/mnt/8623a774d736ed3dc0d2db89b7d07cae85c3d1bcafc245180eec4ffd738f93a5
shm              356K     1  356K    1% /var/lib/docker/containers/260552ebcdf2bf0961329108d3d975110f8ada0a41325f5e7dd81b8ddad9d18b/mounts/shm
/dev/dm-4         10M   11K   10M    1% /var/lib/docker/devicemapper/mnt/6f873e62607e7cac4c4b658c72874c787b90290f74d1159eca81af61cb467cfb
shm              356K     2  356K    1% /var/lib/docker/containers/84c66d9fb5b6ae023d051766f4d35ced87a519a1fee68ca5c89d61ff87cf1e5a/mounts/shm
/dev/dm-2         10M  5.6K   10M    1% /var/lib/docker/devicemapper/mnt/cb3df1ae654ed78802c2e5bd7a51a1b0bdd562855a7c7803750b80b33f5c206e
shm              356K     1  356K    1% /var/lib/docker/containers/22ba2ae2b6859c24623703dcb596527d64257d2d61de53f4d88e00a8e2335211/mounts/shm
/dev/dm-3         10M  4.6K   10M    1% /var/lib/docker/devicemapper/mnt/492a19fc8f3e254c4e5cc691c3300b5fee9d1a849422673bf0c19b4b2d1db571
shm              356K     1  356K    1% /var/lib/docker/containers/39abe855a9b107d4921807332309517697f024b2d169ebc5f409436208f766d0/mounts/shm
/dev/dm-7         10M  7.5K   10M    1% /var/lib/docker/devicemapper/mnt/55c6a6c17c892d149c1cc91fbf42b98f1340ffa30a1da508e3526af7060f3ce2
shm              356K     1  356K    1% /var/lib/docker/containers/bf2e7254cd7e2c6000da61875343580ec6ff5cbf40c017a398ba7479af5720ec/mounts/shm
/dev/dm-8         10M   12K   10M    1% /var/lib/docker/devicemapper/mnt/4e51f48d630041316edd925f1e20d3d575fce4bf19ef39a62756b768460d1a3a
shm              356K     1  356K    1% /var/lib/docker/containers/72d4ae743de490ed580ec9265ddf8e6b90e3a9d2c69bd74050e744c8e262b342/mounts/shm
/dev/dm-6        7.9K  7.3K   623   93% /var/lib/docker/devicemapper/mnt/3dcddaee736017082fedb0996e42b4c7b00fe7b850d9a12c81ef1399fa00dfa5
shm              356K     1  356K    1% /var/lib/docker/containers/9f2bf4e2736d5128d6c240bb10da977183676c081ee07789bee60d978222b938/mounts/shm
/dev/dm-5         10M   27K   10M    1% /var/lib/docker/devicemapper/mnt/65a2bf48cbbfe42f0c235493981e62b90363b4be0a2f3aa0530bbc0b5b29dbe3
shm              356K     1  356K    1% /var/lib/docker/containers/e53d5ababfdefc5c8faf65a4b2d635e2543b5a807b65a4f3cd8553b4d7ef2d06/mounts/shm
/dev/dm-9         10M   53K   10M    1% /var/lib/docker/devicemapper/mnt/3216c48346c3702a5cd2f62a4737cc39666983b8079b481ab714cdb488400b08
shm              356K     1  356K    1% /var/lib/docker/containers/5cd0774a742f54c7c4fe3d4c1307fc93c3c097a861cde5f611a0fa9b454af3dd/mounts/shm
/dev/dm-10        10M  5.2K   10M    1% /var/lib/docker/devicemapper/mnt/6a98acd1428ae670e8f1da62cb8973653c8b11d1c98a8bf8be78f59d2ddba062
shm              356K     1  356K    1% /var/lib/docker/containers/a878042353f6a605167e7f9496683701fd2889f62ba1d6c0dc39c58bc03a8209/mounts/shm
tmpfs            356K     1  356K    1% /run/user/0
EDIT-3
postgres container service:
version: ""3.5""
services:
#other containers go here..
 postgres:
    restart: always
    image: postgres:10
    hostname: postgres
    container_name: fiware-postgres
    expose:
      - ""5432""
    ports:
      - ""5432:5432""
    networks:
      - default
    environment:
      - ""POSTGRES_PASSWORD=password""
      - ""POSTGRES_USER=postgres""
      - ""POSTGRES_DB=postgres""
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
    build:
      context: .
      shm_size: '4gb'
Database size:
postgres=# SELECT pg_size_pretty( pg_database_size('postgres'));
 pg_size_pretty
----------------
 42 GB
(1 row)
EDIT-4
Sorry, but none of the workaround related to this question actually work, including this one
On the dashboard, I have 5 panels intended to display data points. The queries are similar, except that each displays different parameters for temperature, relativeHumidity, illuminance, particles and O3. This is the query:
SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800) as time,
avg(attrvalue::float) as illuminance
FROM urbansense.weather WHERE attrname='illuminance' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
The difference is in the WHERE attrname=#parameterValue statement. I modified the postgresql.conf file to write logs but the logs doesn't seem to provide helpfull tips: here goes the logs:
$ vim postgres-data/log/postgresql-2019-06-26_150012.log
.
.
2019-06-26 15:03:39.298 UTC [45] LOG:  statement: SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800) as time,
        avg(attrvalue::float) as o3
        FROM urbansense.airquality WHERE attrname='O3' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:03:40.903 UTC [41] ERROR:  could not resize shared memory segment ""/PostgreSQL.1197429420"" to 12615680 bytes: No space left on device
2019-06-26 15:03:40.903 UTC [41] STATEMENT:  SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800) as time,
        avg(attrvalue::float) as illuminance
        FROM urbansense.weather WHERE attrname='illuminance' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:03:40.905 UTC [42] FATAL:  terminating connection due to administrator command
2019-06-26 15:03:40.905 UTC [42] STATEMENT:  SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800) as time,
        avg(attrvalue::float) as illuminance
        FROM urbansense.weather WHERE attrname='illuminance' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:03:40.909 UTC [43] FATAL:  terminating connection due to administrator command
2019-06-26 15:03:40.909 UTC [43] STATEMENT:  SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800) as time,
        avg(attrvalue::float) as illuminance
        FROM urbansense.weather WHERE attrname='illuminance' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:03:40.921 UTC [1] LOG:  worker process: parallel worker for PID 41 (PID 42) exited with exit code 1
2019-06-26 15:03:40.922 UTC [1] LOG:  worker process: parallel worker for PID 41 (PID 43) exited with exit code 1
2019-06-26 15:07:04.058 UTC [39] LOG:  temporary file: path ""base/pgsql_tmp/pgsql_tmp39.0"", size 83402752
2019-06-26 15:07:04.058 UTC [39] STATEMENT:  SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800)as time,
        avg(attrvalue::float) as relativeHumidity
        FROM urbansense.weather WHERE attrname='relativeHumidity' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:07:04.076 UTC [40] LOG:  temporary file: path ""base/pgsql_tmp/pgsql_tmp40.0"", size 83681280
2019-06-26 15:07:04.076 UTC [40] STATEMENT:  SELECT to_timestamp(floor((extract('epoch' from recvtime)/ 1800 )) * 1800)as time,
        avg(attrvalue::float) as relativeHumidity
        FROM urbansense.weather WHERE attrname='relativeHumidity' AND attrvalue&lt;&gt;'null' GROUP BY time ORDER BY time asc;
2019-06-26 15:07:04.196 UTC [38] LOG:  temporary file: path ""base/pgsql_tmp/pgsql_tmp38.0"", size 84140032
Anyone with a idea how to solve this?
",<postgresql><docker><grafana>,11183,1,129,3495,6,29,62,46,78856,0,439,6,78,2019-06-25 10:07,2019-06-25 12:28,2019-06-25 12:28,0,0,Intermediate,23
48593016,Postgresql Docker role does not exist,"I downloaded the docker container for postgres: https://hub.docker.com/r/library/postgres/, and did the following:
$ docker run --name satgres -e POSTGRES_PASSWORD=mysecretpassword -d -p 5432:5432 postgres
satgres | ""docker-entrypoint.s…"" | 5 seconds ago | Up 6 seconds | 0.0.0.0:5432-&gt;5432/tcp | satgres
$ docker exec -it c8f1b22cc41e /bin/bash        
# psql -U postgres
psql (10.1)
postgres=# CREATE DATABASE mytest;
postgres=# \password postgres
postgres=# CREATE ROLE ming WITH LOGIN PASSWORD 'pass1234';
postgres=# ALTER ROLE ming CREATEDB;
postgres=# CREATE DATABASE sjk;
postgres=# GRANT ALL PRIVILEGES ON DATABASE youtube TO ming;
postgres=# \connect sjk
youtube=# ALTER ROLE ming lOGIN;  
My plan is to use this database on docker with Django, so first want to check I can connect, but I cant.
$ psql -h localhost -p 5432 -U postgres
$ psql: FATAL:  role ""postgres"" does not exist
$ psql -h localhost -p 5432 -U ming
$ psql: FATAL:  role ""ming"" does not exist
I do the same with PSequal.app, says the same thing,
Im running on Mac High Sierra. 
Why am I getting this error? The role exists, or at least it seems it to me...
",<database><postgresql><docker>,1137,2,17,3302,4,19,39,53,102667,0,297,3,76,2018-02-03 1:19,2018-02-03 4:48,2018-02-03 4:48,0,0,Basic,9
54527277,"can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0","I’m using Ubuntu and run into to a problem when using db:migrate for ruby project. 
rails aborted!
LoadError: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~&gt; 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile.
/home/juan/odin_on_rails/RailsaAPP/bin/rails:9:in `&lt;top (required)&gt;'
/home/juan/odin_on_rails/RailsaAPP/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Caused by:
Gem::LoadError: can't activate sqlite3 (~&gt; 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile.
/home/juan/odin_on_rails/RailsaAPP/bin/rails:9:in `&lt;top (required)&gt;'
/home/juan/odin_on_rails/RailsaAPP/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Tasks: TOP =&gt; db:migrate =&gt; db:load_config
(See full trace by running task with --trace)
",<ruby-on-rails><ruby><sqlite>,960,0,15,761,1,5,3,74,22933,0,0,5,76,2019-02-05 3:13,2019-02-05 6:29,,0,,Basic,9
51552470,"DBeaver, export from database Postgres, table structure (properties) into file .txt","I have little problem with DBeaver. I want to export my structure from each table from database in file .txt. I found how to export all data but I don't need this data, just table structure.
If you have some solutions for export table structure .csv it'll be good.
Here is a an image   about structure of the table:
",<postgresql><dbeaver><export-to-text>,316,2,0,853,1,7,7,48,157591,0,0,4,75,2018-07-27 7:04,2019-02-03 15:37,,191,,Basic,9
49949526,Laravel mysql migrate error,"I recently format my mac book pro, after cloning the proyect from github and install the things I need like MySql and Sequel Pro I tried to migrate the database information but I get this error:
   Illuminate\Database\QueryException  : SQLSTATE[42000]: Syntax error or access violation: 1231 Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER' (SQL: select * from information_schema.tables where table_schema = fisica and table_name = migrations)
  Exception trace:
  1   PDOException::(""SQLSTATE[42000]: Syntax error or access violation: 1231 Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER'"")
Versions: 
Mysql  8.0.11
Laravel 5.6.12
PHP 7.1.14 (cli)
DB_CONNECTION=mysql
DB_HOST=127.0.0.1
DB_PORT=3306
DB_DATABASE=fisica
DB_USERNAME=xxx
DB_PASSWORD=xxx
I created the database from Sequel PRO GUI
",<php><mysql><laravel><laravel-5.6>,838,0,11,3232,2,13,36,78,51665,0,71,10,74,2018-04-20 20:49,2018-04-20 22:10,2018-05-16 13:25,0,26,Basic,8
49730100,Error in phpMyAdmin after updating to v4.8.0: The $cfg['TempDir'] (./tmp/) is not accessible,"phpMyAdmin worked fine with v4.7.9. Now after updating to v4.8.0 today (replacing the old phpmyadmin folder against the new one) I'm getting this message in phpMyAdmin:
  The $cfg['TempDir'] (./tmp/) is not accessible. phpMyAdmin is not able
  to cache templates and will be slow because of this.
I added the folder ./tmp/ like like this: /usr/share/tmp
phpMyAdmin is on: /usr/share/phpmyadmin
This didn't change anything.
Who know this error? What can I do?
",<php><mysql><linux><phpmyadmin>,459,0,5,2988,3,21,60,45,130016,0,105,23,73,2018-04-09 9:51,2018-04-09 10:38,2018-04-09 10:38,0,0,Basic,14
64210167,Unable to connect to Postgres DB due to the authentication type 10 is not supported,"I have recently tried my hands on Postgres. Installed it on local (PostgreSQL 13.0).
Created a maven project and used Spring Data JPA, works just fine. Whereas when I tried using Gradle project, I am not able to connect to the DB and keep getting the following error.
org.postgresql.util.PSQLException: The authentication type 10 is not
supported. Check that you have configured the pg_hba.conf file to
include the client's IP address or subnet, and that it is using an
authentication scheme supported by the driver.    at
org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:614)
~[postgresql-42.1.4.jar:42.1.4]   at
org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:222)
~[postgresql-42.1.4.jar:42.1.4]   at
org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
~[postgresql-42.1.4.jar:42.1.4]   at
org.postgresql.jdbc.PgConnection.(PgConnection.java:194)
~[postgresql-42.1.4.jar:42.1.4]   at
org.postgresql.Driver.makeConnection(Driver.java:450)
~[postgresql-42.1.4.jar:42.1.4]   at
org.postgresql.Driver.connect(Driver.java:252)
~[postgresql-42.1.4.jar:42.1.4]   at
java.sql.DriverManager.getConnection(Unknown Source) [na:1.8.0_261]
at java.sql.DriverManager.getConnection(Unknown Source)
[na:1.8.0_261]    at
org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:94)
[postgresql-42.1.4.jar:42.1.4]    at
org.postgresql.ds.common.BaseDataSource.getConnection(BaseDataSource.java:79)
[postgresql-42.1.4.jar:42.1.4]
I tried using JDBCTemplate as well. Doesn't work
Modified the pg_hba.cfg file referring to this post - Doesn't work
Used the deprecated Lib of  - Doesn't Work either.
Please Suggest me a solution for this problem.
My code and Config:
    @Configuration
    public class DataSourceConfig {
        @Bean
        public DriverManagerDataSource getDataSource() {
            DriverManagerDataSource dataSourceBuilder = new DriverManagerDataSource();
            dataSourceBuilder.setDriverClassName(&quot;org.postgresql.Driver&quot;);
            dataSourceBuilder.setUrl(&quot;jdbc:postgresql://localhost:5432/postgres&quot;);
            dataSourceBuilder.setUsername(&quot;postgres&quot;);
            dataSourceBuilder.setPassword(&quot;root&quot;);
            return dataSourceBuilder;
        }
    }
@Component
public class CustomerOrderJDBCTemplate implements CustomerOrderDao{
    private DataSource dataSource;
    private JdbcTemplate jdbcTemplateObject;
    @Autowired
    ApplicationContext context;
    public void setDataSource() {
        //Getting Bean by Class
        DriverManagerDataSource dataSource = context.getBean(DriverManagerDataSource.class);
        this.dataSource = dataSource;
        this.jdbcTemplateObject = new JdbcTemplate(this.dataSource);
    }
@Override
    public Customer create(Customer customer) {
        setDataSource();
        String sql = &quot;insert into CustomerOrder (customerType, customerPayment) values (?, ?)&quot;;
        //jdbcTemplateObject.update(sql, customerOrder.getCustomerOrderType(), customerOrder.getCustomerOrderPayment());
        KeyHolder holder = new GeneratedKeyHolder();
        jdbcTemplateObject.update(new PreparedStatementCreator() {
            @Override
            public PreparedStatement createPreparedStatement(Connection connection) throws SQLException {
                PreparedStatement ps = connection.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS);
                ps.setString(1, customer.getType());
                ps.setString(2, customer.getPayment());
                return ps;
            }
        }, holder);
        long customerId = holder.getKey().longValue();
        customer.setCustomerID(customerOrderId);
        return customer;
    }
}
dependencies
implementation('org.springframework.boot:spring-boot-starter-web')
    compile(&quot;org.springframework.boot:spring-boot-devtools&quot;)
    compile(group: 'org.postgresql', name: 'postgresql', version: '42.1.4')
    compile(&quot;org.springdoc:springdoc-openapi-ui:1.4.1&quot;)
    compile(&quot;org.springframework:spring-jdbc:5.2.5.RELEASE&quot;)
password_encryption is set like this:
postgres=# show password_encryption;
 password_encryption
---------------------
 scram-sha-256
(1 row)
",<java><postgresql><spring-boot><gradle><postgresql-13>,4301,1,70,1627,4,21,38,67,191841,0,57,20,72,2020-10-05 14:01,2020-10-05 15:27,2020-10-13 13:37,0,8,Advanced,32
53267642,Create new local server in pgadmin?,"I have PostgreSQL 11 and PGadmin 4 installed on windows. Currently I'm connected to a AWS server which hosts all of my data.
I want to create a local server (localhost) as a testing environment where I can experiment. I can't seem to do it though, and the other similar questions on stack don't help. Here's what my process is:
in pgAdmin, right click 'Servers' and go Create>Server
On the 'Create - Server' pop up box, i type in Name: Localserver. For 'connection' I type localhost. Port I leave as default '5432', db: postgres, username: postgres password: empty
click save.
however, I get an error:
  Unable to connect to server:
  could not connect to server: Connection refused (0x0000274D/10061)
  Is the server running on host ""localhost"" (::1) and accepting
  TCP/IP connections on port 5432?
  could not connect to server: Connection refused (0x0000274D/10061)
  Is the server running on host ""localhost"" (127.0.0.1) and accepting
  TCP/IP connections on port 5432?
What should I do? I am the admin if that makes a difference.
",<postgresql><pgadmin>,1036,1,0,1227,1,11,18,68,201621,0,82,7,72,2018-11-12 17:58,2018-11-12 18:08,,0,,Basic,9
59455783,pg_restore: [archiver] unsupported version (1.14) in file header,"I have a live server and development box, call them live and dev respectively both running postgresql. I can see both and manage both with pgadmin4 without trouble, and both are fully functional the one being a live website and the other where I run my website in debug mode on the my dev box. Pretty ordinary setup.
For years I have been running the same bash script I wrote that dumps the live database then restores it on the dev box so I have the latest live snapshot to work with.
Today this fails me with the titled message:
pg_restore: [archiver] unsupported version (1.14) in file header
I have tried to diagnose this, and searched extensively on-line but am bedeviled and have failed so here I am cap in hand for expertise.
To help I will share the following:
$ pg_dump --version
pg_dump (PostgreSQL) 10.11 (Ubuntu 10.11-1.pgdg18.04+1)
$ pg_restore --version
pg_restore (PostgreSQL) 10.11 (Ubuntu 10.11-1.pgdg18.04+1)
$ pg_dump --host=live.lan --port=5432 --dbname=mydb --username=myuser --format=custom &gt; test.backup
$ ls -l test.backup 
-rw-r--r-- 1 bernd bernd 2398358 Dec 23 23:40 test.backup
$ file test.backup 
test.backup: PostgreSQL custom database dump - v1.14-0
$ pg_restore --dbname=mydb test.backup 
pg_restore: [archiver] unsupported version (1.14) in file header
Given pg_dump and pg_restore are identical versions and:
$ which pg_dump
/usr/bin/pg_dump
$ which pg_restore
/usr/bin/pg_restore
$ ls -l /usr/bin/pg_dump /usr/bin/pg_restore
lrwxrwxrwx 1 root root 37 Nov 14 23:23 /usr/bin/pg_dump -&gt; ../share/postgresql-common/pg_wrapper
lrwxrwxrwx 1 root root 37 Nov 14 23:23 /usr/bin/pg_restore -&gt; ../share/postgresql-common/pg_wrapper
I can see they are not just identical versions but are run by the same wrapper script (which happens to be a perl script - now that's a language you don't see much anymore and I used to code in extensively)
So I'm left totally perplexed. Thinking there may be a version issue with the live machine:
$ ssh live.lan
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-72-generic x86_64)
$ which pg_dump
/usr/bin/pg_dump
$ which pg_restore
/usr/bin/pg_restore
$ pg_dump --version
pg_dump (PostgreSQL) 10.10 (Ubuntu 10.10-0ubuntu0.18.04.1)
$ pg_restore --version
pg_restore (PostgreSQL) 10.10 (Ubuntu 10.10-0ubuntu0.18.04.1)
I can see the live box does have slightly older version of pg_dump (which would only matter if pg_dump on my dev box somehow used a RPC to the live box to run its pg_dump).
Now there is maybe a small clue in the fact that my dev box has seen a few postgresql upgrades come through and so for example:
$ pg_lsclusters
Ver Cluster Port Status Owner    Data directory              Log file
10  main    5432 online postgres /var/lib/postgresql/10/main /var/log/postgresql/postgresql-10-main.log
11  main    5433 online postgres /var/lib/postgresql/11/main /var/log/postgresql/postgresql-11-main.log
12  main    5434 online postgres /var/lib/postgresql/12/main /var/log/postgresql/postgresql-12-main.log
The 11 and 12 clusters remain unused as evidenced by empty log files. I'm using 10. But I do notice that:
$ psql --version
psql (PostgreSQL) 12.1 (Ubuntu 12.1-1.pgdg18.04+1)
$ ssh live.lan
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-72-generic x86_64)
$ psql --version
psql (PostgreSQL) 10.10 (Ubuntu 10.10-0ubuntu0.18.04.1)
which is mildly fishy but again not obviously a cause or related:
I am using pg_dump not psql
I am using only the dev boxes pg tools not the live boxes (they should be irrelevant, the whole data transfer theoretically over port 5432 on the live box which delivers a databse dump to pg_dump on my dev box.
Here are the clusters on the love box and it's over port 5432 that on live.lan I'm running pg_dump!
$ pg_lsclusters 
Ver Cluster Port Status Owner    Data directory           Log file
10  main    5432 online postgres /data/postgresql/10/main /var/log/postgresql/postgresql-10-main.log
I am deeply perplexed and hamstrung at present by this. Would deeply appreciate forward moving clues. If I am compelled to fish around in the dark I will probbaly uninstall postgres 11 and 12 again and see if that helps, else I will end up having to trace /usr/share/postgresql-common/pg_wrapper to see how and where the two paths of pg_dump and pg_restore diverge down incompatible version paths.
Update:
A further clue I have uncovered, that permits me a workaround but simply deepens the mystery is as follows:
$ sudo -u postgres pg_dump --host=live.lan --port=5432 --dbname=mydb --username=myuser --format=custom &gt; test.backup
$ sudo -u postgres /usr/lib/postgresql/10/bin/pg_dump --host=live.lan --port=5432 --dbname=mydb --username=myuser --format=custom &gt; test2.backup
$ sudo -u postgres pg_restore -l test.backup
pg_restore: [archiver] unsupported version (1.14) in file header
$ sudo -u postgres pg_restore -l test2.backup
... produces listing of contents ...
$ sudo -u postgres pg_dump --version
pg_dump (PostgreSQL) 10.11 (Ubuntu 10.11-1.pgdg18.04+1)
$ sudo -u postgres /usr/lib/postgresql/10/bin/pg_dump --version
pg_dump (PostgreSQL) 10.11 (Ubuntu 10.11-1.pgdg18.04+1)
That is perplexing beyond belief. The only possible explanations:
in spite of reporting identical version numbers the two pg_dumps are different. I would rule this out as beyond belief.
pg_dump runs pg_wrapper which runs /usr/lib/postgresql/10/bin/pg_dump with some mystery argument(s) that break it!
The second is plausible, and will require me to instrument pg_wrapper to diagnose.
Update 2:
And one instrumentation of pg_wrapper later. It eventuates that pg_dump runs pg_wrapper which runs /usr/lib/postgresql/12/bin/pg_dump yet it runs /usr/lib/postgresql/10/bin/pg_restore ... Go figure! Starting to think this a postgresql version interoperability bug!
Update 3:
Looking deeper into pg_wrapper, nailed the cause, and yes I'd argue it's a pg_wrapper bug of a kind though it may be debatable, hardly though IMHO. Here's what it does:
If --host is provided then it uses the newest version of postgresql installed (12 in my case and this is for pg_dump, so pg_dump 12 creates the dump)
If --host is not provided then it consults the user config (10 in my case, and this is for pg_restore, so pg_restore 10 is run and it can't read a file created by pg_dump 12).
So why is this a bug? Because I have a use config, and I'd like it respected whether or not I'm talking to a remote host. More to the point if I specify a host I certainly don't expect to use the latest local version ignoring local config. I'd expect either to respect local config (as is the case when no remote host is specified) or try to match the rmeote hosts version. Arbitrarily leaning on the latest installed version is IMHO deeply questionable.
BUT it turns out there is a workaround that works. Essentially instead of:
sudo -u postgres pg_restore -l test.backup
this works:
sudo -u postgres pg_restore --host=localhost -l test.backup
By specifying the host ironically we force it to ignore local configs and use the newest version of pg_restore which seems to work fine restoring to a PG 10 cluster.
",<postgresql><ubuntu><pg-dump><pg-restore>,7064,0,61,1884,1,15,32,57,122001,0,80,7,71,2019-12-23 12:57,2020-03-26 15:48,,94,,Advanced,32
62807717,How can I solve Postgresql SCRAM authentication problem?,"I am getting an error after moving the project to production. The error is as follows while running with production server
pg_connect(): Unable to connect to PostgreSQL server: SCRAM
authentication requires libpq version 10 or above.
Here is my PostgreSQL version:
Development Version :
PostgreSQL 11.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623
(Red Hat 4.8.5-36), 64-bit
Production Version :
PostgreSQL 11.5 (EnterpriseDB Advanced Server 11.5.12) on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36), 64-bit
",<postgresql><authentication><libpq>,555,0,0,2387,6,20,31,73,176518,0,63,20,70,2020-07-09 5:00,2020-07-09 6:07,2020-07-09 6:07,0,0,Advanced,32
50958721,Convert a spark DataFrame to pandas DF,"Is there a way to convert a Spark Df (not RDD) to pandas DF
I tried the following:
var some_df = Seq(
 (""A"", ""no""),
 (""B"", ""yes""),
 (""B"", ""yes""),
 (""B"", ""no"")
 ).toDF(
""user_id"", ""phone_number"")
Code:
%pyspark
pandas_df = some_df.toPandas()
Error:
 NameError: name 'some_df' is not defined
Any suggestions.
",<pandas><apache-spark><apache-spark-sql>,307,0,11,4242,7,40,75,47,220050,0,135,3,70,2018-06-21 0:16,2018-06-21 1:43,2018-06-21 1:43,0,0,Basic,6
51750715,"Could not translate host name ""db"" to address using Postgres, Docker Compose and Psycopg2","In one folder I have 3 files: base.py, Dockerfile and docker-compose.yml.
base.py:
import psycopg2
conn = psycopg2.connect(""dbname='base123' user='postgres' host='db' password='pw1234'"")
Dockerfile:
FROM ubuntu:16.04
RUN apt-get update
RUN apt-get -y install python-pip
RUN apt-get update
RUN pip install --upgrade pip
RUN pip install psycopg2-binary
COPY base.py base.py
RUN python base.py
docker-compose.yml:
version: '3'
services:
  db:
    image: 'postgres:latest'
    expose:
      - ""5432""
    environment:
      POSTGRES_PASSWORD: pw1234
      POSTGRES_DB: base123
  aprrka:
    build: .    
    depends_on:
      - db
After I ran docker-compose up, I got the following error:
Traceback (most recent call last):
  File ""base.py"", line 5, in &lt;module&gt;
conn = psycopg2.connect(""dbname='base123' user='postgres' host='db' password='pw1234'"")
   File ""/usr/local/lib/python2.7/dist-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name ""db"" to address: Name or service not known
ERROR: Service 'aprrka' failed to build: The command '/bin/sh -c python base.py' returned a non-zero code: 1
I don't know why I have this error. I exposed port 5432. By default Compose sets up a single network for app. Each service joins the default network, I think that my app with postgres should work together. Did I write incorrect docker-compose.yml?
",<python><postgresql><docker><docker-compose><psycopg2>,1475,0,36,994,1,8,20,58,191709,0,71,7,69,2018-08-08 15:43,2018-08-08 15:56,2018-08-08 15:56,0,0,Basic,14
50803608,Can't export my database from mysql workbench,"I am trying to export my database from MySQL Workbench but I get this during the export progress:
Running: mysqldump.exe
--defaults-file=&quot;c:\users\user\appdata\local\temp\tmp2h91wa.cnf&quot;  --user=root --host=localhost --protocol=tcp --port=3306 --default-character-set=utf8 --skip-triggers &quot;mydb&quot; mysqldump: Couldn't execute 'SELECT COLUMN_NAME,
JSON_EXTRACT(HISTOGRAM, '$.&quot;number-of-buckets-specified&quot;')
FROM information_schema.COLUMN_STATISTICS                WHERE
SCHEMA_NAME = 'mydb' AND TABLE_NAME = 'courses';': Unknown table
'column_statistics' in information_schema (1109)
Operation failed with exitcode 2 20:55:09 Export of
C:\Users\user\Documents\dumps\mydb.sql has finished with 1 errors
",<mysql><database><export><mysql-workbench>,728,0,0,701,1,5,7,49,127634,0,0,16,66,2018-06-11 18:01,2018-06-27 12:44,2018-10-23 8:21,16,134,Basic,14
58938358,MYSQL - Warning: #1681 Integer display width is deprecated,"I'm getting this warning when importing mysql dumps in phpMyAdmin: 
Warning: #1681 Integer display width is deprecated and will be removed in a future release.
I found this on https://dev.mysql.com/worklog/task/?id=13127
Deprecate the ZEROFILL attribute for numeric data types and the display width attribute for integer types.
but i don't really understand what it means. Can someone explain what is the problem generating this warning, and how to resolve it. 
",<mysql><phpmyadmin>,462,2,2,2389,1,21,25,35,94833,0,385,3,66,2019-11-19 16:09,2019-12-01 7:23,2019-12-01 7:23,12,12,Basic,14
51770485,TypeError: Object of type 'DataFrame' is not JSON serializable,"I'm trying to create a plotly graph with some data I've got from my PostgreSQL server, but when I try to graph I'm getting an error: ""TypeError: Object of type 'DataFrame' is not JSON serializable""
Here's the code so far:
import dash
import numpy as np
import pandas as pd
import plotly.offline as py
import plotly.graph_objs as go
import psycopg2 as pg2
import datetime
conn = pg2.connect(database='X',user='X',password=secret)
cur = conn.cursor()
cur.execute(""SELECT * FROM times;"")
a = cur.fetchall()
str(a)
df = pd.DataFrame([[ij for ij in i] for i in a])
df.to_json()
df.rename(columns={0: ""Serial Number"", 1: ""Status"", 2: ""Date"", 3: ""Time"", 4: ""Number""}, inplace=True);
x = df[""Date""]
data = [go.Scatter(
            x=x,
            y=df[""Status""])]
layout = go.Layout(title=""Server Data Visualization"",
                   xaxis = dict(
                   range = [df.head(1),
                            df.tail(1)]),
                    yaxis=dict(title = ""Status""))
fig = go.Figure(data = data, layout = layout)
py.plot(fig)
The df[""Date""] is the date in format of ""2018-08-03"" and the df[""Status""] is either ""Uptime"" or ""Downtime.""
Can someone tell me what I'm doing incorrectly? I'm trying to have this graph basically be dates on the x-axis read in from the sql server, and then two values on the y-axis that represent either the value of ""Uptime"" or ""Downtime.""
Traceback (most recent call last):
  File ""\\srv31data1\users$\User\Desktop\basic.py"", line 37, in &lt;module&gt;
    py.plot(fig)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\plotly\offline\offline.py"", line 469, in plot
    '100%', '100%', global_requirejs=False)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\plotly\offline\offline.py"", line 184, in _plot_html
    cls=utils.PlotlyJSONEncoder)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\plotly\utils.py"", line 161, in encode
    encoded_o = super(PlotlyJSONEncoder, self).encode(o)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\site-packages\plotly\utils.py"", line 229, in default
    return _json.JSONEncoder.default(self, obj)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\encoder.py"", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'DataFrame' is not JSON serializable
Edit: Sorry, forgot to post the traceback!
",<python-3.x><postgresql><pandas><plotly><psycopg2>,2828,0,54,663,1,5,5,42,143451,0,0,2,66,2018-08-09 15:08,2018-08-09 15:23,2018-08-09 15:23,0,0,Basic,6
61097695,'Self signed certificate' error during query the Heroku hosted Postgres database from the Node.js application,"My Node.js app is able to work with local Postgres database via npm pg module. 
I can connect to the Heroku hosted Postgres database (free Hobby Dev plan) via command line with heroku pg:psql command as well. 
But when my Node.js app is trying to query to Heroku hosted Postgres database I am receiving an self signed certificate error.
Here is the output with self signed certificate error:
(node:2100) UnhandledPromiseRejectionWarning: Error: self signed certificate
    at TLSSocket.onConnectSecure (_tls_wrap.js:1051:34)
    at TLSSocket.emit (events.js:189:13)
    at TLSSocket._finishInit (_tls_wrap.js:633:8)
(node:2100) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)
(node:2100) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.
D:\MY\DEV\PROJECTS\AdsSubscribeBot\test.js:57
  if (err) throw err;
           ^
Error: Connection terminated unexpectedly
    at Connection.con.once (D:\MY\DEV\PROJECTS\AdsSubscribeBot\node_modules\pg\lib\client.js:264:9)
    at Object.onceWrapper (events.js:277:13)
    at Connection.emit (events.js:189:13)
    at Socket.&lt;anonymous&gt; (D:\MY\DEV\PROJECTS\AdsSubscribeBot\node_modules\pg\lib\connection.js:76:10)
    at Socket.emit (events.js:194:15)
    at TCP._handle.close (net.js:597:12)
Simpliest way to reproduce this error is to try use the sample code to connecting in Node.js from Heroku devcenter:
https://devcenter.heroku.com/articles/heroku-postgresql#connecting-in-node-js
Here is the sample of the code that causes self signed certificate error:
const connectionString = 'postgres://USERNAME:PASSWORD@HOST:PORT/DB_NAME';
const { Client } = require('pg');
const client = new Client({
  connectionString: connectionString,
  ssl: true
});
client.connect();
client.query('SELECT * FROM users;', (err, res) =&gt; {
  if (err) throw err;
  for (let row of res.rows) {
    console.log(JSON.stringify(row));
  }
  client.end();
});
Maybe someone has faced the same issue and know the way how to solve it.
Thanks in advance for any help.
",<javascript><node.js><postgresql><heroku>,2318,2,40,653,1,5,5,41,36203,0,1,5,65,2020-04-08 9:41,2020-04-09 16:28,2020-04-09 16:28,1,1,Basic,13
48962106,add unique constraint in room database to multiple column,"I have one entity in room
@Entity(foreignKeys ={
        @ForeignKey(entity = Label.class, parentColumns = ""_id"", childColumns = ""labelId"", onDelete = CASCADE),
        @ForeignKey(entity = Task.class, parentColumns = ""_id"", childColumns = ""taskId"", onDelete = CASCADE)
})
public class LabelOfTask extends Data{
    @ColumnInfo(name = ""labelId"")
    private Integer labelId;
    @ColumnInfo(name = ""taskId"")
    private Integer taskId;
}
sql syntax of this entity is as below
CREATE TABLE `LabelOfTask` (
    `_id` INTEGER PRIMARY KEY AUTOINCREMENT,
     `labelId` INTEGER,
     `taskId` INTEGER,
     FOREIGN KEY(`labelId`) REFERENCES `Label`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE ,
     FOREIGN KEY(`taskId`) REFERENCES `Task`(`_id`) ON UPDATE NO ACTION ON DELETE CASCADE
 );
but what change or annotation I need to add in entity class if I want to append below constraint to the auto generated sql schema of the table
unique (labelId, taskId)
Ultimately I want to make combination of labelId and taskId unique in a table(or entity of room) using room library.
",<android><sqlite><database-design><android-room><android-architecture-components>,1070,0,18,1085,1,9,15,79,50747,0,26,5,65,2018-02-24 11:03,2018-02-24 12:16,2018-02-24 12:16,0,0,Basic,9
60420940,"How to fix error ""Error: Database is uninitialized and superuser password is not specified.""","Hello i get this error after i run docker-compose build up
But i get this error
postgres_1 | Error: Database is uninitialized and superuser password is not specified.
Here is a snap shot of the error!
And down below is my docker-compose.yml file
version: '3.6'
Server.js file
services: 
  smart-brain-api:
    container_name: backend
    build: ./
    command: npm start
    working_dir: /usr/src/smart-brain-api
    ports:
      - &quot;3000:3000&quot;
    volumes:
      - ./:/usr/src/smart-brain-api
  #PostGres Database
  postgres:
    image: postgres
    ports:
      - &quot;5432:5432&quot;
",<postgresql><docker><docker-compose>,597,1,17,1299,1,12,22,41,74498,0,13,7,64,2020-02-26 19:12,2020-02-26 23:54,2020-02-26 23:54,0,0,Basic,14
51294268,"pip install mysqlclient returns ""fatal error C1083: Cannot open file: 'mysql.h': No such file or directory","Here is this issue:
I attempt to install mysqlclient like so
C:\Users\amccommon349&gt;pip install mysqlclient
Collecting mysqlclient
  Using cached https://files.pythonhosted.org/packages/ec/fd/83329b9d3e14f7344d1
cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.gz
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\amccommon349\appdata\local\programs\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\AMCCOM~1\\AppData\\Local\\Temp\\pip-install-qcgo48hf\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\AMCCOM~1\AppData\Local\Temp\pip-record-q4yoftj8\install-record.txt --single-version-externally-managed --compile:
c:\users\amccommon349\appdata\local\programs\python\python36\lib\distutils\dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
warnings.warn(msg)
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.6
copying _mysql_exceptions.py -&gt; build\lib.win-amd64-3.6
creating build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\compat.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\connections.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\converters.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\cursors.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\release.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\times.py -&gt; build\lib.win-amd64-3.6\MySQLdb
creating build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CR.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\ER.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FLAG.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
running build_ext
building '_mysql' extension
creating build\temp.win-amd64-3.6
creating build\temp.win-amd64-3.6\Release
C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\bin\HostX86\x64\cl.exe /c/nologo/Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" -Ic:\users\amccommon349\appdata\local\programs\python\python36\include -Ic:\users\amccommon349\appdata\local\programs\python\python36\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\ProgramFiles (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win-amd64-3.6\Release\_mysql.obj /Zl _mysql.c
_mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
I made sure I had all of the files needed from visual studios build tools, I downloaded the mysql-python connector, and updated my pip and setup tools. I am a complete beginner to this and would appreciate any input as to how to go about fixing this error.
",<python><mysql><sql><pip>,3995,1,39,645,1,5,8,81,81264,0,1,25,63,2018-07-11 21:16,2018-07-11 22:10,2018-07-11 22:10,0,0,Basic,14
51716530,AWS Aurora MySQL serverless: how to connect from MySQL Workbench,"I was trying to use AWS Aurora Serverless for MySQL in my project, but I am impossible to connect to it, though I have the endpoint, username, password.
What I have done:
From AWS console managment, I select RDS > Instances > Aurora > Serverless 
Leave the default settings
Create database
AWS will only create an AWS Cluster
I open MySQL Workbench, and use endpoint, username, password to connect the database
Ressult: 
  Your connection attempt failed for user 'admin' from your host to
  server at xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com:3306: 
  Can't connect to MySQL server on
  'xxxxx.cluster-abcdefg1234.eu-west-1.rds.amazonaws.com' (60)
Did I make any wrong steps ? Please advice me.
****EDIT****
I tried to create another Aurora database with capacity type: Provisioned. I can connect to the endpoint seamlessly with username and password by MySql workbench. It means that the port 3306 is opened for workbench. 
About the security group: 
",<mysql><amazon-web-services><serverless><amazon-aurora>,964,2,0,2766,6,26,52,70,44243,0,17,13,63,2018-08-06 22:37,2018-08-06 22:58,,0,,Basic,14
57975093,Create a Superuser in postgres,"I'm looking for setup a Rails Environment with Vagrant, for that purpose the box it's been provisioned through bash shell method and includes among others this line:
sudo -u postgres createuser &lt;superuserusername&gt; -s with password '&lt;superuserpassword&gt;'
But I'm getting a configuration error:
createuser: too many command-line arguments (first is &quot;with&quot;)
Can you help me with the correct syntax for create a Superuser with a password. Thanks.
",<sql><database><postgresql><ubuntu><superuser>,464,0,1,1345,2,11,16,35,170650,0,1,5,62,2019-09-17 13:16,2019-09-17 13:43,2019-09-18 16:47,0,1,Basic,9
58740043,How do I catch a psycopg2.errors.UniqueViolation error in a Python (Flask) app?,"I have a small Python web app (written in Flask) that uses sqlalchemy to persist data to the database.  When I try to insert a duplicate row, an exception is raised, something like this:
(psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""uix_my_column""
I would like to wrap the exception and re-raise my own so I can add my own logging and messaging that is specific to that particular error.  This is what I tried (simplified):
from db import DbApi
from my_exceptions import BadRequest
from psycopg2.errors import UniqueViolation # &lt;-- this does not exist!
class MyClass:
    def __init__(self):
        self.db = DbApi() 
    def create(self, data: dict) -&gt; MyRecord:
        try:
            with self.db.session_local(expire_on_commit=False) as session:
                my_rec = MyRecord(**data)
                session.add(my_rec)
                session.commit()
                session.refresh(my_rec)
                return my_rec
        except UniqueViolation as e:
            raise BadRequest('A duplicate record already exists')
But this fails to trap the error because psycopg2.errors.UniqueViolation isn't actually a class name (!).
In PHP, this would be as easy as catching copy/pasting the classname of the exception, but in Python, this is much more obfuscated.
There was a similar question here, but it didn't deal with this specific use-case and (importantly), it did not clarify how one can identify the root exception class name. 
How does one find out what exception is actually being raised? Why does Python hide this?
",<python><sqlalchemy>,1580,1,20,8967,5,36,51,37,49377,0,109,7,61,2019-11-06 23:49,2019-11-07 0:02,,1,,Basic,13
58961043,How to install libpq-fe.h?,"I cannot figure this out for the life of me.
When I pip install django-tenant-schemas it tries to install the dependency psycopg2 which requires the Python headers and gcc. I have all this installed and still keep getting this error!
./psycopg/psycopg.h:35:10: fatal error: libpq-fe.h: No such file or directory
So to install libpq-fe-h I need to sudo apt-get install libpq-dev..
..which returns..
libpq-dev is already the newest version (10.10-0ubuntu0.18.04.1).
Then when I sudo find / libpq-fe.h it doesn't seem to be in my OS.
I am lost at this point. If anyone can help I would highly appreciate it.
",<django><python-3.x><postgresql><pip>,605,0,7,2134,3,13,38,68,36211,0,319,4,60,2019-11-20 18:24,2019-11-20 18:46,2019-11-20 18:46,0,0,Basic,13
51004516,.NET Core 2.1 Identity get all users with their associated roles,"I'm trying to pull out all my Identity users and their associated roles for a user management admin page. I thought this would be reasonably easy but apparently not. I've tried following the following solution: https://stackoverflow.com/a/43562544/5392786 but it hasn't worked out so far.
Here is what I have so far:
ApplicationUser:
public class ApplicationUser : IdentityUser
{
    public List&lt;IdentityUserRole&lt;string&gt;&gt; Roles { get; set; }
}
DBContext
public class ApplicationDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApplicationDbContext(DbContextOptions&lt;ApplicationDbContext&gt; options)
        : base(options)
    {
    }
}
Startup Identity code
services.AddIdentity&lt;ApplicationUser, IdentityRole&gt;(options =&gt; options.Stores.MaxLengthForKeys = 128)
            .AddEntityFrameworkStores&lt;ApplicationDbContext&gt;()
            .AddDefaultTokenProviders();
Razor Page where I want to display the list:
public class IndexModel : PageModel
{
    private readonly UserManager&lt;ApplicationUser&gt; userManager;
    public IndexModel(UserManager&lt;ApplicationUser&gt; userManager)
    {
        this.userManager = userManager;
    }
    public IEnumerable&lt;ApplicationUser&gt; Users { get; set; }
    public void OnGetAsync()
    {
        this.Users = userManager.Users.Include(u =&gt; u.Roles).ToList();
    }
}
I get the following error when calling userManager.Users.Include(u =&gt; u.Roles).ToList();:
  MySql.Data.MySqlClient.MySqlException: 'Unknown column 'u.Roles.ApplicationUserId' in 'field list''
",<c#><mysql><asp.net-core><entity-framework-core><asp.net-core-identity>,1563,1,31,3834,7,32,56,74,83751,0,100,14,59,2018-06-23 19:44,2018-06-23 22:08,2018-06-23 22:08,0,0,Basic,9
50646216,Sequelize Where if not null,"Lets say I want to do a select command with
WHERE ID=2134
But if the user does not provide the id then it should just not bother with the WHERE ID (since it is null)
How can I handle this with Sequelize? 
",<javascript><sql><node.js><express>,205,0,1,617,1,5,3,37,116192,0,0,5,59,2018-06-01 14:58,2018-06-01 15:20,,0,,Basic,10
51784903,cross-database references are not implemented:,"I am trying to convert SQL inner join query into PostgreSQL inner join query. In this inner join query which tables are using that all tables are not present in one database. we separated tables into two databases i.e. application db and security db 
users and permission table are present in security db 
userrolemapping and department are present in application db
I tried like below but I am getting following error
Error
ERROR:  cross-database references are not implemented: ""Rockefeller_ApplicationDb.public.userrolemapping""
LINE 4:         INNER JOIN ""Rockefeller_ApplicationDb"".public.userro..
SQL Stored Function
SELECT   Department.nDeptID 
    FROM Users INNER JOIN Permission 
         ON Users.nUserID = Permission.nUserID INNER JOIN UserRoleMapping
         ON Users.nUserID = UserRoleMapping.nUserID INNER JOIN Department
         ON Permission.nDeptInst = Department.nInstID
         AND  Department.nInstID = 60
    WHERE     
         Users.nUserID = 3;
PostgreSQL Stored Function
SELECT dep.ndept_id 
        FROM ""Rockefeller_SecurityDb"".public.users as  u 
        INNER JOIN  ""Rockefeller_SecurityDb"".public.permissions p ON u.nuser_id = p.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.userrolemapping as urm ON u.nuser_id = urm.nuser_id
        INNER JOIN ""Rockefeller_ApplicationDb"".public.department dep ON p.ndept_inst = dep.ninst_id
           AND  dep.ninst_id = 60
                        WHERE     
                            u.nuser_id = 3;
",<postgresql>,1490,0,18,1379,4,29,60,57,147816,0,68,5,58,2018-08-10 10:52,2018-08-10 12:43,,0,,Basic,10
62987154,MySQL won't start - error: su: warning: cannot change directory to /nonexistent: No such file or directory,"New to development &amp; self-teaching (thanks Covid) so this could be sloppy :( sorry...
let me start off by saying I don't care about the data in the database - if it is easier to wipe it and start fresh, I'm good with that (don't know how to do that but I'm ok with  it)
Not sure what caused the issue but one day MySQL wouldn't start. Using service MySQL Restart fixed it... two days later it happened again with this error
sarcasticsnark@LB-HP-LT:~/Projects/FMS$ sudo service mysql start
 * Starting MySQL database server mysqld
su: warning: cannot change directory to /nonexistent: No such file or directory
I've tried a bit of &quot;solutions&quot;
I've tried restarting MySQL
I gave myself file permissions to the mysql files (then attempted to reverse that)
I've moved the MySQL directory (then reversed it - hence the copy of the folder &quot;mysql&quot; named &quot;mysql2&quot; below)
My files now look like this and I'm not sure I got the permissions quite right.
sarcasticsnark@LB-HP-LT:/var/lib$ ls
AccountsService  command-not-found  fwupd            logrotate  mysql          mysql2,   private  systemd                  ucf                  usbutils
PackageKit       dbus               git              man-db     mysql-files    pam       python   tpm                      unattended-upgrades  vim
apt              dhcp               initramfs-tools  mecab      mysql-keyring  plymouth  snapd    ubuntu-advantage         update-manager
boltd            dpkg               landscape        misc       mysql-upgrade  polkit-1  sudo     ubuntu-release-upgrader  update-notifier
sarcasticsnark@LB-HP-LT:/var/lib$ cd mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ ls
'#ib_16384_0.dblwr'   TestingGround_development   binlog.000009   binlog.000013   binlog.000017   client-cert.pem   mysql.ibd            server-cert.pem   undo_002
'#ib_16384_1.dblwr'   TestingGround_test          binlog.000010   binlog.000014   binlog.index    client-key.pem    performance_schema   server-key.pem
'#innodb_temp'        auto.cnf                    binlog.000011   binlog.000015   ca-key.pem      debian-5.7.flag   private_key.pem      sys
 FMS_development      binlog.000008               binlog.000012   binlog.000016   ca.pem          mysql             public_key.pem       undo_001
I've re-initialized MySQL (when not running sudoku it errors the below)
2020-07-20T02:29:41.520132Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 4096 (requested 8161)
2020-07-20T02:29:41.520141Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 1967 (requested 4000)
2020-07-20T02:29:41.520561Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) initializing of server in progress as process 2570
2020-07-20T02:29:41.522888Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting.
2020-07-20T02:29:41.522921Z 0 [ERROR] [MY-010187] [Server] Could not open file '/var/log/mysql/error.log' for error logging: Permission denied
2020-07-20T02:29:41.523139Z 0 [ERROR] [MY-013236] [Server] The designated data directory /var/lib/mysql/ is unusable. You can remove all files that the server added to it.
2020-07-20T02:29:41.523187Z 0 [ERROR] [MY-010119] [Server] Aborting
2020-07-20T02:29:41.523313Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.20-0ubuntu0.20.04.1)  (Ubuntu).
/var/log/mysql - does exist and the permissions for it are:
-rw-r----- 1 mysql adm 62273 Jul 19 19:36 error.log
Here is mysql/error.log
2020-07-20T01:50:07.952988Z mysqld_safe Logging to '/var/log/mysql/error.log'.
2020-07-20T01:50:07.986416Z mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
2020-07-20T01:50:08.000603Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 1024 (requested 8161)
2020-07-20T01:50:08.000610Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 431 (requested 4000)
2020-07-20T01:50:08.262922Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.20-0ubuntu0.20.04.1) starting as process 1608
2020-07-20T01:50:08.281623Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-07-20T01:50:08.322464Z 1 [ERROR] [MY-012592] [InnoDB] Operating system error number 2 in a file operation.
2020-07-20T01:50:08.322818Z 1 [ERROR] [MY-012593] [InnoDB] The error means the system cannot find the path specified.
2020-07-20T01:50:08.322947Z 1 [ERROR] [MY-012594] [InnoDB] If you are installing InnoDB, remember that you must create directories yourself, InnoDB does not create them.
2020-07-20T01:50:08.323017Z 1 [ERROR] [MY-012646] [InnoDB] File ./ibdata1: 'open' returned OS error 71. Cannot continue operation
2020-07-20T01:50:08.323105Z 1 [ERROR] [MY-012981] [InnoDB] Cannot continue operation.
2020-07-20T01:50:08.972320Z mysqld_safe mysqld from pid file /var/lib/mysql/LB-HP-LT.pid ended
And the permissions for /var/lib/mysql
sarcasticsnark@LB-HP-LT:/var/lib/mysql$ cd /var/lib
sarcasticsnark@LB-HP-LT:/var/lib$ sudo ls -l mysql
[sudo] password for sarcasticsnark: 
total 58048
-rw-r----- 1 mysql mysql   196608 Jul 19 16:34 '#ib_16384_0.dblwr'
-rw-r----- 1 mysql mysql  8585216 Jul 11 22:54 '#ib_16384_1.dblwr'
drwxr-x--- 2 mysql mysql     4096 Jul 19 16:35 '#innodb_temp'
drwxr-x--- 2 mysql mysql     4096 Jul 15 18:06  FMS_development
drwxr-x--- 2 mysql mysql     4096 Jun 20 09:04  TestingGround_development
drwxr-x--- 2 mysql mysql     4096 Jun 22 20:07  TestingGround_test
-rw-r----- 1 mysql mysql       56 Jun 10 17:43  auto.cnf
-rw-r----- 1 mysql mysql   210461 Jul 15 17:01  binlog.000008
-rw-r----- 1 mysql mysql      179 Jul 15 17:30  binlog.000009
-rw-r----- 1 mysql mysql      156 Jul 15 17:43  binlog.000010
-rw-r----- 1 mysql mysql     2798 Jul 19 15:55  binlog.000011
-rw-r----- 1 mysql mysql      179 Jul 19 15:56  binlog.000012
-rw-r----- 1 mysql mysql      179 Jul 19 16:11  binlog.000013
-rw-r----- 1 mysql mysql      179 Jul 19 16:25  binlog.000014
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000015
-rw-r----- 1 mysql mysql      179 Jul 19 16:27  binlog.000016
-rw-r----- 1 mysql mysql      179 Jul 19 16:34  binlog.000017
-rw-r----- 1 mysql mysql      160 Jul 19 16:27  binlog.index
-rw------- 1 mysql mysql     1680 Jun 10 17:43  ca-key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  ca.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  client-cert.pem
-rw------- 1 mysql mysql     1680 Jun 10 17:43  client-key.pem
-rw-r--r-- 1 mysql mysql        0 Jun 12 15:54  debian-5.7.flag
drwxr-xr-x 2 mysql mysql     4096 Jun 10 17:43  mysql
-rw-r----- 1 mysql mysql 25165824 Jul 19 16:28  mysql.ibd
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  performance_schema
-rw------- 1 mysql mysql     1680 Jun 10 17:43  private_key.pem
-rw-r--r-- 1 mysql mysql      452 Jun 10 17:43  public_key.pem
-rw-r--r-- 1 mysql mysql     1112 Jun 10 17:43  server-cert.pem
-rw------- 1 mysql mysql     1676 Jun 10 17:43  server-key.pem
drwxr-x--- 2 mysql mysql     4096 Jun 10 17:43  sys
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_001
-rw-r----- 1 mysql mysql 12582912 Jul 19 16:34  undo_002
",<mysql><linux>,7120,0,72,593,1,4,5,49,61256,0,0,1,58,2020-07-20 0:37,2020-07-22 18:12,2020-07-22 18:12,2,2,Basic,14
59432964,Relational Data Model for Double-Entry Accounting,"Assume there is a bank, a large shop, etc, that wants the accounting to be done correctly, for both internal accounts, and keeping track of customer accounts.  Rather than implementing that which satisfies the current simple and narrow requirement, which would a 'home brew': those turn out to be a temporary crutch for the current simple requirement, and difficult or impossible to extend when new requirements come it.
As I understand it, Double-Entry Accounting is a method that is well-established, and serves all Accounting and Audit requirements, including those that are not contemplated at the current moment.  If that is implemented, it would:
eliminate the incremental enhancements that would occur over time, and the expense,
there will not be a need for future enhancement.
I have studied this Answer to another question: Derived account balance vs stored account balance for a simple bank account?, it provides good information, for internal Accounts.  A data model is required, so that one can understand the entities; their interaction; their relations, and @PerformanceDBA has given that.  This model is taken from that Answer:
Whereas that is satisfactory for simple internal accounts, I need to see a data model that provides the full Double-Entry Accounting method. 
The articles are need to be added are Journal; internal vs external Transactions; etc..
Ideally I would like to see what those double entry rows look like in database terms, what the whole process will look like in SQL, which entities are affected in each case, etc.  Cases like:
A Client deposits cash to his account
The Bank charges fees once a month to all Clients accounts (sample batch job),
A Client does some operation over the counter, and the Bank charges a fee (cash withdrawal + withdrawal fee),
Mary sends some money from her account, to John's account, which is in the same bank
Let's just call it System instead of Bank, Bank may be  too complex to model, and let the question be about imaginary system which operates with accounts and assets. Customers perform a set of operations with system (deposits, withdrawals, fee for latter, batch fees), and with each other (transfer).
",<sql><database><database-design><relational-database><accounting>,2179,3,5,982,1,10,16,58,40397,0,11,2,57,2019-12-21 1:54,2019-12-24 7:11,2019-12-24 7:11,3,3,Basic,4
48225233,"Gem::LoadError: can't activate pg (~> 0.18), already activated pg-1.0.0","I've been doing the Rails tutorial found here and have been successful up to the point of having to migrate the Comments migration using $ rails db:migrate. Prior to this point, I've been able to generate the Article model and migrate the Articles create migration with no issues. In between these two migrations, nothing has changed in my Gemfile, so I'm not sure what it is Bundler is having an issue with. 
Here are the errors, followed by the full command-line output, along with my Gemfile and schema.rb:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0.
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
Full command-line output
xxx:gangelo: ~/dev/rails/test/blog (master*) ☠  rbenv exec rails db:migrate
rails aborted!
Gem::LoadError: Specified 'postgresql' for database adapter, but the gem is not loaded. Add `gem 'pg'` to your Gemfile (and ensure its version is at the minimum required by ActiveRecord).
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:188:in `rescue in spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:185:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Caused by:
Gem::LoadError: can't activate pg (~&gt; 0.18), already activated pg-1.0.0. Make sure all dependencies are added to Gemfile.
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/postgresql_adapter.rb:2:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/connection_specification.rb:186:in `spec'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_adapters/abstract/connection_pool.rb:880:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/connection_handling.rb:58:in `establish_connection'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:124:in `block (2 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `instance_eval'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:69:in `block in execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:60:in `with_execution_control'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:65:in `execute_hook'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:50:in `block in run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `each'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/lazy_load_hooks.rb:49:in `run_load_hooks'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:326:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/base.rb:25:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:6:in `&lt;class:MySQLDatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:3:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/mysql_database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:74:in `&lt;module:DatabaseTasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:35:in `&lt;module:Tasks&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:2:in `&lt;module:ActiveRecord&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/tasks/database_tasks.rb:1:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `block in require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:258:in `load_dependency'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activesupport-5.1.4/lib/active_support/dependencies.rb:292:in `require'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/activerecord-5.1.4/lib/active_record/railtie.rb:34:in `block (3 levels) in &lt;class:Railtie&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:21:in `block in perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands/rake/rake_command.rb:18:in `perform'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/command.rb:46:in `invoke'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/railties-5.1.4/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `require'
/Users/gangelo/dev/rails/test/blog/bin/rails:9:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/rails.rb:28:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client/command.rb:7:in `call'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/client.rb:30:in `run'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/bin/spring:49:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `load'
/Users/gangelo/dev/rails/test/blog/vendor/bundle/gems/spring-2.0.2/lib/spring/binstub.rb:31:in `&lt;top (required)&gt;'
/Users/gangelo/dev/rails/test/blog/bin/spring:15:in `&lt;top (required)&gt;'
bin/rails:3:in `load'
bin/rails:3:in `&lt;main&gt;'
Tasks: TOP =&gt; db:migrate =&gt; db:load_config
(See full trace by running task with --trace)
Gemfile
source 'https://rubygems.org'
ruby '2.3.1'
git_source(:github) do |repo_name|
  repo_name = ""#{repo_name}/#{repo_name}"" unless repo_name.include?(""/"")
  ""https://github.com/#{repo_name}.git""
end
# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'
gem 'rails', '~&gt; 5.1.4'
# Use sqlite3 as the database for Active Record
# gem 'sqlite3'
# Use postgres as the database for Active Record
gem 'pg'
# Use Puma as the app server
gem 'puma', '~&gt; 3.7'
# Use SCSS for stylesheets
gem 'sass-rails', '~&gt; 5.0'
# Use Uglifier as compressor for JavaScript assets
gem 'uglifier', '&gt;= 1.3.0'
# See https://github.com/rails/execjs#readme for more supported runtimes
# gem 'therubyracer', platforms: :ruby
# Use CoffeeScript for .coffee assets and views
gem 'coffee-rails', '~&gt; 4.2'
# Turbolinks makes navigating your web application faster. Read more: https://github.com/turbolinks/turbolinks
gem 'turbolinks', '~&gt; 5'
# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder
gem 'jbuilder', '~&gt; 2.5'
# Use Redis adapter to run Action Cable in production
# gem 'redis', '~&gt; 3.0'
# Use ActiveModel has_secure_password
# gem 'bcrypt', '~&gt; 3.1.7'
# Use Capistrano for deployment
# gem 'capistrano-rails', group: :development
group :development, :test do
  # Call 'byebug' anywhere in the code to stop execution and get a debugger console
  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]
  # Adds support for Capybara system testing and selenium driver
  gem 'capybara', '~&gt; 2.13'
  gem 'selenium-webdriver'
end
# gma - start
group :development, :test do
  gem 'rspec-rails', '~&gt; 3.5', '&gt;= 3.5.2'
  gem 'rspec-activemodel-mocks', '~&gt; 1.0', '&gt;= 1.0.3'
  gem 'shoulda-matchers', '~&gt; 3.1', '&gt;= 3.1.1'
  gem 'factory_bot_rails', '~&gt; 4.8', '&gt;= 4.8.2'
  gem 'ffaker', '~&gt; 2.2'
  # gem 'timecop', '~&gt; 0.8.1'
end
# gma - end
group :development do
  # Access an IRB console on exception pages or by using &lt;%= console %&gt; anywhere in the code.
  gem 'web-console', '&gt;= 3.3.0'
  gem 'listen', '&gt;= 3.0.5', '&lt; 3.2'
  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring
  gem 'spring'
  gem 'spring-watcher-listen', '~&gt; 2.0.0'
end
# Windows does not include zoneinfo files, so bundle the tzinfo-data gem
gem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]
Schema.rb
ActiveRecord::Schema.define(version: 20180110153949) do
  # These are extensions that must be enabled in order to support this database
  enable_extension ""plpgsql""
  create_table ""articles"", force: :cascade do |t|
    t.string ""title""
    t.text ""text""
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
  end
end
Migration file
class CreateComments &lt; ActiveRecord::Migration[5.1]
  def change
    create_table :comments do |t|
      t.string :commenter
      t.text :body
      t.references :article, foreign_key: true
      t.timestamps
    end
  end
end
",<ruby-on-rails><ruby><postgresql><bundler><rails-migrations>,17720,7,211,3044,4,29,44,35,16680,0,87,2,57,2018-01-12 11:20,2018-01-14 21:25,2018-01-18 1:54,2,6,Basic,4
53293349,Azure data studio schema diagram?,"I just recently downloaded Azure Data Studio with SQL Server Express since I'm using Linux .  Is there an entity-relationship diagramming feature, kind of how SQL Server Management Studio has a database diagram feature?  I want to visually see the relationships with tables in a database if possible.
",<sql><database><entity-relationship><diagram><azure-data-studio>,301,0,0,581,1,4,5,68,52584,0,1,7,57,2018-11-14 4:48,2019-04-02 8:32,,139,,Basic,7
50589064,Get unique values using STRING_AGG in SQL Server,"The following query returns the results shown below:
SELECT 
    ProjectID, newID.value
FROM 
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2') 
Results:
ProjectID   value
---------------------
2           Q96NY7-2
2           O95833
2           O95833
2           Q96NY7-2
2           O95833
2           Q96NY7-2
4           Q96NY7-2
4           Q96NY7-2
Using the newly added STRING_AGG function (in SQL Server 2017) as it is shown in the following query I am able to get the result-set below.
SELECT 
    ProjectID,
    STRING_AGG( newID.value, ',') WITHIN GROUP (ORDER BY newID.value) AS 
NewField
FROM
    [dbo].[Data] WITH(NOLOCK)  
CROSS APPLY 
    STRING_SPLIT([bID],';') AS newID  
WHERE 
    newID.value IN ('O95833', 'Q96NY7-2')  
GROUP BY 
    ProjectID
ORDER BY 
    ProjectID
Results:
ProjectID   NewField
-------------------------------------------------------------
2           O95833,O95833,O95833,Q96NY7-2,Q96NY7-2,Q96NY7-2
4           Q96NY7-2,Q96NY7-2
I would like my final output to have only unique elements as below:
ProjectID   NewField
-------------------------------
2           O95833, Q96NY7-2
4           Q96NY7-2
Any suggestions about how to get this result? Please feel free to refine/redesign from scratch my query if needed.
",<sql><sql-server><sql-server-2017><string-aggregation>,1341,0,41,1107,1,10,19,63,130478,0,57,8,57,2018-05-29 16:33,2018-05-29 16:43,2018-05-29 16:43,0,0,Basic,10
48466959,Query for list of attribute instead of tuples in SQLAlchemy,"I'm querying for the ids of a model, and get a list of (int,) tuples back instead of a list of ids. Is there a way to query for the attribute directly?
result = session.query(MyModel.id).all()
I realize it's possible to do 
results = [r for (r,) in results]
Is it possible for the query to return that form directly, instead of having to process it myself?
",<python><sqlalchemy>,357,0,3,1018,1,11,26,68,38691,0,196,4,54,2018-01-26 17:57,2018-01-26 18:06,2018-01-26 18:06,0,0,Basic,10
53131321,Spring Boot: Jdbc javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify,"I am currently learning more about implementing JDBC and using databases in a Spring Boot webapp, and I encountered the following Stack Trace written in the bottom of the post.
I have created a simple Employee model, and I am trying to execute some database code on the same class which my main() lies in. The model and the main class are the only two java files existing in this whole project. I am trying to implement the following run() code that overrides the one from the interface, CommandLineRunner, but I do not get the logs that should come after log.info(""Part A:""):
log.info(""Part A:"")
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things I noticed:
I noticed that the last line of the log before the stack trace starts comes from: ""Thread-1"" instead of ""main"". I think that means that a thread from somewhere other than the main encountered an error and closed connection before when it should close normally.
Also, I think that because HikariPool closed before ""peer's close_notify"", which I presume that it refers to the normal closure of HikariPool, I am not able to see the final bit of logging that I have kept trying to procure. The final bit of logging that I want to see is the logging of the employee that has become inserted into my database.
The final bit of logging that I want to see should be procured from this line of code:
employees.forEach(employee -&gt; {log.info(employee.toString());
        log.info(""part a"");});
--Things to note:
Because of this line in the log, I thought I would see the employee inserted into my database, but when I queried directly on MySQL Command Line Client, it returned an empty set:
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
I don't understand why a row has been affected when nothing has been inserted into the database.
The stacktrace and logs: (The stacktrace pasted below actually repeats itself several more times but I cut it off for brevity.)
2018-11-03 21:08:32.997  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Starting JdbcTest1Application on KitKat with PID 2408 (C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1\target\classes started by Nano in C:\Users\Nano\Downloads\jdbc-test1\jdbc-test1)
2018-11-03 21:08:33.003  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : No active profile set, falling back to default profiles: default
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Started JdbcTest1Application in 1.024 seconds (JVM running for 1.778)
2018-11-03 21:08:33.770  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Creating tables
2018-11-03 21:08:33.770  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2018-11-03 21:08:34.082  INFO 2408 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2018-11-03 21:08:35.135  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Inserting Baggins Hopkins
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : rows affected: 1
2018-11-03 21:08:35.362  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Querying for employee
2018-11-03 21:08:36.065  INFO 2408 --- [           main] c.j.jdbctest1.JdbcTest1Application       : Part A:
2018-11-03 21:08:36.065  INFO 2408 --- [       Thread-1] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Shutdown initiated...
Sat Nov 03 21:08:36 KST 2018 WARN: Caught while disconnecting...
EXCEPTION STACK TRACE:
** BEGIN NESTED EXCEPTION ** 
javax.net.ssl.SSLException
MESSAGE: closing inbound before receiving peer's close_notify
STACKTRACE:
javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:129)
    at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:117)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:308)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:264)
    at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:255)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:645)
    at java.base/sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:624)
    at com.mysql.cj.protocol.a.NativeProtocol.quit(NativeProtocol.java:1312)
    at com.mysql.cj.NativeSession.quit(NativeSession.java:182)
    at com.mysql.cj.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:1750)
    at com.mysql.cj.jdbc.ConnectionImpl.close(ConnectionImpl.java:720)
    at com.zaxxer.hikari.pool.PoolBase.quietlyCloseConnection(PoolBase.java:135)
    at com.zaxxer.hikari.pool.HikariPool.lambda$closeConnection$1(HikariPool.java:441)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:834)
** END NESTED EXCEPTION **
The Java Code:
@SpringBootApplication
public class JdbcTest1Application implements CommandLineRunner {
    private static final Logger log = LoggerFactory.getLogger(JdbcTest1Application.class);
    @Autowired
    JdbcTemplate jdbcTemplate;
    public static void main(String[] args) {
        SpringApplication.run(JdbcTest1Application.class, args);
    }
    @Override
    public void run(String... args) throws Exception {
        log.info(""Creating tables"");
        jdbcTemplate.execute(""DROP TABLE IF EXISTS employees"");
        jdbcTemplate.execute(""CREATE TABLE employees (emp_id int, name varchar(100), role varchar(100), status varchar(100))"");
        log.info(""Inserting Baggins Hopkins"");
        int rowsAffected = jdbcTemplate.update(""INSERT INTO EMPLOYEE(EMP_ID, NAME, ROLE, STATUS)""
                + "" VALUES(1,'Baggins Hopkins','thief','WORKING')"");
        log.info(""rows affected: ""+ Integer.toString(rowsAffected));
        log.info(""Querying for employee"");
        String sql = ""SELECT emp_id,name,role,status FROM employees"";
        List&lt;Employee&gt; employees = jdbcTemplate.query(sql,(rs, rowNum)-&gt; 
        new Employee(rs.getInt(""emp_id""), rs.getString(""name""),
                rs.getString(""role""),Status.valueOf(rs.getString(""status""))));
        log.info(""Part A:"");
        employees.forEach(employee -&gt; {log.info(employee.toString());
            log.info(""part a"");});
    }
}
Also just in case this matters, I pasted this code from application.properties:
spring.datasource.url=jdbc:mysql://localhost:3306/employee_database
spring.datasource.username=employee
spring.datasource.password=employee
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
",<mysql><spring-boot><spring-jdbc><jdbctemplate><sslexception>,6948,0,87,815,1,9,19,74,120140,0,188,14,54,2018-11-03 12:29,2018-11-04 1:13,2018-11-07 12:55,1,4,Advanced,32
49023821,Nested Join vs Merge Join vs Hash Join in PostgreSQL,"I know how the 
Nested Join
Merge Join
Hash Join  
works and its functionality. 
I wanted to know in which situation these joins are used in Postgres 
",<postgresql><sql-execution-plan>,151,0,0,1224,3,16,34,72,28550,0,115,1,54,2018-02-28 7:09,2018-02-28 7:56,,0,,Advanced,35
54540928,Why is query with phone = N'1234' slower than phone = '1234'?,"I have a field which is a varchar(20)
When this query is executed, it is fast (Uses index seek):
SELECT * FROM [dbo].[phone] WHERE phone = '5554474477'
But this one is slow (uses index scan).
SELECT * FROM [dbo].[phone] WHERE phone = N'5554474477'
I am guessing that if I change the field to an nvarchar, then it would use the Index Seek.
",<sql><sql-server><query-performance>,339,0,2,34370,40,167,239,73,5925,0,838,3,53,2019-02-05 18:35,2019-02-05 18:38,2019-02-05 18:38,0,0,Intermediate,23
50476782,Android P - 'SQLite: No Such Table Error' after copying database from assets,"I have a database saved in my apps assets folder and I copy the database using the below code when the app first opens.
inputStream = mContext.getAssets().open(Utils.getDatabaseName());
        if(inputStream != null) {
            int mFileLength = inputStream.available();
            String filePath = mContext.getDatabasePath(Utils.getDatabaseName()).getAbsolutePath();
            // Save the downloaded file
            output = new FileOutputStream(filePath);
            byte data[] = new byte[1024];
            long total = 0;
            int count;
            while ((count = inputStream.read(data)) != -1) {
                total += count;
                if(mFileLength != -1) {
                    // Publish the progress
                    publishProgress((int) (total * 100 / mFileLength));
                }
                output.write(data, 0, count);
            }
            return true;
        }
The above code runs without problem but when you try to query the database you get an SQLite: No such table exception.
This issue only occurs in Android P, all earlier versions of Android work correctly.
Is this a known issue with Android P or has something changed?
",<android><sqlite><android-9.0-pie>,1189,0,24,825,1,9,18,66,21878,0,2,14,53,2018-05-22 21:34,2018-05-28 12:25,2018-05-31 18:35,6,9,Basic,11
57262748,SQL Server Invalid version: 15 (Microsoft.SqlServer.Smo),"Context: I'm having difficulty modifying a stored procedure in SQL Server 2016. The stored procedure performs parsing of json data within a file. For some reason I'm able to execute the stored procedure and it executes successfully but when I try to modify the stored procedure I get the following message:
Question: Does anyone have any troubleshooting tips? Below is the content of the stored procedure. SQL Server 2016 supports the various functions used including the OPENJSON function.  
USE mattermark_sandbox
GO
CREATE PROCEDURE get_company_data 
AS
IF OBJECT_ID('tempdb..##jsondump') IS NOT NULL DROP TABLE ##jsondump
IF OBJECT_ID('tempdb..##jsonparsed') IS NOT NULL DROP TABLE ##jsonparsed
IF OBJECT_ID('tempdb..##json_loop') IS NOT NULL DROP TABLE ##json_loop
CREATE TABLE ##jsondump (
    [my_json] [nvarchar](max) NULL
) 
-- Create a table to house the parsed content
CREATE TABLE ##jsonparsed (
    [id] [int] NULL,
    [url] [varchar](255) NULL,
    [company_name] [varchar](255) NULL,
    [domain] [varchar](255) NULL
)
-- Clear ##jsondump
TRUNCATE TABLE ##jsondump;
-- Clear ##jsonparsed ( only if you don't want to keep what's already there )
TRUNCATE TABLE ##jsonparsed;
-- Import ( single column ) JSON
--IMPORTANT: Need to be sure the company_data.json file actually exists on the remote server in that directory 
BULK INSERT ##jsondump
FROM 'C:\mattermark_etl_project\company_data.json' -- ( &lt;-- my file, point to your own )
WITH (
    ROWTERMINATOR = '\n'
);
-- Select JSON into ##jsonparsed
SELECT my_json 
INTO ##json_loop
FROM ##jsondump;
--SELECT * FROM ##jsondump;
INSERT INTO ##jsonparsed (
    id, [url], company_name, domain
)
SELECT DISTINCT
    jsn.id, jsn.[url], jsn.company_name, jsn.domain
FROM ##json_loop
OUTER APPLY (
    SELECT * FROM OPENJSON(##json_loop.my_json, '$.companies' )
    WITH (
        id INT '$.id',
        [url] VARCHAR(255) '$.url',
        company_name VARCHAR(255) '$.company_name',
        domain VARCHAR(255) '$.domain'
    )
) AS jsn
DECLARE @bcp_cmd4 VARCHAR(1000);
DECLARE @exe_path4 VARCHAR(200) = 
    ' cd C:\Program Files\Microsoft SQL Server\100\Tools\Binn\ &amp; ';
SET @bcp_cmd4 =  @exe_path4 + 
    ' BCP.EXE ""SELECT ''Company_ID'', ''MatterMark_URL'', ''Company_Name'', ''Domain'' UNION ALL SELECT DISTINCT cast(id as varchar( 12 )) as id, url, company_name, domain FROM ##jsonparsed"" queryout ' +
    ' ""C:\mattermark_etl_project\company_data.txt"" -T -c -q -t0x7c -r\n';
PRINT @bcp_cmd4;
EXEC master..xp_cmdshell @bcp_cmd4,no_output;
SELECT DISTINCT * FROM ##jsonparsed
ORDER BY id ASC;
DROP TABLE ##jsondump 
DROP TABLE ##jsonparsed 
DROP TABLE ##json_loop 
/*
-- To allow advanced options to be changed.  
EXEC sp_configure 'show advanced options', 1;  
GO  
-- To update the currently configured value for advanced options.  
RECONFIGURE;  
GO  
-- To enable the feature.  
EXEC sp_configure 'xp_cmdshell', 1;  
GO  
-- To update the currently configured value for this feature.  
RECONFIGURE;  
GO  
*/
exec xp_cmdshell 'C:\mattermark_etl_project\powershell ""C:\mattermark_etl_project\open_file.ps1""',no_output
",<sql-server><t-sql><open-json>,3092,1,97,1623,4,20,44,57,101322,0,292,4,53,2019-07-30 0:16,2019-08-01 0:10,2019-08-01 0:10,2,2,Basic,11
58866560,flask_sqlalchemy `pool_pre_ping` only working sometimes,"For testing, I amend the MYSQL (RDS) parameters as follows;
wait_timeout = 40 (default was 28800)
max_allowed_packet = 1GB (max - just to be sure issue not caused by small packets)
net_read_timeout = 10
interactive_timeout unchanged
Then tested my app without pool_pre_ping options set (defaults to False), kept the app inactive for 40 seconds, tried to login, and i get
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: Traceback (most recent call last):
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/base.py&quot;, line 1193, in _execute_context
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     context)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/sqlalchemy/engine/default.py&quot;, line 507, in do_execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     cursor.execute(statement, parameters)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 206, in execute
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     res = self._query(query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/cursors.py&quot;, line 312, in _query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     db.query(q)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:   File &quot;/var/www/api_server/venv/lib/python3.6/site-packages/MySQLdb/connections.py&quot;, line 224, in query
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]:     _mysql.connection.query(self, query)
Nov 14 20:05:20 ip-172-31-33-52 gunicorn[16962]: MySQLdb._exceptions.OperationalError: (2013, 'Lost connection to MySQL server during query')
Added the pool_pre_ping like this (Using flask_sqlalchamy version 2.4.1);
import os
from flask import Flask
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(app, options)
        options[&quot;pool_pre_ping&quot;] = True
#        options[&quot;pool_recycle&quot;] = 30
#        options[&quot;pool_timeout&quot;] = 35
db = SQLAlchemy()
class DevConfig():
    SQLALCHEMY_ENGINE_OPTIONS = {'pool_recycle': 280, 'pool_timeout': 100, 'pool_pre_ping': True} # These configs doesn't get applied in engine configs :/
    DEBUG = True
    # SERVER_NAME = '127.0.0.1:5000'
    SQLALCHEMY_DATABASE_URI = os.getenv('SQLALCHEMY_DATABASE_URI_DEV')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
config = dict(
    dev=DevConfig,
)
app = Flask(__name__, instance_relative_config=True)
app.config.from_object(config['dev'])
# INIT DATABASE
db.init_app(app)
with app.app_context():
    db.create_all()
-----------run.py
app.run(host='127.0.0.1', port=5000)
With this, now the webapp manages to get new connection even after MySQL server has closed the previous connection. It always works fine when I access the database right after its closed by server (tried max 50 seconds after)... but when I keep connection inactive for long time (haven't noted, but ~ &gt;10-15 min), again I see same error.
According to the docs, (especially the section Dealing with disconnects), the pool_pre_ping option should handle this kind of scenario at background rite? Or is there any other timeout variable that I need to change in MySQL server?
",<python><mysql><flask><flask-sqlalchemy><connection-pooling>,3480,1,51,2461,1,30,55,62,8866,0,962,2,53,2019-11-14 21:10,2021-07-28 22:32,,622,,Basic,9
55674176,django can't find new sqlite version? (SQLite 3.8.3 or later is required (found 3.7.17)),"I've cloned a django project to a Centos 7 vps and I'm trying to run it now, but I get this error when trying to migrate:
$ python manage.py migrate
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.7.17).
When I checked the version for sqlite, it was 3.7.17, so I downloaded the newest version from sqlite website and replaced it with the old one, and now when I version it, it gives:
$ sqlite3 --version
3.27.2 2019-02-25 16:06:06 bd49a8271d650fa89e446b42e513b595a717b9212c91dd384aab871fc1d0f6d7
Still when I try to migrate the project, I get the exact same message as before which means the newer version is not found. I'm new to linux and would appreciate any help.
",<python><django><sqlite><centos7>,708,0,5,1472,1,18,34,54,109296,0,516,14,52,2019-04-14 10:20,2019-04-20 16:08,2019-04-20 16:08,6,6,Basic,9
56824788,How to connect to windows postgres Database from WSL,"I'm running Postgres 11 service on my Windows computer.
How can I connect to this database from WSL?
When I try su - postgres:
postgres@LAPTOP-NQ52TKOG:~$ psql 
psql: could not connect to server: No such file or directory 
        Is the server running locally and accepting
        connections on Unix domain socket &quot;/var/run/postgresql/.s.PGSQL.5432&quot;
It's trying to connect to a Postgres in WSL. I don't want to run Ubuntu Postgres using:
sudo /etc/init.d/postgresql start
",<postgresql><windows-subsystem-for-linux>,485,0,6,777,2,8,18,46,43606,0,12,6,52,2019-06-30 12:19,2019-11-25 3:44,2019-11-25 3:44,148,148,Basic,9
49389535,Problems with flask and bad request,"I was programming myself a pretty nice api to get some json data from my gameserver to my webspace using json,
but everytime i am sending a request using angular i am getting this:
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request version
(&quot;▒\x9c▒▒{▒'\x12\x99▒▒▒\xadH\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00&quot;)
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9dtTc▒\x93▒4▒M▒▒▒▒▒\x9c▒▒{▒'\x99▒▒▒▒H▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00\x9d\x03\x03▒k,&amp;▒▒ua\x8c\x82\x17\x05▒QwQ$▒0▒▒\x9f▒B1\x98\x19W▒▒▒▒\x00\x00\x14▒+▒/▒,▒0▒\x13▒\x14\x00/\x005\x00')
127.0.0.1 - - [20/Mar/2018 17:07:33] &quot;▒\x9d▒k,&amp;▒▒ua\x8c\x82▒QwQ$▒0▒▒\x9f▒B1\x98W▒▒▒▒▒+▒/▒,▒0▒▒/5&quot;
HTTPStatus.BAD_REQUEST -
127.0.0.1 - - [20/Mar/2018 17:07:33] code 400, message Bad request syntax
('\x16\x03\x01\x00▒\x01\x00\x00▒\x03\x03)▒▒\x1e\xa0▒\t\r\x14g%▒▒\x17▒▒\x80\x8d}▒F▒▒\x08U▒ġ▒▒\x06▒\x00\x00\x1c▒+▒/▒,▒0▒')
g%▒▒▒▒\x80\x8d}▒F▒U▒ġ▒▒▒▒+▒/▒,▒0▒&quot; HTTPStatus.BAD_REQUEST -
My api
from flask import Flask, jsonify
from flaskext.mysql import MySQL
from flask_cors import CORS, cross_origin
app = Flask(__name__)
CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
cors = CORS(app, resources={r""/punishments"": {""origins"": ""http://localhost:5000"" ""*""}})
mysql = MySQL()
# MySQL configurations
app.config['MYSQL_DATABASE_USER'] = 'test'
app.config['MYSQL_DATABASE_PASSWORD'] = 'Biologie1'
app.config['MYSQL_DATABASE_DB'] = 'test'
app.config['MYSQL_DATABASE_HOST'] = 'localhost'
mysql.init_app(app)
@app.route('/punishments', methods=['GET'])
@cross_origin(origin='localhost:5000',headers=['Content- Type','Authorization'])
def get():
    cur = mysql.connect().cursor()
    cur.execute('''select * from test.punishments''')
    r = [dict((cur.description[i][0], value)
              for i, value in enumerate(row)) for row in cur.fetchall()]
    return jsonify({'punishments' : r})
if __name__ == '__main__':
    app.run()
My client function
export class ApiUserService {
  private _postsURL = ""https://localhost:5000/punishments"";
  constructor(private http: HttpClient) {
  }
  getPosts(): Observable&lt;Punishments[]&gt; {
    let headers = new HttpHeaders();
    headers = headers.set('Content-Type', 'application/json; charset=utf-8');
    return this.http
      .get(this._postsURL,{
        headers: {'Content-Type':'application/json; charset=utf-8'}
      })
      .map((response: Response) =&gt; {
        return &lt;Punishments[]&gt;response.json();
      })
      .catch(this.handleError);
  }
  private handleError(error: Response) {
    return Observable.throw(error.statusText);
  }
}
",<flask><angular5><flask-mysql>,2710,2,55,549,1,5,8,76,61697,0,0,6,52,2018-03-20 16:29,2018-08-04 16:02,,137,,Basic,9
61649764,MySQL ERROR 2026 - SSL connection error - Ubuntu 20.04,"I've recently upgraded my local machine OS from Ubuntu 18.04 to 20.04, I'm running my MySQL-server on CentOS (AWS). Post upgrade whenever I'm trying to connect to MySQL server it is throwing SSL connection error. 
$ mysql -u yamcha -h database.yourproject.com -p --port 3309
ERROR 2026 (HY000): SSL connection error: error:1425F102:SSL routines:ssl_choose_client_version:unsupported protocol
But if I pass --ssl-mode=disabled option along with it, I'm able to connect remotely.
$ mysql -u yamcha -h database.yourproject.com -p --port 3309 --ssl-mode=disabled
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 22158946
Server version: 5.7.26 MySQL Community Server (GPL)
Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; 
Queries:  
  How to connect without passing --ssl-mode=disabled  
  How to pass this --ssl-mode=disabled option in my Django application, currently I've defined it as shown below, but I'm still getting the same error.
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'yamcha',
        'USER': 'yamcha',
        'PASSWORD': 'xxxxxxxxxxxxxxx',
        'HOST': 'database.yourproject.com',
        'PORT': '3309',
        'OPTIONS': {'ssl': False},
    }
",<mysql><django><ssl><centos><ubuntu-20.04>,1504,0,31,621,1,5,5,66,83107,0,1,7,52,2020-05-07 4:11,2020-05-21 12:07,,14,,Basic,9
49963923,How do I update MySQL 5.7 to the new MySQL 8.0?,"How do I update to MySQL 8.0 from the default version (5.7)? 
It's important for me for it to make an update and not re-install MySQL so all my data won't be corrupt. 
There is not a lot of info regarding this issue since it was only released not long ago with tons of awesome new features!
This is what I have found that seems like it will only update and not destroy my data. I'm not going to proceed until I'm sure. 
",<mysql><ubuntu><ubuntu-server><mysqlupgrade>,420,2,0,5458,14,80,148,67,92302,0,1300,3,52,2018-04-22 8:20,2018-11-12 11:39,2018-11-12 11:39,204,204,Basic,2
60716482,error: Skipping analyzing 'flask_mysqldb': found module but no type hints or library stubs,"I am using Python 3.6 and flask. I used flask-mysqldb to connect to MySQL, but whenever I try to run mypy on my program I get this error:
Skipping analyzing 'flask_mysqldb': found module but no type hints or
library stubs.
I tried running mypy with the flags ignore-missing-imports or follow-imports=skip. Then I was not getting the error. Why do I get this error?
How can I fix this without adding any additional flags?
",<python><mypy><flask-mysql>,421,0,7,619,1,5,8,53,39785,0,10,2,52,2020-03-17 5:00,2020-03-18 1:26,2020-03-18 1:26,1,1,Basic,3
49794140,"Connection ""default"" was not found with TypeORM","I use TypeORM with NestJS and I am not able to save properly an entity. 
The connection creation works, postgres is running on 5432 port. Credentials are OK too. 
However when I need to save a resource with entity.save() I got :
Connection ""default"" was not found.
Error
    at new ConnectionNotFoundError (/.../ConnectionNotFoundError.ts:11:22)
I checked the source file of TypeORM ConnectionManager (https://github.com/typeorm/typeorm/blob/master/src/connection/ConnectionManager.ts) but it seems that the first time TypeORM creates connection it attributes ""default"" name if we don't provide one, which is the case for me.
I setup TypeORM with TypeOrmModule as 
TypeOrmModule.forRoot({
      type: config.db.type,
      host: config.db.host,
      port: config.db.port,
      username: config.db.user,
      password: config.db.password,
      database: config.db.database,
      entities: [
        __dirname + '/../../dtos/entities/*.entity.js',
      ]
    })
Of course my constants are correct. Any ideas ?
",<node.js><postgresql><typeorm><nestjs>,1014,2,16,764,1,8,17,64,96987,0,86,16,52,2018-04-12 10:25,2018-05-26 12:36,2018-05-26 12:36,44,44,Basic,9
52610485,How to restart PostgreSQL in Ubuntu 18.04,"How to restart PostgreSQL via ssh console?
When i search this thing on SO I only find: postgres, ubuntu how to restart service on startup? get stuck on clustering after instance reboot
",<postgresql>,185,1,0,6847,6,29,50,74,114513,0,1114,4,52,2018-10-02 14:27,2018-10-02 14:27,2018-10-02 14:27,0,0,Basic,10
50497583,when to disconnect and when to end a pg client or pool,"My stack is node, express and the pg module. I really try to understand by the documentation and some outdated tutorials. I dont know when and how to disconnect and to end a client.
For some routes I decided to use a pool. This is my code
const pool = new pg.Pool({
  user: 'pooluser',host: 'localhost',database: 'mydb',password: 'pooluser',port: 5432});
pool.on('error', (err, client) =&gt; {
  console.log('error ', err);  process.exit(-1);
});
app.get('/', (req, res)=&gt;{
  pool.connect()
    .then(client =&gt; {
      return client.query('select ....')
            .then(resolved =&gt; {
              client.release();
              console.log(resolved.rows);
            })
            .catch(e =&gt; { 
              client.release();
              console.log('error', e);
            })
      pool.end();
    })
});
In the routes of the CMS, I use client instead of pool that has different db privileges than the pool.
const client = new pg.Client({
  user: 'clientuser',host: 'localhost',database: 'mydb',password: 'clientuser',port: 5432});    
client.connect();
const signup = (user) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    getUser(user.email)
    .then(getUserRes =&gt; {
      if (!getUserRes) {
        return resolved(false);
      }            
            client.query('insert into user(username, password) values ($1,$2)',[user.username,user.password])
              .then(queryRes =&gt; {
                client.end();
                resolved(true);
              })
              .catch(queryError =&gt; {
                client.end();
                rejeted('username already used');
              });
    })
    .catch(getUserError =&gt; {
      return rejeted('error');
    });
  }) 
};
const getUser = (username) =&gt; {
  return new Promise((resolved, rejeted)=&gt;{
    client.query('select username from user WHERE username= $1',[username])
      .then(res =&gt; {
        client.end();
        if (res.rows.length == 0) {
          return resolved(true);
        }
        resolved(false);
      })
      .catch(e =&gt; {
        client.end();
        console.error('error ', e);
      });
  })
}
In this case if I get a username already used and try to re-post with another username, the query of the getUser never starts and the page hangs. If I remove the client.end(); from both functions, it will work. 
I am confused, so please advice on how and when to disconnect and to completely end a pool or a client. Any hint or explanation or tutorial will be appreciated. 
Thank you
",<node.js><postgresql><node-postgres>,2535,0,68,4312,20,69,130,55,56111,0,468,4,51,2018-05-23 21:15,2018-05-27 14:26,2018-05-31 18:40,4,8,Intermediate,31
50336378,Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER',"I am using MySQL Workbench 8.0. I am trying to dump test data to DB including all the tables, stored procedures and views with data.
When I try to import it's says import finished with one error and the error is 
  Variable 'sql_mode' can't be set to the value of 'NO_AUTO_CREATE_USER'
  Operation failed with exitcode 1
Also after importing if I check the database, only tables have come but there are no stored procedures at all. 
How would one fix this? 
",<mysql><mysql-workbench><data-import>,458,0,0,549,1,5,12,63,100869,0,0,9,51,2018-05-14 17:58,2018-06-06 13:31,,23,,Basic,14
57316744,Docker SQL bind: An attempt was made to access a socket in a way forbidden by its access permissions,"Error-message when creating container in Docker for SQL-server (with Admin-rights):
  ""… Error response from daemon: driver failed programming external
  connectivity on endpoint SQL19b
  (cc372bb961fb8178c2461d26bf16c4232a62e01c5f48b8fcec273370506cc095):
  Error starting userland proxy: listen tcp 0.0.0.0:1433: bind: An
  attempt was made to access a socket in a way forbidden by its access
  permissions.""
excerpts from Log-file:
    [21:39:17.692][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:17.696][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (3.9929ms)\n""
[21:39:17.699][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:17.742][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &gt;&gt; DELETE /v1.40/containers/22810276e261\n""
[21:39:17.758][ApiProxy          ][Info   ] time=""2019-08-01T21:39:17+02:00"" msg=""proxy &lt;&lt; DELETE /v1.40/containers/22810276e261 (16.129ms)\n""
[21:39:17.759][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.866][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; HEAD /_ping\n""
[21:39:27.869][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &lt;&lt; HEAD /_ping (1.6595ms)\n""
[21:39:27.870][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:27.894][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist\n""
[21:39:27.908][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage
[21:39:27.909][APIRequestLogger  ][Info   ] [db460e2b-7d77-4756-be19-665715a9a182] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:27.909][ApiProxy          ][Info   ] time=""2019-08-01T21:39:27+02:00"" msg=""Rewrote mount C:\\Docker\\SQL:/sql (volumeDriver=) to /host_mnt/c/Docker/SQL:/sql""
[21:39:28.049][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/create?name=SQLLinuxLocalPersist (154.5485ms)\n""
[21:39:28.050][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit\n""
[21:39:28.052][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.080][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage
[21:39:28.082][APIRequestLogger  ][Info   ] [a9a496c9-767a-4bd2-917c-f3f1391609dc] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.060][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &gt;&gt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start\n""
[21:39:28.088][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage
[21:39:28.088][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage
[21:39:28.089][APIRequestLogger  ][Info   ] [89bf69bf-5084-4d4b-a887-c7acb99bf131] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.089][APIRequestLogger  ][Info   ] [6ca0e28f-bba3-4f66-afc5-43f6d486c8a2] POST http://unix/usage -&gt; 200 OK took 0ms
[21:39:28.067][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point type:bind""
[21:39:28.068][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""mount point:/host_mnt/c/Docker/SQL""
[21:39:28.205][Moby              ][Info   ] [ 2254.975742] docker0: port 1(veth69918f7) entered blocking state
[21:39:28.250][Moby              ][Info   ] [ 2255.087127] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.295][Moby              ][Info   ] [ 2255.132041] device veth69918f7 entered promiscuous mode
[21:39:28.354][Moby              ][Info   ] [ 2255.176944] IPv6: ADDRCONF(NETDEV_UP): veth69918f7: link is not ready
[21:39:28.439][GoBackendProcess  ][Info   ] Adding tcp forward from 0.0.0.0:1433 to 172.17.0.2:1433
[21:39:28.560][Moby              ][Info   ] [ 2255.385920] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.616][Moby              ][Info   ] [ 2255.442735] device veth69918f7 left promiscuous mode
[21:39:28.667][Moby              ][Info   ] [ 2255.497549] docker0: port 1(veth69918f7) entered disabled state
[21:39:28.826][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/start (767.0192ms)\n""
[21:39:28.829][GoBackendProcess  ][Info   ] error CloseWrite to: The pipe is being closed.
[21:39:28.834][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""Cancel connection...""
[21:39:28.836][ApiProxy          ][Info   ] time=""2019-08-01T21:39:28+02:00"" msg=""proxy &lt;&lt; POST /v1.40/containers/89d13c9d2d2bae095cf66e94b5bb60907a50cb199eb2bdcef9845d493435be07/wait?condition=next-exit (786.0411ms)\n""
This leads to a container created, but without the port allocated. Therefore cannot start the SQL server.
Edit1: The port 1433 doesn't seem to be used (at least it is not listed under ""netstat -abn"" )
",<sql-server><docker>,5308,1,37,1388,3,12,11,77,40448,0,7,6,51,2019-08-01 20:45,2019-08-10 4:07,,9,,Advanced,38
64677836,SQLSTATE[HY000]: General error: 1835 Malformed communication packet on LARAVEL,"Suddenly got
SQLSTATE[HY000]: General error: 1835 Malformed communication packet (SQL: select * from tb_users where (username = 121211) limit 1)
on Laravel.
I already checked this: MySQL: ERROR 2027 (HY000): Malformed packet, but it seems a different case.
I've successfully logged in to MySQL after previously login using SSH (using: mysql -u -p).
I've successfully logged in to MySQL directly from a remote PC (using: mysql -h [IP] -u -p).
But my Laravel got the error I mentioned before. Any experience in this?
",<mysql><laravel><mariadb><mariadb-10.3>,515,1,2,845,1,7,8,79,20380,0,0,16,50,2020-11-04 9:59,2020-11-04 10:28,,0,,Basic,10
48448473,Pyspark convert a standard list to data frame,"The case is really simple, I need to convert a python list into data frame with following code
from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import StringType, IntegerType
schema = StructType([StructField(""value"", IntegerType(), True)])
my_list = [1, 2, 3, 4]
rdd = sc.parallelize(my_list)
df = sqlContext.createDataFrame(rdd, schema)
df.show()
it failed with following error:
    raise TypeError(""StructType can not accept object %r in type %s"" % (obj, type(obj)))
TypeError: StructType can not accept object 1 in type &lt;class 'int'&gt;
",<python><apache-spark><pyspark><apache-spark-sql>,600,0,12,1497,3,17,26,78,146152,0,39,2,50,2018-01-25 17:13,2018-01-25 17:25,2018-01-25 21:21,0,0,Basic,10
57128891,How repair corrupt xampp 'mysql.user' table?,"I used Xampp yesterday to create some simple Web-based utility tool.
Today I wanted to continue working on it but xampp control panel gave me some weir errors.
This is the MySQL Error Log:
2019-07-20 23:47:13 0 [Note] InnoDB: Uses event mutexes
2019-07-20 23:47:13 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-07-20 23:47:13 0 [Note] InnoDB: Number of pools: 1
2019-07-20 23:47:13 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-07-20 23:47:13 0 [Note] InnoDB: Initializing buffer pool, total size = 16M, instances = 1, chunk size = 16M
2019-07-20 23:47:13 0 [Note] InnoDB: Completed initialization of buffer pool
2019-07-20 23:47:13 0 [Note] InnoDB: Starting crash recovery from checkpoint LSN=1819402
2019-07-20 23:47:14 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-07-20 23:47:14 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
2019-07-20 23:47:14 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-07-20 23:47:14 0 [Note] InnoDB: Setting file 'C:\xampp\mysql\data\ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-07-20 23:47:14 0 [Note] InnoDB: File 'C:\xampp\mysql\data\ibtmp1' size is now 12 MB.
2019-07-20 23:47:14 0 [Note] InnoDB: Waiting for purge to start
2019-07-20 23:47:14 0 [Note] InnoDB: 10.3.16 started; log sequence number 1819411; transaction id 257
2019-07-20 23:47:14 0 [Note] InnoDB: Loading buffer pool(s) from C:\xampp\mysql\data\ib_buffer_pool
2019-07-20 23:47:14 0 [Note] InnoDB: Buffer pool(s) load completed at 190720 23:47:14
2019-07-20 23:47:14 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-07-20 23:47:14 0 [Note] Server socket created on IP: '127.0.0.1'.
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Table '.\mysql\user' is marked as crashed and should be repaired
2019-07-20 23:47:14 0 [ERROR] mysqld.exe: Index for table '.\mysql\user' is corrupt; try to repair it
2019-07-20 23:47:14 0 [ERROR] Couldn't repair table: mysql.user
2019-07-20 23:47:14 0 [ERROR] Fatal error: Can't open and lock privilege tables: Index for table 'user' is corrupt; try to repair it
Tried already to repair, but the mySQL Service won't even start, so I'm kinda helpless...
",<mysql><xampp>,2184,0,26,601,1,5,4,69,178564,0,0,25,50,2019-07-20 21:58,2019-07-26 2:05,,6,,Basic,10
54187241,EF Core Connection to Azure SQL with Managed Identity,"I am using EF Core to connect to a Azure SQL Database deployed to Azure App Services. I am using an access token (obtained via the Managed Identities) to connect to Azure SQL database.
Here is how I am doing that:
Startup.cs:
public void ConfigureServices(IServiceCollection services)
{
    //code ignored for simplicity
    services.AddDbContext&lt;MyCustomDBContext&gt;();
    services.AddTransient&lt;IDBAuthTokenService, AzureSqlAuthTokenService&gt;();
}
MyCustomDBContext.cs
public partial class MyCustomDBContext : DbContext
{
    public IConfiguration Configuration { get; }
    public IDBAuthTokenService authTokenService { get; set; }
    public CortexContext(IConfiguration configuration, IDBAuthTokenService tokenService, DbContextOptions&lt;MyCustomDBContext&gt; options)
        : base(options)
    {
        Configuration = configuration;
        authTokenService = tokenService;
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        SqlConnection connection = new SqlConnection();
        connection.ConnectionString = Configuration.GetConnectionString(""defaultConnection"");
        connection.AccessToken = authTokenService.GetToken().Result;
        optionsBuilder.UseSqlServer(connection);
    }
}
AzureSqlAuthTokenService.cs
public class AzureSqlAuthTokenService : IDBAuthTokenService
{
    public async Task&lt;string&gt; GetToken()
    {
        AzureServiceTokenProvider provider = new AzureServiceTokenProvider();
        var token = await provider.GetAccessTokenAsync(""https://database.windows.net/"");
        return token;
    }
}
This works fine and I can get data from the database. But I am not sure if this is the right way to do it.
My questions:
Is this a right way to do it or will it have issues with performance?
Do I need to worry about token expiration? I am not caching the token as of now.
Does EF Core has any better way to handle this?
",<c#><entity-framework-core><azure-active-directory><azure-sql-database><ef-core-2.2>,1924,1,38,973,1,13,28,56,18126,0,11,6,49,2019-01-14 18:32,2019-01-15 2:54,2019-01-15 2:54,1,1,Intermediate,18
54504230,"How to fix ""Error executing DDL ""alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq"" via JDBC Statement""","I'm trying to start a Spring Boot project with a MySQL database, but I have some problem with the database. I try to start my application that, and server is running, but Hibernate doesn’t create Tables, etc.
This is my code:
User Entity
 @Entity
   public class User {
      @Id
      @GeneratedValue(strategy = IDENTITY)
      private Long id;
      private String firstName;
      private String lastName;
      private String email;
      private String password;
      private String description;
      private String profile_photo;
      private LocalDate create;
      private LocalDate update;
      @OneToMany(mappedBy = &quot;eventOwner&quot;)
      private List&lt;Event&gt; ownedEvents;
           public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public String getFirstName() {
    return firstName;
}
public void setFirstName(String firstName) {
    this.firstName = firstName;
}
public String getLastName() {
    return lastName;
}
public void setLastName(String lastName) {
    this.lastName = lastName;
}
public String getEmail() {
    return email;
}
public void setEmail(String email) {
    this.email = email;
}
public String getPassword() {
    return password;
}
public void setPassword(String password) {
    this.password = password;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getProfile_photo() {
    return profile_photo;
}
public void setProfile_photo(String profile_photo) {
    this.profile_photo = profile_photo;
}
public LocalDate getCreate() {
    return create;
}
public void setCreate(LocalDate create) {
    this.create = create;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public List&lt;Event&gt; getOwnedEvents() {
    return ownedEvents;
}
public void setOwnedEvents(List&lt;Event&gt; ownedEvents) {
    this.ownedEvents = ownedEvents;
}}
Event Entity
   @Entity
   @Table(name = &quot;events&quot;)
   public class Event {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private Double longitude;
private Double latitude;
private String description;
private String header;
private LocalDate startData;
private LocalDate endData;
private LocalDate creat;
private LocalDate update;
private Filters filters;
@ManyToOne
@JoinColumn(name = &quot;owner_id&quot;)
private User eventOwner;
public Long getId() {
    return id;
}
public void setId(Long id) {
    this.id = id;
}
public Double getLongitude() {
    return longitude;
}
public void setLongitude(Double longitude) {
    this.longitude = longitude;
}
public Double getLatitude() {
    return latitude;
}
public void setLatitude(Double latitude) {
    this.latitude = latitude;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getHeader() {
    return header;
}
public void setHeader(String header) {
    this.header = header;
}
public LocalDate getStartData() {
    return startData;
}
public void setStartData(LocalDate startData) {
    this.startData = startData;
}
public LocalDate getEndData() {
    return endData;
}
public void setEndData(LocalDate endData) {
    this.endData = endData;
}
public LocalDate getCreat() {
    return creat;
}
public void setCreat(LocalDate creat) {
    this.creat = creat;
}
public LocalDate getUpdate() {
    return update;
}
public void setUpdate(LocalDate update) {
    this.update = update;
}
public Filters getFilters() {
    return filters;
}
public void setFilters(Filters filters) {
    this.filters = filters;
}
public User getEventOwner() {
    return eventOwner;
}
public void setEventOwner(User eventOwner) {
    this.eventOwner = eventOwner;
}
}
And these are my properties:
spring.datasource.url= jdbc:mysql://localhost:3306/some_database?
requireSSL=false&amp;useSSL=false
spring.datasource.username= user
spring.datasource.password= passw
logging.level.org.hibernate.SQL= DEBUG
spring.jpa.hibernate.ddl-auto = create-drop
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL55Dialect
This is the error I get:
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;alter table events drop foreign key FKg0mkvgsqn8584qoql6a2rxheq&quot; via
JDBC Statement
and
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing
DDL &quot;create table events (id bigint not null auto_increment, creat date,
description varchar(255), end_data date, event_type integer, max_age
integer not null, min_age integer not null, open_to_changes bit not null,
pets_allowed bit not null, price_range integer, smoking_allowed bit not
null, header varchar(255), latitude double precision, longitude double
precision, start_data date, update date, owner_id bigint, primary key (id))
engine=InnoDB&quot; via JDBC Statement
at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67) ~[hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:440) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:424) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:315) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:155) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:72) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:310) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:467) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:939) [hibernate-core-5.3.7.Final.jar:5.3.7.Final]
at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.4.RELEASE.jar:5.1.4.RELEASE]
at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) ~[spring-boot-2.1.2.RELEASE.jar:2.1.2.RELEASE]
How can I fix that?
",<java><mysql><hibernate><spring-boot>,10315,0,262,654,1,6,10,50,132840,0,25,22,48,2019-02-03 15:09,2019-02-04 10:16,2019-05-19 10:16,1,105,Basic,13
63361962,ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access,"I'm trying:
mysql&gt; 
LOAD DATA LOCAL INFILE '/var/tmp/countries.csv' 
INTO TABLE countries 
FIELDS TERMINATED BY ',' 
ENCLOSED BY '&quot;' LINES 
TERMINATED BY '\n' 
IGNORE 1 LINES 
(CountryId,CountryCode,CountryDescription,CountryRegion,LastUpdatedDate,created_by,created_on)
SET created_by = 'DH_INITIAL_LOAD', created_on = current_timestamp();
ERROR 2068 (HY000): LOAD DATA LOCAL INFILE file request rejected due to restrictions on access.`
It was working fine, I downloaded pymysql and mysql connector for the python script. I uninstalled and checked still it is not working.
The verion and infile is ON,
 select version() -| 8.0.17
mysql&gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| local_infile  | ON    |
+---------------+-------+
1 row in set (0.00 sec)
",<mysql><load>,846,0,21,1123,1,6,11,37,130807,0,4,10,48,2020-08-11 15:58,2020-08-13 5:40,2020-08-13 5:40,2,2,Basic,14
50846722,What is the difference between Postgres DISTINCT vs DISTINCT ON?,"I have a Postgres table created with the following statement. This table is filled by as dump of data from another service.
CREATE TABLE data_table (
    date date DEFAULT NULL,
    dimension1 varchar(64) DEFAULT NULL,
    dimension2 varchar(128) DEFAULT NULL
) TABLESPACE pg_default;
One of the steps in a ETL I'm building is extracting the unique values of dimension1 and inserting them in another intermediary table.
However, during some tests I found out that the 2 commands below do not return the same results. I would expect for both to return the same sum.
The first command returns more results compared with the second (1466 rows vs. 1504.
-- command 1
SELECT DISTINCT count(dimension1)
FROM data_table;
-- command 2    
SELECT count(*)
FROM (SELECT DISTINCT ON (dimension1) dimension1
FROM data_table
GROUP BY dimension1) AS tmp_table;
Any obvious explanations for this? Alternatively to an explanation, is there any suggestion of any check on the data I should do?
EDIT: The following queries both return 1504 (same as the ""simple"" DISTINCT)
SELECT count(*)
FROM data_table WHERE dimension1 IS NOT NULL;
SELECT count(dimension1)
FROM data_table;
Thank you!
",<sql><postgresql>,1169,0,21,611,1,6,12,64,32580,0,40,5,47,2018-06-13 21:43,2018-06-13 22:11,2018-06-13 22:19,0,0,Basic,8
48793257,Laravel: Check with Observer if Column was Changed on Update,"I am using an Observer to watch if a user was updated.
Whenever a user is updated I would like to check if his email has been changed. 
Is something like this possible?
class UserObserver
{
    /**
     * Listen to the User created event.
     *
     * @param  \App\User  $user
     * @return void
     */
    public function updating(User $user)
    {
      // if($user-&gt;hasChangedEmailInThisUpdate()) ?
    }
}
",<php><mysql><laravel-5>,416,1,18,26612,24,163,253,67,55203,0,2276,4,45,2018-02-14 17:43,2018-02-14 18:19,2018-02-14 18:19,0,0,Basic,9
55863560,"Method ""join"" and class ""DeepCollectionEquality"" aren't defined","Android Studio is giving me 2 errors using sqflite 2.2.0+3:
The method join isn't defined for the class uploadIntoDb.
Undefined class DeepCollectionEquality.
My code :
import 'package:flutter/material.dart';
import 'dart:async';
import 'package:sqflite/sqflite.dart';
class UploadPage extends StatefulWidget {
  @override
  State&lt;StatefulWidget&gt; createState(){
    return new UploadPageState();
  }
}
class UploadPageState extends State&lt;UploadPage&gt;
    with SingleTickerProviderStateMixin {
  @override
  void initState(){
    super.initState();
  }
  @override
  Widget build(BuildContext context) {
    return null;
  }
  Future&lt;void&gt; uploadIntoDb(String valueToUpload) async{
    // Get a location using getDatabasesPath
    var databasesPath = await getDatabasesPath();
    String path = join(databasesPath, 'poa.db');//FIRST PROBLEM
// open the database
    Database database = await openDatabase(path, version: 1,
        onCreate: (Database db, int version) async {
          // When creating the db, create the table
          await db.execute(
              'CREATE TABLE Test (id INTEGER PRIMARY KEY, name TEXT, value INTEGER, num REAL)');
        });
// Insert some records in a transaction
    await database.transaction((txn) async {
      int id1 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(&quot;some name&quot;, 1234, 456.789)');
      print('inserted1: $id1');
      int id2 = await txn.rawInsert(
          'INSERT INTO Test(name, value, num) VALUES(?, ?, ?)',
          ['another name', 12345678, 3.1416]);
      print('inserted2: $id2');
    });
// Update some record
    int count = await database.rawUpdate(
        'UPDATE Test SET name = ?, VALUE = ? WHERE name = ?',
        ['updated name', '9876', 'some name']);
    print('updated: $count');
// Get the records
    List&lt;Map&gt; list = await database.rawQuery('SELECT * FROM Test');
    List&lt;Map&gt; expectedList = [
      {'name': 'updated name', 'id': 1, 'value': 9876, 'num': 456.789},
      {'name': 'another name', 'id': 2, 'value': 12345678, 'num': 3.1416}
    ];
    print(list);
    print(expectedList);
    assert(const DeepCollectionEquality().equals(list, expectedList));//SECOND PROBLEM
// Count the records
    count = Sqflite
        .firstIntValue(await database.rawQuery('SELECT COUNT(*) FROM Test'));
    assert(count == 2);
// Delete a record
    count = await database
        .rawDelete('DELETE FROM Test WHERE name = ?', ['another name']);
    assert(count == 1);
// Close the database
    await database.close();
  }
}
Yes I included dependency sqflite: ^1.1.0.
",<android><sqlite><join><flutter><sqflite>,2617,1,83,3873,5,27,47,69,19933,0,443,1,45,2019-04-26 8:20,2019-04-26 10:30,2019-04-26 10:30,0,0,Basic,9
49811955,Unable to install psycopg2 (pip install psycopg2),"I'm using MAC and python version 2.7.14
Collecting psycopg2
  Could not fetch URL https://pypi.python.org/simple/psycopg2/: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:661) - skipping
  Could not find a version that satisfies the requirement psycopg2 (from versions: )
No matching distribution found for psycopg2
",<python><postgresql><pip><psycopg2>,394,1,4,567,1,4,6,72,118872,0,1,9,45,2018-04-13 7:54,2018-04-13 8:10,,0,,Basic,14
59985030,Syntax error at: OPTIMIZE_FOR_SEQUENTIAL_KEY,"I created a table in Microsoft SQL Server Management Studio and the table worked fine, no errors while building.
Then i was copying the script to my project in visual studio when the following message showed:
  SQL80001: Incorrect syntax ner 'OPTIMIZE_FOR_SEQUENTIAL_KEY'
I don't know why it happened, but this error was showing on this line of the code:
(PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF  )
Do you guys know why the visual studio is showing that error message? How can I fix it?
",<sql-server><visual-studio>,595,0,1,602,1,5,13,65,92805,0,89,3,45,2020-01-30 11:29,2020-01-30 11:48,2020-01-30 11:48,0,0,Basic,14
48190016,SQL correct way of joining if the other parameter is null,"I have this code and its temporary tables so you can run it.
create table #student
(
    id int identity(1,1),
    firstname varchar(50),
    lastname varchar(50)
)
create table #quiz
(
    id int identity(1,1),
    quiz_name varchar(50)
)
create table #quiz_details
(
    id int identity(1,1),
    quiz_id int,
    student_id int
)
insert into #student(firstname, lastname)
values ('LeBron', 'James'), ('Stephen', 'Curry')
insert into #quiz(quiz_name)
values('NBA 50 Greatest Player Quiz'), ('NBA Top 10 3 point shooters')
insert into #quiz_details(quiz_id, student_id)
values (1, 2), (2, 1)
drop table #student
drop table #quiz
drop table #quiz_details
So as you can see lebron james takes the quiz nba top 10 3 point shooters quiz and stephen curry takes the nba 50 greatest player quiz.
All I want is to get the thing that they didn't take yet for example LeBron hasn't taken the 50 greatest player quiz so what I want is like this.
id   quiz_name                    firstname  lastname
----------------------------------------------------
1    NBA 50 Greatest Player Quiz  NULL       NULL 
I want 2 parameters, the id of lebron and the id of the quiz so that I will know that lebron or stephen hasn't taken it yet, but how would I do that if the value of the student_id is still null?
My attempt:
select
    QD.id,
    Q.quiz_name,
    S.firstname,
    S.lastname
from 
    #quiz_details QD
inner join 
    #quiz Q on Q.id = QD.quiz_id
inner join 
    #student S on S.id = QD.student_id
",<sql><sql-server>,1492,0,48,515,0,3,10,81,1916,0,8,5,45,2018-01-10 14:50,2018-01-10 14:56,2018-01-10 14:56,0,0,Basic,10
48279481,Multiple tables with same type of objects in Room database,"I'm using Room as the database for the app. I have a scenario where an Object of a certain type needs to be stored in separate tables. As an example, lets take the Object called Book.java 
Now, I want to have two SQL tables:
Books_Read
Books_To_Read 
ignore any naming conventions for SQL DB please - this is just an example
Problem 
Normally, one would just use @Entity(tableName = ""Books_Read"") in the Book.java class and have a DAO class that will use that table name. 
The thing is; how would I then be able to use the same Book.java class to store in the Books_To_Read table? Since I already defined @Entity(tableName = ""Books_Read"") as part of the Book.java class and I see no where to define the Books_To_Read table for the Book.java class 
The only solution I was able to come up with, which seems a little hackery and unnessasery, was to create a new class - let's call it BookToRead.java  that extends Book.java and define @Entity(tableName = ""Books_To_Read"") in the class. 
Question
Is there a better way to do this or is this the expected way to handle it? 
",<android><sql><database><database-design><android-room>,1070,0,15,7241,9,47,99,45,24992,0,1779,3,45,2018-01-16 10:50,2018-02-19 10:44,,34,,Basic,10
59156537,Unable to load System.Threading.Tasks.Extensions,"I have a web project build on .net framework 4.5.1. We are trying to added PostgreSQL support for the project. Using Nuget, I have installed 4.0.4 npgsql to the project. Under references, I see the following being added to the project.
Npgsql - 4.0.4.0 - Runtime version v4.0.30319
System.Threading.Tasks.Extensions - 4.2.0.0 - Runtime version v4.0.30319
When I tried run the project and connect and get the data from the database, I am getting the following error saying FileNotFoundException:
    System.TypeInitializationException
      HResult=0x80131534
      Message=The type initializer for 'com.rsol.RConfig' threw an exception.
      Source=RConfig
      StackTrace:
       at com.rsol.RConfig.getInstance() in C:\Workspaces\PS\RConfig\RConfig.cs:line 1113
       at RAdmin.Global.Application_Start(Object sender, EventArgs e) in C:\Workspaces\PS\RAdmin\Global.asax.cs:line 528
    Inner Exception 1:
    TypeInitializationException: The type initializer for 'com.rsol.Db.DbMgr' threw an exception.
    Inner Exception 2:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.1, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
    Inner Exception 3:
    FileNotFoundException: Could not load file or assembly 'System.Threading.Tasks.Extensions, Version=4.2.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies. The system cannot find the file specified.
System.Threading.Tasks.Extensions which is installed using Nuget is not getting loaded to the project. When I checked the properties of System.Threading.Tasks.Extensions reference, the dll file exists in the location. I have also tried installing System.Threading.Tasks.Extensions.dll file to assembly using gacutil. I am still getting the same error.
Please let me know if you need any additional information.
Any help is really appreciated.
",<c#><nuget><npgsql>,1963,0,16,1189,2,19,41,72,69195,0,28,10,44,2019-12-03 11:54,2019-12-03 12:12,2019-12-03 12:12,0,0,Basic,14
55457069,"how to fix ""OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly""","Services
My service based on flask + postgresql + gunicorn + supervisor + nginx
When deploying by docker, after running the service, then accessing the api, sometimes it told the error message, and sometimes it workes well.
And the sqlachemy connect database add the parameters 'sslmode:disable'.
File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    Return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    Distilled_params,
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
    Util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 383, in raise_from_cause
    Reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
    Cursor, statement, parameters, context
  File ""/usr/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    Cursor.execute(statement, parameters)
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
    This probably means the server terminated abnormally
    before or while processing the request.
Information
Docker for Mac: version: 2.0.0.3 (31259)
macOS: version 10.14.2
Python: version 2.7.15
Recurrence method
When view port information by command
lsof -i:5432
the port 5432 is postgresql database default port，if the outputconsole was
COMMAND    PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
postgres 86469 user    4u  IPv6 0xxddd      0t0  TCP *:postgresql (LISTEN)
postgres 86469 user    5u  IPv4 0xxddr      0t0  TCP *:postgresql (LISTEN)
it would display the error message:
OperationalError: (psycopg2.OperationalError) server closed the connection unexpectedly
but if the outputconsolelog show this:
COMMAND     PID        USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
com.docke 62421 user   26u  IPv4 0xe93      0t0  TCP 192.168.2.7:6435-&gt;192.168.2.7:postgresql (ESTABLISHED)
postgres  86460 user    4u  IPv6 0xed3      0t0  TCP *:postgresql (LISTEN)
postgres  86460 user    5u  IPv4 0xe513      0t0  TCP *:postgresql (LISTEN)
postgres  86856 user   11u  IPv4 0xfe93      0t0  TCP 192.168.2.7:postgresql-&gt;192.168.2.7:6435 (ESTABLISHED)
the situation, the api would work well.
Becauce of Docker for mac?
Refer link https://github.com/docker/for-mac/issues/2442 , the issue can not solve my problem.
Notice a similar problem?
Refer link Python &amp; Sqlalchemy - Connection pattern -&gt; Disconnected from the remote server randomly
also this issue can not solve my problem.
Solution
flask_sqlachemy need the parameter pool_pre_ping
from flask_sqlalchemy import SQLAlchemy as _BaseSQLAlchemy
class SQLAlchemy(_BaseSQLAlchemy):
    def apply_pool_defaults(self, app, options):
        super(SQLAlchemy, self).apply_pool_defaults(self, app, options)
        options[""pool_pre_ping""] = True
db = SQLAlchemy()
",<postgresql><macos><docker><flask>,3340,4,36,613,1,7,15,38,69282,0,3,4,44,2019-04-01 14:04,2020-03-10 9:39,,344,,Basic,14
59720605,Explain vs explain analyze in PostgreSQL,"I understand that explain in postgresql just estimates the cost of a query and explain analyze does the same and also executes a query and gives the actual results.
But I can't figure out in which cases I should use explain and explain analyze.
",<postgresql>,245,0,0,543,1,4,4,59,12591,0,0,1,44,2020-01-13 16:36,2020-01-13 16:49,2020-01-13 16:49,0,0,Intermediate,23
51916630,MongoDB mongoose Deprecation Warning,"While querying the documents by using collection.find I started getting following warning in my console
  DeprecationWarning: collection.find option [fields] is deprecated and
  will be removed in a later version
Why am I seeing this and how do I fix this? (Possible alternatives)
EDIT: Query Added
Session
        .find({ sessionCode: '18JANMON', completed: false })
        .limit(10)
        .sort({time: 1})
        .select({time: 1, sessionCode: 1});
Mongoose version 5.2.9
",<javascript><node.js><mongodb><mongoose><nosql>,479,0,6,4537,11,46,83,58,42623,0,512,14,44,2018-08-19 10:14,2018-08-19 14:43,2018-08-19 14:43,0,0,Advanced,38
54601529,Efficiently mapping one-to-many many-to-many database to struct in Golang,"Question
When dealing with a one-to-many or many-to-many SQL relationship in Golang, what is the best (efficient, recommended, ""Go-like"") way of mapping the rows to a struct?
Taking the example setup below I have tried to detail some approaches with Pros and Cons of each but was wondering what the community recommends.
Requirements
Works with PostgreSQL (can be generic but not include MySQL/Oracle specific features)
Efficiency - No brute forcing every combination
No ORM - Ideally using only database/sql and jmoiron/sqlx
Example
For sake of clarity I have removed error handling
Models
type Tag struct {
  ID int
  Name string
}
type Item struct {
  ID int
  Tags []Tag
}
Database
CREATE TABLE item (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY
);
CREATE TABLE tag (
  id                      INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  name                    VARCHAR(160),
  item_id                 INT REFERENCES item(id)
);
Approach 1 - Select all Items, then select tags per item
var items []Item
sqlxdb.Select(&amp;items, ""SELECT * FROM item"")
for i, item := range items {
  var tags []Tag
  sqlxdb.Select(&amp;tags, ""SELECT * FROM tag WHERE item_id = $1"", item.ID)
  items[i].Tags = tags
}
Pros
Simple
Easy to understand
Cons
Inefficient with the number of database queries increasing proportional with number of items
Approach 2 - Construct SQL join and loop through rows manually
var itemTags = make(map[int][]Tag)
var items = []Item{}
rows, _ := sqlxdb.Queryx(""SELECT i.id, t.id, t.name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
for rows.Next() {
  var (
    itemID  int
    tagID   int
    tagName string
  )
  rows.Scan(&amp;itemID, &amp;tagID, &amp;tagName)
  if tags, ok := itemTags[itemID]; ok {
    itemTags[itemID] = append(tags, Tag{ID: tagID, Name: tagName,})
  } else {
    itemTags[itemID] = []Tag{Tag{ID: tagID, Name: tagName,}}
  }
}
for itemID, tags := range itemTags {
  items = append(Item{
    ID: itemID,
    Tags: tags,
  })
}
Pros
A single database call and cursor that can be looped through without eating too much memory
Cons
Complicated and harder to develop with multiple joins and many attributes on the struct
Not too performant; more memory usage and processing time vs. more network calls
Failed approach 3 - sqlx struct scanning
Despite failing I want to include this approach as I find it to be my current aim of efficiency paired with development simplicity. My hope was by explicitly setting the db tag on each struct field sqlx could do some advanced struct scanning 
var items []Item
sqlxdb.Select(&amp;items, ""SELECT i.id AS item_id, t.id AS tag_id, t.name AS tag_name FROM item AS i JOIN tag AS t ON t.item_id = i.id"")
Unfortunately this errors out as missing destination name tag_id in *[]Item leading me to believe the StructScan is not advanced enough to recursively loop through rows (no criticism - it is a complicated scenario)
Possible approach 4 - PostgreSQL array aggregators and GROUP BY
While I am sure this will not work I have included this untested option to see if it could be improved upon so it may work.
var items = []Item{}
sqlxdb.Select(&amp;items, ""SELECT i.id as item_id, array_agg(t.*) as tags FROM item AS i JOIN tag AS t ON t.item_id = i.id GROUP BY i.id"")
When I have some time I will try and run some experiments here. 
",<sql><go><struct><sqlx>,3346,0,60,14702,6,48,62,53,18469,0,65,5,44,2019-02-08 23:17,2019-02-09 21:31,2019-02-10 9:26,1,2,Intermediate,22
53975234,Instance of 'SQLAlchemy' has no 'Column' member (no-member),"I'm currently trying to implement steam login into website. But I'm unable to get pass this error within the code. I've created the database object but it keeps showing the error I mentioned earlier. I'm not sure whether SQLAlchemy has changed or what since I used it.
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
app = Flask(__name__)
db = SQLAlchemy(app)
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
The message emitted by pylint is
E1101: Instance of 'SQLAlchemy' has no 'Column' member (no-member)
",<python><python-3.x><flask><sqlalchemy><pylint>,547,0,10,535,1,5,8,75,42567,0,0,6,43,2018-12-30 4:15,2019-02-28 14:45,,60,,Basic,13
51933421,System.Data.SQLite vs Microsoft.Data.Sqlite,"What are the differences between System.Data.SQLite and Microsoft.Data.Sqlite?
I understand that System.Data.SQLite is older and got .NETStandard support after Microsoft.Data.Sqlite, but now both of them support .NETStandard 2.
What are the advantages of one over the other?
",<.net><sqlite><system.data.sqlite>,275,0,0,601,1,5,11,49,21127,0,5,4,43,2018-08-20 14:54,2018-08-26 10:57,2018-08-26 10:57,6,6,Intermediate,19
49984267,java.sql.SQLException: Unknown system variable 'query_cache_size',"I have a app running with JDBC and get data from MySQL, but I can't build it because of this error : 
java.sql.SQLException: Unknown system variable 'query_cache_size'
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2497) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2455) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1369) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.loadServerVariables(ConnectionImpl.java:3777) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:3240) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2249) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2035) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:790) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) ~[mysql-connector-java-5.1.41.jar:5.1.41]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_131]
I have file application.properties here 
#specs.dir=/specs/
#
#################### Spring Boot Data Source Configuration ############
#spring.datasource.driver-class-name=com.mysql.jdbc.Driver
#spring.datasource.url=jdbc:mysql://localhost:3306/savingbooking?useSSL=false
#spring.datasource.username=root
#spring.datasource.password=ZAQ!2wsx
#spring.datasource.initialize=true
#spring.jpa.hibernate.ddl-auto=update
#spring.jpa.properties.hibernate.format_sql=true
#spring.jpa.show-sql=true
#spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
Mysql workbench is 8.0 version
",<java><mysql><spring-boot><jdbc>,2462,0,28,347,1,5,16,62,80235,0,48,11,43,2018-04-23 15:04,2018-04-23 15:10,2018-04-23 15:10,0,0,Basic,13
50409788,MySQL 8 create new user with password not working,"I am using MySQL for several years and the command for the creating the new user till the MySQL 5.x version is as follow:
GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password';
Recently I installed the MySQL 8. In that, this command is not working.
It is throwing following error while firing above command:
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'password'' at line 1
Is there any change of syntax in MySQL 8? What is the correct syntax for creating new user command in MySQL 8?
Note: I tried this syntax in MySQL 5.x versions. It is working correctly in that.
",<mysql><sql><database>,717,0,2,2440,2,29,42,42,53618,0,1886,2,42,2018-05-18 10:56,2018-05-18 15:11,2018-05-18 15:11,0,0,Basic,5
50180667,How can I connect to a database as another user?,"Im currently creating an API for a school project and everything is working good. My setup is: Node v10, Postgres, Koa and so on...
I currently have this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
\c master;
When the init script runs in the docker machine the output I get is this one:
CREATE ROLE
CREATE DATABASE
You are now connected to database ""master"" as user ""postgres"".
So I did change the file to something like this:
CREATE ROLE sa WITH LOGIN PASSWORD 'some-password.';
CREATE DATABASE master WITH OWNER sa;
CONNECT TO master AS main USER sa;
And I get a syntax error:
STATEMENT:  CONNECT TO master AS sa USER sa;
psql:/docker-entrypoint-initdb.d/init.sql:4: ERROR:  syntax error at or near ""CONNECT""
I can't find anywhere in docs (or haven't look very good) how to connect from a .sql file to a database with an specific user.
How would I connect to 'master' with its owner, which is 'sa' from a .sql file?
",<postgresql><psql>,966,0,11,563,1,5,12,68,101774,0,5,4,42,2018-05-04 17:57,2018-05-04 18:45,2018-05-04 18:45,0,0,Basic,9
48956743,Embedded Postgres for Spring Boot Tests,"I'm building a Spring Boot app, backed by Postgres, using Flyway for database migrations. I've been bumping up against issues where I cannot produce a migration that generates the desired outcome in both Postgres, and the embedded unit test database (even with Postgres compatibility mode enabled). So I am looking at using embedded Postgres for unit tests.
I came across an embedded postgres implementation that looks promising, but don't really see how to set it up to run within Spring Boot's unit test framework only (for testing Spring Data repositories). How would one set this up using the mentioned tool or an alternative embedded version of Postgres?
",<spring><postgresql><spring-boot><flyway>,660,1,0,18871,14,73,102,39,75788,0,958,5,42,2018-02-23 21:52,2018-02-23 22:21,2018-02-27 15:11,0,4,Intermediate,30
51082758,How to explode multiple columns of a dataframe in pyspark,"I have a dataframe which consists lists in columns similar to the following. The length of the lists in all columns is not same.
Name  Age  Subjects                  Grades
[Bob] [16] [Maths,Physics,Chemistry] [A,B,C]
I want to explode the dataframe in such a way that i get the following output-
Name Age Subjects Grades
Bob  16   Maths     A
Bob  16  Physics    B
Bob  16  Chemistry  C
How can I achieve this?
",<python><dataframe><apache-spark><pyspark><apache-spark-sql>,412,0,6,698,3,9,25,58,57601,0,19,7,42,2018-06-28 12:19,2018-06-28 12:25,2018-06-28 14:14,0,0,Basic,9
48629799,Postgres image is not creating database,"According to these docs, I can specify the name of the database created by the postgres docker image with the env var POSTGRES_DB. I have set it in my docker-compose file, but it isn't being created.
Here's relevant section from the compose file:
pg:
    image: postgres:10
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: user-auth
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
Here are the logs:
Attaching to userauth_pg_1
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 18:05:54.803 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 18:05:54.806 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 18:05:54.817 UTC [24] LOG:  database system was interrupted; last known up at 2018-02-05 18:03:26 UTC
pg_1   | 2018-02-05 18:05:54.942 UTC [24] LOG:  database system was not properly shut down; automatic recovery in progress
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo starts at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  invalid record length at 0/1633F08: wanted 24, got 0
pg_1   | 2018-02-05 18:05:54.944 UTC [24] LOG:  redo done at 0/1633ED0
pg_1   | 2018-02-05 18:05:54.955 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 18:05:59.140 UTC [31] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:06:15.528 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.120 UTC [33] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:08:46.151 UTC [34] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.138 UTC [35] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:02.926 UTC [36] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.244 UTC [37] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.273 UTC [38] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.602 UTC [39] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:04.910 UTC [40] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.777 UTC [41] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.823 UTC [42] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:05.878 UTC [43] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.663 UTC [44] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:14:06.716 UTC [45] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:16:32.713 UTC [46] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:47:04.603 UTC [47] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 18:51:34.413 UTC [1] LOG:  received smart shutdown request
pg_1   | 2018-02-05 18:51:34.417 UTC [1] LOG:  worker process: logical replication launcher (PID 30) exited with exit code 1
pg_1   | 2018-02-05 18:51:34.419 UTC [25] LOG:  shutting down
pg_1   | 2018-02-05 18:51:34.434 UTC [1] LOG:  database system is shut down
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
pg_1   | 2018-02-05 19:08:42.934 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
pg_1   | 2018-02-05 19:08:42.937 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
pg_1   | 2018-02-05 19:08:42.951 UTC [25] LOG:  database system was shut down at 2018-02-05 18:51:34 UTC
pg_1   | 2018-02-05 19:08:42.956 UTC [1] LOG:  database system is ready to accept connections
pg_1   | 2018-02-05 19:09:04.316 UTC [32] FATAL:  database ""user-auth"" does not exist
pg_1   | 2018-02-05 19:09:18.081 UTC [33] FATAL:  database ""user-auth"" does not exist
",<postgresql><docker>,3825,1,47,2349,2,19,17,36,46297,0,490,6,42,2018-02-05 19:11,2019-01-15 13:50,2019-01-15 13:50,344,344,Basic,14
51121889,Why do we need template0 and template1 in PostgreSQL?,"I'm a beginner in PostgreSQL. I wonder why the \l command in psql shows databases template0 and template1.
I searched the web but unfortunately didn't find the right resources. But I did find that after removing both (template0 &amp; template1) we can't create new databases any more.
",<database><postgresql>,285,0,5,543,1,4,7,45,23198,0,0,1,41,2018-07-01 9:05,2018-07-01 11:45,,0,,Basic,4
51276703,how to store PostgreSQL jsonb using SpringBoot + JPA?,"I'm working on a migration software that will consume unknown data from REST services.
I already think about use MongoDB but I decide to not use it and use PostgreSQL.
After read this I'm trying to implement it in my SpringBoot app using Spring JPA but I don't know to map jsonb in my entity.
Tried this but understood nothing!
Here is where I am:
@Repository
@Transactional
public interface DnitRepository extends JpaRepository&lt;Dnit, Long&gt; {
    @Query(value = ""insert into dnit(id,data) VALUES (:id,:data)"", nativeQuery = true)
    void insertdata( @Param(""id"")Integer id,@Param(""data"") String data );
}
and ...
@RestController
public class TestController {
    @Autowired
    DnitRepository dnitRepository;  
    @RequestMapping(value = ""/dnit"", method = RequestMethod.GET)
    public String testBig() {
        dnitRepository.insertdata(2, someJsonDataAsString );
    }
}
and the table:
CREATE TABLE public.dnit
(
    id integer NOT NULL,
    data jsonb,
    CONSTRAINT dnit_pkey PRIMARY KEY (id)
)
How can I do this?
Note: I don't want/need an Entity to work on. My JSON will always be String but I need jsonb to query the DB
",<postgresql><spring-boot><spring-data-jpa><jsonb>,1137,2,27,1972,4,28,53,64,108223,0,407,5,41,2018-07-11 3:13,2018-07-11 13:28,2018-07-11 13:28,0,0,Basic,10
54885178,What's the difference between utf8_unicode_ci and utf8mb4_0900_ai_ci,"What is the difference between utf8mb4_0900_ai_ci and utf8_unicode_ci database text coding in mysql (especially in terms of performance) ?
Update:
There are similar differences between utf8mb4_unicode_ci and utf8mb4_0900_ai_ci?
",<mysql><unicode>,228,0,2,86767,31,374,349,77,34302,0,7499,1,41,2019-02-26 12:04,2019-03-06 16:16,2019-03-06 16:16,8,8,Intermediate,19
53149484,error: ALTER TYPE ... ADD cannot run inside a transaction block,"I am trying to add new type value to my existing types in PostgreSQL. But I get the following error
  error: ALTER TYPE ... ADD cannot run inside a transaction block
The query I used to add a new value to the type is 
ALTER TYPE public.request_type ADD VALUE ""Check"";
I am actually running above query in migrations file which is created using node-pg-migrate
Here public is my schema.
Any idea why this is failing?
Edit:
The below query executes fine when execute it in pgadmin
ALTER TYPE public.request_type ADD VALUE ""Check"";
But when I run above command through node-pg-migrate migrations it fails and throws above error
",<postgresql><enums><alter>,625,0,3,32938,39,121,164,69,32890,0,1300,7,41,2018-11-05 6:39,2018-11-05 8:18,,0,,Basic,10
52789531,"How do I solve «panic: sql: unknown driver ""postgres"" (forgotten import?)»?","I'm trying to INSERT data into POSTGRES from a .csv (pre-fixed width / tabular ) with GO.
What I've done:
package main
import (
    ""bufio""
    ""database/sql""
    ""encoding/csv""
    ""encoding/json""
    ""fmt""
    ""io""
    ""log""
    ""os""
)
type Consumidor struct {
    CPF string   `json:""CPF""`
    Private  string   `json:""Private""`
    Incompleto  string   `json:""Incompleto""`
    Compras   *Compras `json:""Compras,omitempty""`
}
type Compras struct {
    DataUltimacompra  string `json:""DataUltimacompra""`
    TicketMedio string `json:""TicketMedio""`
    TicketUltimaCompra string `json:""TicketUltimaCompra""`
    LojaMaisFrequente string `json:""LojaMaisFrequente""`
    LojaUltimaCompra string `json:""LojaUltimaCompra""`
}
const (
    host     = ""localhost""
    port     = 5432
    user     = ""postgres""
    password = """"
    dbname   = ""neoway""
)
func main() {
    csvFile, _ := os.Open(""data.csv"")
    reader := csv.NewReader(bufio.NewReader(csvFile))
    var dadosinsert []Consumidor
    for {
        line, error := reader.Read()
        if error == io.EOF {
            break
        } else if error != nil {
            log.Fatal(error)
        }
        dadosinsert = append(dadosinsert, Consumidor{
            CPF: line[0],
            Private:  line[1],
            Incompleto: line[2],
            Compras: &amp;Compras{
                DataUltimacompra:  line[3],
                TicketMedio:  line[4],
                TicketUltimaCompra: line[5],
                LojaMaisFrequente:  line[6],
                LojaUltimaCompra: line[7],
            },
        })
    }
    peopleJson, _ := json.Marshal(dadosinsert)
    fmt.Println(string(peopleJson))
    psqlInfo := fmt.Sprintf(""host=%s port=%d user=%s ""+
        ""password=%s dbname=%s sslmode=disable"",
        host, port, user, password, dbname)
    db, err := sql.Open(""postgres"", psqlInfo)
    if err != nil {
        panic(err)
    }
    defer db.Close()
    sqlStatement := `
INSERT INTO base_teste (CPF,""PRIVATE"",""INCOMPLETO"",""DATA DA ÚLTIMA COMPRA"",""TICKET MÉDIO"",""TICKET DA ÚLTIMA COMPRA"",""LOJA MAIS FREQUÊNTE"",""LOJA DA ÚLTIMA COMPRA"")
)
VALUES ($1, $2, $3, $4, $5, $6, 7$, 8$)
RETURNING id`
    id := 0
    err = db.QueryRow(sqlStatement, 30, ""a"", ""b"", ""c"").Scan(&amp;id)
    if err != nil {
        panic(err)
    }
    fmt.Println(""New record ID is:"", id)
}
when I run, I get this error
  [{""CPF"":""xxxxx"",""Private"":""TRUE"",""Incompleto"":""FALSE"",""Compras"":{""DataUltimacompra"":""12/10/2018"",""TicketMedio"":""200"",""TicketUltimaCompra"":""250"",""LojaMaisFrequente"":""111.111.111-99"",""LojaUltimaCompra"":""111.111.111-88""}}]
  panic: sql: unknown driver ""postgres"" (forgotten import?)
  goroutine 1 [running]: main.main()    C:/Users/Willian/Desktop/NEOWAY
  PROJECT/neoway csv prefixed width importer/main.go:70 +0xbed
  Process finished with exit code 2
",<postgresql><go>,2813,0,85,411,1,4,3,65,50141,0,0,1,41,2018-10-13 4:29,2018-10-13 10:23,,0,,Basic,7
48477861,"Laravel: String data, right truncated: 1406 Data too long for column","I have a table with a column 'hotel'. The project is created in Laravel 5.4, so I used Migrations.
$table-&gt;string('hotel', 50);
This is MYSQL VARCHAR (50). It was working good, because when I was developing I used short hotel names like &quot;HILTON NEW YORK 5&quot;*.
Now the project is on production and customer asked why they can't input long hotel names. I've tested it with such a mock hotel name as &quot;Long long long long long long long long long and very-very-very long hotel name 5 stars&quot;
It gave me an error:
&quot;SQLSTATE[22001]: String data, right truncated: 1406 Data too long for
column 'hotel' at row 1&quot;
I've opened database in my Sequel Pro and changed it
first to VARCHAR (255)
then to TEXT
After each change I tested it with the same &quot;Long long long long long long long long long and very-very-very long hotel name 5 starts&quot; and get the same error (see above).
I've checked the type of column with
SHOW FIELDS FROM table_name
and it gave me
Field | Type
hotel | text
so the type of the field is 'text' indeed  (65 535 characters).
Maybe it's somehow connected with Laravel Migration file (see above) where I set VARCHAR (50) in the beginning? But I can't re-run migration on production, because the table has data now.
Would appreciate any help.
UPDATE:
I discovered that it actually saves that long hotel name in the DB. But user still gets this annoying mistake every time after submitting the form...
",<php><mysql><laravel><text><types>,1449,0,2,1922,3,25,43,64,140378,0,207,12,41,2018-01-27 16:26,2018-01-27 16:29,2018-01-27 16:29,0,0,Basic,9
51119248,Electron app with database,"I'm creating a web app for ticket reservation. The only problem is the database. I don't want to tell my client to install XAMPP or set a database, etc. 
Is there any way to package the app with the database? 
",<javascript><sql><node.js><database><electron>,210,0,0,720,2,7,9,44,39844,0,5,1,41,2018-06-30 23:18,2018-07-01 1:12,2018-07-01 1:12,1,1,Intermediate,20
54149272,Equivalent of ON CONFLICT DO NOTHING for UPDATE postgres,"I want to update rows in my postgres database if the updated version wouldn't violate the primary key constraint. If it would, I want to leave the row as it is.
Assuming the table has primary keys on col1, col2, col3, if I run a query like this:
UPDATE table SET (col1, col2) = ('A', 'B') 
      WHERE col1='D' AND col2='E';
The query will fail and I will get a duplicate key error if there exists two entries:
'A', 'B', 'C'
'D', 'E', 'C'
i.e col3 is the same between an existing row and a row to be updated.
If I was INSERTing rows I would use ON CONFLICT DO NOTHING but I can't find an implementation of this for UPDATE. Does an equivalent exist?
",<sql><postgresql><sql-update><subquery><sql-insert>,649,0,9,909,1,7,20,77,50575,0,130,2,41,2019-01-11 15:15,2019-01-11 16:14,2019-01-11 17:12,0,0,Basic,9
53673763,"Azure Storage Emulator fails to init with ""The database 'AzureStorageEmulatorDb57' does not exist""","I am having an issue with Azure Storage Emulator. I tried to re-initialise the database and got the error below. 
This was after installing Visual Studio 2019 Preview but this may just be a co-incidence. I tried for an hour or so to get it running and then gave up and just  reset my machine with the ""keep my files"" option, re-installed Visual Studio 2017 and the Azure Tools but still see the same problem.  
I know a reset sounds a bit drastic but VS 2019 broke my Azure Functions in VS2017, they would not launch so I wanted a clean install. 
If I manually create the DB with sqllocaldb create (version 13.1.4001.0), the DB gets created fine but the init still fails with the same message.
Any ideas?
  C:\Program Files (x86)\Microsoft SDKs\Azure\Storage
  Emulator>AzureStorageEmulator.exe init
      Windows Azure Storage Emulator 5.7.0.0 command line tool
      Found SQL Instance (localdb)\MSSQLLocalDB.
      Creating database AzureStorageEmulatorDb57 on SQL instance '(localdb)\MSSQLLocalDB'.
      Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid database
  name. To see available databases, use sys.databases..
      One or more initialization actions have failed. Resolve these errors before attempting to run the storage emulator again.
      Error: Cannot create database 'AzureStorageEmulatorDb57' : The database 'AzureStorageEmulatorDb57' does not exist. Supply a valid
  database name. To see available databases, use sys.databases..
",<azure-storage><sql-server-express><azure-storage-emulator>,1526,0,0,12970,7,60,87,54,15885,0,311,17,40,2018-12-07 16:51,2018-12-12 9:18,,5,,Basic,9
50372487,"Android Room database file is empty - .db, .db-shm, .db-wal","Using room in android for database. When I tried to see the data in sqlviewer then no tables found in database file
Myapp.db file is empty.
Data/data/packageName/databases/Myapp.db
",<android><sql><android-room>,181,0,0,1304,2,10,22,58,20934,0,40,5,40,2018-05-16 13:39,2018-05-28 13:56,2018-05-28 13:56,12,12,Basic,9
50014017,Why Presto is faster than Spark SQL,"Why is Presto faster than Spark SQL? 
Besides what is the difference between Presto and Spark SQL in computing architectures and memory management?
",<apache-spark-sql><presto>,148,0,0,1097,2,11,16,48,31727,0,10,3,40,2018-04-25 4:20,2018-04-26 17:41,2018-04-26 19:56,1,1,Intermediate,23
57595926,Could not import package. Warning SQL72012: The object exists in the target,"I exported my Azure database using Tasks > Export Data-tier Application in to a .bacpac file. Recently when I tried to import it into my local database server (Tasks > Import Data-tier Application), I encountered this error:
Could not import package.
Warning SQL72012: The object [MyDatabase_Data] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Warning SQL72012: The object [MyDatabase_Log] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box.
Error SQL72014: .Net SqlClient Data Provider: Msg 12824, Level 16, State 1, Line 5 The sp_configure value 'contained database authentication' must be set to 1 in order to alter a contained database.  You may need to use RECONFIGURE to set the value_in_use.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
Error SQL72014: .Net SqlClient Data Provider: Msg 5069, Level 16, State 1, Line 5 ALTER DATABASE statement failed.
Error SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT 1
           FROM   [master].[dbo].[sysdatabases]
           WHERE  [name] = N'$(DatabaseName)')
    BEGIN
        ALTER DATABASE [$(DatabaseName)]
            SET CONTAINMENT = PARTIAL 
            WITH ROLLBACK IMMEDIATE;
    END
 (Microsoft.SqlServer.Dac)
I followed the advice on other posts and tried to run this on SQL Azure database:
sp_configure 'contained database authentication', 1;  
GO  
RECONFIGURE;  
GO
However, it says 
Could not find stored procedure 'sp_configure'.
I understand the equivalent statement in Azure is: 
https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-database-scoped-configuration-transact-sql?view=sql-server-2017
What is the equivalent statement to ""sp_configure 'contained database authentication', 1;""?
",<sql-server><azure-sql-database>,2244,2,32,1741,1,15,27,40,25251,0,120,4,40,2019-08-21 16:34,2019-08-22 1:20,,1,,Basic,2
48184300,"When I run test cases I get this error: psycopg2.OperationalError: cursor ""_django_curs_140351416325888_23"" does not exist","I'm trying to run test cases, but I get below error.
Run command : python manage.py test
Type 'yes' if you would like to try deleting the test database 'test_project_management_db', or 'no' to cancel: yes
Destroying old test database for alias 'default'...
Traceback (most recent call last):
  File &quot;manage.py&quot;, line 24, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 363, in execute_from_command_line
    utility.execute()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/__init__.py&quot;, line 355, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 29, in run_from_argv
    super(Command, self).run_from_argv(argv)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/base.py&quot;, line 330, in execute
    output = self.handle(*args, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/management/commands/test.py&quot;, line 62, in handle
    failures = test_runner.run_tests(test_labels)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 601, in run_tests
    old_config = self.setup_databases()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/runner.py&quot;, line 546, in setup_databases
    self.parallel, **kwargs
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/test/utils.py&quot;, line 187, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 77, in create_test_db
    self.connection._test_serialized_contents = self.serialize_db_to_string()
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 121, in serialize_db_to_string
    serializers.serialize(&quot;json&quot;, get_objects(), indent=None, stream=out)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/__init__.py&quot;, line 129, in serialize
    s.serialize(queryset, **options)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/core/serializers/base.py&quot;, line 80, in serialize
    for count, obj in enumerate(queryset, start=1):
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/backends/base/creation.py&quot;, line 117, in get_objects
    for obj in queryset.iterator():
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/query.py&quot;, line 53, in __iter__
    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)
  File &quot;/home/rails/Desktop/projects/envs/project_manage_env/local/lib/python2.7/site-packages/django/db/models/sql/compiler.py&quot;, line 880, in execute_sql
    cursor.close()
psycopg2.OperationalError: cursor &quot;_django_curs_140351416325888_23&quot; does not exist
",<python><django><postgresql><django-tests>,3900,0,40,513,1,4,7,40,16924,0,4,4,40,2018-01-10 9:37,2018-03-19 15:18,,68,,Basic,13
50603953,How to add 'created_at' and 'updated_at' columns?,"I need to add 'updated_at' and 'created_at' columns to some already existing table in MySQL database. I've added those colums using MySQL Workbench, but what query should I use to make them work properly? Thanks in advance ;)
",<mysql><sql><database>,226,0,0,471,1,6,12,73,108844,0,19,2,40,2018-05-30 11:50,2018-05-30 11:57,2018-05-30 11:57,0,0,Basic,9
49796452,WampServer - mysqld.exe can't start because MSVCR120.dll is missing,"I've tried to run wampserver on my local side, but mysql server doesn't run. 
when I try to install service, it give me error. I searched the answer all day and found some answers on here and there.
but any solution doesn't work for me. I tried to install warpserver on windows7 home OS vmware
Any help for me?
",<mysql><wordpress><windows-7><vmware><wampserver>,311,0,0,943,1,8,19,53,147725,0,9,9,39,2018-04-12 12:23,2018-04-12 12:27,2018-04-12 15:11,0,0,Basic,14
53735305,How to rename a column name in maria DB,"I am new to SQL, I was trying to change column name in my database's table. I am using 'xampp' with 'maria DB' (OS - Ubuntu 18.04)  
I tried all of the followings:  
ALTER TABLE subject RENAME COLUMN course_number TO course_id;
ALTER TABLE subject CHANGE course_number course_id;
ALTER TABLE subject CHANGE 'course_number' 'course_id';
ALTER TABLE subject  CHANGE COLUMN 'course_number'  course_id varchar(255);
ALTER TABLE subject CHANGE 'course_number' 'course_id' varchar(255);
But the only output I got was:  
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'column course_number to course_id' at line 1  
Could someone please tell me what is the correct answer. I have no idea what to do further.
",<mysql><mariadb><rename><alter>,812,0,5,515,1,4,10,66,47244,0,59,4,39,2018-12-12 2:40,2018-12-16 4:32,2018-12-16 4:32,4,4,Basic,10
51014647,"AWS Postgres DB ""does not exist"" when connecting with PG","I can't seem to connect to my DB instance in AWS. I'm using the pg package and following the examples from the website is not working.
A search for ""aws postgres database does not exist"" really isn't returning anything helpful. Going through the open/closed issues on the PG github isnt helpful either.
Running $nc &lt;RDS endpoint&gt; &lt;port number&gt; returns a success message so it's definitely there. Every value placed in the Client config is copy/pasted from my DB instance.
I'm starting to wonder if the databases have a different name than what it shows in the ""Instances"" section of RDS on AWS?
const client = new Client({
  host     : '&lt;&lt;RDS ENDPOINT&gt;&gt;',
  database : '&lt;&lt;RDS NAME&gt;&gt;', // maybe this isnt the real name?
  user     : '&lt;&lt;username&gt;&gt;',
  password : '&lt;&lt;password&gt;&gt;',
  port     : &lt;&lt;port&gt;&gt;
});
client.connect()
  .then(data =&gt; {
    console.log('connected');
  })
  .catch(err =&gt; {
    console.log(err);
  })
",<postgresql><amazon-web-services><amazon-rds>,996,1,16,1564,1,21,37,44,16680,0,838,7,39,2018-06-24 23:03,2020-01-19 2:19,,574,,Basic,10
48102295,Rename column only if exists,"PostgreSQL does not allow 
ALTER TABLE t RENAME COLUMN IF EXISTS c1 TO c2
...or anything like that.  However, it's very convenient to be able to write scripts which modify DB structure which can be run again without first checking if it has already been run.
How do I write a PostgreSQL function to do exactly this?
",<postgresql><ddl><alter-table>,316,0,1,765,1,6,15,55,42429,0,63,5,39,2018-01-04 19:33,2018-01-06 17:52,2019-11-20 12:13,2,685,Basic,9
48128714,How to make an Inner Join in django?,"I want to show in an Html the name of the city, state, and country of a publication. But they are in different tables.
Here is my models.py
class country(models.Model):
    country_name = models.CharField(max_length=200, null=True)
    country_subdomain = models.CharField(max_length=3, null=True)
    def __str__(self):
        return self.country_name
class countrystate(models.Model):
    state_name = models.CharField(max_length=200, null=True)
    country = models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    importance = models.IntegerField(null=True)
    def __str__(self):
        return self.state_name
class city(models.Model):
    city_name = models.CharField(max_length=200, null=True)
    countrystate = models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.city_name
class publication(models.Model):
    user = ForeignKey(users, on_delete=models.CASCADE, null=False)
    title= models.CharField(max_length=300, null=True)
    country=models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    countrystate=models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    city=models.ForeignKey(city, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.title
Here is my views.py
def publications(request):
    mypublications = publication.objects.filter(user_id=request.session['account_id'])
    dic.update({""plist"": mypublications })
    return render(request, 'blog/mypublications.html', dic)
In a django view, what is the equivalent of the next sql query?
SELECT p.user_id, p.title, c.cuntry_id, c.country_name, s.state_id, s.state_name, y.city_id, y.city_name FROM publication AS p
INNER JOIN country AS c ON c.id = p.country_id
INNER JOIN countrystate AS s ON s.id = p.countrystate_id
INNER JOIN city AS y ON y.id = p.city_id
",<python><mysql><django><orm><inner-join>,1865,0,36,1341,8,34,57,64,95825,0,61,5,39,2018-01-06 15:17,2018-01-06 15:29,2018-01-06 15:29,0,0,Basic,10
60409585,How to upgrade postgresql database from 10 to 12 without losing data for openproject,"My OpenProject management software is installed with default postgresql 10.
Currently the postgresql DB is 12, It is having lot of new features.
I want to upgrade my Postgres DB without losing the data in the DB.
My system is ubuntu 18.04 and hosted  openproject.
I searched the internet and could not find a step by step to upgrade postgresql.
Can you please guide me to install new DB and all data should be in the new DB.
thanks for your help.
",<postgresql><ubuntu-18.04><openproject>,447,0,0,859,3,14,22,45,73377,0,13,4,39,2020-02-26 8:22,2020-04-14 13:11,2020-06-04 15:55,48,99,Basic,14
49776619,sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string,"I'm learning flask web microframework and after initialization of my database I run flask db init I run flask db migrate, to migrate my models classes to the database and i got an error.  I work on Windows 10, the database is MySQL, and extensions install are flask-migrate, flask-sqlalchemy, flask-login.
(env) λ flask db migrate
Traceback (most recent call last):
  File ""c:\python36\Lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python36\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\aka\Dev\dream-team\env\Scripts\flask.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 513, in main
    cli.main(args=args, prog_name=name)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 380, in main
    return AppGroup.main(self, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 257, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\cli.py"", line 90, in migrate
    rev_id, x_arg)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\__init__.py"", line 197, in migrate
    version_path=version_path, rev_id=rev_id)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\command.py"", line 176, in revision
    script_directory.run_env()
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\script\base.py"", line 427, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\pyfiles.py"", line 81, in load_python_file
    module = load_module_py(module_id, path)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\compat.py"", line 83, in load_module_py
    spec.loader.exec_module(module)
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""migrations\env.py"", line 87, in &lt;module&gt;
    run_migrations_online()
  File ""migrations\env.py"", line 70, in run_migrations_online
    poolclass=pool.NullPool)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 465, in engine_from_config
    return create_engine(url, **options)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 424, in create_engine
    return strategy.create(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\strategies.py"", line 50, in create
    u = url.make_url(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 211, in make_url
    return _parse_rfc1738_args(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 270, in _parse_rfc1738_args
    ""Could not parse rfc1738 URL from string '%s'"" % name)
sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string 'mysql/dt_admin:dt2016@localhost/dreamteam_db'
",<python><flask><flask-sqlalchemy><flask-login><flask-migrate>,4154,0,61,473,1,4,8,65,109734,0,2,6,38,2018-04-11 13:36,2018-04-12 15:46,,1,,Basic,14
50166869,Connect to SQL Server in local machine (host) from docker using host.docker.internal,"I'm trying to connect to my SQL Server instance running in my local computer using host.docker.internal (as recommended in https://docs.docker.com/docker-for-windows/networking/#use-cases-and-workarounds)
The host.docker.internal is successfully resolved to an IP, and it's ping-able
And I've opened up the port 1433 in my firewall configuration
Error message
  Connection refused 192.168.65.2:1433
My connection string
  Data Source=host.docker.internal,1433;Initial Catalog=;Persist Security Info=False;User ID=;Password=;MultipleActiveResultSets=True;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;
docker version
Client:
 Version:      18.03.1-ce
 API version:  1.37
 Go version:   go1.9.5
 Git commit:   9ee9f40
 Built:        Thu Apr 26 07:12:48 2018
 OS/Arch:      windows/amd64
 Experimental: false
 Orchestrator: swarm
Server:
 Engine:
  Version:      18.03.1-ce
  API version:  1.37 (minimum version 1.12)
  Go version:   go1.9.5
  Git commit:   9ee9f40
  Built:        Thu Apr 26 07:22:38 2018
  OS/Arch:      linux/amd64
  Experimental: true
Docker for windows version
",<sql-server><docker><docker-networking><docker-for-windows>,1098,3,21,4765,1,18,31,71,27103,0,463,1,38,2018-05-04 3:49,2018-05-04 8:11,2018-05-04 8:11,0,0,Basic,14
51292905,"Flask app with ArcGIS, Arcpy does not run","I have a script that gets a table from MSSQL database and then registers it with ArcGIS. It uses several other arcpy methods as well. I tried to combine it with Flask and developed an HTML interface where you can specify tables. The script runs on console perfectly well, however, when running with Flask on http://127.0.0.1:5000/ , the arcpy functions do not run, then the app throws errors.
I am using my local python directory, so I do not have any problem with importing arcpy on flask. So, I am able to use pymssql functions and create a new table, however when it comes to arcpy function, It throws does not exist error, however, the table exists. I feel like there is something wrong with running arcpy with Flask, but any help would be appreciated.
(2) I tried the same thing in Django but I am having the same problem.
Thanks
forms.py
class createGISLayer(FlaskForm):
    tCreateLayer = SubmitField('Create GIS Layer')
DashboardMain()
   try:
        cursor.execute(QueryCreate)
        print (""Table Created."")
        print(self.dbTablePath)
        descTable = arcpy.Describe(self.dbTablePath)
    except arcpy.ExecuteError:
        print(arcpy.GetMessages())
app.py
if formCreate.tCreateLayer.data and formCreate.validate_on_submit():
    if myLayer is not None:
        try:
            print(""Create GIS Layer"")
            myLayer.dashboardMain()
            flash('GIS Layer created!', 'success')
        except Exception as e:
            print(e.message)
            flash(e.message, 'danger')
index.html
&lt;!-- Create GIS Layer  --&gt;
&lt;div class=""content-section""&gt;
&lt;form name='idCreateGISLayer' action="""" method=""POST""&gt;
&lt;table style=""height: auto; margin-left: auto; margin-right: auto; width: 600px;""&gt;
&lt;tbody&gt;
&lt;tr&gt;
    {{ formCreate.hidden_tag() }}
    &lt;td style=""height: 39px; width: 259px""&gt;
        &lt;h2 style=""text-align: left;""&gt;&lt;font size=""3""&gt;&lt;strong&gt;(2) Create &lt;/strong&gt;&lt;/font&gt;&lt;/h2&gt;
    &lt;/td&gt;
    &lt;td style=""text-align: left; height: 39px;""&gt;
        &lt;div class=""auto-style2""&gt;                                                                
            {{ formCreate.tCreateLayer(class=""btn btn-outline-info"")}}
        &lt;/div&gt;
    &lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
 &lt;/table&gt;
 &lt;/form&gt;
 &lt;/div&gt;
ERROR
Table Created.
F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1
""F:\Projects\Dashboard\Publish.sde\Publish.dbo.A_WebT1"" does not exist
screenshot
",<python><flask><arcpy><pymssql>,2496,3,43,634,0,5,9,62,1695,0,47,3,38,2018-07-11 19:26,2018-07-17 10:26,,6,,Basic,14
59330286,postgres: Index on a timestamp field,"I'm new to postgres and I have a question about the timestamp type.
To set the scene, I have a table like the one below:
CREATE TABLE IF NOT EXISTS tbl_example (
    example_id bigint not null,
    example_name text,
    example_timestamp timestamp,
    primary key (example_id)
);
Now I want to run a query to find me the list of examples based on a specific date, using the timestamp.
For example, the common query that will always be run is:
SELECT example_id, example_name, example_timestamp
 WHERE example_timestamp = date_trunc('datepart', example_timestamp)
 ORDER BY example_timestamp DESC;
However, to speed up the search process I was thinking of adding an index to the example_timestamp field:
CREATE INDEX idx_example_timestamp
          ON tbl_example (example_timestamp);
My question, is how does postgres perform the index on the timestamp - in other words will it  index the timestamp based on the date/ time, or will it go into the seconds/ milliseconds, etc?
Alternatively I was thinking of creating a new column with 'example_date' and indexing on this column instead to simplify things. I wasn't keen on having both a date and a timestamp field as I could get the date from the timestamp field, but for index purposes i thought maybe it might be best to create a separate field.
If anyone has any thoughts on this that would be appreciated?
",<postgresql><jpa-2.0><postgresql-9.4>,1361,0,11,1099,3,20,32,55,59707,0,1,3,38,2019-12-13 21:55,2019-12-13 22:48,,0,,Intermediate,23
58763542,"PG::InvalidParameterValue: ERROR: invalid value for parameter ""client_min_messages"": ""panic""","rake db:create showing error PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error. 
After bundle install tried to run rake db:create commond.
Created database.yml file inside the config folder please find below :
development:
  adapter: postgresql
  encoding: utf8
  database: thor_development1
  username: postgres
  password:
  host: localhost
test:
  adapter: postgresql
  encoding: utf8
  database: thor_test1
  username: postgres
  password:
  host: localhost
PG::InvalidParameterValue: ERROR:  invalid value for parameter ""client_min_messages"": ""panic""
HINT:  Available values: debug5, debug4, debug3, debug2, debug1, log, notice, warning, error.
: SET client_min_messages TO 'panic'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `async_exec'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:128:in `block in execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:373:in `block in log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activesupport-4.1.6/lib/active_support/notifications/instrumenter.rb:20:in `instrument'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/abstract_adapter.rb:367:in `log'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/database_statements.rb:127:in `execute'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql/schema_statements.rb:274:in `client_min_messages='
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:634:in `set_standard_conforming_strings'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:914:in `configure_connection'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:895:in `connect'
/Users/galaxy/.rvm/gems/ruby-2.1.2@folderName/gems/activerecord-4.1.6/lib/active_record/connection_adapters/postgresql_adapter.rb:568:in `initialize'
Trying to install in macOS Catalina
",<ruby-on-rails><ruby><postgresql><activerecord><rubygems>,2577,0,29,377,1,3,6,57,19545,0,0,10,38,2019-11-08 9:12,2019-11-15 20:16,,7,,Basic,14
62584640,Suggested way to run multiple sql statements in python?,"What would be the suggested way to run something like the following in python:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0; DROP TABLE IF EXISTS %s; SET FOREIGN_KEY_CHECKS=1' % (table_name,))
For example, should this be three separate self.cursor.execute(...) statements? Is there a specific method that should be used other than cursor.execute(...) to do something like this, or what is the suggested practice for doing this? Currently the code I have is as follows:
self.cursor.execute('SET FOREIGN_KEY_CHECKS=0;')
self.cursor.execute('DROP TABLE IF EXISTS %s;' % (table_name,))
self.cursor.execute('SET FOREIGN_KEY_CHECKS=1;')
self.cursor.execute('CREATE TABLE %s select * from mytable;' % (table_name,))
As you can see, everything is run separately...so I'm not sure if this is a good idea or not (or rather -- what the best way to do the above is). Perhaps BEGIN...END ?
",<python><mysql><mysql-python>,878,0,8,106276,181,504,871,75,50801,0,3486,9,38,2020-06-25 21:31,2020-06-28 7:37,2020-07-04 19:27,3,9,Basic,5
51614140,How to disable column-statistics in MySQL 8 permanently?,"Since MySQL 8 the column-statistics flag is enabled by default.
So if you try to dump some tables with MySQL Workbench 8.0.12, you get this error message:
  14:50:22 Dumping db (table_name)
  Running: mysqldump.exe --defaults-file=""c:\users\username\appdata\local\temp\tmpvu0mxn.cnf""  --user=db_user --host=db_host --protocol=tcp --port=1337 --default-character-set=utf8 --skip-triggers ""db_name"" ""table_name""
  mysqldump: Couldn't execute 'SELECT COLUMN_NAME,                       JSON_EXTRACT(HISTOGRAM, '$.""number-of-buckets-specified""')                FROM information_schema.COLUMN_STATISTICS                WHERE SCHEMA_NAME = 'db_name' AND TABLE_NAME = 'table_name';': Unknown table 'COLUMN_STATISTICS' in information_schema (1109)
  Operation failed with exitcode 2
  14:50:24 Export of C:\path\to\my\dump has finished with 1 errors
Is there any way in MySQL (Workbench) 8 to disable column-statistics permanently?
Workaround 1
An annoying workaround is doing it by hand via:
mysqldump --column-statistics=0 --host=...
Workaround 2
rename mysqldump
create a shell script (or batch on Windows) 
call the renamed mysqldump with the --column-statistics=0 argument within this script
save it as mysqldump
Workaround 3
download MySQL 5.7
extract mysqldump
use this mysqldump
For example in MySQL Workbench: Edit / Preferences... / Administration / Path to mysqldump Tool
Thanks in advance!
",<mysql><mysql-workbench><mysql-8.0>,1394,0,4,1200,4,12,25,71,51386,0,782,14,37,2018-07-31 13:07,2018-08-01 9:10,,1,,Intermediate,18
63109987,NameError: name '_mysql' is not defined after setting change to mysql,"I have a running Django blog with sqlite3 db at my local machine. What I want is to
convert sqlite3 db to mysql db
change Django settings.py file to serve MySQL db
Before I ran into the first step, I jumped into the second first. I followed this web page (on MacOS). I created databases called djangolocaldb on root user and have those infos in /etc/mysql/my.cnf like this:
# /etc/mysql/my.cnf
[client]
database=djangolocaldb
user=root
password=ROOTPASSWORD
default-character-set=utf8
Of course I created db, but not table within it.
mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| djangolocaldb      |
| employees          |
| information_schema |
| mydatabase         |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
7 rows in set (0.00 sec)
I changed settings.py like this as the web page suggested. Here's how:
# settings.py
...
# Database
# https://docs.djangoproject.com/en/3.0/ref/settings/#databases
DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.mysql',
            #'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
            'OPTIONS' : {
                'read_default_file': '/etc/mysql/my.cnf',
                }
            }
        }
...
Now, when I run python manage.py runserver with my venv activated, I got a brutal traceback like this(I ran python manage.py migrate first, and the traceback looked almost the same anyway):
(.venv) ➜  django-local-blog git:(master) ✗ python manage.py runserver
Watching for file changes with StatReloader
Exception in thread django-main-thread:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 18, in &lt;module&gt;
    from . import _mysql
ImportError: dlopen(/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libmysqlclient.21.dylib
  Referenced from: /Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so
  Reason: image not found
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 926, in _bootstrap_inner
    self.run()
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py&quot;, line 870, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/commands/runserver.py&quot;, line 109, in inner_run
    autoreload.raise_last_exception()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 76, in raise_last_exception
    raise _exception[1]
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 357, in execute
    autoreload.check_errors(django.setup)()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py&quot;, line 53, in wrapper
    fn(*args, **kwargs)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/__init__.py&quot;, line 24, in setup
    apps.populate(settings.INSTALLED_APPS)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/registry.py&quot;, line 114, in populate
    app_config.import_models()
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/config.py&quot;, line 211, in import_models
    self.models_module = import_module(models_module_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/models.py&quot;, line 2, in &lt;module&gt;
    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/base_user.py&quot;, line 47, in &lt;module&gt;
    class AbstractBaseUser(models.Model):
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 121, in __new__
    new_class.add_to_class('_meta', Options(meta, app_label))
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py&quot;, line 325, in add_to_class
    value.contribute_to_class(cls, name)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/options.py&quot;, line 208, in contribute_to_class
    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/__init__.py&quot;, line 28, in __getattr__
    return getattr(connections[DEFAULT_DB_ALIAS], item)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 207, in __getitem__
    backend = load_backend(db['ENGINE'])
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py&quot;, line 111, in load_backend
    return import_module('%s.base' % backend_name)
  File &quot;/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/backends/mysql/base.py&quot;, line 16, in &lt;module&gt;
    import MySQLdb as Database
  File &quot;/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py&quot;, line 24, in &lt;module&gt;
    version_info, _mysql.version_info, _mysql.__file__
NameError: name '_mysql' is not defined
So this NameError: name '_mysql' is not defined is the problem. I installed mysqlclient before, changed settings.py, made db in mysql, but none of the steps made it any helpful yet.
And I noticed that even I changed my settings.py back to sqlite3, my blog spit the same _mysql not defined error. So I ended up reverting my commit and now I'm back to sqlite3 (at least my blog is running with it).
I'm guessing it could be that I didn't convert data first, but I'm not 100% sure of it.
Any suggestion?
",<python><mysql><django><database><sqlite>,7935,2,112,1410,4,13,33,59,85864,0,1911,25,37,2020-07-27 6:44,2020-07-27 12:37,2020-07-27 12:37,0,0,Basic,13
48835309,Django nested transactions - “with transaction.atomic()” -- Seeking Clarification,"In Django nested transactions - “with transaction.atomic()” the question is, given this...
def functionA():
    with transaction.atomic():
        #save something
        functionB()
def functionB():
    with transaction.atomic():
        #save another thing
If functionB fails and rolls back, does functionA roll back too?
Kevin Christopher Henry replies with, ""Yes, if an exception happens in either function they will both be rolled back."" He then quotes the docs, which state:
  atomic blocks can be nested. In this case, when an inner block completes successfully, its effects can still be rolled back if an exception is raised in the outer block at a later point.
This documentation quote doesn't seem to address the original question. The doc is saying that when the INNER BLOCK (which is functionB) completes successfully, its effects can still be rolled back if the OUTER block (which is functionA) raises an exception. But the question refers to the opposite scenario. The question asks, if the INNER block (functionB)  FAILS, is the OUTER block (functionA) rolled back? This doc quote doesn't address that scenario.
However, further down in the doc we see this example...
from django.db import IntegrityError, transaction
@transaction.atomic
def viewfunc(request):
    create_parent()
    try:
        with transaction.atomic():
            generate_relationships()
    except IntegrityError:
        handle_exception()
    add_children()
...followed by this commentary...
  In this example, even if generate_relationships() causes a database error by breaking an integrity constraint, you can execute queries in add_children(), and the changes from create_parent() are still there.
If I'm reading the doc correctly it's saying the call to generate_relationships() (which is analogous to the call to functionB in the original question) can FAIL and the changes made in create_parent() and add_children() will be committed to the database. This seems to contradict Kevin Christopher Henry's answer.
What's puzzling to me is that I see the same question/answer in Django nested Transaction.atomic.
I'm new to both Django and stackoverflow, so I don't have a lot of confidence in my reading of the doc, but it seems to contradict both of these responses. I'm looking for some clarification from someone more experienced. Thanks you so much.
",<sql><django><transactions>,2349,3,33,473,0,4,6,66,6688,0,5,2,37,2018-02-16 21:45,2018-02-16 22:18,2018-02-17 0:00,0,1,Basic,13
50708608,Oracle SQLDeveloper on MacOS won't open after installation of correct Java,"I downloaded the Oracle SQLDeveloper, but when I opened it, it said that it requires a minimum of Java 8 and gave me the website for the download. I went on and downloaded Java 10.0.1, but when I went back on to open SQL, it continued saying it required a minimum of Java 8.
I checked that the Java 10.0.1 had installed correctly, and I'm pretty sure it has. It shows up in System Preferences and when clicked, it opens the Java Control Panel fine.
I'm on a MacOS X El Captain 10.11.6.
",<java><oracle><macos><java-8><oracle-sqldeveloper>,486,0,0,371,1,3,3,72,77033,0,0,17,37,2018-06-05 20:41,2018-06-05 20:54,,0,,Basic,14
64635617,How to set a nullable database field to NULL with typeorm?,"This seems like such a simple question to answer, but finding an answer for this seems impossible.
I am building a password reset feature for a backend application with Express and Typescript. I am using Postgres for the database and Typeorm for data manipulation. I have a User entity with these two columns in my database:
@Column({
    unique: true,
    nullable: true,
})
resetPasswordToken!: string;
@Column({ nullable: true, type: 'timestamp with time zone' })
resetPasswordExpiresAt!: Date;
When a user requests a password reset token the resetPasswordToken and resetPasswordExpiresAt fields get both filled with the desired values. With the token that was sent to the user's e-mail address, the user can reset his/her password. After the user's password is reset, I want to clear these two fields by setting them to null:
user.resetPasswordToken = null;
user.resetPasswordExpiresAt = null;
user.save()
But if I do this Typescript complains about the two lines where I assign the null value:
Type 'null' is not assignable to type 'string'.
and
Type 'null' is not assignable to type 'Date'.
If I change the columns in my entity to accept null like below, the errors disappear:
resetPasswordToken!: string | null;
...
resetPasswordExpiresAt!: Date | null;
But when I start my Express application I get the following error when Typeorm tries  to connect to my database:
Data type &quot;Object&quot; in &quot;User.resetPasswordToken&quot; is not supported by &quot;postgres&quot; database.
How do I set these fields to null?
",<typescript><postgresql><express><typeorm>,1528,0,14,1297,1,10,15,41,67258,0,37,5,37,2020-11-01 18:44,2020-11-02 11:35,2020-11-02 11:35,1,1,Basic,9
49931541,MySQL changing authentication type from standard to caching_sha2_password,"I've setup a new MySQL instance on a computer and every time I add a user it sets the Authentication Type to caching_sha2_password. 
This happens even if I set the Authentication Type to ""Standard"", it then changes it when I save the user. I've also changed the default authentication plug in to ""mysql_native_password"", but it still keeps doing it. 
With it using the caching_sha2_password I can't connect to the database from .net core as I get an error stating:
  MySqlException: Authentication method 'caching_sha2_password' not supported by any of the available plugins
How do I get it to save users with the Standard authentication type?
",<mysql><mysql-workbench>,644,0,4,1135,4,13,22,54,110665,0,17,6,37,2018-04-19 23:07,2018-04-20 12:06,2018-04-20 12:06,1,1,Basic,9
53183023,Android Room Exceptions,"What kinds of exceptions I should consider while working with Android Room.
From my research I found out that there is only one exception that might occur.
Room Exceptions
That is also when you are having Single&lt;T&gt; as a return type and you have an empty return. Other than that I couldn't find any other possible scenario that might throw an exception.
Of course, there might be some exceptions if you have some logical incorrect implementations, like
Editing scheme, but not implementing Migration
Not implementing OnConflictStrategy while inserting
Running Room on Main Thread while not allowing it with allowMainThreadQueries()
I did some research and tried out almost all possible cases, mostly with RxJava return types and I saw one exception mentioned above and that's it.
  Here is my tests that I run
I wanted to make sure that I have implementation for every possible scenario and not have some exception and unexpected crashes. I was thinking of occurrences of SQLite exceptions might happen, but I believe it's wrapped around Room and it will handle. (Not sure)
Can you give any other possible exceptions that might occur?
",<android><sqlite><rx-java><reactive-programming><android-room>,1140,2,5,6522,3,36,66,48,3904,0,164,1,37,2018-11-07 2:58,2023-10-05 13:20,,1793,,Basic,9
50987119,Backup Room database,"I'm trying to backup a room database programmatically.
For that, I'm simply copying the .sqlite file that contains the whole database
But, before copying, due to the fact that room has write ahead logging enabled, we must close the database so that -shm file and -wal file merge into a single .sqlite file. As pointed out here
I run .close() on RoomDatabase object: 
Everything works fine with the backup, BUT, later on, when I try to execute an INSERT query, I get this error: 
android.database.sqlite.SQLiteException: no such table: room_table_modification_log (code 1)
How can I properly re-open room db after I close it?
PS: .isOpen() on RoomDatabase object returns true before INSERT
Room version: 1.1.1-rc1
",<android><sqlite><android-room><sqliteopenhelper><android-architecture-components>,713,2,13,1022,1,12,24,50,16982,0,224,7,37,2018-06-22 11:39,2018-07-27 14:30,2018-07-27 14:30,35,35,Basic,9
52320576,In MySQL SERVER 8.0 the PASSWORD function not working,"Error while executing the PASSWORD function in MySQL Server version 8.0.12
I have the following query:
SELECT * 
FROM users 
WHERE login = 'FABIO' 
  AND pwd = PASSWORD('2018') 
LIMIT 0, 50000
I am getting this error:
  Error Code: 1064. You have an error in your SQL syntax; check the
  manual that corresponds to your MySQL server version for the right
  syntax to use near
",<mysql><mysql-8.0>,376,0,6,499,1,4,11,36,41276,0,19,4,37,2018-09-13 19:30,2018-09-13 19:38,2018-09-13 19:38,0,0,Basic,2
49963383,"""Authentication plugin 'caching_sha2_password'","I'm new to MySql environment and installed :
MySQL with the following commands:
sudo apt-get update
sudo apt-get install mysql-server
mysql_secure_installation
and also installed mysql workbench.
But when I'm trying to connect my localhost getting the follow error:
&quot;Authentication plugin 'caching_sha2_password' cannot be loaded: /usr/lib/mysql/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory&quot;
and even this is the first time I'm posting a question in stackoverflow, sorry for my presentation errors and syntax.
",<mysql>,566,1,3,373,1,3,8,49,64061,0,1,5,37,2018-04-22 7:00,2018-04-25 17:11,,3,,Advanced,38
57872910,The LINQ expression could not be translated and will be evaluated locally,"Im getting this WARNING in EntityFramework Core what is wrong?
I already set MSSQL Datebase to Case Sensitive.
Latin1_General_100_CS_AS
var test = await _context.Students
                .FirstOrDefaultAsync(m =&gt; m.LastName.Equals(""ALEXANDER"", StringComparison.InvariantCultureIgnoreCase));
  Microsoft.EntityFrameworkCore.Query:Warning: The LINQ expression
  'where [m].LastName.Equals(""ALEXANDER"", InvariantCultureIgnoreCase)'
  could not be translated and will be evaluated locally.
",<c#><sql-server><entity-framework><linq><entity-framework-core>,489,0,2,371,1,3,6,81,70644,0,0,4,37,2019-09-10 14:24,2019-09-10 15:24,,0,,Advanced,35
49922023,MYSQL 8.0 - unsupported redo log format,"I have recently updated mysql that was located under my xampp folder, and i've got the following errors, reporting from the log file :
2018-04-19T12:59:19.667059Z 0 [System] [MY-010116] [Server] C:\xampp\mysql\bin\mysqld.exe (mysqld 8.0.11) starting as process 9324
2018-04-19T12:59:20.025280Z 1 [ERROR] [MY-013090] [InnoDB] InnoDB: Unsupported redo log format (0). The redo log was created before MySQL 5.7.9
2018-04-19T12:59:20.026140Z 1 [ERROR] [MY-012930] [InnoDB] InnoDB: Plugin initialization aborted with error Generic error.
2018-04-19T12:59:20.229069Z 1 [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.
2018-04-19T12:59:20.230803Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
2018-04-19T12:59:20.231371Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-04-19T12:59:20.233136Z 0 [System] [MY-010910] [Server] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
I have been told if updating your mysql, you should comment out the deprecated configs that are located in your my.ini, but i had no idea what to comment out, so i left it as it is.
Any idea what causes this?
",<php><mysql><phpmyadmin><xampp>,1163,0,0,445,1,5,10,66,52141,0,14,4,36,2018-04-19 13:12,2018-05-17 21:52,2018-05-17 21:52,28,28,Advanced,38
50838199,Pyspark: Select all columns except particular columns,"I have a large number of columns in a PySpark dataframe, say 200. I want to select all the columns except say 3-4 of the columns. How do I select this columns without having to manually type the names of all the columns I want to select? 
",<python><sql><dataframe><pyspark>,239,0,0,7521,6,36,49,75,65342,0,1311,4,36,2018-06-13 13:13,2018-09-04 7:05,2018-09-04 7:05,83,83,Basic,2
61749304,Connection between DBeaver & MySQL,"I use DBeaver to watch an SQL database on MySQL 8+.
Everything is working, but sometimes, opening DBeaver, I have the following error message :
Public Key Retrieval is not allowed
And then, DBeaver can't connect to MySQL.
In order to fix this problem, I have to reconfigure MySQL.
Is there any simplest way to fix this problem ?
",<mysql><dbeaver>,329,0,7,1086,3,14,32,41,56032,0,313,4,36,2020-05-12 10:24,2020-05-16 6:37,2020-05-16 6:37,4,4,Basic,14
49252670,Iterate rows and columns in Spark dataframe,"I have the following Spark dataframe that is created dynamically:
val sf1 = StructField(""name"", StringType, nullable = true)
val sf2 = StructField(""sector"", StringType, nullable = true)
val sf3 = StructField(""age"", IntegerType, nullable = true)
val fields = List(sf1,sf2,sf3)
val schema = StructType(fields)
val row1 = Row(""Andy"",""aaa"",20)
val row2 = Row(""Berta"",""bbb"",30)
val row3 = Row(""Joe"",""ccc"",40)
val data = Seq(row1,row2,row3)
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.createOrReplaceTempView(""people"")
val sqlDF = spark.sql(""SELECT * FROM people"")
Now, I need to iterate each row and column in sqlDF to print each column, this is my attempt:
sqlDF.foreach { row =&gt;
  row.foreach { col =&gt; println(col) }
}
row is type Row, but is not iterable that's why this code throws a compilation error in row.foreach. How to iterate each column in Row?
",<scala><apache-spark><apache-spark-sql>,894,0,25,1329,23,137,340,44,179916,0,4482,9,36,2018-03-13 9:35,2018-03-13 9:41,2018-09-02 1:37,0,173,Basic,10
50936251,"ERROR: function dblink(unknown, unknown) does not exist","I have defined a foreign server pointing to another database. I then want to execute a function in that database and get back the results.
When I try this:
SELECT * FROM  dblink('mylink','select someschema.somefunction(''test'', ''ABC'')')
or this:
SELECT t.n FROM  dblink('mylink', 'select * from someschema.mytable') as t(n text)
I get the error:
  ERROR: function dblink(unknown, unknown) does not exist
Running as superuser.
",<postgresql><dblink>,429,0,2,521,2,6,9,55,36166,,0,3,35,2018-06-19 20:13,2019-08-05 12:08,,412,,Basic,10
53610385,Docker - Postgres and pgAdmin 4 : Connection refused,"Newbie with docker, I am trying to connect throught localhost my pgAdmin container to the postgres one.
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
0b00555238ba        dpage/pgadmin4      ""/entrypoint.sh""         43 minutes ago      Up 43 minutes       0.0.0.0:80-&gt;80/tcp, 443/tcp   pedantic_turing
e79fb6440a95        postgres            ""docker-entrypoint.s…""   About an hour ago   Up About an hour    0.0.0.0:5432-&gt;5432/tcp        pg-docker
I succeed connecting with psql command.
psql -h localhost -U postgres -d postgres
But when I create the server on pgAdmin with the same parameters as psql I got the following error.
  Unable to connect to server:
  could not connect to server: Connection refused Is the server running
  on host ""localhost"" (127.0.0.1) and accepting TCP/IP connections on
  port 5432? could not connect to server: Address not available Is the
  server running on host ""localhost"" (::1) and accepting TCP/IP
  connections on port 5432?
I succeed to connect throught the IPAddress given by docker inspect on the container.
By the way, I checked postgresql.conf and assert that listen_addresses = '*' and also that pg_hba.conf contain host all all all md5.
But I don't get it, why shouldn't I be able to use the localhost address ? And why does docker even give me an address that is not local ?
",<postgresql><docker>,1423,0,7,720,1,9,23,74,45493,0,26,8,35,2018-12-04 10:05,2019-05-08 7:59,2019-05-28 3:11,155,175,Basic,9
53434849,Cannot install postgres on Ubuntu (E: Unable to locate package postgresql),"So I'm having this problem where for some reason I can't install any package on my ubuntu system.
I'm currently on Ubuntu 16.10.
terminal install logs
Update:
I've done entered those commands and got this.
after update and apt-cache
What should I do now?
",<postgresql><ubuntu><ubuntu-16.10>,255,2,1,371,1,4,7,35,99536,0,6,5,35,2018-11-22 16:15,2018-11-22 16:26,,0,,Intermediate,18
49361088,String_agg for SQL Server before 2017,"Can anyone help me make this query work for SQL Server 2014?
This is working on Postgresql and probably on SQL Server 2017. On Oracle it is listagg instead of string_agg.
Here is the SQL:
select 
    string_agg(t.id,',') AS id
from 
    Table t
I checked on the site some xml option should be used but I could not understand it.
",<sql><sql-server><string-aggregation><string-agg>,329,0,6,1648,4,21,38,36,43394,0,197,2,35,2018-03-19 10:47,2018-03-19 10:49,2018-03-19 10:49,0,0,Intermediate,18
62697071,"Docker-Compose postgres upgrade initdb: error: directory ""/var/lib/postgresql/data"" exists but is not empty","I had postgres 11 installed using docker-compose. I wanted to upgrade it to 12 but even though I have removed the container and its volume but the status of the container says &quot;Restarting&quot;.
Here is my docker-compose file
version: '3.5'
services:
  postgres:
    image: postgres:12
    environment:
      POSTGRES_HOST_AUTH_METHOD: &quot;trust&quot;
    ports:
    - &quot;5432&quot;
    restart: always
    volumes:
    - /etc/postgresql/12/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
    - db_data:/var/lib/postgresql/data
volumes:
  db_data:
However it is not working and the logs has the following issue
2020-07-02T12:54:47.012973448Z The files belonging to this database system will be owned by user &quot;postgres&quot;.
2020-07-02T12:54:47.013030445Z This user must also own the server process.
2020-07-02T12:54:47.013068962Z 
2020-07-02T12:54:47.013222608Z The database cluster will be initialized with locale &quot;en_US.utf8&quot;.
2020-07-02T12:54:47.013261425Z The default database encoding has accordingly been set to &quot;UTF8&quot;.
2020-07-02T12:54:47.013281815Z The default text search configuration will be set to &quot;english&quot;.
2020-07-02T12:54:47.013293326Z 
2020-07-02T12:54:47.013303793Z Data page checksums are disabled.
2020-07-02T12:54:47.013313919Z 
2020-07-02T12:54:47.013450079Z initdb: error: directory &quot;/var/lib/postgresql/data&quot; exists but is not empty
2020-07-02T12:54:47.013487706Z If you want to create a new database system, either remove or empty
2020-07-02T12:54:47.013501126Z the directory &quot;/var/lib/postgresql/data&quot; or run initdb
2020-07-02T12:54:47.013512379Z with an argument other than &quot;/var/lib/postgresql/data&quot;.
How could I remove or empty this /var/lib/postgresql/data when the container is constantly restarting?
Thanks in advance
",<postgresql><docker><docker-compose>,1835,0,27,495,2,6,16,49,60805,0,10,6,34,2020-07-02 13:06,2020-09-10 7:21,,70,,Intermediate,19
54472165,What is wrong with this code. Why can't I use SqlConnection?,"I am 100% newbie to SQl and wanted to make a ConsoleApp with the use of database. I read some about it and tried. When I needed to make SqlConnection, my VS 2019 Preview showed me this 
  Severity   Code    Description Project File    Line    Suppression State
  Error
      CS1069  The type name 'SqlConnection' could not be found in the namespace 'System.Data.SqlClient'.
   This type has been forwarded to assembly 'System.Data.SqlClient, Version=0.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' Consider adding a reference to that assembly.
      ConsoleApp1 C:\Users\User\Desktop\Bald Code\ConsoleApp1\ConsoleApp1\Program.cs  12
      Active
i don't get why it doesn't work
Here's my code
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Program
    {
        static void Main(string[] args)
        {
            string connectionString;
            SqlConnection cnn;
        }
    }
}
",<c#><sql><sqlclient>,950,0,19,479,1,4,6,63,60516,0,2,11,34,2019-02-01 2:44,2019-02-01 2:50,,0,,Basic,2
50770862,The certificate chain was issued by an authority that is not trusted,"I studied the help here on upgrading to aspnetcore 2.1.0
My database is SQLExpress 2016SP1
I am able to add a migration but when I issue 
update-database
at the Package Manager Console, I get an error
  A connection was successfully established with the server, but then an error occurred during the login process.
  (provider: SSL Provider, error: 0 - The certificate chain was issued by an authority that is not trusted.)
The connection string is of the form 
   Server=""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true
The DbContext is
public class ApiDbContext : IdentityDbContext&lt;ApplicationUser&gt;
{
    public ApiDbContext(DbContextOptions&lt;ApiDbContext&gt; options)
        : base(options)
    {
    }
}
The Context Factory is
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var builder = new ConfigurationBuilder();
        builder.AddJsonFile(""appsettings.json"");
        var config = builder.Build();
        var connectionString = config.GetConnectionString(""MyDatabase"");
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
[Update]
If I hard code the connection string inside the implementation of  IDesignTimeDbContextFactory  then I can run the migrations
public class MyContextFactory : IDesignTimeDbContextFactory&lt;ApiDbContext&gt;
{
    public ApiDbContext CreateDbContext(string[] args)
    {
        var optionsBuilder = new DbContextOptionsBuilder&lt;ApiDbContext&gt;();
        var connectionString =    ""Server=myserver;Initial Catalog=mydatabase;Trusted_Connection=True;MultipleActiveResultSets=true"";
        optionsBuilder.UseSqlServer(connectionString);
        return new ApiDbContext(optionsBuilder.Options);
    }
}
Hard coding the connection string is not desirable so I would like a better answer.
I am unclear as to why the implementation of  IDesignTimeDbContextFactory is needed.  (my migration does fail without it )
I have updated my Startup.cs and Progam.cs to match those of a newly generated program where the implementation is not needed.
 public class Program
 {
   public static void Main(string[] args)
    {
        CreateWebHostBuilder(args).Build().Run();
    }
    public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
            .UseStartup&lt;Startup&gt;();
}
public class Startup
{
    public Startup(IConfiguration configuration)
    {
        Configuration = configuration;
    }
    public IConfiguration Configuration { get; }
    // This method gets called by the runtime. Use this method to add services to the container.
    public void ConfigureServices(IServiceCollection services)
    {
        services.Configure&lt;CookiePolicyOptions&gt;(options =&gt;
        {
            options.CheckConsentNeeded = context =&gt; true;
            options.MinimumSameSitePolicy = SameSiteMode.None;
        });
        services.AddDbContext&lt;ApiDbContext&gt;(options =&gt;
            options.UseSqlServer(
                Configuration.GetConnectionString(""MyDatabase"")));
        services.AddDefaultIdentity&lt;IdentityUser&gt;()
            .AddEntityFrameworkStores&lt;ApiDbContext&gt;();
services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
    // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
    public void Configure(IApplicationBuilder app, IHostingEnvironment env)
    {
        if (env.IsDevelopment())
        {
            app.UseDeveloperExceptionPage();
            app.UseDatabaseErrorPage();
        }
        else
        {
            app.UseExceptionHandler(""/Error"");
            app.UseHsts();
        }
        app.UseHttpsRedirection();
        app.UseStaticFiles();
        app.UseCookiePolicy();
        app.UseAuthentication();
        app.UseMvc();
    }
}
[Update]
I updated to VS2017 version 15.7.3.
When I went to repeat the problem and create the first migration, PM displayed the message
The configuration file 'appsettings.json' was not found and is not optional
When I marked this file copy always then both the add-migration and the update-database worked.
",<sql-server><asp.net-core><asp.net-identity><entity-framework-core>,4400,1,91,16220,42,184,322,41,48600,,1471,4,34,2018-06-09 4:15,2018-06-11 16:35,2018-06-11 16:35,2,2,Basic,14
48550653,JOIN same table twice with aliases on SQLAlchemy,"I am trying to port the following query to SQLAlchemy:
SELECT u.username, GROUP_CONCAT(DISTINCT userS.name)
FROM Skills AS filterS 
INNER JOIN UserSkills AS ufs ON filterS.id = ufs.skill_id
INNER JOIN Users AS u ON ufs.user_id = u.id
INNER JOIN UserSkills AS us ON u.id = us.user_id
INNER JOIN Skills AS userS ON us.skill_id = userS.id
WHERE filterS.name IN ('C#', 'SQL')
GROUP BY u.id;
I don't understand how to achieve AS statement in SQLAlchemy. Here is what I currently have:
# User class has attribute skills, that points to class UserSkill
# UserSkill class has attribute skill, that points to class Skill
db.session.query(User.id, User.username, func.group_concat(Skill.name).label('skills')).\
   join(User.skills).\
   join(UserSkill.skill).filter(Skill.id.in_(skillIds)).\
   order_by(desc(func.count(Skill.id))).\
   group_by(User.id).all()
Please help.
",<python><sqlalchemy>,865,0,15,2465,2,22,34,40,36687,0,636,2,34,2018-01-31 20:10,2018-02-01 16:18,2018-02-01 16:18,1,1,Basic,2
48815341,Why is Apache-Spark - Python so slow locally as compared to pandas?,"A Spark newbie here.
I recently started playing around with Spark on my local machine on two cores by using the command:
pyspark --master local[2]
I have a 393Mb text file which has almost a million rows. I wanted to perform some data manipulation operation. I am using the built-in dataframe functions of PySpark to perform simple operations like groupBy, sum, max, stddev.
However, when I do the exact same operations in pandas on the exact same dataset, pandas seems to defeat pyspark by a huge margin in terms of latency.
I was wondering what could be a possible reason for this. I have a couple of thoughts.
Do built-in functions do the process of serialization/de-serialization inefficiently? If yes, what are the alternatives to them?
Is the data set too small that it cannot outrun the overhead cost of the underlying JVM on which spark runs?
Thanks for looking. Much appreciated.
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,889,0,5,447,0,6,11,81,9139,0,3,1,34,2018-02-15 20:01,2018-02-15 20:26,2018-02-15 20:26,0,0,Intermediate,23
49518683,The server time zone value 'CEST' is unrecognized,"I am using hibernate (Hibernate Maven 5.2.15.Final, Mysql-connector Maven 8.0.9-rc) whith mysql 5.7 on lampp environment on linux so. 
I am in Italy (Central European Summer Time) and once March 25, occurs follow error on connection db: 
  The server time zone value 'CEST' is unrecognized or represents more
  than one time zone. You must configure either the server or JDBC
  driver (via the serverTimezone configuration property) to use a more
  specifc time zone value if you want to utilize time zone support.
On mysql console, I ran: 
SHOW GLOBAL VARIABLES LIKE 'time_zone';
SET GLOBAL time_zone='Europe/Rome'; 
but that did not persist. 
Then I added to my.cnf file (in /etc/mysql):
[mysqld] 
default-time-zone = 'Europe/Rome' 
and also:
default_time_zone = 'Europe/Rome' 
but the db server did not start still... 
Why does the error occur? 
Could someone help me? 
Thank you!
",<mysql><timezone><lamp>,884,0,6,489,1,5,9,61,51370,0,1,4,33,2018-03-27 17:06,2018-05-06 15:40,,40,,Basic,14
55958103,Tests using embedded postgres fail with Illegal State Exception,"I'm running some tests against an embedded postgres database using otj-pg-embedded. While the tests run fine locally they fail when run by Gitlab-CI with an Illegal State Exception. 
Gitlab CI builds it and runs tests that don't include otj-pg-embedded just fine.
I've commented out most of the test class and pinpointed the problem to:  
public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
import com.goldfinger.models.AuditLog;
import com.opentable.db.postgres.embedded.FlywayPreparer;
import com.opentable.db.postgres.junit.EmbeddedPostgresRules;
import com.opentable.db.postgres.junit.PreparedDbRule;
import org.junit.*;
import org.junit.runner.RunWith;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;
@RunWith(SpringJUnit4ClassRunner.class)
public class SQLAuditRepositoryTest {
    private static SQLAuditRepository sqlAuditRepository;
    private static AuditLog auditLog_1;
    private static AuditLog auditLog_2;
    private static AuditLog auditLog_3;
    private static List&lt;AuditLog&gt; auditLogList;
    @ClassRule
        public static SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance();
    @Test
    public void simpleTest() {
        assert (2 == 2);
    }
}
This is the stack trace:
java.lang.IllegalStateException: Process [/tmp/embedded-pg/PG-06e3a92a2edb6ddd6dbdf5602d0252ca/bin/initdb, -A, trust, -U, postgres, -D, /tmp/epg6584640257265165384, -E, UTF-8] failed
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.system(EmbeddedPostgres.java:626)
    at com.opentable.db.postgres.embedded.EmbeddedPostgres.initdb(EmbeddedPostgres.java:240)
...
... many lines here
...
    at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
    at java.lang.Thread.run(Thread.java:745)
This is the gitlab-ci.yml
image: java:latest
services:
  - postgres:latest
before_script:
  - export GRADLE_USER_HOME=`pwd`/.gradle
package:
  stage: build
  script:
    - ./gradlew assemble
test:
  stage: test
  script:
  - ./gradlew check
  artifacts:
    reports:
      junit: build/test-results/test/*.xml
Any help will be appreciated.
",<java><postgresql><gitlab-ci>,2249,0,55,331,1,3,4,71,19067,0,1,7,33,2019-05-02 18:18,2019-11-07 9:16,,189,,Basic,14
50709059,MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis teners added. Use emitter.setMaxListeners() to increase limit,"I know this might flag as a duplicate solution but the solution on stack overflow is not working for me.
Problem
(node:5716) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message lis
teners added. Use emitter.setMaxListeners() to increase limit.
My codebase is huge and I facing this error sometimes I don't know why it is happening. I tried to increase the listeners limit but unfortunately, it is not working.
const EventEmitter = require('events');
const emitter = new EventEmitter()
emitter.setMaxListeners(50)
UPDATE
After Some browsing, I run this command to trace warning
node --trace-warnings index.babel.js
Turns out be my socket.io code is the problem I am using socket.io with Redis. This is the error
node:14212) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message li
steners added. Use emitter.setMaxListeners() to increase limit
    at _addListener (events.js:281:19)
    at RedisClient.addListener (events.js:298:10)
    at Namespace.&lt;anonymous&gt; (D:/newProject/services/socket.js:21:17)
    at emitOne (events.js:115:13)
    at Namespace.emit (events.js:210:7)
    at Namespace.emit (D:\newProject\node_modules\socket.io\lib\namespace.js:213:10)
    at D:\newProject\node_modules\socket.io\lib\namespace.js:181:14
    at _combinedTickCallback (internal/process/next_tick.js:131:7)
    at process._tickCallback (internal/process/next_tick.js:180:9)
this is the code (But this code is for more specific tasks it will not execute all the time).
const redis = require('redis');
const config = require('../config');
const sub = redis.createClient(config.REDIS.port, config.REDIS.host);
const pub = redis.createClient(config.REDIS.port, config.REDIS.host);
sub.subscribe('spread');
module.exports = io =&gt; {
  io.on('connection', socket =&gt; {
    /* To find the User Login  */
    let passport = socket.handshake.session.passport; 
    if (typeof passport !== 'undefined') {
      socket.on('typing:send', data =&gt; {
        pub.publish('spread', JSON.stringify(data));
      });
      sub.on('message', (ch, msg) =&gt; {
        // This is the Exact line where I am getting this error
        io.emit(`${JSON.parse(msg).commonID}:receive`, { ...JSON.parse(msg) });
      });
    }
  });
};
",<javascript><mysql><node.js><reactjs>,2274,0,41,2420,6,35,77,42,119087,0,77,4,33,2018-06-05 21:16,2018-06-05 22:11,2018-07-04 6:03,0,29,Basic,14
50702865,Entity Framework Attach/Update confusion (EF Core),"As I understand, when ""Update"" is called, every property within a specific entity is modified. 
The ""Attach"" method, on the other hand, starts the entity off in the ""Unmodified"" state. Then, when an operation takes place on a particular property, that specific property only is modified. So ""Attach"" is more useful for individual property changes, and ""Update"" is more useful when you want to update every property in the entity (I may be wrong in this understanding).
However, what I don't understand is what happens when neither of these two methods are called during a property change. For instance, consider an example with a table called ""students"":
student.City = ""Calgary"";
student.Name = ""John Smith"";
database.SaveChanges();
As we are not marking any property in the entity as modified, how will the generated query from the above code differ?
",<c#><entity-framework><sql-update>,853,0,3,774,1,5,11,48,35009,0,12,2,33,2018-06-05 14:41,2018-06-06 6:43,2018-06-06 6:43,1,1,Basic,9
64198359,"pg Admin 4 - password for ""postgres"" user when trying to connect to PostgreSQL 13 server","I know that this question has been asked other times but I didn't find a solution to this problem!
I downloaded PostgreSQL 13 with pg Admin 4 and when I open it for the first time after installation it asks me for the master password that I was asked to set during installation, after I give the master password and this gets accepted I try to connect to the default server created during the installation: &quot;PostgreSQL 13&quot;.
At this point, it asks me for a password for the user &quot;postgres&quot; that I don't know where to find. Specifically, it says: Please enter the password for the user 'postgres' to connect the server - &quot;PostgreSQL 13&quot;.
I've already tried all the &quot;default&quot; passwords I managed to find on the internet but the error is always the same:
FATAL: password authentication failed for user &quot;postgres&quot;
I've also tried not to insert any password with the resulting error:
fe_sendauth: no password supplied
I don't know what to do. In PostgreSQL 13 the authentication method is encrypted via scram-sha-256. I already tried to set the method to trust, restart the mac, and open pg Admin 4 that keeps asking me for the password to access the server.
I've also tried to use the command line tool but end up encountering the same errors.
Finally, this is how my pg_hba.conf looks like:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
# &quot;local&quot; is for Unix domain socket connections only
local   all             all                                     scram-sha-256
# IPv4 local connections:
host    all             all             127.0.0.1/32            scram-sha-256
# IPv6 local connections:
host    all             all             ::1/128                 scram-sha-256
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   replication     all                                     scram-sha-256
host    replication     all             127.0.0.1/32            scram-sha-256
host    replication     all             ::1/128                 scram-sha-256
PS. I've also tried to uninstall PostgreSQL 13, deleting the postgres user and re-download and re-install everything... nothing changed.
If someone could help me would become my savior, thanks beforehand!
",<postgresql><pgadmin><pgadmin-4><postgresql-13>,2288,0,15,357,1,3,6,79,120510,0,4,20,33,2020-10-04 18:25,2020-10-05 14:12,2020-10-10 4:43,1,6,Basic,14
48605130,How to convert List<Object> into comma separated String,"I am getting list of Address objects from the DB call. 
ArrayList&lt;Address&gt; addresses = new ArrayList&lt;&gt;();
Each Address has an int addressId property.
I am writing an update query where in the IN clause I am sending this whole list of Address objects and I am getting ibatis TypeException. How can I convert List&lt;Address&gt; to a comma separated string which can be sent to update query?
My update query looks like:::
Update tablename set postcode = #{postCode} where id in #{addressID}.
",<java><sql><ibatis>,502,0,9,373,1,4,12,56,57038,0,0,8,33,2018-02-04 5:43,2018-02-04 5:51,,0,,Basic,10
50373427,Node.js can't authenticate to MySQL 8.0,"I have a Node.js program that connects to a local MySQL database with the root account (this is not a production setup).
This is the code that creates the connection:
const mysql = require('mysql');
const dbConn = mysql.createConnection({
    host: 'localhost',
    port: 3306,
    user: 'root',
    password: 'myRootPassword',
    database: 'decldb'
});
dbConn.connect(err =&gt; {
    if (err) throw err;
    console.log('Connected!');
});
It worked with MySQL 5.7, but since installing MySQL 8.0 I get this error when starting the Node.js app:
&gt; node .\api-server\main.js
[2018-05-16T13:53:53.153Z] Server launched on port 3000!
C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:80
        throw err; // Rethrow non-MySQL errors
        ^
Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at Handshake.Sequence._packetToError (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Sequence.js:52:14)
    at Handshake.ErrorPacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\sequences\Handshake.js:130:18)
    at Protocol._parsePacket (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:279:23)
    at Parser.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Parser.js:76:12)
    at Protocol.write (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:39:16)
    at Socket.&lt;anonymous&gt; (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:103:28)
    at emitOne (events.js:116:13)
    at Socket.emit (events.js:211:7)
    at addChunk (_stream_readable.js:263:12)
    at readableAddChunk (_stream_readable.js:250:11)
    --------------------
    at Protocol._enqueue (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:145:48)
    at Protocol.handshake (C:\Users\me\project\node_server\node_modules\mysql\lib\protocol\Protocol.js:52:23)
    at Connection.connect (C:\Users\me\project\node_server\node_modules\mysql\lib\Connection.js:130:18)
    at Object.&lt;anonymous&gt; (C:\Users\me\project\node_server\main.js:27:8)
    at Module._compile (module.js:652:30)
    at Object.Module._extensions..js (module.js:663:10)
    at Module.load (module.js:565:32)
    at tryModuleLoad (module.js:505:12)
    at Function.Module._load (module.js:497:3)
    at Function.Module.runMain (module.js:693:10)
It seems that the root account uses a new password hashing method:
&gt; select User,Host,plugin from user where User=""root"";
+------+-----------+-----------------------+
| User | Host      | plugin                |
+------+-----------+-----------------------+
| root | localhost | caching_sha2_password |
+------+-----------+-----------------------+
...but I don't know why Node.js is unable to connect to it. I have updated all the npm packages and it's still an issue.
I would like to keep the new password hashing method. Can I still make this connection work? Do I have to wait for an update of the MySQL Node.js package, or change a setting on my side?
",<mysql><node.js><authentication><mysql-8.0>,3124,0,48,1721,5,24,44,71,32919,0,4265,5,32,2018-05-16 14:19,2018-05-16 18:40,2018-05-16 18:40,0,0,Basic,13
48420438,"""could not identify an equality operator for type json"" when using distinct","I have the following query:
SELECT 
  distinct(date(survey_results.created_at)), 
  json_build_object(
    'high', 
    ROUND( 
      COUNT(*) FILTER (WHERE ( scores#&gt;&gt;'{medic,categories,motivation}' in('high', 'medium'))) OVER(order by date(survey_results.created_at) ) * 1.0 / 
      (
        CASE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        WHEN 0.0 THEN 1.0 
        ELSE (COUNT(*) FILTER (WHERE (scores#&gt;&gt;'{medic,categories,motivation}' in('high','medium','low'))) OVER(order by date(survey_results.created_at))) 
        END)* 100, 2 ) ) AS childcare FROM survey_results GROUP BY date, scores ORDER BY date asc; 
The problem is with using distinct(date(survey_results.created_at)). With that in place query returns error:
could not identify an equality operator for type json
Here is db fiddle that show that problem. How can I fix that?
",<postgresql>,970,1,13,7462,15,68,135,44,59461,0,499,4,32,2018-01-24 10:40,2018-01-24 10:48,2018-01-24 10:48,0,0,Basic,2
49083573,PHP 7.2.2 + mysql 8.0 PDO gives: authentication method unknown to the client [caching_sha2_password],"I'm using php 7.2.2 and mysql 8.0.
When I try to connect with the right credential I get this error:
PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
Need help to troubleshoot the problem.
",<php><mysql><pdo>,267,0,1,1042,2,11,26,41,64856,0,141,4,32,2018-03-03 11:20,2018-04-30 5:38,2018-06-11 20:06,58,100,Basic,3
49274390,PostgreSQL and Hibernate java.io.IOException: Tried to send an out-of-range integer as a 2-byte value,"I have hibernate query:
getSession()                     
        .createQuery(""from Entity where id in :ids"") 
        .setParameterList(""ids"", ids)
        .list();
where ids is Collection ids, that can contain a lot of ids.
Currently I got exception when collection is very large:
 java.io.IOException: Tried to send an out-of-range integer as a 2-byte value
I heard that postgre has some problem with it. But I can't find the solution how to rewrite it on hql.
",<postgresql><hibernate><exception><ioexception>,465,0,4,320,1,3,4,70,27364,0,0,4,32,2018-03-14 9:47,2018-03-14 10:20,,0,,Basic,6
50570592,Mysql 8 remote access,"I usualy setup correctly MySQL for having remote access.
And currently I got stuck with MySQL 8.
The first thing is that on the mysql.conf.d/mysqld.cnf , I don't have any bind-address line, so I added it by hand (bind-address 0.0.0.0)
And I granted access to the user on '%' 
When I connected I got the message ""Authentication failed""
But it works well on localhost/command line
",<mysql><authentication><mysql-8.0>,379,0,0,617,2,8,14,67,84475,0,7,4,32,2018-05-28 16:44,2018-08-05 11:13,,69,,Advanced,38
64829748,Pgadmin is not loading,"i have recently installed pgadmin4 onto my laptop and when I launch the application, it just gets stuck on the loading. I had a look at the logs and this is what I see:
The logs
2020-11-14 00:22:46: Checking for system tray...
2020-11-14 00:22:46: Starting pgAdmin4 server...
2020-11-14 00:22:46: Creating server object, port:64222, key:2a079549-63da-44d2-8931-efa9de3a847f, logfile:C:/Users/yonis/AppData/Local/pgadmin4.d41d8cd98f00b204e9800998ecf8427e.log
2020-11-14 00:22:46: Python Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib/site-packages;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/DLLs;C:/Program Files/PostgreSQL/13/pgAdmin 4/venv/Lib
2020-11-14 00:22:46: Python Home: C:/Program Files/PostgreSQL/13/pgAdmin 4/venv
2020-11-14 00:22:46: Initializing Python...
2020-11-14 00:22:46: Python initialized.
2020-11-14 00:22:46: Adding new additional path elements
2020-11-14 00:22:46: Redirecting stderr...
2020-11-14 00:22:46: stderr redirected successfully.
2020-11-14 00:22:46: Initializing server...
2020-11-14 00:22:46: Webapp Path: C:/Program Files/PostgreSQL/13/pgAdmin 4/web/pgAdmin4.py
2020-11-14 00:22:46: Server initialized, starting server thread...
2020-11-14 00:22:46: Open the application code and run it.
2020-11-14 00:22:46: Set the port number, key and force SERVER_MODE off
2020-11-14 00:22:46: PyRun_SimpleFile launching application server...
2020-11-14 00:22:47: Application Server URL: http://127.0.0.1:64222/?key=2a079549-63da-44d2-8931-efa9de3a847f
2020-11-14 00:22:47: The server should be up. Attempting to connect and get a response.
2020-11-14 00:22:53: Attempt to connect one more time in case of a long network timeout while looping
2020-11-14 00:22:53: Everything works fine, successfully started pgAdmin4.
",<python><postgresql><pgadmin><heroku-postgres><pgadmin-4>,1753,2,20,321,1,3,3,64,51325,0,0,10,32,2020-11-14 0:34,2020-11-14 10:50,,0,,Basic,14
51375439,Postgres on conflict do update on composite primary keys,"I have a table where a user answers to a question. The rules are that the user can answer to many questions or many users can answer one question BUT a user can answer to a particular question only once. If the user answers to the question again, it should simply replace the old one. Generally the on conflict do update works when we are dealing with unique columns. In this scenario the columns person_id and question_id cannot be unique. However the combination of the two is always unique. How do I implement the insert statement that does update on conflict?
CREATE TABLE ""answer"" (
  ""person_id"" integer NOT NULL REFERENCES person(id), 
  ""question_id"" integer NOT NULL REFERENCES question(id) ON DELETE CASCADE, /* INDEXED */
  ""answer"" character varying (1200) NULL,
  PRIMARY KEY (person_id, question_id) 
);
",<sql><postgresql><upsert>,818,0,8,3055,4,32,59,35,33711,0,111,2,32,2018-07-17 7:20,2018-07-17 7:39,2018-07-17 7:39,0,0,Basic,10
52064130,"QueryFailedError: the column ""price"" contain null values - TypeORM - PostgreSQL","I've created a simple table:
import { Column, Entity, PrimaryGeneratedColumn } from &quot;typeorm&quot;
@Entity()
export class Test {
    @PrimaryGeneratedColumn()
    public id!: number
    @Column({ nullable: false })
    public name!: string
    @Column({ nullable: false, type: &quot;float&quot; })
    public price!: number
}
I generate the migration and run it also. When I have no data in the database and I run the server it succeed. But when I add 1 row in the database and I run it again it appears the following error:
QueryFailedError: the column «price» contain null values
The databse definetely has the row with all the data. I tried a lot of cases and none of them was correct.
Has anybody some idea for it?
",<node.js><postgresql><typescript><typeorm>,724,0,14,1468,4,15,27,68,46673,0,26,8,32,2018-08-28 18:21,2018-08-31 9:36,,3,,Basic,13
52372165,mysql ERROR 1064 (42000): You have an error in your SQL syntax;,"i have installed mysql 8.0.12 into one linux node and when i try to give below grant permission to get access from other nodes, i am getting 42000 error
command issued :
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password';
Return results:
  ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IDENTIFIED BY 'secure1t'' at line 1
Any help would be appreciated.
",<mysql>,478,0,1,407,1,4,8,79,84805,0,0,3,32,2018-09-17 16:29,2018-09-17 16:41,2018-09-17 16:41,0,0,Basic,8
55286397,How to compare character varying (varcar) to UUID in PostgreSQL?,"Operator does not exist: character varying = uuid
Client id is UUId and should be why it is not working.
Where I am wrong, since I have tried almost everything I imagined.
SELECT * FROM ""cobranca_assinatura""
INNER JOIN ""cadastro_cliente"" ON (""cobranca_assinatura"".""cliente_id"" = ""cadastro_cliente"".""id"")
WHERE ""cadastro_cliente"".""nome"" LIKE marcelo% ESCAPE '\'
  [2019-03-21 14:40:34] [42883] ERROR: operator does not exist:
  character varying = uuid [2019-03-21 14:40:34] 
  Dica: No operator
  matches the given name and argument type(s). You might need to add
  explicit type casts.
",<postgresql>,587,0,3,2832,6,39,73,73,47759,,81,3,31,2019-03-21 17:47,2019-03-21 18:04,2019-03-21 18:04,0,0,Basic,2
50088142,Authentication method 'caching_sha2_password' not supported by any of the available plugins,"When I try to connect MySQL (8.0) database with Visual Studio 2018 I get this error message 
  ""Authentication method 'caching_sha2_password' not supported by any of the available plugins""
Also I am unable to retrieve Database name.
I use mysql-for-visualstudio-1.2.7 and mysql-connector-net-8.0.11 for connection.
Is there any possible way to fix it.
",<c#><mysql><authentication><visual-studio-2017>,352,0,2,347,1,4,11,61,85143,0,2,9,31,2018-04-29 15:18,2018-05-01 20:43,2019-11-15 5:06,2,565,Basic,14
48406304,GroupBy and concat array columns pyspark,"I have this data frame
df = sc.parallelize([(1, [1, 2, 3]), (1, [4, 5, 6]) , (2,[2]),(2,[3])]).toDF([&quot;store&quot;, &quot;values&quot;])
+-----+---------+
|store|   values|
+-----+---------+
|    1|[1, 2, 3]|
|    1|[4, 5, 6]|
|    2|      [2]|
|    2|      [3]|
+-----+---------+
and I would like to convert into the follwing df:
+-----+------------------+
|store|      values      |
+-----+------------------+
|    1|[1, 2, 3, 4, 5, 6]|
|    2|            [2, 3]|
+-----+------------------+
I did this:
from  pyspark.sql import functions as F
df.groupBy(&quot;store&quot;).agg(F.collect_list(&quot;values&quot;))
but the solution has this WrappedArrays
+-----+----------------------------------------------+
|store|collect_list(values)                          |
+-----+----------------------------------------------+
|1    |[WrappedArray(1, 2, 3), WrappedArray(4, 5, 6)]|
|2    |[WrappedArray(2), WrappedArray(3)]            |
+-----+----------------------------------------------+
Is there any way to transform the WrappedArrays into concatenated arrays? Or can I do it differently?
",<pyspark><apache-spark-sql>,1091,0,26,1039,2,12,15,63,56011,0,10,6,31,2018-01-23 16:17,2018-01-23 17:05,2018-01-23 17:05,0,0,Basic,10
56118095,Type hints for SQLAlchemy engine and session objects,"I'm trying to add type hints to my SQLAlchemy script:
connection_string: str = ""sqlite:///:memory:""
engine = create_engine(connection_string)
session = Session(bind=engine)
reveal_type(engine)
reveal_type(session)
I've ran this script against mypy but both types comes back as Any. What type should the engine and session variable be?
",<python><sqlalchemy><mypy><type-hinting>,335,0,9,5261,20,85,155,76,17483,,400,2,31,2019-05-13 18:37,2019-05-13 21:07,2019-05-13 21:07,0,0,Basic,3
49148754,Docker container shuts down giving 'data directory has wrong ownership' error when executed in windows 10,"I have my docker installed in Windows. I am trying to install this application. It has given me the following docker-compose.yml file:
version: '2'
services:
  web:
    build:
      context: .
      dockerfile: Dockerfile-nginx
    ports:
    - &quot;8085:80&quot;
    networks:
      - attendizenet
    volumes:
      - .:/usr/share/nginx/html/attendize
    depends_on:
      - php
  php:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    networks: 
      - attendizenet
  php-worker:
    build:
      context: .
      dockerfile: Dockerfile-php
    depends_on:
      - db
      - maildev
      - redis
    volumes:
      - .:/usr/share/nginx/html/attendize
    command: php artisan queue:work --daemon
    networks:
      - attendizenet
  db:
    image: postgres
    environment:
      - POSTGRES_USER=attendize
      - POSTGRES_PASSWORD=attendize
      - POSTGRES_DB=attendize
    ports:
      - &quot;5433:5432&quot;
    volumes:
      - ./docker/pgdata:/var/lib/postgresql/data
    networks:
    - attendizenet
  maildev:
    image: djfarrelly/maildev
    ports:
      - &quot;1080:80&quot;
    networks:
      - attendizenet
  redis:
    image: redis
    networks:
      - attendizenet
networks:
  attendizenet:
    driver: bridge
All the installation goes well, but the PostgreSQL container stops after starting for a moment giving following error.
2018-03-07 08:24:47.927 UTC [1] FATAL:  data directory &quot;/var/lib/postgresql/data&quot; has wrong ownership
2018-03-07 08:24:47.927 UTC [1] HINT:  The server must be started by the user that owns the data directory
A simple PostgreSQL container from Docker Hub works smoothly, but the error occurs when we try to attach a volume to the container.
I am new to docker, so please ignore usage of terms wrongly.
",<postgresql><docker><docker-compose><docker-for-windows>,1895,1,68,717,1,7,19,77,27966,0,17,7,31,2018-03-07 9:48,2018-03-07 9:58,2019-03-11 20:52,0,369,Basic,14
54302088,"How to fix ""Error: The server does not support SSL connections"" when trying to access database in localhost?","I am following Heroku's node.js tutorial to provision a Postgres database. 
After creating a simple table and connecting to localhost:5000/db, I get an error saying ""Error: The server does not support SSL connections"".
I've been searching for solutions for hours but can't seem to fix it. Your help will be greatly appreciated. Thank you!
",<node.js><postgresql><heroku>,339,1,0,313,1,3,5,64,58725,0,14,8,31,2019-01-22 5:59,2020-04-14 0:24,2020-04-14 0:24,448,448,Basic,13
56089400,Postgres Sql `could not determine data type of parameter` by Hibernate,"I have JpaRepository:
@Repository
public interface CardReaderRepository extends JpaRepository&lt;CardReaderEntity, Integer &gt; {
}
when i execute query in this repo same as this:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null or cr.offLoadingDate &gt;= :fromDate ) "" +
            ""AND  (:toDate   is null or cr.offLoadingDate &lt;= :toDate   ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
error is :
org.postgresql.util.PSQLException: ERROR: could not determine data type of parameter $3
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2433) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2178) ~[postgresql-42.2.2.jar:42.2.2]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:306) ~[postgresql-42.2.2.jar:42.2.2]
but when a change query to:
@Query(
    value = ""SELECT new ir.server.component.panel.response."" +
            ""InvalidCardReaderDataDigestResponse("" +
            ""    cr.driverNumber, cr.exploiterCode, cr.lineCode, "" +
            ""    cr.transactionDate, cr.offLoadingDate, "" +
            ""    cr.cardReaderNumber, "" +
            ""    sum(cr.count) , sum(cr.remind) "" +
            "") "" +
            ""FROM CardReaderEntity cr "" +
            ""WHERE cr.company.id = :companyId "" +
            ""AND cr.status.id in :statusIds "" +
            ""AND cr.deleted = false "" +
            ""AND  (:fromDate is null ) "" +
            ""AND  (:toDate   is null ) "" +
            ""group by cr.driverNumber, cr.exploiterCode, cr.lineCode, cr.transactionDate, cr.offLoadingDate, cr.cardReaderNumber""
)
Page&lt;InvalidCardReaderDataDigestResponse&gt; findAllInvalidCardReaderDataDigestByCompanyIdAndStatusIdIn(
        @Param( ""companyId"" )   int companyId,
        @Param( ""fromDate"" )    Date fromDate,
        @Param( ""toDate"" )      Date toDate,
        @Param( ""statusIds"" )   List&lt;Integer&gt; statusIds,
        Pageable pageable
);
this work without error but a want run with fromDate and toDate
note:
offLoadingDate in CardReaderEntity:
@Basic
@Temporal( TemporalType.TIMESTAMP )
public Date getOffLoadingDate() {
    return offLoadingDate;
}
public void setOffLoadingDate(Date offLoadingDate) {
    this.offLoadingDate = offLoadingDate;
}
all of Date import in my java files is java.util.Date.
",<sql><postgresql><hibernate><spring-boot><jpa>,3321,0,63,2225,6,21,37,41,60156,0,1936,3,31,2019-05-11 10:23,2020-10-06 9:53,,514,,Basic,12
57451719,"Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column","I have a json file:
{
  &quot;a&quot;: {
    &quot;b&quot;: 1
  }
}
I am trying to read it:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path)
df.show()
But getting an error:
Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException:
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed
when the referenced columns only include the internal corrupt record
column (named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($&quot;_corrupt_record&quot;.isNotNull).count()
and
spark.read.schema(schema).json(file).select(&quot;_corrupt_record&quot;).show().
Instead, you can cache or save the parsed results and then send the
same query. For example, val df =
spark.read.schema(schema).json(file).cache() and then
df.filter($&quot;_corrupt_record&quot;.isNotNull).count().;
So I tried to cache it as they suggest:
val path = &quot;D:/playground/input.json&quot;
val df = spark.read.json(path).cache()
df.show()
But I keep getting the same error.
",<json><scala><apache-spark><apache-spark-sql>,1038,0,11,10709,24,92,152,80,42093,0,886,4,31,2019-08-11 16:27,2019-08-12 6:28,2019-08-12 6:28,1,1,Basic,14
65398641,Docker connect SQL Server container non-zero code: 1,"I'm trying to create a SQL Server container from a docker-compose.yml but when I run it, it directly stops with some errors. Note: it's running on an Apple M1 chip with docker Preview
docker-compose.yml:
version: &quot;3.7&quot;
services:
  sql-server-db:
    container_name: sql-server-db
    image: mcr.microsoft.com/mssql/server:2019-latest
    ports: 
      - &quot;1433:1433&quot;
    environment: 
      SA_PASSWORD: &quot;ApplePassDockerConnect&quot;
      ACCEPT_EULA: &quot;Y&quot;
The errors I'm getting:
sql-server-db | /opt/mssql/bin/sqlservr: Invalid mapping of address 0x40092b8000 in reserved address space below 0x400000000000. Possible causes:
sql-server-db | 1) the process (itself, or via a wrapper) starts-up its own running environment sets the stack size limit to unlimited via syscall setrlimit(2);
sql-server-db | 2) the process (itself, or via a wrapper) adjusts its own execution domain and flag the system its legacy personality via syscall personality(2);
sql-server-db | 3) sysadmin deliberately sets the system to run on legacy VA layout mode by adjusting a sysctl knob vm.legacy_va_layout.
sql-server-db |
sql-server-db exited with code 1
",<sql-server><docker><apple-m1>,1170,0,11,301,1,3,3,57,16478,0,0,6,30,2020-12-21 19:12,2020-12-21 21:58,,0,,Basic,13
53489250,"Price aside, why ever choose Google Cloud Bigtable over Google Cloud Datastore?","If I have a use case for both huge data storage and searchability, why would I ever choose Google Cloud Bigtable over Google Cloud Datastore?
I've seen a few questions on SO and other sides ""comparing"" Bigtable and Datastore, but it seems to boil down to the same non-specific answers.
Here's my current knowledge and my thoughts:
  Datastore is more expensive.
In the context of this question, let's forget entirely about pricing.
  Bigtable is good for huge datasets.
It seems like Datastore is, too? I'm not seeing what specifically makes Bigtable objectively superior here.
  Bigtable is better than Datastore for analytics.
How? Why? It seems like I can do analytics in Datastore as well, no problem. Why is Bigtable seemingly the unanimous decision industry-wide for analytics? What value do GMail, eBay, etc. get from Bigtable that Datastore can't provide?
  Bigtable is integrated with Hadoop, Spark, etc.
Is Datastore not as well, considering it's built on Bigtable?
From this question, this statement was made in an answer: 
  Bigtable and Datastore are extremely different. Yes, the datastore is build on top of Bigtable, but that does not make it anything like it. That is kind of like saying a car is build on top of [car] wheels, and so a car is not much different from wheels.
However, this seems analogy seems nonsensical, since the car (including the wheels) intrinsically provides more value than just the wheels of a car by themselves. 
It seems at first glance that Bigtable is strictly worse than Datastore, only providing a single index and limiting quick searchability.  What am I missing?
",<google-cloud-platform><nosql><google-cloud-datastore><bigtable><google-cloud-bigtable>,1613,1,0,435,1,6,9,41,15053,0,2,1,30,2018-11-26 21:21,2018-11-27 19:41,2018-11-27 19:41,1,1,Intermediate,17
50927740,"SQLAlchemy: ""create schema if not exists""","I want to do the ""CREATE SCHEMA IF NOT EXISTS"" query in SQLAlchemy.
Is there a better way than this:
    engine = sqlalchemy.create_engine(connstr)
    schema_name = config.get_config_value('db', 'schema_name')
    #Create schema; if it already exists, skip this
    try:
        engine.execute(CreateSchema(schema_name))
    except sqlalchemy.exc.ProgrammingError:
        pass
I am using Python 3.5.
",<python><sql><database><python-3.x><sqlalchemy>,402,0,9,1103,3,14,29,40,25948,0,987,5,30,2018-06-19 11:51,2019-03-25 19:50,2019-03-25 19:50,279,279,Basic,10
50974851,How to optimise this MySQL query? Millions of Rows,"I have the following query:
SELECT 
    analytics.source AS referrer, 
    COUNT(analytics.id) AS frequency, 
    SUM(IF(transactions.status = 'COMPLETED', 1, 0)) AS sales
FROM analytics
LEFT JOIN transactions ON analytics.id = transactions.analytics
WHERE analytics.user_id = 52094 
GROUP BY analytics.source 
ORDER BY frequency DESC 
LIMIT 10 
The analytics table has 60M rows and the transactions table has 3M rows.
When I run an EXPLAIN on this query, I get:
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| # id |  select_type |      table      |  type  |    possible_keys    |        key        |        key_len       |            ref            |   rows   |   Extra   |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
| '1'  |  'SIMPLE'    |  'analytics'    |  'ref' |  'analytics_user_id | analytics_source' |  'analytics_user_id' |  '5'                      |  'const' |  '337662' |  'Using where; Using temporary; Using filesort' |
| '1'  |  'SIMPLE'    |  'transactions' |  'ref' |  'tran_analytics'   |  'tran_analytics' |  '5'                 |  'dijishop2.analytics.id' |  '1'     |  NULL     |                                                 |
+------+--------------+-----------------+--------+---------------------+-------------------+----------------------+---------------------------+----------+-----------+-------------------------------------------------+
I can't figure out how to optimise this query as it's already very basic. It takes around 70 seconds to run this query.
Here are the indexes that exist:
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|   # Table   |  Non_unique |          Key_name          |  Seq_in_index |    Column_name   |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'analytics' |  '0'        |  'PRIMARY'                 |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_user_id'       |  '1'          |  'user_id'       |  'A'       |  '130583'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_product_id'    |  '1'          |  'product_id'    |  'A'       |  '490812'    |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_affil_user_id' |  '1'          |  'affil_user_id' |  'A'       |  '55222'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_source'        |  '1'          |  'source'        |  'A'       |  '24604'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_country_name'  |  '1'          |  'country_name'  |  'A'       |  '39510'     |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '1'          |  'id'            |  'A'       |  '56934235'  |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '2'          |  'user_id'       |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'analytics' |  '1'        |  'analytics_gordon'        |  '3'          |  'source'        |  'A'       |  '56934235'  |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+-------------+-------------+----------------------------+---------------+------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
|    # Table     |  Non_unique |      Key_name     |  Seq_in_index |    Column_name    |  Collation |  Cardinality |  Sub_part |  Packed |  Null  |  Index_type |  Comment |  Index_comment |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
| 'transactions' |  '0'        |  'PRIMARY'        |  '1'          |  'id'             |  'A'       |  '2436151'   |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_user_id'   |  '1'          |  'user_id'        |  'A'       |  '56654'     |  NULL     |  NULL   |  ''    |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'transaction_id' |  '1'          |  'transaction_id' |  'A'       |  '2436151'   |  '191'    |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_analytics' |  '1'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'tran_status'    |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '1'          |  'status'         |  'A'       |  '22'        |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
| 'transactions' |  '1'        |  'gordon_trans'   |  '2'          |  'analytics'      |  'A'       |  '2436151'   |  NULL     |  NULL   |  'YES' |  'BTREE'    |  ''      |  ''            |
+----------------+-------------+-------------------+---------------+-------------------+------------+--------------+-----------+---------+--------+-------------+----------+----------------+
Simplified schema for the two tables before adding any extra indexes as suggested as it didn't improve the situation.
CREATE TABLE `analytics` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) DEFAULT NULL,
  `affil_user_id` int(11) DEFAULT NULL,
  `product_id` int(11) DEFAULT NULL,
  `medium` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `source` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `terms` varchar(1024) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `is_browser` tinyint(1) DEFAULT NULL,
  `is_mobile` tinyint(1) DEFAULT NULL,
  `is_robot` tinyint(1) DEFAULT NULL,
  `browser` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `mobile` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `robot` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `platform` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `referrer` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `domain` varchar(45) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `ip` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `continent_code` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `country_name` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `city` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `analytics_user_id` (`user_id`),
  KEY `analytics_product_id` (`product_id`),
  KEY `analytics_affil_user_id` (`affil_user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=64821325 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
CREATE TABLE `transactions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `transaction_id` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  `pay_key` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sender_email` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `amount` decimal(10,2) DEFAULT NULL,
  `currency` varchar(10) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `status` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `analytics` int(11) DEFAULT NULL,
  `ip_address` varchar(46) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `session_id` varchar(60) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `eu_vat_applied` int(1) DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `tran_user_id` (`user_id`),
  KEY `transaction_id` (`transaction_id`(191)),
  KEY `tran_analytics` (`analytics`),
  KEY `tran_status` (`status`)
) ENGINE=InnoDB AUTO_INCREMENT=10019356 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
If the above can not be optimised any further. Any implementation advice on summary tables will be great. We are using a LAMP stack on AWS. The above query is running on RDS (m1.large).
",<mysql><sql><query-optimization><amazon-rds><sql-optimization>,9303,0,91,56442,101,275,411,39,4394,0,706,13,30,2018-06-21 17:54,2018-06-21 17:55,2018-06-24 13:19,0,3,Intermediate,23
48703318,MySql.Data.EntityFrameworkCore vs Pomelo.EntityFrameworkCore.MySql,"Which database connector should I use in .Net Core 2 enterprise level web app which is going to handle large amount of data.
I have two choices:
Pomelo.EntityFrameworkCore.MySql
MySql.Data.EntityFrameworkCore
But still confuse which one to choose for the development.
I started with MySql.Data.EntityFrameworkCore, which is MySQL official provider but after facing several issues while code first migration I had to google again for some better alternative.
After some research I found Pomelo.EntityFrameworkCore.MySql bit more helpful for my application and it also covered the issues I was facing before.
But it still left me a bit confuse about which one to choose for long term.
Pomelo is workign fine currently but I am not sure if they (pomelo team) will keep always keep it updating and will keep it in sync with the latest .Net Core version available in the market??
MySql is not working as expected but the only plus point with this is : It is provided by MySQl itself.
Please help me to decide
",<mysql><entity-framework><entity-framework-core><asp.net-core-2.0><pomelo-entityframeworkcore-mysql>,1004,0,0,910,1,7,20,78,21118,0,11,0,30,2018-02-09 10:05,,,,,Intermediate,21
48872965,Postgres: \copy syntax,"With PostgreSQL 9.5 on CentOS 7, I have created a database named sample along with several tables.  I have .csv data in /home/MyUser/data for each table. 
 For example, there exists TableName.csv for the table ""TableName"".
How do I load the csv files into each table?
What I've tried doesn't work and I can't figure out what I'm doing wrong.
Load from within the DB
$ psql sample
sample=# COPY ""TableName"" FROM '/home/MyUser/data/TableName.csv' WITH CSV;
ERROR:  could not open file ""/home/MyUser/data/TableName.csv"" for reading: Permission denied
This implies a file permission problem.  All the files in data/ are -rw-r--r-- and the directory itself is drwxr-xr-x.  So file permissions shouldn't be the problem (unless I'm missing something).  The internet says that COPY has problems with permissions and to try \copy.  
Load from CLI
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH CSV
psql: warning: extra command-line argument ""FROM"" ignored
psql: warning: extra command-line argument ""/home/MyUser/data/TableName.csv"" ignored
psql: warning: extra command-line argument ""WITH"" ignored
psql: warning: extra command-line argument ""CSV"" ignored
psql: FATAL:  Peer authentication failed for user ""sample""
This appears to be a syntax error, but I'm not finding the documentation particularly helpful (man psql then /\copy).  I've also tried the following to the same result.
$ psql \copy sample.""TableName"" FROM /home/MyUser/data/TableName.csv WITH CSV
$ psql \copy sample FROM /home/MyUser/data/TableName.csv WITH DELIMITER ','
There are several other permutations which yield similar errors.
Web Resources Used
https://www.postgresql.org/docs/9.5/static/app-psql.html
https://www.postgresql.org/docs/9.5/static/sql-copy.html
The correct COPY command to load postgreSQL data from csv file that has single-quoted data?
https://soleil4716.wordpress.com/2010/08/19/using-copy-command-in-postgresql/
Can I use \copy command into a function of postgresql?
https://wiki.postgresql.org/wiki/COPY
",<postgresql>,2005,10,26,4120,4,43,69,36,91883,0,406,2,30,2018-02-19 19:31,2018-02-20 9:14,2018-02-20 9:14,1,1,Intermediate,21
48446399,SQL auto increment pgadmin 4,"I am trying to make a simple database with an number generator but why do I get the error below?
  ERROR:  syntax error at or near ""AUTO_INCREMENT""
  LINE 2: IDNumber int NOT NULL AUTO_INCREMENT,
Code:
CREATE TABLE Finance
(
    IDNumber int NOT NULL AUTO_INCREMENT,
    FinName varchar(50) NOT NULL,
    PRIMARY KEY(IDNumber)
);
",<sql>,330,0,6,497,1,5,14,78,80449,0,9,6,29,2018-01-25 15:27,2018-01-25 15:30,2018-01-25 15:32,0,0,Basic,8
63585525,SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key are incompatible,"I´m doing migrations in Laravel and this error happens when I proceed with the command PHP artisan migrate:
In Connection.php line 664:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible. (SQL: alter table almacen_movimientos add constraint almacen_movimientos_user_id_foreign foreign key (user_id) references users (id
) on delete restrict)
In PDOStatement.php line 129:
SQLSTATE[HY000]: General error: 3780 Referencing column 'user_id' and referenced column 'id' in foreign key constraint 'almacen_movimientos_user_id_foreign' are incompatible.
My migrations look like this:
almacen_movimientos table
public function up()
{
    Schema::create('almacen_movimientos', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;integer('cliente_proveedor_id');
        $table-&gt;integer('empresa_id');
        $table-&gt;integer('user_id');
        $table-&gt;enum('tipo' , ['ENTRADA' , 'SALIDA' , 'REUBICACION' , 'TRASPASO' , 'DEVOLUCION' , 'MSRO' , 'ENTRADA POR TRASPASO' , 'SALIDA POR TRASPASO'])-&gt;nullable();
        $table-&gt;string('referencia' , 255)-&gt;nullable();
        $table-&gt;string('observaciones' , 255)-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;string('transportista' , 255)-&gt;nullable();
        $table-&gt;string('operador' , 255)-&gt;nullable();
        $table-&gt;string('procedencia' , 255)-&gt;nullable();
        $table-&gt;integer('almacen_id')-&gt;nullable();
        $table-&gt;foreign('cliente_proveedor_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
        $table-&gt;foreign('user_id')-&gt;references('id')-&gt;on('users')-&gt;onDelete('restrict');
        $table-&gt;foreign('almacen_id')-&gt;references('id')-&gt;on('almacenes')-&gt;onDelete('restrict');
    });
}
Users Table
public function up()
{
    Schema::create('users', function (Blueprint $table) {
        $table-&gt;unsignedBigInteger('id');
        $table-&gt;string('name' , 255);
        $table-&gt;string('apellido_paterno' , 115)-&gt;nullable();
        $table-&gt;string('apellido_materno' , 115)-&gt;nullable();
        $table-&gt;dateTime('fecha_nacimiento')-&gt;nullable();
        $table-&gt;string('telefono1' , 10)-&gt;nullable();
        $table-&gt;string('telefono2' , 10)-&gt;nullable();
        $table-&gt;string('calle' , 255)-&gt;nullable();
        $table-&gt;string('numero' , 45)-&gt;nullable();
        $table-&gt;string('colonia' , 255)-&gt;nullable();
        $table-&gt;string('codigo_postal' , 6)-&gt;nullable();
        $table-&gt;string('email' , 255)-&gt;unique();
        $table-&gt;string('user' , 20)-&gt;nullable()-&gt;unique();
        $table-&gt;string('password' , 255);
        $table-&gt;string('palabra_secreta' , 255);
        $table-&gt;string('remember_token' , 100)-&gt;nullable();
        $table-&gt;unsignedInteger('empresa_id')-&gt;nullable();
        $table-&gt;timestamp('created_at');
        $table-&gt;timestamp('updated_at');
        $table-&gt;timestamp('deleted_at');
        $table-&gt;foreign('empresa_id')-&gt;references('id')-&gt;on('empresas')-&gt;onDelete('restrict');
    });
}
Can somebopdy tell me what am I doing wrong? I cannot fix this.
Thank you.
Regards.
",<php><mysql><laravel>,3540,0,57,429,1,6,14,57,50082,0,7,6,29,2020-08-25 19:03,2020-08-25 19:09,,0,,Basic,10
55509638,"Could not load file or assembly 'System.Memory, Version=4.0.1.' in Visual Studio 2015","For a couple of months i had no issue generating the model from DB by deleting it and recreating it . After a pull from git, an issue has been occurred while trying to make the same process . After the second step (connection string creation with DB) there is no further proceed at the 3rd step and no connection string with the data base is being created at the app.config file.I have tried to test the connection with the database credentials and i am getting the following .
When i try to update specific tables from the model diagram as an alternative i get also the below :
System.Data.Entity.Core.EntityException: An error occurred while
closing the provider connection. See the inner exception for details.
---&gt; System.IO.FileNotFoundException: Could not load file or assembly 'System.Memory, Version=4.0.1.0, Culture=neutral,
PublicKeyToken=cc7b13ffcd2ddd51' or one of its dependencies.
I have reinstalled Entity Framework and npgsql packages and tried to add all (the same) assemblies but with no success . Similar answers at Stack did not solve my issue . (I am allowed to work with the current versions with no further updates at VS or any of its packages.)
!Notice : i get all the appropriate data from my services when i use the API calls with the current model (proper communication with DB), but i cannot generate a new model from DB .
Any solutions ?
I am using
Windows 10
VS 2015
EntityFrameWork 6.2.0
Npgsql 3.1.1
.Net v.4.6.2
Asp.net
Thanks in advance !
",<c#><.net><visual-studio-2015><entity-framework-6><npgsql>,1476,1,0,1039,2,16,28,74,69750,0,62,10,29,2019-04-04 6:59,2019-04-04 7:08,,0,,Basic,14
51126162,Laravel Eloquent: is SQL injection prevention done automatically?,"Given the example code (Message is an Eloquent model.):
public function submit(Request $request){
    $this-&gt;validate($request, [
        'name' =&gt; ""required"",
        ""email"" =&gt; ""required""
    ]);
    //database connection
    $message = new Message;
    $message-&gt;name = $request-&gt;input(""name"");
    $message-&gt;email = $request-&gt;input(""email"");
    $message-&gt;save();
}
Does Eloquent use parameterized queries (like PDO) or any other mechanisms to prevent SQL injection?
",<php><sql><laravel><eloquent>,495,0,13,459,1,4,9,42,28881,0,7,1,29,2018-07-01 19:01,2018-07-01 19:18,2018-07-01 19:18,0,0,Basic,14
49695990,Authenticate from Linux to Windows SQL Server with pyodbc,"I am trying to connect from a linux machine to a windows SQL Server with pyodbc.
I do have a couple of constraints:
Need to log on with a windows domain account
Need to use python3
Need to do it from Linux to Windows
Need to connect to a specific instance
I set up the environment as described by microsoft and have it working (I can import pyodbc and use the configured mussel driver).
I am not familiar with Windows domain authentication and what not, so there is where my problem is.
My connection string:
DRIVER={ODBC Driver 17 for SQL Server};SERVER=myserver.mydomain.com;PORT=1433;DATABASE=MyDatabase;Domain=MyCompanyDomain;Instance=MyInstance;UID=myDomainUser;PWD=XXXXXXXX;Trusted_Connection=yes;Integrated_Security=SSPI
Supposedly one should use ""Trusted_Connection"" to use the Windows domain authentication instead of directly authenticating with the SQL server.
The error I get when running pyodbc.connect(connString):
pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]SSPI Provider: No Kerberos credentials available (851968) (SQLDriverConnect)')
From other sources I read this should work on Windows as this code would use the credentials of the currently logged in user.
My question is how can I connect to a Windows SQL Server instance from Linux using Windows Domain credentials.
",<python><sql><linux><windows><pyodbc>,1330,0,2,820,1,9,19,73,48752,0,18,7,29,2018-04-06 15:16,2018-04-06 16:09,2018-04-13 16:51,0,7,Advanced,36
49959601,Configure time zone to mysql docker container,"I have a mysql 5.7 docker container. When I run the mysql command:
SELECT now();
It shows the time -3 hours to my current time (which is logical). I want to set the time zone in a config file. Following the documentation in https://hub.docker.com/_/mysql/ I create a volume in my docker-compose.yml file like the following:
mysqldb:
    image: mysql:5.7.21
    container_name: mysql_container
    ports:
      - ""3306:3306""
    volumes:
      - ./.docker/etc/mysql/custom.cnf:/etc/mysql/conf.d/custom.cnf
When I browse the files inside the container the file custom.cnf is there.
In that file, I tried some of the ways I found as solutions like:
[mysqld]
default_time_zone='Europe/Sofia'
or a compromise solution which is less elegant as the zone will have to be changed twice a year (summer / winter):
[mysqld]
default_time_zone='+03:00'
but none works. I have the sensation the this file is not loaded by mysql at all because if I try to put invalid configuration in there nothing happens either (the container starts normally).
Any suggestions on that?
",<mysql><docker><docker-compose><my.cnf>,1056,2,14,1442,3,21,29,56,52256,0,101,4,29,2018-04-21 19:43,2018-04-24 12:04,2018-04-24 12:31,3,3,Basic,14
50177907,com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server,"I'm new in Databases just started to learn them. I have MySQL Server 8.0., Workbench 8.0, Java connector 5.1.31, Java 1.8 itself. Followed multiple guides for newbies how to start with. 
So there is the case. I have database on localhost and successfully connect to it with workbench and via windows prompt. But after I execute code in java:
        Driver sqlDriver = new FabricMySQLDriver();
        DriverManager.registerDriver(sqlDriver);
        Connection sqlConnection = DriverManager.getConnection(""jdbc:mysql://localhost:3306/?user=root"", ""root"", ""root"");
I get exception com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server.
Why it could happen? I've tried to reinstall MySQL, Cleaned OS variables, looked everywhere and couldn't find solution. Would be glad to find it here.
UPD:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.Util.getInstance(Util.java:383)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1023)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2407)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2328)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:832)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:408)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:417)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:344)
    at java.sql.DriverManager.getConnection(DriverManager.java:664)
    at java.sql.DriverManager.getConnection(DriverManager.java:247)
    at com.company.Main.main(Main.java:11)
Caused by: java.lang.NullPointerException
    at com.mysql.jdbc.ConnectionImpl.getServerCharacterEncoding(ConnectionImpl.java:3309)
    at com.mysql.jdbc.MysqlIO.sendConnectionAttributes(MysqlIO.java:1985)
    at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1911)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1288)
    at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2508)
    at com.mysql.jdbc.ConnectionImpl.connectWithRetries(ConnectionImpl.java:2346)
    ... 13 more
",<java><mysql><jdbc>,3352,0,37,471,1,4,11,72,88224,0,35,2,29,2018-05-04 15:02,2018-05-05 7:21,,1,,Basic,12
54824209,Indexing JSONField in Django PostgreSQL,"I am using a simple model with an attribute that stores all the data for that object in a JSONField. Think of it as way to transfer NoSQL data to my PostgreSQL database. Kinda like this:
from django.contrib.postgres.fields import JSONField   
class Document(models.Model):
    content = JSONField()
Each Document object has (more or less) the same keys in its content field, so I am querying and ordering those documents using those keys. For the querying and ordering, I am using Django's annotate() function. I recently came across this:
https://docs.djangoproject.com/en/2.1/ref/contrib/postgres/indexes/
I also know that PostgreSQL using JSONB, which is apparently indexible. So my question is this: Can I index my content field somehow to make my read operations faster for complex queries? And if so, then how do I do it? The documentation page I linked has no examples.
",<django><postgresql>,877,2,8,8342,21,74,153,45,7275,0,320,3,29,2019-02-22 9:43,2020-09-22 16:45,2022-11-29 19:50,578,1376,Basic,1
49797786,How to get cursor in SQLAlchemy,"I am newbie in Python Flask. In my project we are creating db object using below code.    
    app = Flask(__name__)  
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'  
    db = SQLAlchemy(app)   
I want to get cursor object from db. Can someone please help me on it. 
I know using connection object we can get cursor. But can we get cursor from db object which is created by above way? Thanks. 
",<python><flask><sqlalchemy><flask-sqlalchemy><database-cursor>,417,0,3,1519,3,13,19,48,57414,0,101,4,29,2018-04-12 13:25,2018-04-12 20:15,2018-04-13 11:50,0,1,Basic,3
51534758,Visual Studio Code SQL Syntax Highlighting in .py Files,"I am switching over from atom to VSCode and finding it to be a way better experience for (mostly) python.
One thing I can't seem to work out is that the python syntax highlighting on atom recognised SQL in strings and highlighted it.
I can't seem to find an extension for VSCode to do the same thing.
Does one exist or is there a way to get this highlighting in VSCode?
",<python><sql><visual-studio-code><syntax-highlighting>,370,2,0,553,1,5,17,39,10424,0,27,4,29,2018-07-26 8:50,2019-03-26 23:27,,243,,Intermediate,20
53471882,"MySQL Workbench reports ""is not valid at this position for this server version"" error","For the following SQL query:
SELECT COUNT (distinct first_name) from actor;
I receive the following error message:
""SELECT"" is not valid at this position for this server version, expecting: '(', WITH
I am a total newbie at SQL. How do I resolve this error?
I put the exact same line at another PC with the exact same schema and it worked fine.
",<mysql><mysql-workbench>,344,0,2,292,1,3,6,45,150690,0,1,4,28,2018-11-25 20:54,2018-11-25 21:11,2018-11-25 21:11,0,0,Basic,9
59607616,When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?,"Most applications I have programmed do not use MultipleActiveResultSets=True, but I have seen the option being enabled in a couple of them and in a few tutorials.
This SO question deals with the same topic, but it is very old and I believe that things have changed much in the mean time.
OP argues about executing some non-queries, while performing an ExecuteReader. In this case I believe it to be a bad design since it might be replaced with some batch-style operation, perhaps a stored procedure to minimize the number of round-trips.
When using Entity Framework with ASP.NET Core and receiving an exception related to the data context executing already something in the scope, I treat it as a bug and not thinking about enabling MARS.
Reading this MS Docs article I see that one should pay attention to various aspects such as options (ANSI_NULLS, DATE_FORMAT, LANGUAGE, TEXTSIZE), security context, current database, state variables (@@ERROR, @@ROWCOUNT, @@FETCH_STATUS, @@IDENTITY) when working with MARS enabled.
Also, 10+ years mean much more capable servers being able to hold much more connections if this is really needed (caching should help reduce this need).
So I am wondering if I ever have to consider enabling MARS when working with modern ASP.NET Core applications (3.0+).
Question: When should I use MultipleActiveResultSets=True when working with ASP.NET Core 3.0 and SQL Server 2019+?
Edit to address feedback
I am not interested in an exhaustive analysis, but a couple of appropriate contexts to justify using MARS or not.
A typical example in ASP.NET Core applications is to have database context as scoped (get a database connection from the connection pool per request, make changes, usually one transaction per request/scope). So far, I have treated errors related to multiple queries per connection as my own fault to avoid MARS, but I did so without understanding actually why.
",<sql-server><asp.net-core><entity-framework-core><sql-server-mars>,1906,2,10,22299,19,146,170,40,12139,0,7185,1,28,2020-01-06 6:30,2021-04-10 17:19,2021-04-10 17:19,460,460,Basic,9
52802521,How can I get time.Time in Golang protobuf v3 struct?,"I'm using the google time package github.com/golang/protobuf/ptypes/timestamp in protobuf message file  now.
google.protobuf.Timestamp UpdateTime = 9;
But the UpdateTime property becomes a pointer *timestamp.Timestamp in golang struct after protoc compiling, it's not a time.Time and I can't save these property into Mysql timestamp column.
What can I do?
",<mysql><go><timestamp><protocol-buffers>,356,0,5,427,1,5,6,59,44394,0,0,2,28,2018-10-14 12:14,2018-10-14 14:53,,0,,Basic,9
53623048,Restore database in docker container,"Getting an error below when restoring a AdventureWorks2017 database within a docker container.
Running SQL Server 2019 CTP 2.0 (mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Both backup and target data volume are persisted. 
No problems creating new database. 
Checked the paths and they are correct. Do not have any problems when restoring using 2017-latest docker image.
Anybody else have this issue with 2019-CTP2, workarounds?
  Msg 3634, Level 16, State 1, Line 7 The operating system returned the
  error '2(The system cannot find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017' cannot be restored to
  '/var/opt/mssql/data/AdventureWorks2017.mdf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3634, Level 16, State 1,
  Line 7 The operating system returned the error '2(The system cannot
  find the file specified.)' while attempting
  'RestoreContainer::ValidateTargetForCreation' on
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Msg 3156, Level 16,
  State 5, Line 7 File 'AdventureWorks2017_log' cannot be restored to
  '/var/opt/mssql/log/AdventureWorks2017_log.ldf'. Use WITH MOVE to
  identify a valid location for the file. Msg 3119, Level 16, State 1,
  Line 7 Problems were identified while planning for the RESTORE
  statement. Previous messages provide details. Msg 3013, Level 16,
  State 1, Line 7 RESTORE DATABASE is terminating abnormally.
to create container.
$datapath = ""D:\Foo"";
$logpath = ""D:\Foo"";
$backuppath = ""D:\Foo"";
$pass = "":-)""
$ct = (docker run -e ""ACCEPT_EULA=Y"" -e ""SA_PASSWORD=$pass"" `
    -e ""MSSQL_PID=Developer"" -p 2017:1433 `
    -e ""MSSQL_TCP_PORT=1433"" `
    -v ${datapath}:/var/opt/mssql/data `
    -v ${logpath}:/var/opt/mssql/log `
    -v ${backuppath}:/var/opt/mssql/backup `
    -e ""MSSQL_BACKUP_DIR=/var/opt/mssql/backup"" `
    -e ""MSSQL_DATA_DIR=/var/opt/mssql/data"" ` 
    -e ""MSSQL_LOG_DIR=/var/opt/mssql/log"" `
    -d mcr.microsoft.com/mssql/server:vNext-CTP2.0-ubuntu)
Restore command.
RESTORE DATABASE [AdventureWorks2017] FROM  DISK = N'/var/opt/mssql/backup/AdventureWorks2017.bak' 
WITH  FILE = 1,  
MOVE N'AdventureWorks2017' TO N'/var/opt/mssql/data/AdventureWorks2017.mdf',  
MOVE N'AdventureWorks2017_log' TO N'/var/opt/mssql/log/AdventureWorks2017_log.ldf', 
NOUNLOAD,  STATS = 1 
",<sql-server><docker><sql-server-2019>,2430,0,20,1142,1,8,14,50,15652,0,52,7,28,2018-12-04 23:41,2018-12-05 5:40,2018-12-14 1:39,1,10,Basic,14
51117503,"Python 3.7, Failed building wheel for MySql-Python","I am new to python and I am trying django framework that involves some MySql and ran into this error when try to do pip install mysqlclient and down the lines of cmd messages I got this.
   Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.7
    copying _mysql_exceptions.py -&gt; build\lib.win32-3.7
    creating build\lib.win32-3.7\MySQLdb
    copying MySQLdb\__init__.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\compat.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\connections.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\converters.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\cursors.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\release.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\times.py -&gt; build\lib.win32-3.7\MySQLdb
    creating build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\__init__.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CR.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\ER.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FLAG.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    running build_ext
    building '_mysql' extension
    creating build\temp.win32-3.7
    creating build\temp.win32-3.7\Release
    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\bin\HostX86\x86\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win32-3.7\Release\_mysql.obj /Zl
    _mysql.c
    _mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe' failed with exit status 2
&gt; 
&gt; 
&gt; Command ""c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient"" failed with error code 1 in C:\Users\RONANL~1\AppData\Local\Temp\pip-install-pkbqy3t3\mysqlclient\
anyone knows how to fix this ?
",<python><mysql><django><python-3.x><mysql-python>,4457,0,40,389,1,4,13,59,90537,0,27,13,28,2018-06-30 18:27,2018-10-04 11:24,,96,,Basic,14
49228926,Get city name either do not start with vowels or do not end with vowels,"Query the list of CITY names from STATION that either do not start with vowels or do not end with vowels. Your result cannot contain duplicates.
Input Format
The STATION table is described as follows:
I write the below query, but it's not working for me. Any suggestion?
select distinct city
from station
where city regexp '^[^aeiou].*[^aeiou]$'; 
",<sql>,348,1,3,407,1,4,5,65,171270,0,0,50,28,2018-03-12 5:55,2018-03-12 6:07,2018-03-12 6:07,0,0,Basic,10
54690701,Is there a way to ensure WHERE clause happens after DISTINCT?,"Imagine you have a table comments in your database.
The comment table has the columns, id, text, show, comment_id_no.
If a user enters a comment, it inserts a row into the database
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
If a user wants to update that comment it inserts a new row into the db
| id |  comment_id_no | text | show | inserted_at |
| -- | -------------- | ---- | ---- | ----------- |
| 1  | 1              | hi   | true | 1/1/2000    |
| 2  | 1              | hey  | true | 1/1/2001    |
Notice it keeps the same comment_id_no. This is so we will be able to see the history of a comment.
Now the user decides that they no longer want to display their comment
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
This hides the comment from the end users. 
Now a second comment is made (not an update of the first)
| id |  comment_id_no | text | show  | inserted_at |
| -- | -------------- | ---- | ----- | ----------- |
| 1  | 1              | hi   | true  | 1/1/2000    |
| 2  | 1              | hey  | true  | 1/1/2001    |
| 3  | 1              | hey  | false | 1/1/2002    |
| 4  | 2              | new  | true  | 1/1/2003    |
What I would like to be able to do is select all the latest versions of unique commend_id_no, where show is equal to true. However, I do not want the query to return id=2.
Steps the query needs to take...
select all the most recent, distinct comment_id_nos. (should return id=3 and id=4)
select where show = true (should only return id=4)
  Note: I am actually writing this query in elixir using ecto and would like to be able to do this without using the subquery function. If anyone can answer this in sql I can convert the answer myself. If anyone knows how to answer this in elixir then also feel free to answer. 
",<sql><postgresql><elixir><distinct><where-clause>,2091,0,31,1624,1,17,28,42,2481,0,344,5,28,2019-02-14 12:43,2019-02-14 12:45,2019-02-14 12:58,0,0,Basic,10
51278467,"MySQL collation: utf8mb4_unicode_ci vs ""utf8mb4 - default collation""","Please help me to understand the differences between the collations listed in MySQL Workbench:
utf8mb4_unicode_ci vs utf8mb4 - default collation
p.s. Everyone is recommending using utf8mb4_unicode_ci. If this is so popular why it is not default? What differs it from the default?
I use MySQL 5.7.21.
",<mysql><mysql-workbench><collation>,300,1,3,38596,26,175,195,35,25212,0,2983,1,28,2018-07-11 6:19,2018-07-11 15:18,,0,,Intermediate,19
51959944,"SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1","I am getting below exception only in android 9, after reinstalling everything looks good,
Exception:
android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1...
Code:
Cursor cursor = database.query(......);
    if(cursor == null || cursor.getCount() &lt; 0) { //Here is the error
        Log.d(""Error"", ""count : null"");
        return """";
    }
Edited:
java.lang.RuntimeException: An error occurred while executing doInBackground()
at android.os.AsyncTask$3.done(AsyncTask.java:354)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:383)
at java.util.concurrent.FutureTask.setException(FutureTask.java:252)
at java.util.concurrent.FutureTask.run(FutureTask.java:271)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
at java.lang.Thread.run(Thread.java:764)
Caused by: android.database.sqlite.SQLiteBlobTooBigException: Row too big to fit into CursorWindow requiredPos=0, totalRows=1
at android.database.sqlite.SQLiteConnection.nativeExecuteForCursorWindow(Native Method)
at android.database.sqlite.SQLiteConnection.executeForCursorWindow(SQLiteConnection.java:859)
at android.database.sqlite.SQLiteSession.executeForCursorWindow(SQLiteSession.java:836)
at android.database.sqlite.SQLiteQuery.fillWindow(SQLiteQuery.java:62)
at android.database.sqlite.SQLiteCursor.fillWindow(SQLiteCursor.java:149)
at android.database.sqlite.SQLiteCursor.getCount(SQLiteCursor.java:137)
Thanks in advance guys
",<android><sqlite><android-database>,1586,0,22,322,1,3,10,74,30636,,9,4,28,2018-08-22 4:35,2019-04-24 10:48,,245,,Intermediate,24
50641841,MySQL Docker Container INFILE/ INTO OUTFILE statement on MacOS System,"I hava a Java program and a mysql Docker container (image: mysql:5.7.20).
My MacOs is High Sierra 10.13.4.
The problem in short
Using Docker on MacOS (10.13.4.). Inside a docker container (image: mysql:5.7.20) mostly the queries (executed from a java program)
LOAD DATA INFILE ...
SELECT ... INTO OUTFILE ...
are working fine, but sometimes the java program throws the exceptions:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
btw. the file exists and the permissions should be fine (see longer version)
The longer version
The process is the following:
a .csv file gets created
this .csv file is copied into a directory, which is mounted for the docker container
docker-compose volumes section: - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
then MySQL reads this .csv file into a table:
LOAD DATA INFILE '.csv-file' REPLACE INTO TABLE 'my-table';
then some stuff on that database happens
after that MySQL writes an .csv output file
SELECTtbl.sku,tbl.deleted,tbl.data_source_valuesINTO OUTFILE 'output.csv' FIELDS TERMINATED BY '|' ENCLOSED BY '""' ESCAPED BY '""' FROM (SELECT ...
This project has some java integration-tests for this process. These tests are mostly green, but sometimes they fail with:
SQLException: Can't create/write to file ‘…’ (Errcode: 2 - No such file or directory)
SQLException: Can't get stat of ‘…’ Errcode: 2 - No such file or directory)
SQLException: The MySQL server is running with the --secure-file-priv option so it cannot execute this statement
SQLException: File ‘…’ not found (Errcode: 2 - No such file or directory)
The docker-compose file looks like:
version: '3'
  services:
    datahub_db:
      image: ""mysql:5.7.20""
      restart: always
      environment:
        - MYSQL_ROOT_PASSWORD=${DATAHUB_DB_ROOT_PASSWORD}
        - MYSQL_DATABASE=${DATAHUB_DB_DATABASE}
      volumes:
        - ""datahub_db:/var/lib/mysql""
        - ""./data/datahub/import:/var/lib/mysql-files/datahub/import""
        - ""./data/akeneo/import:/var/lib/mysql-files/akeneo/import""
      ports:
        - ""${DATAHUB_DB_PORT}:3306""
...
volumes:
  datahub_db:
The Log from that Docker database container shows the following (but sometimes, this happened when all test are green, too)
datahub_db_1  | 2018-06-01T10:04:33.937646Z 144 [Note] Aborted connection 144 to db: 'datahub_test' user: 'root' host: '172.18.0.1' (Got an error reading communication packets)
The .csv file inside the datahub container, shows the following, fo ls -lha
root@e02e2074fb6b:/var/lib/mysql- 
files/datahub/import/test/products/kaw# ls -lha
total 4.0K
drwxr-xr-x 3 root root  96 Jun  1 09:36 .
drwxr-xr-x 3 root root  96 Jun  1 09:36 ..
-rw-r--r-- 1 root root 378 Jun  1 06:47 deactivated_product_merged_bub.csv
I think there is no problem, that this file belongs to root because mostly this file can get read by MySQL. When I change to user mysql via su mysql inside the Docker container, I get the following:
$ ls -al
total 4
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 .
drwxr-xr-x 3 mysql mysql  96 Jun  1 09:36 ..
-rw-r--r-- 1 mysql mysql 378 Jun  1 06:47 deactivated_product_merged_bub.csv
Now some strange stuff happened. 
with root user, i could make a cat deactivated_product_merged_bub.csv
with mysql user i couldn't i got: 
Output:
$ cat deactivated_product_merge_bub.csv
cat: deactivated_product_merge_bub.csv: No such file or directory
I made a stat deactivated_product_merged_bub.csv as MySQL user and suddenly I could make a cat on that file (as you see I chmod 777 to that file to get the cat working - but it didn't work).
stat as root
Output:
root@e02e2074fb6b:/var/lib/mysql-files/datahub/import/test/products/kaw# stat 
deactivated_product_merged_bub.csv 
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2018-06-01 09:23:38.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
stat as mysql user
Output:
$ stat deactivated_product_merged_bub.csv
  File: 'deactivated_product_merged_bub.csv'
  Size: 378         Blocks: 8          IO Block: 4194304 regular file
Device: 4fh/79d Inode: 4112125     Links: 1
Access: (0777/-rwxrwxrwx)  Uid: (  999/   mysql)   Gid: (  999/   mysql)
Access: 2018-06-01 09:32:25.000000000 +0000
Modify: 2018-06-01 06:47:44.000000000 +0000
Change: 2018-06-01 09:04:53.000000000 +0000
  Birth: -
Question
Does anyone knows, what happened here or has a hint for what I could search to dig deeper?
My speculation is that it's because using Docker with MacOs and the mounted volume.
",<java><mysql><docker><docker-compose>,4985,0,73,680,0,9,21,65,3956,0,63,3,28,2018-06-01 10:51,2018-10-17 20:55,,138,,Intermediate,24
52697734,Postgresql | remaining connection slots are reserved for non-replication superuser connections,"I am getting an error ""remaining connection slots are reserved for non-replication superuser connections"" at one of PostgreSQL instances.
However, when I run below query from superuser to check available connections, I found that enough connections are available. But still getting the same error.
select max_conn,used,res_for_super,max_conn-used-res_for_super 
res_for_normal 
from 
  (select count(*) used from pg_stat_activity) t1,
  (select setting::int res_for_super from pg_settings where 
name='superuser_reserved_connections') t2,
  (select setting::int max_conn from pg_settings where name='max_connections') t3
Output
I searched this error and everyone is suggesting to increase the max connections like below link.
Heroku &quot;psql: FATAL: remaining connection slots are reserved for non-replication superuser connections&quot;
EDIT
I restarted the server and after some time used connections were almost 210 but i was able to connect to the server from a normal user. 
",<postgresql><postgresql-9.6>,982,2,7,1636,2,22,44,73,19593,0,105,2,28,2018-10-08 7:54,2021-01-03 18:35,,818,,Basic,6
53730591,Sqlite DB Locked on Azure Dotnet Core Entity Framework,"I have a simple asp.net core web app (v2.1), that I deployed to a B1 (and I tried B2) Azure App Service on Linux. When I call dbContext.SaveChanges(), after adding one very simple entity, the request takes about 30 seconds before throwing the following error:
Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
Here is the code. _dbContext is injected with scoped lifetime.
public async Task&lt;IActionResult&gt; SignIn([Bind(""Email,Password,RedirectUrl"")] SignInModel model) {
    if (ModelState.IsValid) {
        var user = _dbContext.Users.Include(u =&gt; u.Claims).FirstOrDefault(u =&gt; u.UserName == model.Email);
        ...
        user.LastLogin = DateTimeOffset.Now;
        await _dbContext.SaveChangesAsync();
        ...
        return Redirect(String.IsNullOrWhiteSpace(model.RedirectUrl) ? ""/"" : model.RedirectUrl); 
    }
    else {
        return View(model);
    }
}
During the 30 seconds, I see, via SSH, that a journal file exists beside by SQLite DB file. It's eventually deleted.
UPDATE: Here are the logs. You can see, that after a single update call, a lock exception is thrown exactly 30 seconds later. 30 seconds is the SQL command timeout. I'm watching the file system using a remote SSH shell, and the journal file is there for ~30 seconds. It's like the combination of the network share used by the app service, and the SQLite file locking logic, is broken or very slow.
2018-12-20T15:06:27.660624755Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[1]
2018-12-20T15:06:27.660656156Z       Request starting HTTP/1.1 POST http://insecuresite.azurewebsites.net/Account/SignIn application/x-www-form-urlencoded 56
2018-12-20T15:06:27.660797960Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660875561Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:Constructor called.
2018-12-20T15:06:27.660885462Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.660890662Z [15:06:27 DBG] InsecureSite.Middleware.MyCookieAuthHandler:HandleAuthenticateAsync called.
2018-12-20T15:06:27.661837484Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.661856585Z       Route matched with {action = ""SignIn"", controller = ""Account""}. Executing action InsecureSite.Controllers.AccountController.SignIn (InsecureSite)
2018-12-20T15:06:27.662465200Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:27.662478400Z       Executing action method InsecureSite.Controllers.AccountController.SignIn (InsecureSite) with arguments (InsecureSite.Models.SignInModel) - Validation state: Valid
2018-12-20T15:06:27.667736726Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Infrastructure[10403]
2018-12-20T15:06:27.667751427Z       Entity Framework Core 2.1.4-rtm-31024 initialized 'AppDbContext' using provider 'Microsoft.EntityFrameworkCore.Sqlite' with options: None
2018-12-20T15:06:27.716864407Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.716886507Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.716892507Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.776374136Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.776410837Z       Executed DbCommand (59ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.776514640Z       SELECT ""u"".""UserId"", ""u"".""FirstName"", ""u"".""LastLogin"", ""u"".""LastName"", ""u"".""PasswordHash"", ""u"".""UserName""
2018-12-20T15:06:27.776526140Z       FROM ""Users"" AS ""u""
2018-12-20T15:06:27.776531140Z       WHERE ""u"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.776536040Z       ORDER BY ""u"".""UserId""
2018-12-20T15:06:27.776540740Z       LIMIT 1
2018-12-20T15:06:27.778553489Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.778567689Z       Executed DbCommand (1ms) [Parameters=[@__model_Email_0='?' (Size = 18)], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.778840096Z       SELECT ""u.Claims"".""UserClaimId"", ""u.Claims"".""Claim"", ""u.Claims"".""UserId"", ""u.Claims"".""Value""
2018-12-20T15:06:27.778852696Z       FROM ""UserClaims"" AS ""u.Claims""
2018-12-20T15:06:27.778857796Z       INNER JOIN (
2018-12-20T15:06:27.778862596Z           SELECT ""u0"".""UserId""
2018-12-20T15:06:27.778869696Z           FROM ""Users"" AS ""u0""
2018-12-20T15:06:27.778874396Z           WHERE ""u0"".""UserName"" = @__model_Email_0
2018-12-20T15:06:27.778879897Z           ORDER BY ""u0"".""UserId""
2018-12-20T15:06:27.780228429Z           LIMIT 1
2018-12-20T15:06:27.780242129Z       ) AS ""t"" ON ""u.Claims"".""UserId"" = ""t"".""UserId""
2018-12-20T15:06:27.780247829Z       ORDER BY ""t"".""UserId""
2018-12-20T15:06:27.789636555Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.789651955Z       Executed DbCommand (0ms) [Parameters=[], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.789657656Z       PRAGMA foreign_keys=ON;
2018-12-20T15:06:27.794111763Z [40m[32minfo[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Command[20101]
2018-12-20T15:06:27.794126763Z       Executed DbCommand (4ms) [Parameters=[@p1='?', @p0='?'], CommandType='Text', CommandTimeout='30']
2018-12-20T15:06:27.794132363Z       UPDATE ""Users"" SET ""LastLogin"" = @p0
2018-12-20T15:06:27.794280267Z       WHERE ""UserId"" = @p1;
2018-12-20T15:06:27.794298667Z       SELECT changes();
2018-12-20T15:06:57.833069471Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Database.Transaction[20205]
2018-12-20T15:06:57.833107571Z       An error occurred using a transaction.
2018-12-20T15:06:57.833113572Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.833118772Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.833123772Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.833128672Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.833133672Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.833138672Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.833143372Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.853805669Z [41m[30mfail[39m[22m[49m: Microsoft.EntityFrameworkCore.Update[10000]
2018-12-20T15:06:57.853833569Z       An exception occurred in the database while saving changes for context type 'InsecureSite.Data.AppDbContext'.
2018-12-20T15:06:57.853928072Z       Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.853938272Z          at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.853943572Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854041474Z          at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854051475Z          at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854056675Z          at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854137377Z          at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854146577Z          at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854208178Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854283180Z          at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854292080Z          at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854299081Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.854366282Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.854384483Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.854389683Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.854455384Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.854463685Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.854468185Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.854529686Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.854652489Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.854673890Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.854748391Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.858109772Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.858123973Z       Executed action InsecureSite.Controllers.AccountController.SignIn (InsecureSite) in 30193.6715ms
2018-12-20T15:06:57.860885139Z [41m[30mfail[39m[22m[49m: Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware[1]
2018-12-20T15:06:57.860899939Z       An unhandled exception has occurred while executing the request.
2018-12-20T15:06:57.860905239Z Microsoft.Data.Sqlite.SqliteException (0x80004005): SQLite Error 5: 'database is locked'.
2018-12-20T15:06:57.861009242Z    at Microsoft.Data.Sqlite.SqliteException.ThrowExceptionForRC(Int32 rc, sqlite3 db)
2018-12-20T15:06:57.861018942Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteReader(CommandBehavior behavior)
2018-12-20T15:06:57.861023842Z    at Microsoft.Data.Sqlite.SqliteCommand.ExecuteNonQuery()
2018-12-20T15:06:57.861120545Z    at Microsoft.Data.Sqlite.SqliteConnectionExtensions.ExecuteNonQuery(SqliteConnection connection, String commandText)
2018-12-20T15:06:57.861130145Z    at Microsoft.Data.Sqlite.SqliteTransaction.Commit()
2018-12-20T15:06:57.861134745Z    at Microsoft.EntityFrameworkCore.Storage.RelationalTransaction.Commit()
2018-12-20T15:06:57.861237547Z    at Microsoft.EntityFrameworkCore.Update.Internal.BatchExecutor.ExecuteAsync(DbContext _, ValueTuple`2 parameters, CancellationToken cancellationToken)
2018-12-20T15:06:57.861311249Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(IReadOnlyList`1 entriesToSave, CancellationToken cancellationToken)
2018-12-20T15:06:57.861320149Z    at Microsoft.EntityFrameworkCore.ChangeTracking.Internal.StateManager.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861392851Z    at Microsoft.EntityFrameworkCore.DbContext.SaveChangesAsync(Boolean acceptAllChangesOnSuccess, CancellationToken cancellationToken)
2018-12-20T15:06:57.861401851Z    at InsecureSite.Controllers.AccountController.SignIn(SignInModel model) in /home/site/repository/InsecureSite/Controllers/AccountController.cs:line 57
2018-12-20T15:06:57.861463253Z    at Microsoft.AspNetCore.Mvc.Internal.ActionMethodExecutor.TaskOfIActionResultExecutor.Execute(IActionResultTypeMapper mapper, ObjectMethodExecutor executor, Object controller, Object[] arguments)
2018-12-20T15:06:57.861472053Z    at System.Threading.Tasks.ValueTask`1.get_Result()
2018-12-20T15:06:57.861541855Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()
2018-12-20T15:06:57.861550055Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()
2018-12-20T15:06:57.861554655Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)
2018-12-20T15:06:57.861629257Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861639057Z    at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()
2018-12-20T15:06:57.861718659Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextResourceFilter()
2018-12-20T15:06:57.861727059Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Rethrow(ResourceExecutedContext context)
2018-12-20T15:06:57.861791861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.Next(State&amp; next, Scope&amp; scope, Object&amp; state, Boolean&amp; isCompleted)
2018-12-20T15:06:57.861800861Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeFilterPipelineAsync()
2018-12-20T15:06:57.861805561Z    at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeAsync()
2018-12-20T15:06:57.861810161Z    at Microsoft.AspNetCore.Builder.RouterMiddleware.Invoke(HttpContext httpContext)
2018-12-20T15:06:57.861880363Z    at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861888363Z    at Microsoft.AspNetCore.StaticFiles.StaticFileMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.861948164Z    at Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware.Invoke(HttpContext context)
2018-12-20T15:06:57.862056667Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.862066767Z       Route matched with {action = ""Error"", controller = ""Home""}. Executing action InsecureSite.Controllers.HomeController.Error (InsecureSite)
2018-12-20T15:06:57.867899207Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[1]
2018-12-20T15:06:57.867914108Z       Executing action method InsecureSite.Controllers.HomeController.Error (InsecureSite) - Validation state: Valid
2018-12-20T15:06:57.868025910Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.868045011Z       Executed action method InsecureSite.Controllers.HomeController.Error (InsecureSite), returned result Microsoft.AspNetCore.Mvc.ViewResult in 0.0771ms.
2018-12-20T15:06:57.868147613Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[1]
2018-12-20T15:06:57.868157914Z       Executing ViewResult, running view Error.
2018-12-20T15:06:57.869182938Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.ViewFeatures.ViewResultExecutor[4]
2018-12-20T15:06:57.869196139Z       Executed ViewResult - view Error executed in 7.5623ms.
2018-12-20T15:06:57.869201439Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker[2]
2018-12-20T15:06:57.869206339Z       Executed action InsecureSite.Controllers.HomeController.Error (InsecureSite) in 7.9125ms
2018-12-20T15:06:57.869222639Z [40m[32minfo[39m[22m[49m: Microsoft.AspNetCore.Hosting.Internal.WebHost[2]
2018-12-20T15:06:57.869228039Z       Request finished in 30208.5835ms 500 text/html; charset=utf-8
2018-12-20T15:08:44  No new trace in the past 1 min(s).
There is not any other requests hitting this web app. I am the only one making requests.
Controllers can read data from the DB file without errors. I see that a couple queries right before the failed write take 3ms and 6ms.
I saw there was an issue with locking and multiple threads, but it is fixed, and I'm on a later version from the fix (2.1.4)
",<linux><azure><docker><sqlite><entity-framework-core>,16441,2,143,3407,1,23,26,45,3526,0,152,4,28,2018-12-11 18:51,2018-12-20 8:19,2022-07-27 10:12,9,1324,Intermediate,23
56389698,Why SUPER privileges are disabled when binary logging option is on?,"there are lot of recommendations over the Internet on how to enable SUPER privileges in case if someone hit the following error: 
  ""ERROR 1419 (HY000): You do not have the SUPER Privilege and Binary Logging is Enabled""
But I wasn't be able to find WHY MySQL disables these privileges when binary logging option is on.
Are there some issues with replication if I use e.g. triggers which modify DB or something else? Whether it's safe and, if no, what kind of issues and under which circumstances I can hit if I will return SUPER privileges back? I think there should be some rationale behind this restriction but don't understand which one.
Does anybody have an answer on this?
Thank you.
",<mysql><mariadb>,689,0,0,433,1,4,9,58,86520,0,10,4,28,2019-05-31 6:22,2019-05-31 6:45,2019-05-31 6:45,0,0,Advanced,33
52324170,AWS RDS for PostgreSQL cannot be connected after several hours,"I created several instances of RDS with PostgreSQL and get the same problems:
I can connect to all of them right after creating the instances.
After several hours (I stop working on it, turn off my laptop), I cannot connect to any of them again.
I use DBeaver for the connections, the error show is ""Connection attempt timed out.""
I attached the information of the
. Hope someone can help me with this problem. Thank you in advance.
",<postgresql><amazon-web-services>,433,1,0,871,2,8,15,46,34799,0,27,5,27,2018-09-14 2:26,2018-09-15 15:53,,1,,Advanced,33
51368395,Convert java.sql.Timestamp to Java 8 ZonedDateTime?,"Migrating Joda time to Java 8
Joda:
UserObject user = new UserObject()
user.setCreatedAt(new DateTime(rs.getTimestamp(""columnName"")));`
Migrating to Java 8
This is my code; it does compile; I am doubtful if it works:
ZonedDateTime.ofInstant(rs.getTimestamp(""columnName"").toLocalDateTime().toInstant(ZoneOffset.UTC),ZoneId.of(""UTC"")));
In some cases, the date is wrong. Any advice?
",<java><sql><java-8><java-time><zoneddatetime>,381,0,3,1981,9,36,58,44,30687,0,22,3,27,2018-07-16 18:52,2018-07-16 19:07,2018-07-18 11:35,0,2,Basic,10
53037124,Partitioning a large skewed dataset in S3 with Spark's partitionBy method,"I am trying to write out a large partitioned dataset to disk with Spark and the partitionBy algorithm is struggling with both of the approaches I've tried.
The partitions are heavily skewed - some of the partitions are massive and others are tiny.
Problem #1:
When I use repartition before partitionBy, Spark writes all partitions as a single file, even the huge ones
val df = spark.read.parquet(&quot;some_data_lake&quot;)
df
  .repartition('some_col).write.partitionBy(&quot;some_col&quot;)
  .parquet(&quot;partitioned_lake&quot;)
This takes forever to execute because Spark isn't writing the big partitions in parallel.  If one of the partitions has 1TB of data, Spark will try to write the entire 1TB of data as a single file.
Problem #2:
When I don't use repartition, Spark writes out way too many files.
This code will write out an insane number of files.
df.write.partitionBy(&quot;some_col&quot;).parquet(&quot;partitioned_lake&quot;)
I ran this on a tiny 8 GB data subset and Spark wrote out 85,000+ files!
When I tried running this on a production data set, one partition that has 1.3 GB of data was written out as 3,100 files.
What I'd like
I'd like for each partition to get written out as 1 GB files.  So a partition that has 7 GB of data will get written out as 7 files and a partition that has 0.3 GB of data will get written out as a single file.
What is my best path forward?
",<apache-spark><apache-spark-sql><partitioning>,1394,0,8,18540,11,106,109,39,10093,0,547,4,27,2018-10-28 23:52,2018-10-29 0:26,2018-10-29 0:26,1,1,Advanced,32
58658690,Retrieve query results as dict in SQLAlchemy,"I am using flask SQLAlchemy and I have the following code to get users from database with raw SQL query from a MySQL database:
connection = engine.raw_connection()
cursor = connection.cursor()
cursor.execute(""SELECT * from User where id=0"")
results = cursor.fetchall()
results variable is a tuple and I want it to be of type dict(). Is there a way to achieve this?
when I was using pymysql to build the db connection I was able to do 
cursor = connection.cursor(pymysql.cursors.DictCursor)
Is there something similar in SQLAlchemy?
Note: The reason I want to do this change is to get rid of using pymysql in my code, and only use SQLAlcehmy features, i.e. I do not want to have ´´´import pymysql´´´ in my code anywhere.
",<python><mysql><sqlalchemy><flask-sqlalchemy>,720,0,6,595,3,7,16,67,55259,0,56,4,27,2019-11-01 11:43,2019-11-01 14:07,2019-11-01 14:07,0,0,Basic,2
53045717,adapter Ecto.Adapters.Postgres was not compiled,"I am not able to create my Phoenix project. Would love some advice on how to fix it.
Setup details:
Ubuntu 16.04.4 LTS 
Erlang/OTP 21 [erts-10.1] [source] [64-bit]
[smp:1:1] [ds:1:1:10] [async-threads:1] [hipe] 
Elixir 1.7.3 (compiled
with Erlang/OTP 20)
Mix 1.7.3 (compiled with Erlang/OTP 20)
Ecto v3.0.0
I am following the Phoenix Up and Running to make an app. 
mix phx.new hello
cd hello
mix ecto.create
last command gives me:
 == Compilation error in file lib/hello/repo.ex ==
 ** (ArgumentError) adapter Ecto.Adapters.Postgres was not compiled, ensure it is correct and it is included as a project dependency
     lib/ecto/repo/supervisor.ex:71: Ecto.Repo.Supervisor.compile_config/2
     lib/hello/repo.ex:2: (module)
     (stdlib) erl_eval.erl:680: :erl_eval.do_apply/6
     (elixir) lib/kernel/parallel_compiler.ex:206: anonymous fn/4 in Kernel.ParallelCompiler.spawn_workers/6
I have postgres installed. I have postgres super user. 
",<postgresql><elixir><phoenix-framework><ecto>,944,1,9,507,0,5,13,76,5442,0,13,4,27,2018-10-29 12:40,2018-10-29 13:41,2018-10-29 13:41,0,0,Basic,3
50689082,to_sql pyodbc count field incorrect or syntax error,"I am downloading Json data from an api website and using sqlalchemy, pyodbc and pandas' to_sql function to insert that data into a MSSQL server.  
I can download up to 10000 rows, however I have to limit the chunksize to 10 otherwise I get the following error:
  DBAPIError: (pyodbc.Error) ('07002', '[07002] [Microsoft][SQL Server
  Native Client 11.0]COUNT field incorrect or syntax error (0)
  (SQLExecDirectW)') [SQL: 'INSERT INTO [TEMP_producing_entity_details]
There are around 500 Million rows to download, it's just crawling at this speed.  Any advice on a workaround?  
Thanks,
",<python><sql-server><pandas><pyodbc>,587,0,0,287,1,4,8,64,19746,0,3,4,27,2018-06-04 21:32,2018-06-05 17:55,2018-06-05 17:55,1,1,Basic,6
48555891,How to insert value into primary key column in SQL Server?,"insert into Student 
values('1', 'joedio', 'newyark', GETDATE())
I get this error message when trying to run this SQL:
  An explicit value for the identity column in table 'Student' can only be specified when a column list is used and IDENTITY_INSERT is ON.
",<sql-server><sql-server-2005>,258,0,2,323,3,5,14,45,119420,0,2,7,26,2018-02-01 5:25,2018-02-01 5:31,,0,,Basic,10
53523051,"ERROR: could not stat file ""XX.csv"": Unknown error","I run this command:
COPY XXX FROM 'D:/XXX.csv'  WITH (FORMAT CSV, HEADER TRUE, NULL 'NULL')
In Windows 7, it successfully imports CSV files of less than 1GB.
If the file is more then 1GB big, I get an &ldquo;unknown error&rdquo;.
[Code: 0, SQL State: XX000]  ERROR: could not stat file ""'D:/XXX.csv'  Unknown error
How can I fix this issue?
",<postgresql><large-files><postgresql-copy>,341,0,2,391,1,3,7,81,16764,0,0,8,26,2018-11-28 15:37,2018-11-29 7:08,,1,,Basic,10
58616005,Import Postgres data into RDS using S3 and aws_s3,"I'm having a hard time importing data from S3 into an RDS postgres instance. According to the docs, you can use this syntax:
aws_s3.table_import_from_s3 (
   table_name text, 
   column_list text, 
   options text, 
   bucket text, 
   file_path text, 
   region text, 
   access_key text, 
   secret_key text, 
   session_token text 
) 
So, in pgAdmin, I did this:
SELECT aws_s3.table_import_from_s3(
  'contacts_1', 
  'firstname,lastname,imported', 
  '(format csv)',
  'com.foo.mybucket', 
  'mydir/subdir/myfile.csv', 
  'us-east-2',
  'AKIAYYXUMxxxxxxxxxxx',
  '3zB4S5jb1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
);
I also tried it with an explicit NULL for the last parameter.
The error message I get is:
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 1 attempt(s) remaining
NOTICE:  CURL error code: 51 when attempting to validate pre-signed URL, 0 attempt(s) remaining
ERROR:  Unable to generate pre-signed url, look at engine log for details.
SQL state: XX000
I checked the server logs and there was no further information.
I have triple-checked the correctness of all the parameters. How do I make this work?
UPDATE:
I can confirm that I can do an s3.getObject() in the Java aws sdk using these same credentials.
",<postgresql><amazon-web-services><amazon-s3><amazon-rds>,1241,1,26,15339,27,93,159,36,27504,0,277,8,26,2019-10-29 22:02,2019-10-30 5:37,,1,,Basic,3
55661806,How to Create an Extension for SSMS 2019 (v18),"SQL Server Management Studio 18 RC1 became available March 28, 2018
This question has already been asked for SSMS 17, but there are slight variations when authoring extensions for different releases of SQL Server Management Studio.
What are the steps to getting a Hello World application up an running in SSMS 2019?
",<sql-server><visual-studio-extensions><ssms-addin><ssms-18>,316,2,0,32040,68,469,671,70,11230,0,13259,1,26,2019-04-13 3:52,2019-04-13 3:52,2019-04-13 3:52,0,0,Intermediate,27
48076425,Difference between N'String' vs U'String' literals in Oracle,"What is the meaning and difference between these queries?
SELECT U'String' FROM dual;
and
SELECT N'String' FROM dual;
",<sql><string><oracle><literals><string-literals>,118,0,2,496,3,8,18,42,14027,0,25,4,26,2018-01-03 11:25,2018-01-03 11:32,2018-01-06 19:37,0,3,Intermediate,19
50351517,Difference between two dates in dates using Google bigquery?,"How can I get the difference in days between 2 timestamp fields in Google Big Query? 
The only function I know is Datediff which only works in Legacy SQL but I'm in Standard SQL. 
For example: the difference between 20180115 to 20180220 is 36 days.
",<sql><google-bigquery><datediff><date-difference>,249,0,1,261,1,3,3,35,93665,0,0,2,26,2018-05-15 13:28,2018-05-15 14:48,,0,,Basic,2
60514629,"unrecognized configuration parameter ""default table access method"" google cloud","I try to import some files to a PostgreSQL database but I get this error:
Falha Importar: 
SET 
SET 
SET 
SET 
SET 
set_config ------------ 
(1 row) 
SET 
SET 
SET 
SET 
SET 
Import error: exit status 3 ERROR: unrecognized configuration parameter ""default_table_access_method""
",<postgresql><google-cloud-platform><google-cloud-sql>,277,0,14,491,1,4,12,35,25549,0,72,2,26,2020-03-03 19:59,2020-03-04 7:49,2020-03-04 7:49,1,1,Basic,14
53024891,ModuleNotFoundError: No module named 'MySQLdb',"After finishing of one of my Flask projects, I uploaded it on github just like everybody else. after a 2-3 months period I downloaded the entire githube repository on another machine to run it. However, the app is not working because the packages are not found giving the following message 
  ModuleNotFoundError: No module named 'Flask'
So I ended up downloading all packages starting from Flask, SQLalchemy,..etc! but I got stuck with MySQLdb:
(MYAPPENV) C:\Users\hp\myapp&gt;python run.py
Traceback (most recent call last):
  File ""run.py"", line 1, in &lt;module&gt;
    from app import app
  File ""C:\Users\hp\myapp\app\__init__.py"", line 4, in &lt;module&gt;
    from instance.config import engine
  File ""C:\Users\hp\myapp\instance\config.py"", line 52, in &lt;module&gt;
    engine = create_engine(""mysql://root:root@localhost/MYAPPDB"")
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\__init__.py"", line 425, in create_engine
return strategy.create(*args, **kwargs)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\engine\strategies.py"", line 81, in create
dbapi = dialect_cls.dbapi(**dbapi_args)
  File ""C:\Users\hp\AppData\Local\Programs\Python\Python37-32\lib\site-packages\sqlalchemy\dialects\mysql\mysqldb.py"", line 102, in dbapi
    return __import__('MySQLdb')
ModuleNotFoundError: No module named 'MySQLdb'
Could anybody please help with this issue? I am using python37 on windows machine. I even tried downloading packages such as mysqlclient,..etc but it didn't work out.
",<python><flask><mysql-python>,1580,0,16,451,1,4,9,68,78861,0,28,8,26,2018-10-27 18:11,2018-10-27 18:17,,0,,Basic,14
51775718,Is there a fundamental difference between INTERSECT and INNER JOIN?,"I understand, that INNER JOIN is made for referenced keys and INTERSECT is not. But afaik in some cases, both of them can do the same thing. So, is there a difference (in performance or anything) between the following two expressions? And if there is, which one is better?
Expression 1:
SELECT id FROM customers 
INNER JOIN orders ON customers.id = orders.customerID;
Expression 2:
SELECT id FROM customers
INTERSECT
SELECT customerID FROM orders
",<sql><inner-join><intersect><sql-except>,447,0,5,1196,1,9,31,59,51310,0,496,2,26,2018-08-09 21:02,2018-08-09 21:04,2018-08-09 21:04,0,0,Basic,8
49606889,Use dbms_output.put_line in Datagrip for .sql files,"I started to use Datagrip for my PL/SQL (school) projects that need the use of DBMS_OUTPUT.PUT_LINE. Before this I was using Oracle SQL developer and I was able to use DBMS_OUTPUT by adding the following: 
SET serveroutput ON;
There is a related question that shows how to enable or disable showing the contents of the DBMS_OUTPUT buffer but this only works for the Database Console tool window. How can I apply this to any .sql file? Currently, I am copying the content of my .sql files and run it in the Console tool window but there must be a better way.
",<oracle><plsql><datagrip><dbms-output>,558,2,2,1117,1,13,35,66,17012,0,464,8,26,2018-04-02 7:13,2018-04-02 7:46,2018-04-02 8:26,0,0,Basic,9
65301011,"JdbcTemplate ""queryForObject"" and ""query"" is deprecated in Spring. What should it be replaced by?","Query for object,
Student student = return jdbcTemplate.queryForObject(&quot;select * from student_id = ?&quot;, new Object[] { studentId }, studentRowMapper);
For query,
List&lt;Student&gt; students = return jdbcTemplate.query(&quot;select * from class_room_id = ?&quot;, new Object[] { classRoomId }, studentRowMapper);
Both jdbcTemplate.queryForObject and jdbcTemplate.query are deprecated in spring boot 2.4.X above
",<java><sql><spring><spring-boot><jdbctemplate>,420,0,4,8520,12,56,107,73,54152,0,591,2,26,2020-12-15 6:27,2020-12-15 6:42,2020-12-15 6:42,0,0,Basic,9
54377052,How to connect to WSL mysql from Host Windows,"I am trying to connect HeidiSql from the host  to my WSL Mysql but I could not get it to connect it 
Error ""can't connect to Mysql server on '127.0.0.1'""
Tried SSH too but could not connect to the server
",<mysql><ubuntu><windows-subsystem-for-linux>,204,1,0,271,1,3,4,66,48200,0,0,7,26,2019-01-26 9:10,2019-03-09 16:21,,42,,Basic,9
60255595,"If I cache a Spark Dataframe and then overwrite the reference, will the original data frame still be cached?","Suppose I had a function to generate a (py)spark data frame, caching the data frame into memory as the last operation.
def gen_func(inputs):
   df = ... do stuff...
   df.cache()
   df.count()
   return df
Per my understanding, Spark's caching works as follows:
When cache/persist plus an action (count())  is called on a data
frame, it is computed from its DAG and cached into memory, affixed
to the object which refers to it.
As long as a reference exists to that object, possibly within other functions/other scopes, the df will continue to be cached, and all DAGs that depend on the df will use the in-memory cached data as a starting point.
If all references to the df are deleted, Spark puts up the cache as memory to be garbage collected. It may not be garbage collected immediately, causing some short-term memory blocks (and in particular, memory leaks if you generate cached data and throw them away too fast), but eventually it will be cleared up.
My question is, suppose I use gen_func to generate a data frame, but then overwrite the original data frame reference (perhaps with a filter or a withColumn).
df=gen_func(inputs)
df=df.filter(&quot;some_col = some_val&quot;)
In Spark, RDD/DF are immutable, so the reassigned df after the filter and the df before the filter refer to two entirely different objects. In this case, the reference to the original df that was cache/counted has been overwritten. Does that mean that the cached data frame is no longer available and will be garbage collected? Does that mean that the new post-filter df will compute everything from scratch, despite being generated from a previously cached data frame?
I am asking this because I was recently fixing some out-of-memory issues with my code, and it seems to me that caching might be the problem. However, I do not really understand the full details yet of what are the safe ways to use cache, and how one might accidentally invalidate one's cached memory. What is missing in my understanding? Am I deviating from best practice in doing the above?
",<python><apache-spark><pyspark><apache-spark-sql>,2046,0,14,401,0,4,8,38,9816,0,4,2,26,2020-02-17 3:34,2020-12-21 20:09,,308,,Basic,5
51783300,Flask-Migrate No Changes Detected to Schema on first migration,"I'm using Flask with Flask-SQLAlchemy and Flask-Migrate to create an application, however when I try to create a migration nothing happens. 
I've created two tables in app/models.py:
from flask import current_app
from . import db
class Student(db.Model):
    __tablename__ = 'students'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, nullable=False)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Student, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Tutor {}&gt;' % self.id
class Tutor(db.Model):
    __tablename__ = 'tutors'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    def __init__(self, **kwargs):
        super(Tutor, self).__init__(**kwargs)
    def __repr__(self):
        return '&lt;Student %r&gt;' % self.id
Then I also have app/__init__.py with the following code:
from flask import Flask
from flask_bootstrap import Bootstrap
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
#from .models import User, Task, Project, UserProject
from config import config
bootstrap = Bootstrap()
db = SQLAlchemy()
migrate = Migrate()
def create_app(config_name='default'):
    #print config_name.name
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    config[config_name].init_app(app)
    bootstrap.init_app(app)
    db.init_app(app)
    migrate.init_app(app, db)
    ## Register the main blueprint for main app functionality
    from .main import main as main_blueprint
    app.register_blueprint(main_blueprint)
    return app
and app.py:
import os
from app import create_app, db
from app.models import Tutor, Student
app = create_app('default')
@app.shell_context_processor
def make_shell_context():
    return dict(db=db, Tutor=Tutor, Student=Student)
I can run flask db init with no problem and it creates the migrations directory and all necessary files with the following output:
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations ... done
Creating directory /Users/Jasmine/projects/flask/flask-tutoring/migrations/versions ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/script.py.mako ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/env.py ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/README ... done
Generating /Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini ... done
Please edit configuration/connection/logging settings in '/Users/Jasmine/projects/flask/flask-tutoring/migrations/alembic.ini' before proceeding.
but when I try and run flask db migrate alembic can't detect that I've got tables in app/models.py. I get the following output:
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.env] No changes in schema detected.
There is no migration script created, its as though models.py doesn't exist.
Apologies if this is a repeated question, but I can't find another example where its the first migration that fails and no migration script at all is created.
I've tried checking if there is already a table created somewhere by running db.drop_all() in the shell but that doesn't seem to be the problem.
UPDATE
I figured out a way to solve this on my own but would like a better understanding of why this worked.
I re-named app.py to flasktutor.py and re-ran export FLASK_APP='flasktutor.py'. Subsequently the migration worked perfectly.
Please could someone explain why when the file was called app.py and I used export FLASK_APP='app.py' the migration did not register changes to the schema.
",<python><python-3.x><flask><flask-sqlalchemy><flask-migrate>,3819,0,85,389,1,3,11,47,26711,0,0,10,26,2018-08-10 9:27,2018-09-05 9:34,,26,,Basic,14
50505893,Updating .NET framework resulting in SQL timeouts,"We have an app which targets .NET 4.5.1, and this has remained unchanged.
However when we upgraded the .NET framework on the server from 4.5.1 -> 4.7.1, we started experiencing SQL timeouts several hours afterwards (the app target remained at 4.5.1).
""Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.""
Other servers which had the same treatment produced the issue also, so we looked for a breaking change in .NET, and found this article: https://blogs.msdn.microsoft.com/dataaccesstechnologies/2016/05/07/connection-timeout-issue-with-net-framework-4-6-1-transparentnetworkipresolution/
That article quotes a different exception type, but might be somewhat related. However I'd be stunned if our DNS lookup took longer than 500ms. Also I'd expect to see far more cases of this connection string config reported and used.
Our app is high traffic, but we're confident we're not leaking connections as this has never been an issues for years until we updated the .NET framework. 
We're going to try applying this fix (and wait >24 hours to see the results), but is there anything else we could have missed? We're not confident this is the solution.
EDIT: Even after rolling .NET back to 4.5.1 and restart all servers, we're still seeing the problem. Nothing else has changed in the codebase, but we've yet to roll back a registry change which enabled 'SchUseStrongCrypto' - if that could be the cause?
",<c#><sql><.net><connection-pooling>,1538,2,1,4373,2,45,75,72,3032,0,472,1,26,2018-05-24 9:35,2018-06-02 20:54,,9,,Intermediate,23
51242938,"Spring Boot Application gets stuck on ""Hikari-Pool-1 - Starting...""","I'm trying to run Spring Boot application connected to PostgreSQL database. However, when it comes to Hikari connection pool initializing, it just gets stuck and nothing goes on. HikariPool-1 - Starting... appears in logs and then nothing happens. 
Logs:
2018-07-09 15:32:48.475  INFO 21920 --- [           main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068: startup date [Mon Jul 09 15:32:48 ALMT 2018]; root of context hierarchy
2018-07-09 15:32:48.736  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'configurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v2.0.3.RELEASE)
2018-07-09 15:32:49.207  INFO 21920 --- [           main] o.paperplane.todoapp.TodoappApplication  : No active profile set, falling back to default profiles: default
2018-07-09 15:32:49.217  INFO 21920 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@1b410b60: startup date [Mon Jul 09 15:32:49 ALMT 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@68999068
2018-07-09 15:32:49.763  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Dbcp2; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Dbcp2.class]] with [Root bean: class [null]; scope=; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.046  INFO 21920 --- [           main] o.s.b.f.s.DefaultListableBeanFactory     : Overriding bean definition for bean 'dataSource' with a different definition: replacing [Root bean: class [null]; scope=refresh; abstract=false; lazyInit=false; autowireMode=3; dependencyCheck=0; autowireCandidate=false; primary=false; factoryBeanName=org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari; factoryMethodName=dataSource; initMethodName=null; destroyMethodName=(inferred); defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]] with [Root bean: class [org.springframework.aop.scope.ScopedProxyFactoryBean]; scope=; abstract=false; lazyInit=false; autowireMode=0; dependencyCheck=0; autowireCandidate=true; primary=false; factoryBeanName=null; factoryMethodName=null; initMethodName=null; destroyMethodName=null; defined in BeanDefinition defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]]
2018-07-09 15:32:50.212  INFO 21920 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=2203ab9b-5bb0-34f2-b496-2cbda1e334a2
2018-07-09 15:32:50.342  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$ff61cd29] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.380  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration' of type [org.springframework.security.config.annotation.configuration.ObjectPostProcessorConfiguration$$EnhancerBySpringCGLIB$$980f9563] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.389  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'objectPostProcessor' of type [org.springframework.security.config.annotation.configuration.AutowireBeanFactoryObjectPostProcessor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.393  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.access.expression.method.DefaultMethodSecurityExpressionHandler@309028af' of type [org.springframework.security.oauth2.provider.expression.OAuth2MethodSecurityExpressionHandler] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.397  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration' of type [org.springframework.security.config.annotation.method.configuration.GlobalMethodSecurityConfiguration$$EnhancerBySpringCGLIB$$bce43815] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.404  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'methodSecurityMetadataSource' of type [org.springframework.security.access.method.DelegatingMethodSecurityMetadataSource] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.424  INFO 21920 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$1b7bd026] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-07-09 15:32:50.692  INFO 21920 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2018-07-09 15:32:50.710  INFO 21920 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2018-07-09 15:32:50.710  INFO 21920 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.31
2018-07-09 15:32:50.714  INFO 21920 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [C:\Program Files\Java\jdk1.8.0_151\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Intel\Shared Files\cpp\bin\Intel64;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\7-Zip;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\Tools\Binn\;C:\Program Files\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\Client SDK\ODBC\130\Tools\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\140\Tools\Binn\ManagementStudio\;D:\node_js\;C:\Program Files\Git\cmd;C:\WINDOWS\System32\OpenSSH\;C:\Users\User\AppData\Local\Microsoft\WindowsApps;C:\Users\User\AppData\Local\GitHubDesktop\bin;C:\Users\User\AppData\Roaming\npm;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;;.]
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2018-07-09 15:32:50.817  INFO 21920 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 1600 ms
2018-07-09 15:32:51.784  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] .s.DelegatingFilterProxyRegistrationBean : Mapping filter: 'springSecurityFilterChain' to: [/*]
2018-07-09 15:32:51.785  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
2018-07-09 15:32:51.786  INFO 21920 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
2018-07-09 15:32:51.885  INFO 21920 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
My pom.xml:
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;org.paperplane&lt;/groupId&gt;
    &lt;artifactId&gt;todoapp&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;todoapp&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.3.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;Finchley.RELEASE&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;
            &lt;version&gt;2.9.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
            &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
My apllication.properties:
spring.datasource.url=jdbc:postgresql://127.0.0.1:55491/TodoAppDatabase
spring.datasource.username=admin
spring.datasource.password=root
spring.datasource.hikari.connection-timeout=10000
spring.datasource.hikari.driver-class-name=org.postgresql.Driver
spring.datasource.hikari.maximum-pool-size=100
spring.data.jpa.repositories.enabled=true
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto = update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQL94Dialect
spring.jpa.properties.hibernate.cache.use_query_cache=true
spring.jpa.properties.hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
How can I overcome this issue?
",<spring><postgresql><spring-boot>,15840,4,153,252,1,3,5,46,52392,,2,10,26,2018-07-09 10:06,2018-11-14 13:30,,128,,Intermediate,23
48280776,NHibernate HQL Generator to support SQL Server 2016 temporal tables,"I am trying to implement basic support for SQL Server 2016 temporal tables in NHibernate 4.x. The idea is to alter SQL statement from  
SELECT * FROM Table t0
to  
SELECT * FROM Table FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00' t0
You can find more info about temporal tables in SQL Server 2016 here
Unfortunately, I've not found any way to insert FOR FOR SYSTEM_TIME AS OF '...' statement between table name and its alias. I'm not sure if custom dialects supports this. The only working solution I have for now is to append FOR SYSTEM_TIME statement within extra WHERE and my output SQL looks like this
SELECT * FROM Table t0 WHERE FOR SYSTEM_TIME AS OF '2018-01-16 00:00:00'=1
To do so, I have implemented generator and dialect as follows:
public static class AuditableExtensions
{
    public static bool AsOf(this IAuditable entity, DateTime date)
    {
        return true;
    }
    public static IQueryable&lt;T&gt; Query&lt;T&gt;(this ISession session, DateTime asOf) where T : IAuditable
    {
        return session.Query&lt;T&gt;().Where(x =&gt; x.AsOf(asOf));
    }
}
public class ForSystemTimeGenerator : BaseHqlGeneratorForMethod
{
    public static readonly string ForSystemTimeAsOfString = ""FOR SYSTEM_TIME AS OF"";
    public ForSystemTimeGenerator()
    {
        SupportedMethods = new[]
        {
            ReflectionHelper.GetMethod(() =&gt; AuditableExtensions.AsOf(null, DateTime.MinValue))
        };
    }
    public override HqlTreeNode BuildHql(MethodInfo method, Expression targetObject, 
        ReadOnlyCollection&lt;Expression&gt; arguments,
        HqlTreeBuilder treeBuilder, 
        IHqlExpressionVisitor visitor)
    {
        return treeBuilder.BooleanMethodCall(nameof(AuditableExtensions.AsOf), new[]
        {
            visitor.Visit(arguments[1]).AsExpression()
        });
    }
}
public class MsSql2016Dialect : MsSql2012Dialect
{
    public MsSql2016Dialect()
    {
        RegisterFunction(nameof(AuditableExtensions.AsOf), new SQLFunctionTemplate(
            NHibernateUtil.Boolean, 
            $""{ForSystemTimeGenerator.ForSystemTimeAsOfString} ?1?2=1""));
    }
}
Can anyone provide any better approach or samples I could use to move forward and insert FOR SYSTEM_TIME AS OF statement between table name and its alias? At this moment the only solution I can see is to alter SQL in OnPrepareStatement in SessionInterceptor but I believe there is some better approach...
",<c#><sql-server><nhibernate><sql-server-2016><temporal-tables>,2420,1,55,762,0,6,25,56,1022,0,54,3,26,2018-01-16 12:00,2018-05-10 5:47,,114,,Intermediate,18
48777993,How do I add a column to a nested struct in a PySpark dataframe?,"I have a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
I'd like to add columns within the state struct, that is, create a dataframe with a schema like
root
 |-- state: struct (nullable = true)
 |    |-- fld: integer (nullable = true)
 |    |-- a: integer (nullable = true)
I tried
df.withColumn('state.a', val).printSchema()
# root
#  |-- state: struct (nullable = true)
#  |    |-- fld: integer (nullable = true)
#  |-- state.a: integer (nullable = true)
",<dataframe><apache-spark><pyspark><struct><apache-spark-sql>,522,0,13,2007,4,23,39,57,55595,0,12,7,26,2018-02-14 0:43,2018-02-14 15:26,2018-02-14 15:26,0,0,Basic,2
48243734,Is there a way to have table name automatically added to Eloquent query methods?,"I'm developing an app on Laravel 5.5 and I'm facing an issue with a specific query scope. I have the following table structure (some fields omitted):
orders
---------
id
parent_id
status
The parent_id column references the id from the same table. I have this query scope to filter records that don't have any children:
public function scopeNoChildren(Builder $query): Builder
{
    return $query-&gt;select('orders.*')
        -&gt;leftJoin('orders AS children', function ($join) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;where('children.status', self::STATUS_COMPLETED);
        })
        -&gt;where('children.id', null);
}
This scope works fine when used alone. However, if I try to combine it with any another condition, it throws an SQL exception:
Order::where('status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
Leads to this:
  SQLSTATE[23000]: Integrity constraint violation: 1052 Column 'status' in where clause is ambiguous
I found two ways to avoid that error:
Solution #1: Prefix all other conditions with the table name
Doing something like this works:
Order::where('orders.status', Order::STATUS_COMPLETED)
    -&gt;noChildren()
    -&gt;get();
But I don't think this is a good approach since it's not clear the table name is required in case other dev or even myself try to use that scope again in the future. They'll probably end up figuring that out, but it doesn't seem a good practice.
Solution #2: Use a subquery
I can keep the ambiguous columns apart in a subquery. Still, in this case and as the table grows, the performance will degrade.
This is the strategy I'm using, though. Because it doesn't require any change to other scopes and conditions. At least not in the way I'm applying it right now.
public function scopeNoChildren(Builder $query): Builder
{
    $subQueryChildren = self::select('id', 'parent_id')
        -&gt;completed();
    $sqlChildren = DB::raw(sprintf(
        '(%s) AS children',
        $subQueryChildren-&gt;toSql()
    ));
    return $query-&gt;select('orders.*')
        -&gt;leftJoin($sqlChildren, function ($join) use ($subQueryChildren) {
            $join-&gt;on('orders.id', '=', 'children.parent_id')
                -&gt;addBinding($subQueryChildren-&gt;getBindings());
         })-&gt;where('children.id', null);
}
The perfect solution
I think that having the ability to use queries without prefixing with table name without relying on subqueries would be the perfect solution.
That's why I'm asking: Is there a way to have table name automatically added to Eloquent query methods?
",<php><mysql><laravel><eloquent>,2608,0,37,3774,6,40,63,41,4259,0,2480,3,25,2018-01-13 19:52,2018-06-02 19:56,2018-06-04 14:57,140,142,Intermediate,20
55693241,How to securely connect to Cloud SQL from Cloud Run?,"How do I connect to the database on Cloud SQL without having to add my credentials file inside the container?
",<google-cloud-sql><google-cloud-run>,110,0,0,992,1,9,17,80,13914,0,3,3,25,2019-04-15 16:08,2019-04-28 16:25,2019-04-28 16:25,13,13,Intermediate,20
56155514,Azure Cosmos DB SQL API UPDATE statement - don't replace whole document,"Can I write an UPDATE statement for Azure Cosmos DB? The SQL API supports queries, but what about updates?
In particular, I am looking to update documents without having to retrieve the whole document. I have the ID for the document and I know the exact path I want to update within the document. For example, say that my document is
{
  ""id"": ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"",
  ""name"": ""Fido"",
  ""weight"": 35,
  ""breed"": ""pomeranian"",
  ""diet"": {
    ""scoops"": 3,
    ""timesPerDay"": 2
  }
}
and I want to update diet.timesPerDay to 1 where the ID is ""9e576f8b-146f-4e7f-a68f-14ef816e0e50"". Can I do that using the Azure SQL API without completely replacing the document?
",<azure><azure-cosmosdb><azure-cosmosdb-sqlapi>,679,0,13,6482,7,43,95,46,43375,0,1337,5,25,2019-05-15 18:23,2019-05-15 18:25,2022-03-10 12:07,0,1030,Basic,3
48270374,Invalid datetime format: 1366 Incorrect string value,"I'm getting this error:
  SQLSTATE[22007]: Invalid datetime format: 1366 Incorrect string value: '\xBD Inch...' for column 'column-name' at row 1
My database, table, and column have the format utf8mb4_unicode_ci also column-name is type text and NULL.
This is the value of the column-name
  [column-name] => Some text before 11 ▒ and other text after, and after.
However I wait that laravel adds quotes to column's values, because the values are separated by commas (,). It should be as follow:
  [column-name] => 'Some text before 11 ▒ and other text after, and after.'
See below the Schema
    Schema::create('mws_orders', function (Blueprint $table) {
        $table-&gt;string('custom-id');
        $table-&gt;string('name');
        $table-&gt;string('description')-&gt;nullable();
        $table-&gt;string('comment')-&gt;nullable();
        $table-&gt;integer('count')-&gt;nullable();
        $table-&gt;text('column-name')-&gt;nullable();
        $table-&gt;timestamps();
        $table-&gt;primary('custom-id');
    });
I have been looking for on google but not any solution, yet.
Anyone has an idea how to solve this issue?
I'm using Laravel 5.5 and MariaDB 10.2.11.
",<php><mysql><laravel><mariadb><laravel-5.5>,1177,0,11,1492,6,32,58,43,39927,0,206,6,25,2018-01-15 20:40,2018-01-15 21:27,2018-01-17 21:09,0,2,Basic,14
54316131,How to create multiple tables in a database in sqflite?,"Im building and app with flutter that uses SQLite database. I have created first table using this piece of code:
 void _createDb(Database db, int newVersion) async {
    await db.execute('''CREATE TABLE cards (id_card INTEGER PRIMARY KEY, 
         color TEXT, type TEXT, rarity TEXT, name TEXT UNIQUE, goldCost INTEGER,
         manaCost INTEGER, armor INTEGER, attack INTEGER, health INTEGER, description TEXT)''');
}
Table gets created and I can access it without problems.
Unfortunately I cannot include more than 1 table that i just created. I tried adding another SQL CREATE TABLE clause in the same method, and repeating method db.execute with a different SQL clause just in the next line.
I'm mimicing code from this tutorial: https://www.youtube.com/watch?v=xke5_yGL0uk
How to add another table within the same database?
",<database><flutter><sqlite><dart><sqflite>,830,2,6,471,2,6,15,52,41385,0,4,7,25,2019-01-22 20:40,2019-02-04 8:15,,13,,Intermediate,15
64350794,"typeORM: ""message"": ""Data type \""Object\"" in \""..."" is not supported by \""postgres\"" database.""","Given the following entity definition:
@Entity()
export class User extends BaseEntity {
  @Column({ nullable: true })
  name!: string | null;
  @Column()
  age!: number;
}
The following error appears:
typeORM:   &quot;message&quot;: &quot;Data type \&quot;Object\&quot; in \&quot;User.name&quot; is not supported by \&quot;postgres\&quot; database.&quot;
...
name: 'DataTypeNotSupportedError',
  message:
   'Data type &quot;Object&quot; in &quot;User.name&quot; is not supported by &quot;postgres&quot; database.' }
When looking at the build, I see that the metadata that's emitted by TS addresses it as object:
__decorate([
    typeorm_1.Column({ nullable: true }),
    __metadata(&quot;design:type&quot;, Object)
], User.prototype, &quot;name&quot;, void 0);
What am I doing wrong?
",<node.js><typescript><postgresql><typeorm><reflect-metadata>,785,0,19,6484,3,28,39,63,15355,0,50,2,25,2020-10-14 9:54,2020-10-14 9:54,,0,,Intermediate,15
49509615,How do I use parameters in VBA in the different contexts in Microsoft Access?,"I've read a lot about SQL injection, and using parameters, from sources like bobby-tables.com. However, I'm working with a complex application in Access, that has a lot of dynamic SQL with string concatenation in all sorts of places.
It has the following things I want to change, and add parameters to, to avoid errors and allow me to handle names with single quotes, like Jack O'Connel.
It uses:
DoCmd.RunSQL to execute SQL commands
DAO recordsets
ADODB recordsets
Forms and reports, opened with DoCmd.OpenForm and DoCmd.OpenReport, using string concatenation in the WhereCondition argument
Domain aggregates like DLookUp that use string concatenation
The queries are mostly structured like this:
DoCmd.RunSQL ""INSERT INTO Table1(Field1) SELECT Field1 FROM Table2 WHERE ID = "" &amp; Me.SomeTextbox
What are my options to use parameters for these different kinds of queries?
This question is intended as a resource, for the frequent how do I use parameters comment on various posts
",<sql><vba><ms-access>,982,1,6,31848,13,42,67,68,13793,0,513,2,25,2018-03-27 9:47,2018-03-27 9:47,2018-03-27 9:47,0,0,Basic,3
64906396,"fetch API always returns {""_U"": 0, ""_V"": 0, ""_W"": null, ""_X"": null}","The below code always return the below wired object
{&quot;_U&quot;: 0, &quot;_V&quot;: 0, &quot;_W&quot;: null, &quot;_X&quot;: null}
as response.
Here is my code
    getData = () =&gt; {
        fetch('http://192.168.64.1:3000/getAll',{
            method: 'GET',
            headers: {
                Accept: 'application/json',
                'Content-Type': 'application/json'
            }
        })
        .then((response) =&gt; {
            console.log('Response:')
            console.log(response.json())
            console.info('=================================')
        })
        .catch(err =&gt; console.error(err));
    } 
    componentDidMount(){
        this.getData();
    }
I am using node, express, Mysql as backend and react-native frontend
my backend code is here
app.get('/getAll',(req,res) =&gt; {
    console.log('getAll method Called');
    con.query('select * from dummy',(err,results,fields) =&gt; {
        if(err) throw err;
        console.log('Response');
        console.log(results);
        res.send(results);
    });
});
The above code gives correct output in console but fetch API is not.
i cant find solution for the my problem. Thanks in advance.
",<mysql><node.js><reactjs><react-native><fetch>,1194,1,29,321,1,3,7,52,28228,0,24,4,25,2020-11-19 6:24,2020-11-19 6:42,2020-11-19 6:42,0,0,Basic,1
52142645,How to improve SQLite insert performance in Python 3.6?,"Background
I would like to insert 1-million records to SQLite using Python. I tried a number of ways to improve it but it is still not so satisfied. The database load file to memory using 0.23 second (search pass below) but SQLite 1.77 second to load and insert to file.
Environment
Intel Core i7-7700 @ 3.6GHz
16GB RAM
Micron 1100 256GB SSD, Windows 10 x64
Python 3.6.5 Minconda
sqlite3.version 2.6.0  
GenerateData.py
I generate the 1 million test input data with the same format as my real data.
import time
start_time = time.time()
with open('input.ssv', 'w') as out:
    symbols = ['AUDUSD','EURUSD','GBPUSD','NZDUSD','USDCAD','USDCHF','USDJPY','USDCNY','USDHKD']
    lines = []
    for i in range(0,1*1000*1000):
        q1, r1, q2, r2 = i//100000, i%100000, (i+1)//100000, (i+1)%100000
        line = '{} {}.{:05d} {}.{:05d}'.format(symbols[i%len(symbols)], q1, r1, q2, r2)
        lines.append(line)
    out.write('\n'.join(lines))
print(time.time()-start_time, i)
input.ssv
The test data looks like this.
AUDUSD 0.00000 0.00001
EURUSD 0.00001 0.00002
GBPUSD 0.00002 0.00003
NZDUSD 0.00003 0.00004
USDCAD 0.00004 0.00005
...
USDCHF 9.99995 9.99996
USDJPY 9.99996 9.99997
USDCNY 9.99997 9.99998
USDHKD 9.99998 9.99999
AUDUSD 9.99999 10.00000
// total 1 million of lines, taken 1.38 second for Python code to generate to disk
Windows correctly shows 23,999,999 bytes file size.
Baseline Code InsertData.py
import time
class Timer:
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed)) 
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        infile.read()
Basic I/O
with open('input.ssv', 'r') as infile:
    infile.read()
  Imported in 0.13 seconds or 7.6 M per second
It tests the read speed.
with open('input.ssv', 'r') as infile:
    with open('output.ssv', 'w') as outfile:
        outfile.write(infile.read()) // insert here
  Imported in 0.26 seconds or 3.84 M per second
It tests the read and write speed without parsing anything
with open('input.ssv', 'r') as infile:
    lines = infile.read().splitlines()
    for line in lines:
        pass # do insert here
  Imported in 0.23 seconds or 4.32 M per second
When I parse the data line by line, it achieves a very high output.
This gives us a sense about how fast the IO and string processing operations on my testing machine.
1.   Write File
outfile.write(line)
  Imported in 0.52 seconds or 1.93 M per second
2.   Split to floats to string
tokens = line.split()
sym, bid, ask = tokens[0], float(tokens[1]), float(tokens[2])
outfile.write('{} {:.5f} {%.5f}\n'.format(sym, bid, ask)) // real insert here
  Imported in 2.25 seconds or 445 K per second
3.   Insert Statement with autocommit
conn = sqlite3.connect('example.db', isolation_level=None)
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  When isolation_level = None (autocommit), program takes many hours to complete (I could not wait for such a long hours)
Note the output database file size is 32,325,632 bytes, which is 32MB. It is bigger than the input file ssv file size of 23MB by 10MB.
4.   Insert Statement with BEGIN (DEFERRED)
conn = sqlite3.connect('example.db', isolation_level=’DEFERRED’) # default
c.execute(""INSERT INTO stocks VALUES ('{}',{:.5f},{:.5f})"".format(sym,bid,ask))
  Imported in 7.50 seconds or 133,296 per second
This is the same as writing BEGIN, BEGIN TRANSACTION or BEGIN DEFERRED TRANSACTION, not BEGIN IMMEDIATE nor BEGIN EXCLUSIVE.
5.   Insert by Prepared Statement
Using the transaction above gives a satisfactory results but it should be noted that using Python’s string operations is undesired because it is subjected to SQL injection. Moreover using string is slow compared to parameter substitution.
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(sym,bid,ask)])
  Imported in 2.31 seconds or 432,124 per second
6.   Turn off Synchronous
Power failure corrupts the database file when synchronous is not set to EXTRA nor FULL before data reaches the physical disk surface. When we can ensure the power and OS is healthy, we can turn synchronous to OFF so that it doe not synchronized after data handed to OS layer.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
  Imported in 2.25 seconds or 444,247 per second
7.   Turn off journal and so no rollback nor atomic commit
In some applications the rollback function of a database is not required, for example a time series data insertion. When we can ensure the power and OS is healthy, we can turn journal_mode to off so that rollback journal is disabled completely and it disables the atomic commit and rollback capabilities.
conn = sqlite3.connect('example.db', isolation_level='DEFERRED')
c = conn.cursor()
c.execute('''PRAGMA synchronous = OFF''')
c.execute('''PRAGMA journal_mode = OFF''')
  Imported in 2.22 seconds or 450,653 per second
8.   Using in-memory database
In some applications writing data back to disks is not required, such as applications providing queried data to web applications.
conn = sqlite3.connect("":memory:"")
  Imported in 2.17 seconds or 460,405 per second
9.   Faster Python code in the loop
We should consider to save every bit of computation inside an intensive loop, such as avoiding assignment to variable and string operations.
9a.  Avoid assignment to variable
tokens = line.split()
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(tokens[0], float(tokens[1]), float(tokens[2]))])
  Imported in 2.10 seconds or 475,964 per second
9b.  Avoid string.split()
When we can treat the space separated data as fixed width format, we can directly indicate the distance between each data to the head of data.
It means line.split()[1] becomes line[7:14]
c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], float(line[7:14]), float(line[15:]))])
  Imported in 1.94 seconds or 514,661 per second
9c.  Avoid float() to ?
When we are using executemany() with ? placeholder, we don’t need to turn the string into float beforehand.
executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
  Imported in 1.59 seconds or 630,520 per second
10.  The fastest full functioned and robust code so far
import time
class Timer:    
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        elapsed = time.time()-self.start
        print('Imported in {:.2f} seconds or {:.0f} per second'.format(elapsed, 1*1000*1000/elapsed))
import sqlite3
conn = sqlite3.connect('example.db')
c = conn.cursor()
c.execute('''DROP TABLE IF EXISTS stocks''')
c.execute('''CREATE TABLE IF NOT EXISTS stocks
             (sym text, bid real, ask real)''')
c.execute('''PRAGMA synchronous = EXTRA''')
c.execute('''PRAGMA journal_mode = WAL''')
with Timer() as t:
    with open('input.ssv', 'r') as infile:
        lines = infile.read().splitlines()
        for line in lines:
            c.executemany(""INSERT INTO stocks VALUES (?,?,?)"", [(line[0:6], line[7:14], line[15:])])
        conn.commit()
        conn.close()
  Imported in 1.77 seconds or 564,611 per second
Possible to get faster?
I have a 23MB file with 1 million records composing of a piece of text as symbol name and 2 floating point number as bid and ask. When you search pass above, the test result shows a 4.32 M inserts per second to plain file. When I insert to a robust SQLite database, it drops to 0.564 M inserts per second. What else you may think of to make it even faster in SQLite? What if not SQLite but other database system?
",<python><performance><sql-insert>,7738,0,104,641,1,8,10,37,12793,0,186,3,25,2018-09-03 2:51,2019-10-24 2:05,,416,,Intermediate,23
52287553,How to create a copy of a dataframe in pyspark?,"I have a dataframe from which I need to create a new dataframe with a small change in the schema by doing the following operation.
&gt;&gt;&gt; X = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])
&gt;&gt;&gt; schema_new = X.schema.add('id_col', LongType(), False)
&gt;&gt;&gt; _X = X.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)
The problem is that in the above operation, the schema of X gets changed inplace. So when I print X.columns I get 
&gt;&gt;&gt; X.columns
['a', 'b', 'id_col']
but the values in X are still the same
&gt;&gt;&gt; X.show()
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  3|  4|
+---+---+
To avoid changing the schema of X, I tried creating a copy of X using three ways 
- using copy and deepcopy methods from the copy module
- simply using _X = X
The copy methods failed and returned a 
RecursionError: maximum recursion depth exceeded
The assignment method also doesn't work
&gt;&gt;&gt; _X = X
&gt;&gt;&gt; id(_X) == id(X)
True
Since their id are the same, creating a duplicate dataframe doesn't really help here and the operations done on _X reflect in X.
So my question really is two fold
how to change the schema outplace (that is without making any changes to X)?
and more importantly, how to create a duplicate of a pyspark dataframe?
Note:
This question is a followup to this post
",<python><apache-spark><pyspark><apache-spark-sql>,1338,1,30,7707,15,70,110,65,95019,0,2100,6,25,2018-09-12 4:35,2018-09-12 6:55,2018-09-12 6:56,0,0,Basic,9
51087037,SQL Server json truncated (even when using NVARCHAR(max) ),"DECLARE @result NVARCHAR(max);
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
SELECT @result;
This returns a json string of ~43000 characters, with some results truncated.
SET @result = (SELECT * FROM table
               FOR JSON AUTO, ROOT('Data'))
This returns a json string of ~2000 characters. Is there any way to prevent any truncation? Even when dealing with some bigdata and the string is millions and millions of characters?
",<sql><json><sql-server>,466,0,8,583,1,8,14,66,31442,0,3,10,25,2018-06-28 15:51,2018-07-23 22:17,,25,,Basic,9
50163432,Oracle's default DATE format,"First time using Oracle SQL (I'm used to MySQL).  I'm finding conflicting info on what the default date format is.  After several attempts having to use TO_DATE with my INSERT INTO my_table statements, I finally found the database I'm using expects DD-MON-YY (i.e. 25-JAN-18).  Yet on various pages here in stackoverflow and elsewhere, I see some that say default is YYYYMMDD or DD/MM/YYYY or YYYY-MM-DD.  Why so many conflicting pieces of information?
",<sql><oracle><date>,453,0,0,827,4,15,35,50,141248,0,31,4,25,2018-05-03 20:32,2018-05-03 20:37,2018-05-03 20:37,0,0,Basic,3
59998409,Error Code: 3685. Illegal argument to a regular expression,"I am trying to find an exact number in MySQL 8.0 using the below SQL statement
SELECT * FROM rulebook.node__body 
WHERE body_value REGEXP ""[[:&lt;:]]DVP[[:&gt;:]]"";
when i am running the above SQL statement i am getting below error
  Error Code: 3685. Illegal argument to a regular expression
could you please anyone tell me where i am  making mistake.
",<mysql-8.0>,353,1,2,1011,2,16,29,77,8655,0,31,4,25,2020-01-31 5:34,2020-01-31 6:10,,0,,Basic,1
50126503,"Homebrew, MySQL 8 support","Anyone have the inside scoop on when Homebrew will be updated to support MySQL 8's first general release (8.0.11)? I can't seem to find it by searching, but I bet someone here knows :)
",<mysql><version-control><homebrew><mysql-5.7><mysql-8.0>,185,0,0,991,3,14,24,71,24342,0,52,4,25,2018-05-02 2:56,2018-06-06 11:03,2018-06-14 6:54,35,43,Intermediate,21
60536206,Speed up odbc::dbFetch,"I'm trying to analyze data stored in an SQL database (MS SQL server) in R, and on a mac. Typical queries might return a few GB of data, and the entire database is a few TB. So far, I've been using the R package odbc, and it seems to work pretty well.
However, dbFetch() seems really slow. For example, a somewhat complex query returns all results in ~6 minutes in SQL server, but if I run it with odbc and then try dbFetch, it takes close to an hour to get the full 4 GB into a data.frame. I've tried fetching in chunks, which helps modestly: https://stackoverflow.com/a/59220710/8400969. I'm wondering if there is another way to more quickly pipe the data to my mac, and I like the line of thinking here: Quickly reading very large tables as dataframes
What are some strategies for speeding up dbFetch when the results of queries are a few GB of data? If the issue is generating a data.frame object from larger tables, are there savings available by &quot;fetching&quot; in a different manner? Are there other packages that might help?
Thanks for your ideas and suggestions!
",<r><sql-server><dataframe><odbc><fread>,1076,2,9,1344,0,13,31,58,1175,0,660,2,25,2020-03-04 23:30,2022-09-28 11:45,,938,,Intermediate,23
50378664,Permission denied inside /var/www/html when creating a website and it's files with the apache2 server,"
UPDATE** The screenshot is within atom, but when I navigate to the directory using the file explorer, and right click, the option to rename or create a new folder are restricted and I cannot click on them.  
I just finished setting up the LAMP stack on my fresh UBUNTU 18.04 installation. I have everything working, the default /var/www/html/index.html page from Apache2 is being served on localhost, no port forwarding or any unique domain name, i just wanna run this on my network from my computer for now. 
If there is a simple way to create multiple websites and easily choose which folder to serve than that's fine, but I want to serve just one website for now. 
When I go to my /var/www/html folder and try to edit the index.html file it says permission denied. What do I need to do in order to work inside this directory for the remaining time that I am building the website. I am signed in as the root user on my system. 
Also, if I do change permissions to allow me to work in this directory, what does it mean for people trying to access my server if it was available to the public. (RIGHT NOW JUST ON LOCALHOST).
Lemme know if you need more info or explanation thanks!
",<mysql><permissions><apache2><lamp><ubuntu-18.04>,1181,1,0,1543,1,13,17,44,125725,0,33,8,25,2018-05-16 19:33,2018-05-16 19:36,2018-05-16 20:19,0,0,Basic,14
54380363,"Microsoft SQL Server stuck at ""Install_SQLSupport_CPU64_Action""","While installing SQL Server 2017 Developer Edition, I got stuck at ""Install_SQLSupport_CPU64_Action"", this happened to me for the second time, once at work and once at home.
After searching online I found no solution.
",<sql><sql-server><installation><freeze>,218,0,0,1356,1,10,14,77,30749,0,2,2,25,2019-01-26 16:33,2019-01-26 16:38,2019-01-26 16:38,0,0,Basic,14
58808332,Should we ever check for mysqli_connect() errors manually?,"The PHP manual for mysqli_connect() suggests checking for the return value and displaying the error messages on the screen.
$link = mysqli_connect(""127.0.0.1"", ""my_user"", ""my_password"", ""my_db"");
if (!$link) {
    echo ""Error: Unable to connect to MySQL."" . PHP_EOL;
    echo ""Debugging errno: "" . mysqli_connect_errno() . PHP_EOL;
    echo ""Debugging error: "" . mysqli_connect_error() . PHP_EOL;
    exit;
}
Similarly for OOP-style constructor this is suggested:
$mysqli = new mysqli('localhost', 'my_user', 'my_password', 'my_db');
if ($mysqli-&gt;connect_error) {
    die('Connect Error (' . $mysqli-&gt;connect_errno . ') '
            . $mysqli-&gt;connect_error);
}
Some users on Stack Overflow even used code with mysqli_error($conn) such as this:
$conn = mysqli_connect('localhost', 'a', 'a');
if (!$con) {
    die('Could not connect: ' . mysqli_error($conn));
}
However, in the past few weeks I have been asking myself a question, why would I need to do that? The output of the first example is:
  Warning:  mysqli_connect(): (HY000/1045): Access denied for user
  'my_user'@'localhost' (using password: YES) in
  C:\xampp\...\mysqli.php on line 4
  Error: Unable to connect to MySQL. Debugging errno: 1045 Debugging
  error: Access denied for user 'my_user'@'localhost' (using password:
  YES)
As you can see the error message is displayed twice! The manual ""debugging"" actually provides less information. 
Should we ever manually check for connection errors? Would we ever get more information this way than from the automatic warning? Is this the recommended practice?  
",<php><mysqli><error-handling>,1583,1,18,31395,25,88,134,62,7671,0,5269,1,25,2019-11-11 20:33,2019-11-11 20:33,2019-11-11 20:33,0,0,Basic,13
61910260,SQLiteConstraintException error showing after start of every activity,"I have this error popping out in logcat all the time. It always shows after every change of activity. And sometimes, the app disappears and in a second it shows again. There is not any fatal error in logcat, all I see is this:
2020-05-20 11:53:26.422 2940-8484/? E/SQLiteDatabase: Error inserting flex_time=3324000 job_id=-1 period=6650000 source=16 requires_charging=0 preferred_network_type=1 target_class=com.google.android.gms.measurement.PackageMeasurementTaskService user_id=0 target_package=com.google.android.gms tag=Measurement.PackageMeasurementTaskService.UPLOAD_TASK_TAG task_type=0 required_idleness_state=0 service_kind=0 source_version=201516000 persistence_level=1 preferred_charging_state=1 required_network_type=0 runtime=1589968406417 retry_strategy={""maximum_backoff_seconds"":{""3600"":0},""initial_backoff_seconds"":{""30"":0},""retry_policy"":{""0"":0}} last_runtime=0
android.database.sqlite.SQLiteConstraintException: UNIQUE constraint failed: pending_ops.tag, pending_ops.target_class, pending_ops.target_package, pending_ops.user_id (code 2067 SQLITE_CONSTRAINT_UNIQUE)
    at android.database.sqlite.SQLiteConnection.nativeExecuteForLastInsertedRowId(Native Method)
    at android.database.sqlite.SQLiteConnection.executeForLastInsertedRowId(SQLiteConnection.java:879)
    at android.database.sqlite.SQLiteSession.executeForLastInsertedRowId(SQLiteSession.java:790)
    at android.database.sqlite.SQLiteStatement.executeInsert(SQLiteStatement.java:88)
    at android.database.sqlite.SQLiteDatabase.insertWithOnConflict(SQLiteDatabase.java:1599)
    at android.database.sqlite.SQLiteDatabase.insert(SQLiteDatabase.java:1468)
    at aplm.a(:com.google.android.gms@201516038@20.15.16 (120406-309763488):76)
    at aplb.a(:com.google.android.gms@201516038@20.15.16 (120406-309763488):173)
    at aplb.a(:com.google.android.gms@201516038@20.15.16 (120406-309763488):21)
    at aplb.a(:com.google.android.gms@201516038@20.15.16 (120406-309763488):167)
    at aphk.run(:com.google.android.gms@201516038@20.15.16 (120406-309763488):8)
    at sob.b(:com.google.android.gms@201516038@20.15.16 (120406-309763488):12)
    at sob.run(:com.google.android.gms@201516038@20.15.16 (120406-309763488):7)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
    at sub.run(:com.google.android.gms@201516038@20.15.16 (120406-309763488):0)
    at java.lang.Thread.run(Thread.java:919)
But it does not show anywhere to the code is there any solution to it?
Edit: Google libraries:
implementation 'com.google.android.material:material:1.1.0'
implementation 'com.google.firebase:firebase-core:17.1.0'
implementation 'com.google.firebase:firebase-database:19.0.0'
implementation 'com.google.firebase:firebase-analytics:17.0.1'
implementation 'com.google.firebase:firebase-perf:19.0.0'
implementation 'com.google.android.gms:play-services-ads:18.1.1'
",<android><android-sqlite>,2963,0,25,443,1,7,16,65,11340,,154,3,25,2020-05-20 10:00,2020-07-19 10:50,2020-07-28 15:01,60,69,Basic,13
50350741,Room @Relation with composite Primary Key,"my question is an extension of this question (also mine :) ) -> Room composite Primary Key link to Foreign Key
So, if I have this class:
public class FoodWithIngredients extends Food{
    @Relation(parentColumn = ""id"", entityColumn = ""food_id"", entity = 
    Ingredient.class)
    private List&lt;Ingredient&gt; mIngredients;
}
But the PrimaryKey of ""Food"" table is composite (primaryKeys = {""id"", ""language_id""}).
How I can make the @Relation returns records where ""parentColumn = {""id"", ""language_id""}, entityColumn = {""food_id"", food_language_id}"" ?
",<android><sqlite><android-room><composite-primary-key>,553,1,11,1513,2,17,34,48,4784,0,119,3,24,2018-05-15 12:52,2019-06-18 7:52,,399,,Basic,8
51492078,Drop column if exists in SQL Server 2008 r2,"I am using SQL Server 2008 R2.
I want to drop the column if it is already exists in the table else not throw any error.
Tried:
ALTER TABLE Emp 
DROP COLUMN IF EXISTS Lname;
Error: 
  Incorrect syntax near the keyword 'IF'.
By searching I came to know that, this option is available from 2016.
What is the alternative in the SQL Server 2008 R2?
",<sql-server><sql-server-2008-r2>,344,0,2,6902,26,75,133,73,66168,0,788,3,24,2018-07-24 6:42,2018-07-24 6:44,2018-07-24 6:44,0,0,Basic,8
50081527,Can't connect to MySQL from Java: NullPointerException inside MySQL driver connection logic,"I'm trying to connect to a database I created with MySQL in my Java program, but it always fails.
For the sake of example, here is my code:
import java.sql.*;
public class Squirrel {
    public static void main(String[] args) {
        String user;
        String password;
        Connection connection;
        Statement statement;
        try {
            Class.forName(""com.mysql.jdbc.Driver"");
            connection = DriverManager.getConnection(
                ""jdbc:mysql://localhost:3306"", user, password);
            statement = connection.createStatement();
            // Other code
        } catch (ClassNotFoundException | SQLException e) {
            e.printStackTrace();
        } finally {
            try {
                if (statement != null) {
                    statement.close();
                }
                if (connection != null) {
                    connection.close();
                }
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }
}
I am able to connect to the database from within IntelliJ and have added the mysql-connector-java-5.1.40.jar added to the project, but each time I run the program DriverManager.getConnection() throws this:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Could not create connection to database server.
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at com.mysql.jdbc.Util.getInstance(Util.java:408)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:918)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:897)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:886)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:860)
    at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2330)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2083)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:806)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:410)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:328)
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:678)
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:229)
    at Squirrel.main(Squirrel.java:12)
Caused by: java.lang.NullPointerException
    at com.mysql.jdbc.ConnectionImpl.getServerCharset(ConnectionImpl.java:2997)
    at com.mysql.jdbc.MysqlIO.sendConnectionAttributes(MysqlIO.java:1934)
    at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1863)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1226)
    at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2253)
    at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2284)
... 13 more
",<java><mysql>,3829,0,68,537,1,5,15,38,24219,0,63,5,24,2018-04-28 22:13,2018-04-28 22:19,2018-04-29 0:28,0,1,Basic,12
51803216,lower_case_table_names Settings in MySQL 8.0.12,"I've just compiled the version MySQL 8.0.12 in a Ubuntu 16.0.4.
After following the instructions in the website and making the following my.cnf file:
[mysqld]
datadir=/usr/local/mysql/data
socket=/tmp/mysql.sock
port=3306
log-error=/usr/local/mysql/data/localhost.localdomain.err
user=mysql
secure_file_priv=/usr/local/mysql/mysql-files
local_infile=OFF
log_error = /var/log/mysql/error.log
# Remove case sensitive in table names
lower_case_table_names=1
I get the following error:
2018-08-11T19:45:06.461585Z 1 [ERROR] [MY-011087] [Server] Different lower_case_table_names settings for server ('1') and data dictionary ('0').
What should I change so that data dictionary is aligned to server settings?
",<mysql><lowercase><data-dictionary>,703,0,14,2259,3,22,26,59,83242,0,211,7,24,2018-08-11 19:53,2018-08-11 21:47,2018-08-11 21:47,0,0,Basic,14
52815608,ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Problem in mysql used in nodejs
const mysql      = require('mysql');
var connection = mysql.createConnection({
  host     : 'localhost',
  user     : 'root',
  password : '123456789',
  database : 'userdata'
});
connection.connect(function(err) {
  if (err) {
    console.error('error connecting: ' + err);
    return;
  }
 console.log('connected as id ' + connection.threadId);
});
Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
",<mysql><node.js>,519,0,16,287,1,3,14,64,45241,0,10,5,24,2018-10-15 11:20,2019-01-04 20:54,2019-01-04 20:54,81,81,Basic,14
60395574,"How to disable ""clr strict security"" in SQL Server","I enabled clr integration (i.e. SQLCLR) by running:
EXEC sp_configure 'clr enabled', 1;  
RECONFIGURE;  
Now when I try:
EXEC sp_configure 'clr strict security', 0;
RECONFIGURE;
I get an error saying the setting does not exist:
Msg 15123, Level 16, State 1, Procedure sp_configure, Line 62
The configuration option 'clr strict security' does not exist, or it may be an advanced option.
I know the proper solution to my problem is signing the assembly containing the stored procedures to allow it running with strict security but for now I need the quick and dirty fix.
",<sql-server><configuration><sqlclr>,569,0,4,3281,1,28,41,38,38213,0,384,1,24,2020-02-25 13:19,2020-02-25 13:37,2020-02-25 13:37,0,0,Basic,14
62917136,"'AddEntityFramework*' was called on the service provider, but 'UseInternalServiceProvider' wasn't called in the DbContext options configuration","I'm upgrading an ASP.NET Core application from Framework 2.2 to 3.1. It also uses Entity Framework Core.
In the Startup.ConfigureServices method, there is this code:
services.AddEntityFrameworkNpgsql()
    .AddDbContext&lt;MainDbContext&gt;(options =&gt; options
        .UseNpgsql(Configuration.GetConnectionString(&quot;MainDbContext&quot;)));
Everything was fine with .NET Core 2.2. With .NET Core 3.1, I get this warning on every application start:
'AddEntityFramework*' was called on the service provider, but 'UseInternalServiceProvider' wasn't called in the DbContext options configuration. Remove the 'AddEntityFramework*' call as in most cases it's not needed and might cause conflicts with other products and services registered in the same service provider.
Looking up the UseInternalServiceProvider method, it looks like that should be called on the options to pass on the main service provider. Unfortunately, at this point, the service provider does not exist yet. It is just about to be built.
I don't understand what the problem is and what this warning wants to tell me, but failed to do. How can I make that warning go away? The web doesn't know about this message yet.
",<c#><asp.net-core><.net-core><entity-framework-core><npgsql>,1188,0,5,18981,23,117,217,60,11079,,1123,1,24,2020-07-15 14:23,2020-07-15 14:32,2020-07-15 14:32,0,0,Basic,14
55478489,"android Room, how to delete multiple rows in the table by id list","Having a Android Room with three tables timestamp, index, details, and all three have 
@PrimaryKey @ColumnInfo(name = ""id"") var id: Int = 0
having fun clearDataByID(idList: List&lt;Int&gt;)  to clear data from all three tables by the id in the idList
Dao as:
@Dao
interface DataDAO {
@Transaction
fun clearDataByID(idList: List&lt;Int&gt;) {
    deleteDataInTimestamp(idList)
    deleteDataIndex(idList)
    deleteDataDetails(idList)
}
@Query(""delete from timestamp where id in :idList"")
fun deleteDataInTimestamp(idList: List&lt;Int&gt;)
@Query(""delete from index where id in :idList"")
fun deleteDataIndex(idList: List&lt;Int&gt;)
@Query(""delete from details where id in :idList"")
fun deleteDataDetails(idList: List&lt;Int&gt;)
}
but it gets compiler error (similar for all three)
error: no viable alternative at input 'delete from timestamp where id in :idList'
public abstract void deleteDataInTimestamp(@org.jetbrains.annotations.NotNull()
if delete by single id it worked.  
How to delete by a list of ids?
@Query(""delete from timestamp where id = :id"")
fun deleteSingleTimestamp(id: Int)
",<android-room><sql-delete>,1094,0,31,10115,12,74,155,65,13620,0,251,2,24,2019-04-02 15:31,2019-04-02 20:34,2022-09-20 3:46,0,1267,Basic,9
56328832,Transactionscope throwing exception this platform does not support distributed transactions while opening connection object,"TransactionScope is throwing a exception in .net core 2.2
In this example I created a scope of TransactioScop.
Opening SQL transaction for one database which is working fine.
After the first transaction I´m calling commit which will commit the SQL transaction.
I try to open call transaction for another database while creating a transaction and the system is throwing an exception
This platform does not support distributed transactions.
tried to remove SQL transaction
c#
using (TransactionScope scop =new TransactionScope(TransactionScopeOption.Required, TransactionScopeAsyncFlowOption.Enabled))
{       
    _db1UOW.Begin(); //creating sql transaction
    await _db1UOW.IDenialDetailsRepositorydb1.InsertDenialDetails(denialsDetails);
    await _db1UOW.IRuleDetailsRepositorydb1.InsertRulesDetails(rulesDetails);
    _db1UOW.Commit(); //commitng sql transaction
    _db2UOW.Begin(); //creating sql transaction (but while opening connection object its throwing exception as This platform does not support distributed transactions)
    await _db2UOW.IRuleDetailsRepository.GetRulesDetails();
    await _db2UOW.IDenialDetailsRepository.InsertDenialDetails(denialsDetails);
    var data = await _db2UOW.IRuleDetailsRepository.InsertRulesDetails(rulesDetails);
    _db2UOW.Commit(); //commitng sql transaction
    scop.Complete();
}
Message
&quot;This platform does not support distributed transactions.&quot;  at System.Transactions.Distributed.DistributedTransactionManager.GetDistributedTransactionFromTransmitterPropagationToken(Byte[] propagationToken)
   at System.Transactions.TransactionInterop.GetDistributedTransactionFromTransmitterPropagationToken(Byte[] propagationToken)
   at System.Transactions.TransactionStatePSPEOperation.PSPEPromote(InternalTransaction tx)
   at System.Transactions.TransactionStateDelegatedBase.EnterState(InternalTransaction tx)
   at System.Transactions.EnlistableStates.Promote(InternalTransaction tx)
   at System.Transactions.Transaction.Promote()
   at System.Transactions.TransactionInterop.ConvertToDistributedTransaction(Transaction transaction)
   at System.Transactions.TransactionInterop.GetExportCookie(Transaction transaction, Byte[] whereabouts)
   at System.Data.SqlClient.SqlInternalConnection.GetTransactionCookie(Transaction transaction, Byte[] whereAbouts)
   at System.Data.SqlClient.SqlInternalConnection.EnlistNonNull(Transaction tx)
   at System.Data.SqlClient.SqlInternalConnection.Enlist(Transaction tx)
   at System.Data.SqlClient.SqlInternalConnectionTds.Activate(Transaction transaction)
   at System.Data.ProviderBase.DbConnectionPool.PrepareConnection(DbConnection owningObject, DbConnectionInternal obj, Transaction transaction)
   at System.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at System.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at System.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   at System.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at System.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at System.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at System.Data.SqlClient.SqlConnection.Open()
",<c#><sql-server><.net-core>,3921,0,37,375,1,2,5,56,29197,0,0,2,24,2019-05-27 15:25,2019-05-27 20:17,2019-05-27 20:17,0,0,Advanced,32
50295175,How to check XAMPP's version on Windows?,"In Linux, its simple. I just type /opt/lampp/lampp status, and it tells me about the XAMPP version.
Version: XAMPP for Linux 5.6.35-0
Apache is running.
MySQL is running.
ProFTPD is running.
What is the equivalent command for XAMPP on windows?
",<php><mysql><apache><xampp>,244,0,4,1799,3,11,17,74,95723,0,14,7,24,2018-05-11 15:03,2018-09-27 15:41,,139,,Basic,1
56724622,How to fix 'postgres.h' file not found problem?,"I am trying to write a base type for PostgreSQL in C (using xcode), and I already installed PostgreSQL 11, but it seems that postgres.h cannot be simply included in the file (""'postgres.h' file not found"").
Could someone tell me how to fix that problem? And can I write code under an arbitary directory, or do I have to write under the PostgreSQL directory?
Or perhaps the question should be: how to install header files like postgres.h?
",<c><postgresql><postgresql-11>,438,0,2,459,1,5,13,54,31979,0,27,3,24,2019-06-23 14:02,2019-06-25 5:37,,2,,Intermediate,19
51817841,Create database view from django model,"I learned sql ""view"" as a virtual table to facilitate the SQL operations,  like
MySQL [distributor]&gt; CREATE VIEW CustomerEMailList AS
    -&gt; SELECT cust_id, cust_name, cust_email
    -&gt; FROM Customers
    -&gt; WHERE cust_email IS NOT NULL;
Query OK, 0 rows affected (0.026 sec)
MySQL [distributor]&gt; select * from customeremaillist;
+------------+---------------+-----------------------+
| cust_id    | cust_name     | cust_email            |
+------------+---------------+-----------------------+
| 1000000001 | Village Toys  | sales@villagetoys.com |
| 1000000003 | Fun4All       | jjones@fun4all.com    |
| 1000000004 | Fun4All       | dstephens@fun4all.com |
| 1000000005 | The Toy Store | kim@thetoystore.com   |
| 1000000006 | toy land      | sam@toyland.com       |
+------------+---------------+-----------------------+
5 rows in set (0.014 sec)
When I checked the Django documentation subsequently, there are no such functionality to create a virtual ""model table"" which could simplify the data manipulation.
Should I forget the virtual table ""view"" when using Django ORM?
",<django><sql-view>,1094,0,17,20291,20,83,143,67,28907,0,1063,3,24,2018-08-13 8:09,2018-08-13 8:43,2018-08-13 8:43,0,0,Intermediate,19
48951549,docker and mysql: Got an error reading communication packets,"I have a problem with connectivity in docker. I use an official mysql 5.7 image and Prisma server. When I start it via prisma cli, that uses docker compose underneath (described here) everything works. 
But I need to start this containers programmatically via docker api and in this case connections from app are dropped with [Note] Aborted connection 8 to db: 'unconnected' user: 'root' host: '164.20.10.2' (Got an error reading communication packets).
So what I doo:
Creating a bridge network:
const network = await docker.network.create({
Name: manifest.name + '_network',
IPAM: {
  ""Driver"": ""default"",
  ""Config"": [
    {
      ""Subnet"": ""164.20.0.0/16"",
      ""IPRange"": ""164.20.10.0/24""
    }
  ]
}});
Creating mysql container and attaching it to network
const mysql = await docker.container.create({
Image: 'mysql:5.7',
Hostname: manifest.name + '-mysql',
Names: ['/' + manifest.name + '-mysql'],
NetworkingConfig: {
  EndpointsConfig: {
    [manifest.name + '_network']: {
      Aliases: [manifest.name + '-mysql']
    }
  }
},
Restart: 'always',
Args: [
  ""mysqld"",
  ""--max-connections=1000"",
  ""--sql-mode=ALLOW_INVALID_DATES,ANSI_QUOTES,ERROR_FOR_DIVISION_BY_ZERO,HIGH_NOT_PRECEDENCE,IGNORE_SPACE,NO_AUTO_CREATE_USER,NO_AUTO_VALUE_ON_ZERO,NO_BACKSLASH_ESCAPES,NO_DIR_IN_CREATE,NO_ENGINE_SUBSTITUTION,NO_FIELD_OPTIONS,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_UNSIGNED_SUBTRACTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ONLY_FULL_GROUP_BY,PIPES_AS_CONCAT,REAL_AS_FLOAT,STRICT_ALL_TABLES,STRICT_TRANS_TABLES,ANSI,DB2,MAXDB,MSSQL,MYSQL323,MYSQL40,ORACLE,POSTGRESQL,TRADITIONAL""
],
Env: [
  'MYSQL_ROOT_PASSWORD=secret'
]
});
await network.connect({
   Container: mysql.id
});
await mysql.start();
Then I wait Mysql to boot, create needed databases and needed Prisma containers from prismagraphql/prisma:1.1 and start them. App server resolves mysql host correctly, but connections are dropped by mysql.
Telnet from app container to mysql container in 3306 port responds correctly:
J
5.7.21U;uH  Kem']#45T]2mysql_native_password
What am I doing wrong?
",<mysql><docker><docker-api><prisma><prisma-graphql>,2044,2,43,2396,3,28,42,80,20919,,26,3,24,2018-02-23 15:46,2019-04-12 15:15,,413,,Basic,14
51825799,Known reasons why sqlite3_open_v2 can take over 60s on windows?,"I will start with TL;DR version as this may be enough for some of you:
We are trying to investigate an issue that we see in diagnostic data of our C++ product.
The issue was pinpointed to be caused by timeout on sqlite3_open_v2 which supposedly takes over 60s to complete (we only give it 60s).
We tried multiple different configurations, but never were able to reproduce even 5s delay on this call.
So the question is if maybe there are some known scenarios in which sqlite3_open_v2 can take that long (on windows)?
Now to the details:
We are using version 3.10.2 of SQLite. We went through changelogs from this version till now and nothing we've found in the bugfixes section seems to suggest that there was some issue that was addressed in consecutive SQLite releases and may have caused our problem.
The issue we see affects around 0.1% unique user across all supported versions of windows (Win 7, Win 8, Win 10). There are no manual user complaints/reports about that - this can suggest that a problem happens in the context where something serious enough is happening with the user machine/system that he doesn't expect anything to work. So something that indicates system-wide failure is a valid possibility as long as it can possibly happen for 0.1% of random windows users.
There are no data indicating that the same issue ever occurred on Mac which is also supported platform with large enough sample of diagnostic data.
We are using Poco (https://github.com/pocoproject/poco, version: 1.7.2) as a tool for accessing our SQLite database, but we've analyzed the Poco code and it seems that failure on this code level can only (possibly) explain ~1% of all collected samples. This is how we've determined that the problem lies in sqlite3_open_v2 taking a long time.
This happens on both DELETE journal mode as well as on WAL.
It seems like after this problem happens the first time for a particular user each consecutive call to sqlite3_open_v2 takes that long until the user restarts whole application (possibly machine, no way to tell from our data).
We are using following flags setup for sqlite3_open_v2 (as in Poco):
sqlite3_open_v2(..., ..., SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE | SQLITE_OPEN_URI, NULL);
This usually doesn't happen on startup of the application so it's not likely to be caused by something happening while our application is not running. This includes power cuts offs causing data destruction (which tends to return SQLITE_CORRUPT anyway, as mentioned in https://www.sqlite.org/howtocorrupt.html).
We were never able to reproduce this issue locally even though we tried different things:
Multiple threads writing and reading from DB with synchronization required by a particular journaling system.
Keeping SQLite connection open for a long time and working on DB normally in a meanwhile.
Trying to hit HDD hard with other data (dumping /dev/rand (WSL) to multiple files from different processes while accessing DB normally).
Trying to force antivirus software to scan DB on every file access (tested with Avast with basically everything enabled including ""scan on open"" and ""scan on write"").
Breaking our internal synchronization required by particular journaling systems.
Calling WinAPI CreateFile with all possible combinations of file sharing options on DB file - this caused issues but sqlite3_open_v2 always returned fast - just with an error.
Calling WinAPI LockFile on random parts of DB file which is btw. nice way of reproducing SQLITE_IOERR, but no luck with reproducing the discussed issue.
Some additional attempts to actually stretch the Poco layer and double-check if our static analysis of codes is right.
We've tried to look for similar issues online but anything somewhat relevant we've found was here sqlite3-open-v2-performance-degrades-as-number-of-opens-increase. This doesn't seem to explain our case though, as the numbers of parallel connections are way beyond what we have as well as what would typical windows users have (unless there is some somewhat popular app exploiting SQLite which we don't know about).
It's very unlikely that this issue is caused by db being accessed through network share as we are putting the DB file inside %appdata% unless there is some pretty standard windows configuration which sets %appdata% to be a remote share.
Do you have any ideas what can cause that issue?
Maybe some hints on what else should we check or what additional diagnostic data that we can collect from users would be useful to pinpoint the real reason why that happens?
Thanks in advance
",<sqlite><poco>,4552,5,13,652,0,6,18,72,429,0,79,0,24,2018-08-13 15:29,,,,,Advanced,32
60790801,"Azure SQL DB Error, This location is not available for subscription","I am having pay as you go subscription and I am creating an Azure SQL server.
While adding server, on selection of location, I am getting this error:
This location is not available for subscriptions
Please help.
",<azure-sql-server>,212,0,1,243,0,2,5,71,9393,0,0,5,24,2020-03-21 17:12,2020-03-24 2:28,2020-03-24 2:28,3,3,Intermediate,19
50923408,Connect to postgres in docker container from host machine,"How can I connect to postgres in docker from a host machine?
docker-compose.yml
version: '2'
networks:
    database:
        driver: bridge
services:
    app:
        build:
            context: .
            dockerfile: Application.Dockerfile
        env_file:
            - docker/Application/env_files/main.env
        ports:
            - ""8060:80""
        networks:
           - database
        depends_on:
            - appdb
    appdb:
        image: postdock/postgres:1.9-postgres-extended95-repmgr32
        environment:
            POSTGRES_PASSWORD: app_pass
            POSTGRES_USER: www-data
            POSTGRES_DB: app_db
            CLUSTER_NODE_NETWORK_NAME: appdb
            NODE_ID: 1
            NODE_NAME: node1
        ports:
            - ""5432:5432""
        networks:
            database:
                aliases:
                    - database
docker-compose ps
           Name                          Command               State               Ports
-----------------------------------------------------------------------------------------------------
appname_app_1     /bin/sh -c /app/start.sh         Up      0.0.0.0:8060-&gt;80/tcp
appname_appdb_1   docker-entrypoint.sh /usr/ ...   Up      22/tcp, 0.0.0.0:5432-&gt;5432/tcp
From container I can connect successfully. Both from app container and db container.
List of dbs and users from running psql inside container:
# psql -U postgres
psql (9.5.13)
Type ""help"" for help.
postgres=# \du
                                       List of roles
    Role name     |                         Attributes                         | Member of
------------------+------------------------------------------------------------+-----------
 postgres         | Superuser, Create role, Create DB, Replication, Bypass RLS | {}
 replication_user | Superuser, Create role, Create DB, Replication             | {}
 www-data         | Superuser                                                  | {}
postgres=# \l
                                       List of databases
      Name      |      Owner       | Encoding |  Collate   |   Ctype    |   Access privileges
----------------+------------------+----------+------------+------------+-----------------------
 app_db         | postgres         | UTF8     | en_US.utf8 | en_US.utf8 |
 postgres       | postgres         | UTF8     | en_US.utf8 | en_US.utf8 |
 replication_db | replication_user | UTF8     | en_US.utf8 | en_US.utf8 |
 template0      | postgres         | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +
                |                  |          |            |            | postgres=CTc/postgres
 template1      | postgres         | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +
                |                  |          |            |            | postgres=CTc/postgres
(5 rows)
DB image is not official postgres image. But Dockerfile in GitHub seem looking fine.
cat /var/lib/postgresql/data/pg_hba.conf from DB container:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
# ""local"" is for Unix domain socket connections only
local   all             all                                     trust
# IPv4 local connections:
host    all             all             127.0.0.1/32            trust
# IPv6 local connections:
host    all             all             ::1/128                 trust
# Allow replication connections from localhost, by a user with the
# replication privilege.
#local   replication     postgres                                trust
#host    replication     postgres        127.0.0.1/32            trust
#host    replication     postgres        ::1/128                 trust
host all all all md5
host replication replication_user 0.0.0.0/0 md5
I tried both users with no luck
$ psql -U postgres -h localhost
psql: FATAL:  role ""postgres"" does not exist
$ psql -h localhost -U www-data appdb -W
Password for user www-data:
psql: FATAL:  role ""www-data"" does not exist
Looks like on my host machine there is already PSQL running on that port. How can I check it?
",<postgresql><docker><docker-compose>,4060,0,83,488,1,12,31,78,35736,0,61,6,24,2018-06-19 8:00,2018-06-21 11:15,,2,,Basic,9
49643047,Update multiple rows in sequelize with different conditions,"I'm trying to perform an update command with sequelize on rows in a postgres database. I need to be able to update multiple rows that have different conditions with the same value.
For example, assume I have a user table that contains the following fields:
ID
First Name
Last Name
Gender
Location
createdAt
Assume, I have 4 records in this table, I want to update records with ID - 1 and 4 with a new location say Nigeria.
Something like this: SET field1 = 'foo' WHERE id = 1, SET field1 = 'bar' WHERE id = 2
How can I achieve that with sequelize?
",<javascript><postgresql><sequelize.js>,548,0,1,1679,6,18,31,46,65126,0,12,3,24,2018-04-04 4:53,2018-04-04 5:02,2018-04-04 5:02,0,0,Basic,9
54302793,dyld: Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib,"I am facing the following issue on Mac when I run rake ts:index for Thinking Sphinx indexing:
dyld: Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib
I am using mysql version 8.0.13 for osx10.13 on x86_64.
How can I resolve this issue?
",<mysql><ruby-on-rails><macos><thinking-sphinx>,250,0,2,1337,1,10,16,72,19325,0,15,2,24,2019-01-22 6:57,2019-01-22 7:01,2019-01-22 7:01,0,0,Basic,14
51008807,NodeJS MySQL Client does not support authentication protocol,"When I am trying to connect with mysql 8.0 I am getting this error. How can I fix this ? 
code: 'ER_NOT_SUPPORTED_AUTH_MODE',
errno: 1251,
sqlMessage: 'Client does not support authentication protocol requested by server; 
consider upgrading MySQL client',
sqlState: '08004',
fatal: true
",<mysql><node.js><database><backend>,287,0,6,870,1,12,26,56,64904,0,166,3,24,2018-06-24 9:58,2018-11-25 0:26,,154,,Basic,14
57459643,TypeORM: Dynamically set database schema for EntityManager (or repositories) at runtime?,"Situation:
For our SaaS API we use schema-based multitenancy, which means every customer (~tenant) has its own separate schema within the same (postgres) database, without interfering with other customers. Each schema consists of the same underlying entity-model. 
Everytime a new customer is registered to the system, a new isolated schema is automatically created within the db. This means, the schema is created at runtime and not known in advance. The customer's schema is named according to the customer's domain.
For every request that arrives at our API, we extract the user's tenancy-affiliation from the JWT and determine which db-schema to use to perform the requested db-operations for this tenant.
Problem
After having established a connection to a (postgres) database via TypeORM (e.g. using createConnection), our only chance to set the schema for a db-operation is to resort to the createQueryBuilder:
const orders = await this.entityManager
  .createQueryBuilder()
  .select()
  .from(`${tenantId}.orders`, 'order') // &lt;--- setting schema-prefix here
  .where(""order.priority = 4"")
  .getMany();
This means, we are forced to use the QueryBuilder as it does not seem to be possible to set the schema when working with the EntityManager API (or the Repository API).
However, we want/need to use these APIs, because they are much simpler to write, require less code and are also less error-prone, since they do not rely on writing queries ""manually"" employing a string-based syntax.
Question
In case of TypeORM, is it possible to somehow set the db-schema when working with the EntityManager or repositories?
Something like this?
// set schema when instantiating manager
const manager = connection.createEntityManager({ schema: tenantDomain });
// should find all matching ""order"" entities within schema
const orders = manager.find(Order, { priority: 4 })
// should find a matching ""item"" entity within schema using same manager
const item = manager.findOne(Item, { id: 321 })
Notes:
The db-schema needs to be set in a request-scoped way to avoid setting the schema for other requests, which may belong to other customers. Setting the schema for the whole connection is not an option.
We are aware that one could create a whole new connection and set the schema for this connection, but we want to reuse the existing connection. So simply creating a new connection to set the schema is not an option.
",<postgresql><orm><multi-tenant><nestjs><typeorm>,2417,6,17,14367,9,58,74,56,24059,0,1380,3,24,2019-08-12 10:36,2019-10-09 9:09,2020-02-08 15:26,58,180,Basic,4
49178334,Prevent model hydration on Eloquent queries,"Is it possible to have an eloquent query builder return StdClass rather then Model?
For example User::where('age', '&gt;', 34)-&gt;get() returns a Collection of User models.
Whereas DB::table('users')-&gt;where('age', '&gt;', 34)-&gt;get() returns a Collection of StdClass objects. Much faster.
Therefore:
Is it possible to prevent hydrating eloquent models and return StdClass objects as a database query builder would, but still leverage the usefulness of an eloquent query builder syntax?
",<php><mysql><laravel><performance><eloquent>,492,0,9,13560,23,91,160,68,5574,0,1176,3,24,2018-03-08 16:49,2018-03-08 21:23,2019-01-20 4:24,0,318,Basic,2
52085191,ModuleNotFoundError: No module named 'pyodbc' when importing pyodbc into py script,"I've written a short python script which tries to import the pyodbc extension package so I can access my SQL table.
import pyodbc as pyodbc
cnxn = pyodbc.connect('Driver={SQL Server};'
                      'Server=DESKTOP-UO8KJOP;'
                      'Database=ExamplePFData'
                      'Trusted_Connection=yes;')
I have definitely installed the extension by using: pip install pyodbc. And when I go to install it again, cmd says: Requirement already satisfied: pyodbc in ... and I've found the pyd file in my directories.
I have also tried installing pypyodbc, which didn't work.
The error I get is:
Traceback (most recent call last):
File ""C:\Users\Jerry\Documents\Python\SQLembed.py"", line 5, in &lt;module&gt;
import pyodbc as pyodbc
ModuleNotFoundError: No module named 'pyodbc'
(where line 5 is the 'import pyodbc' line)
I have tried copying the pyodbc.cp37-win_amd64.pyd file into my Python Scripts folder and into the folder where my pip.exe file is.
Currently python is my Python37 folder. 
pyodbc.cp37-win_amd64.pyd is in Python > Lib > site-packages.
Can anyone help me fix this error please so that I can import pyodbc?
Do all of the python extensions/modules that I install via pip need to be in the same folder/directory as python.exe?
",<python><sql><sql-server><sqlite><pyodbc>,1265,0,9,251,1,2,6,65,138157,0,3,13,24,2018-08-29 19:56,2018-08-29 20:57,,0,,Basic,14
56108003,How to replace null value with value from the next row,"I need support in my sql query code. I have to replace null value in a column with not-null value from the next row. 
as a example we can use this code:
declare   @value table (r# int, value varchar(15))
insert into @value ( r#, value ) values
 (1, NULL   ) ,
 (2, 'January'), 
 (3, 'February' ), 
 (4, NULL    ),
 (5, 'March'  ),
 (6, NULL    ),
(7, Null  ),
(8, 'December' ),
(9, Null ),
(10, Null  ),
(11, Null  ),
(12, 'November' ),
(13, Null )
select * from @value
When I use lead function I get this value but it does not work with NULLs.
What I need is to get:
1 January
2 January
3 February
4 March
5 March
6 December
7 December
8 December
9 November
10 November
11 November
12 November
13 NULL
Bu from my query :
SELECT r#, 
  value
 ,case when value is null  then Lead(value) OVER ( order by  r#  asc) else value end as RESULT 
FROM @value
order by r#
I have:
",<sql><sql-server>,870,1,34,265,1,2,7,68,10346,0,9,4,24,2019-05-13 7:59,2019-05-13 8:35,2019-05-13 10:53,0,0,Basic,9
58952919,"PostgreSQL: background worker ""logical replication launcher"" exited with exit code 1","Using our own instance of Gitlab we get the error background worker ""logical replication launcher"" exited with exit code 1 when trying to use the postgres service in our runners. Haven't found anything useful over the internet. Any idea what's going on? 
Versions:
Gitlab 12.4.3
gitlab-runner 12.5.0 (limit of 4 concurrent jobs)
postgres 12.1 (tried with 11 and same result)
DigitalOcean droplet: CPU-Optimized / 8 GB / 4 vCPUs 
Relevant part in gitlab-ci.yml:
image: golang:1.12
services:
  - postgres
variables:
  POSTGRES_USER: postgres
  POSTGRES_DB: xproject_test
  POSTGRES_PASSWORD: postgres
Failure log:
Running with gitlab-runner 12.5.0 (577f813d)
  on xproject sEZeszwx
Using Docker executor with image xproject-ci ...
Starting service postgres:latest ...
Pulling docker image postgres:latest ...
Using docker image sha256:9eb7b0ce936d2eac8150df3de7496067d56bf4c1957404525fd60c3640dfd450 for postgres:latest ...
Waiting for services to be up and running...
*** WARNING: Service runner-sEZeszwx-project-18-concurrent-0-postgres-0 probably didn't start properly.
Health check error:
service ""runner-sEZeszwx-project-18-concurrent-0-postgres-0-wait-for-service"" timeout
Health check container logs:
Service container logs:
2019-11-20T10:16:23.805738908Z The files belonging to this database system will be owned by user ""postgres"".
2019-11-20T10:16:23.805807212Z This user must also own the server process.
2019-11-20T10:16:23.805818432Z 
2019-11-20T10:16:23.806094094Z The database cluster will be initialized with locale ""en_US.utf8"".
2019-11-20T10:16:23.806120707Z The default database encoding has accordingly been set to ""UTF8"".
2019-11-20T10:16:23.806208494Z The default text search configuration will be set to ""english"".
2019-11-20T10:16:23.806264704Z 
2019-11-20T10:16:23.806282587Z Data page checksums are disabled.
2019-11-20T10:16:23.806586302Z 
2019-11-20T10:16:23.806931287Z fixing permissions on existing directory /var/lib/postgresql/data ... ok
2019-11-20T10:16:23.807763042Z creating subdirectories ... ok
2019-11-20T10:16:23.808045789Z selecting dynamic shared memory implementation ... posix
2019-11-20T10:16:23.835644353Z selecting default max_connections ... 100
2019-11-20T10:16:23.866604734Z selecting default shared_buffers ... 128MB
2019-11-20T10:16:23.928432088Z selecting default time zone ... Etc/UTC
2019-11-20T10:16:23.929447992Z creating configuration files ... ok
2019-11-20T10:16:24.122662589Z running bootstrap script ... ok
2019-11-20T10:16:24.706975030Z performing post-bootstrap initialization ... ok
2019-11-20T10:16:24.819117668Z initdb: warning: enabling ""trust"" authentication for local connections
2019-11-20T10:16:24.819150100Z You can change this by editing pg_hba.conf or using the option -A, or
2019-11-20T10:16:24.819157763Z --auth-local and --auth-host, the next time you run initdb.
2019-11-20T10:16:24.819272849Z syncing data to disk ... ok
2019-11-20T10:16:24.819313390Z 
2019-11-20T10:16:24.819328954Z 
2019-11-20T10:16:24.819340787Z Success. You can now start the database server using:
2019-11-20T10:16:24.819349374Z 
2019-11-20T10:16:24.819357407Z     pg_ctl -D /var/lib/postgresql/data -l logfile start
2019-11-20T10:16:24.819365840Z 
2019-11-20T10:16:24.857656160Z waiting for server to start....2019-11-20 10:16:24.857 UTC [46] LOG:  starting PostgreSQL 12.1 (Debian 12.1-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
2019-11-20T10:16:24.860371378Z 2019-11-20 10:16:24.860 UTC [46] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
2019-11-20T10:16:24.886271885Z 2019-11-20 10:16:24.886 UTC [47] LOG:  database system was shut down at 2019-11-20 10:16:24 UTC
2019-11-20T10:16:24.892844968Z 2019-11-20 10:16:24.892 UTC [46] LOG:  database system is ready to accept connections
2019-11-20T10:16:24.943542403Z  done
2019-11-20T10:16:24.943591286Z server started
2019-11-20T10:16:25.084670051Z CREATE DATABASE
2019-11-20T10:16:25.086153670Z 
2019-11-20T10:16:25.086604000Z 
2019-11-20T10:16:25.086694058Z /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
2019-11-20T10:16:25.086711933Z 
2019-11-20T10:16:25.088473308Z 2019-11-20 10:16:25.088 UTC [46] LOG:  received fast shutdown request
2019-11-20T10:16:25.090893184Z waiting for server to shut down....2019-11-20 10:16:25.090 UTC [46] LOG:  aborting any active transactions
2019-11-20T10:16:25.092499368Z 2019-11-20 10:16:25.092 UTC [46] LOG:  background worker ""logical replication launcher"" (PID 53) exited with exit code 1
2019-11-20T10:16:25.093942785Z 2019-11-20 10:16:25.093 UTC [48] LOG:  shutting down
2019-11-20T10:16:25.112341160Z 2019-11-20 10:16:25.112 UTC [46] LOG:  database system is shut down
2019-11-20T10:16:25.189351710Z  done
2019-11-20T10:16:25.189393803Z server stopped
2019-11-20T10:16:25.189929555Z 
2019-11-20T10:16:25.189967760Z PostgreSQL init process complete; ready for start up.
2019-11-20T10:16:25.189982340Z 
2019-11-20T10:16:25.214046388Z 2019-11-20 10:16:25.213 UTC [1] LOG:  starting PostgreSQL 12.1 (Debian 12.1-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
2019-11-20T10:16:25.214092434Z 2019-11-20 10:16:25.213 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
2019-11-20T10:16:25.214172706Z 2019-11-20 10:16:25.214 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
2019-11-20T10:16:25.219769380Z 2019-11-20 10:16:25.219 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
2019-11-20T10:16:25.241614800Z 2019-11-20 10:16:25.241 UTC [64] LOG:  database system was shut down at 2019-11-20 10:16:25 UTC
2019-11-20T10:16:25.248887712Z 2019-11-20 10:16:25.248 UTC [1] LOG:  database system is ready to accept connections
*********
",<postgresql><docker><gitlab><gitlab-ci-runner>,5737,0,85,3528,2,27,38,61,38744,,810,4,24,2019-11-20 11:08,2019-11-20 11:14,2019-12-10 11:18,0,20,Advanced,32
53746197,Cannot delete rows from a temporal history table,"I've recently discovered temporal tables in SQL Server. I'd like to start using this functionality. However the biggest hurdle is not being able to delete records from it. Due to GDPR compliance this is an absolute must.
Deleting records from a history table obviously leads to the error: 
  Cannot delete rows from a temporal history table
So to be able to delete records from a history table I have to disable SYSTEM_VERSIONING, then delete, and then re-enable SYSTEM_VERSIONING. Unless there's another way I'm not aware of?
Since it's not possible to use GO's in a stored procedure/SqlCommand, how can I ensure deleting a history record does not mess with other transactions e.g. updates sent to the temporal table during deleting records from the history table will still result in records being added to the history table? 
I've tried creating a stored procedure to wrap it in one transaction but this fails because the ALTER TABLE statement disabling the SYSTEM_VERSIONING is not executed yet leading to the same error.
CREATE PROCEDURE [dbo].[OrderHistoryDelete]
     (@Id UNIQUEIDENTIFIER)
AS
BEGIN
    BEGIN TRANSACTION
    ALTER TABLE [dbo].[Order] SET ( SYSTEM_VERSIONING = OFF )
    -- No GO possible here obviously.
    DELETE FROM [dbo].[OrderHistory] WITH (TABLOCKX) 
    WHERE [Id] = @Id
    ALTER TABLE [dbo].[Order] SET ( SYSTEM_VERSIONING = ON (HISTORY_TABLE = [dbo].[OrderHistory]))
    COMMIT TRANSACTION
END
GO
",<sql-server><t-sql>,1433,0,22,2601,3,24,41,39,26493,0,49,2,24,2018-12-12 15:22,2018-12-12 18:59,2018-12-12 18:59,0,0,Advanced,32
50766928,Presto array contains an element that likes some pattern,"For example, one column in my table is an array, I want to check if that column contains an element that contains substring ""denied"" (so elements like ""denied at 12:00 pm"", ""denied by admin"" will all count, I believe I will have to use ""like"" to identify the pattern). How to write sql for this? 
",<sql><presto><trino>,297,0,0,2117,5,23,48,50,85061,0,110,4,24,2018-06-08 19:00,2018-06-08 20:48,,0,,Basic,10
50322966,Changing Django development Database from the default SQLite to PostgreSQL,"What are the steps I need to take to migrate from the default SQLite database to Postgres database?
I'm doing this to get my local development environment as close to my live server (which uses postrgres).
Or is there a reason why local development uses SQLite? Is it not recommended to use Postgres for local development?
",<python><django><postgresql><sqlite>,323,0,0,8357,27,108,215,41,19371,0,258,4,24,2018-05-14 4:07,2018-05-14 4:15,2018-05-14 4:15,0,0,Basic,10
56911833,Postgres and alembic - Assuming SERIAL sequence,"I have a postgres DB, which I manage through SQLAlchemy and alembic (for migrations). When creating a DB migration  through alembic, I get the following INFO in the console.
INFO  [alembic.ddl.postgresql] Detected sequence named 'my_table_name_id_seq' as owned by integer column 'my_table_name(id)', assuming SERIAL and omitting
My model looks like
class CopyJob(db.Model):
    __tablename__ = ""my_table_name""
    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
I interpret the above as a warning. One line is generated for each of my tables. I have two questions:
Am I doing something wrong when getting the warning above
What should I fix / explicitly set, in order to make it go away. Migrations are too verbose.
Thank you!
",<python><postgresql><flask><alembic>,744,0,5,3514,5,34,62,60,6597,0,1019,1,24,2019-07-06 6:35,2021-08-08 22:57,,764,,Basic,5
61441186,"PostgreSQL: What is meant by ""please specify covering index name""","I wanted to edit the actions in a table. However I get the error message ""Please specify covering index name."" when I try to edit the FK. How do I fix this?
The table consists of only two columns:
The foreign keys:
category FK:
",<postgresql><pgadmin-4>,228,3,0,385,1,4,12,75,14817,0,8,12,23,2020-04-26 12:57,2020-04-26 14:38,2020-04-26 14:38,0,0,Basic,9
50672815,Unable to retrieve project metadata. Ensure it's an MSBuild-based .NET Core project,"I've been researching a lot online but did not find a proper solution. I was trying to use Entity Framework Core with MySQL by using database-first scaffold method to mapping table model while always received this error when applied the command
Unable to retrieve project metadata. Ensure it's an MSBuild-based .NET Core project. If you're using custom BaseIntermediateOutputPath or MSBuildProjectExtensionsPath values, Use the --msbuildprojectextensionspath option.
This is the command I am using to scaffold the database model:
Scaffold-DbContext ""server=localhost;port=3306;user=root;password=1234;database=world"" ""Pomelo.EntityFrameworkCore.MySql"" -OutputDir .\Models -f
And this is my .Net Core project setting:
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=""Microsoft.EntityFrameworkCore.Tools"" Version=""2.0.1"" /&gt;
    &lt;PackageReference Include=""Pomelo.EntityFrameworkCore.MySql"" Version=""2.0.1"" /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;DotNetCliToolReference Include=""Microsoft.EntityFrameworkCore.Tools.DotNet"" Version=""2.0.1"" /&gt;
  &lt;/ItemGroup&gt;
",<mysql><entity-framework><asp.net-core-2.0><ef-database-first><entity-framework-core-2.1>,1088,0,10,3491,9,33,60,81,16440,0,76,7,23,2018-06-04 2:40,2018-09-24 23:04,,112,,Basic,9
52926064,How to connect to SQL Server docker container from another container?,"I have pulled and run SQL Server 2017 container image using the following command:
docker pull microsoft/mssql-server-linux
docker run --name mssql -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=!Abcd123' -p 1433:1433 -d microsoft/mssql-server-linux
And I also deployed an ASP.NET Core Web API application to a Docker container, using the following commands:
dotnet publish -c Release -o Output
docker build -t apitest .
docker run -p 3000:80 --name apitest_1 apitest
The content of Dockerfile:
FROM microsoft/dotnet
COPY Output /app
WORKDIR /app
EXPOSE 80/tcp
ENTRYPOINT [""dotnet"", ""DockerSQLTest.dll""]
In my Web API application, I have created an Entity Framework Core migration which will create the database and seed some data. In Configure method of Startup class, I add the following code to apply the pending migrations to the database:
public async void Configure(IApplicationBuilder app,
                            IHostingEnvironment env, 
                            StudentDbContext dbContext)
{
    await dbContext.Database.MigrateAsync();
    ...
}
And the database connection string is retrieved from appsettings.json which contains the following section:
""ConnectionStrings"": {
    ""DefaultConnection"": ""Server=localhost,1433;Database=student;User Id=sa;Password=!Abcd123;""
}
But the app cannot run correctly, the exception message:
fail: WebApplication6.Startup[0]
  System.Threading.Tasks.TaskCanceledException: A task was canceled.
     at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnectionAsync(Boolean errorsExpected, CancellationToken cancellationToken)
     at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenAsync(CancellationToken cancellationToken, Boolean errorsExpected)
     at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerDatabaseCreator.&lt;&gt;c__DisplayClass20_0.&lt;&lt;ExistsAsync&gt;b__0&gt;d.MoveNext()
  --- End of stack trace from previous location where exception was thrown ---
     at Microsoft.EntityFrameworkCore.SqlServer.Storage.Internal.SqlServerExecutionStrategy.ExecuteAsync[TState,TResult](TState state, Func`4 operation, Func`4 verifySucceeded, CancellationToken cancellationToken)
     at Microsoft.EntityFrameworkCore.Migrations.HistoryRepository.ExistsAsync(CancellationToken cancellationToken)
     at Microsoft.EntityFrameworkCore.Migrations.Internal.Migrator.MigrateAsync(String targetMigration, CancellationToken cancellationToken)
Is there anything wrong?
",<sql-server><docker><asp.net-core><entity-framework-core><asp.net-core-2.0>,2465,0,33,1076,1,8,21,78,24905,0,175,4,23,2018-10-22 9:23,2018-10-22 9:45,2018-11-16 7:33,0,25,Basic,9
49355530,System.Data.SqlClient is not supported on this platform,"I'm using ASP.NET Core 2 with Entity Framework Core 2.0.2. I created a context and Add-Migrations command in Package Manager Controller works fine.
However when Update-Database command is used, I get an error:
  System.Data.SqlClient is not supported on this platform
I can't figure out where the problem is. Can you help me? Thanks.
My .csproj file:
&lt;Project Sdk=""Microsoft.NET.Sdk.Web""&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netcoreapp2.0&lt;/TargetFramework&gt;
    &lt;DebugType&gt;portable&lt;/DebugType&gt;
    &lt;PreserveCompilationContext&gt;true&lt;/PreserveCompilationContext&gt;
    &lt;DockerComposeProjectPath&gt;..\docker-compose.dcproj&lt;/DockerComposeProjectPath&gt;
  &lt;/PropertyGroup&gt;
  &lt;ItemGroup&gt;
    &lt;Folder Include=""wwwroot\"" /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;PackageReference Include=""Autofac.Extensions.DependencyInjection"" Version=""4.2.1"" /&gt;
    &lt;PackageReference Include=""Microsoft.AspNetCore.All"" Version=""2.0.6"" /&gt;
    &lt;PackageReference Include=""Microsoft.EntityFrameworkCore"" Version=""2.0.2"" /&gt;
    &lt;PackageReference Include=""Microsoft.EntityFrameworkCore.Design"" Version=""2.0.2"" /&gt;
    &lt;PackageReference Include=""Microsoft.EntityFrameworkCore.SqlServer"" Version=""2.0.2"" /&gt;
    &lt;PackageReference Include=""Microsoft.EntityFrameworkCore.Tools"" Version=""2.0.2"" /&gt;
    &lt;PackageReference Include=""Swashbuckle.AspNetCore"" Version=""2.3.0"" /&gt;
  &lt;/ItemGroup&gt;
  &lt;ItemGroup&gt;
    &lt;DotNetCliToolReference Include=""Microsoft.VisualStudio.Web.CodeGeneration.Tools"" Version=""2.0.2"" /&gt;
    &lt;DotNetCliToolReference Include=""Microsoft.EntityFrameworkCore.Tools.DotNet"" Version=""2.0.2"" /&gt;
  &lt;/ItemGroup&gt;
&lt;/Project&gt;
",<c#><sql-server><asp.net-core><entity-framework-core><database-migration>,1755,0,32,397,1,5,14,45,37910,0,74,11,23,2018-03-19 3:50,2018-03-22 19:35,2018-03-22 19:35,3,3,Basic,9
62333176,Docker difference postgres:12 from postgres:12-alpine,"Docker hub contains several versions(tag) of Postgres db such as: 
12.3, 12, latest
12.3-alpine, 12-alpine, alpine
-...
What is diff between postgres version 12.3 and 12.3-alpine?
",<postgresql><docker>,180,0,0,525,2,7,17,36,16863,,366,3,23,2020-06-11 20:38,2020-06-11 20:43,2020-06-11 20:44,0,0,Intermediate,19
61162288,run-as package: not debuggable,"I'm trying to extract the database file from my Android device (non rooted Exynos Galaxy S9 running One UI 2.0) and every time I open up Android Studio 3.6.2 -> Device File Explorer I get the message ""run-as package: not debuggable"". 
This happens to every app I have in the list, not just the one i'm interested in.
Also, this issue persists with adb shell.
Can anyone help? (sorry if this has incomplete information or was posted in the wrong section)
",<android><database><sqlite><pull>,454,0,0,351,1,2,7,63,32354,0,0,1,23,2020-04-11 19:09,2020-04-11 19:21,2020-04-11 19:21,0,0,Basic,9
51542703,knex: select rows that are in certain date range,"How can I select rows from the table that are in certain date range with knex queries? For example, selecting rows from last seven days.
Knex version: 0.15.0
DB: PostgreSQL
",<postgresql><knex.js>,173,0,0,8953,13,34,41,52,47916,0,1343,5,23,2018-07-26 15:38,2018-08-03 8:17,2018-08-03 8:17,8,8,Basic,10
65422890,"How to use time-series with Sqlite, with fast time-range queries?","Let's say we log events in a Sqlite database with Unix timestamp column ts:
CREATE TABLE data(ts INTEGER, text TEXT);   -- more columns in reality
and that we want fast lookup for datetime ranges, for example:
SELECT text FROM data WHERE ts BETWEEN 1608710000 and 1608718654;
Like this, EXPLAIN QUERY PLAN gives SCAN TABLE data which is bad, so one obvious solution is to create an index with CREATE INDEX dt_idx ON data(ts).
Then the problem is solved, but it's rather a poor solution to have to maintain an index for an already-increasing sequence / already-sorted column ts for which we could use a B-tree search in O(log n) directly. Internally this will be the index:
ts           rowid
1608000001   1
1608000002   2
1608000012   3
1608000077   4
which is a waste of DB space (and CPU when a query has to look in the index first).
To avoid this:
(1) we could use ts as INTEGER PRIMARY KEY, so ts would be the rowid itself. But this fails because ts is not unique: 2 events can happen at the same second (or even at the same millisecond).
See for example the info given in SQLite Autoincrement.
(2) we could use rowid as timestamp ts concatenated with an increasing number. Example:
 16087186540001      
 16087186540002
 [--------][--]
     ts     increasing number 
Then rowid is unique and strictly increasing (provided there are less than 10k events per second), and no index would be required. A query WHERE ts BETWEEN a AND b would simply become WHERE rowid BETWEEN a*10000 AND b*10000+9999.
But is there an easy way to ask Sqlite to INSERT an item with a rowid greater than or equal to a given value? Let's say the current timestamp is 1608718654 and two events appear:
  CREATE TABLE data(ts_and_incr INTEGER PRIMARY KEY AUTOINCREMENT, text TEXT);
  INSERT INTO data VALUES (NEXT_UNUSED(1608718654), &quot;hello&quot;)  #16087186540001 
  INSERT INTO data VALUES (NEXT_UNUSED(1608718654), &quot;hello&quot;)  #16087186540002
More generally, how to create time-series optimally with Sqlite, to have fast queries WHERE timestamp BETWEEN a AND b?
",<sql><database><sqlite><time-series><auto-increment>,2056,1,33,42385,103,394,707,41,13968,0,3760,2,23,2020-12-23 10:38,2020-12-23 21:49,2020-12-23 21:49,0,0,Intermediate,16
50828098,Permissions For Google Cloud SQL Import Using Service Accounts,"I've exported MySQL Database following the MySQL Export Guide successfully. 
Now, I'm trying to import MySQL Database following the MySQL Import Guide.
I've checked the permissions for the service_account_email I'm using, and I have allowed both Admin SQL and Admin Storage permissions. 
I was able to successfully activate my service account using this command locally:
gcloud auth activate-service-account &lt;service_account_email&gt; --key-file=&lt;service_account_json_file&gt;  
After I ran the command:
gcloud sql import sql &lt;instance&gt; &lt;gstorage_file&gt; --database=&lt;db_name&gt; --async
I got this information:
{
  ""error"": {
    ""errors"": Array[1][
      {
        ""domain"": ""global"",
        ""reason"": ""required"",
        ""message"": ""Login Required"",
        ""locationType"": ""header"",
        ""location"": ""Authorization""
      }
    ],
    ""code"": 401,
    ""message"": ""Login Required""
  }
}
Other Things I've Tried
I also tried using the service_account_email of my SQL instance, which came from:
gcloud sql instances describe &lt;instance_name&gt;
But, it seems to have the same error.
Question
Based on the REST API JSON error I'm given, how do I ""login"" using the service_account_email so I wouldn't get the 401 Error?
",<google-cloud-platform><google-cloud-storage><google-cloud-sql><user-permissions><service-accounts>,1243,2,22,1840,2,24,51,45,14274,0,240,5,23,2018-06-13 1:56,2018-06-14 15:27,2018-12-04 8:43,1,174,Intermediate,17
62111066,mysqlclient installation error in AWS Elastic Beanstalk,"I am deploying a django with mysql app on AWS Elastic Beanstalk, so mysqlclient library is needed. mysqlclient needs python3-devel and mysql-devel package to be installed, so I have the custom config file for it 01_packages.config:
packages: 
  yum:
    python3-devel: []
    mysql-devel: []
Deployment fails and the log file /var/log/cfn-init.log (mentioned in Beanstalk logs) shows the error: 
2020-05-31 02:17:37,565 [INFO] -----------------------Starting build-----------------------
2020-05-31 02:17:37,572 [INFO] Running configSets: Infra-EmbeddedPreBuild
2020-05-31 02:17:37,575 [INFO] Running configSet Infra-EmbeddedPreBuild
2020-05-31 02:17:37,579 [INFO] Running config prebuild_0_doyouknow
2020-05-31 02:17:41,831 [ERROR] mysql-devel is not available to be installed
2020-05-31 02:17:41,831 [ERROR] Error encountered during build of prebuild_0_doyouknow: Yum does no
t have mysql-devel available for installation
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py"", line 542, in run_config
    CloudFormationCarpenter(config, self._auth_config).build(worklog)
  File ""/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py"", line 229, in build
    changes['packages'][manager] = CloudFormationCarpenter._packageTools[manager]().apply(packages,
 self._auth_config)
  File ""/usr/lib/python2.7/site-packages/cfnbootstrap/rpm_tools.py"", line 74, in apply
    raise ToolError(""Yum does not have %s available for installation"" % pkg_spec)
ToolError: Yum does not have mysql-devel available for installation
2020-05-31 02:17:41,834 [ERROR] -----------------------BUILD FAILED!------------------------
However, I tried to install it manually on my Ec2 instance through yum install mysql-devel and it is installed successfully.
My python version is 3.7 and my requirements.txt file content is:
asgiref==3.2.7
Django==3.0.5
django-cors-headers==3.2.1
django-dotenv==1.4.2
django-social-share==1.4.0
mysqlclient==1.4.6
numpy==1.18.4
pandas==1.0.3
Pillow==7.1.1
python-dateutil==2.8.1
pytz==2019.3
six==1.14.0
sqlparse==0.3.1
xlrd==1.2.0
",<mysql><amazon-ec2><amazon-elastic-beanstalk>,2098,0,43,1420,1,12,29,70,5681,0,34,2,23,2020-05-31 3:06,2020-05-31 12:48,2020-05-31 12:48,0,0,Basic,14
54375913,Athena: Query exhausted resources at scale factor,"I am running a query like: 
SELECT f.*, p.countryName, p.airportName, a.name AS agentName
FROM (
    SELECT 
        f.outboundlegid, 
        f.inboundlegid,
        f.querydatetime,
        cast(f.agent as bigint) as agent,
        cast(f.querydestinationplace as bigint) as querydestinationplace,
        f.queryoutbounddate,
        f.queryinbounddate,
        f.quoteageinminutes,
        f.price
    FROM flights f
    WHERE querydatetime &gt;= '2018-01-02'
    AND querydatetime &lt;= '2019-01-10'
) f
INNER JOIN (
  SELECT airportId, airportName, countryName
  FROM airports
  WHERE countryName IN ('Philippines', 'Indonesia', 'Malaysia', 'Hong Kong', 'Thailand', 'Vietnam')
) p
ON f.querydestinationplace = p.airportId
INNER JOIN agents a
ON f.agent = a.id
ORDER BY f.outboundlegid, f.inboundlegid, f.agent, querydatetime DESC
What's wrong with it? Or how can I optimize it? It gives me 
  Query exhausted resources at this scale factor
I have a flights table and I want to query for flights inside a specific country
",<sql><amazon-web-services><query-optimization><amazon-athena><presto>,1027,0,25,85007,187,498,808,60,28981,0,798,1,23,2019-01-26 5:30,2019-01-26 9:56,2019-01-26 9:56,0,0,Intermediate,23
65013610,Way to have SQL intellisense or autocompletion inside code strings in Visual Studio Code?,"In JetBrains IDE's like PHPStorm this is a built-in feature. E.g. in the code below, editing the SQL inside the string would autocomplete as SQL and suggest table/column names from an active database connection.
query(&quot;SELECT * FROM users LIMIT 50&quot;);
When using Visual Studio Code or similar editors like Theia, this functionality would be supplied by a plugin. Unfortunately, I haven't come across a plugin on the marketplace that has this feature. Some have autocomplete for .sql files, but not inline SQL. It's hard to believe this isn't possible yet in such a popular editor.
Has anyone found a solution for this?
Plugins I've tried so far (I'm specifically looking for Postgres):
SQLTools
PostgreSQL - privately maintained
PostgreSQL - abandonware by Microsoft
",<sql><postgresql><visual-studio-code><autocomplete><intellisense>,776,7,2,349,0,5,12,81,8648,0,84,2,23,2020-11-25 22:31,2022-09-04 17:10,,648,,Intermediate,20
57905821,Tables and views keep on fetching in MYSQL,"I am fairly new to MYSQL. I just today installed MYSQL on my Windows 10 computer for my personal learning. I installed mySQL workbench 8.0 and created some tables in the database. But the problem I am facing is the tables, views, stored procedures and functions are keep on fetching. I see some solutions online to run a linux command to resolve the problem. But mine is on windows and not sure where and how to execute the command. Can somebody please help me resolving the problem I am facing with MYSQL.
",<mysql>,507,1,0,668,1,8,21,37,25737,,27,5,23,2019-09-12 11:33,2020-06-26 15:42,,288,,Basic,6
48481028,Building SQL Server Database Project In Ubuntu,"I'm building an ASP.NET Core 2.0 Web API application that is hosted in an Ubuntu environment. So far, I've had great success getting things building and running (for the .NET Core app) in Ubuntu.
For the database, I have a SqlProj included in my solution. The project includes typical things such as tables, SPs, and pre/post deployment scripts. I'm using the following command (on my Windows-based dev machine) to build and deploy this project:
msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
When I take this approach, everything builds and deploys properly; however, since I will be taking advantage of the .NET Core CLI commands + CI/CD that targets an Ubuntu environment, I'd like to do something more like:
dotnet msbuild .\MyProject.DB.sqlproj /t:Build /t:Publish /P:SqlPublishProfilePath=""./PublishProfiles/MyProject.DB.publish.xml""
In Windows, I immediately get the error:
error MSB4019: The imported project ""C:\Program Files\dotnet\sdk\2.1.4\Microsoft\VisualStudio\v11.0\SSDT\Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
Basically, what I'm asking is how to successfully build and deploy a SqlProj project in an Ubuntu environment. I've tried Googling, but I have had zero luck thus far. All of the similar issues that I've found were for individuals who were editing their .proj file to target their VS folder's SSDT. All of these individuals were fixing the issue in Windows. This approach will not work in Ubuntu, since the targets file uses Windows registry keys.
EDIT: I'm aware that SSDT is needed in order to perform such a deployment using MSBuild. I've found no evidence that installing/using SSDT is even possible in Ubuntu. If it is not, perhaps there is an alternative solution?
FYI, I'm aware that using a code-first approach with EF Core is possible. I'm attempting to take the raw SP approach (along with leveraging indexes) and keep track of all of my code using SqlProj instead. This will all be stored and CI/CDed from a Git repo.
",<sql-server><ubuntu><msbuild><.net-core><sql-server-data-tools>,2151,0,3,669,0,5,21,62,5222,0,146,7,23,2018-01-27 22:05,2018-01-28 18:10,2020-04-28 18:14,1,822,Intermediate,17
50833992,Postgresql | No space left on device,"I am getting space issue while running a batch process on PostgreSQL database.
However, df -h command shows that machine has enough space
below is the exact error 
org.springframework.dao.DataAccessResourceFailureException: PreparedStatementCallback; SQL [INSERT into BATCH_JOB_INSTANCE(JOB_INSTANCE_ID, JOB_NAME, JOB_KEY, VERSION) values (?, ?, ?, ?)]; ERROR: could not extend file ""base/16388/16452"": No space left on device
  Hint: Check free disk space.
What is causing this issue? 
EDIT
postgres data directory is /var/opt/rh/rh-postgresql96/lib/pgsql/data
df -h /var/opt/rh/rh-postgresql96/lib/pgsql/data
Filesystem      Size  Used Avail Use% Mounted on
/dev/xvda2      100G   63G   38G  63% /
",<postgresql><database-administration><postgresql-9.6>,700,1,7,1636,2,22,44,68,67654,0,105,1,23,2018-06-13 9:40,2018-06-13 11:22,2018-06-13 11:22,0,0,Advanced,33
50813493,NameError: name 'dbutils' is not defined in pyspark,"I am running a pyspark job in databricks cloud. I need to write some of the csv files to databricks filesystem (dbfs) as part of this job and also i need to use some of the dbutils native commands like,
#mount azure blob to dbfs location
dbutils.fs.mount (source=""..."",mount_point=""/mnt/..."",extra_configs=""{key:value}"")
I am also trying to unmount once the files has been written to the mount directory. But, when i am using dbutils directly in the pyspark job it is failing with 
NameError: name 'dbutils' is not defined
Should i import any of the package to use dbutils in pyspark code ? Thanks in advance.
",<apache-spark-sql><azure-blob-storage><databricks>,610,0,3,1079,5,12,18,37,25162,0,73,3,23,2018-06-12 9:16,2018-10-23 11:17,,133,,Basic,12
59850951,When to use float vs decimal,"I'm building this API, and the database will store values that represent one of the following:
percentage
average
rate
I honestly have no idea how to represent something that the range is between 0 and 100% in numbers. Should it be
0.00 - 1.00
0.00 - 100.00
any other alternative that I don't know
Is there a clear choice for that? A global way of representing on databases something that goes from 0 to 100% percent? Going further, what's the correct that type for it, float or decimal?
Thank you.
",<mysql><sql><types><floating-point><decimal>,499,0,0,1198,7,17,30,66,8111,0,33,7,23,2020-01-22 0:08,2020-01-22 0:20,,0,,Basic,8
49035178,Unable to locate System.Data.SqlClient reference,"I have a fresh Visual Studio 2017 Professional install. I'm building a quick POC Console application using .NET 4.7.1, and I'm unable to find the reference for System.Data.SqlClient.
I have scoured my system, and located 4 versions of System.Data.SqlClient.dll, but none are correct and won't compile.
I have also attempted to use System.Data, but no reference to SqlClient is located within. I have manually added the dll/reference for System.Data, but also did not resolve the reference issue.
My application is really simple at the moment, and it will NOT compile due to this missing reference.
What steps do I need to do to get this resolved?
using System;
using System.Data;
using System.Data.SqlClient;
namespace ConsoleApp1
{
    class Database
    {
        public void Start()
        {
            string connString = @""server=(local);initial     catalog=MyDatabase;Integrated Security=SSPI;"";
            using (SqlConnection conn = new SqlConnection(connString))
            {
                conn.Open();
                using (SqlCommand cmd = new SqlCommand(""SELECT TOP 10 ID, Name FROM TableA"", conn))
                {
                    using (SqlDataReader reader = cmd.ExecuteReader())
                    {
                        while(reader.Read())
                        {
                            Console.WriteLine(""ID: [{0}], Name: [{1}]"", reader.GetValue(0), reader.GetValue(1));
                        }
                    }
                }
            }
        }
    }
}
",<c#><sqlclient>,1511,0,29,267,1,2,9,37,70492,,3,7,23,2018-02-28 17:17,2018-08-15 14:53,2019-03-06 20:52,168,371,Basic,9
53249276,docker-compose mysql init sql is not executed,"I am trying to set up a mysql docker container and execute init sql script. Unfortunately the sql script is not executed. What am I doing wrong?
version: '3.3'
services:
  api:
    container_name: 'api'
    build: './api'
  ports:
    - target: 8080
      published: 8888
      protocol: tcp
      mode: host
  volumes:
    - './api:/go/src/app'
  depends_on:
    - 'mysql'
 mysql:
  image: 'mysql:latest'
  container_name: 'mysql'
  volumes:
    - ./db_data:/var/lib/mysql:rw
    - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  restart: always
  environment:
    MYSQL_USER: test
    MYSQL_PASSWORD: test
    MYSQL_ROOT_PASSWORD: test
    MYSQL_DATABASE: test
  ports:
    - '3306:3306'
volumes:
  db_data:
I execute file with docker-compose up -d --build
",<mysql><sql><database><docker><docker-compose>,773,0,31,479,1,4,12,44,21615,0,16,3,23,2018-11-11 13:35,2018-11-11 14:56,2018-11-12 2:32,0,1,Basic,9
48967318,SQL Server Web vs Standard edition,"I have found out that there's two versions of SQL Server types that are very different in terms of pricing...
The Web version from my host provider costs about 13$ per 2 core packs, whereas the Standard edition is right around 200$.
From my standpoint, we expect our database to be around 150-200GB in size, only few tables would take up most of that space.
So my only concern is would the web version of SQL Server support this large database and not cause any performance issues to the end users?
How different is index rebuilding on Web and Standard version?
Can someone help me out with this?
",<sql><sql-server><performance><sql-server-2008><database-indexes>,597,0,0,3693,15,55,120,81,95448,0,406,1,23,2018-02-24 20:32,2018-02-24 21:28,2018-02-24 21:28,0,0,Basic,9
52148305,How to cascade delete document in mongodb?,"I have user and photo documents in Mongodb. Each photo belongs to user and a photo maybe shared among users. Lets say user1 has p1,p2,p3 photos and user2 has p3,p4,p5 photos. If I delete user1 (manually using tools like Compass), p1 and p2 should also be deleted but not p3. How to achieve this and what kind of database structure I need to define?
Currently if I delete user1, no photos are deleted and remain in databse which now makes the database corrupted from the point of view of the application using the database.
Its Spring Boot app and User and Photo are declared as:
import lombok.Builder;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.DBRef;
import org.springframework.data.mongodb.core.mapping.Document;
@Document
@Data
@Builder
public class User {
    @Id
    private String id;
    @DBRef
    private Set&lt;Photo&gt; photos;
    private String name;
}
@Document
@Data
@Builder
public class Photo {
    @Id
    private String id;
    private String fileName;
}
",<mongodb><spring-boot><nosql>,1051,0,33,11616,40,116,195,69,20278,0,416,4,23,2018-09-03 10:54,2018-09-11 0:04,2018-09-11 0:04,8,8,Basic,9
52048778,"OperationalError: cursor ""_django_curs_<id>"" does not exist","We have an online store web-app which is powered by django, postgresql and heroku. 
For a specific campaign (you can think a campaign like a product to purchase), we have sold 10k+ copies successfully. Yet some of our users are encountered this error according to our Sentry reports. Common specification of these users is; none of them have address information before the purchase. Generally, users fill out address form right after registering. If they don't, they need to fill the form while purchasing the product and submit them together. 
This is how the trace looks like:
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist
(66 additional frame(s) were not displayed)
...
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 31, in render_form
    return render_to_string('widgets/store_form_renderer.html', ctx)
  File ""store/apps/store_main/templatetags/store_form_filters.py"", line 20, in render_widget
    return render_to_string('widgets/store_widget_renderer.html', ctx)
  File ""store/apps/store_main/widgets.py"", line 40, in render
    attrs=attrs) + ""&lt;span class='js-select-support select-arrow'&gt;&lt;/span&gt;&lt;div class='js-select-support select-arrow-space'&gt;&lt;b&gt;&lt;/b&gt;&lt;/div&gt;""
OperationalError: cursor ""_django_curs_140398688327424_146"" does not exist 
So another weird common thing, there are exception messages between sql queries before the failure. You can see it in the image below: 
I'm adding it if they are somehow related. What may also be related is, the users who get this error are the users who tries to purchase the campaign right after a bulk mailing. So, extensive traffic might be the reason yet we are also not sure.
We asked Heroku about the problem since they are hosting the postgres, yet they do not have any clue either.
I know the formal reason of this error is trying to reach the cursor after a commit. Since it is destroyed after transaction, trying to reach it cause this error yet I don't see this in our scenario. We are not touching the cursor in any way. What am I missing? What may produce this error? How to prevent it? Any ideas would be appreciated.
",<python><django><postgresql><sentry><heroku-postgres>,2170,1,12,408,1,3,16,74,19009,0,69,4,23,2018-08-28 1:13,2019-04-29 11:11,,244,,Advanced,32
53117988,sequelize select and include another table alias,"I'm using sequelize to acess a postgres database and I want to query for a city and for example include the ""Building"" table but I want to rename the output to ""buildings"" and return the http response  but I have this error:
  { SequelizeEagerLoadingError: building is associated to city using an alias. You'v
  e included an alias (buildings), but it does not match the alias defined in your a
  ssociation.
    City.findById(req.params.id,{
      include: [
        {
          model: Building, as: ""buildings""
        }
      ]
    }).then(city =&gt;{
      console.log(city.id);
         res.status(201).send(city);
    }) .catch(error =&gt; {
     console.log(error);
     res.status(400).send(error)
   });
city Model
            const models = require('../models2');
            module.exports = (sequelize, DataTypes) =&gt; {
              const City = sequelize.define('city', {
              name: { type: DataTypes.STRING, allowNull: false },
                status: { type: DataTypes.INTEGER, allowNull: false },
                latitude: { type: DataTypes.BIGINT, allowNull: false },
                longitude: { type: DataTypes.BIGINT, allowNull: false },
              }, { freezeTableName: true});
              City.associate = function(models) {
                // associations can be defined here
                 City.hasMany(models.building,{as: 'building', foreignKey: 'cityId'})
              };
              return City;
            };
",<node.js><postgresql><sequelize.js><sequelize-cli><sequelize-typescript>,1461,0,28,1835,4,28,54,60,58794,0,84,1,23,2018-11-02 11:44,2018-11-02 13:00,2018-11-02 13:00,0,0,Advanced,37
52081473,Aggregate Overlapping Segments to Measure Effective Length,"I have a road_events table:
create table road_events (
    event_id number(4,0),
    road_id number(4,0),
    year number(4,0),
    from_meas number(10,2),
    to_meas number(10,2),
    total_road_length number(10,2)
    );
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (1,1,2020,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (2,1,2000,25,50,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (3,1,1980,0,25,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (4,1,1960,75,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (5,1,1940,1,100,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (6,2,2000,10,30,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (7,2,1975,30,60,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (8,2,1950,50,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (9,3,2050,40,90,100);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (10,4,2040,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (11,4,2013,0,199,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (12,4,2001,0,200,200);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (13,5,1985,50,70,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (14,5,1985,10,50,300);
insert into road_events (event_id, road_id, year, from_meas, to_meas, total_road_length) values (15,5,1965,1,301,300);
commit;
select * from road_events;
  EVENT_ID    ROAD_ID       YEAR  FROM_MEAS    TO_MEAS TOTAL_ROAD_LENGTH
---------- ---------- ---------- ---------- ---------- -----------------
         1          1       2020         25         50               100
         2          1       2000         25         50               100
         3          1       1980          0         25               100
         4          1       1960         75        100               100
         5          1       1940          1        100               100
         6          2       2000         10         30               100
         7          2       1975         30         60               100
         8          2       1950         50         90               100
         9          3       2050         40         90               100
        10          4       2040          0        200               200
        11          4       2013          0        199               200
        12          4       2001          0        200               200
        13          5       1985         50         70               300
        14          5       1985         10         50               300
        15          5       1965          1        301               300
I want to select the events that represent the most recent work on each road.
This is a tricky operation, because the events often pertain to only a portion of the road. This means that I can't simply select the most recent event per road; I need to only select the most recent event mileage that doesn't overlap.
Possible logic (in order):
I'm reluctant to guess at how this problem could be solved, because it could end up hurting more than it helps (kind of like the XY Problem). On the other hand, it might provide insight into the nature of the problem, so here it goes:
Select the most recent event for each road. We'll call the most recent event: event A.
If event A  is &gt;= total_road_length, then that's all I need. The algorithm ends here.
Else, get the next chronological event (event B) that does not have the same extents as event A. 
If the extents of event B overlap the extents of event A, then only get the portion(s) of event B that do not overlap. 
Repeat steps 3 and 4 until the total event length is = total_road_length. Or stop when there are no more events for that road.
Question:
I know it's a tall order, but what would it take to do this?
This is a classic linear referencing problem. It would be extremely helpful if I could do linear referencing operations as part of queries.
The result would be:
  EVENT_ID    ROAD_ID       YEAR  TOTAL_ROAD_LENGTH   EVENT_LENGTH
---------- ---------- ----------  -----------------   ------------
         1          1       2020                100             25
         3          1       1980                100             25
         4          1       1960                100             25
         5          1       1940                100             25
         6          2       2000                100             20
         7          2       1975                100             30
         8          2       1950                100             30
         9          3       2050                100             50
        10          4       2040                200            200
        13          5       1985                300             20
        14          5       1985                300             40
        15          5       1965                300            240
Related question: Select where number range does not overlap 
",<sql><oracle><select><oracle12c><asset-management>,5605,2,80,296,1,18,66,48,764,0,683,6,22,2018-08-29 15:46,2018-08-29 23:44,2018-08-30 21:05,0,1,Intermediate,17
52517529,"How to create schema in Postgres DB, before liquibase start to work?","I have standalone application. It’s on java, spring-boot, postgres and it has liquibase. 
I need to deploy my app and liquibase should create all tables, etc. But it should do it into custom schema not in public. All service tables of liquibase (databasechangelog and databasechangeloglock) should be in custom schema too. How can I create my schema in DB before liquibase start to work? I must do it inside my app when it’s deploying, in config or some like. Without any manual intervention into the DB.
application.properties:
spring.datasource.jndi-name=java:/PostgresDS
spring.jpa.properties.hibernate.default_schema=my_schema
spring.jpa.show-sql = false
spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
spring.datasource.continue-on-error=true
spring.datasource.sql-script-encoding=UTF-8
liquibase.change-log = classpath:liquibase/changelog-master.yaml
liquibase.default-schema = my_schema
UPD:
When liquibase start, it's create two tables databasechangelogs and one more table. After that, liquibase start working. But I want liquibase in liquibase.default-schema = my_schema, but it's not exist when liquibase start to work and it an error: exception is liquibase.exception.LockException: liquibase.exception.DatabaseException: ERROR: schema ""my_schema"" does not exist
I want liquibase work in custom schema, not in public:
liquibase.default-schema = my_schema
but before liquibase can do it, the schema must be created. Liquibase can't do this because it not started yet and for start it needs schema.
Vicious circle.
",<java><spring><postgresql><liquibase>,1559,0,11,480,1,3,16,71,20783,0,99,4,22,2018-09-26 12:13,2018-09-27 14:21,2018-09-27 14:21,1,1,Basic,4
49390115,How to find my permissions in a SQL Server database?,"I'm a user of a SQL Sever database, and I want to know my access rights / permissions in the database I'm using. What SQL query should I use to do so?
Thanks
",<sql><sql-server><database><user-permissions><access-rights>,158,0,0,632,1,9,19,61,45192,,63,2,22,2018-03-20 17:00,2018-03-20 17:16,2018-03-20 17:16,0,0,Basic,9
60301008,Failed to validate connection (This connection has been closed.). Possibly consider using a shorter maxLifetime value,"I see the following error message:
HikariPool-1 - Failed to validate connection
org.postgresql.jdbc.PgConnection@f162126 (This connection has been
closed.). Possibly consider using a shorter maxLifetime value.
frequently refreshing the same page gives the above warning after
exceeding maxLifetime
This was my original database configuration:
spring.datasource.hikari.auto-commit=false  
spring.datasource.hikari.idleTimeout=180000  
spring.datasource.hikari.minimumIdle=5  
spring.datasource.hikari.leakDetectionThreshold=240000    
spring.datasource.hikari.maximumPoolSize=10  
logging.level.com.zaxxer.hikari=TRACE  
spring.datasource.hikari.connectionTimeout=30000  
spring.datasource.hikari.maxLifetime=300000  
logging.level.com.zaxxer.hikari.HikariConfig=DEBUG  
The application is working fine if I change the following properties:
spring.datasource.hikari.maximumPoolSize=100
spring.datasource.hikari.maxLifetime=60000
Can any one explain what is happening exactly?
",<postgresql><spring-boot><database-connection><connection-pooling>,975,0,13,329,1,2,3,63,33427,0,0,2,22,2020-02-19 13:02,2021-01-15 10:26,,331,,Intermediate,23
52473260,Count distinct multiple columns in redshift,"I am trying to count rows which have a distinct combination of 2 columns in Amazon redshift. The query I am using is - 
select count(distinct col1, col2)
from schemaname.tablename
where some filters
It is throwing me this error - 
  Amazon Invalid operation: function count(character varying, bigint) does not exist`
I tried casting bigint to char but it didn't work.
",<sql><amazon-redshift>,368,0,5,507,2,4,12,70,26861,0,113,5,22,2018-09-24 5:41,2018-09-24 5:45,2018-09-24 5:45,0,0,Basic,2
57914342,visual studio 2019 open solution file incompatible,"I think I was using visual studio 2017 and wrote a SSIS package. Now I installed visual studio 2019 and can't open the solution file. Error:
  Unsupported This version of Visual Studio is unable to open the
  following projects. The project types may not be installed or this
  version of Visual Studio may not support them.  For more information
  on enabling these project types or otherwise migrating your assets,
  please see the details in the ""Migration Report"" displayed after
  clicking OK.
     - ABC, ""C:\Users\XYZ\ABC.dtproj""
  Non-functional changes required Visual Studio will automatically make
  non-functional changes to the following projects in order to enable
  them to open in Visual Studio 2015, Visual Studio 2013, Visual Studio
  2012, and Visual Studio 2010 SP1. Project behavior will not be
  impacted.
     - ABC_SSIS, ""C:\Users\XYZ\ABC_SSIS.sln""
I tried ""Right-click on the project and reload"" - didn't work.
I tried to confirm SSDT is installed:
it is installed at the installation interface, but doesn't exist in extension manager:
",<visual-studio><ssis><sql-server-data-tools>,1061,2,0,3984,11,43,67,59,55613,0,135,5,22,2019-09-12 20:52,2019-09-17 16:25,2019-09-17 16:25,5,5,Basic,6
50070877,Postgres Psycopg2 Create Table,"I am new to Postgres and Python. I try to create a simple user table but I don't know why it isn't created.
The error message doesn't appear,
    #!/usr/bin/python
    import psycopg2
    try:
        conn = psycopg2.connect(database = &quot;projetofinal&quot;, user = &quot;postgres&quot;, password = &quot;admin&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;)
    except:
        print(&quot;I am unable to connect to the database&quot;) 
    cur = conn.cursor()
    try:
        cur.execute(&quot;CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);&quot;)
    except:
        print(&quot;I can't drop our test database!&quot;)
    conn.close()
    cur.close()
Any help or hint would be appreciated.
Thank you.
",<python><database><postgresql><python-3.6><postgresql-10>,745,0,16,353,1,2,11,74,31830,0,3,1,22,2018-04-27 22:06,2018-04-27 22:10,2018-04-27 22:10,0,0,Basic,6
56462616,How to use pg_restore with AWS RDS correctly to restore postgresql database,"I am trying to restore my Postgresql database to AWS RDS. I think I am almost there. I can get a dump, and recreate the db locally, but I am missing the last step to restore it to AWS RDS. 
Here is what I am doing: 
I get my dump
$ pg_dump -h my_public dns -U myusername -f dump.sql myawsdb
I create a local db in my shell called test: 
create database test;
I put the dump into my test db
$ psql -U myusername -d test -f dump.sql
so far so good. 
I get an error: psql:dump.sql:2705: ERROR:  role ""rdsadmin"" does not exist, but I think I can ignore it, because my db is there with all the content. (I checked with \list and \connect test).
Now I want to restore this dump/test to my AWS RDS. 
Following this https://gist.github.com/syafiqfaiz/5273cd41df6f08fdedeb96e12af70e3b 
I now should do: 
pg_restore -h &lt;host&gt; -U &lt;username&gt; -c -d &lt;database name&gt; &lt;filename to be restored&gt;
But what is my filename and what is my database name?
I tried: 
pg_restore -h mydns -U myusername -c -d myawsdbname test
pg_restore -h mydns -U myusername -c -d myawsdbname dump.sql
and a couple of more options that I don't recall. 
Most of the times it tells me something like: pg_restore: [archiver] could not open input file ""test.dump"": No such file or directory
Or, for the second: input file appears to be a text format dump. Please use psql.
Can somone point me into the right direction? Help is very much appreciated!
EDIT: So I created a .dump file using $ pg_dump -Fc mydb &gt; db.dump
Using this file I think it works. Now I get the error [archiver (db)] could not execute query: ERROR:  role ""myuser"" does not exist
    Command was: ALTER TABLE public.users_user_user_permissions_id_seq OWNER TO micromegas;
Can I ingore that?
EDIT2: I got rid of the error adding the flags--no-owner --role=mypguser --no-privileges --no-owner
",<django><postgresql><amazon-rds><restore><dump>,1841,2,12,1549,2,20,49,53,22116,0,141,2,22,2019-06-05 14:31,2020-11-20 17:25,2020-11-20 17:25,534,534,Intermediate,31
56538035,Finding sum and grouping in sequelize,"I have a donations table as follows.
Donations Table
| id| amount | member_id |
|---|--------|-----------|
| 0 |   500  |         01|
| 1 |  1000  |         02|
| 2 |  2000  |         01|
How to find sum and group the table by member id as follows.
| amount | member_id |
|--------|-----------|
|  2500  |         01|
|  1000  |         02|
I tried to use the following code but it doesnt seem to work.
const salesValue = await DONATIONS.sum('amount', {
    group: 'member_id'
});
",<mysql><node.js><sequelize.js>,481,0,12,3658,10,31,55,47,55042,0,97,2,22,2019-06-11 6:49,2019-06-11 9:24,,0,,Basic,2
56013334,Spark dynamic frame show method yields nothing,"So I am using AWS Glue auto-generated code to read csv file from S3 and write it to a table over a JDBC connection. Seems simple, Job runs successfully with no error but it writes nothing. When I checked the Glue Spark Dynamic Frame it does contents all the rows (using .count()). But when do a .show() on it yields nothing.
.printSchema() works fine. Tried logging the error while using .show(), but no errors or nothing is printed. Converted the DynamicFrame to the data frame using .toDF and the show method it works. 
I thought there is some problem with the file, trying to narrow to certain columns. But even with just 2 columns in the file same thing. Clearly marked string in double quotes, still no success.
We have things like JDBC connection that needs to be picked from Glue configuration. Which I guess regular spark data frame can't do. Hence need dynamic frame working.
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
datasource0 = glueContext.create_dynamic_frame.from_options('s3', {'paths': ['s3://bucket/file.csv']}, 'csv', format_options={'withHeader': True,'skipFirst': True,'quoteChar':'""','escaper':'\\'})
datasource0.printSchema()
datasource0.show(5)
Output
root
|-- ORDERID: string
|-- EVENTTIMEUTC: string
Here is what the converting to regular data frame yields.
datasource0.toDF().show()
Output
+-------+-----------------+
|ORDERID|     EVENTTIMEUTC|
+-------+-----------------+
|      2| ""1/13/2018 7:50""|
|      3| ""1/13/2018 7:50""|
|      4| ""1/13/2018 7:50""|
|      5| ""1/13/2018 7:50""|
|      6| ""1/13/2018 8:52""|
|      7| ""1/13/2018 8:52""|
|      8| ""1/13/2018 8:53""|
|      9| ""1/13/2018 8:53""|
|     10| ""1/16/2018 1:33""|
|     11| ""1/16/2018 2:28""|
|     12| ""1/16/2018 2:37""|
|     13| ""1/17/2018 1:17""|
|     14| ""1/17/2018 2:23""|
|     15| ""1/17/2018 4:33""|
|     16| ""1/17/2018 6:28""|
|     17| ""1/17/2018 6:28""|
|     18| ""1/17/2018 6:36""|
|     19| ""1/17/2018 6:38""|
|     20| ""1/17/2018 7:26""|
|     21| ""1/17/2018 7:28""|
+-------+-----------------+
only showing top 20 rows
Here is the some data.
ORDERID, EVENTTIMEUTC
1, ""1/13/2018 7:10""
2, ""1/13/2018 7:50""
3, ""1/13/2018 7:50""
4, ""1/13/2018 7:50""
5, ""1/13/2018 7:50""
6, ""1/13/2018 8:52""
7, ""1/13/2018 8:52""
8, ""1/13/2018 8:53""
9, ""1/13/2018 8:53""
10, ""1/16/2018 1:33""
11, ""1/16/2018 2:28""
12, ""1/16/2018 2:37""
13, ""1/17/2018 1:17""
14, ""1/17/2018 2:23""
15, ""1/17/2018 4:33""
16, ""1/17/2018 6:28""
17, ""1/17/2018 6:28""
18, ""1/17/2018 6:36""
19, ""1/17/2018 6:38""
20, ""1/17/2018 7:26""
21, ""1/17/2018 7:28""
22, ""1/17/2018 7:29""
23, ""1/17/2018 7:46""
24, ""1/17/2018 7:51""
25, ""1/18/2018 2:22""
26, ""1/18/2018 5:48""
27, ""1/18/2018 5:50""
28, ""1/18/2018 5:50""
29, ""1/18/2018 5:51""
30, ""1/18/2018 5:53""
100, ""1/18/2018 10:32""
101, ""1/18/2018 10:33""
102, ""1/18/2018 10:33""
103, ""1/18/2018 10:42""
104, ""1/18/2018 10:59""
105, ""1/18/2018 11:16""
",<python><pyspark><apache-spark-sql><aws-glue>,3183,0,84,657,4,11,21,77,23237,,33,1,22,2019-05-06 22:51,2023-05-19 15:42,,1474,,Intermediate,25
48879601,How do I query the length of a Django ArrayField?,"I have an ArrayField in a model, I'm trying to annotate the length of this field ( so far without any luck) 
F('field_name__len') won't work since join is not allowed inside F().  Even 
ModelName.objets.values('field_name__len') is not working
Any idea?
I'm using django 1.11
",<django><postgresql><django-models><django-queryset>,276,0,3,1424,1,13,29,51,8829,0,272,3,22,2018-02-20 7:06,2018-02-20 7:16,2018-07-31 11:50,0,161,Basic,10
57646341,Check if table exists in hive metastore using Pyspark,"I am trying to check if a table exists in hive metastore if not, create the table. And if the table exists, append data.
I have a snippet of the code below:
spark.catalog.setCurrentDatabase(&quot;db_name&quot;)
db_catalog = spark.catalog.listTables(dbName = 'table_name)
if any(table_name in row for row in db_catalog):
    add data
else:
    create table
However, I am getting an error.
&gt;&gt;&gt; ValueError: Some of types cannot be determined after inferring
I am unable to resolve the value error as I get the same errors for other databases' tables created in hive metastore. Is there another way to check if table exists in hive metastore?
",<python-3.x><apache-spark><hive><pyspark><apache-spark-sql>,648,0,7,3097,10,55,81,69,52330,0,30,9,22,2019-08-25 13:15,2019-08-26 4:12,,1,,Basic,10
49379538,How take mysqldump with UTF8?,"I am trying to take mysql dump with command:
mysqldump -u xxxx -p dbxxx &gt; xxxx270613.sql
what is command to take mysqldump with UTF8 ?
",<mysql>,138,0,1,3483,2,20,30,65,49695,0,166,3,22,2018-03-20 8:36,2018-03-20 8:46,2018-03-20 8:46,0,0,Basic,10
56807876,ODCIAggregateMerge without parallel_enabled,"These are quotes from Oracle docs:
  [Optional] Merge by combining the two aggregation contexts and return a single context. This operation combines the results of aggregation over subsets in order to obtain the aggregate over the entire set. This extra step can be required during either serial or parallel evaluation of an aggregate. If needed, it is performed before step 4:  
,
  The ODCIAggregateMerge() interface is invoked to compute super aggregate values in such rollup operations.
We have an aggregate function, that we do NOT want to ever run in parallel.
The reason is that the merging of contexts would be resource consuming and would force us to use different data structures than we are using now, effectively offseting any performance benefits from parallel execution.
Thus, we did not declare our function as parallel_enabled, and instead return ODCIconst.Error in ODCIAggregateMerge 'just in case'.
However, the first quote docs claim, that merge may occur even in serial evaluation.
Super-aggregates (rollup, cube) are obvious examples, but are there any others?
I've been totally unable to reproduce it with simple group by, merge is never called without parallel_enabled and it seems that always only one context is created within the group.  
Is it safe to assume that without the parallel_enabled set, merge will never be run?
Have you ever seen a counterexample to that rule?
",<sql><oracle><plsql><aggregate>,1400,1,0,925,0,6,24,44,660,0,172,1,22,2019-06-28 13:30,2020-07-06 5:56,,374,,Intermediate,18
55950386,"BigQuery - No matching signature for operator = for argument types: INT64, STRING","Im getting a weird error(Maybe im getting this error for the first time) from BQ.
No matching signature for operator = for argument types: INT64, STRING. 
Supported signatures: ANY = ANY at [27:1]
Query:
SELECT col1
    ,col2
    ,col3
FROM tbl1
JOIN t2 ON t1.id = t2.id
JOIN t3 on t2.id = t3.id
JOIN t4 on t4.id = t1.id
Error line JOIN t2.id = t3.id  t2.id is showing this error.
its an integer column.
",<sql><google-cloud-platform><google-bigquery>,404,0,10,2832,6,40,91,38,128667,0,57,3,22,2019-05-02 10:25,2019-05-02 10:30,,0,,Basic,2
51075096,Flask-Admin create view with SQLAlchemy context-sensitive functions,"I have a data model which has a column that depends on other column values, following the instructions in this page I've created a context-sensitive function which is used to determine the value of this particular column on creation, something like this:
def get_column_value_from_context(context):
    # Instructions to produce value
    return value
class MyModel(db.Model):
    id = db.Column(db.Integer,
                   primary_key=True)
    my_column = db.Column(db.String(64),
                          nullable=False,
                          default=get_column_value_from_context)
    name = db.Column(db.String(32),
                     nullable=False,
                     unique=True,
                     index=True)
    title = db.Column(db.String(128),
                      nullable=False)
    description = db.Column(db.String(256),
                            nullable=False)
This approach works pretty decent, I can create rows without problems from the command line or using a script.
I've also added a ModelView to the app using Flask-Admin:
class MyModelView(ModelView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
admin.add_view(MyModelView(MyModel, db.session))
This also works pretty decent until I click the Create button in the list view. I receive this error:
  AttributeError: 'NoneType' object has no attribute 'get_current_parameters'
Because the implementation of the create_model handler in the ModelView is this:
def create_model(self, form):
    """"""
        Create model from form.
        :param form:
            Form instance
    """"""
    try:
        model = self.model()
        form.populate_obj(model)
        self.session.add(model)
        self._on_model_change(form, model, True)
        self.session.commit()
    except Exception as ex:
        if not self.handle_view_exception(ex):
            flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
            log.exception('Failed to create record.')
        self.session.rollback()
        return False
    else:
        self.after_model_change(form, model, True)
    return model
and here there isn't a context when the model is instantiated. So, I've created a custom view where the model instantiation in the creation handler could be redefined:
class CustomSQLAView(ModelView):
    def __init__(self, *args, **kwargs):
        super(CustomSQLAView, self).__init__(*args, **kwargs)
    def create_model(self, form):
        """"""
            Create model from form.
            :param form:
                Form instance
        """"""
        try:
            model = self.get_populated_model(form)
            self.session.add(model)
            self._on_model_change(form, model, True)
            self.session.commit()
        except Exception as ex:
            if not self.handle_view_exception(ex):
                flash(gettext('Failed to create record. %(error)s', error=str(ex)), 'error')
                log.exception('Failed to create record.')
            self.session.rollback()
            return False
        else:
            self.after_model_change(form, model, True)
        return model
    def get_populated_model(self, form):
        model = self.model()
        form.populate_obj(model)
        return model
Now I can redefine the get_populated_model method to instantiate the model in the usual way:
class MyModelView(CustomSQLAView):
    can_view_details = True
    can_set_page_size = True
    can_export = True
    def get_populated_model(self, form):
        model = self.model(
            name=form.name.data,
            title=form.title.data,
            description=form.description.data,
        )
        return model
Despite that this works, I suspect it breaks something. Flask-Admin has several implementation of the populate_obj method of forms and fields, so I would like to keep everything safe.
What is the proper way to do this?
",<python><flask><flask-sqlalchemy><flask-admin>,3931,2,104,626,2,8,20,37,2001,0,226,1,22,2018-06-28 5:11,2018-09-21 21:54,,85,,Basic,6
56373970,Insert multiple records in Sqflite,"How to insert quickly multiple records in sqflite? The standard quickly method is:
await database.insert(table, object.toMap())
But I don't think that insert record one to one with a cycle is a good idea.
Or I can insert all list with a transaction?
",<sqlite><flutter><sqflite>,250,0,2,10529,3,42,48,41,22138,0,480,7,21,2019-05-30 7:51,2019-05-30 8:33,,0,,Basic,9
58352334,Spring Data / Hibernate save entity with Postgres using Insert on Conflict Update Some fields,"I have a domain object in Spring which I am saving using JpaRepository.save method and using Sequence generator from Postgres to generate id automatically.
@SequenceGenerator(initialValue = 1, name = ""device_metric_gen"", sequenceName = ""device_metric_seq"")
public class DeviceMetric extends BaseTimeModel {
    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = ""device_metric_gen"")
    @Column(nullable = false, updatable = false)
    private Long id;
///// extra fields
My use-case requires to do an upsert instead of normal save operation (which I am aware will update if the id is present). I want to update an existing row if a combination of three columns (assume a composite unique) is present or else create a new row.
This is something similar to this:
INSERT INTO customers (name, email)
VALUES
   (
      'Microsoft',
      'hotline@microsoft.com'
   ) 
ON CONFLICT (name) 
DO
      UPDATE
     SET email = EXCLUDED.email || ';' || customers.email;
One way of achieving the same in Spring-data that I can think of is:
Write a custom save operation in the service layer that
Does a get for the three-column and if a row is present
Set the same id in current object and do a repository.save
If no row present, do a normal repository.save
Problem with the above approach is that every insert now does a select and then save which makes two database calls whereas the same can be achieved by postgres insert on conflict feature with just one db call.
Any pointers on how to implement this in Spring Data?
One way is to write a native query insert into values (all fields here). The object in question has around 25 fields so I am looking for an another better way to achieve the same.
",<spring><postgresql><spring-boot><spring-data-jpa>,1713,1,25,1395,3,17,34,65,10096,0,200,2,21,2019-10-12 8:34,2022-12-26 14:50,,1171,,Intermediate,18
50664648,Why even use *DB.exec() or prepared statements in Golang?,"I'm using golang with Postgresql.
It says here that for operations that do not return rows (insert, delete, update) we should use exec()
  If a function name includes Query, it is designed to ask a question of the database, and will return a set of rows, even if it’s empty. Statements that don’t return rows should not use Query functions; they should use Exec().
Then it says here:
  Go creates prepared statements for you under the covers. A simple db.Query(sql, param1, param2), for example, works by preparing the sql, then executing it with the parameters and finally closing the statement.
If query() uses under the covers prepared statements why should I even bother using prepared statements?
",<sql><database><postgresql><go><prepared-statement>,702,2,2,23944,33,137,186,56,32402,0,1968,1,21,2018-06-03 8:38,2018-06-03 11:52,2018-06-03 11:52,0,0,Intermediate,18
54159964,How to remove nulls with array_remove Spark SQL built-in function,"Spark 2.4 introduced new useful Spark SQL functions involving arrays, but I was a little bit puzzled when I found out that the result of
select array_remove(array(1, 2, 3, null, 3), null) is null and not [1, 2, 3, 3].
Is this the expected behavior? Is it possible to remove nulls using array_remove?
As a side note, for now the alternative I am using is a higher order function in Databricks:
select filter(array(1, 2, 3, null, 3), x -&gt; x is not null)
",<arrays><dataframe><apache-spark><apache-spark-sql><null>,455,0,5,2291,1,17,33,78,19343,0,1097,6,21,2019-01-12 13:17,2019-01-14 6:20,2019-01-14 6:20,2,2,Intermediate,15
56411055,Function uuid_generate_v4() does not exist postgres 11,"I am trying to use node-pg-migrate and run migrations to create tables in my node project.
When I run migrations I get function uuid_generate_v4() does not exist.
I did check in my extensions and uuid-ossp is available.
extname  | extowner | extnamespace | extrelocatable | extversion | extconfig | extcondition 
-----------+----------+--------------+----------------+------------+-----------+--------------
 plpgsql   |       10 |           11 | f              | 1.0        |           | 
 uuid-ossp |    16384 |         2200 | t              | 1.1        |           | 
(2 rows)
I expect my migrations to run but it fails. I am using Postgres 11 on Mac.
Postgres installed from here - https://postgresapp.com/
",<postgresql><postgresql-11>,712,2,6,329,1,2,11,37,31257,0,5,3,21,2019-06-01 23:07,2019-06-01 23:33,,0,,Basic,14
63301495,PDOException: Packets out of order. Expected 0 received 1. Packet size=23,"I have a Laravel Spark project that uses Horizon to manage a job queue with Redis.
Locally, (on my Homestead box, Mac OS) everything works as expected, but on our new Digital Ocean (Forge provisioned) Droplet, which is a memory-optimized 256GB, 32vCPUs, 10TB, and 1x 800GB VPS, I keep getting the error:
PDOException: Packets out of order. Expected 0 received 1. Packet size=23
Or some variation of that error, where the packet size info may be different.
After many hours/days of debugging and research, I have come across many posts on StackOverflow and elsewhere, that seem to indicate that this can be fixed by doing a number of things, listed below:
Set PDO::ATTR_EMULATE_PREPARES to true in my database.php config. This has absolutely no effect on the problem, and actually introduces another issue, whereby integers are cast as strings.
Set DB_HOST to 127.0.0.1 instead of localhost, so that it uses TCP instead of a UNIX socket. Again, this has no effect.
Set DB_SOCKET to the socket path listed in MySQL by logging into MySQL (MariaDB) and running show variables like '%socket%'; which lists the socket path as /run/mysqld/mysqld.sock. I also leave DB_HOST set to localhost. This has no effect either. One thing I did note, was that the pdo_mysql.default_socket variable is set to /var/run/mysqld/mysqld.sock, I'm not sure if this is part of the problem?
I have massively increased the MySQL configuration settings found in /etc/mysql/mariadb.conf.d/50-server.cnf to the following:
key_buffer_size = 2048M
max_allowed_packet = 2048M
max_connections = 1000
thread_concurrency = 100
query_cache_size = 256M
I must admit, that changing these settings was a last resort/clutching at straws type scenario. However, this did alleviate the issue to some degree, but it did not fix it completely, as MySQL still fails 99% of the time, albeit at a later stage.
In terms of the queue, I have a total of 1,136 workers split between 6 supervisors/queues and it's all handled via Laravel Horizon, which is being run as a Daemon.
I am also using the Laravel Websockets PHP package for broadcasting, again, which is also being run as a Daemon.
My current environment configuration is as follows (sensitive info omitted).
APP_NAME=&quot;App Name&quot;
APP_ENV=production
APP_DEBUG=false
APP_KEY=thekey
APP_URL=https://appurl.com
LOG_CHANNEL=single
DB_CONNECTION=mysql
DB_HOST=127.0.0.1
DB_PORT=3306
DB_DATABASE=databse
DB_USERNAME=username
DB_PASSWORD=password
BROADCAST_DRIVER=pusher
CACHE_DRIVER=file
QUEUE_CONNECTION=redis
SESSION_DRIVER=file
SESSION_LIFETIME=120
REDIS_HOST=127.0.0.1
REDIS_PASSWORD=null
REDIS_PORT=6379
MAIL_MAILER=smtp
MAIL_HOST=smtp.gmail.com
MAIL_PORT=587
MAIL_USERNAME=name@email.com
MAIL_PASSWORD=password
MAIL_ENCRYPTION=tls
MAIL_FROM_ADDRESS=name@email.com
MAIL_FROM_NAME=&quot;${APP_NAME}&quot;
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=&quot;us-east-1&quot;
AWS_BUCKET=
PUSHER_APP_ID=appid
PUSHER_APP_KEY=appkey
PUSHER_APP_SECRET=appsecret
PUSHER_APP_CLUSTER=mt1
MIX_PUSHER_APP_KEY=&quot;${PUSHER_APP_KEY}&quot;
MIX_PUSHER_APP_CLUSTER=&quot;${PUSHER_APP_CLUSTER}&quot;
AUTHY_SECRET=
CASHIER_CURRENCY=usd
CASHIER_CURRENCY_LOCALE=en
CASHIER_MODEL=App\Models\User
STRIPE_KEY=stripekey
STRIPE_SECRET=stripesecret
# ECHO SERVER
LARAVEL_WEBSOCKETS_PORT=port
The server setup is as follows:
Max File Upload Size: 1024
Max Execution Time: 300
PHP Version: 7.4
MariaDB Version: 10.3.22
I have checked all logs (see below) at the time the MySQL server crashes/goes away, and there is nothing in the MySQL logs at all. No error whatsoever. I also don't see anything in:
/var/log/nginx/error.log
/var/log/nginx/access.log
/var/log/php7.4-fpm.log
I'm currently still digging through and debugging, but right now, I'm stumped. This is the first time I've ever come across this error.
Could this be down to hitting the database (read/write) too fast?
A little information on how the queues work.
I have an initial controller that dispatches a job to the queue.
Once this job completes, it fires an event which then starts the process of running several other listeners/events in sequence, all of which depend on the previous jobs completing before new events are fired and new listeners/jobs take up the work.
In total, there are 30 events that are broadcast.
In total, there are 30 listeners.
In total there are 5 jobs.
These all work sequentially based on the listener/job that was run and the event that it fires.
I have also monitored the laravel.log live and when the crash occurs, nothing is logged at all. Although, I do occasionally get production.ERROR: Failed to connect to Pusher. whether MySQL crashes or not, so I don't think that has any bearing on this problem.
I even noticed that the Laravel API rate limit was being hit, so I made sure to drastically increase that from 60 to 500. Still no joy.
Lastly, it doesn't seem to matter which Event, Job, or Listener is running as the error occurs on random ones. So, not sure it's code-specific, although, it may well be.
Hopefully, I've provided enough background and detailed information to get some help with this, but if I've missed anything, please do let me know and I'll add it to the question. Thanks.
",<php><mysql><laravel><sockets><pdo>,5198,1,80,4076,5,41,67,48,54687,0,46,6,21,2020-08-07 12:05,2020-11-27 16:33,,112,,Advanced,32
64061101,"Microsoft SQL Server - best way to 'Update if exists, or Insert'","I've been searching around for the answers to this question, and there's some conflicting or ambiguous information out there, finding it hard to find a for-sure answer.
My context: I'm in node.js using the 'mssql' npm package. My SQL server is Microsoft SQL Server 2014.
I have a record that may or may not exist in a table already -- if it exists I want to update it, otherwise I want to insert it. I'm not sure what the optimal SQL is, or if there's some kind of 'transaction' I should be running in mssql. I've found some options that seem good, but I'm not sure about any of them:
Option 1:
how to update if exists or insert
Problem with this is I'm not even sure this is valid syntax in MSSQL. I do like it though, and it seems to support doing multiple rows at once too which I like.
INSERT INTO table (id, user, date, points)
    VALUES (1, 1, '2017-03-03', 25),
           (2, 1, '2017-03-04', 25),
           (3, 2, '2017-03-03', 100),
           (4, 2, '2017-03-04', 150)
    ON DUPLICATE KEY UPDATE points = VALUES(points)
Option 2:
don't know if there's any problem with this one, just not sure if it's optimal. Doesn't seem to support multiple simultaneous rows
update test set name='john' where id=3012
IF @@ROWCOUNT=0
   insert into test(name) values('john');
Option 3: Merge, https://dba.stackexchange.com/questions/89696/how-to-insert-or-update-using-single-query
Some people say this is a bit buggy or something? This also apparently supports multiple at once which I like.
MERGE dbo.Test WITH (SERIALIZABLE) AS T
USING (VALUES (3012, 'john')) AS U (id, name)
    ON U.id = T.id
WHEN MATCHED THEN 
    UPDATE SET T.name = U.name
WHEN NOT MATCHED THEN
    INSERT (id, name) 
    VALUES (U.id, U.name);
",<sql><sql-server>,1719,2,17,13274,3,40,77,46,24210,0,996,3,21,2020-09-25 9:05,2020-09-25 9:18,2020-09-25 9:18,0,0,Advanced,32
62821983,"TypeORM: ""No migrations pending"" when attempting to run migrations manually","I have a new web app and I've written a migrator to create a user table. However, no matter what I try, typeorm does not appear to find this migrator and hence, does not run it.
My file structure (other files/folders not shown):
├── Server
│   ├── dist
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.js
|   |   |   ├── 1234567891234567890-AddUserTable.js.map
|   |   |   ├── 1234567891234567890-AddUserTable.d.ts
│   ├── src
|   |   ├── Migrations
|   |   |   ├── 1234567891234567890-AddUserTable.ts
|   |   ├── app.module.ts
app.module.ts
@Module({
    imports: [
        ConfigModule.forRoot({ envFilePath: '.env' }),
        TypeOrmModule.forRootAsync({
            imports: [ConfigModule],
            useFactory: (configService: ConfigService) =&gt; ({
                type: 'mysql',
                host: configService.get('TYPEORM_HOST'),
                port: +configService.get&lt;number&gt;('TYPEORM_PORT'),
                username: configService.get('TYPEORM_USERNAME'),
                password: configService.get('TYPEORM_PASSWORD'),
                database: configService.get('TYPEORM_DATABASE'),
                synchronize: configService.get('TYPEORM_SYNCHRONIZE'),
                entities: [__dirname + '/**/*.entity{.ts,.js}'],
                migrations: [__dirname + '/Migrations/**/*.js'],
                migrationsRun: false,
                cli: {
                    migrationsDir: './Migrations',
                },
            }),
            inject: [ConfigService],
        }),
    ],
    controllers: [],
    providers: [],
})
export class AppModule {
    constructor(private connection: Connection) {}
}
In order to run this, in my console window, I type: nest start in order get my Server started.
Then, I run npx typeorm migration:run which I get:
query: SELECT * FROM `INFORMATION_SCHEMA`.`COLUMNS` WHERE `TABLE_SCHEMA` = 'myDB' AND `TABLE_NAME` = 'migrations'
query: SELECT * FROM `myDB`.`migrations` `migrations` ORDER BY `id` DESC
No migrations are pending
If I look in my DB, I see a migrations table with no entries inside.
I have tried to delete my migrator file and create it again with a more recent timestamp and that does not work either.
npx typeorm migration:create -n &quot;MyMigratorName&quot;
Any help would be greatly appreciated.
",<mysql><nestjs><typeorm>,2309,0,46,10553,24,79,146,65,25335,0,234,5,21,2020-07-09 19:07,2020-08-27 13:40,,49,,Advanced,32
50456780,Run MySQL on Port 3307 Using Docker Compose,"I am trying to create multiple Prisma database services on a single machine. I have been unable to create a MySQL database on a port other than 3306 using Docker Compose. 
docker-compose.yml 
version: '3'
services:
hackernews:
    image: prismagraphql/prisma:1.8
    restart: always
    ports:
    - ""${CLIENT_PORT}:${INTERNAL_PORT}""
    environment:
    PRISMA_CONFIG: |
        port: $INTERNAL_PORT
        managementApiSecret: $PRISMA_MANAGEMENT_API_SECRET
        databases:
        default:
            connector: mysql
            host: mysql
            port: $SQL_INTERNAL_PORT
            user: root
            password: $SQL_PASSWORD
            migrations: true
mysql:
    image: mysql:5.7
    restart: always
    environment:
    MYSQL_ROOT_PASSWORD: $SQL_PASSWORD
    volumes:
    - ./custom/:/etc/mysql/conf.d/my.cnf
    - mysql:/var/lib/mysql
volumes:
mysql:
docker-compose.override.yml 
version: '3'
services:
mysql:
    expose:
    - ""${SQL_INTERNAL_PORT}""
    ports:
    - ""${SQL_CLIENT_PORT}:${SQL_INTERNAL_PORT}""
Error:
hackernews_1  | Exception in thread ""main"" java.sql.SQLTransientConnectionException: database - Connection is not available, request timed out after 5008ms.
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145)
hackernews_1  |     at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83)
hackernews_1  |     at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18)
hackernews_1  |     at slick.jdbc.JdbcBackend$BaseSession.&lt;init&gt;(JdbcBackend.scala:439)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217)
hackernews_1  |     at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38)
hackernews_1  |     at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
hackernews_1  |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
hackernews_1  |     at java.lang.Thread.run(Thread.java:748)
hackernews_1  | Caused by: java.sql.SQLNonTransientConnectionException: Could not connect to address=(host=mysql)(port=3307)(type=master) : Connection refused (Connection refused)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.get(ExceptionMapper.java:161)
hackernews_1  |     at org.mariadb.jdbc.internal.util.exceptions.ExceptionMapper.connException(ExceptionMapper.java:79)
hackernews_1  |     at org.mariadb.jdbc.internal.protocol.AbstractConnectProtocol.connectWithoutProxy(AbstractConnectProtocol.java:1040)
hackernews_1  |     at org.mariadb.jdbc.internal.util.Utils.retrieveProxy(Utils.java:490)
hackernews_1  |     at org.mariadb.jdbc.MariaDbConnection.newConnection(MariaDbConnection.java:144)
hackernews_1  |     at org.mariadb.jdbc.Driver.connect(Driver.java:90)
hackernews_1  |     at slick.jdbc.DriverDataSource.getConnection(DriverDataSource.scala:101)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341)
hackernews_1  |     at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570)
hackernews_1  |     at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563)
hackernews_1  |     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
docker ps 
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                                  PORTS                              NAMES
ab721996469d        mysql:5.7                  ""docker-entrypoint.s…""   42 minutes ago      Up 55 seconds                           3306/tcp, 0.0.0.0:3307-&gt;3307/tcp   two_mysql_1
7aab98e2b8d7        prismagraphql/prisma:1.8   ""/bin/sh -c /app/sta…""   2 hours ago         Restarting (1) Less than a second ago                                      two_hackernews_1
.env
SQL_PASSWORD=myuniquepassword
SQL_INTERNAL_PORT=3307
SQL_CLIENT_PORT=3307
",<mysql><docker><docker-compose><prisma>,4804,0,73,343,2,3,9,40,52457,0,0,3,21,2018-05-21 21:21,2018-05-21 22:17,2018-05-21 22:17,0,0,Basic,10
56760683,"Windows API call ""RegGetValueW"" returned error code: 0","I am getting Windows API Call Error Code :  0 
I have installed SSMS 2018 and was trying to find out if I have any other versions installed. For that I ran sqllocaldb versions in CMD but got the following message:
  Windows API call ""RegGetValueW"" returned error code: 0.
When I checked manually (via Control Panel), I saw that I have 2015 and 2016 versions installed. So Why it are they not showing in CMD. 
I tried to find other solutions but found nothing that made sense to me. 
",<sql-server><ssms><localdb>,483,0,1,221,0,3,6,57,8219,0,0,3,21,2019-06-25 19:16,2019-10-07 18:49,,104,,Basic,14
65184035,Alembic ignore specific tables,"I'm using alembic to manage database migrations as per user defined sqlalchemy models. My challenge is that I'd like for alembic to ignore any creation, deletion, or changes to a specific set of tables.
Note: My Q is similar to this question Ignoring a model when using alembic autogenerate but is different in that I want to control alembic from outside the model definition.
Here's a sample table I want to ignore:
from sqlalchemy import MetaData
from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base(metadata=MetaData())
class Ignore1(Base):
    &quot;&quot;&quot;
    Signed in to the account...
    &quot;&quot;&quot;
    __tablename__ = 'ignore_1'
    __table_args__ = {
        'info':{'skip_autogenerate':True}
        }
    id = Column(Integer, primary_key=True)
    foo = Column(String(20), nullable=True)
Example code (which does not solve my issue): 
In alembic/env.py
# Ideally this is stored in my actual database, but for now, let's assume we have a list...
IGNORE_TABLES = ['ignore_1', 'ignore_2']
def include_object(object, name, type_, reflected, compare_to):
    &quot;&quot;&quot;
    Should you include this table or not?
    &quot;&quot;&quot;
    if type_ == 'table' and (name in IGNORE_TABLES or object.info.get(&quot;skip_autogenerate&quot;, False)):
        return False
    elif type_ == &quot;column&quot; and object.info.get(&quot;skip_autogenerate&quot;, False):
        return False
    return True
# Then add to config
context.configure(
    ...
    include_object=include_object,
    ...
    )
",<python><sqlalchemy><alembic>,1553,1,37,9698,2,51,73,68,6140,0,6370,1,21,2020-12-07 14:55,2020-12-11 22:23,2020-12-11 22:23,4,4,Basic,14
52943627,Convert a pandas dataframe to a PySpark dataframe,"I have a script with the below setup.
I am using:
1) Spark dataframes to pull data in
2) Converting to pandas dataframes after initial aggregatioin
3) Want to convert back to Spark for writing to HDFS
The conversion from Spark --> Pandas was simple, but I am struggling with how to convert a Pandas dataframe back to spark.
Can you advise?
from pyspark.sql import SparkSession
import pyspark.sql.functions as sqlfunc
from pyspark.sql.types import *
import argparse, sys
from pyspark.sql import *
import pyspark.sql.functions as sqlfunc
import pandas as pd
def create_session(appname):
    spark_session = SparkSession\
        .builder\
        .appName(appname)\
        .master('yarn')\
        .config(""hive.metastore.uris"", ""thrift://uds-far-mn1.dab.02.net:9083"")\
        .enableHiveSupport()\
        .getOrCreate()
    return spark_session
### START MAIN ###
if __name__ == '__main__':
    spark_session = create_session('testing_files')
I've tried the below - no errors, just no data! To confirm, df6 does have data &amp; is a pandas dataframe
df6 = df5.sort_values(['sdsf'], ascending=[""true""])
sdf = spark_session.createDataFrame(df6)
sdf.show()
",<python-3.x><pandas><pyspark><apache-spark-sql>,1156,0,23,1906,2,23,46,51,67478,,122,1,21,2018-10-23 7:40,2018-10-23 13:05,2018-10-23 13:05,0,0,Basic,6
53634650,Hash function in spark,"I'm trying to add a column to a dataframe, which will contain hash of another column.
I've found this piece of documentation:
https://spark.apache.org/docs/2.3.0/api/sql/index.html#hash
And tried this:  
import org.apache.spark.sql.functions._
val df = spark.read.parquet(...)
val withHashedColumn = df.withColumn(""hashed"", hash($""my_column""))
But what is the hash function used by that hash()? Is that murmur, sha, md5, something else?  
The value I get in this column is integer, thus range of values here is probably [-2^(31) ... +2^(31-1)].
Can I get a long value here? Can I get a string hash instead?
How can I specify a concrete hashing algorithm for that?
Can I use a custom hash function?
",<scala><apache-spark><hash><apache-spark-sql>,698,2,8,4199,6,47,66,68,27595,0,2310,2,21,2018-12-05 14:34,2019-05-23 14:38,,169,,Basic,3
48348366,"Eloquent join using ""USING"" clause with N query","I'm using Slim Framework with Illuminate Database.
I want to make JOIN query with USING clause. Let's say given Sakila database. Diagram:
How to make join with USING clause (not ON) in eloquent model?
SELECT film_id,title,first_name,last_name 
FROM film_actor 
INNER join film USING(film_id) -- notice 
INNER join actor USING(actor_id) -- notice 
What I want is an eager loading with EXACT 1 query. The use of eloquent relationships described in the API is not meeting my expectation, since any eager relation use N+1 query. I want to make it less IO to database.
FilmActor model :
class FilmActor extends Model
{
    protected $table = 'film_actor';
    protected $primaryKey = [&quot;actor_id&quot;, &quot;film_id&quot;];
    protected $incrementing = false;
    protected $appends = ['full_name'];
    // i need to make it in Eloquent model way, so it easier to manipulate
    public function getFullNameAttribute()  
    {
        $fn = &quot;&quot;;
        $fn .= isset($this-&gt;first_name) ? $this-&gt;first_name .&quot; &quot;: &quot;&quot;;
        $fn .= isset($this-&gt;last_name) ? $this-&gt;last_name .&quot; &quot;: &quot;&quot;;
        return $fn; 
    }
    public function allJoin()
    {
        // how to join with &quot;USING&quot; clause ?
        return self::select([&quot;film.film_id&quot;,&quot;title&quot;,&quot;first_name&quot;,&quot;last_name&quot;])
            -&gt;join(&quot;film&quot;, &quot;film_actor.film_id&quot;, '=', 'film.film_id')  
            -&gt;join(&quot;actor&quot;, &quot;film_actor.actor_id&quot;, '=', 'actor.actor_id');  
        //something like
        //return self::select(&quot;*&quot;)-&gt;joinUsing(&quot;film&quot;,[&quot;film_id&quot;]);
        //or
        //return self::select(&quot;*&quot;)-&gt;join(&quot;film&quot;,function($join){
        //    $join-&gt;using(&quot;film_id&quot;);
        //});
    }
}
So, in the controller I can get the data like
$data = FilmActor::allJoin()  
        -&gt;limit(100)  
        -&gt;get();`  
But there's a con, if I need to add extra behavior (like where or order).
$data = FilmActor::allJoin()
        -&gt;where(&quot;film.film_id&quot;,&quot;1&quot;)   
        -&gt;orderBy(&quot;film_actor.actor_id&quot;)  
        -&gt;limit(100)  
        -&gt;get();`  
I need to pass table name to avoid ambiguous field. Not good. So I want for further use, I can do
$kat = $request-&gt;getParam(&quot;kat&quot;,&quot;first_name&quot;);  
// [&quot;film_id&quot;, &quot;title&quot;, &quot;first_name&quot;, &quot;last_name&quot;]  
// from combobox html  
// adding &quot;film.film_id&quot; to combo is not an option  
// passing table name to html ?? big NO
$search = $request-&gt;getParam(&quot;search&quot;,&quot;&quot;);
$order = $request-&gt;getParam(&quot;order&quot;,&quot;&quot;);
$data = FilmActor::allJoin()
        -&gt;where($kat,&quot;like&quot;,&quot;%$search%&quot;)   
        -&gt;orderBy($order)  
        -&gt;limit(100)  
        -&gt;get();`  
",<php><mysql><sql><eloquent><slim>,2969,4,60,3402,6,34,58,62,1075,0,84,3,21,2018-01-19 19:38,2018-02-09 1:48,,21,,Basic,3
57257965,DBeaver restore SQL Server .bak file,"I am just trying to restore a SQL Server .bak file in my DBeaver UI. But I have no idea how to do this - can someone help please? 
I created a database, but when I right click on it, there are no restore options.
",<sql-server><restore><dbeaver>,213,1,2,2114,7,23,49,56,37217,0,59,4,21,2019-07-29 16:31,2019-07-29 18:50,,0,,Intermediate,20
55755095,PostgreSQL- ModuleNotFoundError: No module named 'psycopg2',"I can confirm psycopg2 is install (using conda install -c anaconda psycopg2) but the it seems psycopg2 cannot be imported to my python script or the interpreter is unable to locate it. I also tried installing using pip3, requirements are satisfied, meaning psycopg2 is already istalled, but cannot understand why I script isn't able to import it. Using Mac (OS v10.14.4) 
$ python create_tables.py
Traceback (most recent call last):
  File ""create_tables.py"", line 1, in &lt;module&gt;
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'
$ pip3 install psycopg2
Requirement already satisfied: psycopg2 in /usr/local/lib/python3.7/site-packages (2.8.2)
$ pip3 install psycopg2-binary
Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.7/site-packages (2.8.2)
python -V
Python 3.7.0
Any idea why  this happen?
EDIT: create_table.py
import psycopg2
from config import config
def create_tables():
    """""" create tables in the PostgreSQL database""""""
    commands = (
        """"""
        CREATE TABLE vendors (
            vendor_id SERIAL PRIMARY KEY,
            vendor_name VARCHAR(255) NOT NULL
        )
        """""",
        """""" CREATE TABLE parts (
                part_id SERIAL PRIMARY KEY,
                part_name VARCHAR(255) NOT NULL
                )
        """""",
        """"""
        CREATE TABLE part_drawings (
                part_id INTEGER PRIMARY KEY,
                file_extension VARCHAR(5) NOT NULL,
                drawing_data BYTEA NOT NULL,
                FOREIGN KEY (part_id)
                REFERENCES parts (part_id)
                ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""",
        """"""
        CREATE TABLE vendor_parts (
                vendor_id INTEGER NOT NULL,
                part_id INTEGER NOT NULL,
                PRIMARY KEY (vendor_id , part_id),
                FOREIGN KEY (vendor_id)
                    REFERENCES vendors (vendor_id)
                    ON UPDATE CASCADE ON DELETE CASCADE,
                FOREIGN KEY (part_id)
                    REFERENCES parts (part_id)
                    ON UPDATE CASCADE ON DELETE CASCADE
        )
        """""")
    conn = None
    try:
        # read the connection parameters
        params = config()
        # connect to the PostgreSQL server
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        # create table one by one
        for command in commands:
            cur.execute(command)
        # close communication with the PostgreSQL database server
        cur.close()
        # commit the changes
        conn.commit()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()
if __name__ == '__main__':
    create_tables()
",<python><python-3.x><psycopg2><postgresql-9.1>,2788,0,77,3495,6,29,62,80,32251,0,439,3,21,2019-04-19 0:08,2019-04-19 0:19,2019-04-19 1:09,0,0,Basic,9
48112591,Generate ansi sql INSERT INTO,"I have Oracle database with 10 tables. Some of the tables have CLOB data text. I need to export data from these tables pro-grammatically using java. The export data should be in ANSI INSERT INTO SQL format, for example: 
INSERT INTO table_name (column1, column2, column3, ...)
VALUES (value1, value2, value3, ...);
The main idea is that I need to import this data into three different databases:
ORACLE, MSSQL and MySQL. As I know all these databases support ANSI INSERT INTO. But I have not found any java API/framework for generating data SQL scripts. And I do not know how to deal with CLOB data, how to export it. 
What is the best way to export data from a database with java? 
UPDATE: (01.07.2018) 
I guess it is impossible to insert text data more than 4000 bytes according to this answer. How to generate PL\SQL scripts using java programmatically? Or is there any other export format which supports ORACLE, MSSQL, etc?
",<java><sql><oracle>,928,1,2,2689,3,33,50,76,2959,0,2360,7,21,2018-01-05 11:21,2018-01-05 11:27,,0,,Basic,9
49783108,PostgreSQL 10 on Linux - LC_COLLATE locale en_US.utf-8 not valid,"
  ERROR: invalid locale name: ""en_US.utf-8""
Running Ubuntu server 18.04 Beta 2 with PostgreSQL 10.
In running a database creation script that worked on 9.5, I am now seeing an issue with 'en_US.UTF-8' as a locale:
CREATE DATABASE db WITH TEMPLATE = template0 ENCODING = 'UTF8' LC_COLLATE = 'en_US.UTF-8' LC_CTYPE = 'en_US.UTF-8';
I know this may be redundant as I understand the default to be 'en_US.etf-8'.  Removing the LC_COLLATE and LC_CTYPE parameters let me run my script.
So did the locale definitions change somehow for V 10?  Or is there something else now happening?  I couldn't find anything on this in the Postgres 10 manual.
",<postgresql>,639,0,1,1603,2,10,17,81,44629,0,0,6,21,2018-04-11 19:31,2018-04-12 19:40,2018-04-12 19:40,1,1,Basic,9
59065629,"A field with precision 10, scale 2 must round to an absolute value less than 10^8","I have a django-field total_price in postgres database version 9.3.11.
Here is the code:
total_value = models.DecimalField(decimal_places=100, default=0, max_digits=300)
I want to convert it to proper 2 decimal place. So I wrote this:
total_value = models.DecimalField(decimal_places=2, default=0, max_digits=10)
My migration file
# -*- coding: utf-8 -*-
from __future__ import unicode_literals
from django.db import models, migrations
class Migration(migrations.Migration):
    dependencies = [
        ('my_table', '0040_my_table_skipped'),
    ]
    operations = [
        migrations.AlterField(
            model_name='my_table',
            name='total_value',
            field=models.DecimalField(default=0, 
                                      max_digits=10, 
                                      decimal_places=2),
        ),
    ]
When I run command python manage.py migrate
I get an error from postgresql:
A field with precision 10, scale 2 must round to an absolute value less than 10^8.
",<python><django><postgresql><django-models><postgresql-9.3>,1003,0,26,9182,13,57,85,55,61396,,327,3,21,2019-11-27 8:03,2020-04-26 7:43,,151,,Basic,9
49088401,Spark from_json with dynamic schema,"I am trying to use Spark for processing JSON data with variable structure(nested JSON). Input JSON data could be very large with more than 1000 of keys per row and one batch could be more than 20 GB. 
Entire batch has been generated from 30 data sources and 'key2' of each JSON can be used to identify the source and structure for each source is predefined.
What would be the best approach for processing such data?
I have tried using from_json like below but it works only with fixed schema and to use it first I need to group the data based on each source and then apply the schema. 
Due to large data volume my preferred choice is to scan the data only once and extract required values from each source, based on predefined schema.
import org.apache.spark.sql.types._ 
import spark.implicits._
val data = sc.parallelize(
    """"""{""key1"":""val1"",""key2"":""source1"",""key3"":{""key3_k1"":""key3_v1""}}""""""
    :: Nil)
val df = data.toDF
val schema = (new StructType)
    .add(""key1"", StringType)
    .add(""key2"", StringType)
    .add(""key3"", (new StructType)
    .add(""key3_k1"", StringType))
df.select(from_json($""value"",schema).as(""json_str""))
  .select($""json_str.key3.key3_k1"").collect
res17: Array[org.apache.spark.sql.Row] = Array([xxx])
",<json><apache-spark><apache-spark-sql>,1233,0,19,284,1,2,9,70,44295,0,2,3,21,2018-03-03 19:37,2018-03-04 3:54,,1,,Intermediate,18
55581114,COUNT(id) or MAX(id) - which is faster?,"I have a web server on which I've implemented my own messaging system.
I am at a phase where I need to create an API that checks if the user has new messages.
My DB table is simple:
ID - Auto Increment, Primary Key (Bigint)
Sender - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Recipient - Varchar (32) // Foreign Key to UserID hash from Users DB Table
Message - Varchar (256) //UTF8 BIN
I am considering making an API that will estimate if there are new messages for a given user. I am thinking of using one of these methods:
A) Select count(ID) of messages where sender or recipient is me.
(if this number &gt; previous number, I have a new message)
B) Select max(ID) of messages where sender or recipient is me.
(if max(ID) &gt; than previous number, I have a new message)
My question is:  Can I calculate somehow what method will consume fewer server resources? Or is there some article? Maybe another method I didn't mention?
",<php><mysql><performance>,949,0,6,336,0,6,18,50,4952,0,9,4,21,2019-04-08 20:15,2019-04-08 20:19,2019-04-08 20:19,0,0,Intermediate,23
53784468,Postgres 10.3: SELECT queries hang for hours,"My application is using Postgres as DBMS, the version of Postgres that i'm using is 10.3 with the extension Postgis installed. 
Occasionally i noticed that in random interval of times the dbms become slow and get stuck on a few SELECT queries.
From pg_stat_activity i noticed that the wait_event_type and wait_event of these queries is as follows: 
 select wait_event_type, wait_event from pg_stat_activity where state='active'; 
 wait_event_type |  wait_event  
-----------------+--------------
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 LWLock          | buffer_io
 IO              | DataFileRead
 IO              | DataFileRead
                 | 
 IO              | DataFileRead
 LWLock          | buffer_io
 LWLock          | buffer_io
(33 rows)
My assumption, after checking the docs, is that the hardware underneath has some issues and then the problem i'm facing is not related to the application, or the type of query, but to the hardware itself.
Anybody ever faced this kind of issue? 
",<postgresql><postgresql-10>,1735,1,40,1490,2,12,22,74,12768,0,67,1,21,2018-12-14 17:37,2021-04-28 11:28,,866,,Intermediate,23
55297807,When do Postgres column or table names need quotes and when don't they?,"Let's consider the following postgres query:
SELECT * 
FROM ""MY_TABLE""
WHERE ""bool_var""=FALSE 
 AND ""str_var""='something';
The query fails to respond properly when I remove quotes around ""str_var"" but not when I do the same around ""bool_var"". Why? What is the proper way to write the query in that case, no quotes around the boolean column and quotes around the text column? Something else?
",<postgresql><quoted-identifier>,391,0,6,594,1,5,10,68,19822,0,144,2,21,2019-03-22 10:39,2019-03-22 10:46,2019-03-22 12:21,0,0,Basic,8
53641403,Search in Json column with Laravel,"In my emails table, I have a column named To with column-type Json. This is how values are stored:
[
    {
        ""emailAddress"": {
            ""name"": ""Test"", 
            ""address"": ""test@example.com""
        }
    }, 
    {
        ""emailAddress"": {
            ""name"": ""Test 2"", 
            ""address"": ""test2@example.com""
        }
    }
]
Now I want a collection of all emails sent to ""test@example.com"". I tried:
DB::table('emails')-&gt;whereJsonContains('to-&gt;emailAddress-&gt;address', 'test@example.com')-&gt;get();
(see https://laravel.com/docs/5.7/queries#json-where-clauses)
but I do not get a match. Is there a better way to search using Laravel (Eloquent)?
In the debugbar, I can see that this query is ""translated"" as:
select * from `emails` where json_contains(`to`-&gt;'$.""emailAddress"".""address""', '\""test@example.com\""'))
",<php><mysql><json><laravel><laravel-5>,845,2,18,1571,3,21,43,44,49425,0,176,7,21,2018-12-05 21:59,2018-12-06 7:43,2018-12-06 15:27,1,1,Basic,7
52088355,FluentMySQL connection using Unix Socket,"I'm following the Getting started section for the MySQL package on the Vapor Documentation, which I'm able to follow step by step and, as a result, I have successfully established a connection to the MySQL database, using custom database credentials like this:
/// Register providers first
try services.register(FluentMySQLProvider())
// MySQL database
let mySQLConfig = MySQLDatabaseConfig(hostname: ""localhost"",
                                      port: 3306,
                                      username: ""root"",
                                      password: ""thisismyrootpassword"",
                                      database: ""lol_database"",
                                      capabilities: .default,
                                      characterSet: MySQLCharacterSet.utf8_general_ci,
                                      transport: MySQLTransportConfig.cleartext)
services.register(mySQLConfig)
Based on the MySQLDatabaseConfig object's documentation I'm unable to find if it is possible to connect to a MySQL database based on a Unix Socket configuration.
What I'll be able to provide to the application under the production environment it's just the database name, the username, password and the Socket path, which will be in the form /cloudsql/project1:us-central1:instance1
For more reference, what I'm trying to do is connect from a Google Cloud App Engine flexible environment to a SQL database based on this tutorial: https://cloud.google.com/appengine/docs/flexible/nodejs/using-cloud-sql#setting_up_your_local_environment The environment of course will be Vapor still that's the only way for a database client to establish connection to the database server.
Thank you for your help.
",<mysql><swift><vapor>,1714,4,16,1509,2,19,27,38,335,0,821,1,21,2018-08-30 2:17,2021-07-09 0:32,2021-07-09 0:32,1044,1044,Basic,3
58233866,SQLSTATE[HY000] [1045] Access denied for user 'root'@'localhost' (using password: NO) . DB_HOST set to localhost,"I moved the Laravel project from localhost to server. Which I have done every step on the server.
I am able to view the login page on my server. The problem is I am not able to connect with my MySQL server.
My .env file:
APP_NAME=Transport
APP_ENV=local
APP_KEY=base64:mrakeyidharhaikonsdf
APP_DEBUG=true
APP_URL=http://localhost
LOG_CHANNEL=stack
DB_CONNECTION=mysql
DB_HOST=localhost
DB_PORT=3306
DB_DATABASE=transport_db
DB_USERNAME=root
DB_PASSWORD=mypass
I tried to change the host to 127.0.0.1 and also tried to put my server's IP address. It didn't helped me. Am I missing something?
My error:
SQLSTATE[HY000] [1045] Access denied for user 'root'@'localhost' (using password: NO) (SQL: select count(*) as aggregate from users where email = user.email@gmail.com)
I know this question may have answers already on Stack Overflow. But I have different issue here.
",<php><mysql><laravel><laravel-6>,867,1,18,1603,2,17,34,45,170223,0,650,16,21,2019-10-04 9:46,2019-10-04 10:16,2019-10-04 10:17,0,0,Basic,6
48053955,Alembic Migrations on Multiple Models,"I am attempting to create a revision with --autogenerate using Alembic for two Models, but am receiving a duplicate table keys error. Does, a schema need to be specified?  If so, how can it be set?  The documentation I've read says to use __table_args__ = {'schema': 'somename'}, but that hasn't helped.  Any tips or suggestions are greatly appreciated.
My current setup is:
base.py
from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()
workspace.py
from sqlalchemy import Column, Integer, String
from base import Base
class WorkspaceModel(Base):
    __tablename__ = 'workspaces'
    id = Column(Integer, primary_key=True)
    name = Column(String)
host.py
from sqlalchemy import Column, Integer, String
from base import Base
class HostModel(Base):
    __tablename__ = 'hosts'
    id = Column(Integer, primary_key=true)
    ip = Column(String)
alembic/env.py
from host import HostModel
from workspace import WorkspaceModel
target_metadata = [HostModel.metadata, WorkspaceModel.metadata]
Error
ValueError: Duplicate table keys across multiple MetaData objects: ""hosts"", ""workspaces""
",<python><sqlalchemy><alembic>,1110,0,26,4468,2,29,39,66,10030,0,136,3,21,2018-01-01 22:59,2020-02-25 6:14,2020-07-15 23:34,785,926,Basic,6
59582390,Confusion Around Creating a VPC Access Connector,"I am trying to set up Serverless VPC access
  Serverless VPC Access enables you to connect from your Cloud Functions directly to Compute Engine VM instances, Memorystore instances, Cloud SQL instances,
Sounds great. But the documentation is not super friendly to a beginner. Step 2 is to create a connector, about which I have a couple of questions:
  In the Network field, select the VPC network to connect to.
My dropdown here contains only ""Default"". Is this normal? What should IO expect to see here?
  In the IP range field, enter an unused CIDR /28 IP range. Addresses in this range are used as source addresses for traffic sent through the connector. This IP range must not overlap with any existing IP address reservations in your VPC network.
I don't know what to do here. I tried using the information in the linked document to first) enter an IP from the region I had selected, and, second) enter an IP from outside that region. Both resulted in connectors that were created with the error. ""Connector is in a bad state, manual deletion is recommended""
The documentation continues with a couple of troubleshooting steps if the creation fails:
  Specify an IP range that does not overlap with any existing IP address reservations in the VPC network.
I don't know what this means. Maybe like, if I have other connectors I should be sure the IP range for the new one doesn't overlap with those. That's just a guess, but anyway I have none.
  Grant your project permission to use Compute Engine VM images from the project with ID serverless-vpc-access-images. See Setting image access constraints for information on how to update your organization policy accordingly.
This leads me to another document about updating my organization's ""Image Policy"". This one has me so out of my depth, I don't even think I should be here.
This has all started with just wanting to connect to a SQL Server instance from Firebase. Creating the VPC connector seems like a good step, but I've just fallen at every hurdle. Can a cloud-dweller please help me with a few of these points of confusion? 
",<google-cloud-platform><cloud><google-cloud-sql><vpc>,2087,4,1,14803,32,109,235,56,12457,0,2623,3,21,2020-01-03 16:56,2020-01-04 4:10,,1,,Basic,3
58676909,How to speed up spark df.write jdbc to postgres database?,"I am new to spark and am attempting to speed up appending the contents of a dataframe, (that can have between 200k and 2M rows) to a postgres database using df.write:
df.write.format('jdbc').options(
      url=psql_url_spark,
      driver=spark_env['PSQL_DRIVER'],
      dbtable=""{schema}.{table}"".format(schema=schema, table=table),
      user=spark_env['PSQL_USER'],
      password=spark_env['PSQL_PASS'],
      batchsize=2000000,
      queryTimeout=690
      ).mode(mode).save()
I tried increasing the batchsize but that didn't help, as completing this task still took ~4hours. I've also included some snapshots below from aws emr showing more details about how the job ran. The task to save the dataframe to the postgres table was only assigned to one executor (which I found strange), would speeding this up involve dividing this task between executors?
Also, I have read spark's performance tuning docs but increasing the batchsize, and queryTimeout have not seemed to improve performance. (I tried calling df.cache() in my script before df.write, but runtime for the script was still 4hrs)
Additionally, my aws emr hardware setup and spark-submit are:
Master Node (1): m4.xlarge
Core Nodes (2): m5.xlarge
spark-submit --deploy-mode client --executor-cores 4 --num-executors 4 ...
",<postgresql><apache-spark><pyspark><apache-spark-sql>,1287,3,15,1040,1,15,44,41,19782,0,57,4,21,2019-11-03 2:15,2020-04-25 22:10,2020-04-26 8:59,174,175,Intermediate,23
50945477,Count rows in partition with Order By,"I was trying to understand PARTITION BY in postgres by writing a few sample queries. I have a test table on which I run my query.
id integer | num integer
___________|_____________
1          | 4 
2          | 4
3          | 5
4          | 6
When I run the following query, I get the output as I expected.
SELECT id, COUNT(id) OVER(PARTITION BY num) from test;
id         | count
___________|_____________
1          | 2 
2          | 2
3          | 1
4          | 1
But, when I add ORDER BY to the partition,
SELECT id, COUNT(id) OVER(PARTITION BY num ORDER BY id) from test;
id         | count
___________|_____________
1          | 1 
2          | 2
3          | 1
4          | 1
My understanding is that COUNT is computed across all rows that fall into a partition. Here, I have partitioned the rows by num. The number of rows in the partition is the same, with or without an ORDER BY clause. Why is there a difference in the outputs?
",<sql><postgresql><window-functions>,939,0,22,612,3,7,13,80,54789,0,352,3,21,2018-06-20 10:00,2018-06-20 10:18,2018-06-20 10:18,0,0,Basic,3
52355143,Is it possible to delete old records from clickhouse table?,"As far as I know, clickhouse allows only inserting new data. But is it possible to delete block older then some period to avoid overflow of HDD?
",<sql><clickhouse>,145,0,0,8840,28,117,207,76,39391,0,2932,3,21,2018-09-16 14:36,2018-10-05 14:27,2018-10-23 9:05,19,37,Basic,3
63997315,UNION types text and bigint cannot be matched,"I'm running a complex stored procedure and I'm getting an error when I have 3 unions, but with 2 unions no error. If I remove either of the top two unions it runs fine. If I make one of the NULLs a 0, it runs fine. The error is &quot;UNION types text and bigint cannot be matched&quot;
```lang-sql
SELECT NULL AS total_time_spent 
FROM tbl1
GROUP BY student_id 
UNION ALL 
SELECT NULL AS total_time_spent
FROM tbl2
GROUP BY student_id 
UNION ALL 
SELECT sum(cast((&quot;value&quot; -&gt;&gt; 'seconds') AS integer)) AS total_time_spent 
FROM tbl3 
GROUP BY student_id
```
I've tried all kinds of casting on the sum result or the sum input. The json that I'm pulling from is either NULL, [] or something like this:
[{&quot;date&quot;: &quot;2020-09-17&quot;, &quot;seconds&quot;: 458}]
",<postgresql><union-all>,785,0,14,1643,5,21,41,54,43648,0,73,1,21,2020-09-21 17:34,2020-09-22 2:21,2020-09-22 2:21,1,1,Basic,2
55825283,How to convert array to string in laravel?,"I am getting input from checkbox values in array using bootstrap form.
I am using array for storing checkbox values. How i convert this array to string . Because database only take string values.
Here is my code
&lt;div class=""form-group col-md-12""&gt;
    &lt;div class=""custom-control custom-checkbox custom-control-inline""&gt;
        &lt;input type=""checkbox"" id=""eduPrimary"" name=""education[]"" 
        class=""custom-control-input"" value=""primary"" /&gt;
        &lt;label class=""custom-control-label"" for=""eduPrimary""&gt;primary&lt;/label&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=""form-group col-md-12""&gt;
    &lt;div class=""custom-control custom-checkbox custom-control-inline""&gt;
        &lt;input type=""checkbox"" id=""eduSecondary"" name=""education[]"" 
        class=""custom-control-input"" value=""secondary"" /&gt;
        &lt;label class=""custom-control-label"" for=""eduSecondary""&gt;secondary&lt;/label&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=""form-group col-md-12""&gt;
    &lt;div class=""custom-control custom-checkbox custom-control-inline""&gt;
        &lt;input type=""checkbox"" id=""eduUniversity"" name=""education[]"" 
        class=""custom-control-input"" value=""university"" /&gt;
        &lt;label class=""custom-control-label""for=""eduUniversity""&gt;university&lt;/label&gt;
    &lt;/div&gt;
&lt;/div&gt;
In backend i am using laravel to store values to database But it run error that storing array to string in mysql.
public function store(Request $request,AdProfile $adprofile)
{
    $adprofile-&gt;education = $request-&gt;education[];
    $adprofile-&gt;save();
    return redirect()-&gt;route('adprofile.profilecomplete');
}
",<php><mysql><laravel>,1654,0,27,792,1,6,17,78,99378,0,128,8,21,2019-04-24 8:11,2019-04-24 8:17,2019-04-24 9:59,0,0,Basic,2
61148791,PostgreSQL on Elastic Beanstalk (Amazon Linux 2),"With former generation of Amazon Linux, all I needed to do is add the following in .ebextensions in order to use PostgreSQL:
packages:
    yum:
        postgresql93-devel: []
Now when I deploy on EB with the following platform:
Python 3.7 running on 64bit Amazon Linux 2/3.0.0
I get the following error on deployment:
[ERROR] Error occurred during build: Yum does not have postgresql93-devel available for installation
Therefore it is impossible to deploy as I need to connect to a PostgreSQL database in RDS.
What config in .ebextensions do I need to do?
",<linux><postgresql><amazon-web-services><amazon-elastic-beanstalk><yum>,556,0,4,916,0,10,19,64,7173,0,73,4,21,2020-04-10 21:14,2020-04-10 21:22,2020-08-01 10:59,0,113,Basic,14
56022874,"BigQuery replaced most of my Spark jobs, am I missing something?","I've been developing Spark jobs for some years using on-premise clusters and our team recently moved to the Google Cloud Platform allowing us to leverage the power of BigQuery and such.
The thing is, I now often find myself writing processing steps in SQL more than in PySpark since it is : 
easier to reason about (less verbose)
easier to maintain (SQL vs scala/python code)
you can run it easily on the GUI if needed
fast without having to really reason about partitioning, caching and so on...
In the end, I only use Spark when I've got something to do that I can't express using SQL. 
To be clear, my workflow is often like : 
preprocessing (previously in Spark, now in SQL)
feature engineering (previously in Spark, now mainly in SQL)
machine learning model and predictions (Spark ML)
Am I missing something ?
Is there any con in using BigQuery this way instead of Spark ?
Thanks
",<sql><apache-spark><apache-spark-sql><google-bigquery><bigdata>,885,0,0,667,0,5,16,60,5633,0,1,2,20,2019-05-07 12:41,2019-07-31 23:26,,85,,Intermediate,20
50907968,Server is not configured for RPC,"Looking for my job history I foudn the error below:
06/18/2018 00:00:01,MBS_Lojas_ExportaMR_OutrasLojas,Error,1,WIN-VRT-01\SQL2008,MBS_Lojas_ExportaMR_OutrasLojas,Passo1,,Executed as user: WIN-VRT-01\integracao. Server 'x.y.z' is not configured for RPC. [SQLSTATE 42000] (Error 7411).  The step failed.,01:11:15,16,7411,,,,0
I have this linked server with the option RPC and RPC Out with the values assigned to true.
In the job I have this
EXEC master.dbo.sp_serveroption @server=N'x.y.z', @optname=N'rpc', @optvalue=N'true'
EXEC master.dbo.sp_serveroption @server=N'x.y.z', @optname=N'rpc out', @optvalue=N'true'
I can't find out why is this happening and none of the solutions posted for this error could help me to debug this issue.
",<sql-server>,736,0,3,840,2,8,27,41,47285,0,8,3,20,2018-06-18 10:59,2019-04-22 16:54,,308,,Basic,13
56503583,Error while installing SQL Server 2017 Express showing sqlncli.msi is missing in some path,"I am trying to install SQL Server 2017 Express, but it is throwing this error:
sqlncli.msi is not found in the path
Screenshot illustrating the sqlncli.msi error:
",<sql-server-2017-express>,163,1,0,265,1,3,12,45,18529,0,0,3,20,2019-06-08 5:01,2019-06-12 13:40,,4,,Basic,14
51893065,"PostgreSQL & BDR: Is BDR truly multi-master, is it Open Source and EOL for 1.x in 2019?","I am confused regarding PostgreSQL BDR and I have several questions:
Question 1: Is BDR truly multi-master for PostgreSQL?
According to the docs here, it says that:
  The BDR (Bi-Directional Replication) project adds multi-master
  replication to PostgreSQL 9.4
but if I read on 2ndQuadrant, I read the following:
If I read that part, they don't mention multi-master much at all; just that a ""second master, working in passive"", which indicates its not a real master?
Question 2: Is BDR open-source?
I read here that it is, at least that it was:
  BDR is the first open source multi-master replication system for PostgreSQL
Is it still? Because when I look, I am often directed to 2ndQuadrants webpage, and that gives me the impression that its not open-source, when they say that:
  How can you get Postgres-BDR?
  Just fill out the contact form below and a PostgreSQL expert will be in touch shortly!
Sounds like selling to me =)
Question 3: What version is what?
I read that 2ndQuadrant released version 1.0.5 in March this year. I also read on 2ndQuadrants webpage that
  In the complex environment of replication, the 3rd generation of BDR achieves...
The 3rd gen? Is version 1.0.5 that same 3rd gen, or is it something else?
Also, the same page says that:
  Note for current Postgres-BDR users: BDR 1.x  will reach EOL in December 2019. Our team of PostgreSQL experts can help plan and execute your upgrade with minimal impact and almost zero downtime. Contact us today and a member of our professional services team will be in touch with you as soon as possible.
So, 1.0.5 was released in March, but has EOL in December 2019? Is 2.x not open-source, so some license cost associated with it, and 1.x is EOL 2019?
",<postgresql><postgresql-bdr><postgres-bdr>,1719,4,0,19797,35,97,156,76,11323,0,615,3,20,2018-08-17 10:08,2018-08-24 10:50,2018-11-13 1:41,7,88,Intermediate,19
50828041,How to open database sqlite file on iPhone real device?,"I'm debbuging a app in my real device by cable. I've a iPhone 6. I want check my database and operate with sqlite3 to query my results. The other questions and tutorials explain to do this only in simulator but I'm using a real iPhone.
In AppDelegate, I prints the path of database:
print(NSSearchPathForDirectoriesInDomains(.documentDirectory, .userDomainMask, true).last! as String)
/Users/myname/Library/Developer/CoreSimulator/Devices/DAE93E57-7004-45F6-9B93-E79CA1AEEEFA/data/Containers/Data/Application/D7A4F27E-6F11-4941-A1B0-0337ABF788AB/Documents
So, I take the path and access from terminal and access my database with sqlite3 DatabaseFile
But when I debugging in my device, the path that's printed not works. I tried use the printed path
cd /var/mobile/Containers/Data/Application/3257D423-C198-41A5-B29D-B31E99F84F34/Documents
/usr/bin/CD: line 4: cd: /var/mobile/Containers/Data/Application/3257D423-C198-41A5-B29D-B31E99F84F34/Documents: No such file or directory
This error happens because this is of iOS system, I think.
",<ios><swift><xcode><sqlite>,1037,0,5,3905,9,45,100,55,14829,0,529,5,20,2018-06-13 1:45,2018-06-13 5:49,,0,,Basic,9
52000903,How to start flyway after database initialization in Docker,"I have following docker compose file(docker-compose-dev.yml):
version: '3'
services:
  my_sql_db:
    image: percona:latest
    container_name: my_sql_db
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: abhs
    ports:
    - ""3306:3306""
  migration:
    image: boxfuse/flyway:latest
    container_name: flyway_migration
    volumes:
      - ./flyway_scripts/src/main/resources/db/migration:/flyway/sql
    command: -url=jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false -user=root -password=password migrate
    depends_on:
    - my_sql_db
and following docker-compose.yml:
version: '3'
services:
  migration:
    image: boxfuse/flyway:latest
    container_name: flyway_migration
    volumes:
     - ./flyway_scripts/src/main/resources/db/migration:/flyway/sql
Then I execute following command:
docker-compose -f docker-compose.yml -f docker-compose-dev.yml up
And It lead to error:
In logs I see following:
my_sql_db    | Initializing database
flyway_migration  | Flyway Community Edition 5.1.4 by Boxfuse
flyway_migration  |
my_sql_db    | 2018-08-24T08:47:41.616694Z 0 [Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
my_sql_db    | 2018-08-24T08:47:41.616747Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
flyway_migration  | ERROR:
flyway_migration  | Unable to obtain connection from database (jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false) for user 'root': Could not connect to address=(host=my_sql_db)(port=3306)(type=master) : Connection refused (Connection refused)
flyway_migration  | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
flyway_migration  | SQL State  : 08
flyway_migration  | Error Code : -1
flyway_migration  | Message    : Could not connect to address=(host=my_sql_db)(port=3306)(type=master) : Connection refused (Connection refused)
my_sql_db    | 2018-08-24T08:47:43.024690Z 0 [Warning] InnoDB: New log files created, LSN=45790
flyway_migration  |
my_sql_db    | 2018-08-24T08:47:43.443625Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.
my_sql_db    | 2018-08-24T08:47:43.588008Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 5dc59a4f-a77a-11e8-b6cb-0242ac130002.
my_sql_db    | 2018-08-24T08:47:43.760654Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.
my_sql_db    | 2018-08-24T08:47:44.518107Z 0 [Warning] CA certificate ca.pem is self signed.
my_sql_db    | 2018-08-24T08:47:44.925466Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
my_sql_db    | 2018-08-24T08:47:54.762213Z 1 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.762517Z 1 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.762889Z 1 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.763244Z 1 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.763472Z 1 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.763788Z 1 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.763928Z 1 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | 2018-08-24T08:47:54.764128Z 1 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
my_sql_db    | Database initialized
my_sql_db    | MySQL init process in progress...
my_sql_db    | 2018-08-24T08:47:58.970290Z 0 [Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
my_sql_db    | 2018-08-24T08:47:58.970345Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.
my_sql_db    | 2018-08-24T08:47:58.974061Z 0 [Note] mysqld (mysqld 5.7.22-22) starting as process 58 ...
my_sql_db    | 2018-08-24T08:47:58.999651Z 0 [Note] InnoDB: PUNCH HOLE support available
my_sql_db    | 2018-08-24T08:47:58.999685Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
my_sql_db    | 2018-08-24T08:47:58.999689Z 0 [Note] InnoDB: Uses event mutexes
my_sql_db    | 2018-08-24T08:47:58.999692Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
my_sql_db    | 2018-08-24T08:47:58.999695Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.8
my_sql_db    | 2018-08-24T08:47:58.999698Z 0 [Note] InnoDB: Using Linux native AIO
my_sql_db    | 2018-08-24T08:47:59.000153Z 0 [Note] InnoDB: Number of pools: 1
my_sql_db    | 2018-08-24T08:47:59.000426Z 0 [Note] InnoDB: Using CPU crc32 instructions
my_sql_db    | 2018-08-24T08:47:59.002306Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
my_sql_db    | 2018-08-24T08:47:59.006893Z 0 [Note] InnoDB: Completed initialization of buffer pool
my_sql_db    | 2018-08-24T08:47:59.013219Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
my_sql_db    | 2018-08-24T08:47:59.024242Z 0 [Note] InnoDB: Crash recovery did not find the parallel doublewrite buffer at /var/lib/mysql/xb_doublewrite
my_sql_db    | 2018-08-24T08:47:59.026263Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
my_sql_db    | 2018-08-24T08:47:59.066469Z 0 [Note] InnoDB: Created parallel doublewrite buffer at /var/lib/mysql/xb_doublewrite, size 3932160 bytes
my_sql_db    | 2018-08-24T08:47:59.071752Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
my_sql_db    | 2018-08-24T08:47:59.072052Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
my_sql_db    | 2018-08-24T08:47:59.422155Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
my_sql_db    | 2018-08-24T08:47:59.423325Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
my_sql_db    | 2018-08-24T08:47:59.423376Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
my_sql_db    | 2018-08-24T08:47:59.423900Z 0 [Note] InnoDB: Waiting for purge to start
my_sql_db    | 2018-08-24T08:47:59.474066Z 0 [Note] InnoDB: Percona XtraDB (http://www.percona.com) 5.7.22-22 started; log sequence number 2595255
my_sql_db    | 2018-08-24T08:47:59.474647Z 0 [Note] Plugin 'FEDERATED' is disabled.
my_sql_db    | 2018-08-24T08:47:59.499970Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
my_sql_db    | 2018-08-24T08:47:59.500004Z 0 [Note] Skipping generation of SSL certificates as certificate files are present in data directory.
my_sql_db    | 2018-08-24T08:47:59.500382Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
my_sql_db    | 2018-08-24T08:47:59.501263Z 0 [Warning] CA certificate ca.pem is self signed.
my_sql_db    | 2018-08-24T08:47:59.522151Z 0 [Note] Skipping generation of RSA key pair as key files are present in data directory.
my_sql_db    | 2018-08-24T08:47:59.531657Z 0 [Note] InnoDB: Buffer pool(s) load completed at 180824  8:47:59
Looks like flyway starts before database initialization and hence could not connect to database and I see the error below.
How can I fix that problem?
P.S.
I googled similar questions and I found the following piece of advice: https://github.com/vishnubob/wait-for-it but I am novice in docker  and I don't understand how to put it into my docker compose file
P.S.2
I tried to put file wait-fot-it.sh near the compose file and execute:
command: [""./wait-for-it.sh"", ""mysql:3306"", ""--"", ""-url=jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false -user=root -password=password migrate""]
But I returns ERROR: Invalid argument: ./wait-for-it.sh
P.S.3
I tried approach from ""Duplicated"" topic:
version: '3'
services:
  my_sql_db:
    image: percona:latest
    container_name: my_sql_db
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: abhs
    ports:
    - ""3306:3306""
    healthcheck:
      test: [""CMD"", ""mysqladmin"" ,""ping"", ""-h"", ""localhost""]
      timeout: 20s
      retries: 10
  migration:
    image: boxfuse/flyway:latest
    container_name: flyway_migration
    volumes:
      - ./flyway_scripts/src/main/resources/db/migration:/flyway/sql
    command: -url=jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false -user=root -password=password migrate
    depends_on:
      my_sql_db:
        condition: service_healthy
but I see following error:
$ docker-compose -f docker-compose.yml -f docker-compose-dev.yml up
The Compose file '.\docker-compose-dev.yml' is invalid because:
services.migration.depends_on contains an invalid type, it should be an array
P.S.4
for that approach I see following error:
version: '3'
services:
  my_sql_db:
    image: percona:latest
    container_name: my_sql_db
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: abhs
    ports:
    - ""3306:3306""
    healthcheck:
      test: [""CMD"", ""mysqladmin"" ,""ping"", ""-h"", ""localhost""]
      timeout: 20s
      retries: 10
  migration:
    image: boxfuse/flyway:latest
    container_name: flyway_migration
    volumes:
      - ./flyway_scripts/src/main/resources/db/migration:/flyway/sql
    command: dockerize wait jdbc:mysql://my_sql_db:3306 -url=jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false -user=root -password=password migrate
    depends_on:
      - my_sql_db
I see following error:
flyway_migration  | ERROR: Invalid argument: dockerize
UPDATE_1
wait-for-it.sh content:
#!/usr/bin/env bash
#   Use this script to test if a given TCP host/port are available
cmdname=$(basename $0)
echoerr() { if [[ $QUIET -ne 1 ]]; then echo ""$@"" 1&gt;&amp;2; fi }
usage()
{
    cat &lt;&lt; USAGE &gt;&amp;2
Usage:
    $cmdname host:port [-s] [-t timeout] [-- command args]
    -h HOST | --host=HOST       Host or IP under test
    -p PORT | --port=PORT       TCP port under test
                                Alternatively, you specify the host and port as host:port
    -s | --strict               Only execute subcommand if the test succeeds
    -q | --quiet                Don't output any status messages
    -t TIMEOUT | --timeout=TIMEOUT
                                Timeout in seconds, zero for no timeout
    -- COMMAND ARGS             Execute command with args after the test finishes
USAGE
    exit 1
}
wait_for()
{
    if [[ $TIMEOUT -gt 0 ]]; then
        echoerr ""$cmdname: waiting $TIMEOUT seconds for $HOST:$PORT""
    else
        echoerr ""$cmdname: waiting for $HOST:$PORT without a timeout""
    fi
    start_ts=$(date +%s)
    while :
    do
        if [[ $ISBUSY -eq 1 ]]; then
            nc -z $HOST $PORT
            result=$?
        else
            (echo &gt; /dev/tcp/$HOST/$PORT) &gt;/dev/null 2&gt;&amp;1
            result=$?
        fi
        if [[ $result -eq 0 ]]; then
            end_ts=$(date +%s)
            echoerr ""$cmdname: $HOST:$PORT is available after $((end_ts - start_ts)) seconds""
            break
        fi
        sleep 1
    done
    return $result
}
wait_for_wrapper()
{
    # In order to support SIGINT during timeout: http://unix.stackexchange.com/a/57692
    if [[ $QUIET -eq 1 ]]; then
        timeout $BUSYTIMEFLAG $TIMEOUT $0 --quiet --child --host=$HOST --port=$PORT --timeout=$TIMEOUT &amp;
    else
        timeout $BUSYTIMEFLAG $TIMEOUT $0 --child --host=$HOST --port=$PORT --timeout=$TIMEOUT &amp;
    fi
    PID=$!
    trap ""kill -INT -$PID"" INT
    wait $PID
    RESULT=$?
    if [[ $RESULT -ne 0 ]]; then
        echoerr ""$cmdname: timeout occurred after waiting $TIMEOUT seconds for $HOST:$PORT""
    fi
    return $RESULT
}
# process arguments
while [[ $# -gt 0 ]]
do
    case ""$1"" in
        *:* )
        hostport=(${1//:/ })
        HOST=${hostport[0]}
        PORT=${hostport[1]}
        shift 1
        ;;
        --child)
        CHILD=1
        shift 1
        ;;
        -q | --quiet)
        QUIET=1
        shift 1
        ;;
        -s | --strict)
        STRICT=1
        shift 1
        ;;
        -h)
        HOST=""$2""
        if [[ $HOST == """" ]]; then break; fi
        shift 2
        ;;
        --host=*)
        HOST=""${1#*=}""
        shift 1
        ;;
        -p)
        PORT=""$2""
        if [[ $PORT == """" ]]; then break; fi
        shift 2
        ;;
        --port=*)
        PORT=""${1#*=}""
        shift 1
        ;;
        -t)
        TIMEOUT=""$2""
        if [[ $TIMEOUT == """" ]]; then break; fi
        shift 2
        ;;
        --timeout=*)
        TIMEOUT=""${1#*=}""
        shift 1
        ;;
        --)
        shift
        CLI=(""$@"")
        break
        ;;
        --help)
        usage
        ;;
        *)
        echoerr ""Unknown argument: $1""
        usage
        ;;
    esac
done
if [[ ""$HOST"" == """" || ""$PORT"" == """" ]]; then
    echoerr ""Error: you need to provide a host and port to test.""
    usage
fi
TIMEOUT=${TIMEOUT:-15}
STRICT=${STRICT:-0}
CHILD=${CHILD:-0}
QUIET=${QUIET:-0}
# check to see if timeout is from busybox?
# check to see if timeout is from busybox?
TIMEOUT_PATH=$(realpath $(which timeout))
if [[ $TIMEOUT_PATH =~ ""busybox"" ]]; then
        ISBUSY=1
        BUSYTIMEFLAG=""-t""
else
        ISBUSY=0
        BUSYTIMEFLAG=""""
fi
if [[ $CHILD -gt 0 ]]; then
    wait_for
    RESULT=$?
    exit $RESULT
else
    if [[ $TIMEOUT -gt 0 ]]; then
        wait_for_wrapper
        RESULT=$?
    else
        wait_for
        RESULT=$?
    fi
fi
if [[ $CLI != """" ]]; then
    if [[ $RESULT -ne 0 &amp;&amp; $STRICT -eq 1 ]]; then
        echoerr ""$cmdname: strict mode, refusing to execute subprocess""
        exit $RESULT
    fi
    exec ""${CLI[@]}""
else
    exit $RESULT
fi
P.S.5
Also I tried this:
version: '3'
services:
  my_sql_db:
    image: percona:latest
    container_name: my_sql_db
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: abhs
    ports:
    - ""3306:3306""
    healthcheck:
      test: [""CMD"", ""mysqladmin"" ,""ping"", ""-h"", ""localhost""]
      timeout: 20s
      retries: 10
  migration:
    image: boxfuse/flyway:latest
    container_name: flyway_migration
    volumes:
     - ./flyway_scripts/src/main/resources/db/migration:/flyway/sql
    entrypoint: [""wait-for-it.sh"", ""mysql:3306"", ""--"", ""docker-entrypoint.sh""]      
    command: -url=jdbc:mysql://my_sql_db:3306/abhs?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false -user=root -password=password migrate
    depends_on:
      - my_sql_db
It leads to error:
Creating flyway_migration ... error
ERROR: for flyway_migration  Cannot start service migration: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""wait-for-it.sh\"": executable file not found in $PATH"": unknown
ERROR: for migration  Cannot start service migration: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""wait-for-it.sh\"": executable file not found in $PATH"": unknown
Encountered errors while bringing up the project.
",<java><mysql><docker><flyway><depends>,15862,4,344,36992,119,374,727,63,20204,0,3581,3,20,2018-08-24 8:59,2018-08-27 12:12,,3,,Advanced,32
60014874,How to use TypeScript with Sequelize,"I already have my server application written in Node, PostgreSQL, Sequelize using Fastify.
Now I would like to use TypeScript. Can anyone tell me how to begin rewriting my Server application using TypeScript.
",<node.js><postgresql><typescript><sequelize.js><fastify>,209,0,0,499,1,3,13,48,43713,0,61,3,20,2020-02-01 7:20,2020-02-02 2:08,2020-02-24 3:31,1,23,Intermediate,20
59563423,"Cannot drop a database in Azure Data Studio, because it's currently in use","I cannot drop custom databases in Azure Data Studio, because they are currently in use.
I've been looking for various ways to close the zap database, but I cannot find any in the UI. 
Only by restarting Azure Data Studio, is the zap database in an ""Auto Closed"" state, which lets me drop it using:
drop database zap;
How do I close a connection to a database without restarting Azure Data Studio?
Open:
Auto-closed:
",<sql><sql-server><windows>,416,2,3,11770,22,94,194,48,23330,0,364,2,20,2020-01-02 12:33,2020-01-02 12:35,2020-01-02 12:36,0,0,Intermediate,15
52719378,Failed to find valid data directory. MySQL generic binary installion,"Im going to install mysql to linux server. But I dont have root access to that server. So I created two folders called mysql and mysqldata. mysql folder holds binary files. mysqldata folder holds data and the logs.
my.cnf
[mysqld]
user                    = mysql
port                    = 3306
bind-address            = localhost
basedir                 = /home/nwn/mysql/mysql-8.0
socket                  = /home/nwn/mysqldata/instA/socket/mysql.sock
datadir                 = /home/nwn/mysqldata/instA/data
tmpdir                  = /home/nwn/mysqldata/instA/tmp
secure_file_priv        = /home/nwn/mysqldata/instA/mysql-files
max_connections         = 150
# Logging
log-bin                 = /home/nwn/mysqldata/instA/logs/instA-binlog
log-error               = /home/nwn/mysqldata/instA/logs/instA-errorlog.err
slow_query_log          = 1
slow_query_log_file     = /home/nwn/mysqldata/instA/logs/instA-slowquery.log
long_query_time         = 0.5
# InnoDB
innodb_data_home_dir    = /home/nwn/mysqldata/instA/innodb/data
innodb_data_file_path   = ibdata1:50M;ibdata2:12M:autoextend:max:500M
innodb_log_group_home_dir = /home/nwn/mysqldata/instA/innodb/log
innodb_buffer_pool_size = 32M
# MyISAM
key_buffer_size         = 16M
server_id                = 1
I did all the other configurations.
when I run following command 
mysql-8.0]$ bin/mysqld --defaults-file=~/mysqldata/instA/my.cnf --initialize-insercure
I have following logs in the error_log
 cat ~/mysqldata/instA/logs/instA-errorlog.err
2018-10-09T10:39:51.127424Z 0 [Warning] [MY-010139] [Server] Changed limits: max_open_files: 1024 (requested 8160)
2018-10-09T10:39:51.127523Z 0 [Warning] [MY-010142] [Server] Changed limits: table_open_cache: 432 (requested 4000)
2018-10-09T10:39:51.383986Z 0 [Warning] [MY-010101] [Server] Insecure configuration for --secure-file-priv: Location is accessible to all OS users. Consider choosing a different directory.
2018-10-09T10:39:51.384043Z 0 [System] [MY-010116] [Server] /home/nwn/mysql/mysql-8.0/bin/mysqld (mysqld 8.0.12) starting as process 32654
2018-10-09T10:39:51.386625Z 0 [Warning] [MY-010122] [Server] One can only use the --user switch if running as root
2018-10-09T10:39:51.394675Z 1 [ERROR] [MY-011011] [Server] Failed to find valid data directory.
2018-10-09T10:39:51.394817Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
2018-10-09T10:39:51.394831Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-10-09T10:39:51.395363Z 0 [System] [MY-010910] [Server] /home/nwn/mysql/mysql-8.0/bin/mysqld: Shutdown complete (mysqld 8.0.12)  MySQL Community Server - GPL.
",<mysql><mysqladministrator>,2594,0,39,267,1,3,10,72,65137,0,1,6,20,2018-10-09 10:57,2020-05-25 13:45,,594,,Basic,14
48370045,Android Room Persistence Library - How to find entities with ids contained in list of ids?,"I am trying to do the following query in my DAO.
   @Query(""SELECT * FROM objects WHERE obj_id IN :ids"")
   List&lt;Object&gt; queryObjects(List&lt;String&gt; ids);
It gives me this compile-time error:
Error: no viable alternative at input 'SELECT * FROM objects WHERE obj_id IN :ids'
Both List&lt;String&gt; ids as well as String... ids and Sring[] ids don't work. However, since I don't know how many ids I will have in compile-time and therefore, I need a list/array and not varargs. 
How can I make this SQL query work?
",<java><android><sql><sqlite><android-room>,524,0,6,5875,6,34,62,42,10642,0,70,2,20,2018-01-21 18:07,2018-01-21 18:23,2018-01-21 18:23,0,0,Basic,10
51062920,pip install mysqlclient : Microsoft Visual C++ 14.0 is required,"i'm tryng to import mysqlclient library for python with pip, when i use the command
pip install mysqlclient it return an error:
Collecting mysqlclient
Using cached     https://files.pythonhosted.org/packages/ec/fd/83329b9d3e14f7344d1cb31f128e6dbba70c5975c9e57896815dbb1988ad/mysqlclient-1.3.13.tar.gz
Installing collected packages: mysqlclient
Running setup.py install for mysqlclient ... error
Complete output from command c:\users\astrina\appdata\local\programs\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\astrina\\AppData\\Local\\Temp\\pip-install-40l_x_f4\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\astrina\AppData\Local\Temp\pip-record-va173t5v\install-record.txt --single-version-externally-managed --compile:
c:\users\astrina\appdata\local\programs\python\python36\lib\distutils\dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
  warnings.warn(msg)
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.6
copying _mysql_exceptions.py -&gt; build\lib.win-amd64-3.6
creating build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\compat.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\connections.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\converters.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\cursors.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\release.py -&gt; build\lib.win-amd64-3.6\MySQLdb
copying MySQLdb\times.py -&gt; build\lib.win-amd64-3.6\MySQLdb
creating build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\__init__.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\CR.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\ER.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\FLAG.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win-amd64-3.6\MySQLdb\constants
running build_ext
building '_mysql' extension
error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools
----------------------------------------
Command ""c:\users\astrina\appdata\local\programs\python\python36\python.exe -u -c ""import setuptools, 
tokenize;__file__='C:\\Users\\astrina\\AppData\\Local\\Temp\\pip-install- 
40l_x_f4\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open) 
(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, 
__file__, 'exec'))"" install --record C:\Users\astrina\AppData\Local\Temp\pip- 
record-va173t5v\install-record.txt --single-version-externally-managed -- 
compile"" failed with error code 1 in C:\Users\astrina\AppData\Local\Temp\pip- 
install-40l_x_f4\mysqlclient\
I've already installed Microsoft Build Tools 2015 but the problem persist
",<python><mysql><pip><build-tools>,3259,2,43,420,2,5,18,37,85328,0,42,13,20,2018-06-27 12:33,2018-08-09 7:00,2018-08-10 6:19,43,44,Intermediate,24
50151476,"""HostName not verified error message"" on SSL connection in postgresql","I created server.crt, server.key and root.crt files on Centos 7 and put the same onto the C:\Users\xxxx\AppData\Roaming\postgresql folder in windows as i am running the postgresql server on windows. Now on running my applications using SSL, i am getting the error as 
  ""The host name could not be verified""
Any help please.
",<postgresql><ssl><ssl-certificate>,325,0,4,355,1,3,15,64,16822,0,4,3,20,2018-05-03 9:26,2018-05-03 10:38,,0,,Advanced,45
56416437,Confusion about URI path to configure SQLite database,"Hi I am building a web application using Flask and Sqlite3. I had issues with connecting the database for a while and it did not work when I wrote this:
#version 1
app.config['SQLALCHEMY_DATABASE_URI'] =
'sqlite:////C:/Users/Giang/PyCharmProjects/FlaskWebBlog/FlaskWebBlog/site.db'
Python gave me operational error: can not open database because I wrote with 4 slashes after the colon. After reading sqlalchemy documentation and doing so many trials, I found out this worked:
#with 3 slashes, version 2
app.config['SQLALCHEMY_DATABASE_URI'] = 
 'sqlite:///C:/Users/Giang/PyCharmProjects/FlaskWebBlog/FlaskWebBlog/site.db'
or this with 4 slashes but no C:
#version 3
app.config['SQLALCHEMY_DATABASE_URI'] = 
'sqlite:////Users/Giang/PyCharmProjects/FlaskWebBlog/FlaskWebBlog/site.db'
I am confused because based on the documentation of connecting strings: The file specification for the SQLite database is taken as the “database” portion of the URL. Note that the format of a SQLAlchemy url is:
driver://user:pass@host/database
This means that the actual filename to be used starts with the characters to the right of the third slash. So connecting to a relative filepath looks like:
# relative path
e = create_engine('sqlite:///path/to/database.db')
An absolute path, which is denoted by starting with a slash, means you need four slashes:
# absolute path
e = create_engine('sqlite:////path/to/database.db')
SO according to this, if I use absolute path, I need 4 slashes, but when I did that with version 1, python gave me errors. And when I used 3 slashes for absolute path in version 2, it worked.
So I am really confused. Can anyone explain for me why ? I would really appreciate it. Thank you
",<python><sqlite><uri><relative-path><absolute-path>,1696,0,16,411,1,5,10,77,36603,0,13,2,20,2019-06-02 15:30,2019-06-02 16:53,,0,,Basic,4
51972843,Polymorphic entities in Room,"There are 3 entities in my Room DB:
Album, PhotosMediaItem and VideosMediaItem.
VideosMediaItem and PhotosMediaItem inherit from MediaItem.
MediaItem is not an entity in the DB, it's just an abstract base class.
I would like to create a query that returns all the photos and videos media items in a specific album with descending order based on their creation date.
So the query will create a list of MediaItems but with the derived types. (PhotoMediaItem or VideoMediaItem) in a polymorphic way.
Here's what I've tried:
    @Query(""SELECT * FROM PhotosMediaItem WHERE PhotosMediaItem = :albumId "" +
        ""UNION SELECT * FROM VideosMediaItem WHERE VideosMediaItem = :albumId"" +
        "" ORDER by CreationDate DESC"")
    List&lt;MediaItem&gt; getAllMediaInAlbum(int albumId);
This won't work obviously, because it tries to initiate MediaItem object, and it is not my intention. I want this query to initiate the derived class, PhotoMediaItem or VideoMediaItem
Here's how my query looked like before the migration to Room, using the regular SQLiteHelper, and it worked just fine:
public ArrayList&lt;MediaItem&gt; getMediaListByAlbumId(int palbumId)
{
    Cursor cursor = null;
    try{
        ArrayList&lt;MediaItem&gt; mediaList = new ArrayList&lt;&gt;();
        String selectQuery = ""SELECT ""+ mPhotoId +"",""+ mPhotoCreationDate +"", 0 AS mediaType, '' FROM ""+ mPhotosTableName + "" WHERE "" + this.mPhotoAlbumId + ""=""+palbumId +
                "" UNION "" +
                ""SELECT ""+ mVideoId +"",""+ mVideoCreationDate + "" ,1 AS mediaType, "" + mVideoLength + "" FROM "" + mVideosTableName + "" WHERE "" + this.mVideoAlbumId +""=""+palbumId +
                "" ORDER BY CreationDate DESC"";
        cursor = mDB.rawQuery(selectQuery, null);
        // looping through all rows and adding to list
        if (cursor.moveToFirst()){
            do {
                // MediaHolder consists of the media ID and its type
                int mediaType = cursor.getInt(2);
                MediaItem mediaItem = null;
                if (mediaType == 0) {
                    mediaItem = new PhotoMediaItem(cursor.getInt(0), null, palbumId);
                } else if (mediaType == 1) {
                    mediaItem = new VideoMediaItem(cursor.getInt(0), null, palbumId, cursor.getLong(3));
                }
                mediaList.add(mediaItem);
            }
            while (cursor.moveToNext());
        }
        return mediaList;
    }
    finally  {
        if(cursor != null){
            cursor.close();
        }
    }
}
How can I achieve the same effect using Room then?
",<android><sqlite><polymorphism><union><android-room>,2577,0,49,3190,13,53,85,52,4819,0,215,2,20,2018-08-22 18:13,2018-08-31 10:42,2018-08-31 10:42,9,9,Basic,10
49610908,Exporting a PostgreSQL query to a csv file using Python,"I need to export some rows from a table in a PostgreSQL database to a .csv file using a Python script:
#!/usr/bin/python
# -*- coding: utf-8 -*-
import sys, psycopg2
...
    conn = psycopg2.connect(""dbname=dbname user=user password=password"")
    cur = conn.cursor()
    sql = ""\copy (SELECT * FROM table WHERE month=6) TO '/mnt/results/month/table.csv' WITH CSV DELIMITER ';';""
    cur.execute(sql)
    cur.close()
...
But when I run the script I get this:
Syntax error at or near «\»
LINE 1: \copy (SELECT * FROM TABLE WHERE month=6) TO '...
Does anyone know what can be wrong or give me a tip about?
",<python><sql><postgresql><export-to-csv><psycopg2>,603,0,17,435,1,3,13,43,26778,0,175,4,20,2018-04-02 12:00,2018-04-02 13:43,2018-04-02 13:51,0,0,Basic,10
51538158,PostgreSQL update all value to upper case for one column,"I have a table : Customer and column = [name,surname,language]
I want to update all language columns value to upper case how can I do it?
I have seen upper() method but it used on select operations. I need to update.
",<sql><postgresql>,217,0,0,1103,4,14,27,46,14405,0,19,1,20,2018-07-26 11:50,2018-07-26 11:59,2018-07-26 11:59,0,0,Basic,2
59553246,What's the recommended way to do database migrations with Ktor + Exposed (Kotlin)?,"The Ktor or Exposed frameworks do not have any built-in support for database migrations. What's the recommended way to do this?
",<sql><database><kotlin><migration>,128,0,0,806,2,11,29,71,11334,0,14,4,20,2020-01-01 14:14,2020-01-23 21:24,,22,,Basic,3
62955635,How to restore a PostgreSQL database from dump file in dbeaver?,"In our company we have a dump of PostgreSQL database - file db.sql. It weighs 8 Gigabyte. How to restore this database in DBeaver? And we don't have another databases in DBeaver 7.0.5.
I have digged all Internet and haven't found anything how to do this without another database/
",<postgresql><dump><dbeaver>,280,0,1,381,1,2,12,37,49790,,25,2,20,2020-07-17 14:02,2020-07-17 14:57,2020-07-17 14:57,0,0,Basic,10
54973536,FOR JSON PATH results in SSMS truncated to 2033 characters,"I'm concatenating strings together using ""for JSON path('')"".
I have set the Tools->Options->SQL Server->Results to Grid options to max.
I have set the Tools->Options->SQL Server->Results to Text options to max.
Executing the query in Grid mode and copying the one row/one column results, I see the return value is limited to 2033 characters.
How can I ensure the returned value isn't truncated?
",<sql-server><t-sql><ssms>,396,0,0,371,1,3,9,48,14922,0,8,10,20,2019-03-03 20:42,2019-03-03 20:57,2019-03-03 20:57,0,0,Basic,10
48866673,malformed array literal - PostgreSQL,"I want to copy an array from jsonb field to a PostgreSQL array column:
CREATE TABLE survey_results (
    id integer NOT NULL,
    areas text[],  
    raw jsonb DEFAULT '{}'::jsonb
);
INSERT INTO survey_results (id, raw)
    VALUES (1, '{""areas"": [""test"", ""test2""]}');
UPDATE survey_results SET areas = CAST(raw#&gt;&gt;'{areas}' AS text[]);
This returns me?
ERROR: malformed array literal: ""[""test"", ""test2""]"" Detail: ""["" must introduce explicitly-specified array dimensions.
How can I fix that?
http://sqlfiddle.com/#!17/d8122/2
",<sql><postgresql>,530,2,11,7462,15,68,135,69,98953,0,499,2,20,2018-02-19 12:52,2018-02-19 13:03,2018-02-19 13:03,0,0,Basic,2
49596061,TypeORM updating entity/table,"This is my User entity:
@PrimaryGeneratedColumn()
userId: number;
@Column({type:""varchar"", length:""300""})
userName: string;
@OneToOne(type =&gt; UserProfile, {cascadeAll:true})
@JoinColumn()
userProfile: UserProfile;
@OneToOne(type =&gt; UserCredential, {cascadeAll:true, eager:true})
@JoinColumn()
userCredential: UserCredential;
@OneToOne(type =&gt; BusinessUnit, {cascadeAll:true})
@JoinColumn()
businessUnit: BusinessUnit;
@ManyToMany(type =&gt; ProductCategory)
@JoinTable()
productCategory: ProductCategory[];
and this is my new data which i want to update:
User {
  userName: 'qweret@gmail.com',
  userProfile: 
   UserProfile {
     firstName: 'dcds',
     lastName: 'Faiz',
     mobile: '42423423',
     addressLine1: 'Delhi',
     addressLine2: 'Delhi',
     city: '-',
     country: '-',
     zipCode: '234243',
     homeTelephone: '-',
     dayOfBirth: 0,
     monthOfBirth: 0,
     yearOfBirth: 0 },
  userCredential: UserCredential { credential: 'abcd@123' } }
i'm searching user by its userId.
return await getManager()
               .createQueryBuilder(User, ""user"")
               .where(""user.userId = :id"", {id})
               .getOne();
the above query gives me result:
User {
  createdDate: 2018-03-29T06:45:16.322Z,
  updatedDate: 2018-04-01T06:28:24.171Z,
  userId: 1,
  userName: 'qweret@gmail.com' }
i want to update my user table and its related tables
manager.save(user)
will insert a new row in the table instead of updating the existing the row. Is there a way to update the whole user table and its related table without updating every column manually?
i don't want to perform this task like this:
let user = await userRepositiory.findOneById(1);
user.userName = ""Me, my friends and polar bears"";
await userRepository.save(user);
let userProfile = await userProfileRepository.findOneById(1);
 userProfile.firstName = """";
 userProfile.lastName = """";
    ....// etc
 await userRepository.save(userProfile);
 // and so on update other tables.
",<mysql><node.js><database><typeorm>,1972,0,59,211,1,2,4,39,48282,0,0,3,20,2018-04-01 8:02,2019-09-03 15:48,,520,,Basic,11
56301656,Disable Lazy Loading in Entity Framework Core,"There are plenty of posts about how to disable lazy loading in Entity Framework, but the same techniques don't work in EF Core. I found the LazyLoadingEnabled property in the change tracker, but this doesn't seem to work at all.
Everything points to this in EF:
this.Configuration.LazyLoadingEnabled = false;
But, the Configuration property is missing in EF Core.
Here is an example of what I am talking about:
public class TestContext : DbContext
{
    public DbSet&lt;Person&gt; People { get; set; }
    public DbSet&lt;Address&gt; Addresses { get; set; }
    public TestContext()
    {
        this.ChangeTracker.LazyLoadingEnabled = false;
    }
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {            
        var connection = new SqliteConnection($""Data Source=Test.db"");
        connection.Open();
        var command = connection.CreateCommand();
        command.CommandText = $""PRAGMA foreign_keys = ON;"";
        command.ExecuteNonQuery();
        optionsBuilder.UseSqlite(connection);
        optionsBuilder.UseLazyLoadingProxies(false);
        base.OnConfiguring(optionsBuilder);
    }
    private static void Main(string[] args)
    {
        Console.WriteLine(""Hello World!"");
        using (var context = new TestContext())
        {
            context.Database.EnsureCreated();
            var personKey = Guid.NewGuid().ToString();
            var addressKey = Guid.NewGuid().ToString();
            context.People.Add(new Entities.Person { PersonKey = personKey, BillingAddress = new Entities.Address { AddressKey = addressKey } });
            context.SaveChanges();
        }
        using (var context = new TestContext())
        {
            var people = context.People.ToList();
            foreach (var person in people)
            {
                if (person.BillingAddress == null) throw new Exception(""The billing address wasn't loaded"");
            }
        }
    }
}
The above throws an exception because BillingAddress is not getting loaded even though I turned lazy loading off.
I suspect this is a bug, but please tell me it isn't. I logged it here: https://github.com/aspnet/EntityFrameworkCore/issues/15802
You can download the sample here:
https://www.dropbox.com/s/mimvgvcmibr7em2/EFSQLiteTest.7z?dl=0
",<c#><sqlite><.net-core><entity-framework-core>,2289,4,56,7010,5,51,106,51,31973,0,263,1,20,2019-05-25 4:17,2019-11-27 15:12,,186,,Basic,11
55701029,How to insert value to identity column in PostgreSQL 11.1,"I would like to insert my own value to identity column.
Table Schema:
CREATE TABLE public.userdetail (
    userdetailid int4 NOT NULL GENERATED ALWAYS AS IDENTITY,
    username varchar(30) NOT NULL,
    ""password"" varchar(1000) NOT NULL,
    CONSTRAINT pk_userdetail PRIMARY KEY (userdetailid)
);
Insert Query:
INSERT INTO UserDetail (UserDetailId,UserName, Password) 
  VALUES(1,'admin', 'password');
Here insert query throwing below error: 
  cannot insert into column ""userdetailid""
Is there any command exists to force insert to identity column like MS SQL :
 SET IDENTITY_INSERT UserDetail ON
Let me know if you have any solution.
",<database><postgresql>,636,0,9,1089,3,12,24,68,33643,0,20,1,20,2019-04-16 5:20,2019-04-16 5:27,2019-04-16 5:27,0,0,Basic,10
50983177,How to connect to PostgreSQL using docker-compose?,"Want to use docker-compose to run api application and postgresql database together. 
docker-compose file:
version: '3'
volumes:
  database_data:
    driver: local
services:
  db:
    image: postgres:latest
    volumes:
      - database_data:/var/lib/postgresql/data
  api:
    build: ./api
    expose:
      - 8080
    ports:
      - 8080:8080
    volumes:
      - ./api:/usr/src/app/
    links:
      - db
    environment:
      - PGHOST=db
      - PGDATABASE=postgres
      - PGUSER=postgres
Api main.go file:
func main() {
    db, err = gorm.Open(""postgres"", ""host=db port=5432 user=postgres dbname=postgres"")
  // ...
}
When run the services, got message from log:
api_1     | [GIN] 2018/06/22 - 07:31:10 | 404 |      1.4404ms |      172.20.0.1 | GET      /posts
api_1     |
api_1     | (sql: database is closed)
api_1     | [2018-06-22 07:31:10]
api_1     |
api_1     | (sql: database is closed)
api_1     | [2018-06-22 07:31:10]
api_1     | [GIN] 2018/06/22 - 07:32:14 | 403 |        15.6µs |      172.20.0.1 | GET      /posts
db_1      | 2018-06-22 07:34:27.296 UTC [81] FATAL:  role ""root"" does not exist
db_1      | 2018-06-22 07:34:36.897 UTC [90] FATAL:  role ""root"" does not exist
Does this way not good? host=db in the connection string? Since db is the docker compose service name.
Add
It can work:
https://docs.docker.com/samples/library/postgres/#-or-via-psql
",<postgresql><docker><go><service><docker-compose>,1376,2,45,633,3,13,26,74,52641,0,12,1,20,2018-06-22 7:50,2018-09-27 19:19,,97,,Basic,10
50019457,Why does Spark Planner prefer sort merge join over shuffled hash join?,"Why does Spark Planner in Spark 2.3 prefer a sort merge join over a shuffled hash join? In other words, why is spark.sql.join.preferSortMergeJoin configuration property internal and turned on by default? What's wrong with a shuffled hash join? Is this specific to Spark that it does computations in distributed fashion or something else more inherent in the join algorithm?
You can find the property used in the JoinSelection execution planning strategy here and here that looks like:
case ... if !conf.preferSortMergeJoin &amp;&amp; ... =&gt;
  Seq(joins.ShuffledHashJoinExec(...))
",<apache-spark><join><apache-spark-sql>,583,3,4,73211,27,243,423,67,10622,0,5053,1,20,2018-04-25 10:04,2018-04-25 12:03,2018-04-25 12:03,0,0,Intermediate,23
48522640,Failure to connect to Docker Postgresql instance from Python,"I am using Docker to ""containerize"" a PostgreSQL deployment. I can spin up the container and connect to PostgreSQL via the command line as shown below:
minime2@CEBERUS:~/Projects/skunkworks$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
dc176901052a        df:pg               ""docker-entrypoint...""   About an hour ago   Up About an hour    5432/tcp            vigilant_agnesi
minime2@CEBERUS:~/Projects/skunkworks$ CONTAINER_ID=dc176901052a
minime2@CEBERUS:~/Projects/skunkworks$ IP=$(docker inspect -f '{{.NetworkSettings.Networks.bridge.IPAddress}}' $CONTAINER_ID)
minime2@CEBERUS:~/Projects/skunkworks$ echo $IP
172.17.0.2
minime2@CEBERUS:~/Projects/skunkworks$ docker exec -it vigilant_agnesi psql -U postgres -W cookiebox
Passwod for user postgres:
psql (9.6.5)
Type ""help"" for help
cookiebox#
Now attempting connection with Python:
Python 3.5.2 (default, Sep 14 2017, 22:51:06) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import psycopg2
&gt;&gt;&gt; conn = psycopg2.connect(""dbname='cookiebox' user='postgres' host='172.17.0.2' password='nunyabiznes'"")                                     Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/minime2/Projects/skunkworks/archivers/env/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host ""172.17.0.2"" and accepting
        TCP/IP connections on port 5432?
&gt;&gt;&gt; 
Can anyone explain why I can't connect to PostgreSQL using Python - even though I'm using the same arguments/parameters that enable a successful connection at the command line (using docker exec?).
[[Additional Info]]
As suggested by @Itvhillo, I tried to use a desktop application to connect to the PG service. I run the docker service using the following command:
docker run -i -p 5432:5432 --name $CONTAINER_NAME $DOCKER_IMAGE
I am using Db Visualizer to connect to the database, and I have set the hostname to 'localhost'. I can successfully ping the port, but still get an error message when I try to connect to the database (possible permissions related error):
An error occurred while establishing the connection:
Long Message:
The connection attempt failed.
Details:
   Type: org.postgresql.util.PSQLException
   SQL State: 08001
Incidentally, this is the tail end of the output for the PG service instance:
PostgreSQL init process complete; ready for start up.
LOG:  could not bind IPv6 socket: Cannot assign requested address
HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
LOG:  database system was shut down at 2018-01-30 16:21:59 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
[[Additional Info2]]
Here is the tail end of my Dockerfile:
# modified target locations (checked by login onto Docker container)
# show hba_file;
# show config_file;
#################################################################################
# From here: https://docs.docker.com/engine/examples/postgresql_service/
# Adjust PostgreSQL configuration so that remote connections to the
# database are possible.
RUN echo ""host all  all    0.0.0.0/0  md5"" &gt;&gt; /var/lib/postgresql/data/pg_hba.conf
# And add ``listen_addresses`` to ``/var/lib/postgresql/data/postgresql.conf``
RUN echo ""listen_addresses='*'"" &gt;&gt; /var/lib/postgresql/data/postgresql.conf
#################################################################################
EXPOSE 5432
# Add VOLUMEs to allow backup of config, logs and databases
VOLUME  [""/etc/postgresql"", ""/var/log/postgresql"", ""/var/lib/postgresql"", ""/usr/lib/postgresql/""]
",<python><postgresql><docker><psycopg2>,4008,2,67,65877,82,222,348,46,13011,0,948,5,20,2018-01-30 13:29,2018-02-01 14:42,2018-02-06 17:45,2,7,Advanced,38
51228905,Rails error installing mysql2 (mysql2-0.3.20),"I am trying to get a rails project up and running on my local machine.  When I do bundle install 
Fetching mysql2 0.3.20
Installing mysql2 0.3.20 with native extensions
Gem::Ext::BuildError: ERROR: Failed to build gem native extension.
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-
0.3.20/ext/mysql2
/Users/mac/.rvm/rubies/ruby-2.3.1/bin/ruby -r ./siteconf20180707-33936-1toblx7.rb extconf.rb
checking for ruby/thread.h... yes
checking for rb_thread_call_without_gvl() in ruby/thread.h... yes
checking for rb_thread_blocking_region()... no
checking for rb_wait_for_single_fd()... yes
checking for rb_hash_dup()... yes
checking for rb_intern3()... yes
-----
Using mysql_config at /usr/local/bin/mysql_config
-----
checking for mysql.h... yes
checking for errmsg.h... yes
checking for mysqld_error.h... yes
-----
Setting rpath to /usr/local/Cellar/mysql/8.0.11/lib
-----
creating Makefile
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:367:33: warning: implicit conversion loses integer precision: 'long' to 'unsigned int' [-Wshorten-64-to-32]
        elapsed_time = end_time - start_time;
                     ~ ~~~~~~~~~^~~~~~~~~~~~
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:775:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:806:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:807:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:810:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:811:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:812:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:843:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1165:56: warning: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'unsigned int' [-Wshorten-64-to-32]
  mysql2rb = mysql2_mysql_enc_name_to_rb(charset_name, charset_name_len);
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~               ^~~~~~~~~~~~~~~~
client.c:1198:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
2 warnings and 10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20 for inspection.
Results logged to /Users/mac/.rvm/gems/ruby-2.3.1/extensions/x86_64-darwin-17/2.3.0/mysql2-0.3.20/gem_make.out
An error occurred while installing mysql2 (0.3.20), and Bundler cannot continue.
Make sure that `gem install mysql2 -v '0.3.20' --source 'https://rubygems.org/'` succeeds before bundling.
In Gemfile:
  mysql2
I then follow the instructions and 
gem install mysql2 -v '0.3.20' --source 'https://rubygems.org/'
Building native extensions.  This could take a while...
ERROR:  Error installing mysql2:
    ERROR: Failed to build gem native extension.
    current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
/Users/mac/.rvm/rubies/ruby-2.3.1/bin/ruby -r ./siteconf20180707-34132-p3fohi.rb extconf.rb
checking for ruby/thread.h... yes
checking for rb_thread_call_without_gvl() in ruby/thread.h... yes
checking for rb_thread_blocking_region()... no
checking for rb_wait_for_single_fd()... yes
checking for rb_hash_dup()... yes
checking for rb_intern3()... yes
-----
Using mysql_config at /usr/local/bin/mysql_config
-----
checking for mysql.h... yes
checking for errmsg.h... yes
checking for mysqld_error.h... yes
-----
Setting rpath to /usr/local/Cellar/mysql/8.0.11/lib
-----
creating Makefile
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:367:33: warning: implicit conversion loses integer precision: 'long' to 'unsigned int' [-Wshorten-64-to-32]
        elapsed_time = end_time - start_time;
                     ~ ~~~~~~~~~^~~~~~~~~~~~
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:775:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:806:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:807:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:810:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:811:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:812:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:843:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1165:56: warning: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'unsigned int' [-Wshorten-64-to-32]
  mysql2rb = mysql2_mysql_enc_name_to_rb(charset_name, charset_name_len);
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~               ^~~~~~~~~~~~~~~~
client.c:1198:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
2 warnings and 10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20 for inspection.
Results logged to /Users/mac/.rvm/gems/ruby-2.3.1/extensions/x86_64-darwin-17/2.3.0/mysql2-0.3.20/gem_make.out
I have tried the solution mentioned here  
Ruby gem mysql2 install failing
brew install mysql
Warning: mysql 8.0.11 is already installed and up-to-date
To reinstall 8.0.11, run `brew reinstall mysql`
and then 
gem install mysql2
Building native extensions.  This could take a while...
Successfully installed mysql2-0.5.2
Parsing documentation for mysql2-0.5.2
Done installing documentation for mysql2 after 0 seconds
1 gem installed
but when I do bundle install again I still get all of these errors
strangely if I run
 brew reinstall mysql
==&gt; Reinstalling mysql 
==&gt; Downloading https://homebrew.bintray.com/bottles/mysql-8.0.11.high_sierra.bottle.tar.gz
Already downloaded: /Users/mac/Library/Caches/Homebrew/mysql-8.0.11.high_sierra.bottle.tar.gz
==&gt; Pouring mysql-8.0.11.high_sierra.bottle.tar.gz
==&gt; /usr/local/Cellar/mysql/8.0.11/bin/mysqld --initialize-insecure --user=mac --basedir=/usr/local/Cellar/mysql/8.0.11 --datadir=/usr/local/var/mysql --tmpdir=/tmp
Last 15 lines from /Users/mac/Library/Logs/Homebrew/mysql/post_install.01.mysqld:
2018-07-07 23:56:01 -0500
/usr/local/Cellar/mysql/8.0.11/bin/mysqld
--initialize-insecure
--user=mac
--basedir=/usr/local/Cellar/mysql/8.0.11
--datadir=/usr/local/var/mysql
--tmpdir=/tmp
2018-07-08T04:56:01.743929Z 0 [System] [MY-013169] [Server] /usr/local/Cellar/mysql/8.0.11/bin/mysqld (mysqld 8.0.11) initializing of server in progress as process 35410
2018-07-08T04:56:01.746039Z 0 [ERROR] [MY-010457] [Server] --initialize specified but the data directory has files in it. Aborting.
2018-07-08T04:56:01.746086Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-07-08T04:56:01.746293Z 0 [System] [MY-010910] [Server] /usr/local/Cellar/mysql/8.0.11/bin/mysqld: Shutdown complete (mysqld 8.0.11)  Homebrew.
Warning: The post-install step did not complete successfully
You can try again using `brew postinstall mysql`
==&gt; Caveats
We've installed your MySQL database without a root password. To secure it run:
    mysql_secure_installation
MySQL is configured to only allow connections from localhost by default
To connect run:
    mysql -uroot
To have launchd start mysql now and restart at login:
  brew services start mysql
Or, if you don't want/need a background service you can just run:
  mysql.server start
==&gt; Summary
🍺  /usr/local/Cellar/mysql/8.0.11: 254 files, 232.6MB
which is confusing and truthfully I don't 100% percent understand and may very well be a root of the problem, but don't know.
I have updated to the latest version of XCode and installed the Development Tools
here is what my gemfile.lock looks like 
GEM
  remote: https://rubygems.org/
  specs:
    actionmailer (4.2.0)
      actionpack (= 4.2.0)
      actionview (= 4.2.0)
      activejob (= 4.2.0)
      mail (~&gt; 2.5, &gt;= 2.5.4)
      rails-dom-testing (~&gt; 1.0, &gt;= 1.0.5)
    actionpack (4.2.0)
      actionview (= 4.2.0)
      activesupport (= 4.2.0)
      rack (~&gt; 1.6.0)
      rack-test (~&gt; 0.6.2)
      rails-dom-testing (~&gt; 1.0, &gt;= 1.0.5)
      rails-html-sanitizer (~&gt; 1.0, &gt;= 1.0.1)
    actionview (4.2.0)
      activesupport (= 4.2.0)
      builder (~&gt; 3.1)
      erubis (~&gt; 2.7.0)
      rails-dom-testing (~&gt; 1.0, &gt;= 1.0.5)
      rails-html-sanitizer (~&gt; 1.0, &gt;= 1.0.1)
    activejob (4.2.0)
      activesupport (= 4.2.0)
      globalid (&gt;= 0.3.0)
    activemodel (4.2.0)
      activesupport (= 4.2.0)
      builder (~&gt; 3.1)
    activerecord (4.2.0)
      activemodel (= 4.2.0)
      activesupport (= 4.2.0)
      arel (~&gt; 6.0)
    activesupport (4.2.0)
      i18n (~&gt; 0.7)
      json (~&gt; 1.7, &gt;= 1.7.7)
      minitest (~&gt; 5.1)
      thread_safe (~&gt; 0.3, &gt;= 0.3.4)
      tzinfo (~&gt; 1.1)
    addressable (2.4.0)
    arel (6.0.3)
    authority (3.1.0)
      activesupport (&gt;= 3.0.0)
      rake (&gt;= 0.8.7)
    autoprefixer-rails (6.3.6.2)
      execjs
    aws_cf_signer (0.1.3)
    bcrypt (3.1.11)
    better_errors (2.1.1)
      coderay (&gt;= 1.0.0)
      erubis (&gt;= 2.6.6)
      rack (&gt;= 0.9.0)
    binding_of_caller (0.7.2)
      debug_inspector (&gt;= 0.0.1)
    bootstrap-sass (3.3.6)
      autoprefixer-rails (&gt;= 5.2.1)
      sass (&gt;= 3.3.4)
    builder (3.2.2)
    byebug (9.0.4)
    carrierwave (0.11.2)
      activemodel (&gt;= 3.2.0)
      activesupport (&gt;= 3.2.0)
      json (&gt;= 1.7)
      mime-types (&gt;= 1.16)
      mimemagic (&gt;= 0.3.0)
    celluloid (0.16.0)
      timers (~&gt; 4.0.0)
    chartkick (2.0.0)
    ckeditor (4.1.6)
      cocaine
      orm_adapter (~&gt; 0.5.0)
    climate_control (0.0.3)
      activesupport (&gt;= 3.0)
    cloudinary (1.1.7)
      aws_cf_signer
      rest-client
    cocaine (0.5.8)
      climate_control (&gt;= 0.0.3, &lt; 1.0)
    coderay (1.1.1)
    coffee-rails (4.1.1)
      coffee-script (&gt;= 2.2.0)
      railties (&gt;= 4.0.0, &lt; 5.1.x)
    coffee-script (2.4.1)
      coffee-script-source
      execjs
    coffee-script-source (1.10.0)
    concurrent-ruby (1.0.2)
    connection_pool (2.2.0)
    debug_inspector (0.0.2)
    devise (4.1.1)
      bcrypt (~&gt; 3.0)
      orm_adapter (~&gt; 0.1)
      railties (&gt;= 4.1.0, &lt; 5.1)
      responders
      warden (~&gt; 1.2.3)
    dotenv (2.1.1)
    dotenv-rails (2.1.1)
      dotenv (= 2.1.1)
      railties (&gt;= 4.0, &lt; 5.1)
    erubis (2.7.0)
    execjs (2.7.0)
    faraday (0.11.0)
      multipart-post (&gt;= 1.2, &lt; 3)
    font-awesome-rails (4.6.3.0)
      railties (&gt;= 3.2, &lt; 5.1)
    friendly_id (5.1.0)
      activerecord (&gt;= 4.0.0)
    globalid (0.3.6)
      activesupport (&gt;= 4.1.0)
    groupdate (3.0.0)
      activesupport (&gt;= 3)
    hashie (3.5.5)
    hitimes (1.2.4)
    i18n (0.7.0)
    ice_cube (0.11.1)
    jbuilder (2.4.1)
      activesupport (&gt;= 3.0.0, &lt; 5.1)
      multi_json (~&gt; 1.2)
    jquery-rails (4.1.1)
      rails-dom-testing (&gt;= 1, &lt; 3)
      railties (&gt;= 4.2.0)
      thor (&gt;= 0.14, &lt; 2.0)
    jquery-ui-rails (4.2.1)
      railties (&gt;= 3.2.16)
    json (1.8.3)
    jwt (1.5.6)
    kaminari (0.16.3)
      actionpack (&gt;= 3.0.0)
      activesupport (&gt;= 3.0.0)
    libv8 (3.16.14.15)
    loofah (2.0.3)
      nokogiri (&gt;= 1.5.9)
    mail (2.6.4)
      mime-types (&gt;= 1.16, &lt; 4)
    meta-tags (2.1.0)
      actionpack (&gt;= 3.0.0)
    mime-types (3.1)
      mime-types-data (~&gt; 3.2015)
    mime-types-data (3.2016.0521)
    mimemagic (0.3.1)
    mini_magick (4.5.1)
    mini_portile2 (2.0.0)
    minitest (5.9.0)
    multi_json (1.12.1)
    multi_xml (0.6.0)
    multipart-post (2.0.0)
    mysql2 (0.3.20)
    newrelic_rpm (3.15.2.317)
    nokogiri (1.6.7.2)
      mini_portile2 (~&gt; 2.0.0.rc2)
    oauth2 (1.3.1)
      faraday (&gt;= 0.8, &lt; 0.12)
      jwt (~&gt; 1.0)
      multi_json (~&gt; 1.3)
      multi_xml (~&gt; 0.5)
      rack (&gt;= 1.2, &lt; 3)
    omniauth (1.6.1)
      hashie (&gt;= 3.4.6, &lt; 3.6.0)
      rack (&gt;= 1.6.2, &lt; 3)
    omniauth-facebook (4.0.0)
      omniauth-oauth2 (~&gt; 1.2)
    omniauth-oauth2 (1.4.0)
      oauth2 (~&gt; 1.0)
      omniauth (~&gt; 1.2)
    orm_adapter (0.5.0)
    polyamorous (1.3.0)
      activerecord (&gt;= 3.0)
    quiet_assets (1.1.0)
      railties (&gt;= 3.1, &lt; 5.0)
    rack (1.6.5)
    rack-canonical-host (0.2.2)
      addressable (&gt; 0, &lt; 3)
      rack (&gt;= 1.0.0, &lt; 3)
    rack-protection (1.5.3)
      rack
    rack-test (0.6.3)
      rack (&gt;= 1.0)
    rails (4.2.0)
      actionmailer (= 4.2.0)
      actionpack (= 4.2.0)
      actionview (= 4.2.0)
      activejob (= 4.2.0)
      activemodel (= 4.2.0)
      activerecord (= 4.2.0)
      activesupport (= 4.2.0)
      bundler (&gt;= 1.3.0, &lt; 2.0)
      railties (= 4.2.0)
      sprockets-rails
    rails-deprecated_sanitizer (1.0.3)
      activesupport (&gt;= 4.2.0.alpha)
    rails-dom-testing (1.0.7)
      activesupport (&gt;= 4.2.0.beta, &lt; 5.0)
      nokogiri (~&gt; 1.6.0)
      rails-deprecated_sanitizer (&gt;= 1.0.1)
    rails-html-sanitizer (1.0.3)
      loofah (~&gt; 2.0)
    rails-i18n (4.0.8)
      i18n (~&gt; 0.7)
      railties (~&gt; 4.0)
    rails_12factor (0.0.3)
      rails_serve_static_assets
      rails_stdout_logging
    rails_serve_static_assets (0.0.5)
    rails_stdout_logging (0.0.5)
    railties (4.2.0)
      actionpack (= 4.2.0)
      activesupport (= 4.2.0)
      rake (&gt;= 0.8.7)
      thor (&gt;= 0.18.1, &lt; 2.0)
    rake (11.1.2)
    ranked-model (0.4.0)
      activerecord (&gt;= 3.1.12)
    ransack (1.7.0)
      actionpack (&gt;= 3.0)
      activerecord (&gt;= 3.0)
      activesupport (&gt;= 3.0)
      i18n
      polyamorous (~&gt; 1.2)
    rdoc (4.2.2)
      json (~&gt; 1.4)
    redis (3.3.0)
    redis-namespace (1.5.2)
      redis (~&gt; 3.0, &gt;= 3.0.4)
    ref (2.0.0)
    responders (2.2.0)
      railties (&gt;= 4.2.0, &lt; 5.1)
    rest-client (1.6.7)
      mime-types (&gt;= 1.16)
    sass (3.4.22)
    sass-rails (5.0.4)
      railties (&gt;= 4.0.0, &lt; 5.0)
      sass (~&gt; 3.1)
      sprockets (&gt;= 2.8, &lt; 4.0)
      sprockets-rails (&gt;= 2.0, &lt; 4.0)
      tilt (&gt;= 1.1, &lt; 3)
    sdoc (0.4.1)
      json (~&gt; 1.7, &gt;= 1.7.7)
      rdoc (~&gt; 4.0)
    sidekiq (3.4.2)
      celluloid (~&gt; 0.16.0)
      connection_pool (~&gt; 2.2, &gt;= 2.2.0)
      json (~&gt; 1.0)
      redis (~&gt; 3.2, &gt;= 3.2.1)
      redis-namespace (~&gt; 1.5, &gt;= 1.5.2)
    sidetiq (0.6.3)
      celluloid (&gt;= 0.14.1)
      ice_cube (= 0.11.1)
      sidekiq (&gt;= 3.0.0)
    sinatra (1.4.7)
      rack (~&gt; 1.5)
      rack-protection (~&gt; 1.4)
      tilt (&gt;= 1.3, &lt; 3)
    slim (3.0.7)
      temple (~&gt; 0.7.6)
      tilt (&gt;= 1.3.3, &lt; 2.1)
    slim-rails (3.0.1)
      actionmailer (&gt;= 3.1, &lt; 5.0)
      actionpack (&gt;= 3.1, &lt; 5.0)
      activesupport (&gt;= 3.1, &lt; 5.0)
      railties (&gt;= 3.1, &lt; 5.0)
      slim (~&gt; 3.0)
    spring (1.7.1)
    sprockets (3.6.0)
      concurrent-ruby (~&gt; 1.0)
      rack (&gt; 1, &lt; 3)
    sprockets-rails (3.0.4)
      actionpack (&gt;= 4.0)
      activesupport (&gt;= 4.0)
      sprockets (&gt;= 3.0.0)
    temple (0.7.7)
    therubyracer (0.12.2)
      libv8 (~&gt; 3.16.14.0)
      ref
    thor (0.19.1)
    thread_safe (0.3.5)
    tilt (2.0.4)
    timers (4.0.4)
      hitimes
    tzinfo (1.2.2)
      thread_safe (~&gt; 0.1)
    uglifier (3.0.0)
      execjs (&gt;= 0.3.0, &lt; 3)
    warden (1.2.6)
      rack (&gt;= 1.0)
    web-console (2.3.0)
      activemodel (&gt;= 4.0)
      binding_of_caller (&gt;= 0.7.2)
      railties (&gt;= 4.0)
      sprockets-rails (&gt;= 2.0, &lt; 4.0)
PLATFORMS
  ruby
DEPENDENCIES
  authority (~&gt; 3.0)
  bcrypt (~&gt; 3.1.7)
  better_errors
  bootstrap-sass (~&gt; 3.3.6)
  byebug
  carrierwave
  chartkick
  ckeditor
  cloudinary
  coffee-rails (~&gt; 4.1.0)
  devise
  dotenv-rails
  font-awesome-rails
  friendly_id
  groupdate
  jbuilder (~&gt; 2.0)
  jquery-rails
  jquery-ui-rails (~&gt; 4.2.1)
  kaminari (~&gt; 0.15)
  meta-tags
  mini_magick
  mysql2 (~&gt; 0.3.20)
  newrelic_rpm
  omniauth-facebook
  quiet_assets
  rack-canonical-host
  rails (= 4.2.0)
  rails-i18n
  rails_12factor
  ranked-model
  ransack
  sass-rails (~&gt; 5.0)
  sdoc (~&gt; 0.4.0)
  sidekiq (~&gt; 3.4.2)
  sidetiq (~&gt; 0.6)
  sinatra (&gt;= 1.3.0)
  slim-rails
  spring
  therubyracer
  uglifier (&gt;= 1.3.0)
  web-console (~&gt; 2.0)
RUBY VERSION
   ruby 2.3.1p112
BUNDLED WITH
   1.12.5
I also tried giving more specific commands like this answer recommended
https://stackoverflow.com/a/32869742/8239783
but I still get the same errors. 
gem install mysql2 -v '0.3.20' -- --with-mysql-config=/usr/local/Cellar/mysql/8.0.11/bin/mysql_config
Building native extensions with: '--with-mysql-config=/usr/local/Cellar/mysql/8.0.11/bin/mysql_config'
This could take a while...
ERROR:  Error installing mysql2:
    ERROR: Failed to build gem native extension.
    current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
/Users/mac/.rvm/rubies/ruby-2.3.1/bin/ruby -r ./siteconf20180708-38248-vd681p.rb extconf.rb --with-mysql-config=/usr/local/Cellar/mysql/8.0.11/bin/mysql_config
checking for ruby/thread.h... yes
checking for rb_thread_call_without_gvl() in ruby/thread.h... yes
checking for rb_thread_blocking_region()... no
checking for rb_wait_for_single_fd()... yes
checking for rb_hash_dup()... yes
checking for rb_intern3()... yes
-----
Using mysql_config at /usr/local/Cellar/mysql/8.0.11/bin/mysql_config
-----
checking for mysql.h... yes
checking for errmsg.h... yes
checking for mysqld_error.h... yes
-----
Setting rpath to /usr/local/Cellar/mysql/8.0.11/lib
-----
creating Makefile
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:367:33: warning: implicit conversion loses integer precision: 'long' to 'unsigned int' [-Wshorten-64-to-32]
        elapsed_time = end_time - start_time;
                     ~ ~~~~~~~~~^~~~~~~~~~~~
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:775:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:806:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:807:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:810:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:811:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:812:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:843:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1165:56: warning: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'unsigned int' [-Wshorten-64-to-32]
  mysql2rb = mysql2_mysql_enc_name_to_rb(charset_name, charset_name_len);
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~               ^~~~~~~~~~~~~~~~
client.c:1198:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
2 warnings and 10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/mac/.rvm/gems/ruby-2.3.1/gems/mysql2-0.3.20 for inspection.
Results logged to /Users/mac/.rvm/gems/ruby-2.3.1/extensions/x86_64-darwin-17/2.3.0/mysql2-0.3.20/gem_make.out
Here is the log for my mysql_config file 
mysql_config
Usage: /usr/local/bin/mysql_config [OPTIONS]
Compiler: Clang 9.1.0.9020039
Options:
        --cflags         [-I/usr/local/Cellar/mysql/8.0.11/include/mysql ]
        --cxxflags       [-I/usr/local/Cellar/mysql/8.0.11/include/mysql ]
        --include        [-I/usr/local/Cellar/mysql/8.0.11/include/mysql]
        --libs           [-L/usr/local/Cellar/mysql/8.0.11/lib -lmysqlclient -lssl -lcrypto]
        --libs_r         [-L/usr/local/Cellar/mysql/8.0.11/lib -lmysqlclient -lssl -lcrypto]
        --plugindir      [/usr/local/Cellar/mysql/8.0.11/lib/plugin]
        --socket         [/tmp/mysql.sock]
        --port           [0]
        --version        [8.0.11]
        --variable=VAR   VAR is one of:
                pkgincludedir [/usr/local/Cellar/mysql/8.0.11/include/mysql]
                pkglibdir     [/usr/local/Cellar/mysql/8.0.11/lib]
                plugindir     [/usr/local/Cellar/mysql/8.0.11/lib/plugin]
I am on a Mac High Sierra 10.13.4
I would love any advice I could get on it as it is driving me crazy.  Please let me know any other information I could provide to ask a better question.    
Thank you in advance
",<mysql><ruby-on-rails><rubygems><mysql2>,23894,6,656,281,1,3,7,57,21794,0,0,6,20,2018-07-08 4:46,2018-07-11 16:59,,3,,Basic,2
64647851,The stock item was unable to be saved. Please try again. Magento 2.4.0,"I installed magento 2.4.0 on my Linux server from godaddy. I'm unable to add the products to it. It is showing the Error is The stock item was unable to be saved. Please try again. Can any one help me from this please
",<php><mysql><linux><magento><magento2>,218,1,1,331,1,2,6,69,24333,0,0,9,20,2020-11-02 14:53,2020-11-04 5:14,,2,,Basic,14
56554159,TypeError: Object of type 'datetime' is not JSON serializable (with serialize function),"I'm  I getting this TypeError: Object of type 'datetime' is not JSON serializable error, even though I have a specific serialize function described in my model.
This is my code:
Flask route (rendered by React):
menus.py
@menus_bp.route('/menus', methods=['GET', 'POST'])
def menus():
    response_object = {
        'status': 'fail',
        'message': 'Invalid payload.'
        }
    try:
        user = User.query.filter_by(id=1).first()
        if user.menu == []:
            return edit_menu()
        else:
            template = render_template('menus.html')
            response_object = {
                'status': 'success',
                'message': 'success',
                'data': [{""restaurant"": user.restaurant,
                          ""menu"": menu,
                          ""content"": template}] # template passed to React
                }
            # db method
            Create_Menu(user=user)
        return jsonify(response_object), 200
    except (exc.IntegrityError, ValueError):
        db.session.rollback()
        return jsonify(response_object), 400
methods.py
def Create_Menu(user):
    menu = Menu(user=user)
    db.session.add(menu)
    db.session.commit()
    return {""status"": True,
            ""menu"": menu}
and finally the Menu model, which has a serialize() function:
class Menu(db.Model):
    __tablename__='menu'
    """"""
    Model for storing menus. 
    """"""   
    id = db.Column(db.Integer, primary_key=True)
    created = db.Column(db.DateTime, default=func.now(), nullable=False)       
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'))
    def __init__(self, user):
        self.user = user
    def serialize(self):
       return { 'id' : self.id,
                'created': self.created,
                'coffees' : [ item.serialize() for item in self.coffees]}
But I'm getting the following traceback:
TypeError: Object of type 'datetime' is not JSON serializable
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 2309, in __call__
return self.wsgi_app(environ, start_response)
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 2295, in wsgi_app
response = self.handle_exception(e)
File ""/usr/lib/python3.6/site-packages/flask_restful/__init__.py"", line 269, in error_router
return original_handler(e)
File ""/usr/lib/python3.6/site-packages/flask_cors/extension.py"", line 161, in wrapped_function
return cors_after_request(app.make_response(f(*args, **kwargs)))
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 1741, in handle_exception
reraise(exc_type, exc_value, tb)
File ""/usr/lib/python3.6/site-packages/flask/_compat.py"", line 34, in reraise
raise value.with_traceback(tb)
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
response = self.full_dispatch_request()
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""/usr/lib/python3.6/site-packages/flask_restful/__init__.py"", line 269, in error_router
return original_handler(e)
File ""/usr/lib/python3.6/site-packages/flask_cors/extension.py"", line 161, in wrapped_function
return cors_after_request(app.make_response(f(*args, **kwargs)))
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
reraise(exc_type, exc_value, tb)
File ""/usr/lib/python3.6/site-packages/flask/_compat.py"", line 34, in reraise
raise value.with_traceback(tb)
File ""/usr/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
rv = self.dispatch_request()
File ""/usr/lib/python3.6/site-packages/flask_debugtoolbar/__init__.py"", line 125, in dispatch_request
return view_func(**req.view_args)
File ""/usr/lib/python3.6/site-packages/flask_restful/__init__.py"", line 462, in wrapper
return self.make_response(data, code, headers=headers)
File ""/usr/lib/python3.6/site-packages/flask_restful/__init__.py"", line 491, in make_response
resp = self.representations[mediatype](data, *args, **kwargs)
File ""/usr/lib/python3.6/site-packages/flask_restful/representations/json.py"", line 21, in output_json
dumped = dumps(data, **settings) + ""\n""
File ""/usr/lib/python3.6/json/__init__.py"", line 238, in dumps
**kw).encode(obj)
File ""/usr/lib/python3.6/json/encoder.py"", line 201, in encode
chunks = list(chunks)
File ""/usr/lib/python3.6/json/encoder.py"", line 430, in _iterencode
yield from _iterencode_dict(o, _current_indent_level)
File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
yield from chunks
File ""/usr/lib/python3.6/json/encoder.py"", line 437, in _iterencode
o = _default(o)
File ""/usr/lib/python3.6/json/encoder.py"", line 180, in default
o.__class__.__name__)
TypeError: Object of type 'datetime' is not JSON serializable
It used to work when I was redering templates with Flask at backend, but now with frontend requets, it breaks with the erro above.
what is wrong now? why does not my serialize function work anymore? 
",<python><json><flask><serialization><sqlalchemy>,5392,0,112,9734,30,103,203,52,30001,0,1849,3,20,2019-06-12 3:08,2019-06-12 12:45,2019-06-12 12:45,0,0,Intermediate,15
50172067,SqlConnection Error if EXE is executed from network path,"
  First of all: everything you will read in this post happens only if Windows 10 April 2018 update is installed. No problem before installation and after update uninstallation.
After installing Windows 10 1803 update, all my VB program (VB6, .NET and WPF) running from network mapped drive or UNC path can't connect to SQL server, no problem if the same executable is placed and executed from local drive (tested on 2 pc in the same network):
Remote SQL server, exe on local drive: OK
Same remote SQL server, same exe on mapped network drive (with full read/write access): ERROR
This is the error (maybe not significat to solve this problem):
  A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: SQL Network Interfaces, error: 26 - Error Locating Server/Instance Specified).
Simple VB.NET code to reproduce the problem (place the code in a simple form with a button in the button_click event, set values to connect to the SQL server, compile, save the exe file on a network path and execute it):
Dim myConnectionString As String
Dim mySqlConnectionStringBuilder As New SqlConnectionStringBuilder()
mySqlConnectionStringBuilder.DataSource = myServer
mySqlConnectionStringBuilder.InitialCatalog = myDatabase
mySqlConnectionStringBuilder.UserID = myUtente
mySqlConnectionStringBuilder.Password = myPassword
myConnectionString = mySqlConnectionStringBuilder.ConnectionString
Dim mySqlConnection As New SqlConnection(myConnectionString)
mySqlConnection.Open()  &lt;- error
Exception:
System.Data.SqlClient.SqlException (0x80131904): Si è verificato un errore di rete o specifico dell'istanza mentre si cercava di stabilire una connessione con SQL Server. Il server non è stato trovato o non è accessibile. Verificare che il nome dell'istanza sia corretto e che SQL Server sia configurato in modo da consentire connessioni remote. (provider: SQL Network Interfaces, error: 26 - Errore nell'individuazione del server/dell'istanza specificata)
   in System.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, SqlCredential credential, Object providerInfo, String newPassword, SecureString newSecurePassword, Boolean redirectedUserInstance, SqlConnectionString userConnectionOptions, SessionData reconnectSessionData, DbConnectionPool pool, String accessToken, Boolean applyTransientFaultHandling, SqlAuthenticationProviderManager sqlAuthProviderManager)
   in System.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, DbConnectionPoolKey poolKey, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection, DbConnectionOptions userOptions)
   in System.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnectionPool pool, DbConnection owningObject, DbConnectionOptions options, DbConnectionPoolKey poolKey, DbConnectionOptions userOptions)
   in System.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   in System.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   in System.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   in System.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   in System.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   in System.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   in System.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   in System.Data.SqlClient.SqlConnection.TryOpenInner(TaskCompletionSource`1 retry)
   in System.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   in System.Data.SqlClient.SqlConnection.Open()
   in RiepilogoOreTimer.RiepilogoOreTimerWindow.ConnessioneOK()
ClientConnectionId:00000000-0000-0000-0000-000000000000
Error Number:-1,State:0,Class:20
Any ideas?
Update:
If I uninstall the April 2018 update the issue go away and the program works fine even if executed on a network drive, but this can't be the solution...
Update 08/05/2018:
I noticed that April 2018 update brought some changes in security:
  Windows 10, version 1803 provides additional protections:
  New Attack surface reduction rules 
  Controlled folder access can now block disk sectors
Could it be the cause of the issue?
I'm not a security manager so I can't say if this can cause my problem
Update 09/05/2018: I found this information in this post:
  Windows 10 update 1803 does not open network connections on
  executables files on SMBv1 share (as Windows Server 2003)
but I don't know what SMBv1 is... somebody can help me?
",<sql-server><vb.net><windows-10>,5616,2,27,372,0,2,13,65,4324,0,1,4,20,2018-05-04 9:49,2018-05-08 21:08,2018-05-09 10:47,4,5,Advanced,37
49323419,manually create replication slot for publication in PostgreSQL 10,"I am trying to get a stream of updates for certain tables from my PostgreSQL database. The regular way of getting all updates looks like this:
You create a logical replication slot
pg_create_logical_replication_slot('my_slot', 'wal2json');
And either connect to it using pg_recvlogical or making special SQL queries. This allows you to get all the actions from the database in json (if you used wal2json plugin or similar) and then do whatever you want with that data.
But in PostgreSQL 10 we have Publication/Subscription mechanism which allows us to replicate selected tables only. This is very handy because a lot of useless data is not being sent. The process looks like this:
First, you create a publication
CREATE PUBLICATION foo FOR TABLE herp, derp;
Then you subscribe to that publication from another database
CREATE SUBSCRIPTION mysub CONNECTION &lt;connection stuff&gt; PUBLICATION foo;
This creates a replication slot on a master database under the hood and starts listening to updates and commit them to the same tables on a second database. This is fine if your job was to replicate some tables, but want to get a raw stream for my stuff.
As I mentioned, the CREATE SUBSCRIPTION query is creating a replication slot on the master database under the hood, but how can I create one manually without the subscription and a second database? Here the docs say:
  To make this work, create the replication slot separately (using the function pg_create_logical_replication_slot with the plugin name pgoutput)
According to the docs, this is possible, but pg_create_logical_replication_slot only creates a regular replication slot. Is the pgoutput plugin responsible for all the magic? If yes, then it becomes impossible to use other plugins like wal2json with publications.
What am I missing here?
",<postgresql><replication><postgresql-10>,1804,2,8,988,2,10,25,66,18753,0,164,2,20,2018-03-16 14:42,2019-06-21 12:00,,462,,Advanced,39
54598531,What determines if rails includes id: :serial in a table definition?,"I'm working with an existing rails app, using postgresql. Its schema.rb file has id: :serial for many, but not all, tables:
create_table ""foos"", id: :serial, force: :cascade do |t|
When I run rails db:migrate:reset, id: :serial is removed. We are all on the same version of postgres, but different OSes. I haven't exhaustively tested the behavior between machines, but I think there is a difference between machines.
The rails version is the same as it was when the project started.
The project did start with sqlite3. When I switch to that and regenerate the file, same behavior.
What could cause this option to be removed in my environment?
here's some code that is probably relevant:
https://github.com/rails/rails/blob/b2eb1d1c55a59fee1e6c4cba7030d8ceb524267c/activerecord/lib/active_record/connection_adapters/postgresql/column.rb#L15-L21
https://github.com/rails/rails/blob/b2eb1d1c55a59fee1e6c4cba7030d8ceb524267c/activerecord/lib/active_record/connection_adapters/postgresql/schema_dumper.rb#L26-L42
update
I just tried rails db:migrate:reset on colleague's machines, and I was wrong! their environments also remove id: :serial.
I looked closer at recent migrations from a colleague, and the most recent one did not create id: :serial in schema.rb either.
",<ruby-on-rails><postgresql><rails-migrations>,1264,4,7,22715,29,156,228,62,8117,0,1154,2,20,2019-02-08 18:48,2019-02-14 4:56,2019-02-14 16:24,6,6,Basic,8
56742757,Why is Azure SQL database so expensive?,"For a small personal coding project I recently created a SQL database in Azure. For the past weeks I have been hardly using the database, out of 2 GB available space I have been using only 13 MB.
However, the database costs me 6,70 EUR per day and I don't understand why this is the case. Read a few topics/posts stating that the costs with similar use should be around 5-7 EUR per month, not per day.
This is the configuration for the database:
No elastic pool
General purpose, Gen5, 2 vCores
West Europe
Does anyone have an idea about what could be causing the costs per month to be so high? 
",<sql><azure><azure-sql-database>,595,0,0,1033,2,12,21,47,13493,0,132,7,20,2019-06-24 19:24,2019-06-24 19:33,2019-06-25 2:32,0,1,Advanced,39
49387428,Which MySQL connector do I use: mysql-connector-java-5.1.46.jar or mysql-connector-java-5.1.46-bin.jar What is the difference?,"I am preparing to use jdbc for the first time and am in the process of installing the jdbc driver for MySQL.
However, it is unclear to me which of these files to move to the WEB_INF/lib folder in Eclipse.  They both seem to contain the same content and are included together in the downloaded zip file for the MySQL connector.
I have searched everywhere but have been unable to find any documentation to explain which of these files to use.
",<java><mysql><jdbc><mysql-connector>,441,0,0,2364,1,19,24,51,8481,0,108,1,20,2018-03-20 14:52,2018-03-20 15:38,2018-03-20 15:38,0,0,Intermediate,19
56808425,SQLAlchemy (psycopg2.ProgrammingError) can't adapt type 'dict',"Couldn't find a solution on the web for my problem.
I am trying to insert this pandas df to a Postgresql table using SQLAlchemy 
Pandas 0.24.2 
sqlalchemy 1.3.3
python 3.7
Relevant part of my code is below:
engine = create_engine('postgresql://user:pass@host:5432/db')
file = open('GameRoundMessageBlackjackSample.json', 'r', encoding='utf-8')
json_dict = json.load(file)
df = json_normalize(json_dict, record_path='cards', meta=['bet', 'dealerId', 'dealerName', 'gameOutcome', 'gameRoundDuration', 'gameRoundId', 'gameType', 'tableId', 'win'])
df = df[['win', 'betAmount', 'bets']]
df.to_sql('test_netent_data', engine, if_exists='append')
When I try to load this table to sql without the column 'bets' everyting works as expected. But when I include it i get the following error:
sqlalchemy.exc.ProgrammingError: (psycopg2.ProgrammingError) can't adapt 
type 'dict'
[SQL: INSERT INTO test_netent_data (index, win, ""betAmount"", bets) VALUES (%(index)s, %(win)s, %(betAmount)s, %(bets)s)]
[parameters: ({'index': 0, 'win': '2000.00', 'betAmount': '1212112', 'bets': [{'name': '1', 'amount': '1212112'}]}, {'index': 1, 'win': '2000.00', 'betAmount': '1212000', 'bets': [{'name': '1', 'amount': '1212000'}]}, {'index': 2, 'win': '2000.00', 'betAmount': '1212112', 'bets': [{'name': '1', 'amount': '1212112'}]}, {'index': 3, 'win': '2000.00', 'betAmount': '1212000', 'bets': [{'name': '1', 'amount': '1212000'}]}, {'index': 4, 'win': '2000.00', 'betAmount': '1212112', 'bets': [{'name': '1', 'amount': '1212112'}]}, {'index': 5, 'win': '2000.00', 'betAmount': '1212000', 'bets': [{'name': '1', 'amount': '1212000'}]}, {'index': 6, 'win': '2000.00', 'betAmount': '1212112', 'bets': [{'name': '1', 'amount': '1212112'}]}, {'index': 7, 'win': '2000.00', 'betAmount': '1212000', 'bets': [{'name': '1', 'amount': '1212000'}]})]
(Background on this error at: http://sqlalche.me/e/f405)
I have checked the type of this column but it is (type object) no different from other columns. Ive also tried to convert it to string and got a bunch of other errors.
I believe there should be a simple solution which I can't get my head around.
",<json><python-3.x><pandas><postgresql><sqlalchemy>,2123,2,13,269,1,2,11,74,50387,0,13,4,20,2019-06-28 14:04,2019-06-28 14:55,2019-06-28 14:55,0,0,Advanced,33
50711207,How to set sql dialect in IntelliJ,"I'm working with IntelliJ 2017. I want to set the SQL dialect for an sql file. After I created the file, there is a message at the top saying the SQL dialect for the file is not set. When I click on the ""Change Dialect to.."" link, it opens an SQL Dialects menu. On the right, there is a pulldown link to select the dialect, but the pull down menu is empty. I went into Preferences > Languages and Frameworks > SQL Dialects to try to add some, but that menu is also empty and I can't seem to modify any of the fields. 
How do I add SQL dialects to IntelliJ, so I can select one for this file? Below is a photo of the menu I get when I select ""Change Dialect to..."" in the sql file 
Below is a photo of the menu I get from Perferences > Languages and Frameworks > SQL Dialects. I can't edit any of the fields.
",<sql><intellij-idea>,808,2,0,267,1,5,13,68,23063,0,0,1,20,2018-06-06 1:53,2018-06-06 5:11,,0,,Intermediate,20
49291428,Error: could not determine PostgreSQL version from '10.3' - Django on Heroku,"I tried to push from local env to Heroku master. No new requirements from the previous commit. However, I received an Error which saying the system could not determine PostgreSQL version from ""10.3"".
Here is my requirements list:
amqp==1.4.9
anyjson==0.3.3
appdirs==1.4.3
awscli==1.11.89
billiard==3.3.0.23
boto==2.46.1
botocore==1.5.52
celery==3.1.25
Collectfast==0.5.2
colorama==0.3.7
dj-database-url==0.4.2
Django==1.11.1
django-celery==3.2.1
django-recaptcha==1.3.0
django-redis-cache==1.7.1
django-storages==1.5.2
django-storages-redux==1.3.2
docutils==0.13.1
gunicorn==19.7.0
honcho==0.5.0
jmespath==0.9.2
kombu==3.0.37
olefile==0.44
packaging==16.8
Pillow==4.3.0
psycopg2==2.6.2
pyasn1==0.2.3
pyparsing==2.2.0
python-dateutil==2.6.0
pytz==2018.3
PyYAML==3.12
redis==2.10.5
reportlab==3.4.0
rsa==3.4.2
s3transfer==0.1.10
selenium==3.4.0
six==1.10.0
vine==1.1.4
virtualenv==15.1.0
virtualenvwrapper-win==1.2.1
whitenoise==3.3.0
and below is the error in the build log.
Collecting amqp==1.4.9 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 1))
         Downloading amqp-1.4.9-py2.py3-none-any.whl (51kB)
       Collecting anyjson==0.3.3 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 2))
         Downloading anyjson-0.3.3.tar.gz
       Collecting appdirs==1.4.3 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 3))
         Downloading appdirs-1.4.3-py2.py3-none-any.whl
       Collecting awscli==1.11.89 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 4))
         Downloading awscli-1.11.89-py2.py3-none-any.whl (1.2MB)
       Collecting billiard==3.3.0.23 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 5))
         Downloading billiard-3.3.0.23.tar.gz (151kB)
       Collecting boto==2.46.1 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 6))
         Downloading boto-2.46.1-py2.py3-none-any.whl (1.4MB)
       Collecting botocore==1.5.52 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 7))
         Downloading botocore-1.5.52-py2.py3-none-any.whl (3.5MB)
       Collecting celery==3.1.25 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 8))
         Downloading celery-3.1.25-py2.py3-none-any.whl (526kB)
       Collecting Collectfast==0.5.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 9))
         Downloading Collectfast-0.5.2-py3-none-any.whl
       Collecting colorama==0.3.7 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 10))
         Downloading colorama-0.3.7-py2.py3-none-any.whl
       Collecting dj-database-url==0.4.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 11))
         Downloading dj_database_url-0.4.2-py2.py3-none-any.whl
       Collecting Django==1.11.1 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 12))
         Downloading Django-1.11.1-py2.py3-none-any.whl (6.9MB)
       Collecting django-celery==3.2.1 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 13))
         Downloading django-celery-3.2.1.tar.gz (91kB)
       Collecting django-recaptcha==1.3.0 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 14))
         Downloading django-recaptcha-1.3.0.tar.gz
       Collecting django-redis-cache==1.7.1 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 15))
         Downloading django-redis-cache-1.7.1.tar.gz
       Collecting django-storages==1.5.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 16))
         Downloading django_storages-1.5.2-py2.py3-none-any.whl (51kB)
       Collecting django-storages-redux==1.3.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 17))
         Downloading django_storages_redux-1.3.2-py2.py3-none-any.whl (41kB)
       Collecting docutils==0.13.1 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 18))
         Downloading docutils-0.13.1-py3-none-any.whl (536kB)
       Collecting gunicorn==19.7.0 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 19))
         Downloading gunicorn-19.7.0-py2.py3-none-any.whl (112kB)
       Collecting honcho==0.5.0 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 20))
         Downloading honcho-0.5.0.tar.gz
       Collecting jmespath==0.9.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 21))
         Downloading jmespath-0.9.2-py2.py3-none-any.whl
       Collecting kombu==3.0.37 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 22))
         Downloading kombu-3.0.37-py2.py3-none-any.whl (240kB)
       Collecting olefile==0.44 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 23))
         Downloading olefile-0.44.zip (74kB)
       Collecting packaging==16.8 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 24))
         Downloading packaging-16.8-py2.py3-none-any.whl
       Collecting Pillow==4.3.0 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 25))
         Downloading Pillow-4.3.0-cp35-cp35m-manylinux1_x86_64.whl (5.8MB)
       Collecting psycopg2==2.6.2 (from -r /tmp/build_28bcf3a327daae7657433628289c1501/requirements.txt (line 26))
         Downloading psycopg2-2.6.2.tar.gz (376kB)
           Complete output from command python setup.py egg_info:
           running egg_info
           creating pip-egg-info/psycopg2.egg-info
           writing pip-egg-info/psycopg2.egg-info/PKG-INFO
           writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt
           writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt
           writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'
           Error: could not determine PostgreSQL version from '10.3'
           ----------------------------------------
       Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-24iic71n/psycopg2/
 !     Push rejected, failed to compile Python app.
 !     Push failed
Has anyone ever get through this?
",<django><git><postgresql><heroku><psycopg2>,6308,0,106,253,1,2,9,52,20608,0,43,1,20,2018-03-15 4:08,2018-03-15 5:38,2018-03-15 5:38,0,0,Intermediate,17
61981787,"Is there a rule that forbids to name its entity class ""User"" when working with PostgreSQL and Spring Boot?","guys. I'm having a problem with a Spring boot 2.3.0  and PostgreSQL 12 project. I have an entity class I called User whose code is as follows:
@Entity
@NoArgsConstructor
@Data
@AllArgsConstructor
@Builder
public class User {
    @Id @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String username;
    private String email;
    private String firstName;
    private String lastName;
    private int age;
}
And my application.properties file looks like this:
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.hibernate.show-sql=true
spring.datasource.url=jdbc:postgresql://localhost:5432/dbname
spring.datasource.username=postgres
spring.datasource.password=*********
The concern is that when I execute my project I get a mistake like this:
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL ""drop table if exists user cascade"" via JDBC Statement
    at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67) ~[hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlString(SchemaDropperImpl.java:375) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.applySqlStrings(SchemaDropperImpl.java:359) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.dropFromMetadata(SchemaDropperImpl.java:241) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.performDrop(SchemaDropperImpl.java:154) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:126) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaDropperImpl.doDrop(SchemaDropperImpl.java:112) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:145) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:73) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:314) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:468) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1249) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_252]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_252]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_252]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_252]
Caused by: org.postgresql.util.PSQLException: ERREUR: erreur de syntaxe sur ou près de « user »
  Position : 22
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2533) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2268) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:313) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:448) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:369) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:310) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:296) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:273) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:268) ~[postgresql-42.2.12.jar:42.2.12]
    at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95) ~[HikariCP-3.4.5.jar:na]
    at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java) ~[HikariCP-3.4.5.jar:na]
    at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54) ~[hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    ... 18 common frames omitted
2020-05-24 06:16:33.429  WARN 13636 --- [         task-1] o.h.t.s.i.ExceptionHandlerLoggedImpl     : GenerationTarget encountered exception accepting command : Error executing DDL ""create table user (id int8 generated by default as identity, age int4 not null, email varchar(255), first_name varchar(255), last_name varchar(255), username varchar(255), primary key (id))"" via JDBC Statement
org.hibernate.tool.schema.spi.CommandAcceptanceException: Error executing DDL ""create table user (id int8 generated by default as identity, age int4 not null, email varchar(255), first_name varchar(255), last_name varchar(255), username varchar(255), primary key (id))"" via JDBC Statement
    at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:67) ~[hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlString(SchemaCreatorImpl.java:439) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.applySqlStrings(SchemaCreatorImpl.java:423) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.createFromMetadata(SchemaCreatorImpl.java:314) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.performCreation(SchemaCreatorImpl.java:166) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:135) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.internal.SchemaCreatorImpl.doCreation(SchemaCreatorImpl.java:121) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:156) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:73) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:314) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:468) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:1249) [hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:58) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:391) [spring-orm-5.2.6.RELEASE.jar:5.2.6.RELEASE]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_252]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_252]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_252]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_252]
Caused by: org.postgresql.util.PSQLException: ERREUR: erreur de syntaxe sur ou près de « user »
  Position : 14
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2533) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2268) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:313) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:448) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:369) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:310) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:296) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:273) ~[postgresql-42.2.12.jar:42.2.12]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:268) ~[postgresql-42.2.12.jar:42.2.12]
    at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95) ~[HikariCP-3.4.5.jar:na]
    at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java) ~[HikariCP-3.4.5.jar:na]
    at org.hibernate.tool.schema.internal.exec.GenerationTargetToDatabase.accept(GenerationTargetToDatabase.java:54) ~[hibernate-core-5.4.15.Final.jar:5.4.15.Final]
    ... 18 common frames omitted
However when I change the name of my class e.g. to Person 
@Entity
@NoArgsConstructor
@Data
@AllArgsConstructor
@Builder
public class Person {
    @Id @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String username;
    private String email;
    private String firstName;
    private String lastName;
    private int age;
}
I don't get any errors. Could someone help me with this in the following way or at least explain me what is wrong with the entity class User.
I would also like to add that when in doubt I tested this code with MySQL as an alternative to PostgreSQL and got no errors.
",<java><postgresql><hibernate><spring-boot><jpa>,10925,0,106,193,1,1,4,50,12911,0,0,3,19,2020-05-24 4:52,2020-05-24 5:07,2020-05-24 5:07,0,0,Intermediate,19
52207233,Hibernate saves/retrieves date minus day if application uses another timezone than MySQL,"I have an application started on tomcat on MACHINE_A with timezone GMT+3.
I use remote MySQL server started on MACHINE_B with timezone UTC.
We use spring-data-jpa for persistence.
As an example of the problem, I will show the repository:
public interface MyRepository extends JpaRepository&lt;MyInstance, Long&gt; {
    Optional&lt;MyInstance&gt; findByDate(LocalDate localDate);
}
If I pass localDate for 2018-09-06, I get entities where the date is 2018-09-05(previous day)
In the logs I see:  
2018-09-06 18:17:27.783 TRACE 13676 --- [nio-8080-exec-3] o.h.type.descriptor.sql.BasicBinder      : binding parameter [1] as [DATE] - [2018-09-06]
I googled that question a lot and found several articles with the same content(for example https://moelholm.com/2016/11/09/spring-boot-controlling-timezones-with-hibernate/)
So, I have the following application.yml:
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/MYDB?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC
    username: root
    password: *****
  jpa:
    hibernate:
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
    properties:
      hibernate:
        show_sql: true
        use_sql_comments: true
        format_sql: true
        type: trace
        jdbc:
          time_zone: UTC
But it doesn't help.
We use the following connector:
&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;8.0.12&lt;/version&gt;
&lt;/dependency&gt;
How can I resolve my problem?
P.S.
I tried to run both applications with the same time zone. In this case, everything works as expected.
P.S.2
I tried to use MySQL driver 6.0.6 version but it doesn't change anything.
",<java><mysql><hibernate><spring-boot><timezone>,1834,2,29,36992,119,374,727,78,9947,0,3581,8,19,2018-09-06 15:11,2018-09-06 19:54,,0,,Intermediate,23
64320136,ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115),"I'm trying to make a MySQL database on my Raspberry Pi 4, but it isn't going very well, using localhost instead works perfectly but I want to remote control it from my Windows 10 computer on the same internet. When I create a user with the address of 192.168.1.15 by doing this:
sudo mysql -u root
CREATE USER 'lasse'@'192.168.1.15' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON *.* TO 'lasse'@'192.168.1.15';
FLUSH PRIVILEGES
exit
I try to login again using this:
mysql -u lasse -h 192.168.1.15 -ppassword // didnt work, error: ERROR 2002 (HY000): Can't connect to MySQL server on '192.168.1.15' (115)
mysql -u user -h 192.168.1.2 -P 3306 -ppassword // didnt work either, same error.
I have these packages installed:
mariadb-client
mariadb-server
default-mysql-server
",<mysql><mariadb>,773,0,10,191,1,1,3,75,66630,0,0,6,19,2020-10-12 14:49,2020-10-12 14:54,,0,,Basic,9
48261997,room migration using existing boolean column types,"What i found out so far 
All the @entity annotated classes are processed during compiletime and an Implementation for Database class is generated. Then before accessing the db, validateMigration method of this generated class is called. This validateMigration method verifies with the existing db schema via raw query
PRAGMA table_info mytable name 
(see L208 of android.arch.persistence.room.util.TableInfo.java)
Now the problem 
My sqlite3 db has some columns with column type as BOOLEAN. (which slqite internally handles to int). Now when i create room entities say 
public someEntity {
     @columnInfo(name=""someName"")
     public Boolean myValue;
}
The room's create table query will be
Create Table someEntity ( myValue INTEGER)
Where as when we query the existing db with PRAGMA table_info someEntity we get
1|myValue|BOOLEAN|0||0
As explained above room verifies the ( sqlite to room ) migration by comparing field name, column type etc. And since the column types dont match (BOOLEAN and INTEGER) it throws an error saying migration failed.
Can anyone suggest a workaround to this ? Can we make room create BOOLEAN column type in sqlite ? (Also afaik we can't change/alter column types of existing tables.)
PS: I also see a similar issue with VARCHAR - Using an existing VARCHAR column with Room
",<java><android><sqlite><android-room>,1306,1,8,491,1,4,12,43,13150,0,45,2,19,2018-01-15 11:23,2019-03-22 17:29,2019-03-22 17:29,431,431,Basic,9
62708607,How to fix diesel_cli link libpq.lib error with Postgres tools installed in Docker?,"I'm trying (for hours now) to install the cargo crate diesel_cli for postgres. However, every time I run the recommended cargo command:
cargo install diesel_cli --no-default-features --features postgres
I wait a few minutes just to see the same build fail with this message:
note: LINK : fatal error LNK1181: cannot open input file 'libpq.lib'
error: aborting due to previous error
error: failed to compile `diesel_cli v1.4.1`, intermediate artifacts can be found at `C:\Users\&lt;user name here&gt;\AppData\Local\Temp\cargo-installUU2DtT`
Caused by:
  could not compile `diesel_cli`.
I'm running postgres in a docker container and have the binaries on my C:\pgsql with the lib and bin directories both on the PATH so I can't figure out why it's not linking. What else could be required they didn't mention in the docs?
",<postgresql><rust><rust-cargo><libpq><rust-diesel>,820,1,13,3137,4,26,57,56,11733,0,342,8,19,2020-07-03 4:38,2020-08-18 8:36,2020-08-18 8:36,46,46,Basic,9
55976471,How can I use AWS's Dynamo Db with Django?,"I am developing web applications, APIs, and backends using the Django MVC framework. A major aspect of Django is its implementation of an ORM for models. It is an exceptionally good ORM. Typically when using Django, one utilizes an existing interface that maps one's Django model to a specific DBMS like Postgres, MySQL, or Oracle for example.
I have some specific needs, requirements regarding performance and scalability, so I really want to use AWS's Dynamo DB because it is highly cost efficient, very performant, and scales really well.
While I think Django allows one to implement their own interface for a DBMS if one wishes to do so, it is clearly advantageous to be able to use an existing DBMS interface when constructing one's Django models if one exists.
Can someone recommend a Django model interface to use so I can construct a model in Django that uses AWS's Dynamo DB?
How about one using MongoDB?
",<python><django><orm><nosql><amazon-dynamodb>,914,0,0,773,3,7,15,65,26399,0,139,5,19,2019-05-03 20:08,2019-11-22 6:35,,203,,Intermediate,20
56239790,Update SQL Query with populated variables from AJAX functions over multiple PHP Pages,"i try to get help with that question. 
All in all Q: It doesnt Update my DB entry like this Step by Step Order how i think it could be done.
its a bit difficult to explain, but i try to explain it step by step with minimal and readable Code. I use the original code, its hard to convert it in reproducible Examples.
A.1 Page ma_aktuelle_ReadOut.php
 There is a php Part
 &lt;?php echo ""&lt;a href='ma_Testende.php?TestergebnisID=&amp;TestaufstellungID="". $row['TestaufstellungID'].""&amp;TesterID="".$row['TesterID'].""' title='Test stoppen' data-toggle='tooltip' class='stoppen'&gt;   &lt;span class='glyphicon glyphicon-stop'&gt;&lt;/span&gt;&lt;/a&gt;"";
?&gt;
When i click this link the following javascript function is called and ask me ""really stop?""
&lt;script language=""JavaScript"" type=""text/javascript""&gt;
$(document).ready(function(){
  $(""a.stoppen"").click(function(e){
   if(!confirm('Wirklich stoppen?')){
    e.preventDefault();
    $('.alert').show()
    return false;
    }
    return true;
            });
        });
&lt;/script&gt;
&lt;style&gt;
 .alert {
  display: none;
    }
&lt;/style&gt;
When i cklick ""yes"" it opens the second Page.
A 2 Page ma_Testende.php
On this Page are 2 AJAX JS Functions.
The first Ajax is asking for ""Datum"" via type:get from the following next page and wait till success (see Page B 3):
&lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js""&gt;
/* function to get Stoptime for Cycle from DB.TesterCycleCount zu erhalten  */ 
$(document).ready(async function(){
var Datum;
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value from TesterID */ 
await $.ajax({ /* First Ajax function */
            url: 'ma_get-TesterID_Testende.php',
            type: 'get', 
            data: {TesterID:TesterID}, 
            dataType: 'json',
            success:function(response){ 
                var CID = response['CID'];
                Datum = response['Datum'];
                console.log(response);
            },
             error: function(jqxhtt, status, exception) {
                 console.log(exception);
         alert('Exception:', exception)
            }
        });
console.log();
        var TestaufstellungID = ""&lt;?php echo $_GET['TestaufstellungID']; ?&gt;"";
         $.ajax({ /* Second Ajax function */
            url: 'ma_TestendeSQL.php',
            type: 'get', 
            data: {TestaufstellungID:TestaufstellungID, Datum: Datum}, 
            dataType: 'json',
            success:function(data){ 
            alert('Successfully called');
     },
     error: function(jqxhr, status, exception) {
         console.log(exception);
         alert('Exception:', exception)
            }
        });
        });
&lt;/script&gt;
B 3 Page ma_get-TesterID_Testende.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
                $sqlreadZeit = ""Select TOP 1 CID,Datum from DB.dbo.TesterCycleCount where TesterID = '"".$_GET['TesterID'].""' order by Datum DESC"";
                $result1 = sqlsrv_query($conn, $sqlreadZeit);
                $zeiten_arr = array();
                while ($row = sqlsrv_fetch_array($result1, SQLSRV_FETCH_ASSOC)) {
                $CID = $row['CID'];
                $Datum = $row['Datum']-&gt;format('d.m.Y h:m:s');
                $zeiten_arr[] = array(""CID"" =&gt; $CID, ""Datum"" =&gt; $Datum);
                                }
    header('Content-type: application/json');
  echo json_encode($zeiten_arr); 
?&gt;
Back with the ""Datum"" the second AJAX is called (see Page A 2) 
With the ""Datum"" and ""TestaufstellungID"" as variable it should be call the next Page and Update the DB entry with the populated variablles.
B. 4 Page ma_TestendeSQL.php
&lt;?php
$cinfo = array(
    ""Database"" =&gt; $database,
    ""UID"" =&gt; $username,
    ""PWD"" =&gt; $password
);
$conn = sqlsrv_connect($server, $cinfo);
$TestaufstellungID = $_GET['TestaufstellungID'];
$Testende= $_GET['Datum'];
$Testdatum = date('Y-d-m');
$stop = $connection-&gt;prepare(""WITH UpdateTestende AS (
  SELECT TOP 1  * from DB.dbo.Testergebnisse 
  WHERE TestaufstellungID = :TestaufstellungID
  ORDER BY TestergebnisID DESC 
)
update UpdateTestende 
set Testende = :Testende,
Datum = :Testdatum"");
$stop-&gt;execute(array(':TestaufstellungID' =&gt; $TestaufstellungID, ':Testdatum' =&gt; $Testdatum, ':Testende' =&gt; $Testende));
    header('Content-type: application/json');
?&gt;
The php variable $Testende get the populated ""Datum"" from the Ajax functions. All in all at the end it should be Update, when i click on the link the ( Page A 1) my DB entry with the populated ""Datum"" which i get from the first Ajax call ( Page A 2 ) from the SQL Query ( Page B 3) back to the second AJAX Call ( Page A 2 )  than with the data: {TestaufstellungID:TestaufstellungID, Datum: Datum}  to the last page ( Page B 4) 
But it doesnt Update my DB entry like this Step by Step Order how i think it could be done. 
Encapsulated is the SQL-Code working fine. With the Code header('Content-type: application/json'); the browser tell me the following when i click on the link from ( Page A 1 )
SyntaxError: JSON.parse: unexpected character at line 1 column 1 of the JSON data
Thats why i posted all the Step i think on one point the variables are not passed right to the next page or they are empty becasue the code is not executed in the right order Server/Client PHP/JS or Asynchronous problem... 
The console.log tell me nothing. At the moment i have no idea where to start with the debugging?
Hope someone can help me. thx
Edit: iam pretty sure the ajax call is empty, but i dont see it in which step the values getting empty
Edit2:
AJAX Call is empty or is not starting.
Further invstigation:  The Ajax alert me the error part with empty exception and dont alert me the success part. So it doesnt  go to the page ma_get-TesterID_Testende.php or it doesnt  return back the Datum .
Could be not enabled Cross-Site-Scripting be the Problem? 
But in another Page is a similiar Ajax Call working fine.
$(document).ready(function(){
var TesterID = ""&lt;?php echo $_GET['TesterID']; ?&gt;""; /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    $(""#TesterID"").change(function(){ /* Wenn du änderst und vom Select Feld auswählst */
        var TesterID = $(this).val(); /* value der Tester erhalten */ 
        $.ajax({ /* AJAX aufrufen */
            url: 'ma_get-TesterID.php',
            type: 'get', /* Methode zum übertragen der Daten */
            data: {TesterID:TesterID}, /* Daten zu übermitteln */
            dataType: 'json',
            success:function(response){ /* Die zurückgegebenene Daten erhalten */
                var len = response.length;
                $(""#Teststart"").empty(); /* Die erhaltenden Daten werden bei der ID angezeigt */
                for( var i = 0; i&lt;len; i++){
                    var CID = response[i]['CID'];
                    var Datum = response[i]['Datum'];
                    $(""#Teststart"").append(""&lt;option value='""+Datum+""'&gt;""+Datum+""&lt;/option&gt;"");
                }
            }
        });
    });
});
In this example the Ajax Call starts when i change the value from a Dropdown selection Form.    Is there a difference? 
How this Ajax should work i try to explain in my other question step by step, how it my application should be execute.
Update SQL Query with populated variables from AJAX functions over multiple PHP Pages
Edit 3:
JQuery Version:
https://code.jquery.com/jquery-3.4.1.js
",<javascript><php><jquery><sql-server><ajax>,8300,5,155,668,0,4,17,42,777,,18,1,19,2019-05-21 13:48,2019-05-28 23:31,,7,,Intermediate,31
63684133,prisma can't connect to postgresql,"I've tried to connect Prisma with postgreSQL several times.
prisma show this error message : &quot;Error: undefined: invalid port number in &quot;postgresql://postgres:password@localhost:5432/linker&quot;)&quot;.
-error
-prisma/.env
DATABASE_URL=postgresql://postgres:password@localhost:5432/linker
-schema.prisma
datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}
So, first, I checked the port number to see if it was right and 5432 is right because I use the default port number. I also checked the postgresql.conf file, which is set to &quot;listen_address=&quot;*&quot;&quot; , &quot;port=5432&quot;.
And I went into pgAdmin4 and saw server's properties. the port number was 5432 as shown below image, and the username was set &quot;postgres&quot;.
I don't know why prisma can't connect
Did i something missed?
",<postgresql><prisma>,861,3,5,345,1,3,10,66,15046,0,28,5,19,2020-09-01 8:36,2021-06-02 8:09,2021-07-01 16:23,274,303,Basic,6
65409104,What does Room return if a query had no results,"Having a difficult time finding this answer and the documentation doesn't seem to answer the question.
If I have a basic ROOM query,
@Query(&quot;SELECT * FROM geotable WHERE geohash = :geohash&quot;)
abstract suspend fun getGeoTable(geohash: String) : GeoTable
and there is no such item that uses this primary key, what happens?  Android studio says that the DAO object will never return a null.  It seems that EmptyResultSetException only gets thrown when you have Single using RxJava which I am not using.  So what does plain old ROOM throw when it finds nothing?
",<android><sqlite><android-room>,567,0,2,1073,2,12,21,73,5379,0,16,4,19,2020-12-22 12:53,2021-04-21 9:24,,120,,Basic,13
64474420,PostgreSQL authentication method 10 not supported,"I'm trying to follow the diesel.rs tutorial using PostgreSQL. When I get to the Diesel setup step, I get an &quot;authentication method 10 not supported&quot; error. How do I resolve it?
",<postgresql><rust><rust-diesel>,187,1,0,393,1,4,11,41,100878,0,0,3,19,2020-10-22 2:34,2020-10-22 6:04,,0,,Basic,14
58033899,Why is Spark broadcast exchange data size bigger than raw size on join?,"I am doing a broadcast join of two tables A and B.
B is a cached table created with the following Spark SQL:
create table B as select segment_ids_hash from  stb_ranker.c3po_segments
      where
        from_unixtime(unix_timestamp(string(dayid), 'yyyyMMdd')) &gt;= CAST('2019-07-31 00:00:00.000000000' AS TIMESTAMP)
      and
        segmentid_check('(6|8|10|12|14|371|372|373|374|375|376|582|583|585|586|587|589|591|592|594|596|597|599|601|602|604|606|607|609|610|611|613|615|616)', seg_ids) = true
cache table B
The column 'segment_ids_hash' is of integer type and the result contains 36.4 million records.
The cached table size is about 140 MB, as shown below
Then I did the join as follows:
select count(*) from A broadcast join B on A.segment_ids_hash = B.segment_ids_hash
Here broadcast exchange data size is about 3.2 GB.
My question is why the broadcast exchange data size (3.2GB) is so much bigger than the raw data size (~140 MB). What are the overheads? Is there any way to reduce the broadcast exchange data size?
Thanks
",<apache-spark><apache-spark-sql>,1033,2,7,1497,3,17,26,37,5192,0,39,2,19,2019-09-20 19:02,2020-02-24 11:20,2020-02-24 11:20,157,157,Intermediate,23
60457719,Query without WHILE Loop,"We have appointment table as shown below. Each appointment need to be categorized as ""New"" or ""Followup"". Any appointment (for a patient) within 30 days of first appointment (of that patient) is Followup.  After 30 days, appointment is again ""New"". Any appointment within 30 days become ""Followup"".
I am currently doing this by typing while loop.
How to achieve this without WHILE loop?
Table
CREATE TABLE #Appt1 (ApptID INT, PatientID INT, ApptDate DATE)
INSERT INTO #Appt1
SELECT  1,101,'2020-01-05' UNION
SELECT  2,505,'2020-01-06' UNION
SELECT  3,505,'2020-01-10' UNION
SELECT  4,505,'2020-01-20' UNION
SELECT  5,101,'2020-01-25' UNION
SELECT  6,101,'2020-02-12'  UNION
SELECT  7,101,'2020-02-20'  UNION
SELECT  8,101,'2020-03-30'  UNION
SELECT  9,303,'2020-01-28' UNION
SELECT  10,303,'2020-02-02' 
",<sql><sql-server><t-sql><sql-server-2016>,804,1,12,22285,67,261,420,56,1312,0,5076,10,19,2020-02-28 18:58,2020-02-28 19:13,2020-03-02 16:58,0,3,Basic,10
64414030,How to use nested pydantic models for sqlalchemy in a flexible way,"from fastapi import Depends, FastAPI, HTTPException, Body, Request
from sqlalchemy import create_engine, Boolean, Column, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Session, sessionmaker, relationship
from sqlalchemy.inspection import inspect
from typing import List, Optional
from pydantic import BaseModel
import json
SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
app = FastAPI()
# sqlalchemy models
class RootModel(Base):
    __tablename__ = &quot;root_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someRootText = Column(String)
    subData = relationship(&quot;SubModel&quot;, back_populates=&quot;rootData&quot;)
class SubModel(Base):
    __tablename__ = &quot;sub_table&quot;
    id = Column(Integer, primary_key=True, index=True)
    someSubText = Column(String)
    root_id = Column(Integer, ForeignKey(&quot;root_table.id&quot;))
    rootData = relationship(&quot;RootModel&quot;, back_populates=&quot;subData&quot;)
# pydantic models/schemas
class SchemaSubBase(BaseModel):
    someSubText: str
    class Config:
        orm_mode = True
class SchemaSub(SchemaSubBase):
    id: int
    root_id: int
    class Config:
        orm_mode = True
class SchemaRootBase(BaseModel):
    someRootText: str
    subData: List[SchemaSubBase] = []
    class Config:
        orm_mode = True
class SchemaRoot(SchemaRootBase):
    id: int
    class Config:
        orm_mode = True
class SchemaSimpleBase(BaseModel):
    someRootText: str
    class Config:
        orm_mode = True
class SchemaSimple(SchemaSimpleBase):
    id: int
    class Config:
        orm_mode = True
Base.metadata.create_all(bind=engine)
# database functions (CRUD)
def db_add_simple_data_pydantic(db: Session, root: SchemaRootBase):
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic_generic(db: Session, root: SchemaRootBase):
    # this fails:
    db_root = RootModel(**root.dict())
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_pydantic(db: Session, root: SchemaRootBase):
    # start: hack: i have to manually generate the sqlalchemy model from the pydantic model
    root_dict = root.dict()
    sub_dicts = []
    # i have to remove the list form root dict in order to fix the error from above
    for key in list(root_dict):
        if isinstance(root_dict[key], list):
            sub_dicts = root_dict[key]
            del root_dict[key]
    # now i can do it
    db_root = RootModel(**root_dict)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    # end: hack
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    return db_root
def db_add_nested_data_nopydantic(db: Session, root):
    print(root)
    sub_dicts = root.pop(&quot;subData&quot;)
    print(sub_dicts)
    db_root = RootModel(**root)
    for sub_dict in sub_dicts:
        db_root.subData.append(SubModel(**sub_dict))
    db.add(db_root)
    db.commit()
    db.refresh(db_root)
    # problem
    &quot;&quot;&quot;
    if I would now &quot;return db_root&quot;, the answer would be of this:
    {
        &quot;someRootText&quot;: &quot;string&quot;,
        &quot;id&quot;: 24
    }
    and not containing &quot;subData&quot;
    therefore I have to do the following.
    Why?
    &quot;&quot;&quot;
    from sqlalchemy.orm import joinedload
    db_root = (
        db.query(RootModel)
            .options(joinedload(RootModel.subData))
            .filter(RootModel.id == db_root.id)
            .all()
    )[0]
    return db_root
# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
@app.post(&quot;/addNestedModel_pydantic_generic&quot;, response_model=SchemaRootBase)
def addSipleModel_pydantic_generic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addSimpleModel_pydantic&quot;, response_model=SchemaSimpleBase)
def add_simple_data_pydantic(root: SchemaSimpleBase, db: Session = Depends(get_db)):
    data = db_add_simple_data_pydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_nopydantic&quot;)
def add_nested_data_nopydantic(root=Body(...), db: Session = Depends(get_db)):
    data = db_add_nested_data_nopydantic(db=db, root=root)
    return data
@app.post(&quot;/addNestedModel_pydantic&quot;, response_model=SchemaRootBase)
def add_nested_data_pydantic(root: SchemaRootBase, db: Session = Depends(get_db)):
    data = db_add_nested_data_pydantic(db=db, root=root)
    return data
Description
My Question is:
How to make nested sqlalchemy models from nested pydantic models (or python dicts)  in a generic way and write them to the database in &quot;one shot&quot;.
My example model is called RootModel and has a list of submodels called &quot;sub models&quot; in subData key.
Please see above for pydantic and sqlalchemy definitions.
Example:
The user provides a nested json string:
{
  &quot;someRootText&quot;: &quot;string&quot;,
  &quot;subData&quot;: [
    {
      &quot;someSubText&quot;: &quot;string&quot;
    }
  ]
}
Open the browser and call the endpoint /docs.
You can play around with all endpoints and POST the json string from above.
/addNestedModel_pydantic_generic
When you call the endpoint /addNestedModel_pydantic_generic it will fail, because sqlalchemy cannot create the nested model from pydantic nested model directly:
AttributeError: 'dict' object has no attribute '_sa_instance_state' 
​/addSimpleModel_pydantic
With a non-nested model  it works.
The remaining endpoints are showing &quot;hacks&quot; to solve the problem of nested models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with pydantic models.
/addNestedModel_pydantic
In this endpoint is generate the root model and andd the submodels with a loop in a non-generic way with python dicts.
My solutions are only hacks, I want a generic way to create nested sqlalchemy models either from pydantic (preferred) or from a python dict.
Environment
OS: Windows,
FastAPI Version : 0.61.1
Python version: Python 3.8.5
sqlalchemy: 1.3.19
pydantic : 1.6.1
",<sqlalchemy><fastapi><pydantic>,6555,0,207,191,1,1,4,81,18993,0,1,4,19,2020-10-18 13:41,2021-04-30 11:29,,194,,Intermediate,17
52177610,How to create auto incrementing / SERIAL id columns using DBeaver with PostgreSQL?,"I am a new user for both PostgreSQL and DBeaver (Community edition ver. 5.1.6) and was looking for a way to create an auto incrementing ID column in a table through the DBeaver GUI.
From my research I can see that:
You can set this up easily using SQL eg. id SERIAL NOT_NULL
The underlying problem is that there is no such thing as a 'Serial data type', and that SERIAL equates to nextval('table_name_id_seq').
When I create a table using the SERIAL command in SQL the resulting id column has a nextval('exampletable_id_seq'::regclass') value in the 'Default' attribute.
I have attempted to manually input the nextval() command within the 'Default' attribute for the column in DBeaver in a new table, for example. nextval('mytable_id_seq') with and without the '::regclass;. However this is not working.
I appreciate that doing this in SQL would be easier, and that there is a previously asked question at: Problems de Serial data type in DBeaver &amp; PostgreSQL.
However, I could not find a satisfactory answer and the option of being able to do this through the GUI would be useful, especially if other setup is being done through the DBeaver GUI.
Specifically, my question is:
Is there a functionality for DBeaver to add auto incrementing id's through the GUI?
If so, what would be the steps to do this.
",<postgresql><dbeaver>,1308,1,0,703,2,7,13,77,65316,0,58,1,19,2018-09-05 5:24,2018-09-05 22:15,2018-09-05 22:15,0,0,Basic,3
48804649,how to perform a SELECT on a JSON column in mysql/mariaDB,"how to apply WHERE clause on JSON column to perform a SELECT query on a table which is having two columns (id Integer, attr JSON). The JSON is nested and in the filter condition there is only one key value pair of json is allowed. This key value pair can be anywhere in the Josn.
+----+-----------------------------------------------------------------
| id | attr                                                                                          
|
+----+-----------------------------------------------------------------
|  1 | {""id"":""0001"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55}                                         
|
|  2 | {""id"":""0002"",""type"":""donut"",""name"":""Cake"",""ppu"":0.55,""batters"":
       {""batter1"":100,""batter2"":200}} 
+----+-----------------------------------------------------------------
",<mysql><mariadb><mysql-python>,814,0,9,471,1,6,14,76,41449,0,18,3,19,2018-02-15 10:13,2018-02-15 12:12,2018-02-15 12:12,0,0,Basic,10
52657760,Android Room: How to Migrate Column Renaming?,"Issue
My app is crashing because I am not handling migration properly. I'm looking for a solution to migrate the name of 1 column in my table. 
In my project I a have a room table named 'content' with a Double attribute 'archivedCount'. In the latest version of the app the attribute archivedCount attribute is re-named to dismissCount, still as type Double.
Original Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var archiveCount: Double) : Parcelable {...}
New Content model
@Entity(tableName = ""content"")
data class Content(@PrimaryKey var id: String, var dismissCount: Double) : Parcelable {...}
Attempted Solution
After reading a Google Developer Advocate's explanation Understanding migrations with Room, I attempted her solution outlined in the post's section Migrations with complex schema changes which entails making a copy of the original table, deleting the old table, then renaming the newly created table.
With the following approach below there is a runtime error on this line: database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users""); because I already cleared my app's cache so the old table no longer exists. 
Can I update a single column without re-creating the entire table?
static final Migration MIGRATION_1_2 = new Migration(1, 2) {
@Override
public void migrate(SupportSQLiteDatabase database) {
    // Create the new table
    database.execSQL(
            ""CREATE TABLE content_new (id TEXT, dismissCount REAL, PRIMARY KEY(id))"");
    // Copy the data
    database.execSQL(""INSERT INTO content_new (id, dismissCount) SELECT id, archiveCount FROM users"");
    // Remove the old table
    database.execSQL(""DROP TABLE content"");
    // Change the table name to the correct one
    database.execSQL(""ALTER TABLE content_new RENAME TO content"");
  }
};
",<android><sql><android-sqlite><android-room>,1862,1,19,9836,12,72,137,58,12753,0,1581,2,19,2018-10-05 2:55,2018-10-05 7:01,2018-10-07 3:56,0,2,Basic,10
56355516,"How to resolve the ""psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist"" when running django unittests on Travis","I'm using Travis for CI/CD as part of my Django app, with a postgresql database. (Django 2.1.4)
The build consistently fails on Travis as soon as the tests run. I receive this error:
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
I have tried: makemigrations, migrate auth, migrate myapp, migrate --run-syncdb.  All of which fail with the same error.
The tests run locally with a sqlite3 database, and on a prod-like heroku environment with a postgresql database.
.travis.yml
...
before script:
-psql -c 'create database travis_ci_test;' -U postgres
services:
-postgresql
script:
-yes | python3 manage.py makemigrations
-python3 manage.py migrate auth
-python3 manage.py migrate --run-syncdb
-python3 manage.py tests test/unit_tests
settings.py
...
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'travis_ci_test',
        'USER': 'postgres',
        'PASSWORD': '',
        'HOST': 'localhost',
        }
    }
...
INSTALLED_APPS = [...
        'django.contrib.auth',
        ]
Here is the output from the failing build on Travis.  'migrate auth' is successful (I think this is the crucial line for auth_user : Applying auth.0001_initial... OK)
0.22s$ psql -c 'create database travis_ci_test;' -U postgres
CREATE DATABASE
1.50s$ yes | python3 manage.py makemigrations
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
No changes detected
The command ""yes | python3 manage.py makemigrations"" exited with 0.
1.68s$ python3 manage.py migrate auth
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'races.apps.RacesConfig', 'storages']
Operations to perform:
  Apply all migrations: auth
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length... OK
  Applying auth.0009_alter_user_last_name_max_length... OK
The command ""python3 manage.py migrate auth"" exited with 0.
1.57s$ python3 manage.py migrate --run-syncdb
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp.apps.MyAppConfig', 'storages']
Operations to perform:
  Synchronize unmigrated apps: messages, myapp, staticfiles, storages
  Apply all migrations: admin, auth, contenttypes, sessions
Synchronizing apps without migrations:
  Creating tables...
    Creating table myapp_model1
    Creating table myapp_model2
    Creating table myapp_model3
    Creating table myapp_model4
    Creating table myapp_model5
    Running deferred SQL...
Running migrations:
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying sessions.0001_initial... OK
The command ""python3 manage.py migrate --run-syncdb"" exited with 0.
1.40s$ python3 manage.py test tests/unit_tests
TEST_ENV...
AWS_INTEGRATION...
Databases ... {'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'travis_ci_test', 'USER': 'postgres', 'PASSWORD': '', 'HOST': 'localhost'}
Installed Apps ... ['django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'app.apps.MyAppConfig', 'storages']
Creating test database for alias 'default'...
Traceback (most recent call last):
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
psycopg2.errors.UndefinedTable: relation ""auth_user"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""manage.py"", line 10, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 381, in execute_from_command_line
    utility.execute()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 375, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 26, in run_from_argv
    super().run_from_argv(argv)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 316, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/test.py"", line 56, in handle
    failures = test_runner.run_tests(test_labels)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 604, in run_tests
    old_config = self.setup_databases()
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/runner.py"", line 551, in setup_databases
    self.parallel, **kwargs
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/test/utils.py"", line 174, in setup_databases
    serialize=connection.settings_dict.get('TEST', {}).get('SERIALIZE', True),
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/creation.py"", line 68, in create_test_db
    run_syncdb=True,
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/__init__.py"", line 148, in call_command
    return command.execute(*args, **defaults)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 353, in execute
    output = self.handle(*args, **options)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/base.py"", line 83, in wrapped
    res = handle_func(*args, **kwargs)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 172, in handle
    self.sync_apps(connection, executor.loader.unmigrated_apps)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/core/management/commands/migrate.py"", line 310, in sync_apps
    self.stdout.write(""    Running deferred SQL...\n"")
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 106, in __exit__
    self.execute(sql)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/base/schema.py"", line 133, in execute
    cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 68, in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 77, in _execute_with_wrappers
    return executor(sql, params, many, context)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/travis/virtualenv/python3.6.7/lib/python3.6/site-packages/django/db/backends/utils.py"", line 85, in _execute
    return self.cursor.execute(sql, params)
django.db.utils.ProgrammingError: relation ""auth_user"" does not exist
The command ""python3 manage.py test tests/unit_tests"" exited with 1.
",<django><python-3.x><postgresql><travis-ci><psycopg2>,9076,0,142,519,1,5,18,81,36847,0,28,3,19,2019-05-29 7:49,2019-12-03 18:47,2019-12-03 18:47,188,188,Advanced,32
58085851,Access denied for user 'root'@'localhost' with mariadb 10.4.8 docker container using docker compose and issue while attaching external volume,"I am new to Docker, I was trying to crate docker container of mariadb for my application but when I start running mariadb container it shows Access denied for user 'root'@'localhost' (using password: YES) dockerfile 
Following is the docker compose I am using.
version: '3'
services:
  mysql:
    image: mariadb
    container_name: mariadb
    volumes:
      - dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_ROOT_USER: root
      MYSQL_USER: root
      MYSQL_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  dbvolume:
After trying for multiple times by referring few links I was able to connect my application to docker container but it failed to import AppDatabase.sql script at the time of creating docker container.
But now by using same docker compose file I am not able to connect mariadb to my application and I think even it's not importing SQL script to the database (based on logs I have observed).
Following is the docker log generated while running docker compose:
$ docker logs 3fde358ff015
2019-09-24 17:40:37 0 [Note] mysqld (mysqld 10.4.8-MariaDB-1:10.4.8+maria~bionic) starting as process 1 ...
2019-09-24 17:40:37 0 [Note] InnoDB: Using Linux native AIO
2019-09-24 17:40:37 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2019-09-24 17:40:37 0 [Note] InnoDB: Uses event mutexes
2019-09-24 17:40:37 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
2019-09-24 17:40:37 0 [Note] InnoDB: Number of pools: 1
2019-09-24 17:40:37 0 [Note] InnoDB: Using SSE2 crc32 instructions
2019-09-24 17:40:37 0 [Note] mysqld: O_TMPFILE is not supported on /tmp (disabling future attempts)
2019-09-24 17:40:37 0 [Note] InnoDB: Initializing buffer pool, total size = 256M, instances = 1, chunk size = 128M
2019-09-24 17:40:37 0 [Note] InnoDB: Completed initialization of buffer pool
2019-09-24 17:40:37 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2019-09-24 17:40:37 0 [Note] InnoDB: Upgrading redo log: 2*50331648 bytes; LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: Starting to delete and rewrite log files.
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile101 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Setting log file ./ib_logfile1 size to 50331648 bytes
2019-09-24 17:40:38 0 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0
2019-09-24 17:40:38 0 [Note] InnoDB: New log files created, LSN=21810033
2019-09-24 17:40:38 0 [Note] InnoDB: 128 out of 128 rollback segments are active.
2019-09-24 17:40:38 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2019-09-24 17:40:38 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2019-09-24 17:40:38 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2019-09-24 17:40:38 0 [Note] InnoDB: Waiting for purge to start
2019-09-24 17:40:38 0 [Note] InnoDB: 10.4.8 started; log sequence number 21810033; transaction id 14620
2019-09-24 17:40:38 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2019-09-24 17:40:38 0 [Note] Plugin 'FEEDBACK' is disabled.
2019-09-24 17:40:38 0 [Note] Server socket created on IP: '::'.
2019-09-24 17:40:38 0 [Warning] 'proxies_priv' entry '@% root@c980daa43351' ignored in --skip-name-resolve mode.
2019-09-24 17:40:38 0 [Note] InnoDB: Buffer pool(s) load completed at 190924 17:40:38
2019-09-24 17:40:38 0 [Note] Reading of all Master_info entries succeeded
2019-09-24 17:40:38 0 [Note] Added new Master_info '' to hash table
2019-09-24 17:40:38 0 [Note] mysqld: ready for connections.
Version: '10.4.8-MariaDB-1:10.4.8+maria~bionic'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  mariadb.org binary distribution
SQL Script I am trying to import:
create database appdata;
use appdata;
CREATE TABLE `appdatadetails` (
  `Name` varchar(8) NOT NULL,
  `appIndex` int(11) NOT NULL,
  `connector` varchar(16) DEFAULT NULL,
  `intName` varchar(12) DEFAULT NULL,
  `intIndex` int(11) DEFAULT NULL,
  PRIMARY KEY (`Name`,`appIndex`)
) 
Please help me to understand what I am doing wrong, I have tried all possible solution posted on different blogs. 
Update:
Latest Update:
I was able to up and running mariadb docker image with 10.1. But if I attach volume then still I am facing issue.
Docker Compose:
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - container-volume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
volumes:
  container-volume:
And the log error message, If I attach container-volume volume.
Creating mariadb ... done
Attaching to mariadb
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] mysqld (mysqld 10.1.41-MariaDB-1~bionic) starting as process 1 ...
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using mutexes to ref count buffer pool pages
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: The InnoDB memory heap is disabled
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Compressed tables use zlib 1.2.11
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using Linux native AIO
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Using SSE crc32 instructions
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Initializing buffer pool, size = 256.0M
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Completed initialization of buffer pool
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] InnoDB: Highest supported file format is Barracuda.
mariadb  | InnoDB: No valid checkpoint found.
mariadb  | InnoDB: A downgrade from MariaDB 10.2.2 or later is not supported.
mariadb  | InnoDB: If this error appears when you are creating an InnoDB database,
mariadb  | InnoDB: the problem may be that during an earlier attempt you managed
mariadb  | InnoDB: to create the InnoDB data files, but log file creation failed.
mariadb  | InnoDB: If that is the case, please refer to
mariadb  | InnoDB: http://dev.mysql.com/doc/refman/5.6/en/error-creating-innodb.html
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' init function returned error.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.
mariadb  | 2019-09-25  6:56:26 140542855440384 [Note] Plugin 'FEEDBACK' is disabled.
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Unknown/unsupported storage engine: InnoDB
mariadb  | 2019-09-25  6:56:26 140542855440384 [ERROR] Aborting
mariadb  | 
mariadb exited with code 1
If I remove container-volume then It is importing .sql script and running well and good.
Updated with working script: Before I was using mariadb 10.4.8 or latest and facing issue(s) to access DB and attaching external volume.
Now I have downgraded (As suggested by @Adiii) and tried. It runs perfectlly and we no need to specify external: true in volumes service
version: '3'
services:
  mysql:
    image: mariadb:10.1
    container_name: mariadb
    volumes:
      - ./dbvolume:/var/lib/mysql
      - ./AppDatabase.sql:/docker-entrypoint-initdb.d/AppDatabase.sql
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: appdata
    ports:
      - ""3333:3306""
",<mysql><docker><docker-compose><mariadb>,7774,2,122,1863,6,41,58,80,40726,0,101,1,19,2019-09-24 17:58,2019-09-24 18:13,2019-09-24 18:13,0,0,Advanced,32
53885511,"pq: sorry, too many clients already","I am getting pq: sorry, too many clients already error when I am calling the GetMessages() multiple times. 
Please find the updated code:
main() code
func main() {
  dbConn, err := InitDB()
  if err != nil {
    Log.Error(""Connection Error: "", err.Error())
    return
  }
  defer dbConn.Close()
  go run()
  var input string
  fmt.Scanln(&amp;input)
}
Database connection code is:
func InitDB()(*sql.DB, error) {
  connectionString := fmt.Sprintf(""user=%v password='%v' dbname=%v sslmode=disable"", USER, PASSWORD, DATABASE)
  db, err = sql.Open(DRIVER, connectionString)
  return db, err
}
run goroutine:
func run() {
  for {
    messages, err := GetMessages()
    if err != nil {
      Log.Error(""Connection Error: "", err.Error())
      return
    }
    log.Info(messages)
  }
}
GetMessages() function code: 
func GetMessages() (messages []string, err error) {
    rows, err := db.Query(`SELECT message1, message2, message3, message4, message5,
            message6, message7, message8, message9, message10, message11, message12, message13, 
            message14, message15, message16, message17, message18, message19, message20, message21,
            message22, message23, message24, message25, message26, message27, message28, message29,
            message30, message31, message32, message33, message34, message35, message36, message37,
            message38, message39, message40, message41, message42, message43, message44, message45,
            message46, message47, message48 FROM table1 WHERE id=1`)
    if err != nil {
        Log.Error(""Query error"", err)
        return messages, err
    }
    var pointers []interface{}
    defer rows.Close()
    for rows.Next() {
        pointers = make([]interface{}, 48)
        messages = make([]string, 48)
        for i, _ := range pointers {
            pointers[i] = &amp;messages[i]
        }
        err = rows.Scan(pointers...)
        if err != nil {
            Log.Error(""Failed to scan row"", err)
            return messages, err
        }
    }
    return messages, nil
}
I checked this answer and I have used scan but still it isn't working
UPDATE
Issue was in another function. I was using db.Query without closing the returned rows object and was repeatedly calling that function. I've updated my code; used db.Exec instead of db.Query and it's working now. Thank you so much @mkopriva for this answer. :)
",<postgresql><go>,2375,3,68,1034,4,17,27,80,18829,0,828,2,19,2018-12-21 13:21,2021-08-24 18:13,,977,,Advanced,32
51109440,Parameter must be an array or an object that implements Countable in phpmyadmin,"When I try and view the wp_posts table in phpmyadmin, I see this error message, but have no idea what it means and have never seen it before.
Can someone help me try and get rid of this somehow?
Warning in ./libraries/sql.lib.php#613
count(): Parameter must be an array or an object that implements Countable
Backtrace
./libraries/sql.lib.php#2128: PMA_isRememberSortingOrder(array)
./libraries/sql.lib.php#2079: PMA_executeQueryAndGetQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
./sql.php#221: PMA_executeQueryAndSendQueryResponse(
array,
boolean true,
string 'afterhours',
string 'wp_posts',
NULL,
NULL,
NULL,
NULL,
NULL,
NULL,
string '',
string './themes/original/img/',
NULL,
NULL,
NULL,
string 'SELECT * FROM `wp_posts`',
NULL,
NULL,
)
",<sql><wordpress>,919,0,46,4196,7,25,71,59,80218,0,142,2,19,2018-06-29 21:10,2018-06-29 22:16,,0,,Advanced,32
53885749,Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type,"I have a DB which has a custom data type FollowEntityType as a column.
@Entity(primaryKeys = arrayOf(&quot;id&quot;, &quot;type&quot;), tableName = &quot;follow_info&quot;)
data class FollowInfoEntity(
        @ColumnInfo(name = &quot;id&quot;) var id: String,
        @ColumnInfo(name = &quot;type&quot;) var type: FollowEntityType,
)
Since it is a custom data type, I have defined a type converter.
class FollowDatabaseTypeConverter {
    @TypeConverter
    fun toFollowEntity(entityType: String?): FollowEntityType? {
        return FollowEntityType.from(entityType ?: Constants.EMPTY_STRING)
    }
    @TypeConverter
    fun toString(entityType: FollowEntityType?): String? {
        return entityType?.name
    }
}
This works fine and I am able to store/retrieve values in the DB. However, in one of the queries, the build fails.
This is the query.
@Query(&quot;select * from follow_info where type in (:entityTypeList)&quot;)
fun getFollowedEntitiesByType(entityTypeList: List&lt;FollowEntityType&gt;) : List&lt;FollowInfoEntity&gt;
The build fails with the following error.
Query method parameters should either be a type that can be converted into a database column or a List / Array that contains such type. You can consider adding a Type Adapter for this.
    java.util.List&lt;? extends FollowEntityType&gt; entityTypeList, @org.jetbrains.annotations.NotNull()
The error is for entityTypeList field, and according to the error, this type should be one of the columns in the DB. I already have FollowEntityType as one of the column types. I don't understand why it is failing. Please help me out as I am not finding any solution to solve this problem.
",<android><sqlite><kotlin><android-room>,1662,0,22,7198,3,38,62,75,15646,0,359,7,19,2018-12-21 13:39,2021-06-20 5:31,,912,,Basic,7
57008781,How to execute large amount of sql queries asynchronous and in threads,"Problem: I have huge amount of sql queries (around 10k-20k) and I want to run them asynchronous in 50 (or more) threads. 
I wrote a powershell script for this job, but it is very slow (It took about 20 hours to execute all). Desired result is 3-4 hours max.
Question: How can I optimize this powershell script? Should I reconsider and use another technology like python or c#?
I think it's powershell issue, because when I check with whoisactive the queries are executing fast. Creating, exiting and unloading jobs takes a lot of time, because for each thread is created separate PS instances.
My code:
$NumberOfParallerThreads = 50;
$Arr_AllQueries = @('Exec [mystoredproc] @param1=1, @param2=2',
                    'Exec [mystoredproc] @param1=11, @param2=22',
                    'Exec [mystoredproc] @param1=111, @param2=222')
#Creating the batches
$counter = [pscustomobject] @{ Value = 0 };
$Batches_AllQueries = $Arr_AllQueries | Group-Object -Property { 
    [math]::Floor($counter.Value++ / $NumberOfParallerThreads) 
};
forEach ($item in $Batches_AllQueries) {
    $tmpBatch = $item.Group;
    $tmpBatch | % {
        $ScriptBlock = {
            # accept the loop variable across the job-context barrier
            param($query) 
            # Execute a command
            Try 
            {
                Write-Host ""[processing '$query']""
                $objConnection = New-Object System.Data.SqlClient.SqlConnection;
                $objConnection.ConnectionString = 'Data Source=...';
                $ObjCmd = New-Object System.Data.SqlClient.SqlCommand;
                $ObjCmd.CommandText = $query;
                $ObjCmd.Connection = $objConnection;
                $ObjCmd.CommandTimeout = 0;
                $objAdapter = New-Object System.Data.SqlClient.SqlDataAdapter;
                $objAdapter.SelectCommand = $ObjCmd;
                $objDataTable = New-Object System.Data.DataTable;
                $objAdapter.Fill($objDataTable)  | Out-Null;
                $objConnection.Close();
                $objConnection = $null;
            } 
            Catch 
            { 
                $ErrorMessage = $_.Exception.Message
                $FailedItem = $_.Exception.ItemName
                Write-Host ""[Error processing: $($query)]"" -BackgroundColor Red;
                Write-Host $ErrorMessage 
            }
        }
        # pass the loop variable across the job-context barrier
        Start-Job $ScriptBlock -ArgumentList $_ | Out-Null
    }
    # Wait for all to complete
    While (Get-Job -State ""Running"") { Start-Sleep 2 }
    # Display output from all jobs
    Get-Job | Receive-Job | Out-Null
    # Cleanup
    Remove-Job *
}
UPDATE:
Resources: The DB server is on a remote machine with: 
24GB RAM, 
8 cores, 
500GB Storage, 
SQL Server 2016
We want to use the maximum cpu power.
Framework limitation: The only limitation is not to use SQL Server to execute the queries. The requests should come from outside source like: Powershell, C#, Python, etc. 
",<sql><multithreading><powershell><asynchronous><parallel-processing>,3007,0,69,1731,2,27,42,46,5273,0,1616,6,19,2019-07-12 14:14,2019-07-13 7:37,2019-07-25 13:37,1,13,Intermediate,23
57193597,Mocking a Sqlalchemy session for pytest,"I don't know if this can be done but I'm trying to mock my db.session.save.
I'm using flask and flask-alchemy.
db.py
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
The unit test
def test_post(self):
    with app.app_context():
        with app.test_client() as client:
            with mock.patch('models.db.session.save') as mock_save:
                with mock.patch('models.db.session.commit') as mock_commit:
                    data = self.gen_legend_data()
                    response = client.post('/legends', data=json.dumps([data]), headers=access_header)
                    assert response.status_code == 200
                    mock_save.assert_called()
                    mock_commit.assert_called_once()
And the method:
def post(cls):
    legends = schemas.Legends(many=True).load(request.get_json())
    for legend in legends:
        db.session.add(legend)
    db.session.commit()
    return {'message': 'legends saved'}, 200
I'm trying to mock the db.session.add and db.session.commit. I've tried db.session.save and legends.models.db.session.save and models.db.session.save. They all came back with the save error:
ModuleNotFoundError: No module named 'models.db.session'; 'models.db' is not a package
I don't get the error and I'm not sure how to solve it.
Or am I doing something that is totally wrong in wanting to mock a db.session?
Thanks.
Desmond
",<python><sqlalchemy><mocking><flask-sqlalchemy><pytest>,1384,0,28,755,2,7,26,76,32620,0,8,1,19,2019-07-25 2:44,2019-07-25 14:19,2019-07-25 14:19,0,0,Intermediate,30
49890142,Is it possible to change the background color of the Object Explorer and the Result Menu in SQL Server Management Studio 2017,"Pretty much what the title says. Just started learning SQL server. I found how to 'unlock' the dark theme and how to change the fonts and sizes but i still can't change the color in the Object Explorer and the Result Menu in SQL Server Management Studio 2017. Is it possible that this is all we got till now... Cause it is hard for me to believe that there is no working full dark theme for SQL Server Management Studio 2017 or the possibility to change it manualy. Any help would be much appreciated. Also is there another IDE or editor for SQL server 2017 other than SQL Server Management Studio 2017, like there are a LOT diffirent editors for programing languages like Sublime, NotePad++, VS Code etc...
",<sql-server>,708,0,0,973,4,15,33,44,20862,,65,3,19,2018-04-18 2:16,2020-10-12 8:56,2020-10-12 8:56,908,908,Basic,14
50643007,What is the difference between fetch Next and fetch First in the Order By [...] OFFSET [..] FETCH [...] clause?,"I executed two sample queries on AdventureWorks2016 and gave me the same result.
When should I use NEXT or FIRST keyword then?
select LastName + ' ' + FirstName 
from person.person
order by LastName asc OFFSET 10 rows **Fetch next** 10 rows only
select LastName + ' ' + FirstName
from person.person
order by LastName asc OFFSET 10 rows **Fetch first** 10 rows only
",<sql><sql-server>,365,0,7,973,3,12,26,59,10751,,163,2,19,2018-06-01 12:02,2018-06-01 12:12,2018-06-01 12:12,0,0,Basic,10
49198831,sqlite3.dylib: illegal multi-threaded access to database connection,"I have an iOS app that uses sqlite3 and I'm facing issues with multi-threading crashing the app with the illegal multi-threaded access to database connection message. Of course, it's because I'm using multi-threading; the problem is, my sqlite3 instance is configured to use multi-thread:
sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
Even though I'm using multi-threading (sqlite3 build was also compiled with the multi-threading flag), it causes my app to crash when multiple threads write or read the database simultaneously.
Crash report
Application Specific Information:
BUG IN CLIENT OF sqlite3.dylib: illegal multi-threaded access to database connection
Exception Type:  EXC_BREAKPOINT (SIGTRAP)
Exception Codes: 0x0000000000000001, 0x00000001823ed2fc
Termination Signal: Trace/BPT trap: 5
Termination Reason: Namespace SIGNAL, Code 0x5
Terminating Process: exc handler [0]
Triggered by Thread:  12
Thread 12 Crashed:
0   libsqlite3.dylib                0x00000001823ed2fc sqlite3MutexMisuseAssert + 144 (sqlite3.c:23788)
1   libsqlite3.dylib                0x00000001823ed2ec sqlite3MutexMisuseAssert + 128 (once.h:84)
2   libsqlite3.dylib                0x000000018235248c sqlite3LockAndPrepare + 320 (sqlite3.c:23801)
3   MyCodeCall.m ...........
I've been struggling with this issue for a while and I couldn't find any reference to this on google unfortunately.
UPDATE
+(sqlite3*) getInstance {
  if (instance == NULL) {
    sqlite3_shutdown();
    sqlite3_config(SQLITE_CONFIG_MULTITHREAD);
    sqlite3_initialize();
    NSLog(@&quot;isThreadSafe %d&quot;, sqlite3_threadsafe());
    const char *path = [@&quot;./path/to/db/db.sqlite&quot; cStringUsingEncoding:NSUTF8StringEncoding];
    if (sqlite3_open_v2(path, &amp;database, SQLITE_OPEN_READWRITE|SQLITE_OPEN_CREATE, NULL) != SQLITE_OK) {
      NSLog(@&quot;Database opening failed!&quot;);
    }
  }
  return instance;
}
",<ios><sqlite>,1885,0,36,9650,3,25,40,79,10254,0,79,4,19,2018-03-09 17:11,2018-03-12 12:08,2018-03-12 12:08,3,3,Intermediate,23
48112363,Rails migration - add unique index allowing skipping null values,"I wanted like to add unique constraint to column name in existing table psql
created table (PostgreSQL) :
class CreateRequests &lt; ActiveRecord::Migration
  def change
    create_table :requests do |t|
      t.integer :user_id
      t.string :name
      t.string :address
      t.timestamps null: true
    end
    add_index :requests, :user_id
  end
end
So I added validation uniqueness in model
class Request &lt; ModelBase
  belongs_to :user
  validates :user, presence: true
  validates :name, uniqueness: true, allow_blank: true
  ...
and migration like this:
def change
    add_index :user_requests, :name, unique: true
end
but I noticed that in some cases name can be empty can I add_index with condition were :name is null / not null?
Edit: yes, I want those empty values to stay in the database (I don't need to modify past request). I think that I need to edit migration to be more accurate to the actual state of db.
",<ruby-on-rails><psql>,928,0,21,1456,1,15,29,57,9766,0,70,5,19,2018-01-05 11:09,2018-01-05 11:15,2018-01-08 9:23,0,3,Intermediate,23
54300263,Connect to AWS RDS Postgres database with python,"I have an existing postgres table in RDS with a database name my-rds-table-name
I've connected to it using pgAdmin4 with the following configs of a read-only user:
host_name = ""my-rds-table-name.123456.us-east-1.rds.amazonaws.com""
user_name = ""my_user_name""
password = ""abc123def345""
I have verified that I can query against the table.
However, I cannot connect to it using python:
SQLAlchemy==1.2.16
psycopg2-binary==2.7.6.1
mysqlclient==1.4.1
With:
import psycopg2
engine = psycopg2.connect(
    database=""my-rds-table-name"",
    user=""my_user_name"",
    password=""abc123def345"",
    host=""my-rds-table-name.123456.us-east-1.rds.amazonaws.com"",
    port='5432'
)
It fails with 
psycopg2.OperationalError: FATAL:  database ""my-rds-table-name"" does not exist
Similarly, if I try to connect to it with sqlalchemy:
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL:  database ""my-rds-table-name"" does not exist
What am I missing?
",<python-3.x><postgresql><amazon-web-services><amazon-rds>,945,0,17,8955,10,64,103,38,36512,0,2868,2,19,2019-01-22 2:06,2019-01-22 17:54,2019-01-22 17:54,0,0,Basic,3
49361286,Unittesting with Pyspark: unclosed socket warnings,"I want to do unittesting with PySpark. The tests itself work, however for each test I get
ResourceWarning: unclosed &lt;socket.socket [...]&gt; and
ResourceWarning: unclosed file &lt;_io.BufferedWriter [...]&gt; warnings and
DeprecationWarnings regarding invalid escape sequences. 
I'd like to understand why / how to solve this to not clutter my unittest output with these warnings.
Here is a MWE:
# filename: pyspark_unittesting.py
# -*- coding: utf-8 -*-
import unittest
def insert_and_collect(val_in):
    from pyspark.sql import SparkSession
    with SparkSession.builder.getOrCreate() as spark:
        col = 'column_x'
        df = spark.createDataFrame([(val_in,)], [col])
        print('one')
        print(df.count())
        print('two')
        collected = df.collect()
        print('three')
        return collected[0][col]
class MyTest(unittest.TestCase):
    def test(self):
        val = 1
        self.assertEqual(insert_and_collect(val), val)
        print('four')
if __name__ == '__main__':
    val = 1
    print('inserted and collected is equal to original: {}'
          .format(insert_and_collect(val) == val))
    print('five')
If I call this with python pyspark_unittesting.py the output is:
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
one
1  
two
three
inserted and collected is equal to original: True
five
If I call this with python -m unittest pyspark_unittesting however, the output is:
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:1890: DeprecationWarning: invalid escape sequence \*
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:398: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/readwriter.py:759: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/streaming.py:618: DeprecationWarning: invalid escape sequence \`
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
/opt/spark/current/python/lib/pyspark.zip/pyspark/sql/functions.py:1519: DeprecationWarning: invalid escape sequence \d
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
/usr/lib/python3.6/subprocess.py:766: ResourceWarning: subprocess 10219 is still running
  ResourceWarning, source=self)
/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__
  return f(*args, **kwds)
one
1                                                                               
two
/usr/lib/python3.6/socket.py:657: ResourceWarning: unclosed &lt;socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49330), raddr=('127.0.0.1', 44169)&gt;
  self._sock = None
three
four
.
----------------------------------------------------------------------
Ran 1 test in 7.394s
OK
sys:1: ResourceWarning: unclosed file &lt;_io.BufferedWriter name=5&gt;
Edit 2018-03-29
Regarding @acue's answer, I tried out calling the script using subprocess.Popen - very much like it is done within the unittest module:
In [1]: import pathlib
      : import subprocess
      : import sys
      : 
      : here = pathlib.Path('.').absolute()
      : args = [sys.executable, str(here / 'pyspark_unittesting.py')]
      : opts = dict(stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd='/tmp')
      : p = subprocess.Popen(args, **opts)
      : out, err = [b.splitlines() for b in p.communicate()]
      : print(out)
      : print(err)
      : 
      : 
[b'one',
 b'1',
 b'two',
 b'three',
 b'inserted and collected is equal to original: True',
 b'five']
[b""Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties"",
 b'Setting default log level to ""WARN"".',
 b'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',
 b'',
 b'[Stage 0:&gt;                                                          (0 + 0) / 8]',
 b'[Stage 0:&gt;                                                          (0 + 8) / 8]',
 b'                                                                                ']
The Resource Warnings do not appear...
",<python><python-3.x><pyspark><python-unittest><apache-spark-sql>,5017,0,106,1671,3,19,34,80,3559,0,100,1,19,2018-03-19 10:57,2020-12-13 11:38,,1000,,Intermediate,30
55685341,"Django testing: Got an error creating the test database: database ""database_name"" already exists","I have a problem with testing. It's my first time writing tests and I have a problem.
I just created a test folder inside my app users, and test_urls.py for testing the urls.
When I type:
python manage.py test users
It says:
  Creating test database for alias 'default'... Got an error creating
  the test database: database ""database_name"" already exists
  Type 'yes' if you would like to try deleting the test database
  'database_name', or 'no' to cancel:
What does it mean? What happens if I type yes? Do I lose all my data in database?
",<python><django><database><postgresql>,541,0,3,400,1,8,17,37,19713,0,42,2,19,2019-04-15 8:37,2019-04-15 8:41,2019-04-15 8:41,0,0,Intermediate,30
50450162,"PostgreSQL ""tuple already updated by self""","Our database seems to be broken, normally it uses about 1-2% of cpu, but if we run some additional backend services making UPDATE and INSERT queries for 10M rows table (about 1 query per 3 second) everything is going to hell (including CPU increase from 2% to 98% usage).
We have decided to debug what's going on, run VACUUM and ANALYZE to learn what's wrong with db but...
production=# ANALYZE VERBOSE users_user;
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
ERROR:  tuple already updated by self
we are not able to finish ANALYZE on ANY of the tables and could not find any information about this issue. Any suggestions what can be wrong?
 PostgreSQL 9.6.8 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16), 64-bit
Additional info as requested in comments:
  Maybe you have a corrupted pg_class
SELECT * FROM pg_class WHERE relname = 'users_user';
Output: https://pastebin.com/WhmkH34U
  So the first thing to do would be to kick out all other sessions and
  try again
There are no additional sessions, we have dumped the whole DB on the new testing server, issue still occur, there are no clients connected to this DB
",<postgresql><postgresql-9.6>,1462,2,8,925,1,9,12,55,3858,,12,1,19,2018-05-21 13:37,2018-05-29 17:16,,8,,Intermediate,23
52344453,"""Loading class com.mysql.jdbc.Driver ... is deprecated"" message","I got an error
Loading class com.mysql.jdbc.Driver. This is deprecated. The new
driver class is com.mysql.cj.jdbc.Driver. The driver is
automatically registered via the SPI and manual loading of the driver
class is generally unnecessary.
Could someone explain why?
",<java><mysql><jdbc>,265,0,0,383,2,5,14,58,26754,0,15,2,19,2018-09-15 11:52,2018-09-15 12:43,2018-09-15 12:43,0,0,Advanced,38
50421670,Mysql ERROR : not connected,"I am trying to connect to MySQL database from MySQL shell on windows. 
No matter what I type in MySQL shell, it keeps giving me error : 'Not connected'. 
Query eg 1 : mysql --host=localhost --port=3306 --user=root -p;
Query eg 2 : mysql -u root -p
O/P : ERROR: Not connected
I have MySQL server installed on my machine. Also MySQL service is running in the background.
Also, I was able to connect from MySQL workbench. 
ERROR MESSAGE
MySQL Workbench Connection
",<mysql><sql><database><command-line><mysql-workbench>,461,2,3,827,1,6,11,63,72960,0,18,5,19,2018-05-19 4:07,2018-05-19 4:24,2018-05-19 6:31,0,0,Intermediate,30
48558005,PostgreSQL 9.6. Issue dropping index,"In a legacy PostgreSQL DB I tried to drop an existing index issuing the command:
DROP INDEX testing.idx_testing_data_model_output_data_id;
and see the error:
ERROR:  index ""&lt;index name&gt;"" does not exist
But I can see the index using the \d &lt;table name&gt; command:
leg=# \d testing.data_model
                                           Table ""testing.data_model""
     Column     |            Type             |                                 Modifiers
----------------+-----------------------------+---------------------------------------------------------------------------
 id             | bigint                      | not null default nextval('testing.data_model_id_seq'::regclass) 
 input_data     | text                        | 
 output_data_id | bigint                      | 
Indexes:
    ""pk_testing_data_model"" PRIMARY KEY, btree (id)
    ""idx_testing_data_model_output_data_id"" btree (output_data_id)
Ok, when I try to create the index I receive the following error:
ERROR:  relation ""&lt;index name&gt;"" already exists
It seems that somehow the index creation or index dropping was not successfully complete. How can I resolve this issue?
",<postgresql><ddl>,1162,0,14,737,1,11,31,59,9138,,339,1,19,2018-02-01 8:07,2022-03-17 12:06,,1505,,Advanced,32
51149902,Sql Table data type for email address?,"What data type should I use for an email? Just started to learn SQL, and I tried to make some columns, here's table for ID, Username, Password, Money, and Email.
Did I make that correctly?
",<sql><sql-server><database>,189,1,0,223,1,2,15,67,85221,,12,4,19,2018-07-03 8:25,2018-07-03 8:38,2018-07-03 9:41,0,0,Basic,4
57978671,SSMS crashes when try to modify database diagram (v18.2),"When I try to modify a database diagram created before the application restart and crashes when trying to access.
It happen only when I save the diagram and close the application. When I try to reopen it throws me an error then restart the SSMS.
I'm running SQL Server 14.0.100 Express Edition.
I reviewed the Microsoft Event Viewer and I get this:
  Faulting application name: Ssms.exe, version: 2019.150.18142.0, time stamp: 0x5d3573be
  Faulting module name: DataDesigners.dll, version: 2019.150.18142.0, time stamp: 0x5d3573f0
  Exception code: 0xc0000005
  Fault offset: 0x00004be8
  Faulting process id: 0x5ec8
  Faulting application start time: 0x01d56d761e232f6c
  Faulting application path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Ssms.exe
  Faulting module path: C:\Program Files (x86)\Microsoft SQL Server Management Studio 18\Common7\IDE\Tools\VDT\DataDesigners.dll
  Report Id: e797c8be-6448-4547-9f6f-146cd92d8178
  Faulting package full name: 
  Faulting package-relative application ID: 
",<sql-server><database><ssms><diagram><designer>,1041,0,0,781,1,5,22,50,12240,0,223,2,19,2019-09-17 16:45,2019-09-20 1:02,2019-09-20 1:02,3,3,Advanced,32
50003906,Storing UUID as string in mysql using JPA,"I came across a blog of using UUID with Hibernate and MySql. Now the problem is, whenever I take a look at the database the ID's will be non-readable format (binary-16). How can I store UUID as a readable format like 7feb24af-fc38-44de-bc38-04defc3804fe instead of ¡7ôáßEN¹º}ÅÑs
I was using this code 
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""BINARY(16)"" )
private UUID id;
And the result is ¡7ôáßEN¹º}ÅÑs. But I want it as readable UUID so I used the following code which didn't help me
@Id
@GeneratedValue( generator = ""uuid2"" )
@GenericGenerator( name = ""uuid2"", strategy = ""uuid2"" )
@Column( name = ""id"", columnDefinition = ""CHAR(32)"" )
private UUID id;
How to save the UUID as a string instead of binary(16) without changing the java type UUID
",<java><mysql><hibernate><spring-data-jpa><uuid>,851,0,12,810,2,7,23,53,27624,0,57,4,19,2018-04-24 14:13,2018-07-19 16:42,2018-07-19 16:42,86,86,Basic,10
48576847,"How to combine first name, middle name and last name in SQL server","you can refer the below queries to get the same-
1
select FirstName +' '+ MiddleName +' ' + Lastname as Name from TableName.
2
select CONCAT(FirstName , ' ' , MiddleName , ' ' , Lastname) as Name from 
  TableName
3
select Isnull(FirstName,' ') +' '+ Isnull(MiddleName,' ')+' '+ Isnull(Lastname,' ') 
from TableName.
Note: Point 1 query return if all columns have some value if anyone is null or empty then it will return null for all, means Name will return &quot;NULL&quot; value.
To avoid the point number 1, you can use point number 2 or point number 3 -
We can use IsNull or CONCAT keyword to get the same.
If anyone containing null value then ' ' (blank space) will add with next value.
",<sql-server>,693,0,7,301,1,3,8,63,199946,0,4,15,19,2018-02-02 6:24,2018-02-02 6:28,,0,,Basic,10
50346326,"ProgrammingError: relation ""django_session"" does not exist","Got this error after changing my database from sqlite to postgresql. I've made all my settings changes: 
Here's my settings:
DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.postgresql_psycopg2"",
        'NAME': ""postr1"",
        'USER': ""zorgan"",
        'PASSWORD': config('DB_PASSWORD'),
        'HOST': ""localhost"",
        'PORT': '',
    }
}
as well as performing makemigrations and migrations which were all successful. So I'm able to succesfully start my local server:
System check identified no issues (0 silenced).
May 15, 2018 - 08:59:39
Django version 1.11.8, using settings 'draft1.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
however when I go to the site it returns this error:
ProgrammingError at /news/
relation ""django_session"" does not exist
LINE 1: ...ession_data"", ""django_session"".""expire_date"" FROM ""django_se...
Any idea what the problem is?
",<python><django><postgresql>,937,1,20,8357,27,108,215,59,31004,0,258,3,19,2018-05-15 9:09,2018-05-15 9:33,2018-05-15 9:33,0,0,Basic,10
55018986,Postgresql select count query takes long time,"I have a table named events in my Postgresql 9.5 database. And this table has about 6 million records.
I am runnig a select count(event_id) from events query. But this query takes 40seconds. This is very long time for a database. My event_id field of table is primary key and indexed. Why this takes very long time? (Server is ubuntu vm on vmware has 4cpu)
Explain:
""Aggregate  (cost=826305.19..826305.20 rows=1 width=0) (actual time=24739.306..24739.306 rows=1 loops=1)""
""  Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""  -&gt;  Seq Scan on event_source  (cost=0.00..812594.55 rows=5484255 width=0) (actual time=0.014..24087.050 rows=6320689 loops=1)""
""        Buffers: shared hit=13 read=757739 dirtied=53 written=48""
""Planning time: 0.369 ms""
""Execution time: 24739.364 ms""
",<sql><postgresql><postgresql-9.5>,790,0,8,6499,14,83,181,79,17848,0,141,2,19,2019-03-06 8:53,2019-03-06 11:24,,0,,Intermediate,23
57048744,Checking Concurrency on an Entity without updating the Row Version,"I have a parent entity that I need to do a concurrency check (as annotated as below) 
[Timestamp]
public byte[] RowVersion { get; set; }
I have a bunch of client processes that access readonly values out of this parent entity and primarily update its child entities.
The constraint
Clients should not interfere with each other's work, (e.g. updating child records should not throw a concurrency exception on the parent entity). 
I have a server process that does update this parent entity, and in this case the client process needs to throw if the parent entity has been changed. 
 Note : The client's concurrency check is sacrificial, the server's workflow is mission critical. 
The problem
I need to check (from the client process) if the parent entity has changed without updating the parents entity's row version. 
It's easy enough to do a concurrency check on the parent entity in EF:
// Update the row version's original value
_db.Entry(dbManifest)
      .Property(b =&gt; b.RowVersion)
      .OriginalValue = dbManifest.RowVersion; // the row version the client originally read
// Mark the row version as modified
_db.Entry(dbManifest)
       .Property(x =&gt; x.RowVersion)
       .IsModified = true;
The IsModified = true is the deal breaker because it forces the row version to change. Or, said in context, this check from the client process will cause a row version change in the parent entity, which interferes needlessly with the other client processes' workflows.
 A work around : I could potentially wrap the SaveChanges from the client process in a Transaction and then a subsequent read of the parent entity's row version, in-turn, rolling back if the row version has changed.
Summary 
Is there an out-of-the-box way with Entity Framework where I can SaveChanges (in the client process for the child entities) yet also check if the parent entity's row version has changed (without updating the parent entities row version).
",<c#><sql-server><entity-framework><entity-framework-6><database-concurrency>,1941,0,14,79780,9,103,142,41,5504,0,2181,3,19,2019-07-16 0:48,2019-07-20 20:10,2019-07-23 14:01,4,7,Advanced,32
59710811,NOT LIKE ANY query in Snowflake,"I am trying to run the following query in Snowflake:
SELECT * FROM chapters 
WHERE
title NOT LIKE ANY ('Summary%', 'Appendix%')
but it errors out. I know Snowflake support LIKE ANY query syntax. But I am not sure why my query is not working.
",<sql-like><snowflake-cloud-data-platform>,242,0,3,3973,11,58,101,52,29218,0,394,3,19,2020-01-13 5:04,2020-01-13 5:29,2020-01-13 5:29,0,0,Basic,9
50546500,How to connect to docker mysql container on remote machine,"I have two machines. My machine with IP1(Europe), and other machine with public IP2(USA). On IP2 I have mysql container running with volume /var/lib/mysql set to be replicated in some folder on the host machine ~/mysqldatabase. Firewall rule for port 3306 is added. Also I have ssh connection to the machine. So I'm not sure where to start. Usually when there is not docker I add just 
bind-address = 0.0.0.0
as configuration in mysql and I open the firewall port 3306 (or an other one that points to mysql) and the things work.
So probably I can install mysql-server package (the host is ubuntu16.04) outside of docker on the IP2 machine and to set it to point to the ~/mysqldatabase folder, but is it really necessary to do that? 
Is there way to connect directly from IP1 to IP2:mysql_container:mysql_database
I run the mysql docker container in two ways. One is with docker file. And the other one is with systemctl service.
Part of the docker-compose.yml:
version: ""3""
services:
  mysql:
    image: mysql:5.7
    volumes:
      - /home/username/mysqldatabase:/var/lib/mysql
    ports:
      - '3306:3306'
    environment:
        MYSQL_ROOT_PASSWORD: rootpass
        MYSQL_DATABASE: somedb
        MYSQL_USER: someuser
        MYSQL_PASSWORD: someuserpassword
mysql.service 
[Unit]
Description=Run %p
Requires=docker.service
After=docker.service
[Service]
Restart=always
ExecStartPre=-/usr/bin/docker kill %p
ExecStartPre=-/usr/bin/docker rm -f %p
docker run --rm --name mysql -v /home/username/mysqldatabase:/var/lib/mysql -p 3306:3306 \
-e MYSQL_ROOT_PASSWORD=rootpass -e MYSQL_DATABASE=somedb -e MYSQL_USER=someuser -e MYSQL_PASSWORD=someuserpassword \
mysql:5.7
ExecStop=/usr/bin/docker stop %p
[Install]
WantedBy=multi-user.target
To make the things more simple lets say that I use only the second approach.
I DON""T have :
1.mysql-client, mysql-server or mysql on the IP2 machine
2.and I haven't set anywhere to bind to 0.0.0.0 because I want to connnect directly to the mysql container. Probably I have to set it
to bind to 0.0.0.0 inside this container.
Result for the firewall 
sudo netstat -tunlp | grep 3306
tcp6       0      0 :::3306                 :::*                    LISTEN      24717/docker-proxy
",<mysql><docker><remote-access><ports>,2223,0,42,6598,4,46,61,39,49403,0,1797,6,19,2018-05-26 19:52,2018-05-26 23:57,2021-05-09 18:31,0,1079,Advanced,39
52120182,Bigquery - json_extract all elements from an array,"i'm trying to extract two key from every json in an arry of jsons(using sql legacy)
currently i am using json extract function :
json_extract(json_column , '$[1].X') AS X,
json_extract(json_column , '$[1].Y') AS Y,
how can i make it run on every json at the 'json arry column', and not just [1] (for example)?
An example json:
[
{""blabla"":000,""X"":1,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""2""},
{""blabla"":000,""X"":3,""blabla"":000,""blabla"":000,""blabla"":000,,""Y"":""4""},
]   
thanks in advance!
",<sql><arrays><json><google-bigquery><legacy-sql>,493,0,9,219,1,2,7,52,50180,0,0,2,19,2018-08-31 17:23,2018-08-31 21:03,,0,,Basic,10
52494572,"SQL Alchemy Parametrized Query , binding table name as parameter gives error","I am using parametrized query utilizing Text object in SQL alchemy and are getting different result.
Working example:     
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM Cars 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p2': id_cars})
Example that produces sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax)
import sqlalchemy as sqlal
from sqlalchemy.sql import text
    db_table = 'Cars'
    id_cars = 8
    query = text(""""""SELECT * 
                    FROM :p1 
                    WHERE idCars = :p2
                 """""")
    self.engine.execute(query, {'p1': db_table, 'p2': id_cars})
Any idea on how I can run the query with a dynamic table name that are also protected from sql injection?
",<python><sql><prepared-statement>,910,0,20,237,0,2,9,66,4842,0,8,2,19,2018-09-25 9:15,2019-04-29 2:08,,216,,Basic,10
51844616,High number of live/dead tuples in postgresql/ Vacuum not working,"There is a table , which has 200 rows . But number of live tuples showing there is more than that (around 60K) .
select count(*) from subscriber_offset_manager;
 count 
-------
   200
(1 row)
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      61453 |          5
(1 row)
But as seen from pg_stat_activity and pg_locks , we are not able to track any open connection .
SELECT query, state,locktype,mode
FROM pg_locks
JOIN pg_stat_activity
  USING (pid)
WHERE relation::regclass = 'subscriber_offset_manager'::regclass
  ;
 query | state | locktype | mode 
-------+-------+----------+------
(0 rows)
I also tried  full vacuum on this table  , Below are results : 
All the times no rows are removed 
some times all the live tuples become dead tuples . 
Here is output .
vacuum FULL VERBOSE ANALYZE subscriber_offset_manager;
INFO:  vacuuming ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": found 0 removable, 67920 nonremovable row versions in 714 pages
DETAIL:  67720 dead row versions cannot be removed yet.
CPU 0.01s/0.06u sec elapsed 0.13 sec.
INFO:  analyzing ""public.subscriber_offset_manager""
INFO:  ""subscriber_offset_manager"": scanned 710 of 710 pages, containing 200 live rows and 67720 dead rows; 200 rows in sample, 200 estimated total rows
VACUUM
 SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |        200 |      67749
and after 10 sec 
SELECT schemaname,relname,n_live_tup,n_dead_tup FROM pg_stat_user_tables  where relname='subscriber_offset_manager' ORDER BY n_dead_tup
;
 schemaname |          relname          | n_live_tup | n_dead_tup 
------------+---------------------------+------------+------------
 public     | subscriber_offset_manager |      68325 |        132
How Our App query to this table . 
Our application generally select some rows and based on some business calculation, update the row  . 
select  query --  select based on some id 
select * from subscriber_offset_manager where shard_id=1 ;
update query --  update some other column  for this selected shard id
around 20 threads do this in parallel  and One thread works on only one row .
app is writen in java and we are using hibernate to do db operations . 
Postgresql version is 9.3.24
One more interesting observation : 
 -  when i stop my java app and then do full vacuum , it works fine (number of rows and live tuples become equal). So there is something wrong if we select and update continuously from java app . – 
Problem/Issue 
These live tuples some times go to dead tuples and after some times again comes to live . 
Due to above behaviour select from the table taking time and increasing load on server as lots of live/deadtuples are there ..
",<postgresql><performance><hibernate><postgresql-9.3>,3198,0,41,1311,1,12,29,52,18315,0,71,3,19,2018-08-14 15:09,2018-08-14 15:47,2018-08-14 20:41,0,0,Intermediate,23
60377396,Postgresql connection marked as broken by Hikari,"I have Spring application that regularly inserts records in a PostgreSQL-DB. Now, after a scheduled set of imports is done it takes a few seconds until I get the following warning:
com.zaxxer.hikari.pool.ProxyConnection   : HikariPool-1 - Connection org.postgresql.jdbc.PgConnection@7d18a7dc marked as broken because of SQLSTATE(08006), ErrorCode(0)
followed by the this exception:
org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:335) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:143) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:132) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java) ~[HikariCP-3.2.0.jar:na]
    at org.jooq.tools.jdbc.DefaultPreparedStatement.execute(DefaultPreparedStatement.java:209) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:432) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:613) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350) ~[jooq-3.11.9.jar:na]
    at org.jooq.impl.Tools$10$1.block(Tools.java:4377) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3118) ~[na:na]
    at org.jooq.impl.Tools$10.get(Tools.java:4374) ~[jooq-3.11.9.jar:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[na:na]
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1692) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) ~[na:na]
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177) ~[na:na]
Caused by: java.io.EOFException: null
    at org.postgresql.core.PGStream.receiveChar(PGStream.java:308) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1952) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308) ~[postgresql-42.2.5.jar:42.2.5]
On most occurrences this also sets my database in recovery mode.
On the next run of the scheduler the hikari pool warns about the connections that could not be validated:
HikariPool-1 - Failed to validate connection org.postgresql.jdbc.PgConnection@389c2816 (This connection has been closed.). Possibly consider using a shorter maxLifetime value.
I fiddled around with the validation-timeout and max-lifetime settings of hikari but none of the changes seemed to have any effect.
",<spring><postgresql><kotlin><jooq><hikaricp>,3480,0,30,609,0,5,16,38,8229,0,15,0,19,2020-02-24 13:54,,,,,Intermediate,23
56564410,Tables could not be fetched - Error loading schema content,"I open workbench and connect to a local database on XAMPP and when open the connection the schema show the error message:
  ""tables could not be fetched""
",<mysql-workbench><workbench><kie-workbench><sql-workbench-j>,154,0,0,161,1,2,7,38,54009,0,1,13,19,2019-06-12 14:20,2020-02-22 20:37,,255,,Intermediate,15
51136693,how to check HikariCP connection pooling is working or not in Java?,"I have written following properties in my configuration files I am using Log4j 
in my application When I am running a project.
I am getting following message.does that mean connection pooling is configured in my project? if not then how it will be?
INFO: internal.ConnectionProviderInitiator - HHH000130: Instantiating explicit connection provider: com.zaxxer.hikari.hibernate.HikariConnectionProvider
I have referred following link also
link here
Datasource settings
hibernate.datasource.driver-class-name=com.mysql.jdbc.Driver
hibernate.datasource.url=jdbc:mysql://localhost:3306/mydb
hibernate.datasource.username=root
hibernate.datasource.password=root
HikariCP Settings
hibernate.hikari.dataSource.url=jdbc:mysql://localhost:3306/mydb
hibernate.hikari.idleTimeout=10
hibernate.hikari.maximumPoolSize=30
hibernate.hikari.minimumIdle=15
hibernate.connection.provider_class=com.zaxxer.hikari.hibernate.HikariConnectionProvider
hibernate.hikari.dataSourceClassName=com.mysql.jdbc.jdbc2.optional.MysqlDataSource
",<java><mysql><hibernate><connection-pooling><hikaricp>,1012,1,11,774,1,10,40,51,28351,0,531,4,19,2018-07-02 12:52,2018-07-02 12:59,,0,,Intermediate,15
59675402,Django Full Text SearchVectorField obsolete in PostgreSQL,"I'm using Django's inbuilt full text search with PostgreSQL.
The Django docs say that performance can be improved by using a SearchVectorField. That field keeps a pre-generated ts_vector column with all the relevant lexemes alongside the model, rather than generating it on the fly during every search.
However, with this approach the ts_vector must be updated whenever the model is updated. To keep it synchronised, the Django docs suggest using &quot;triggers&quot;, and refer us to the PostgreSQL documentation for more details.
However, the PostgreSQL docs themselves say that the trigger approach is now obsolete. Instead of manually updating the ts_vector column, it is better to keep the column automatically up-to-date by using a stored generated column.
How can I use PostgreSQL's recommended approach with Django?
",<django><postgresql><full-text-search>,824,4,4,1178,2,14,28,73,2348,0,2151,2,18,2020-01-10 4:08,2020-01-10 5:30,,0,,Basic,3
51118815,"Error: could not determine PostgreSQL version from ""10.4""","
Complete output from command python setup.py egg_info: running
egg_info creating pip-egg-info/psycopg2.egg-info writing
pip-egg-info/psycopg2.egg-info/PKG-INFO writing top-level names to
pip-egg-info/psycopg2.egg-info/top_level.txt writing dependency_links
to pip-egg-info/psycopg2.egg-info/dependency_links.txt writing
manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt' Error:
could not determine PostgreSQL version from '10.4'
Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-install-lR9u0X/psycopg2/
Does anyone know what's the issue? Trying to run pgadmin in virtualenv and can't figure out because of this error.
",<python><postgresql><psycopg2>,660,0,0,197,1,2,6,51,11373,0,3,2,18,2018-06-30 21:54,2018-08-04 14:42,2018-08-04 14:42,35,35,Basic,14
65478350,ERROR: column is of type json but expression is of type character varying in Hibernate,"I need to map two columns of entity class as json in postgres using spring data jpa. After reading multiple stackoverflow posts and baeldung post ,
How to map a map JSON column to Java Object with JPA
https://www.baeldung.com/hibernate-persist-json-object
I did configuration as below. However, I am facing error &quot;ERROR: column &quot;headers&quot; is of type json but expression is of type character varying&quot;
Please provide some pointer to resolve this issue.
I have an entity class as below
@Entity
@Data
@SuperBuilder
@NoArgsConstructor
@AllArgsConstructor
public class Task {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    private Integer id;
    private String url;
    private String httpMethod;
    @Convert(converter = HashMapConverter.class)
    @Column(columnDefinition = &quot;json&quot;)
    private Map&lt;String, String&gt; headers;
    @Convert(converter = HashMapConverter.class)
    @Column(columnDefinition = &quot;json&quot;)
    private Map&lt;String, String&gt; urlVariables;
}
I have created a test class to test if entity is persisted or not. On running this junit, below test case is failing with error as below
@SpringBootTest
class TaskRepositoryTest {
    private static Task randomTask = randomTask();
    @Autowired
    private TaskRepository taskRepository;
    @BeforeEach
    void setUp() {
        taskRepository.deleteAll();
        taskRepository.save(randomTask);
    }
    public static Task randomTask() {
        return randomTaskBuilder().build();
    }
    public static TaskBuilder randomTaskBuilder() {
        Map&lt;String,String&gt; headers = new HashMap&lt;&gt;();
        headers.put(randomAlphanumericString(10),randomAlphanumericString(10));
        Map&lt;String,String&gt; urlVariables = new HashMap&lt;&gt;();
        urlVariables.put(randomAlphanumericString(10),randomAlphanumericString(10));
        return builder()
                .id(randomPositiveInteger())
                .httpMethod(randomAlphanumericString(10))
                .headers(headers)
                .urlVariables(urlVariables)
                .url(randomAlphanumericString(10)));
    }
}
Using liquibase, I have created table in postgres DB and I could see column datatype as json.
databaseChangeLog:
  - changeSet:
      id: 1
      author: abc
      changes:
        - createTable:
            tableName: task
            columns:
              - column:
                  name: id
                  type: int
                  autoIncrement: true
                  constraints:
                    primaryKey: true
              - column:
                  name: url
                  type: varchar(250)
                  constraints:
                    nullable: false
                    unique: true
              - column:
                  name: http_method
                  type: varchar(50)
                  constraints:
                    nullable: false
              - column:
                  name: headers
                  type: json
              - column:
                  name: url_variables
                  type: json
      rollback:
        - dropTable:
            tableName: task
",<json><postgresql><spring-boot><spring-data-jpa><liquibase>,3153,4,88,2135,6,28,47,50,33104,0,48,5,18,2020-12-28 13:26,2020-12-29 9:05,2020-12-29 9:05,1,1,Basic,14
51814383,Why CTE (Common Table Expressions) in some cases slow down queries comparing to temporary tables in SQL Server,"I have several cases where my complex CTE (Common Table Expressions) are ten times slower than the same queries using the temporary tables in SQL Server.
My question here is in regards to how SQL Server process the CTE queries, it looks like it tries to join all the separated queries instead of storing the results of each one and then trying to run the following ones. So that might be the reason why it is so faster when using temporary tables.
For example:
Query 1: using Common Table Expression:
;WITH Orders AS
(
    SELECT
        ma.MasterAccountId,
        IIF(r.FinalisedDate IS NULL, 1, 0)) [Status]
    FROM 
        MasterAccount ma
    INNER JOIN 
        task.tblAccounts a ON a.AccountNumber = ma.TaskAccountId 
                           AND a.IsActive = 1
    LEFT OUTER JOIN 
        task.tblRequisitions r ON r.AccountNumber = a.AccountNumber 
    WHERE 
        ma.IsActive = 1
        AND CAST(r.BatchDateTime AS DATE) BETWEEN @fromDate AND @toDate
        AND r.BatchNumber &gt; 0
),
StockAvailability AS
(
    SELECT sa.AccountNumber,
           sa.RequisitionNumber,
           sa.RequisitionDate,
           sa.Lines,
           sa.HasStock,
           sa.NoStock,
           CASE WHEN sa.Lines = 0 THEN 'Empty'
                WHEN sa.HasStock = 0 THEN 'None'
                WHEN (sa.Lines &gt; 0 AND sa.Lines &gt; sa.HasStock) THEN 'Partial'
                WHEN (sa.Lines &gt; 0 AND sa.Lines &lt;= sa.HasStock) THEN 'Full'
            END AS [Status]
    FROM
    (
        SELECT
                r.AccountNumber,
                r.RequisitionNumber,
                r.RequisitionDate,
                COUNT(rl.ProductNumber) Lines,
                SUM(IIF(ISNULL(psoh.AvailableStock, 0) &gt;= ISNULL(rl.Quantity, 0), 1, 0)) AS HasStock,
                SUM(IIF(ISNULL(psoh.AvailableStock, 0) &lt; ISNULL(rl.Quantity, 0), 1, 0)) AS NoStock
        FROM task.tblrequisitions r 
        INNER JOIN task.tblRequisitionLines rl ON rl.RequisitionNumber = r.RequisitionNumber
        LEFT JOIN ProductStockOnHandSummary psoh ON psoh.ProductNumber = rl.ProductNumber
        WHERE dbo.fn_RemoveUnitPrefix(r.BatchNumber) = 0
          AND r.UnitId = 1
          AND r.FinalisedDate IS NULL
          AND r.RequisitionStatus = 1 
          AND r.TransactionTypeNumber = 301 
        GROUP BY r.AccountNumber, r.RequisitionNumber, r.RequisitionDate
    ) AS sa
),
Available AS
(
    SELECT  ma.MasterAccountId,
            SUM(IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                            CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END)) AS AvailableStock,
            SUM(IIF(sa.[Status] IN ('Full', 'Partial', 'None'), 1, 0))  AS OrdersAnyStock, 
            SUM(IIF(sa.RequisitionDate &lt; dbo.TicksToTime(ma.DailyOrderCutOffTime, @toDate),
                    IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                                CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END), 0)) AS AvailableBeforeCutOff                             
    FROM MasterAccount ma
    INNER JOIN StockAvailability sa ON sa.AccountNumber = ma.TaskAccountId
    GROUP BY ma.MasterAccountId, ma.IsPartialStock
),
Totals AS
(
    SELECT 
        o.MasterAccountId,
        COUNT(o.MasterAccountId) AS BatchedOrders
    FROM Orders o
    GROUP BY o.MasterAccountId
)
SELECT a.MasterAccountId,
       ISNULL(t.BatchedOrders, 0) BatchedOrders,
       ISNULL(t.PendingOrders, 0) PendingOrders,
       ISNULL(av.AvailableStock, 0) AvailableOrders,
       ISNULL(av.AvailableBeforeCutOff, 0) AvailableCutOff,
       ISNULL(av.OrdersAnyStock, 0) AllOrders
FROM MasterAccount a
LEFT OUTER JOIN Available av ON av.MasterAccountId = a.MasterAccountId
LEFT OUTER JOIN Totals t ON t.MasterAccountId = a.MasterAccountId
WHERE a.IsActive = 1
Query 2: using temporary tables:
DROP TABLE IF EXISTS #Orders
CREATE TABLE #Orders (MasterAccountId int, [Status] int);
INSERT INTO #Orders
SELECT
    ma.MasterAccountId,
    dbo.fn_GetBatchPickingStatus(ma.BatchPickingOnHold,
                                    iif(r.GroupNumber &gt; 0, 1, 0),
                                    iif(r.FinalisedDate is null, 1, 0)) [Status]
FROM MasterAccount ma (nolock)
INNER JOIN wh3.dbo.tblAccounts a (nolock) on a.AccountNumber = dbo.fn_RemoveUnitPrefix(ma.TaskAccountId) and a.IsActive = 1
LEFT OUTER JOIN wh3.dbo.tblRequisitions r (nolock) on r.AccountNumber = a.AccountNumber 
WHERE cast(r.BatchDateTime as date) between @fromDate and @toDate
    AND r.BatchNumber &gt; 0
    AND ma.IsActive = 1
DROP TABLE IF EXISTS #StockAvailability
Create Table #StockAvailability (AccountNumber int, RequisitionNumber int, RequisitionDate datetime, Lines int, HasStock int, NoStock int);
Insert Into #StockAvailability
SELECT
        r.AccountNumber,
        r.RequisitionNumber,
        r.RequisitionDate,
        COUNT(rl.ProductNumber) Lines,
        SUM(IIF(ISNULL(psoh.AvailableStock, 0) &gt;= ISNULL(rl.Quantity, 0), 1, 0)) AS HasStock,
        SUM(IIF(ISNULL(psoh.AvailableStock, 0) &lt; ISNULL(rl.Quantity, 0), 1, 0)) AS NoStock
FROM WH3.dbo.tblrequisitions r (nolock)
INNER JOIN WH3.dbo.tblRequisitionLines rl (nolock) ON rl.RequisitionNumber = r.RequisitionNumber
LEFT JOIN ProductStockOnHandSummary psoh (nolock) ON psoh.ProductNumber = rl.ProductNumber -- Joined with View          
WHERE r.BatchNumber = 0
    AND r.FinalisedDate is null
    AND r.RequisitionStatus = 1 
    AND r.TransactionTypeNumber = 301 
GROUP BY r.AccountNumber, r.RequisitionNumber, r.RequisitionDate
DROP TABLE IF EXISTS #StockAvailability2
Create Table #StockAvailability2 (AccountNumber int, RequisitionNumber int, RequisitionDate datetime, Lines int, HasStock int, NoStock int, [Status] nvarchar(7));
Insert Into #StockAvailability2
SELECT sa.AccountNumber,
        sa.RequisitionNumber,
        sa.RequisitionDate,
        sa.Lines,
        sa.HasStock,
        sa.NoStock,
        CASE WHEN sa.Lines = 0 THEN 'Empty'
            WHEN sa.HasStock = 0 THEN 'None'
            WHEN (sa.Lines &gt; 0 AND sa.Lines &gt; sa.HasStock) THEN 'Partial'
            WHEN (sa.Lines &gt; 0 AND sa.Lines &lt;= sa.HasStock) THEN 'Full'
        END AS [Status]
FROM #StockAvailability sa
DROP TABLE IF EXISTS #Available
Create Table #Available (MasterAccountId int, AvailableStock int, OrdersAnyStock int, AvailableBeforeCutOff int);
INSERT INTO #Available
SELECT  ma.MasterAccountId,
        SUM(IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                        CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END)) AS AvailableStock,
        SUM(IIF(sa.[Status] IN ('Full', 'Partial', 'None'), 1, 0))  AS OrdersAnyStock, 
        SUM(IIF(sa.RequisitionDate &lt; dbo.TicksToTime(ma.DailyOrderCutOffTime, @toDate),
                IIF(ma.IsPartialStock = 1,  CASE WHEN sa.[Status] IN ('Full', 'Partial') THEN 1 ELSE 0 END, 
                                            CASE WHEN sa.[Status] = 'Full' THEN 1 ELSE 0 END), 0)) AS AvailableBeforeCutOff                             
FROM MasterAccount ma (NOLOCK)
INNER JOIN #StockAvailability2 sa ON sa.AccountNumber = dbo.fn_RemoveUnitPrefix(ma.TaskAccountId)
GROUP BY ma.MasterAccountId, ma.IsPartialStock
;WITH Totals AS
(
    SELECT 
        o.MasterAccountId,
        COUNT(o.MasterAccountId) AS BatchedOrders,
        SUM(IIF(o.[Status] IN (0,1,2), 1, 0)) PendingOrders
    FROM #Orders o (NOLOCK)
    GROUP BY o.MasterAccountId
)
SELECT a.MasterAccountId,
       ISNULL(t.BatchedOrders, 0) BatchedOrders,
       ISNULL(t.PendingOrders, 0) PendingOrders,
       ISNULL(av.AvailableStock, 0) AvailableOrders,
       ISNULL(av.AvailableBeforeCutOff, 0) AvailableCutOff,
       ISNULL(av.OrdersAnyStock, 0) AllOrders
FROM MasterAccount a (NOLOCK)
LEFT OUTER JOIN #Available av (NOLOCK) ON av.MasterAccountId = a.MasterAccountId
LEFT OUTER JOIN Totals t (NOLOCK) ON t.MasterAccountId = a.MasterAccountId
WHERE a.IsActive = 1
",<sql><sql-server><sql-server-2008><sql-server-2005><sql-server-2012>,8024,0,178,1591,2,27,56,61,6256,0,1672,3,18,2018-08-13 2:10,2018-08-13 2:15,2018-08-13 2:15,0,0,Intermediate,23
51596967,Is a PostgreSQL trigger asynchronous?,"Sorry but I am new to SQL triggers and need some help.
I am after creating a trigger on UPDATE to one of my PostgreSQL tables.  The trigger calls DELETE on a different table.   My question is:  Is the trigger portion of the request asynchronous or synchronous to the UPDATE request?
For more information here are my trigger and function:
CREATE OR REPLACE FUNCTION expire_table() RETURNS trigger AS $expire_table$
  BEGIN
     DELETE FROM object_store WHERE p_time &lt; NOW() - INTERVAL '6 months';
     RETURN NEW;
  END;
$expire_table$ LANGUAGE 'plpgsql';
CREATE TRIGGER expire_table_trigger
  AFTER UPDATE ON objects
EXECUTE PROCEDURE expire_table();
My hope is that the trigger portion is asynchronous or that there is a way of making it so.
",<postgresql>,746,0,13,759,0,10,29,68,7948,0,30,1,18,2018-07-30 15:03,2018-07-30 15:19,2018-07-30 15:19,0,0,Intermediate,23
57214850,Trying to upgrade SQLite on Amazon EC2,"I need SQLite minimum version 3.8 to support a MediaWiki install on Amazon EC2. Amazon Linux is based on CentOS and the latest version available in the yum repository is SQLite 3.7.17.
The downloads available from sqlite.org don't include 64-bit Linux. There is a GitHub repository that has a prebuilt 64-bit version, however it's only the command line version. I put it at /usr/bin:
$ which sqlite3
/usr/bin/sqlite3
$ sqlite3 --version
sqlite3: /lib64/libtinfo.so.5: no version information available (required by sqlite3)
3.26.0 2018-12-01 12:34:55 bf8c1b2b7a5960c282e543b9c293686dccff272512d08865f4600fb58238b4f9
But MediaWiki still complains I have SQLite 3.7.17 installed. When I test it I get:
$ cat x.php
&lt;?php
print_r(SQLite3::version());
?&gt;
Run it:
$ php7 x.php
Array
(
    [versionString] =&gt; 3.7.17
    [versionNumber] =&gt; 3007017
)
I am guessing this is because of these libraries:
$ sudo find / -name &quot;libsqlite*&quot;
/usr/lib64/libsqlite3.so.0
/usr/lib64/libsqlite3.so.0.8.6
How can I download/rebuild or otherwise install a later version of these SQLite libraries?
",<php><sqlite><mediawiki>,1095,1,20,509,0,6,17,46,9105,0,117,4,18,2019-07-26 7:10,2020-07-14 15:35,2020-07-14 15:35,354,354,Intermediate,23
60059084,What does Using join buffer (Block Nested Loop) mean with EXPLAIN mysql command in the Extra column?,"I am trying to optimize my query.
And getting Using join buffer (Block Nested Loop)
for one of the table with
EXPLAIN SELECT command.
I have no idea what does it mean.
I tried googling about, but I haven't found the explanation.
",<mysql><explain>,229,0,1,2189,3,11,35,62,21052,0,2513,3,18,2020-02-04 14:03,2020-02-04 14:35,2020-02-04 14:35,0,0,Intermediate,23
61545680,Postgresql partition and sqlalchemy,"SQLAlchemy doc explain how to create a partitioned table. But it does not explains how to create partitions.
So if I have this :
#Skipping create_engine and metadata
Base = declarative_base()
class Measure(Base):
    __tablename__ = 'measures'
    __table_args__ = {
        postgresql_partition_by: 'RANGE (log_date)'
    }
    city_id = Column(Integer, not_null=True)
    log_date = Columne(Date, not_null=True)
    peaktemp = Column(Integer)
    unitsales = Column(Integer)
class Measure2020(Base):
    """"""How am I suppposed to declare this ? """"""
I know that most of the I'll be doing SELECT * FROM measures WHERE logdate between XX and YY. But that seems interesting.
",<python><postgresql><sqlalchemy><postgresql-12>,672,1,16,888,1,6,20,77,15231,0,95,4,18,2020-05-01 15:38,2020-07-23 9:27,2021-06-20 21:18,83,415,Basic,11
48965204,"How to select rows, and nearby rows","SQL Fiddle
Background
I have a table of values that some need attention:
| ID      | AddedDate   |
|---------|-------------|
|       1 | 2010-04-01  |
|       2 | 2010-04-01  |
|       3 | 2010-04-02  |
|       4 | 2010-04-02  |
|       5 | NULL        | &lt;----------- needs attention
|       6 | 2010-04-02  |
|       7 | 2010-04-03  |
|       8 | 2010-04-04  |
|       9 | 2010-04-04  |
| 2432659 | 2016-06-15  |
| 2432650 | 2016-06-16  |
| 2432651 | 2016-06-17  |
| 2432672 | 2016-06-18  |
| 2432673 | NULL        | &lt;----------- needs attention
| 2432674 | 2016-06-20  |
| 2432685 | 2016-06-21  |
I want to select the rows where AddedDate is null, and i want to select rows around it. In this example question it would be sufficient to say rows where the ID is ±3. This means i want:
| ID      | AddedDate   |
|---------|-------------|
|       2 | 2010-04-01  | ─╮
|       3 | 2010-04-02  |  │
|       4 | 2010-04-02  |  │
|       5 | NULL        |  ├──ID values ±3
|       6 | 2010-04-02  |  │
|       7 | 2010-04-03  |  │
|       8 | 2010-04-04  | ─╯
| 2432672 | 2016-06-18  | ─╮
| 2432673 | NULL        |  ├──ID values ±3
| 2432674 | 2016-06-20  | ─╯
  Note: In reality it's a table of 9M rows, and 15k need attention.
Attempts
First i create a query that builds the ranges i'm interested in returning:
SELECT
  ID-3 AS [Low ID],
  ID+3 AS [High ID]
FROM Items
WHERE AddedDate IS NULL
Low ID   High ID
-------  -------
2        8 
2432670  2432676
So my initial attempt to use this does work:
WITH dt AS (
   SELECT ID-3 AS Low, ID+3 AS High
   FROM Items
   WHERE AddedDate IS NULL
)
SELECT * FROM Items
WHERE EXISTS(
    SELECT 1 FROM dt
    WHERE Items.ID BETWEEN dt.Low AND dt.High)
But when i try it on real data:
9 million total rows
15,000 interesting rows
subtree cost of 63,318,400
it takes hours (before i give up and cancel it)
There's probably a more efficient way.
Bonus Reading
Select a row and rows around it
Select Rows with matching columns from SQL Server
How can I search for rows &quot;around&quot; a given string value?
How to get N rows starting from row M from sorted table in T-SQL
",<sql><sql-server><sql-server-2012>,2117,6,52,248457,256,876,1230,81,2737,0,5616,4,18,2018-02-24 16:49,2018-02-24 16:59,2018-02-24 17:29,0,0,Intermediate,23
49956713,"pip install mysqlclient failed ""Running setup.py bdist_wheel for mysqlclient ... error""","I'm trying to run my Python 3 project on my newly formatted Mac OS High Sierra 10.13.4, by first running pipenv install to get the dependencies, but that fails.
Specifically, the part where it fails to install dependencies is the mysqlclient part.
This error message shows up:
_mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
              my_bool recon = reconnect;
              ^
_mysql.c:1895:58: error: use of undeclared identifier 'recon'
              mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                     ^
Configuration
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; python --version
Python 2.7.10
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; python3 --version
Python 3.6.5
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew install mysql-connector-c
Warning: mysql-connector-c 6.1.11 is already installed, its just not linked
You can use `brew link mysql-connector-c` to link this version.
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew link mysql-connector-c
Linking /usr/local/Cellar/mysql-connector-c/6.1.11... 
Error: Could not symlink bin/my_print_defaults
Target /usr/local/bin/my_print_defaults
is a symlink belonging to mysql. You can unlink it:
  brew unlink mysql
To force the link and overwrite all conflicting files:
  brew link --overwrite mysql-connector-c
To list all files that would be deleted:
  brew link --overwrite --dry-run mysql-connector-c
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; mysql --version
mysql  Ver 8.0.11 for macos10.13 on x86_64 (MySQL Community Server - GPL)
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew install mysql
Warning: mysql 5.7.22 is already installed and up-to-date
To reinstall 5.7.22, run `brew reinstall mysql`
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; brew info openssl
openssl: stable 1.0.2o (bottled) [keg-only]
SSL/TLS cryptography library
https://openssl.org/
/usr/local/Cellar/openssl/1.0.2o_1 (1,791 files, 12.3MB)
  Poured from bottle on 2018-04-20 at 13:06:42
From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/openssl.rb
==&gt; Dependencies
Build: makedepend ✘
==&gt; Options
--without-test
    Skip build-time tests (not recommended)
==&gt; Caveats
A CA file has been bootstrapped using certificates from the SystemRoots
keychain. To add additional certificates (e.g. the certificates added in
the System keychain), place .pem files in
  /usr/local/etc/openssl/certs
and run
  /usr/local/opt/openssl/bin/c_rehash
This formula is keg-only, which means it was not symlinked into /usr/local,
because Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries.
If you need to have this software first in your PATH run:
  echo 'export PATH=""/usr/local/opt/openssl/bin:$PATH""' &gt;&gt; ~/.bash_profile
For compilers to find this software you may need to set:
    LDFLAGS:  -L/usr/local/opt/openssl/lib
    CPPFLAGS: -I/usr/local/opt/openssl/include
Full terminal error log
darrenkarlsapalo@admins-MacBook-Pro ~/g/t/thesis-nltk&gt; pip install mysqlclient
Collecting mysqlclient
  Using cached https://files.pythonhosted.org/packages/6f/86/bad31f1c1bb0cc99e88ca2adb7cb5c71f7a6540c1bb001480513de76a931/mysqlclient-1.3.12.tar.gz
Building wheels for collected packages: mysqlclient
  Running setup.py bdist_wheel for mysqlclient ... error
  Complete output from command /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-wheel-_frnx3t4 --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.9-x86_64-3.6
  copying _mysql_exceptions.py -&gt; build/lib.macosx-10.9-x86_64-3.6
  creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/connections.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/converters.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/cursors.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/release.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  copying MySQLdb/times.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
  creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  copying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
  running build_ext
  building '_mysql' extension
  creating build/temp.macosx-10.9-x86_64-3.6
  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch x86_64 -g -Dversion_info=(1,3,12,'final',0) -D__version__=1.3.12 -I/usr/local/mysql/include -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c _mysql.c -o build/temp.macosx-10.9-x86_64-3.6/_mysql.o
  _mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
                  my_bool recon = reconnect;
                  ^
  _mysql.c:1895:58: error: use of undeclared identifier 'recon'
                  mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                         ^
  2 errors generated.
  error: command 'gcc' failed with exit status 1
  ----------------------------------------
  Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-record-vkhrazcr/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.9-x86_64-3.6
    copying _mysql_exceptions.py -&gt; build/lib.macosx-10.9-x86_64-3.6
    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/connections.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/converters.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/cursors.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/release.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    copying MySQLdb/times.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb
    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants
    running build_ext
    building '_mysql' extension
    creating build/temp.macosx-10.9-x86_64-3.6
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch x86_64 -g -Dversion_info=(1,3,12,'final',0) -D__version__=1.3.12 -I/usr/local/mysql/include -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c _mysql.c -o build/temp.macosx-10.9-x86_64-3.6/_mysql.o
    _mysql.c:1894:3: error: use of undeclared identifier 'my_bool'
                    my_bool recon = reconnect;
                    ^
    _mysql.c:1895:58: error: use of undeclared identifier 'recon'
                    mysql_options(&amp;self-&gt;connection, MYSQL_OPT_RECONNECT, &amp;recon);
                                                                           ^
    2 errors generated.
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
Command ""/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-record-vkhrazcr/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/75/hg3nj2sx13567pbv76wdycqm0000gn/T/pip-install-mj5god72/mysqlclient/
I'm frustrated enough to want to uninstall all my python installations, but I'm just afraid I might screw something up with Mac that relies on the prebuilt outdated Python version.
How can I get my dependencies to be installed correctly?
",<mysql><python-3.x><macos><pipenv>,10445,3,152,1829,2,19,37,44,10329,0,914,3,18,2018-04-21 14:19,2018-05-15 4:13,2018-05-15 4:13,24,24,Intermediate,23
52377469,Failed to open the referenced table,"I am new with SQL and I am not entirely sure why I am getting the error: ERROR 1824 (HY000) at line 5: Failed to open the referenced table 'products'
Operation failed with exitcode 1
Here is my code
drop database if exists cc;
create database cc /*!40100 default character set utf8 */;
use cc;
create table Customers(
  CustomerID int not null,
  FirstName varchar(255),
  LastName varchar(255),
  address varchar(255),
  phoneNO varchar(11),
  prodID int,
  quantity int,
  primary key (CustomerID),
  foreign key (prodID) references Products(itemID)
);
create table Employees(
   EmployeeID int not null,
   FirstName varchar(255),
   LastName varchar(255),
   address varchar(255),
   phoneNO varchar(11),
   ManagerID int not null,
   primary key (EmployeeID),
   foreign key (managerID) references Managers(mgrID)
 );
create table Managers(
    mgrID int not null,
    salary float,
   MaxSupervisingCapacity int,
   foreign key (mgrID) references Employees(EmployeeID),
   primary key (mgrID) 
);
",<mysql><sql>,1003,0,34,191,1,1,5,66,79129,0,0,5,18,2018-09-18 0:51,2018-09-18 1:07,,0,,Basic,10
55708079,Spark: optimise writing a DataFrame to SQL Server,"I am using the code below to write a DataFrame of 43 columns and about 2,000,000 rows into a table in SQL Server:
dataFrame
  .write
  .format(""jdbc"")
  .mode(""overwrite"")
  .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
  .option(""url"", url)
  .option(""dbtable"", tablename)
  .option(""user"", user)
  .option(""password"", password)
  .save()
Sadly, while it does work for small DataFrames it's either extremely slow or gets timed out for large ones. Any hints on how to optimize it?
I've tried setting rewriteBatchedStatements=true
Thanks.
",<sql><sql-server><database><scala><apache-spark>,558,0,11,662,1,11,25,46,19707,0,15,5,18,2019-04-16 12:23,2019-04-16 22:05,2019-04-25 9:51,0,9,Intermediate,23
49660672,What to try to get BigQuery to CAST BYTES to STRING?,"BigQuery Standard SQL documentation suggests that BYTE fields can be coerced into STRINGS. 
We have a byte field that is the result of SHA256 hashing a field using BigQuery itself. 
We now want to coerce it to a STRING, yet when we run ""CAST(field_name to STRING)"" we get an error: 
  Query Failed Error: Invalid cast of bytes to UTF8 string
What is preventing us from getting a string from this byte field? Is it surmountable? If so, what is the solution?
",<sql><casting><google-bigquery>,457,1,0,5872,10,48,78,80,29688,0,565,3,18,2018-04-04 21:22,2018-04-04 21:35,,0,,Basic,2
52533487,STRING_AGG not behaving as expected,"I have the following query:
WITH cteCountryLanguageMapping AS (
    SELECT * FROM (
        VALUES
            ('Spain', 'English'),
            ('Spain', 'Spanish'),
            ('Sweden', 'English'),
            ('Switzerland', 'English'),
            ('Switzerland', 'French'),
            ('Switzerland', 'German'),
            ('Switzerland', 'Italian')
    ) x ([Country], [Language])
)
SELECT
    [Country],
    CASE COUNT([Language])
        WHEN 1 THEN MAX([Language])
        WHEN 2 THEN STRING_AGG([Language], ' and ')
        ELSE STRING_AGG([Language], ', ')
    END AS [Languages],
    COUNT([Language]) AS [LanguageCount]
FROM cteCountryLanguageMapping
GROUP BY [Country]
I was expecting the value inside Languages column for Switzerland to be comma separated i.e.:
  | Country     | Languages                                 | LanguageCount
--+-------------+-------------------------------------------+--------------
1 | Spain       | Spanish and English                       | 2
2 | Sweden      | English                                   | 1
3 | Switzerland | French, German, Italian, English          | 4
Instead I am getting the below output (the 4 values are separated by and):
  | Country     | Languages                                 | LanguageCount
--+-------------+-------------------------------------------+--------------
1 | Spain       | Spanish and English                       | 2
2 | Sweden      | English                                   | 1
3 | Switzerland | French and German and Italian and English | 4
What am I missing?
Here is another example:
SELECT y, STRING_AGG(z, '+') AS STRING_AGG_PLUS, STRING_AGG(z, '-') AS STRING_AGG_MINUS
FROM (
    VALUES
        (1, 'a'),
        (1, 'b')
) x (y, z)
GROUP by y
  | y | STRING_AGG_PLUS | STRING_AGG_MINUS
--+---+-----------------+-----------------
1 | 1 | a+b             | a+b
Is this a bug in SQL Server?
",<sql><sql-server><sql-server-2017><string-aggregation>,1896,0,44,5754,11,51,77,38,2667,0,479,1,18,2018-09-27 9:14,2018-09-27 9:52,2018-09-27 9:52,0,0,Basic,2
50436910,EF core not creating tables on migrate method,"Hey I just started using EF core and everything works fine. I call the the context.Database.Migrate() method and it creates a database. But even though my context object has a DBSet&lt;T&gt;, it doesn't create any tables except for the migration history.
Can anyone help me with this issue?
",<c#><sql-server><asp.net-core><entity-framework-core>,291,0,2,887,3,15,28,66,30706,0,18,4,18,2018-05-20 16:19,2018-05-20 16:27,2018-05-20 16:27,0,0,Basic,9
50612577,CSV copy to Postgres with array of custom type using JDBC,"I have a custom type defined in my database as
CREATE TYPE address AS (ip inet, port int);
And a table that uses this type in an array:
CREATE TABLE my_table (
  addresses  address[] NULL
)
I have a sample CSV file with the following contents
{(10.10.10.1,80),(10.10.10.2,443)}
{(10.10.10.3,8080),(10.10.10.4,4040)}
And I use the following code snippet to perform my COPY:
    Class.forName(""org.postgresql.Driver"");
    String input = loadCsvFromFile();
    Reader reader = new StringReader(input);
    Connection connection = DriverManager.getConnection(
            ""jdbc:postgresql://db_host:5432/db_name"", ""user"",
            ""password"");
    CopyManager copyManager = connection.unwrap(PGConnection.class).getCopyAPI();
    String copyCommand = ""COPY my_table (addresses) "" + 
                         ""FROM STDIN WITH ("" + 
                           ""DELIMITER '\t', "" + 
                           ""FORMAT csv, "" + 
                           ""NULL '\\N', "" + 
                           ""ESCAPE '\""', "" +
                           ""QUOTE '\""')"";
    copyManager.copyIn(copyCommand, reader);
Executing this program produces the following exception:
Exception in thread ""main"" org.postgresql.util.PSQLException: ERROR: malformed record literal: ""(10.10.10.1""
  Detail: Unexpected end of input.
  Where: COPY only_address, line 1, column addresses: ""{(10.10.10.1,80),(10.10.10.2,443)}""
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2422)
    at org.postgresql.core.v3.QueryExecutorImpl.processCopyResults(QueryExecutorImpl.java:1114)
    at org.postgresql.core.v3.QueryExecutorImpl.endCopy(QueryExecutorImpl.java:963)
    at org.postgresql.core.v3.CopyInImpl.endCopy(CopyInImpl.java:43)
    at org.postgresql.copy.CopyManager.copyIn(CopyManager.java:185)
    at org.postgresql.copy.CopyManager.copyIn(CopyManager.java:160)
I have tried with different combinations of the parentheses in the input but cannot seem to get the COPY working. Any ideas where I might be going wrong?
",<java><database><postgresql><jdbc><postgresql-9.5>,2031,0,36,13115,19,60,93,41,6180,0,221,3,18,2018-05-30 20:01,2018-06-03 18:04,2018-06-03 18:04,4,4,Basic,9
50803109,How to store pandas DataFrame in SQLite DB,"I am unable to find good tutorial on this topic. I am having a pandas data frame, df as
Fu(varchar)  val
aed          544.8
jfn          5488
vivj         89.3
vffv         87.5
I want to create a database and a table and store the dataframe in it
",<python><pandas><sqlite>,248,0,6,81,1,1,10,44,20429,0,17,2,18,2018-06-11 17:24,2018-06-11 17:35,2018-06-11 17:35,0,0,Basic,9
50120467,Order queryset by alternating value,"I have the following model:
class Entry(models.Model):
    name = models.Charfield(max_length=255)
    client = models.Charfield(max_length=255)
client is the name of the client as could have values like facebook, google, and so on.
Is it possible to order the queryset so that the result is alternating the values of client?
What I expect is something like this:
Entry.objects.order_by('alternate client') --&gt; 
| client   | name   |
| google   | robert |
| facebook | linda  |
| google   | kate   | 
| facebook | jack   |
| google   | nina   |
| facebook | pierre |    
I am using django2.x and postgres if that helps.
EDIT:
Some additional info / requirements.
I have about 10 to 20 differtent clients
Entry does also have a created DateField. If possible result should also be ordered by date
I want to use pagination for Entry so solution should be using django's ORM 
",<django><postgresql><django-models><django-queryset>,876,0,17,11229,5,52,75,51,8096,0,1615,3,18,2018-05-01 16:49,2018-05-01 17:46,2018-05-05 16:50,0,4,Basic,9
54922433,Postgresql fatal the database system is starting up - windows 10,"I have installed postgresql on windows 10 on usb disk.
Every day when i start my pc in work from sleep and plug in the disk again then trying to start postgresql i get this error:
FATAL: the database system is starting up
The service starts with following command:
E:\PostgresSql\pg96\pgservice.exe ""//RS//PostgreSQL 9.6 Server""
It is the default one.
logs from E:\PostgresSql\data\logs\pg96
2019-02-28 10:30:36 CET [21788]: [1-1] user=postgres,db=postgres,app=[unknown],client=::1 FATAL:  the database system is starting up
2019-02-28 10:31:08 CET [9796]: [1-1] user=postgres,db=postgres,app=[unknown],client=::1 FATAL:  the database system is starting up
I want this start up to happen faster.
",<postgresql>,696,0,10,1854,5,36,74,75,37196,0,326,1,18,2019-02-28 9:36,2019-02-28 10:38,2019-02-28 10:38,0,0,Basic,14
63489949,How to roll back a transaction on error in PostgreSQL?,"I'm writing a script for PostgreSQL and since I want it to be executed atomically, I'm wrapping it inside a transaction.
I expected the script to look something like this:
BEGIN
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
EXCEPTION
    WHEN OTHERS THEN
        ROLLBACK;
END; -- A.k.a. COMMIT;
However, in this case pgAdmin warns me about a syntax error right after the initial BEGIN. If I terminate the command there by appending a semicolon like so: BEGIN; it instead informs me about error near EXCEPTION.
I realize that perhaps I'm mixing up syntax for control structures and transactions, however I couldn't find any mention of how to roll back a failed transaction in the docs (nor in SO for that matter).
I also considered that perhaps the transaction is rolled back automatically on error, but it doesn't seem to be the case since the following script:
BEGIN;
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
COMMIT;
warns me that: ERROR: current transaction is aborted, commands ignored until end of transaction block and I have to then manually ROLLBACK; the transaction.
It seems I'm missing something fundamental here, but what?
EDIT:
I tried using DO as well like so:
DO $$
BEGIN
-- 1) Execute some valid actions;
-- 2) Execute some action that causes an error.
EXCEPTION
    WHEN OTHERS THEN
        ROLLBACK;
END; $$
pgAdmin hits me back with a: ERROR: cannot begin/end transactions in PL/pgSQL. HINT: Use a BEGIN block with an EXCEPTION clause instead. which confuses me to no end, because that is exactly what I am (I think) doing.
POST-ACCEPT EDIT:
Regarding Laurenz's comment: &quot;Your SQL script would contain a COMMIT. That ends the transaction and rolls it back.&quot; - this is not the behavior that I observe. Please consider the following example (which is just a concrete version of an example I already provided in my original question):
BEGIN;
-- Just a simple, self-referencing table.
CREATE TABLE &quot;Dummy&quot; (
    &quot;Id&quot; INT GENERATED ALWAYS AS IDENTITY,
    &quot;ParentId&quot; INT NULL,
    CONSTRAINT &quot;PK_Dummy&quot; PRIMARY KEY (&quot;Id&quot;),
    CONSTRAINT &quot;FK_Dummy_Dummy&quot; FOREIGN KEY (&quot;ParentId&quot;) REFERENCES &quot;Dummy&quot; (&quot;Id&quot;)
);
-- Foreign key violation terminates the transaction.
INSERT INTO &quot;Dummy&quot; (&quot;ParentId&quot;)
VALUES (99);
COMMIT;
When I execute the script above, I'm greeted with: ERROR: insert or update on table &quot;Dummy&quot; violates foreign key constraint &quot;FK_Dummy_Dummy&quot;. DETAIL: Key (ParentId)=(99) is not present in table &quot;Dummy&quot;. which is as expected.
However, if I then try to check whether my Dummy table was created or rolled back like so:
SELECT EXISTS (
    SELECT FROM information_schema.&quot;tables&quot;
    WHERE &quot;table_name&quot; = 'Dummy');
instead of a simple false, I get the same error that I already mentioned twice: ERROR: current transaction is aborted, commands ignored until end of transaction block. Then I have to manually terminate the transaction via issuing ROLLBACK;.
So to me it seems that either the comment mentioned above is false or at least I'm heavily misinterpreting something here.
",<postgresql><exception><transactions><plpgsql><rollback>,3252,0,49,631,2,7,18,46,25722,0,162,2,18,2020-08-19 15:06,2020-08-19 16:46,2020-08-19 16:46,0,0,Basic,14
55596620,Unterminated dollar quote,"How to use IF statement in the PostgreSql (11 version)? I tried just raw IF usage but got problem (syntax error at or near “IF”). To resolve this problem people propose to use 'do &amp;&amp;' but it does not work as well (Unterminated dollar quote started at position 3 in SQL DO $$ BEGIN IF ......). Here is my SQL code:
DO $$
BEGIN
  IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'categories')) THEN
    CREATE TABLE IF NOT EXISTS categories
    (
      id   SERIAL NOT NULL,
      name character varying(40),
      CONSTRAINT categories_pkey PRIMARY KEY (id)
    );
    INSERT INTO categories (name) VALUES ('Games');
    INSERT INTO categories (name) VALUES ('Multimedia');
    INSERT INTO categories (name) VALUES ('Productivity');
    INSERT INTO categories (name) VALUES ('Tools');
    INSERT INTO categories (name) VALUES ('Health');
    INSERT INTO categories (name) VALUES ('Lifestyle');
    INSERT INTO categories (name) VALUES ('Other');
  END IF;
END
$$;
All I need is to create table and insert some init data to it if table does not exist.
",<sql><postgresql>,1084,0,20,247,1,2,7,75,39611,0,51,3,18,2019-04-09 15:45,2019-04-09 15:49,2019-04-09 15:49,0,0,Basic,14
57586148,Handle same function running and processing the same data at the same time,"I have a php system that that allow customer to buy things (make an order) from our system using e-wallet (store credit).
here's the database example
**sales_order**
+--------+-------+----------+--------+--------------+-----------+
|order_id| price |product_id| status |already_refund|customer_id|
+--------+-------+----------+--------+--------------+-----------+
|   1    | 1000  |    1     |canceled|      1       |     2     |
|   2    | 2000  |    2     |pending |      0       |     2     |
|   3    | 3000  |    3     |complete|      0       |     1     | 
+--------+-------+----------+--------+--------------+-----------+
**ewallet**
+-----------+-------+
|customer_id|balance|
+-----------+-------+
|     1     | 43200 |
|     2     | 22500 |
|     3     | 78400 |
+-----------+-------+
table sales_order contain the order that customer made, the column already_refund is for a flag that canceled order already refunded.
I'm running a cron every 5 minutes to check if order with status pending can be canceled and after that it can refund the money to the customer ewallet
function checkPendingOrders(){
   $orders = $this-&gt;orderCollection-&gt;filter(['status'=&gt;'pending']);
   foreach($orders as $order){
     //check if order is ready to be canceled
     $isCanceled = $this-&gt;isCanceled($order-&gt;getId());
     if($isCanceled === false) continue;
     if($order-&gt;getAlreadyRefund() == '0'){ // check if already refund
       $order-&gt;setAlredyRefund('1')-&gt;save();
       $this-&gt;refund($order-&gt;getId()); //refund the money to customer ewallet
     }
     $order-&gt;setStatus('canceled')-&gt;save();
   }
}
The problem the 2 different cron schedule can  process the same data at the same time using this function and it will make  the refund process can be called twice , so the customer will receive double refund amount. How can i handle this kind of problem, when a 2 same function running at the same time to process same data ? the if clause that i made can't handle this kind of issue
update
i've tried to use microtime in session as validation and lock the table row in MySQL, so at the beginning i set the variable to contain the microtime , than when i stored in a unique session generated by order_id , and then i add a condition to match the microtime value with the session before locking the Table Row and update my ewallet table
function checkPendingOrders(){
   $orders = $this-&gt;orderCollection-&gt;filter(['status'=&gt;'pending']);
   foreach($orders as $order){
     //assign unique microtime to session
     $mt = round(microtime(true) * 1000);
     if(!isset($_SESSION['cancel'.$order-&gt;getId()])) $_SESSION['cancel'.$order-&gt;getId()] = $mt;
     //check if order is ready to be canceled
     $isCanceled = $this-&gt;isCanceled($order-&gt;getId());
     if($isCanceled === false) continue;
     if($order-&gt;getAlreadyRefund() == '0'){ // check if already refund
       $order-&gt;setAlreadyRefund('1')-&gt;save();
       //check if microtime is the same as the first one that running
       if($_SESSION['cancel'.$order-&gt;getId()] == $mt){
        //update using lock row
        $this-&gt;_dbConnection-&gt;beginTransaction(); 
        $sqlRaws[] =  ""SELECT * FROM ewallet WHERE customer_id = "".$order-&gt;getCustomerId()."" FOR UPDATE;"";
        $sqlRaws[] =  ""UPDATE ewallet SET balance =(balance+"".$order-&gt;getPrice()."") WHERE customer_id = "".$order-&gt;getCustomerId()."";"";
        foreach ($sqlRaws as $sqlRaw) {
          $this-&gt;_dbConnection-&gt;query($sqlRaw);
        }
        $this-&gt;_dbConnection-&gt;commit(); 
       }
     }
     unset($_SESSION['cancel'.$order-&gt;getId()]);
     $order-&gt;setStatus('canceled')-&gt;save();
   }
}
but the problem still persist when i'm doing a strees test, because there is a case when the same function process the same data at the same microtime and start mysql transaction at the same exact time
",<php><mysql><concurrency><race-condition>,3922,0,60,469,0,4,15,73,879,0,41,11,18,2019-08-21 7:02,2019-08-21 7:51,,0,,Advanced,32
48282321,ValueError: Cannot convert column into bool,"I'm trying build a new column on dataframe as below:
l = [(2, 1), (1,1)]
df = spark.createDataFrame(l)
def calc_dif(x,y):
    if (x&gt;y) and (x==1):
        return x-y
dfNew = df.withColumn(""calc"", calc_dif(df[""_1""], df[""_2""]))
dfNew.show()
But, I get:
Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-2807412651452069487.py"", line 346, in &lt;module&gt;
Exception: Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-2807412651452069487.py"", line 334, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 38, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 36, in calc_dif
  File ""/usr/hdp/current/spark2-client/python/pyspark/sql/column.py"", line 426, in __nonzero__
    raise ValueError(""Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', ""
ValueError: Cannot convert column into bool: please use '&amp;' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.
Why It happens? How can I fix It? 
",<apache-spark><pyspark><apache-spark-sql>,976,0,18,455,2,4,9,47,60521,0,0,4,18,2018-01-16 13:24,2018-01-16 13:29,2018-01-16 13:29,0,0,Basic,2
49275868,How to filter JSON Array in Django JSONField,"i am getting crazy with filtering a (postgres) JSONField in Django 2.0.3.
The json is stored as an array. E.g.
tasks = [{""task"":""test"",""level"":""10""},{""task"":""test 123"",""level"":""20""}]
What i've tried:
myModel.objects.filter(""tasks__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0__contains""=""test"")
myModel.objects.filter(""tasks__0__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0_x__contains""=""test"")
myModel.objects.filter(""tasks__0_x__task__contains""=""test"")
What goes wrong?
What i want to do is a icontains - but as i already read there is not support for icontains on jsonfields in Django right now...
",<python><django><postgresql><django-models>,635,0,6,912,1,9,27,55,17487,0,65,4,18,2018-03-14 10:54,2018-03-14 11:21,2018-12-27 9:09,0,288,Basic,3
51541561,Module not found: Can't resolve 'dns' in pg/lib,"I did create-react-app and also installed sequelize and pg. But when I do npm start, I get the following error - 
./node_modules/pg/lib/connection-parameters.js
Module not found: Can't resolve 'dns' in '/Users/vedant/Web Dev/device_psql/node_modules/pg/lib'
Here is the App.js file - 
import React, { Component } from 'react';
const Sequelize = require('sequelize');
const sequelize = new Sequelize('postgres', 'postgres', 'password', {
host: 'localhost',
dialect: 'postgres',
pool: {
  max: 5,
  min: 0,
  acquire: 30000,
  idle: 10000
},
// http://docs.sequelizejs.com/manual/tutorial/querying.html#operators
operatorsAliases: false
});
class App extends Component {
render() {
  return (
    &lt;div &gt;
      &lt;p&gt;Test&lt;/p&gt;
    &lt;/div&gt;
  );
}
}
export default App;
Also, in the package.json file, I have sequelize and pg. What could be the problem? I have tried to delete the node_modules folder and doing npm install, but no luck.
Thanks in advance.
",<node.js><reactjs><postgresql><sequelize.js>,970,1,28,277,2,5,13,61,33692,0,3,4,18,2018-07-26 14:40,2018-08-10 8:13,,15,,Basic,9
56343611,Insert sqlite flutter without freezing the interface,"I'm trying to insert a lot of rows (about 12k or more) in a sqlite memory database using flutter.
I get data from the API and use a compute function in order to process data from Json.
Now I need to add these data to a database in memory, in order to do so I use a transaction with a batch.
batchInsertEventSong(List&lt;EventSong&gt; rows) async {
   Database db = await instance.database;
   db.transaction((txn) async {
      Batch batch = txn.batch();
      for (var song in rows) {
         Map&lt;String, dynamic&gt; row = {
           DatabaseHelper.columnIdSong: song.id,
           DatabaseHelper.columnTitle: song.title,
           DatabaseHelper.columnInterpreter: song.interpreter
         };
         batch.insert(table, row);
       }
      batch.commit();
   }
}
But this function is blocking my UI during insertions, I tried also with compute but I can't pass the class db or the class batch.
I hadn't clear how to execute this process in another thread or (since I can't use isolates) executing without blocking my UI.
Any advice?
",<sqlite><flutter><dart><sqflite>,1047,0,15,415,0,5,11,38,7011,0,8,1,18,2019-05-28 13:42,2019-07-21 16:14,2019-07-21 16:14,54,54,Intermediate,23
65043926,Entity Framework Core Many to Many change navigation property names,"I have a table called &quot;LogBookSystemUsers&quot; and I want to setup many to many functionality in EF Core 5. I almost have it working but the problem is my ID columns are named SystemUserId and LogBookId but when EF does the join it tries to use SystemUserID and LogBookID. This is my current configuration code:
modelBuilder.Entity&lt;SystemUser&gt;()
            .HasMany(x =&gt; x.LogBooks)
            .WithMany(x =&gt; x.SystemUsers)
            .UsingEntity(x =&gt;
            {
                x.ToTable(&quot;LogBookSystemUsers&quot;, &quot;LogBooks&quot;);
            });
I tried this:
modelBuilder.Entity&lt;SystemUser&gt;()
            .HasMany(x =&gt; x.LogBooks)
            .WithMany(x =&gt; x.SystemUsers)
            .UsingEntity&lt;Dictionary&lt;string, object&gt;&gt;(&quot;LogBookSystemUsers&quot;,
                x =&gt; x.HasOne&lt;LogBook&gt;().WithMany().HasForeignKey(&quot;LogBookId&quot;),
                x =&gt; x.HasOne&lt;SystemUser&gt;().WithMany().HasForeignKey(&quot;SystemUserId&quot;),
                x =&gt; x.ToTable(&quot;LogBookSystemUsers&quot;, &quot;LogBooks&quot;));
But that just adds two new columns instead of setting the names of the current columns.
This is all database first. I don't want to have to use a class for the many to many table because I do this all over in my project and I don't want a bunch of useless classes floating around. Any ideas?
",<c#><postgresql><entity-framework-core><ef-core-5.0>,1411,0,18,1862,2,20,38,62,8508,0,313,1,18,2020-11-27 21:15,2020-11-28 3:55,2020-11-28 3:55,1,1,Intermediate,18
52993172,how to restart mysql service from mysql docker container,"I'm using tommylau/mysql docker image which provides mysql installed.
But after all i can't find how to restart mysql service inside running container (there is no mysql service or /etc/init.d/mysqld)
Any idea how to find how to restart mysql?
",<mysql><bash><docker>,244,0,0,401,1,3,9,37,39403,0,1,2,18,2018-10-25 15:35,2018-10-25 17:36,,0,,Basic,9
61783623,How to ping/test connection to SQL Server without software (like through cmd)?,"Is there a way to test a connection to a SQL Server through the command line or without any extra software? I've tried the ping and telnet methods shown in this article, but they both fail to find my SQL Server. Note that the connection is fine, I can connect to the server through SSMS, but it would be useful to be able to troubleshoot the connection otherwise.
For example, we have people working from home and I want to be able to test the connection to the database server without having to install SSMS on their machine.
The server name looks like: SERVER\SQLEXPRESS
And I tried ping SERVER\SQLEXPRESS (Ping request could not find host) and telnet SERVER 1433 (Could not open connection to the host, on port 1433)
EDIT: I can ping the server just fine ping SERVER
EDIT2: Everything I have tried I test with both the name and the ip. The DNS is fine so it shouldn't cause an issue to use one or the other
",<sql-server><cmd><ping>,910,1,4,629,2,8,20,51,47177,,60,1,18,2020-05-13 19:51,2021-01-28 14:46,2021-01-28 14:46,260,260,Basic,9
64412515,How to show generated SQL / raw SQL in TypeORM queryBuilder,"I developed typeorm querybuilder. For the purpose of debugging, I'd like to show the generated SQL query.
I tested printSql() method, but it didn't show any SQL query.
const Result = await this.attendanceRepository
  .createQueryBuilder(&quot;attendance&quot;)
  .innerJoin(&quot;attendance.child&quot;, &quot;child&quot;)
  .select([&quot;attendance.childId&quot;,&quot;child.class&quot;,&quot;CONCAT(child.firstName, child.lastName)&quot;])
  .where(&quot;attendance.id= :id&quot;, { id: id })
  .printSql()
  .getOne()
console.log(Result);
It returned the following:
Attendance { childId: 4, child: Child { class: 'S' } }
My desired result is to get the generated SQL query.
Is there any wrong point? Is there any good way to get the SQL query?
",<sql><typeorm>,748,0,13,4847,10,49,79,73,44685,0,448,2,18,2020-10-18 11:02,2020-10-18 13:06,2020-10-18 13:06,0,0,Basic,3
60711576,How do I write SQLAlchemy test fixtures for FastAPI applications,"I am writing a FastAPI application that uses a SQLAlchemy database. I have copied the example from the FastAPI documentation, simplifying the database schema for concisions' sake. The complete source is at the bottom of this post.
This works. I can run it with uvicorn sql_app.main:app and interact with the database via the Swagger docs. When it runs it creates a test.db in the working directory.
Now I want to add a unit test. Something like this.
from fastapi import status
from fastapi.testclient import TestClient
from pytest import fixture
from main import app
@fixture
def client() -&gt; TestClient:
    return TestClient(app)
def test_fast_sql(client: TestClient):
    response = client.get(&quot;/users/&quot;)
    assert response.status_code == status.HTTP_200_OK
    assert response.json() == []
Using the source code below, this takes the test.db in the working directory as the database. Instead I want to create a new database for every unit test that is deleted at the end of the test.
I could put the global database.engine and database.SessionLocal inside an object that is created at runtime, like so:
    class UserDatabase:
        def __init__(self, directory: Path):
            directory.mkdir(exist_ok=True, parents=True)
            sqlalchemy_database_url = f&quot;sqlite:///{directory}/store.db&quot;
            self.engine = create_engine(
                sqlalchemy_database_url, connect_args={&quot;check_same_thread&quot;: False}
            )
            self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
            models.Base.metadata.create_all(bind=self.engine)
but I don't know how to make that work with main.get_db, since the Depends(get_db) logic ultimately assumes database.engine and database.SessionLocal are available globally.
I'm used to working with Flask, whose unit testing facilities handle all this for you. I don't know how to write it myself. Can someone show me the minimal changes I'd have to make in order to generate a new database for each unit test in this framework?
The complete source of the simplified FastAPI/SQLAlchemy app is as follows.
database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={&quot;check_same_thread&quot;: False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
models.py
from sqlalchemy import Column, Integer, String
from database import Base
class User(Base):
    __tablename__ = &quot;users&quot;
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String)
    age = Column(Integer)
schemas.py
from pydantic import BaseModel
class UserBase(BaseModel):
    name: str
    age: int
class UserCreate(UserBase):
    pass
class User(UserBase):
    id: int
    class Config:
        orm_mode = True
crud.py
from sqlalchemy.orm import Session
import schemas
import models
def get_user(db: Session, user_id: int):
    return db.query(models.User).filter(models.User.id == user_id).first()
def get_users(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.User).offset(skip).limit(limit).all()
def create_user(db: Session, user: schemas.UserCreate):
    db_user = models.User(name=user.name, age=user.age)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user
main.py1
from typing import List
from fastapi import Depends, FastAPI, HTTPException
from sqlalchemy.orm import Session
import schemas
import models
import crud
from database import SessionLocal, engine
models.Base.metadata.create_all(bind=engine)
app = FastAPI()
# Dependency
def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()
@app.post(&quot;/users/&quot;, response_model=schemas.User)
def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    return crud.create_user(db=db, user=user)
@app.get(&quot;/users/&quot;, response_model=List[schemas.User])
def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    users = crud.get_users(db, skip=skip, limit=limit)
    return users
@app.get(&quot;/users/{user_id}&quot;, response_model=schemas.User)
def read_user(user_id: int, db: Session = Depends(get_db)):
    db_user = crud.get_user(db, user_id=user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail=&quot;User not found&quot;)
    return db_user
",<sqlalchemy><fastapi><test-fixture>,4587,2,135,16484,12,75,112,67,11667,0,627,1,18,2020-03-16 18:42,2020-04-05 15:56,2020-04-05 15:56,20,20,Intermediate,31
60925289,"HikariPool-1 - Connection marked as broken because of SQLSTATE(08006), ErrorCode(0)","After we setup PostgreSQL server with SSL certificates we get this error very often. 
This happens on the flows with a lot of interaction with the database(update a lot of entries/insert)   
HikariPool-1 - Connection marked as broken because of SQLSTATE(08006), ErrorCode(0)
j.n.SocketException: Connection reset by peer (Write failed)
    at j.n.SocketOutputStream.socketWrite0(SocketOutputStream.java)
    at j.n.SocketOutputStream.socketWrite(SocketOutputStream.java:110)
    at j.n.SocketOutputStream.write(SocketOutpu`enter code here`tStream.java:150)
    at s.s.s.SSLSocketOutputRecord.deliver(SSLSocketOutputRecord.java:320)
    at s.s.s.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:983)
    ... 63 common frames omitted
Wrapped by: j.n.s.SSLProtocolException: Connection reset by peer (Write failed)
    at s.security.ssl.Alert.createSSLException(Alert.java:126)
    at s.s.s.TransportContext.fatal(TransportContext.java:321)
    at s.s.s.TransportContext.fatal(TransportContext.java:264)
    at s.s.s.TransportContext.fatal(TransportContext.java:259)
    at s.s.s.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:988)
    at j.i.BufferedOutputStream.write(BufferedOutputStream.java:123)
    at j.i.FilterOutputStream.write(FilterOutputStream.java:108)
    at o.p.core.PGStream.send(PGStream.java:252)
    at o.p.c.v.QueryExecutorImpl.sendParse(QueryExecutorImpl.java:1440)
    at o.p.c.v.QueryExecutorImpl.sendOneQuery(QueryExecutorImpl.java:1767)
    at o.p.c.v.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1328)
    at o.p.c.v.QueryExecutorImpl.execute(QueryExecutorImpl.java:300)
    ... 56 common frames omitted
Wrapped by: o.p.u.PSQLException: An I/O error occurred while sending to the backend
",<postgresql><hikaricp>,1731,0,23,351,2,3,11,69,33014,0,1,3,18,2020-03-30 6:42,2020-04-29 17:16,,30,,Advanced,39
56772235,How do I pass an environment variable into a .sql file?,"I'm setting environment variables in a Dockerfile which I want to reference in a .sql file. That file is in /docker-entrypoint-initdb.d since initdb will run it.
How can pass an environment variable? (e.g. SELECT * FROM $myEnvironmentVariableHere;)
",<postgresql><docker><environment-variables>,249,0,4,181,1,1,4,59,11396,,0,1,18,2019-06-26 11:56,2019-06-26 14:04,,0,,Intermediate,15
49490640,"How to read "".gz"" compressed file using spark DF or DS?","I have a compressed file with .gz format, Is it possible to read the file directly using spark DF/DS? 
Details : File is csv with tab delimited.
",<apache-spark><apache-spark-sql><gzip><apache-spark-dataset>,145,0,0,573,4,9,24,37,40282,0,24,1,18,2018-03-26 11:43,2018-03-27 1:17,2018-03-27 1:17,1,1,Basic,3
54128295,"""ModuleNotFoundError: No module named 'pysqlcipher3'"" error while using Python 3.7 on windows 10","I am trying to decrypt one database file using Python 3.7. To decrypt it, I have to use pysqlcipher3 version for python 3.7. To install it, I have tried by using both commands:
pip3 install pysqlcipher3
and
pip install pysqlcipher3
and both the commands have showed successful installation of the pysqlcipher package. However, when I try to import pysqlcipher3 in my Python project by using this line:
from pysqlcipher3 import dbapi2 as sqlite
it displays this error:
ModuleNotFoundError: No module named 'pysqlcipher3
I have checked various GitHub projects, but none of them provide a clear working solution. The Python packages website says to install libsqlcipher in your OS but this time the issue is same, no documentation and link regarding for the installation of libsqlcipher for Windows 10. Can anyone provide me with proper installation steps, or any document, or any video tutorial, regarding the same? Or is there some issue with the import statement?
",<python><python-3.x><pysqlcipher>,964,0,9,1240,0,18,37,40,5195,,171,7,18,2019-01-10 12:03,2020-10-10 6:30,,639,,Basic,14
48160627,Partition data for efficient joining for Spark dataframe/dataset,"I need to join many DataFrames together based on some shared key columns. For a key-value RDD, one can specify a partitioner so that data points with same key are shuffled to same executor so joining is more efficient (if one has shuffle related operations before the join). Can the same thing can be done on Spark DataFrames or DataSets?
",<apache-spark><apache-spark-sql><partitioning><apache-spark-dataset>,339,0,2,1192,2,14,30,80,38848,0,120,2,18,2018-01-09 2:22,2018-01-09 7:08,2018-01-09 13:32,0,0,Intermediate,22
57371164,Django + Postgres: A string literal cannot contain NUL (0x00) characters,"I'm syncing a large amount of data and I'm getting this error back: A string literal cannot contain NUL (0x00) characters. Obviously this is a postgres problem, but I'm not quite sure how to solve it. Is there a way to strip null characters out at the Django model level? I have a large set of fields that I'm syncing.
",<python><django><postgresql>,319,0,1,4786,8,48,84,42,18800,0,459,2,18,2019-08-06 7:41,2019-08-06 8:39,,0,,Basic,14
49081896,converting a struct to a json when querying athena,"I have an athena table which I did not create or manage, but can query. one of the fields is a struct type. for the sake of the example let's suppose it looks like this:
my_field struct&lt;a:string,
                b:string,
                c:struct&lt;d:string,e:string&gt;
                &gt;
Now, I know how to query specific fields within this struct. But in one of my queries I need to extract the complete struct. so I just use:
select my_field from my_table
and the result looks like a string:
{a=aaa, b=bbb, c={d=ddd, e=eee}}
I want to get the result as a json string:
{""a"":""aaa"", ""b"":""bbb"",""c"":{""d"":""ddd"", ""e"":""eee""}}
this string will then be processed by another application, this is why i need it in json format.
How can I achieve this?
EDIT:
Better still, is there a way to query the struct in a way that flattens it? so the result would look like:
a   |   b   |   c.d  |  c.e   |
-------------------------------
aaa |   bbb |   ddd  |  eee   |
",<sql><json><struct><amazon-athena>,958,0,10,3342,6,25,32,69,12934,,31,3,18,2018-03-03 7:41,2018-03-04 22:35,,1,,Basic,10
48519633,SQLAlchemy cast boolean column to int,"I have a table of user records and I wish to see the average when some condition is met. the condition column is a boolean.
In Postgresql I could cast it easily:
select id, avg(cond::INT) from table group by id;
But in SQLAlchemy I couldn't find anything equivalent to ::INT.
How do i deal with the type conversion?
I have
orm_query(Table.id, func.avg(Table.cond))
which, of course, returns an error.
",<python><postgresql><sqlalchemy>,401,0,3,1891,3,21,37,78,34334,0,79,2,18,2018-01-30 10:52,2018-01-30 10:58,2018-01-30 10:58,0,0,Basic,1
62127983,ERROR 1449 (HY000): The user specified as a definer ('mysql.infoschema'@'localhost') does not exist,"I am trying to list all tables from mysql database on ubuntu os. But I am getting this error all time;
mysql&gt; use mysql;
Database changed
mysql&gt; show tables;
ERROR 1449 (HY000): The user specified as a definer ('mysql.infoschema'@'localhost') does not exist
I have checked my mysql version:
mysql  Ver 8.0.20 for Linux on x86_64 (MySQL Community Server - GPL)
So it seems it is last version of mysql.
How can I fix this error?
Please help
",<mysql><ubuntu>,445,0,5,1049,1,24,50,58,38635,0,3,5,18,2020-06-01 8:08,2020-06-01 9:26,2020-06-01 9:26,0,0,Basic,14
54667071,MySQL 5.6 - table locks even when ALGORITHM=inplace is used,"I'm running the following ALTER command on a MySQL 5.6 database on a large table with 60 million rows:
ALTER TABLE `large_table` ADD COLUMN `note` longtext NULL, 
ALGORITHM=INPLACE, LOCK=NONE;
Despite specifying both ALGORITHM=INPLACE and LOCK=NONE, the table gets locked and essentially takes down the app until the migration is complete.
I verified that the table was indeed locked by checking the value of the In_use column on the output of the SHOW OPEN TABLES command. It was set to 1.
From what I gather in the MySQL documentation, this operation should not be locking the table. And, MySQL is supposed to fail the command if it is not able to proceed without a lock. I upgraded the database to MySQL 5.7 to see if it's any better, but I face the same problem on 5.7 too.
Is this an expected behavior? How do I find out what's going wrong here?
",<mysql><table-locking>,851,0,8,13480,18,79,130,62,16731,0,438,2,18,2019-02-13 9:47,2019-02-18 18:45,2019-02-18 18:45,5,5,Intermediate,23
56676303,mismatched input 'from' expecting <EOF> SQL,"I am running a process on Spark which uses SQL for the most part. In one of the workflows I am getting the following error:
mismatched input 'from' expecting 
The code is
 select a.ACCOUNT_IDENTIFIER,a.LAN_CD, a.BEST_CARD_NUMBER,  
 decision_id, 
 case when a.BEST_CARD_NUMBER = 1 then 'Y' else 'N' end as best_card_excl_flag 
 from (select a.ACCOUNT_IDENTIFIER,a.LAN_CD, a. decision_id row_number()
 over (partition by CUST_GRP_MBRP_ID 
    order by coalesce(BEST_CARD_RANK,999)) as BEST_CARD_NUMBER 
 from Accounts_Inclusions_Exclusions_Flagged a) a 
I cannot figure out what the error is for the life of me
I've tried checking for comma errors or unexpected brackets but that doesn't seem to be the issue.
",<sql><apache-spark-sql>,709,0,7,179,1,1,3,81,239712,,0,4,17,2019-06-19 21:44,2019-06-19 21:54,,0,,Basic,10
50483132,How to secure access from App Service To Azure Sql Database using virtual network?,"Scenario
I want to use virtual network in order to limit access to Azure Database only from my App Service, so that I can turn of ""Allow access to App Services"" in firewall settings
What I have done:
I went to App Service -> Networking -> VNET Integration -> Setup -> Create New Virtual Network
I've created new VNET with default settings.
When VNET was created I went to App Service -> Networking -> VNET Integration and ensured that the VNET is connected
I went to SQL Firewall settigs -> Virtual Network -> Add existing Virtual Newtork and selected my VNET. I've left default subnet and address space: ""default / 10.0.0.0/24"" and I've left IgnoreMissingServiceEndpoint flag unchecked.
I can now see Microsoft.Sql service endpoint in my VNET:
Question
However, I'm still getting 
  SqlException: Cannot open server 'my-sqlserver' requested by the
  login. Client with IP address '52.233..' is not allowed to
  access the server.:
What am I missing?
",<azure><azure-sql-database><azure-web-app-service><firewall><azure-virtual-network>,951,2,0,25638,30,155,304,40,9425,0,236,4,17,2018-05-23 8:11,2018-05-23 8:34,2018-05-23 8:34,0,0,Intermediate,29
51050590,Run SQL script after start of SQL Server on docker,"I have a Dockerfile with below code
FROM microsoft/mssql-server-windows-express
COPY ./create-db.sql .
ENV ACCEPT_EULA=Y
ENV sa_password=##$wo0RD!
CMD sqlcmd -i create-db.sql
and I can create image but when I run container with the image I don't see created database on the SQL Server because the script is executed before SQL Server was started.
Can I do that the script will be execute after start the service with SQL Server?
",<t-sql><docker><dockerfile><sql-server-express>,429,0,5,1769,4,15,23,72,32817,0,161,2,17,2018-06-26 20:02,2018-06-26 21:14,2018-06-26 21:14,0,0,Basic,14
56759646,Docker Laravel Mysql: could not find driver,"When I run docker-compose up and do some composer commands, I get error
  In Connection.php line 664: could not find driver (SQL: select id, name from users  In Connector.php line 68: could not find driver ...
Why cant Laravel connect to mysql? I can do mysql -h db in docker-compose exec web bash and it works. 
My setup
docker-compose.yml
version: '3'
services:
  web:
    build: ./webserver
    ports:
      - ""80:80""
      - ""443:443""
    volumes:
      - //docker/dockertest/webserver/app:/var/www/vhosts/app
    links:
      - db
    command:
       - /usr/local/bin/apache2_install_composer_dependencies.sh
  db:
    image: mysql:8.0
    container_name: db
    ports:
      - ""3306:3306""
    command: --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_DATABASE: myDb
      MYSQL_USER: user
      MYSQL_PASSWORD: test
      MYSQL_ROOT_PASSWORD: test
    volumes:
      - //docker/dockertest/install/db_dump:/docker-entrypoint-initdb.d
      - persistent:/var/lib/mysql
    networks:
      - default
  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    links:
      - db:db
    ports:
      - 8000:80
    environment:
      MYSQL_USER: user
      MYSQL_PASSWORD: test
      MYSQL_ROOT_PASSWORD: test
volumes:
  persistent:
Laravel .env (I reference these values in config/database.php)
...
DB_CONNECTION=mysql
#host points to Docker container
DB_HOST=db
DB_PORT=3306
DB_DATABASE=myDb
DB_USERNAME=user
DB_PASSWORD=test
....
webserver/Dockerfile
FROM php:7.2.19-apache-stretch
# Configure Apache server
COPY config_apache/sites-available /etc/apache2/sites-available
# Create symlink in sites-enabled
WORKDIR /etc/apache2/sites-enabled
RUN ln -s /etc/apache2/sites-available/app.conf app.conf
RUN mkdir -p /var/www/vhosts/app/logs
COPY /build_files/install_composer_dependencies.sh /usr/local/bin/apache2_install_composer_dependencies.sh
RUN apt-get update -y &amp;&amp; apt-get install -y  curl nano libapache2-mod-geoip git zip unzip mysql-client
# Install Composer
RUN curl -o /tmp/composer-setup.php https://getcomposer.org/installer \
&amp;&amp; curl -o /tmp/composer-setup.sig https://composer.github.io/installer.sig \
# Verify installer
&amp;&amp; php -r ""if (hash('SHA384', file_get_contents('/tmp/composer-setup.php')) !== trim(file_get_contents('/tmp/composer-setup.sig'))) { unlink('/tmp/composer-setup.php'); echo 'Invalid installer' . PHP_EOL; exit(1); }"" \
&amp;&amp; php /tmp/composer-setup.php --no-ansi --install-dir=/usr/local/bin --filename=composer --snapshot \
&amp;&amp; rm -f /tmp/composer-setup.*
RUN a2enmod rewrite
RUN a2enmod geoip
RUN service apache2 restart
",<php><mysql><laravel><docker><docker-compose>,2621,2,93,1306,1,16,30,39,27464,0,1512,2,17,2019-06-25 17:57,2019-06-25 18:11,2019-06-26 12:17,0,1,Basic,14
65117930,Postgresql ERROR: cannot drop columns from view,"I'm new to postgresql. Can anyone suggest the reason and solution to this error? However it works if I select an extra sum(s.length) but i don't won't this in my results.
code:
create or replace view q1(&quot;group&quot;,album,year)
as
select g.name, a.title, a.year
from Groups g, Albums a, Songs s
where a.made_by = g.id and s.on_album = a.id
group by a.title, g.name, a.year
order by sum(s.length) desc 
limit 1
;
Error message:
ERROR:  cannot drop columns from view
",<sql><postgresql>,470,0,9,267,2,3,5,41,15079,,0,1,17,2020-12-02 23:59,2020-12-03 1:07,,1,,Intermediate,18
52067058,How to autogenerate UUID for Postgres in Python?,"I'm trying to create objects in Postgres db.
I'm using this approach https://websauna.org/docs/narrative/modelling/models.html#uuid-primary-keys
class Role(Base):
    __tablename__ = 'role'
    # Pass `binary=False` to fallback to CHAR instead of BINARY
    id = sa.Column(UUIDType(binary=False), primary_key=True)
But when I create object 
user_role = Role(name='User')
db.session.add(user_role)
db.session.commit()
I have the following error:
sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) null value in column ""id"" violates not-null constraint
Looks like I didn't provide any ID. So, how I can make the database auto-generate it or generate on my own?
",<python><postgresql><sqlalchemy><uuid>,665,2,9,4991,7,54,95,78,21639,0,517,4,17,2018-08-28 22:33,2018-08-28 23:20,2018-08-29 6:56,0,1,Basic,9
54024991,Error in mysqldump: mysqldump: [ERROR] unknown variable 'database=someDb',"I am getting the following error:  
 mysqldump: [ERROR] unknown variable 'database=myDB1'
when I run this command:
 mysqldump -u root -p myDB2  &gt; someFile
There is a db by the name myDB1 and of course, a db by the name myDB2.
This worked before (or so I think). I do not recollect changing my.cnf or any other configuration file. 
",<mysql>,334,0,2,966,1,9,25,64,12414,,81,1,17,2019-01-03 15:11,2019-03-16 13:22,,72,,Basic,14
57107452,Why does Postgres choose index scan instead of index seek to fetch one record by primary key?,"I have the following table:
create table documents 
(
     id serial not null primary key,
     key varchar(50) not null,
     document jsonb
);
It has over 100M records and when I run a query to get 1 record by primary key:
 select * from documents where id = 20304050
It uses the index scan to get it:
Index Scan using documents_pkey on documents (cost=0.57..8.59 rows=1 width=533) (actual time=0.010..0.011 rows=0 loops=1)
  Index Cond: (id = 20304050)
Planning Time: 0.070 ms
Execution Time: 0.024 ms
Why does Postgres choose to use an index scan instead of an index seek?
Edit:
I came from the SQL Server world where it was a distinction between an index scan and an index seek. In Postgres, there is no such thing as an index seek.
",<sql><postgresql>,738,0,11,2251,0,19,22,50,3989,,84,1,17,2019-07-19 7:22,2022-10-26 4:42,2022-10-26 4:42,1195,1195,Intermediate,23
59843904,Postgres full text search and spelling mistakes (aka fuzzy full text search),"I have a scenario, where I have data for informal communications that I need to be able to search. Therefore I want full text search, but I also to make sense of spelling mistakes. Question is how do I take spelling mistakes into account in order to be able to do fuzzy full text search??
This is very briefly discussed in Postgres Full Text Search is Good Enough where the article discusses misspelling.
So I have built a table of ""documents"", created indexes etc.
CREATE TABLE data (
  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, 
  text TEXT NOT NULL);
I can create an additional column of type tsvector and index accordingly...
alter table data 
  add column search_index tsvector 
  generated always as (to_tsvector('english', coalesce(text, ''))) 
  STORED;
create index search_index_idx on data using gin (search_index);
I have for example, some text where the data says ""baloon"", but someone may search ""balloon"", so I insert two rows (one deliberately misspelled)...
insert into data (text) values ('baloon');
insert into data (text) values ('balloon');
select * from data;
id |  text   | search_index
----+---------+--------------
 1 | baloon  | 'baloon':1
 2 | balloon | 'balloon':1
... and perform full text searches against the data...
select * from data where search_index @@ plainto_tsquery('balloon');
 id |  text   | search_index
----+---------+--------------
  2 | balloon | 'balloon':1
(1 row)
But I don't get back results for the misspelled version ""baloon""... So using the suggestion in the linked article I've built a lookup table of all the words in my lexicon as follows...
  ""you may obtain good results by appending the similar lexeme to your tsquery""
CREATE TABLE data_words AS SELECT word FROM ts_stat('SELECT to_tsvector(''simple'', text) FROM data');
CREATE INDEX data_words_idx ON data_words USING GIN (word gin_trgm_ops);
... and I can search for similar words which may have been misspelled
select word, similarity(word, 'balloon') as similarity from data_words where similarity(word, 'balloon') > 0.4 order by similarity(word, 'balloon');
  word   | similarity
---------+------------
 baloon  |  0.6666667
 balloon |          1
... but how do I actually include misspelled words in my query?
Isn't this what the article above means?
select plainto_tsquery('balloon' || ' ' || (select string_agg(word, ' ') from data_words where similarity(word, 'balloon') &gt; 0.4));
         plainto_tsquery
----------------------------------
 'balloon' &amp; 'baloon' &amp; 'balloon'
(1 row)
... plugged into an actual search, and I get no rows!
select * from data where text @@ plainto_tsquery('balloon' || ' ' || (select string_agg(word, ' ') from data_words where similarity(word, 'balloon') &gt; 0.4));
select * from data where search_index @@ phraseto_tsquery('baloon balloon'); -- no rows returned
I'm not sure where I'm going wrong here - can any shed any light? I feel like I'm super close to getting this going...?
",<postgresql><full-text-search><fuzzy-search>,2953,1,40,4829,3,29,42,56,6413,0,81,2,17,2020-01-21 15:03,2020-01-21 17:29,2020-01-21 17:29,0,0,Advanced,40
52975567,get first N elements from dataframe ArrayType column in pyspark,"I have a spark dataframe with rows as - 
1   |   [a, b, c]
2   |   [d, e, f]
3   |   [g, h, i]
Now I want to keep only the first 2 elements from the array column.
1   |   [a, b]
2   |   [d, e]
3   |   [g, h]
How can that be achieved?    
Note - Remember that I am not extracting a single array element here, but a part of the array which may contain multiple elements.
",<apache-spark><pyspark><apache-spark-sql>,369,0,6,768,3,11,25,72,30295,0,137,2,17,2018-10-24 18:15,2018-10-25 12:51,2018-10-25 14:09,1,1,Basic,10
59269813,"Writing more than 50 millions from Pyspark df to PostgresSQL, best efficient approach","What would be the most efficient way to insert millions of records say 50-million from a Spark dataframe to Postgres Tables.
I have done this from spark to 
MSSQL in the past by making use of bulk copy and batch size option which was successful too.
Is there something similar that can be here for Postgres?
Adding the code I have tried and the time it took to run the process:
def inserter():
    start = timer()
    sql_res.write.format(""jdbc"").option(""numPartitions"",""5"").option(""batchsize"",""200000"")\
    .option(""url"", ""jdbc:postgresql://xyz.com:5435/abc_db"") \
    .option(""dbtable"", ""public.full_load"").option(""user"", ""root"").option(""password"", ""password"").save()
    end = timer()
    print(timedelta(seconds=end-start))
inserter()
So I did the above approach for 10 million records and had 5 parallel connections as specified in numPartitions and also tried batch size of 200k.
The total time it took for the process was 0:14:05.760926 (fourteen minutes and five seconds).
Is there any other efficient approach which would reduce the time?
What would be the efficient or optimal batch size I can use ? Will increasing my batch size do the job quicker ? Or opening multiple connections i.e > 5 help me make the process quicker ?
On an average 14 mins for 10 million records is not bad, but looking for people out there who would have done this before to help answer this question.
",<postgresql><apache-spark><pyspark><apache-spark-sql><bigdata>,1389,0,9,2414,1,13,35,51,6942,0,145,1,17,2019-12-10 14:46,2019-12-21 15:57,2019-12-21 15:57,11,11,Advanced,40
62107273,"How to build .sqlproj projects using ""dotnet"" tool?","Unable to build .Net .sqlproj using ""dotnet"" command line tool. Here's the error:
dotnet\sdk\3.1.300\Microsoft\VisualStudio\v11.0\SSDT\Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found
I have installed SSDT tool but it's not available at the above location. 
Note: It's buildings without any error using Visual Studio and MSBuild
",<.net><asp.net-mvc><azure-devops><sql-server-data-tools><dotnet-tool>,340,0,1,240,1,4,11,76,14762,0,50,4,17,2020-05-30 18:53,2020-06-01 2:06,2020-06-01 2:06,2,2,Basic,7
51917812,MySQL Error 1064 when adding foreign key with MySQL Workbench,"So I have an error, when I want to add a foreign key. I added the foreign key in MySQL Workbench with an EER Diagram Model. The lines workbench tries to add are:`
CREATE SCHEMA IF NOT EXISTS `barberDB` DEFAULT CHARACTER SET latin1 ;
USE `barberDB` ;
-- -----------------------------------------------------
-- Table `barberDB`.`BARBER`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `barberDB`.`BARBER` (
  `ID` INT NOT NULL AUTO_INCREMENT,
  `name` VARCHAR(45) NULL,
  PRIMARY KEY (`ID`))
ENGINE = InnoDB;
-- -----------------------------------------------------
-- Table `barberDB`.`CUSTOMER`
-- -----------------------------------------------------
CREATE TABLE IF NOT EXISTS `barberDB`.`CUSTOMER` (
  `ID` INT NOT NULL AUTO_INCREMENT,
  `name` VARCHAR(45) NULL,
  `isHaircut` INT NULL,
  `isBeard` INT NULL,
  `isEyebrows` INT NULL,
  `BARBER_ID` INT NOT NULL,
  PRIMARY KEY (`ID`),
  INDEX `fk_CUSTOMER_BARBER_idx` (`BARBER_ID` ASC) VISIBLE,
  CONSTRAINT `fk_CUSTOMER_BARBER`
    FOREIGN KEY (`BARBER_ID`)
    REFERENCES `barberDB`.`BARBER` (`ID`)
    ON DELETE CASCADE
    ON UPDATE CASCADE)
ENGINE = InnoDB;
SET SQL_MODE=@OLD_SQL_MODE;
SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS;
SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS;
The error I get is:
ERROR: Error 1064: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'VISIBLE,
  CONSTRAINT `fk_CUSTOMER_BARBER`
    FOREIGN KEY (`BARBER_ID`)
    REF' at line 12
SQL Code:
        -- -----------------------------------------------------
        -- Table `barberDB`.`CUSTOMER`
        -- -----------------------------------------------------
        CREATE TABLE IF NOT EXISTS `barberDB`.`CUSTOMER` (
          `ID` INT NOT NULL AUTO_INCREMENT,
          `name` VARCHAR(45) NULL,
          `isHaircut` INT NULL,
          `isBeard` INT NULL,
          `isEyebrows` INT NULL,
          `BARBER_ID` INT NOT NULL,
          PRIMARY KEY (`ID`),
          INDEX `fk_CUSTOMER_BARBER_idx` (`BARBER_ID` ASC) VISIBLE,
          CONSTRAINT `fk_CUSTOMER_BARBER`
            FOREIGN KEY (`BARBER_ID`)
            REFERENCES `barberDB`.`BARBER` (`ID`)
            ON DELETE CASCADE
            ON UPDATE CASCADE)
        ENGINE = InnoDB
SQL script execution finished: statements: 6 succeeded, 1 failed
Fetching back view definitions in final form.
Nothing to fetch
I searched for solutions and find that parenthesis are important or backticks, but since the code is created by the tool, it seems correct to me. 
What could cause the error?
",<mysql><sql><mysql-workbench>,2597,0,64,427,1,4,20,63,23334,0,23,7,17,2018-08-19 12:46,2019-01-23 6:41,2019-01-23 6:41,157,157,Basic,13
50008226,Query to display spent credits from transactional table,"I am working with a table that contains credit transactions where I want to display who's credits were spent when a sale is made. 
In the table:
Credits are added by an entity using a unique entity code (recorded in column GivenByUserCode)
Credit additions always have such a code.
Credits that are spent will always have a negative value.
Credits that are spent will not have an entity code (value of GivenByUserCode is null).
Using the above data as an example if a user makes a purchase on 2018-01-02 the report should show all these credits originated from BM01. What add's complexity is that a purchase could be split over multiple additions, see the purchase on 2018-02-03 which is divided over 3 additions.
I think the solution will have something to do with using cte and over but I have no experience using these. I did find a similar (not same) problem on SqlServerCentral.
Any help / direction would be most appreciated.
Input and DDL
DECLARE @CreditLogs TABLE(CreditLogId int not null identity(1,1), Credits INT NOT NULL, OccurredOn DATETIME2(7) NOT NULL, GivenByUserCode VARCHAR(100) NULL)
INSERT INTO @CreditLogs (Credits, OccurredOn, GivenByUserCode) VALUES
  (10,  '2018-01-01', 'BM01')
, (10,  '2018-01-01', 'BM01')
, (-10, '2018-01-02', NULL)
, (-5,  '2018-01-04', NULL)
, (5,   '2018-02-01', 'SP99')
, (40,  '2018-02-02', 'BM02')
, (-40, '2018-02-03', NULL)
, (-4,  '2018-03-05', NULL)
Input in table form
CreditLogId | Credits | OccurredOn | GivenByUserCode
------------+---------+------------+----------------
          1 |      10 | 2018-01-01 |            BM01
          2 |      10 | 2018-01-01 |            BM01
          3 |     -10 | 2018-01-02 |            NULL
          4 |      -5 | 2018-01-04 |            NULL
          5 |       5 | 2018-02-01 |            SP99
          6 |      40 | 2018-02-02 |            BM02
          7 |     -40 | 2018-02-03 |            NULL
          8 |      -4 | 2018-03-05 |            NULL
Expected output
SELECT *
FROM (VALUES
     (3, '2018-01-02', 10, 'BM01')
    ,(4, '2018-01-04', 5, 'BM01')
    ,(7, '2018-02-03', 5, 'BM01')
    ,(7, '2018-02-03', 5, 'SP99')
    ,(7, '2018-02-03', 30, 'BM02')
    ,(8, '2018-03-05', 4, 'BM02')
) expectedOut (CreditLogId, OccurredOn, Credits, GivenByUserCode)
Produces output
CreditLogId | Occurred on | Credits | GivenByUserCode
------------+-------------+---------+----------------
          3 |  2018-01-02 |      10 |            BM01
          4 |  2018-01-04 |       5 |            BM01
          7 |  2018-02-03 |       5 |            BM01
          7 |  2018-02-03 |       5 |            SP99
          7 |  2018-02-03 |      30 |            BM02
          8 |  2018-03-05 |       4 |            BM02
Code so far
It's not much and I am not sure where to go from here.
WITH totals AS (
    SELECT CreditLogId, OccurredOn, credits, sum(credits) OVER(ORDER BY OccurredOn) AS TotalSpent
    FROM @CreditLogs
    WHERE Credits &lt; 0
)
SELECT *
FROM totals
Additional clarification
The expected output is for each spent credit amount where those credits came from. Credits are spent in on a first in first out (FIFO) basis. Here an explanation of each value in the sample output in the hope that this clarifies the desired output.
For the spending of 10 credits (credit log id 3) can be traced back to an addition from credit log id 1
For the spending of 5 credits (credit log id 4) can be traced back to an addition from credit log id 2 (as credit log id 1 was ""used up"")
For the spending of 40 credits in credit log id 7 can be traced back to
Remainder of addition from credit log id 2, 5 credits
Credit log id 5 (addition of 5)
Credit log id 6 (addition of 40 so 10 remaining)
For the spending of 4 credits in credit log 8 the balance of credit log id 6 is used
Note that a total balance of 6 credits remains, the balance does not have to zero out but will never be in the negative as users can only spend what they have.
",<sql><sql-server><sql-server-2012>,3932,3,52,61121,10,101,175,78,627,0,1620,2,17,2018-04-24 18:06,2018-04-26 22:51,2018-04-26 23:29,2,2,Basic,10
56643961,Initialize Postgres db in Docker Compose,"I have the following docker-compose.yml file:
version: '3'
services:
  postgres:
    image: postgres
    container_name: postgres
    ports:
      - ""5431:5432""
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=anime
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
This configuration starts a Postgres database. In volume I defined init.sql, which should set up a table:
CREATE TABLE anime ( 
  anime_id INT PRIMARY KEY,
  title TEXT
);
Then, I would like to fill the Postgres database with data from the CSV file.
I tried to add another volume to docker-compose:
 - ./preload.sql:/preload/preload.sql
with that script:
copy anime FROM 'docker/data/AnimeList.csv' DELIMITER ',' CSV HEADER;
The CSV file is located in the data folder, on the same level as docker-compose.yml.
But it is not working. The database is created correctly, but it doesn't have the table and data. When I connect to Docker container, run 'psql command and try to get anime table, I get the following error:
Did not find any relation named ""anime"".
My question is: how to preload the Postgres container with the CSV data file in docker-compose?
",<postgresql><docker><docker-compose>,1193,0,26,1559,2,20,45,41,24355,0,500,1,17,2019-06-18 7:36,2019-06-26 13:41,2019-06-26 13:41,8,8,Basic,14
51974855,Amazon Athena: Convert bigint timestamp to readable timestamp,"I am using Athena to query the date stored in a bigInt format. I want to convert it to a friendly timestamp.
I have tried:
from_unixtime(timestamp DIV 1000) AS readableDate
And
to_timestamp((timestamp::bigInt)/1000, 'MM/DD/YYYY HH24:MI:SS') at time zone 'UTC' as readableDate
I am getting errors for both. I am new to AWS. Please help!
",<sql><amazon-web-services><amazon-athena><presto>,336,0,2,417,2,6,11,64,23840,0,0,1,17,2018-08-22 20:46,2019-10-01 6:50,,405,,Basic,10
54507881,Docker-compose : mysqld: Can't create/write to file '/var/lib/mysql/is_writable' (Errcode: 13 - Permission denied),"I'm having an issue when starting the db service with docker compose:
 version: '3'
 services:
 # Mysql DB
    db:
        image: percona:5.7
        #build: ./docker/mysql
        volumes:
          - ""./db/data:/var/lib/mysql""
          - ""./db/init:/docker-entrypoint-initdb.d""
          - ""./db/backups:/tmp/backups""
          - ""./shared/home:/home""   
          - ""./shared/root:/home""  
        restart: unless-stopped
        environment:
            MYSQL_ROOT_PASSWORD: root
            MYSQL_DATABASE: db_name
            MYSQL_USER: user
            MYSQL_PASSWORD: pass
       ports:
       - ""3307:3306""
I have tried everything with no luck:
""./db/data:/var/lib/mysql:rw""
Creating a dockerfile and create from build instead of image:
FROM percona:5.7
RUN adduser mysql
RUN sudo chown mysql /var/lib/mysql
RUN sudo chgrp mysql /var/lib/mysql
Also I have tried to add a user on db service:
user: ""1000:50""
But any of those could solve that.. What I'm missing? 
MySQL 5.7 installation error `mysqld: Can&#39;t create/write to file &#39;/var/lib/mysql/is_writable&#39;`
",<mysql><docker><docker-compose>,1080,1,28,1554,2,17,31,80,31464,0,791,7,17,2019-02-03 21:42,2019-02-03 22:22,2019-02-04 7:21,0,1,Basic,14
54471608,"Unable to connect from Intellij to mySql running in docker container - ""specified database user/password combination is rejected""","Currently unable to connect from Intellij to mySql running locally on docker container on ubuntu.
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| tasklogs           |
+--------------------+
+----------+-----------+------+
| DATABASE | HOST      | USER |
+----------+-----------+------+
| tasklogs | localhost | dev  |
| tasklogs | localhost | root |
+----------+-----------+------+
+-----------------------------------------------------------+
| Grants for dev@localhost                                  |
+-----------------------------------------------------------+
| GRANT USAGE ON *.* TO `dev`@`localhost`                   |
| GRANT ALL PRIVILEGES ON `tasklogs`.* TO `dev`@`localhost` |
+-----------------------------------------------------------+
docker ps -a:
When I connect via intellij:
i.e. ""The specified database user/password combination is rejected: [28000][1045] Access denied for user 'dev'@'localhost' (using password: YES)""
I am putting in the right password.
Any help really appreciated.
Thanks,
",<mysql><docker><intellij-idea>,1131,2,23,1474,1,13,31,45,21589,0,210,4,17,2019-02-01 1:17,2019-02-01 9:56,2019-02-01 9:56,0,0,Basic,13
54835516,windows subsystem install mysql server,"installing mysql-server on ubuntu 18.04 subsystem windows 10.
Cannot open /proc/net/unix: No such file or directory
Cannot stat file /proc/1/fd/5: Operation not permitted
Cannot stat file /proc/3/fd/7: Operation not permitted
help please
",<mysql><windows-10><windows-subsystem-for-linux>,238,1,3,270,1,2,6,64,14054,0,1,6,17,2019-02-22 21:35,2019-03-19 20:45,,25,,Basic,14
49230311,Is it possible to enforce SQLite datatypes?,"From my understanding, sqlite's datatype isn't associated to its column but to the data themselves: this practically means that you can insert any data to any column.
Is it possible to prohibit such a behaviour? I mean, I want sqlite to raise an error (or at least a warning) when I accidentally try to insert text to integer, for example.
",<sqlite>,340,0,0,2568,4,25,48,76,2563,0,137,3,17,2018-03-12 7:45,2018-03-12 9:41,2018-03-12 17:44,0,0,Basic,4
53662948,How do I know if any index is used in a query | PostgreSQL 11?,"I am little bit confused and need some advice. I use PostgreSQL 11 database. I have such pretty simple sql statement:
SELECT DISTINCT ""CITY"", ""AREA"", ""REGION""
    FROM youtube
WHERE
    ""CITY"" IS NOT NULL
AND
    ""AREA"" IS NOT NULL
AND
    ""REGION"" IS NOT NULL
youtube table which I use in sql statement has 25 million records. I think for thats why query takes 15-17 seconds to complete. For web project where I use that query it's too long. I'm trying to speed up the request.
I create such index for youtube table:
CREATE INDEX youtube_location_idx ON public.youtube USING btree (""CITY"", ""AREA"", ""REGION"");
After this step I run query again but it takes the same time to complete. It seems like query don't use index. How do I know if any index is used in a query?
EXPLAIN ANALYZE return:
",<sql><postgresql><sql-execution-plan><postgresql-performance>,792,1,11,4874,17,89,195,58,27309,0,345,3,17,2018-12-07 3:47,2018-12-07 4:33,2018-12-07 4:33,0,0,Basic,10
51446322,Flask SQLAlchemy - set expire_on_commit=False only for current session,"How can I set the option expire_on_commit=False only for the current session in Flask-SQLAlchemy?
I can set the option at the inizialization of the SQLAlchemy object with:
db = SQLAlchemy(app, session_options={""expire_on_commit"": False})
but in this way all sessions created by Flask-SQLAlchemy will have the option set to False, instead i want to set it only for one session.
I tried db.session.expire_on_commit = False but it does not seem to have any effect.
",<python><flask><sqlalchemy><flask-sqlalchemy>,462,0,4,607,2,8,16,67,15100,0,24,1,17,2018-07-20 15:56,2018-07-21 3:07,2018-07-21 3:07,1,1,Basic,6
57435858,How do you create merge_asof functionality in PySpark?,"Table A has many columns with a date column, Table B has a datetime and a value. The data in both tables are generated sporadically with no regular interval. Table A is small, table B is massive.
I need to join B to A under the condition that a given element a of A.datetime corresponds to
B[B['datetime'] &lt;= a]]['datetime'].max()
There are a couple ways to do this, but I would like the most efficient way.
Option 1
Broadcast the small dataset as a Pandas DataFrame. Set up a Spark UDF that creates a pandas DataFrame for each row merges with the large dataset using merge_asof.
Option 2
Use the broadcast join functionality of Spark SQL: set up a theta join on the following condition
B['datetime'] &lt;= A['datetime']
Then eliminate all the superfluous rows.
Option B seems pretty terrible... but please let me know if the first way is efficient or if there is another way.
EDIT: Here is the sample input and expected output:
A =
+---------+----------+
| Column1 | Datetime |
+---------+----------+
|    A    |2019-02-03|
|    B    |2019-03-14|
+---------+----------+
B =
+---------+----------+
|   Key   | Datetime |
+---------+----------+
|    0    |2019-01-01|
|    1    |2019-01-15|
|    2    |2019-02-01|
|    3    |2019-02-15|
|    4    |2019-03-01|
|    5    |2019-03-15|
+---------+----------+
custom_join(A,B) =
+---------+----------+
| Column1 |   Key    |
+---------+----------+
|    A    |     2    |
|    B    |     4    |
+---------+----------+
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,1465,0,38,729,0,8,20,73,5735,0,31,3,17,2019-08-09 19:08,2019-08-13 14:21,2021-09-02 10:27,4,755,Intermediate,22
59682554,Android Q: SQLite database in scoped storage,"In Android Q introduced new Scoped storage feature, which says:
  apps that target Android 10 (API level 29) and higher are given scoped access into external storage, or scoped storage, by default. Such apps have access only to the app-specific directory on external storage, as well as specific types of media that the app has created.
I have my app that creates SQLite database on external storage, such that when app uninstalls database still alive and can be used later as recovery or be used outside of Android device (let's say in PC)
How should I achieve the same effect with Android Q? More precisely if database stored in external public directory - how can I read this database using standard SQLiteOpenHelper?
",<java><android><sqlite><kotlin><android-sqlite>,721,1,1,16567,18,74,146,74,1404,0,684,1,17,2020-01-10 13:21,2020-01-17 17:38,,7,,Basic,3
52123133,Rails jsonb - Prevent JSON keys from reordering when jsonb is saved to Postgresql database,"I have a column amount_splits that I need to save my JSON to in the key order I've specified.
How do I prevent Rails / Postgres jsonb from auto sorting my JSON keys when I save it to the database? (for creating or updating)
It looks like it's trying to sort alphabetically, but does a poor job at it.
Here's what I'm saving:
{
    ""str_fee"": 3.17,       # key 1
    ""eva_fee"": 14.37,      # key 2
    ""fran_royalty"": 14.37, # key 3
    ""fran_amount"": 67.09   # key 4
}
This is how it actually saves:
{
    ""eva_fee"": 14.37,     # key 2
    ""str_fee"": 3.17,      # key 1
    ""fran_amount"": 67.09, # key 4
    ""fran_royalty"": 14.37 # key 3
}
Purpose:
Before you answer ""sorting doesn't matter when the JSON is consumed on the receiving end"", stop and think first please... and please read on
I need the keys to be sorted in the way I need them sorted because the client interface that consumes this JSON is displaying the JSON to developers that need the keys to be in the order that the documentation tells them its in. And the reason it needs to be in that order is to display the process of what calculations happened in which order first:
The correct order tells the developer:
The str_fee was applied first, then the eva_fee, then the fran_royalty... making fran_amount the ending amount.
But based on how jsonb sorts this, it incorrectly tells our developers that:
The eva_fee was applied first, then the str_fee, then the fran_amount... making fran_royalty the ending amount.
",<ruby-on-rails><json><postgresql><ruby-on-rails-5><jsonb>,1481,0,23,2203,5,30,40,58,9292,0,2678,4,17,2018-08-31 22:12,2018-08-31 22:12,2018-08-31 22:12,0,0,Intermediate,23
61663848,Sequelize.authenticate() do not working (no success or error response) for google cloud sql connection inside Docker,"I am building api server with typescript ,express and Sequelize.
This is my database connection class.
export class Database {
  private _sequelize: Sequelize;
  private config: DBConfigGroup = dbConfig;
  private env = process.env.NODE_ENV as string;
  constructor() {
    this._sequelize = new Sequelize({
      dialect: 'postgres',
      database: this.config[this.env].database,
      username: this.config[this.env].username,
      password: this.config[this.env].password,
      host: this.config[this.env].host,
    });
  }
  async connect(): Promise&lt;void&gt; {
    try {
      console.log('start connect');
      await this._sequelize.authenticate();
      console.log('Connection has been established successfully.'.green.bold);
    } catch (error) {
      console.error('Unable to connect to the database:'.red.bold, error);
    }
  }
  async close(): Promise&lt;void&gt; {
    try {
      await this._sequelize.close();
      console.log('Connection has been close successfully.'.red.bold);
    } catch (error) {
      console.error('Unable to close to the database:'.yellow.bold, error);
    }
  }
  get sequelize(): Sequelize {
    return this._sequelize;
  }
}
So in my server.ts, i was calling this
dotenv.config({
  path: './config/environments/config.env',
});
const database = new Database();
console.log('Init db');
database
  .connect()
  .then(() =&gt; {
    console.log('success db ininit');
  })
  .catch((err) =&gt; {
    console.log('fail db init: ', err);
  }); 
/**
 * Server Activation
 */
const server = app.listen(PORT, () =&gt; {
  console.log(`Server running in ${process.env.NODE_ENV} mode on port ${PORT}`.yellow.bold);
});
So everythings works fine in my local development 
But when i try to run it with docker compose with this config
// docker-compose.yml
version: '3.4'
services:
  api:
    stdin_open: true
    tty: true
    build:
      context: ./api
      dockerfile: Dockerfile.dev
    ports:
      - ""5000:5000""
    volumes:
      - /app/node_modules
      - ./api:/app
This is my Dockerfile.dev file
FROM node:alpine
ENV NODE_ENV=development
WORKDIR /app
EXPOSE 5000
COPY package.json package-lock.json* ./ 
RUN npm install &amp;&amp; npm cache clean --force
COPY . .
CMD [""npm"", ""run"", ""dev:docker""]
But the problem is when i run docker-compose up --build
I only see these logs
api_1  | Init db
api_1  | start connect
api_1  | Server running in development mode on port 5000
So basiccally there is no response whether sequelize.authenticate() is success or fail, 
just Server running in development mode on port 5000 log out and nothing after all. 
Why don't it works inside docker like in the local, any config that i missed?
Thanks
",<node.js><google-cloud-platform><docker-compose><sequelize.js><google-cloud-sql>,2683,0,100,3967,9,40,76,52,9743,0,43,2,17,2020-05-07 17:19,2020-05-14 7:49,,7,,Intermediate,23
50630056,Spring Projections not returning the State Details,"I have a Country and State table for which I have integrated with Spring Data JPA. I have created a function public Page&lt;CountryDetails&gt; getAllCountryDetails in my CountryServiceImpl for getting all the Country and the corresponding State details. The service is working fine and is giving me the below output:
{
  ""content"": [
    {
      ""id"": 123,
      ""countryName"": ""USA"",
      ""countryCode"": ""USA"",
      ""countryDetails"": ""XXXXXXXX"",
      ""countryZone"": ""XXXXXXX"",
      ""states"": [
        {
          ""id"": 23,
          ""stateName"": ""Washington DC"",
          ""countryCode"": ""USA"",
          ""stateCode"": ""WAS"",
          ""stateDetails"": ""XXXXX"",
          ""stateZone"": ""YYYYYY""
        },
        {
          ""id"": 24,
          ""stateName"": ""Some Other States"",
          ""countryCode"": ""USA"",
          ""stateCode"": ""SOS"",
          ""stateDetails"": ""XXXXX"",
          ""stateZone"": ""YYYYYY""
        }
      ]
    }
  ],
  ""last"": false,
  ""totalPages"": 28,
  ""totalElements"": 326,
  ""size"": 12,
  ""number"": 0,
  ""sort"": null,
  ""numberOfElements"": 12,
  ""first"": true
}
My Complete code is as given below:
CountryRepository.java
@Repository
public interface CountryRepository extends JpaRepository&lt;CountryDetails, Integer&gt; {
    @Query(value = ""SELECT country FROM Country country GROUP BY country.countryId ORDER BY ?#{#pageable}"", 
    countQuery = ""SELECT COUNT(*) FROM Country country GROUP BY country.countryId ORDER BY ?#{#pageable}"")
    public Page&lt;CountryDetails&gt; findAll(Pageable pageRequest);
}
CountryServiceImpl.java
@Service
public class CountryServiceImpl implements CountryService {
    @Autowired
    private CountryRepository countryRepository;
    @Override
    public Page&lt;CountryDetails&gt; getAllCountryDetails(final int page, final int size) {
        return countryRepository.findAll(new PageRequest(page, size));
    }
}
CountryDetails.java
@Entity
@Table(name = ""country"", uniqueConstraints = @UniqueConstraint(columnNames = ""id""))
public class CountryDetails {
    @Id
    @GeneratedValue
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    private String countryName;
    private String countryCode;
    private String countryDetails;
    private String countryZone;
    @JsonManagedReference
    @OneToMany(fetch = FetchType.LAZY, mappedBy = ""countryDetails"")
    private List&lt;State&gt; states;
    // getters / setters omitted
}
State.java
@Entity
@Table(name = ""state"", uniqueConstraints = @UniqueConstraint(columnNames = ""id""))
public class State {
    @Id
    @GeneratedValue
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    private String stateName;
    private String countryCode;
    private String stateCode;
    private String stateDetails;
    private String stateZone;
    @JsonBackReference
    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = ""countryCode"", nullable = false, insertable = false, updatable = false, foreignKey = @javax.persistence.ForeignKey(name=""none"",value = ConstraintMode.NO_CONSTRAINT))
    private CountryDetails countryDetails;
    // getters / setters omitted
}
Now the Problem
Actually what I want the country service to return with minimal information like as shown below
{
  ""content"": [
    {
      ""countryName"": ""USA"",
      ""countryCode"": ""USA"",
      ""states"": [
        {
          ""stateCode"": ""WAS""
        },
        {
          ""stateCode"": ""SOS""
        }
      ]
    }
  ],
  ""last"": false,
  ""totalPages"": 28,
  ""totalElements"": 326,
  ""size"": 12,
  ""number"": 0,
  ""sort"": null,
  ""numberOfElements"": 12,
  ""first"": true
}
So for achieving that I have used Projections like as shown below
CountryProjection .java
public interface CountryProjection {
    public String getCountryName();
    public String getCountryCode();
    public List&lt;StateProjection&gt; getStates();
}
StateProjection .java
public interface StateProjection {
    public String getStateCode();
}
CountryServiceImpl.java
@Repository
public interface CountryRepository extends JpaRepository&lt;CountryDetails, Integer&gt; {
    @Query(value = ""SELECT country.countryName AS countryName, country.countryCode AS countryCode FROM Country country GROUP BY country.countryId ORDER BY ?#{#pageable}"", 
    countQuery = ""SELECT COUNT(*) FROM Country country GROUP BY country.countryId ORDER BY ?#{#pageable}"")
    public Page&lt;CountryProjection&gt; findAll(Pageable pageRequest);
}
But now the service is returning any of the state details like as shown below
{
  ""content"": [
    {
      ""countryName"": ""USA"",
      ""countryCode"": ""USA""
    }
  ],
  ""last"": false,
  ""totalPages"": 28,
  ""totalElements"": 326,
  ""size"": 12,
  ""number"": 0,
  ""sort"": null,
  ""numberOfElements"": 12,
  ""first"": true
} 
How can we get the minimal state details also like as shown below
{
  ""content"": [
    {
      ""countryName"": ""USA"",
      ""countryCode"": ""USA"",
      ""states"": [
        {
          ""stateCode"": ""WAS""
        },
        {
          ""stateCode"": ""SOS""
        }
      ]
    }
  ],
  ""last"": false,
  ""totalPages"": 28,
  ""totalElements"": 326,
  ""size"": 12,
  ""number"": 0,
  ""sort"": null,
  ""numberOfElements"": 12,
  ""first"": true
}
Can anyone please help me on this
",<java><mysql><hibernate><spring-data-jpa><spring-projections>,5233,0,175,4756,17,96,181,67,3148,0,445,5,17,2018-05-31 17:49,2018-05-31 18:47,,0,,Intermediate,23
51016980,java.sql.SQLException: Unknown system variable 'tx_isolation',"I am using play framework and I want to connect db, but I can't because I am getting following error:
play.api.Configuration$$anon$1: Configuration error[Cannot connect to database [default]]
Caused by: play.api.Configuration$$anon$1: Configuration error[Failed to initialize pool: Unknown system variable 'tx_isolation']
Caused by: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: Unknown system variable 'tx_isolation' java.sql.SQLException: Unknown system variable 'tx_isolation
I tried to find tx_isolation, but it doesn't exist:
mysql&gt; show variables like 'tx_isolation';
Empty set (0.00 sec)
So what is and how can I find tx_isolation?
Sorry. this is my error code. and I use mysql 8.0.11. so i find 'transaction_isolation'
play.db {
  config = ""db""
  default = ""default""
}
db {
//TODO : 작업필요
  default.driver = com.mysql.jdbc.Driver
  default.url = ""jdbc:mysql://127.0.0.1:3306/testPlayDB""
  default.username = root
  default.password = ""321A@654""
}
Error cause Default.url = ""jdbc:mysql://127.0.0.1:3306/testPlayDB""
i use Scala, playframework and StackOverflow first time...
Thank you.
",<java><mysql><database><playframework>,1138,0,18,191,1,1,4,44,38205,0,0,4,17,2018-06-25 5:47,2018-06-25 6:02,,0,,Basic,14
60571849,MySQL Workbench cannot open on Mac,"MySQL Workbench on Mac (10.14) opens only for a moment and closes immediately without any (visible) error message. Re-installing it does not solve the problem.
What can I do to fix this problem?
",<mysql><macos><mysql-workbench>,195,0,0,331,1,2,8,46,32289,0,24,14,17,2020-03-06 21:26,2020-03-09 22:50,,3,,Basic,14
57120577,How to set query timeout in Sequelize?,"I'm looking to see how one can set the timeout time of queries in Sequelize.
I've looked into the Sequelize docs for some info but I can't quite find what I'm looking for. The closest I have found is the ""pools.acquire"" option, but I'm not looking to set the timeout for an incoming connection, but rather the timeout of an ongoing query so that I may short-circuit deadlocks quickly.
http://docs.sequelizejs.com/class/lib/sequelize.js~Sequelize.html
Here is my sample code:
const db = new Sequelize( database, username, password, {
  host   : hostname,
  dialect: ""mysql"",
  define : {},
  pool: {
    max : 10,
    min : 0,
    idle: 10000
  },
})
Any insight would be greatly appreciated!
",<mysql><node.js><sequelize.js><timeout>,692,2,10,435,2,6,11,78,36205,0,5,6,17,2019-07-19 23:23,2019-11-07 17:45,,111,,Basic,10
53086514,How to stop MySQL after running it using mysqld_safe?,"I'm using mysqld_safe to be able to create a password for my root user (under Ubuntu 18.04, it is not asked on the installation).
To start MySQL, I have done:
$ sudo mysqld_safe --skip-grant-tables&amp;
Now, the MySQL daemon is running and I can't stop it. Stopping it by killing the process prevent me to start another MySQL daemon because the previous one did not gave back the resources, resulting in errors like:
2018-10-31T14:50:40.238735Z 0 [ERROR] InnoDB: Unable to lock ./ibdata1 error: 11
2018-10-31T14:50:40.238815Z 0 [Note] InnoDB: Check that you do not already have another mysqld process using the same InnoDB data or log files.
So how can I stop the MySQL daemon when it have been started using mysqld_safe?
",<mysql><linux><mysql-5.7>,722,0,6,1582,2,18,40,78,21976,0,178,2,17,2018-10-31 15:07,2018-10-31 15:07,2018-10-31 15:07,0,0,Basic,9
61513431,Copying/Cloning database in Datagrip(MySQL),"How can I copy a schema to another schema I've created in Datagrip, essentially creating a clone of the original. For some reason my CMD prompt is not set for MySQL, and I have not found the way to do it via the Datagrip user interface.
",<mysql><database><datagrip>,237,0,0,301,1,2,8,59,16440,0,15,2,17,2020-04-29 23:31,2020-04-30 11:53,,1,,Basic,10
52646025,SQL Management Studio Task Import Data is greyed out,"Database: SQL2017 Express
Management Studio 18
System: x64
I have sysdmin / db owner etc &amp; database is accessible/updateable.
""This feature is not currently available in this version or the database is not available""
I solved this by using (Menu start-> type ""Import"" which runs DTSWizard.exe), but it still doesnt work from SSMS.
",<sql-server><ssms>,335,1,0,1037,2,14,25,46,14342,0,244,3,17,2018-10-04 11:53,2018-10-31 10:11,,27,,Basic,14
49785023,How can I create and load a second database in ddev?,"I have a Drupal multisite that needs to have one database for each site, and want it to run in ddev, but ddev just has the one database by default, named 'db'. How can I get a second database?
",<mysql><drupal><mariadb><multisite><ddev>,193,0,0,10405,1,49,94,78,8609,0,562,3,17,2018-04-11 21:58,2018-04-11 21:58,2018-04-11 21:58,0,0,Basic,14
51509499,How do I view a PostgreSQL database on heroku with a GUI?,"I have a rails app on heroku that is using a Postgre database. My database has > 40 tables and > 10,000 rows. I would like to delete a lot of data, but it would be much easier if I was able to view and interact with it in a GUI table. I can access my data in rails console, but it's taking too long.
",<postgresql><user-interface><heroku>,300,0,0,193,1,1,7,73,12703,0,0,5,17,2018-07-25 1:05,2018-07-25 11:50,,0,,Advanced,37
54485148,SSL Connection Error while using MySQL Connector with Python,"I'm using Python 3.6 (but I get the same error with Python 2.7) and mysql-connector-python in an Anaconda's environment to code a simple script to access my database hosted in Hostgator. This is the error:
Traceback (most recent call last):
  File ""/home/usuario/anaconda3/envs/fakebook/lib/python3.6/site-packages/mysql/connector/connection_cext.py"", line 176, in _open_connection
    self._cmysql.connect(**cnx_kwargs)
_mysql_connector.MySQLInterfaceError: SSL connection error: SSL_CTX_set_tmp_dh failed
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""teste.py"", line 6, in &lt;module&gt;
    passwd=""senha""
  File ""/home/usuario/anaconda3/envs/fakebook/lib/python3.6/site-packages/mysql/connector/__init__.py"", line 172, in connect
    return CMySQLConnection(*args, **kwargs)
  File ""/home/usuario/anaconda3/envs/fakebook/lib/python3.6/site-packages/mysql/connector/connection_cext.py"", line 78, in __init__
    self.connect(**kwargs)
  File ""/home/usuario/anaconda3/envs/fakebook/lib/python3.6/site-packages/mysql/connector/abstracts.py"", line 731, in connect
    self._open_connection()
  File ""/home/usuario/anaconda3/envs/fakebook/lib/python3.6/site-packages/mysql/connector/connection_cext.py"", line 179, in _open_connection
    sqlstate=exc.sqlstate)
mysql.connector.errors.InterfaceError: 2026 (HY000): SSL connection error: SSL_CTX_set_tmp_dh failed
My code is very simple, it just try to connect to the database:
import mysql.connector
mydb = mysql.connector.connect(
    host=""192.xxx.xxx.xx"",
    user=""root"",
    passwd=""senha""
)
print(mydb)
I've used this library several times on other computers, I've also connected to this same database through my computer, using the same library and it always worked with this code. I tried with MySQL Workbench and seems it is connecting to the database using the same credentials that are in my code. I've already tried to ask help to the server support, my IP is allowed to access the database and even so I can't connect with Python. I tried to reinstall the library, but nothing changed. 
Thank you everyone!
",<python><mysql><python-3.x><python-2.7>,2133,0,28,859,2,9,10,37,25433,0,2,5,17,2019-02-01 18:23,2019-02-02 12:55,2019-02-02 12:55,1,1,Basic,14
54348801,generate_series() equivalent in snowflake,"I'm trying to find the snowflake equivalent of generate_series() (the PostgreSQL syntax).
SELECT generate_series(timestamp '2017-11-01', CURRENT_DATE, '1 day')
",<sql><generate-series><snowflake-cloud-data-platform>,160,0,1,187,1,2,6,79,18912,0,3,7,17,2019-01-24 14:22,2019-01-24 23:31,2019-01-24 23:31,0,0,Intermediate,17
50601917,Set column order in alembic when adding a new column,"I'm struggling to find a clean way (without raw SQL) to set the column order in alembic. For example, I would like to put a new column called 'name' after the 'id' column, something like this:
from alembic import op
import sqlalchemy as sa
...
op.add_column(
    'people',
    sa.Column(
        'name',
        sa.String(),
        nullable=False
    ),
    after='id'
)
But of course, alembic does not have the 'after' parameter, therefore this code fails and I have not found an equivalent to this 'after' parameter in the docs. I'm only able to append the column at the end of the table.
Can anybody suggest how to achieve in alembic/sqlalchemy what I want? Is it possible without raw SQL?
",<python><sqlalchemy><database-migration><alembic>,694,0,14,2223,3,20,30,50,5460,0,72,2,17,2018-05-30 10:05,2019-12-18 11:33,,567,,Intermediate,20
54307238,Edit a dtsx through SSMS,"I created and executed a dtsx with SSMS corresponding wizard:
This was to import a flat file in an existing table.
At the end I saved the ""package"" as a .dtsx file
Now I need to modify the column mappings and re-execute this package. 
Is there any way I can do it, using SQL Server Management Studio?
I tried opening the file, but it opens this dialog:
Where I cannot edit the mappings any more.
Update: 
I understand that ""editing"" a dtsx is not a simple thing, yet is there a reason why the wizard could not be run again with the values already pre-set? Like opening the wizard in the last step and navigate ""back"" on the previous steps. This is existing functionality after all...
Is there any trick I could do this? From command line maybe? This would suit my need fine.
",<sql-server><ssis><ssms><etl><sql-server-2016>,775,2,0,3111,4,31,60,36,22298,0,363,4,17,2019-01-22 11:25,2019-01-22 15:52,2019-01-22 19:58,0,0,Intermediate,20
48236252,How to use a pre-populated sqlite database my App in Flutter,"As the question in the title says.
My database file could be pretty large so I don't want to make copies of it unnecessarily and I certainly don't want to build it in situ.
If the db file is a flutter asset, is there a way for sqlite to access it directly?
I've seen suggestions that I should copy the raw data of the asset into a file and then access it but that is a waste of storage.  Or can I then delete the asset?
Is there a simple way of deploying the database as something other than an asset, ie as a raw file?
",<sqlite><mobile><flutter>,520,0,0,467,1,4,5,55,8560,0,0,1,17,2018-01-13 1:51,2018-01-26 17:23,,13,,Intermediate,23
48363789,WARNING: Module mcrypt ini file doesn't exist under /etc/php/7.2/mods-available,"I've been trying to install phpmyadmin in Ubuntu 16.04.3 LTS having a lamp installed, php 7.2, mysql Ver 15.1 Distrib 10.2.12-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2 and apache2.
and I am following this article from digitalOcean, but when I came to the part that I need to run sudo phpenmod mcrypt I got a message saying..
  WARNING: Module mcrypt ini file doesn't
  exist under /etc/php/7.2/mods-available
I am doing this on ubuntu installed in godaddy
Can you give best solution for this?
",<php><mysql><server><ubuntu-16.04><lamp>,509,1,1,8435,15,59,87,75,42404,0,1240,2,17,2018-01-21 4:48,2018-05-07 16:43,2018-05-07 16:43,106,106,Basic,14
64614651,System.TypeLoadException: Method 'Create' in type 'MySql.Data.EntityFrameworkCore.Query.Internal.MySQLSqlTranslatingExpressionVisitorFactory',"I'm trying to add a new user to the database with the following code:
public async void SeedUsers(){
    int count=0;
    if(count&gt;0){
        return;
    }
    else{
        string email=&quot;jujusafadinha@outlook.com.br&quot;;
        _context.Add(new User{LoginEmail=email});  
        await _context.SaveChangesAsync();  
    }
}
But it keeps giving me the following error:
System.TypeLoadException: Method 'Create' in type 'MySql.Data.EntityFrameworkCore.Query.Internal.MySQLSqlTranslatingExpressionVisitorFactory' from &gt;assembly 'MySql.Data.EntityFrameworkCore, Version=8.0.22.0, Culture=neutral, &gt;PublicKeyToken=c5687fc88969c44d' does not have an implementation.
at MySql.Data.EntityFrameworkCore.Extensions.MySQLServiceCollectionExtensions.AddEntityFrameworkMySQL(IServ&gt;iceCollection services)
at MySql.Data.EntityFrameworkCore.Infrastructure.Internal.MySQLOptionsExtension.ApplyServices(IServiceColle&gt;ction services)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.ApplyServices(IDbContextOptions &gt;options, ServiceCollection services)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.&lt;c__DisplayClass4_0.g__BuildServiceProvider|3()
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.&lt;&gt;c__DisplayClass4_0.b__2(Int64 k)
at System.Collections.Concurrent.ConcurrentDictionary2.GetOrAdd(TKey key, Func2 valueFactory)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.GetOrAdd(IDbContextOptions options, &gt;Boolean providerRequired)
at Microsoft.EntityFrameworkCore.DbContext.get_InternalServiceProvider()
at Microsoft.EntityFrameworkCore.DbContext.get_DbContextDependencies()
at Microsoft.EntityFrameworkCore.DbContext.EntryWithoutDetectChanges[TEntity](TEntity entity)
at Microsoft.EntityFrameworkCore.DbContext.SetEntityState[TEntity](TEntity entity, EntityState &gt;entityState)
at Microsoft.EntityFrameworkCore.DbContext.Add[TEntity](TEntity entity)
at contatinApi.Data.SeedData.SeedUsers() in D:\dev\contatinapi\Data\SeedData.cs:line 24
at System.Threading.Tasks.Task.&lt;&gt;c.b__139_1(Object state)
at System.Threading.QueueUserWorkItemCallback.&lt;&gt;c.&lt;.cctor&gt;b__6_0(QueueUserWorkItemCallback quwi)
at System.Threading.ExecutionContext.RunForThreadPoolUnsafe[TState](ExecutionContext &gt;executionContext, Action`1 callback, TState&amp; state)
at System.Threading.QueueUserWorkItemCallback.Execute()
at System.Threading.ThreadPoolWorkQueue.Dispatch()
at System.Threading._ThreadPoolWaitCallback.PerformWaitCallback()
The User class:
public class User
    {
        public int Id{get;set;}
        public string LoginEmail{get;set;}
        public string UserName{get;set;}
        public DateTime CreatedAt{get;set;}
        public List&lt;Contact&gt; Contacts {get;set;}
        public List&lt;ContactList&gt; ContactLists{get;set;}
    }
Both the EF Core and MySQL packages are updated. Also, i tried using stored procedures and it gave the same results.
The content of Ex was:
The value of Ex was {System.TypeLoadException: Method 'Create' in type 'MySql.Data.EntityFrameworkCore.Query.Internal.MySQLSqlTranslatingExpressionVisitorFactory' from assembly 'MySql.Data.EntityFrameworkCore, Version=8.0.22.0, Culture=neutral, PublicKeyToken=c5687fc88969c44d' does not have an implementation.
at MySql.Data.EntityFrameworkCore.Extensions.MySQLServiceCollectionExtensions.AddEntityFrameworkMySQL(IServiceCollection services)
at MySql.Data.EntityFrameworkCore.Infrastructure.Internal.MySQLOptionsExtension.ApplyServices(IServiceCollection services)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.ApplyServices(IDbContextOptions options, ServiceCollection services)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.&lt;&gt;c__DisplayClass4_0.g__BuildServiceProvider|3()
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.&lt;&gt;c__DisplayClass4_0.b__2(Int64 k)
at System.Collections.Concurrent.ConcurrentDictionary2.GetOrAdd(TKey key, Func2 valueFactory)
at Microsoft.EntityFrameworkCore.Internal.ServiceProviderCache.GetOrAdd(IDbContextOptions options, Boolean providerRequired)
at Microsoft.EntityFrameworkCore.DbContext.get_InternalServiceProvider()
at Microsoft.EntityFrameworkCore.DbContext.get_DbContextDependencies()
at Microsoft.EntityFrameworkCore.DbContext.Microsoft.EntityFrameworkCore.Internal.IDbContextDependencies.get_StateManager()
at Microsoft.EntityFrameworkCore.Internal.InternalDbSet1.EntryWithoutDetectChanges(TEntity entity) at Microsoft.EntityFrameworkCore.Internal.InternalDbSet1.Add(TEntity entity)
at contatinApi.Data.SeedData.SeedUsers() in D:\dev\ContatinApi\Data\SeedData.cs:line 40}
",<c#><mysql><asp.net-core><entity-framework-core>,4654,0,23,181,1,1,4,74,21664,0,0,6,17,2020-10-30 19:29,2020-10-30 20:08,,0,,Advanced,32
56259032,"In Django, how can I prevent a ""Save with update_fields did not affect any rows."" error?","I'm using Django and Python 3.7.  I have this code
article = get_article(id)
...
article.label = label
article.save(update_fields=[""label""])
Sometimes I get the following error on my ""save"" line ...
    raise DatabaseError(""Save with update_fields did not affect any rows."")
django.db.utils.DatabaseError: Save with update_fields did not affect any rows.
Evidently, in the ""..."" another thread may be deleting my article.  Is there another way to rewrite my ""article.save(...)"" statement such that if the object no longer exists I can ignore any error being thrown?
",<django><python-3.x><django-models><sql-update>,566,0,6,15145,139,461,847,37,8868,0,814,3,17,2019-05-22 14:22,2019-05-22 14:30,2019-05-25 19:23,0,3,Basic,13
54350881,Error installing mysqlclient for python on Ubuntu 18.04,"I installed Python 2.7.15rci and Python 3.6.7 on Ubuntu. When i did 'pip list' on virtualenv it returns me:
Django (2.1.5)
pip (9.0.1)
pkg-resources (0.0.0)
pytz (2018.9)
setuptools (39.0.1)
wheel (0.32.3)
I'm trying to install mysqlclient (pip install mysqlclient) and returns an error.
  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory
  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
  ----------------------------------------
  Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command /home/david/env/project/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-pq18uxjj/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-y28h4ou0-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/david/env/project/include/site/python3.6/mysqlclient:
    /usr/lib/python3.6/distutils/dist.py:261: UserWarning: Unknown distribution option: 'long_description_content_type'
      warnings.warn(msg)
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/__init__.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/_exceptions.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/compat.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/connections.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/converters.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/cursors.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/release.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    copying MySQLdb/times.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb
    creating build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/__init__.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CLIENT.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/CR.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/ER.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    copying MySQLdb/constants/FLAG.py -&gt; build/lib.linux-x86_64-3.6/MySQLdb/constants
    running build_ext
    building 'MySQLdb._mysql' extension
    creating build/temp.linux-x86_64-3.6
    creating build/temp.linux-x86_64-3.6/MySQLdb
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Dversion_info=(1,4,1,'final',0) -D__version__=1.4.1 -I/usr/include/mysql -I/home/david/env/project/include -I/usr/include/python3.6m -c MySQLdb/_mysql.c -o build/temp.linux-x86_64-3.6/MySQLdb/_mysql.o
    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    ----------------------------------------
Command ""/home/david/env/project/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-pq18uxjj/mysqlclient/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-y28h4ou0-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/david/env/project/include/site/python3.6/mysqlclient"" failed with error code 1 in /tmp/pip-build-pq18uxjj/mysqlclient/
So, I have tried different methods found like: 
sudo apt-get install python-dev
sudo apt-get install python3-dev
sudo apt-get install libmysqlclient-dev
and some more... but none of them work for me and the problem persists. 
Any suggestions?
Thank you!
",<mysql><django><python-3.x><pip><mysql-python>,4113,0,52,175,1,1,11,57,21264,0,5,7,17,2019-01-24 16:09,2019-01-26 20:16,2019-01-26 20:16,2,2,Intermediate,23
50138337,Logging slow queries on Google Cloud SQL PostgreSQL instances,"The company I work for uses Google Cloud SQL to manage their SQL databases in production.
We're having performance issues and I thought it'd be a good idea (among other things) to see/monitor all queries above a specific threshold (e.g. 250ms).
By looking at the PostgreSQL documentation I think log_min_duration_statement seems like the flag I need.
  log_min_duration_statement (integer)
  Causes the duration of each completed statement to be logged if the statement ran for at least the specified number of milliseconds. Setting this to zero prints all statement durations. 
But judging from the Cloud SQL documentation I see that is only possible to set a narrow set of database flags (as in for each DB instance) but as you can see from here log_min_duration_statement is not among those supported flags.
So here comes the question. How do I log/monitor my slow PostgreSQL queries with Google Cloud SQL? If not possible then what kind of tool/methodologies do you suggest I use to achieve a similar result?
",<postgresql><performance><monitoring><google-cloud-sql>,1013,2,2,26394,15,132,131,75,9748,0,3370,4,17,2018-05-02 15:30,2018-05-03 9:55,2019-04-08 14:32,1,341,Intermediate,23
51459170,Connect with postgreSQL schema,"I am looking to connect and query to a PostgreSQL. But I only want to connect to a particular Schema.
As per the doc (JDBC) we can use 
jdbc:postgresql://localhost:5432/mydatabase?searchpath=myschema
or update As of 9.4 you can specify the url with the new currentSchema parameter like so:
jdbc:postgresql://localhost:5432/mydatabase?currentSchema=myschema
But I am unable to do so with golang SQL driver;
As per the documents, we can also use SET search_path TO myschema,public;
 But I only want to declare it for once during initializing but I think this needs to be executed every time for new connection. 
Also I am using following code please help me identify the correct parameters to be passed to this in order to only connect with schema 
db, err := sql.Open(""postgres"", `dbname=`+s.settings.Database+
` user=`+s.settings.Username+` password=`+s.settings.Password+
` host=`+s.settings.Url+` sslmode=disable`) 
Adding currentSchema=myschema or searchpath=myschema is not working!
Is there a way I can only connect to a particular database-schema in GO
",<postgresql><go><database-schema>,1059,1,8,1697,5,20,27,58,23466,,44,2,17,2018-07-21 18:54,2018-07-21 20:23,2018-07-23 15:47,0,2,Basic,10
50181716,SQL Constraint Validate Unique Values,"I have a table and I need to restrict the data inserted, I already use a trigger but I was wondering if I could do the same task with a constraint.
The fields are:
Id
Date
Passport
...
Deleted
The constraint must allow n records while the Deleted field is 1, but if the Deleted field is 0 there must be only one row with the same Date and Passport.
Should work for this steps:
Adding a row with Id=1, Date=2018-05-01, Passport=MPEUIE80, Deleted=0
Deleting the row with Id=1, so the field Deleted will be 1
Adding a row with Id=2, Date=2018-05-01, Passport=MPEUIE80, Deleted=0
Deleting the row with Id=2, so the field Deleted will be 1
Adding a row with Id=3, Date=2018-05-01, Passport=MPEUIE80, Deleted=0
Adding a row with Id=4, Date=2018-05-01, Passport=MPEUIE80, Deleted=0
Till the fifth step, everything works, but in the last step there must be an error, that is because I can't handle two rows with the same Date, same Passport and both with Deleted=0
Thanks in advance.
",<sql><sql-server><sql-server-2008><constraints>,976,0,5,263,0,2,13,78,1466,0,114,1,17,2018-05-04 19:11,2018-05-04 19:46,2018-05-04 19:46,0,0,Basic,10
62373766,How to create a unique index containing multiple fields where one is a foreign key,"I am trying to create an index with multiple fields, one of the field is a foriegn key to another table. However i get the following error:
  Error: Index ""player_id_UNIQUE"" contains column that is missing in the
  entity (Earning): player_id
Given that player_id is a foriegn key that im joining how do i handle this
import { Column, Entity, Index, JoinColumn, ManyToOne, PrimaryColumn } from ""typeorm"";
import { PersonPlayer } from ""./PersonPlayer"";
import { Team } from ""./Team"";
    @Entity()
    @Index(""player_id_UNIQUE"", [""player_id"", ""period"", ""year""], { unique: true })
    export class Earning {
        @PrimaryColumn({length: 36})
        id: string;
        @Column({nullable: true})
        year: number;
        @Column({type: 'decimal', nullable: true})
        amount: number;
        @Column({nullable: true, length: 45})
        period: string;
        @ManyToOne(() =&gt; Team, {nullable: true})
        @JoinColumn({name: 'team_id'})
        team: Team;
        @ManyToOne(() =&gt; PersonPlayer, {nullable: true})
        @JoinColumn({name: 'player_id'})
        player: PersonPlayer;
        @Column({nullable: true, length: 45})
        dtype: string;
    }
When i generate this entity and create the sql table (without the index) i see player_id as one of the columns. But it appears that typeorm is not able to recognize right now with the index that player_id exists in the entity through the joincolumn relationship.
",<sql><node.js><typeorm>,1444,0,34,18367,65,163,276,74,13000,0,120,3,17,2020-06-14 14:18,2021-02-05 7:19,2021-02-05 7:19,236,236,Intermediate,18
51114731,Check if JSON value empty in MySQL?,"Imagine a table which tracks baseball pitchers like so...
     +------------+--------------------+-------+
     | id | name               | secondary_pitch |
     +------------+--------------------+-------+
     | 13 | Chris Sale         | ['Curveball','Slider'] |
     | 14 | Justin Verlander   | ['Fastball','Changeup'] |
     | 15 | CC Sabathia        | ['Fastball','Curveball'] |
     | 16 | Sonny Grey         |    ['Slider'] |
     | 17 | Aldoris Chapman    |    [] |
     +------------+--------------------+-------+
Notice the secondary_pitch column has a JSON value. So if a pitcher, like Chapman, has no secondary pitch, it will not return null, instead it returns an empty JSON string ('[]').
How then can I get a count of the number of pitchers who have no secondary pitch?
I can't do...
  select count(*) from pitchers where secondary_pitch is null
",<mysql><sql><json><null>,861,0,10,3340,13,49,82,77,47088,0,138,5,17,2018-06-30 12:02,2018-06-30 12:06,2018-06-30 12:06,0,0,Basic,10
57226520,Select does not return values Postgres-11.4,"I am using pgAdmin-4 and created a database with a single table, but select returns an error message: 'table oid'
I'm using a normal select query.
SELECT * FROM escola
This happens with PostgreSQL 11.4.
",<postgresql><pgadmin-4>,203,0,1,171,0,1,6,45,5618,0,31,2,17,2019-07-26 20:14,2019-07-26 22:25,2019-07-27 11:41,0,1,Intermediate,18
53284762,NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:snowflake,"I've installed all the necessary packages:
pip install --upgrade snowflake-sqlalchemy
I am running this test code from the snowflake docs:
from sqlalchemy import create_engine
engine = create_engine(
    'snowflake://{user}:{password}@{account}/'.format(
        user='&lt;your_user_login_name&gt;',
        password='&lt;your_password&gt;',
        account='&lt;your_account_name&gt;',
    )
)
try:
    connection = engine.connect()
    results = connection.execute('select current_version()').fetchone()
    print(results[0])
finally:
    connection.close()
    engine.dispose()
My output should be the snowflake version e.g. 1.48.0
But I get the error
NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:snowflake
(I am trying to run this in Anaconda)
",<python><sqlalchemy><snowflake-cloud-data-platform>,761,0,17,1370,4,24,38,47,40821,0,260,6,17,2018-11-13 15:54,2019-08-15 22:46,,275,,Intermediate,18
53564839,CodeIgniter Transactions - trans_status and trans_complete return true but nothing being committed,"Problem:
I have written a function in my model to insert an order into my database. I am using transactions to make sure that everything commits or else it will be rolled back. 
My problem is that CodeIgniter is not showing any database errors, however it is rolling back the transaction but then returning TRUE for trans_status. However, this only happens if there is a discount on the order. If there is no discount on the order, everything commits and works properly.
I am currently using CodeIgniter 3.19, PHP (7.2), mySQL (5.7), and Apache 2.4. (Working on Ubuntu 18.04)
The function logic works as such:
Inserts the order array into tbl_orders
Saves order_id, and goes through each of the order products (attaches order_id) and inserts the product in tbl_order_products, 
Saves order_product_id and attaches it to an array of users attendance options and inserts that into tbl_order_attendance
Takes the payment transaction array (attaches the order_id) and inserts that into tbl_transactions
IF there is a discount on the order, it decreases the discount_redeem_count (number of redeemable discount codes) by 1.
Actual Function
[Function]: 
public function add_order(Order $order, array $order_products, Transaction $transaction = NULL){
  $this-&gt;db-&gt;trans_start();
  $order-&gt;create_order_code();
  $order_array = $order-&gt;create_order_array();
  $this-&gt;db-&gt;insert('tbl_orders', $order_array);
  $order_id = $this-&gt;db-&gt;insert_id();
  $new_order = new Order($order_id);
  foreach($order_products as $key=&gt;$value){
    $order_products[$key]-&gt;set_order($new_order);
    $order_product_array = $order_products[$key]-&gt;create_order_product_array();
    $this-&gt;db-&gt;insert('tbl_order_products', $order_product_array);
    $order_product_id = $this-&gt;db-&gt;insert_id();
    $product = $order_products[$key]-&gt;get_product();
    switch ($product-&gt;get_product_class()){
        case 'Iteration':
            $this-&gt;db-&gt;select('module_id, webcast_capacity, in_person_capacity');
            $this-&gt;db-&gt;from('tbl_modules');
            $this-&gt;db-&gt;where('iteration_id', $product-&gt;get_product_class_id());
            $results = $this-&gt;db-&gt;get()-&gt;result_array();
            break;
        case 'Module':
            $this-&gt;db-&gt;select('module_id, webcast_capacity, in_person_capacity');
            $this-&gt;db-&gt;from('tbl_modules');
            $this-&gt;db-&gt;where('module_id', $product-&gt;get_product_class_id());
            $results = $this-&gt;db-&gt;get-&gt;result_array();
            break;
      }
      if(!empty($results)){
        foreach($results as $result){
        $module_id = $result['module_id'];
        if($result['webcast_capacity'] !== NULL &amp;&amp; $result['in_person_capacity'] !== NULL){
          $attendance_method = $order_products[$key]-&gt;get_attendance_method();
        }elseif($result['webcast_capacity'] !== NULL &amp;&amp; $result['in_person_capacity'] === NULL){
          $attendance_method = 'webcast';
        }elseif($result['webcast_capacity'] === NULL &amp;&amp; $result['in_person_capacity'] !== NULL){
          $attendance_method = 'in-person';
        }
        $order_product_attendance_array = array(
          'order_product_id' =&gt; $order_product_id,
          'user_id' =&gt; $order_products[$key]-&gt;get_customer(true),
          'module_id' =&gt; $module_id,
          'attendance_method' =&gt; $attendance_method,
        );
        $order_product_attendance[] = $order_product_attendance_array;
      }
      $this-&gt;db-&gt;insert_batch('tbl_order_product_attendance', $order_product_attendance);
    }
    if(!empty($order_products[$key]-&gt;get_discount())){
      $discount = $order_products[$key]-&gt;get_discount();
    }
  }
  if(!empty($transaction)){
    $transaction-&gt;set_order($new_order);
    $transaction_array = $transaction-&gt;create_transaction_array();
    $this-&gt;db-&gt;insert('tbl_transactions', $transaction_array);
    $transaction_id = $this-&gt;db-&gt;insert_id();
  }
  if(!empty($discount)){
    $this-&gt;db-&gt;set('discount_redeem_count', 'discount_redeem_count-1', false);
    $this-&gt;db-&gt;where('discount_id', $discount-&gt;get_discount_id());
    $this-&gt;db-&gt;update('tbl_discounts');
  }
  if($this-&gt;db-&gt;trans_status() !== false){
    $result['outcome'] = true;
    $result['insert_id'] = $order_id;
    return $result;
  }else{
    $result['outcome'] = false;
    return $result;
  }
}
When this function completes with a discount, both trans_complete and trans_status return TRUE. However the transaction is never committed.
What I've tried:
I have dumped the contents of $this-&gt;db-&gt;error() after each query and there are no errors in any of the queries.
I have used this-&gt;db-&gt;last_query() to print out each query and then checked the syntax online to see if there were any problems, there were none.
I also tried changing to using CodeIgniters Manual Transactions like:
[Example]
$this-&gt;db-&gt;trans_begin();
 // all the queries
if($this-&gt;db-&gt;trans_status() !== false){
    $this-&gt;db-&gt;trans_commit();
    $result['outcome'] = true;
    $result['insert_id'] = $order_id;
    return $result;
}else{
    $this-&gt;db-&gt;trans_rollback();
    $result['outcome'] = false;
    return $result;
}
I have tried echoing and var_dumping all of the return insert_ids and they all work, I have also outputted the affected_rows() of the UPDATE query and it is showing that 1 row was updated. However, still nothing being committed:
[Values Dumped]
int(10) // order_id
int(10) // order_product_id
array(3) { 
    [""module_id""]=&gt; string(1) ""1"" 
    [""webcast_capacity""]=&gt; string(3) ""250"" 
    [""in_person_capacity""]=&gt; string(3) ""250"" } // $results array (modules)
array(1) { 
    [0]=&gt; array(4) { 
        [""order_product_id""]=&gt; int(10 
        [""user_id""]=&gt; string(1) ""5"" 
        [""module_id""]=&gt; string(1) ""1"" 
        [""attendance_method""]=&gt; string(7) ""webcast"" } } // order_product_attendance array
int(9) // transaction_id
int(1) // affected rows
string(99) ""UPDATE `tbl_discounts` 
            SET discount_redeem_count = discount_redeem_count- 1 
            WHERE `discount_id` = 1"" // UPDATE query
- I have also tried replacing the last UPDATE query with a completely different one that tries to update a different table with different values. That query ALSO did not work, which makes me think that I am hitting some sort of memory limit with the transaction. However, when monitoring mysqld processes, none of them seem to spike or have difficulty.
I have tried submitting an order that doesn't have a discount and the entire process works! Which leads me to believe that my problem is with my UPDATE query. [After Update:] But it seems that the update query is working as well.
Suggestions Tried:
We have tried setting log_threshold to 4, and looked through the CodeIgniter Log Files which shows no history of a rollback. 
We have checked the mySQL Query Log:
[Query Log]
2018-12-03T15:20:09.452725Z         3 Query     UPDATE `tbl_discounts` SET discount_redeem_count = discount_redeem_count-1 WHERE `discount_id` = '1'
2018-12-03T15:20:09.453673Z         3 Quit
It shows that a QUIT command is being sent directly after the UPDATE query. This would initiate a rollback, however the trans_status is returning TRUE.
I also changed my my.cnf file for mySQL to have innodb_buffer_pool_size=256M and innodb_log_file_size=64M. There was no change in the outcome. 
As @ebcode recommended, I changed UPDATE query to use a simple_query() instead of using default methods from CodeIgniter's Query Builder Class:
[Simple Query]
if(!empty($discount)){
    $this-&gt;db-&gt;simple_query('UPDATE `tbl_discounts` SET '.
    'discount_redeem_count = discount_redeem_count-1 WHERE '.
    '`discount_id` = \''.$discount['discount_id'].'\'');
}
However, this produced did not affect the outcome any differently.
If you have an idea that I haven't tried yet, or need more information from me, please comment and I will reply promptly.
Question:
Why does trans_status return TRUE if none of my transaction is being committed?
In order to try and bring some clarity to users just finding this question now, the latest updates to the post will appear in italics *
",<php><mysql><codeigniter>,8291,0,158,639,0,6,22,55,4507,,71,6,17,2018-11-30 21:00,2018-12-04 4:47,2018-12-14 19:05,4,14,Advanced,33
52952734,How to perform date_trunc query in Postgres using SQLAlchemy,"I can't seem to be able to translate the following query into SQLAlchemy. 
I would like to translate the following query: 
SELECT date_trunc('day', time), ""PositionReport"".callsign FROM tvh_aircraft.""PositionReport"" WHERE ""PositionReport"".reg = 'PH-BVA'
GROUP BY 1, ""PositionReport"".callsign
I've tried the following, but with no luck. 
flight_days = session\
        .query(PositionReport)\
        .filter(PositionReport.reg == reg) \
        .group_by(func.date_trunc('day', PositionReport.time))\
        .group_by('1')\
        .all()
    trunc_date = func.date_trunc('day', PositionReport.time)
    flight_days = session.query(trunc_date, PositionReport.callsign) \
        .filter(PositionReport.reg == reg) \
        .group_by(""date_trunc_1"")
Thanks in advance for your help. 
",<python><sql><postgresql><sqlalchemy>,785,0,13,415,2,5,19,59,7882,0,11,1,17,2018-10-23 15:27,2018-10-26 11:20,2018-10-26 11:20,3,3,Basic,10
52190109,How to query JSON Array in Postgres with SqlAlchemy?,"I have a SqlAlchemy model defined
from sqlalchemy.dialects.postgresql import JSONB
class User(db.Model):
    __tablename__ = ""user""
    id = db.Column(db.Integer, primary_key=True)
    nickname = db.Column(db.String(255), nullable=False)
    city = db.Column(db.String(255))
    contact_list = db.Column(JSONB)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
def add_user():
    user = User(nickname=""Mike"")
    user.contact_list = [{""name"": ""Sam"", ""phone"": [""123456"", ""654321""]}, 
                         {""name"": ""John"", ""phone"": [""159753""]},
                         {""name"": ""Joe"", ""phone"": [""147889"", ""98741""]}]
    db.session.add(user)
    db.session.commit()
if __name__ == ""__main__"":
    add_user()
How can I retrieve the name from my contact_list using phone? For example, I have the 147889, how can I retrieve Joe?
I have tried this
User.query.filter(User.contact_list.contains({""phone"": [""147889""]})).all()
But, it returns me an empty list, []
How can I do this?
",<sqlalchemy><flask-sqlalchemy>,993,0,22,332,1,2,8,55,17978,0,44,1,17,2018-09-05 16:54,2018-09-05 17:06,2018-09-05 17:06,0,0,Basic,10
56185746,DISTINCT NULL return single NULL in SQL Server,"In Sql Server, NULL IS NOT EQUAL TO NULL.(Why does NULL = NULL evaluate to false in SQL server)
Then why the following code returns single NULL.
CREATE TABLE #TEMP1
    (ID INT)
INSERT INTO #TEMP1
SELECT NULL
UNION ALL
SELECT NULL
SELECT DISTINCT ID FROM #TEMP1
DROP TABLE #TEMP1
ID
------
NULL
I expected 
ID
------
NULL
NULL
",<sql-server>,327,1,16,187,0,1,6,38,1642,0,23,2,17,2019-05-17 11:43,2019-05-17 11:51,2019-05-17 11:51,0,0,Basic,2
51051440,"""Server sent charset (255) unknown to the client"" Set MySQL charset to utf8 w/o /etc/my.cnf?","I'm trying to connect to a MySQL database from phpMyAdmin. But when I put in username and password I get two error messages saying:
mysqli_real_connect(): Server sent charset (255) unknown to the client. Please, report to the developers
mysqli_real_connect(): (HY000/2054): Server sent charset unknown to the client. Please, report to the developers
I'm using MySQL 8.0.11 and phpMyAdmin 4.8.2 
I found this answer for a similar problem: 
 PDO::__construct(): Server sent charset (255) unknown to the client. Please, report to the developers
Here is the important part: ""MySQL 8 changed the default charset to utfmb4. But some clients don't know this charset. Hence when the server reports its default charset to the client, and the client doesn't know what the server means, it throws this error.""
The solution given is to change the default charset back to utf8 by adding a few lines to /etc/my.cnf and restart mysqld.
My problem is that /etc/my.cnf doesn't exist anywhere in my files so I can't change the default charset there. All of the other places I've looked end up referring to /my.cnf or reference older versions.
So, how do I change the default charset to utf8 without a /etc/my.cnf for MySQL 8?
",<php><mysql><pdo><utf-8>,1208,1,3,171,1,1,6,54,33527,0,0,3,17,2018-06-26 21:13,2019-01-06 4:53,,194,,Basic,14
59668237,Disable AutoDetectChanges on Entity Framework Core,"someone knows how to disable the AutoDetectChanges on EFCore?
I need to do it because I have to make an huge import in my database, and can't find information on web.
Tried this but it's not working:
_context.Configuration.AutoDetectChangesEnabled = false;
Say configuration does not exist.
Thanks a lot.
",<c#><sql><asp.net-core><entity-framework-core><record>,305,0,1,491,2,10,23,49,18033,0,60,3,17,2020-01-09 16:24,2020-01-09 16:27,2020-01-09 18:45,0,0,Basic,14
50439029,How to execute a raw sql query with multiple statement with laravel,"Is there anyway I can execute a query with multiple statement like the one below, using laravel framework.
I have tried using DB::statement but returned a sql syntax error, but when I execute the same query on phpmyadmin I works, its so frustrating.
Please help me.
EG
LOCK TABLE topics WRITE;
SELECT @pRgt := rgt FROM topics WHERE id = ?;
UPDATE topics SET lft = lft + 2 WHERE rgt &gt; @pRgt;
UPDATE topics SET rgt = rgt + 2 WHERE rgt &gt;= @pRgt;
INSERT INTO topics (title, overview, article, image, lft, rgt)
VALUES (?, ?, ?, ?, @pRgt, @pRgt + 1);
UNLOCK TABLES;
",<php><mysql><laravel>,566,0,12,1350,3,13,22,57,13810,0,197,2,17,2018-05-20 20:21,2018-05-20 21:04,2018-05-20 21:04,0,0,Basic,10
49508697,"Aggregate for each day over time series, without using non-equijoin logic","Initial Question
Given the following dataset paired with a dates table:
MembershipId | ValidFromDate | ValidToDate
==========================================
0001         | 1997-01-01    | 2006-05-09
0002         | 1997-01-01    | 2017-05-12
0003         | 2005-06-02    | 2009-02-07
How many Memberships were open on any given day or timeseries of days?
Initial Answer
Following this question being asked here, this answer provided the necessary functionality:
select d.[Date]
      ,count(m.MembershipID) as MembershipCount
from DIM.[Date] as d
    left join Memberships as m
        on(d.[Date] between m.ValidFromDateKey and m.ValidToDateKey)
where d.CalendarYear = 2016
group by d.[Date]
order by d.[Date];
though a commenter remarked that There are other approaches when the non-equijoin takes too long.
Followup
As such, what would the equijoin only logic look like to replicate the output of the query above?
Progress So Far
From the answers provided so far I have come up with the below, which outperforms on the hardware I am working with across 3.2 million Membership records:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
)
,e as
(
    select d.[Date] as d
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
),c as
(
    select isnull(s.d,e.d) as d
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
)
select d.[Date]
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
;
Following on from that, to split this aggregate into constituent groups per day I have the following, which is also performing well:
declare @s date = '20160101';
declare @e date = getdate();
with s as
(
    select d.[Date] as d
        ,s.MembershipGrouping as g
        ,count(s.MembershipID) as s
    from dbo.Dates as d
        join dbo.Memberships as s
            on d.[Date] = s.ValidFromDateKey
    group by d.[Date]
            ,s.MembershipGrouping
)
,e as
(
    select d.[Date] as d
        ,e..MembershipGrouping as g
        ,count(e.MembershipID) as e
    from dbo.Dates as d
        join dbo.Memberships as e
            on d.[Date] = e.ValidToDateKey
    group by d.[Date]
            ,e.MembershipGrouping
),c as
(
    select isnull(s.d,e.d) as d
            ,isnull(s.g,e.g) as g
            ,sum(isnull(s.s,0) - isnull(e.e,0)) over (partition by isnull(s.g,e.g) order by isnull(s.d,e.d)) as c
    from s
        full join e
            on s.d = e.d
                and s.g = e.g
)
select d.[Date]
    ,c.g
    ,c.c
from dbo.Dates as d
    left join c
        on d.[Date] = c.d
where d.[Date] between @s and @e
order by d.[Date]
        ,c.g
;
Can anyone improve on the above?
",<sql><sql-server><t-sql><date><join>,3058,1,95,12063,3,25,53,80,2101,0,633,6,17,2018-03-27 9:07,2018-04-04 7:23,2018-04-07 9:18,8,11,Advanced,33
53397668,MySql: set a variable with a list,"I've been researching about MySQL statements and similar issues for some thime now and I don't seem to be having any luck.
Can I create a variable that would store a 1 column data result to be used in multiple queries? Would it be efficient to do so or am I confused about how a DB differs from a PL?
I was thinking something like the following pseudo code
SET list = Select ID from Sales;
Update items set aValue = X where salesID in list
delete SalesMessages where salesId in list
I've got about 8 updates to do in a Store Procedure I could do the select for every case instead of creating a variable, but I doesn't feel like the best approach. Any help?
",<mysql><sql>,657,0,3,769,2,7,31,59,35557,0,191,1,17,2018-11-20 16:44,2018-11-20 17:40,2018-11-20 17:40,0,0,Basic,10
53061182,MySQL connection over SSL with Laravel,"I have a problem connecting to my database over ssl in a Laravel application. My config is as follows:
       'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8mb4',
            'collation' =&gt; 'utf8mb4_unicode_ci',
            'prefix' =&gt; '',
            'strict' =&gt; true,
            'engine' =&gt; null,
            'sslmode' =&gt; 'require',
            'options'   =&gt; array(
                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT =&gt; false,
                PDO::MYSQL_ATTR_SSL_KEY =&gt; '/certs/client-key.pem',
                PDO::MYSQL_ATTR_SSL_CERT =&gt; '/certs/client-cert.pem',
                PDO::MYSQL_ATTR_SSL_CA =&gt; '/certs/ca.pem',
            ),
        ],
DB_CONNECTION=mysql
DB_HOST=xxx.xxx.xxx.xxx
DB_PORT=3306
DB_DATABASE=db_name
DB_USERNAME=db_user
DB_PASSWORD=xxxx
These setting give me the following error on the MySQL server.
2018-10-30T09:18:25.403712Z 71 [Note] Access denied for user 'user'@'xxx.xxx.xxx.xxx' (using password: YES)
Through mysql-client it works perfectly from the client server.
mysql -u user -p -h xxx.xxx.xxx.xxx --ssl-ca=/certs/ca.pem --ssl-cert=/certs/client-cert.pem --ssl-key=/certs/client-key.pem
Output of `\s' command performed on the client while connected with the above command.
mysql  Ver 15.1 Distrib 10.1.26-MariaDB, for debian-linux-gnu (x86_64) using readline 5.2
Connection id:      16
Current database:   database
Current user:       user@xxx.xxx.xxx.xxx
SSL:            Cipher in use is DHE-RSA-AES256-SHA
Current pager:      stdout
Using outfile:      ''
Using delimiter:    ;
Server:         MySQL
Server version:     5.7.24-0ubuntu0.18.10.1 (Ubuntu)
Protocol version:   10
Connection:     167.99.215.179 via TCP/IP
Server characterset:    latin1
Db     characterset:    latin1
Client characterset:    utf8mb4
Conn.  characterset:    utf8mb4
TCP port:       3306
Uptime:         23 min 29 sec
Threads: 2  Questions: 38  Slow queries: 0  Opens: 135  Flush tables: 1  Open tables: 128  Queries per second avg: 0.026
--------------
The application runs inside a docker container based on the php:7.2.11-fpm image. And is configured with the following php extensions.
RUN docker-php-ext-install bcmath zip pdo_mysql json tokenizer
MySQL version:
Server version: 5.7.24-0ubuntu0.18.10.1 (Ubuntu)
PHP version:
PHP 7.2.11 (cli) (built: Oct 16 2018 00:46:29) ( NTS )
PDO version:
PDO
PDO support =&gt; enabled
PDO drivers =&gt; sqlite, mysql
pdo_mysql
PDO Driver for MySQL =&gt; enabled
Client API version =&gt; mysqlnd 5.0.12-dev - 20150407 - $Id: 38fea24f2847fa7519001be390c98ae0acafe387
OpenSSL version:
openssl
OpenSSL support =&gt; enabled
OpenSSL Library Version =&gt; OpenSSL 1.1.0f  25 May 2017
OpenSSL Header Version =&gt; OpenSSL 1.1.0f  25 May 2017
Openssl default config =&gt; /usr/lib/ssl/openssl.cnf
Directive =&gt; Local Value =&gt; Master Value
openssl.cafile =&gt; no value =&gt; no value
openssl.capath =&gt; no value =&gt; no value
Any thoughts on this? This is keeping me busy for hours...
",<mysql><laravel><pdo>,3346,0,75,2423,2,20,38,55,27983,0,53,3,17,2018-10-30 9:32,2018-10-30 11:57,2018-10-30 11:57,0,0,Basic,2
53540056,What causes Spring Boot Fail-safe cleanup (collections) to occur,"I have a Java Spring Boot application with the following entities related to the below exception
SProduct
@Entity
@Table(
        name = &quot;product&quot;,
        indexes = @Index(
                name = &quot;idx_asin&quot;,
                columnList = &quot;asin&quot;,
                unique = true
        )
)
public class SProduct implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @Column(name = &quot;asin&quot;, unique = false, nullable = false, length = 10)
    private String asin;
    @Column(name = &quot;rootcategory&quot;)
    private Long rootcategory;
    @Column(name = &quot;imageCSV&quot;, unique = false, nullable = true, length = 350)
    private String imagesCSV;
    @Column(name = &quot;title&quot;, unique = false, nullable = true, length = 350)
    private String title;
    private Date created;
    @OneToMany(fetch = FetchType.EAGER, mappedBy = &quot;mainProduct&quot;, cascade = CascadeType.ALL)
    private Set&lt;FBT&gt; fbts;
    @OneToOne(fetch = FetchType.EAGER, mappedBy = &quot;downloadProductId&quot;, cascade = CascadeType.ALL)
    private Download download;
FBT
@Entity
@Table(
    name = &quot;fbt&quot;,
    uniqueConstraints={@UniqueConstraint(columnNames = {&quot;main_product_id&quot; , &quot;collection&quot;})},
    indexes = {@Index(
        name = &quot;idx_main_product_id&quot;,
        columnList = &quot;main_product_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt1id&quot;,
        columnList = &quot;product_fbt1_id&quot;,
        unique = false),
        @Index(
        name = &quot;idx_product_fbt2id&quot;,
        columnList = &quot;product_fbt2_id&quot;,
        unique = false)
        }
)
public class FBT implements Serializable {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private long id;
    @ManyToOne
    @JoinColumn(name = &quot;main_product_id&quot;)
    private SProduct mainProduct;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt1_id&quot;)
    private SProduct sproductFbt1;
    @ManyToOne
    @JoinColumn(name = &quot;product_fbt2_id&quot;)
    private SProduct sproductFbt2;
    @Column(name = &quot;bsr&quot;, nullable = false)
    private int bsr;
    private Date collection;
I had the following query in my fbt repository
  FBT findByMainProductAndCollection(SProduct mainProduct,Date collection);
which caused the following messages to be output exception when the data exists in the database for the mainProduct and collection but returns null otherwise.
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@69b7fcfc&amp;lt;rs=HikariProxyResultSet@325408381 wrapping com.mysql.jdbc.JDBC42ResultSet@108693fa&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@47c40535&amp;lt;rs=HikariProxyResultSet@2005129089 wrapping com.mysql.jdbc.JDBC42ResultSet@9894f70&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5b0cd175&amp;lt;rs=HikariProxyResultSet@1598144514 wrapping com.mysql.jdbc.JDBC42ResultSet@6a7ff475&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@f67e2cc&amp;lt;rs=HikariProxyResultSet@319200129 wrapping com.mysql.jdbc.JDBC42ResultSet@215b8a6&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : org.hibernate.engine.loading.internal.CollectionLoadContext@5961afc0&amp;lt;rs=HikariProxyResultSet@1772496904 wrapping com.mysql.jdbc.JDBC42ResultSet@5956a59b&amp;gt;&lt;/message&gt;
  &lt;message&gt;HHH000160: On CollectionLoadContext#cleanup, localLoadingCollectionKeys contained [1] entries&lt;/message&gt;
  &lt;message&gt;HHH000100: Fail-safe cleanup (collections) : 
I decided to abandon the above and write a @query to count as I only need to determine if the data exists or not and this has prevented the issue which is making me think I should change all my code to use @query.
 @Query(&quot;select count(*) as count from FBT where main_product_id = :id and collection= :collection&quot;)
    int countByMainProductIdAndCollection(@Param(&quot;id&quot;) long id, @Param(&quot;collection&quot;) Date collection);
Although this similarly also occurs seemingly randomly on updates into the database of one SProduct when the product exists in the database already.
SProductRepo.saveAndFlush(s);
I say randomly as 11 applications running the same code exit at random intervals with the above messages. There are no exceptions generated by the code and 10000's of successful database updates occur with the same code that leads to the failure. The code stops when trying to update the database where it has worked previously.
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5c414639&lt;rs=HikariProxyResultSet@1241510017 wrapping Result set representing update count of 13&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@5595c065&lt;rs=HikariProxyResultSet@2140082434 wrapping Result set representing update count of 14&gt;
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.CollectionLoadContext - HHH000160: On CollectionLoadContext#cleanup, localLoa
dingCollectionKeys contained [1] entries
&quot;&quot;2018-12-28 00:56:06 [KeepaAPI-RetryScheduler] WARN  org.hibernate.engine.loading.internal.LoadContexts - HHH000100: Fail-safe cleanup (collections) : org.hibernate.eng
ine.loading.internal.CollectionLoadContext@2956fe24&lt;rs=HikariProxyResultSe
Additionally the SProduct findByAsin(String asin) query cause the same problem however the query in the database works perfectly and this used to work in spring boot.
mysql&gt; select * from product where asin=&quot;B004FXJOQO&quot;;
| id | asin       | created    | imagecsv                                                                        | rootcategory | title                                                                                                        |  9 | B004FXJOQO | 2018-08-04 | 41T0ZwTvSSL.jpg,61V90AZKbGL.jpg,51AdEGCTZqL.jpg,51LDnCYfR0L.jpg,71bbIw43PjL.jpg |       228013 | Dual Voltage Tester, Non Contact Tester for High and Low Voltage with 3-m Drop Protection Klein Tools NCVT-2 |
1 row in set (0.00 sec)
What I would like to know is what are the general reasons this kind of messages get generated?
Why do they stop my application despite try catch statements around my insertion statements that are the last executed statements in my code?
Are there log debugging settings useful to determine the exact reason for why the messages are generated?
Is there a way to turn off or control this functionality?
Pom
  &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.1.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;maven-dependency-plugin.version&gt;2.10&lt;/maven-dependency-plugin.version&gt;
        &lt;maven.test.skip&gt;true&lt;/maven.test.skip&gt;
    &lt;/properties&gt;
    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;Keepa&lt;/id&gt;
            &lt;name&gt;Keepa Repository&lt;/name&gt;
            &lt;url&gt;https://keepa.com/maven/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;
        &lt;/dependency&gt; 
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;javax.mail&lt;/groupId&gt;
            &lt;artifactId&gt;mail&lt;/artifactId&gt;
            &lt;version&gt;1.4.7&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
            &lt;artifactId&gt;commons-compress&lt;/artifactId&gt;
            &lt;version&gt;1.18&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-jetty&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-oauth2&lt;/artifactId&gt;
            &lt;version&gt;v1-rev120-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client-java6&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.oauth-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-oauth-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-services-gmail&lt;/artifactId&gt;
            &lt;version&gt;v1-rev48-1.22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.5.0&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                    &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.jcraft&lt;/groupId&gt;
            &lt;artifactId&gt;jsch&lt;/artifactId&gt;
            &lt;version&gt;0.1.54&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.myjeeva.digitalocean&lt;/groupId&gt;
            &lt;artifactId&gt;digitalocean-api-client&lt;/artifactId&gt;
            &lt;version&gt;2.16&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
            &lt;artifactId&gt;gson&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.keepa.api&lt;/groupId&gt;
            &lt;artifactId&gt;backend&lt;/artifactId&gt;
            &lt;version&gt;LATEST&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.jdeferred&lt;/groupId&gt;
            &lt;artifactId&gt;jdeferred-core&lt;/artifactId&gt;
            &lt;version&gt;1.2.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;22.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build
I increased the memory from 1gb to 2gb however the memory is only 30% of what is available.
Any thoughts as to what the issue is?
",<java><mysql><hibernate><spring-boot>,14871,1,242,1534,2,17,39,71,36405,0,148,8,17,2018-11-29 13:23,2018-12-28 10:45,2019-01-03 20:10,29,35,Advanced,32
49780491,Plotting Histogram for all columns in a Data Frame,"I am trying to draw histograms for all of the columns in my data frame.
I imported pyspark and matplotlib. 
df is my data frame variable.
plt is matplotlib.pyplot variable
I was able to draw/plot histogram for individual column, like this:
bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
But when I try to plot it for all variables I am having issues. Here is the for loop I have so far:
for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
How would I do it? Thanks in advance.
",<python><apache-spark><pyspark><apache-spark-sql>,661,0,8,2692,8,43,69,67,40210,0,128,2,17,2018-04-11 16:45,2018-04-11 17:53,2018-04-11 17:53,0,0,Basic,10
58573628,How do you get the url of a postgresql database you made in pgadmin?,"so i created a postgresql database and i want to link it to my python code using sqlalchemy but since i made the database in pgadmin i dont know what to mention as the url of the database in the code.
thank you in advance.
",<python><postgresql><sqlalchemy><pgadmin>,223,0,0,568,1,5,14,40,39250,0,25,3,17,2019-10-26 18:38,2019-10-26 19:56,,0,,Basic,10
51823868,Activity Monitor always Paused in SSMS 17.8.1,"Whenever I open the Activity Monitor in SQL Server Management Studio 17.8.1(14.0.17277) the overview always switches to (paused).  Even if I choose resume, it quickly changes back to paused.
This happens on a variety of SQL Servers and SQL Server versions (2005 through 2016) so I don't believe it is a conflict with old vs new SQL Setups.
I can run Activity Monitor in SSMS 2012 (11.0.2100.60) on the same servers with no error which confirms that the service is actually running and functional.
Any help or insights would be appreciated.  I'm not a fan of switching back and forth between two management studios if I can help it. (I uses 17 so I can have context menus when right clicking on items in SSMS which wont work on 2016 servers in older versions of the studio).
",<sql-server><ssms-2012><activity-monitor><ssms-2017>,774,0,0,373,1,4,10,46,31182,0,35,4,17,2018-08-13 13:45,2019-03-06 12:20,,205,,Basic,14
48868156,How to install the specific version of postgres?,"I need to install a postgres 9.6.6 on Ubuntu 14.04, but the command
apt-get install postgresql-9.6
installes  9.6.7.
How can I install 9.6.6?
An output of command
apt-cache policy postgresql-9.6
is 
postgresql-9.6:
  Installed: 9.6.7-1.pgdg14.04+1
  Candidate: 9.6.7-1.pgdg14.04+1
  Version table:
 *** 9.6.7-1.pgdg14.04+1 0
        500 http://apt.postgresql.org/pub/repos/apt/ trusty-pgdg/main amd64 Packages
        100 /var/lib/dpkg/status
",<linux><postgresql><ubuntu>,443,1,9,957,2,15,30,57,38158,0,19,3,17,2018-02-19 14:19,2018-02-19 14:24,,0,,Basic,14
54738120,Build new Rails app Error loading the 'sqlite3' without evidently write verion,"When generate new rails app, and start the server ""rails s"", first massage I got:
  Puma caught this error: Error loading the 'sqlite3' Active Record adapter. Missing a gem it depends on? can't activate sqlite3 (~> 1.3.6), already activated sqlite3-1.4.0. Make sure all dependencies are added to Gemfile. (LoadError)
after reload a page:
  ActiveRecord::ConnectionNotEstablished
  No connection pool with 'primary' found.
  def retrieve_connection(spec_name) #:nodoc:
    pool = retrieve_connection_pool(spec_name)
    raise ConnectionNotEstablished, ""No connection pool with '#{spec_name}' found."" unless pool
    pool.connection
  end
I reinstall ruby, rails, bundler, all except rvm
and I don't know what to do
P.S.
this error disappears when I evidently write sqlite3 verion, but it should work fine from a without it!!!
Help!What to do? or maybe reinstall all of it?
",<ruby-on-rails><ruby><sqlite><rvm><bundler>,872,0,5,173,0,1,10,68,3939,0,2,3,17,2019-02-17 22:03,2019-02-18 3:02,2019-02-18 3:02,1,1,Basic,14
56526882,PgAdmin Error: Failed to decrypt the saved password,"After updating my pgAdmin version to V4-4.8 I can not login to my database anymore. I'm trying to login on Windows 10.
The error:
Failed to decrypt the saved password.
Error: 'utf-8' codec can't decode byte 0xc3 in position 0: invalid continuation byte
",<postgresql><pgadmin>,253,0,5,654,0,6,10,60,5811,,74,1,17,2019-06-10 12:57,2019-07-23 1:43,2019-07-23 1:43,43,43,Basic,14
48780903,"'limit' clause in Oracle SQL ""SQL command not properly ended""","I know that questions related to 'limit' have been asked before here,
and I have already referred to them. My question is somewhat different.
Here's my query:
select id,somecol from sometable where someval=2 order by id desc limit 3
I'm getting an error saying 'SQL command not properly ended'.
How do I resolve this? If you need additional information, feel free to tell me so.
",<sql><oracle><limit>,379,0,1,173,1,1,8,50,46578,0,0,2,17,2018-02-14 6:24,2018-02-14 6:37,2018-02-14 6:59,0,0,Basic,2
54809012,knex insert multiple rows,"I have a problem inserting many rows into postgres db with knex.
I have dynamic number of rows needed to be inserted. The result i expect is:
insert row four times (four is for an example. I dont know exact number of inserts as it comes dynamically from frontend):
field_id will be diffrent in every row: (1,2,3,4) - i have array of these ID's
id_of_product will be always the same
value will be always diffrent: (req.body[id] that comes from Frontend) - ID in brackets is same value as the field_id from an
array
How i can achieve that? I tried looping it with forEach, but it's async operation so i can't use .then() as it will be called four times
Here's what i tried. i dont know how to set field_id and req.body to take it dynamically.
fields = [1,2,3,4]
Expected result:
knex creates 4 inserts as follows:
field_id: 1,
product_id: some static id
value: frontValue[1]
ETC
knex('metadata').insert(
 [{ field_id: fields, 
    product_id: product_id, 
    value: req.body[fields] 
 }]
)
",<sql><postgresql><knex.js>,989,0,6,273,2,5,13,67,32728,0,3,1,17,2019-02-21 14:09,2019-02-21 14:31,2019-02-21 14:31,0,0,Basic,9
51977699,Turn Presto columns to rows via rotation,"This is the desired input and desired output. I'm unfamiliar with the terms used in SQL or Presto and the documentation seems to point to using map_agg but I think the problem here is dynamically creating columns but was curious if this is possible where the a, b, ... columns are known and finite.  
I'd be great to know the proper function for this in SQL or Presto and of course if this is possible. Preferably in a way that doesn't involve manually adding a clause per desired row => column. There must be a way to do this automatically or by providing a list of values to filter rows that get converted to headers (As in how 'a' below gets moved to being a column title) 
table_a: 
id | key | value
 0 |   'a'  |   1
 1 |   'b'  |   2
Then becomes desired:
id | 'a' | 'b' 
 0    1    2
The closest I can get is to use map_agg to get a set of key: values which can be pulled one at a time in the output. However the desired solution would be to not have to explicitly list every key I want outputted in the end and instead explode or roll out all keys of kvs:
with subquery_A as (
  select 
    id,   
    map_agg(A.key, A.value) as ""kvs""
  from A as a
  group by 1
)
select 
  sub_a.id,
  sub_a.kvs['a'],
  sub_a.kvs['b']
from subquery_A as sub_a
",<sql><string><pivot><presto>,1252,0,29,3507,5,32,38,40,25637,0,156,2,17,2018-08-23 2:51,2018-08-28 11:19,,5,,Basic,9
54334202,How to execute a stored procedure against linked server?,"We currently execute a stored procedure against a linked server using:
EXECUTE [LinkedServer].[DatabaseName].[dbo].[MyProcedure]
For example:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
And this works fine; querying through a linked server works fine.
Disabling the deprecated &quot;Remote Access&quot; feature
There is apparently a little known, seldom used, feature known as remote access. Microsoft has very little to say about what this feature is, except to say here:
This configuration option is an obscure SQL Server to SQL Server communication feature that is deprecated, and you probably shouldn't be using it.
ⓘ Important
This feature will be removed in the next version of Microsoft SQL Server. Do not use this feature in new development work, and modify applications that currently use this feature as soon as possible. Use sp_addlinkedserver instead.
The remote access option only applies to servers that are added by using sp_addserver, and is included for backward compatibility.
And from the SQL Server 2000 Books Online:
 Note Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
We only ever added linked servers, and have never used this &quot;remote access&quot; feature, and have never added a server using sp_addserver.
We're all good. Right?
Except turning off remote access breaks everything
An auditor has mentioned that we should turn off the remote access feature:
it's a security checkbox on their clipboard
it's deprecated by Microsoft
it's hardly ever used
and we don't use it
Should be fine, right?
Microsoft documents how to turn off this hardly used feature:
Configure the remote access Server Configuration Option
EXEC sp_configure 'remote access', 0 ;  
GO  
RECONFIGURE ;  
GO
Except when we do: everything goes to hell:
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure? 
Just to be absolutely sure I'm using a linked server, I:
EXECUTE sp_dropserver @server='screwdriver', @dropLogins='droplogins'
EXECUTE sp_addlinkedserver N'screwdriver', N'SQL Server'
and re-run my procedure call:
EXECUTE Screwdriver.CRM.dbo.GetCustomer 619
Msg 7201, Level 17, State 4, Procedure GetCustomer, Line 1
Could not execute procedure on remote server 'Screwdriver' because SQL Server is not configured for remote access. Ask your system administrator to reconfigure SQL Server to allow remote access.
Worse than failure!?
I can confirm that the server is a linked server (and is not a &quot;remote&quot; server) using:
SELECT SrvName, IsRemote 
FROM master..sysservers
WHERE SrvName = 'Screwdriver'
Srvname      IsRemote
-----------  --------
screwdriver  0
Or using the modern objects:
SELECT Name, Is_Linked
FROM sys.servers
WHERE Name = 'Screwdriver'
Name         Is_linked
-----------  --------
screwdriver  1
To Sum Up
We're at the point now where:
I've disabled remote access
Remote access only applies to servers added through sp_addserver
it doesn't apply to servers added through sp_addlinkedserver
I'm accessing a server added through sp_addlinkedserver
Why isn't it working?
Which brings me to my question:
How to execute a stored procedure against a linked server?
And the corollary:
How to not execute a stored procedure against an added (i.e. &quot;remote&quot;) server?
Bonus Chatter
The remote access configuration option, that you adjust using sp_configure, is also exposed through the user interface. The SSMS UI describes the feature incorrectly:
It incorrect phrasing is:
Allow remote connections to this server
It should be phrased:
Allow remote connections to from this server.
The Books Online also document the feature incorrectly:
Allow remote connections to this server
Controls the execution of stored procedures from remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from a remote server.
It should be:
Allow remote connections to from this server
Controls the execution of stored procedures from to remote servers running instances of SQL Server. Selecting this check box has the same effect as setting the sp_configure remote access option to 1. Clearing it prevents execution of stored procedures from to a remote server.
It makes sense that the youngsters at Microsoft these days don't remember what a 20 year old deprecated feature that they've never touched does.
Documentation from BOL 2000
SQL Server 2000 was the last time this feature was documented. Reproduced here for posterity and debugging purposes:
Configuring Remote Servers
A remote server configuration allows a client connected to one instance of Microsoft® SQL Server™ to execute a stored procedure on another instance of SQL Server without establishing another connection. The server to which the client is connected accepts the client request and sends the request to the remote server on behalf of the client. The remote server processes the request and returns any results to the original server, which in turn passes those results to the client.
If you want to set up a server configuration in order to execute stored procedures on another server and do not have existing remote server configurations, use linked servers instead of remote servers. Both stored procedures and distributed queries are allowed against linked servers; however, only stored procedures are allowed against remote servers.
 Note  Support for remote servers is provided for backward compatibility only. New applications that must execute stored procedures against remote instances of SQL Server should use linked servers instead.
See also
Executing a Stored Procedure on a Linked Server
SQL Linked server - Remote access error when executing SP
SQL Server Error: Could not execute remote procedure
MSDN Blogs: Unable to execute a remote stored procedure over a linked server (archive.is)
Configure the remote access Server Configuration Option
sp_addlinkedserver (Transact-SQL)
sp_configure (Transact-SQL)
Security Audit requires turning Remote Access off on all SQL 2005 and SQL 2008 Servers
Alternate title: Disabling SQL Server Remote Access breaks stored procedures.
",<sql-server><sql-server-2008-r2>,6489,16,29,248457,256,876,1230,36,60670,0,5616,4,17,2019-01-23 19:16,2019-01-24 13:43,2019-01-24 13:43,1,1,Basic,9
51768880,Duplicate columns in Oracle query using row limiting clause,"Since Oracle 12c, we can finally use the SQL standard row limiting clause like this:
SELECT * FROM t FETCH FIRST 10 ROWS ONLY
Now, in Oracle 12.1, there was a limitation that is quite annoying when joining tables. It's not possible to have two columns of the same name in the SELECT clause, when using the row limiting clause. E.g. this raises ORA-00918 in Oracle 12.1
SELECT t.id, u.id FROM t, u FETCH FIRST 10 ROWS ONLY
This is a restriction is documented in the manual for all versions 12.1, 12.2, 18.0:
The workaround is obviously to alias the columns
SELECT t.id AS t_id, u.id AS u_id FROM t, u FETCH FIRST 10 ROWS ONLY
Or to resort to ""classic"" pagination using ROWNUM or window functions.
Curiously, though, the original query with ambiguous ID columns runs just fine from Oracle 12.2 onwards. Is this a documentation bug, or an undocumented feature?
",<sql><oracle><pagination>,858,2,6,213305,130,697,1529,48,583,0,6816,2,17,2018-08-09 13:52,2018-09-08 6:27,,30,,Advanced,35
52197486,"FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off","It has been working fine for last several months; and suddenly started noticing this error in application,
FATAL: pg_hba.conf rejects connection for host ""127.0.0.1"", user ""postgres"", database ""prod"", SSL off
pg_hba.conf has,
# IPv4 local connections:
host    all             all             127.0.0.1/32            md5
host    all             all             0.0.0.0/0               md5
postgresql.conf has,
listen_addresses = '*'
Both file have not been touched/changed for many months.
Has anybody faced similar issue in a running environment ?
I have gone through several connection related issues on stoackoverflow; but they all point to one of these two files being misconfigured. Thats not the issue in this case.
The root cause is found and fixed.
This is what happened (for the benefit of those who might encounter such a strange issue)
Three mysterious entires were found in pg_hba.conf, right at the top of the file
These had reject method configured for user postgres, pgsql &amp; pgdbadm
None of our team members added them
Because these were right at the top, even before ""# PostgreSQL Client Authentication Configuration File...."" comment starts, we couldn't notice it.
I am still not sure, how these appeared there
It might be some upgrade issue - but we haven't updated Postgres
It might be a partially successful hacking attempt - still investigating this
But to be on safer side, we have changed server credentials and looking into other hardening methods.
It just might save someone a sleepless night, if such an issue occurs, in a perfectly running environment.
",<postgresql>,1583,0,5,161,1,1,5,35,39425,0,3,1,16,2018-09-06 6:06,2018-10-12 10:54,,36,,Basic,14
55508830,How to upgrade sqlite 3.8.2 to >= 3.8.3,"In a virtual Env with Python 3.7.2, I am trying to run django's python manage.py startap myapp and I get this error:
raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later is required (found 3.8.2).
I'm running Ubuntu Trusty 14.04 Server.
How do I upgrade or update my sqlite version to >=3.8.3?
I ran
$ apt list --installed | grep sqlite
libaprutil1-dbd-sqlite3/trusty,now 1.5.3-1 amd64 [installed,automatic]
libdbd-sqlite3/trusty,now 0.9.0-2ubuntu2 amd64 [installed]
libsqlite3-0/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
libsqlite3-dev/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
python-pysqlite2/trusty,now 2.6.3-3 amd64 [installed]
python-pysqlite2-dbg/trusty,now 2.6.3-3 amd64 [installed]
sqlite3/trusty-updates,trusty-security,now 3.8.2-1ubuntu2.2 amd64 [installed]
and
sudo apt install --only-upgrade libsqlite3-0
Reading package lists... Done
Building dependency tree      
Reading state information... Done
libsqlite3-0 is already the newest version.
0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.
EDIT:
the settings.py is stock standard:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
",<python><django><sqlite>,1375,0,24,301,1,2,6,80,31448,0,1,5,16,2019-04-04 5:59,2019-04-12 16:50,,8,,Basic,14
48327065,Why does conversion from DATETIME to DATETIME2 appear to change value?,"I had a stored procedure comparing two dates. From the logic of my application, I expected them to be equal. However, the comparison failed. The reason for this was the fact that one of the values was stored as a DATETIME and had to be CONVERT-ed to a DATETIME2 before being compared to the other DATETIME2. Apparently, this changed its value. I have run this little test:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2='2018-01-18 16:12:25.1130000'
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
Which gave me the following result:
Why is there the difference of 333333ns between these values? I thought that a DATETIME2, as a more precise type, should be able to accurately represent all the values which can be stored in a DATETIME? The documentation of DATETIME2 only says:
  When the conversion is from datetime, the date and time are copied. The fractional precision is extended to 7 digits.
No warnings about the conversion adding or subtracting 333333ns to or from the value! So why does this happen?
I am using SQL Server 2016. 
edit: Strangely, on a different server I am getting a zero difference. Both are SQL Server 2016 but the one where I have the problem has compatibility level set to 130, the one where I don't has it set to 120. Switching between them changes this behaviour.
edit2: DavidG suggested in the comments that the value I am using can be represented as a DATETIME2 but not a DATETIME. So I have modified my test to make sure that the value I am assigning to @DateTime2 is a valid DATETIME value:
DECLARE @DateTime DATETIME='2018-01-18 16:12:25.113'
DECLARE @DateTime2 DATETIME2=CONVERT(DATETIME2, @DateTime)
SELECT @DateTime, @DateTime2, DATEDIFF(NANOSECOND, @DateTime, @DateTime2)
This helps a little because the difference is smaller but still not zero:
",<sql><sql-server><t-sql><datetime>,1848,3,16,3879,2,27,40,60,4195,0,421,4,16,2018-01-18 17:25,2018-01-18 17:29,2018-01-18 17:43,0,0,Intermediate,17
52731361,SQLAlchemy Group By With Full Child Objects,"Imagine the following Media table:
| site       | show_id | time |
| ---------------------|-------|
| CNN        | 1       | 'a'   |
| ABC        | 2       | 'b'   |
| ABC        | 5       | 'c'   |
| CNN        | 3       | 'd'   |
| NBC        | 4       | 'e'   |
| NBC        | 5       | 'f'   |
--------------------------------
I would like to iterate over query results grouped by show_id and have tried this query:
listings = session.query(Media).filter(Media.site == ""CNN"").group_by(Media.show_id).all()
Here's how I would like to iterate over the results:
for showtimes in listings:
    for show in showtimes:
        print(show.time)
But that query doesn't give me all of the grouped child objects.  What am I missing?
",<python><sqlalchemy>,727,0,13,3820,5,43,56,49,9843,0,153,2,16,2018-10-10 1:26,2018-10-12 6:30,2018-10-12 6:30,2,2,Intermediate,17
62442611,InnoDB initialization warnings,"I am using docker on Windows 10 to build full web stack (php, nginx, mysql 8). Using docker compose.
docker-compose.yml 
Here I am using services to build web applcation. But i will show you only mysql service
version: '3.8'
services:
    db:
        build: services/mysql
        container_name: db
        image: projects/laradock_mysql:latest
        env_file: ../.env
        restart: on-failure
        volumes:
            - ./storage/data:/var/lib/mysql
        ports:
            - 3306:3306
        networks:
            - sites
networks:
    sites:
        driver: bridge
        ipam:
            driver: default
            config:
                - subnet: 10.100.36.0/24
MySQL DockerFile
FROM mysql:8
COPY conf/my.cnf /etc/mysql/conf.d
EXPOSE 3306
MySQL config file
[mysqld]
innodb_flush_method=O_DSYNC
innodb_flush_log_at_trx_commit=0
default-authentication-plugin=mysql_native_password
Output log when docker-compose up
2020-06-18T04:40:52.552867Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-06-18T04:40:55.549460Z 1 [Warning] [MY-012579] [InnoDB] fallocate(15, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.568116Z 1 [Warning] [MY-012579] [InnoDB] fallocate(16, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.582720Z 1 [Warning] [MY-012579] [InnoDB] fallocate(17, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.600203Z 1 [Warning] [MY-012579] [InnoDB] fallocate(18, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.614565Z 1 [Warning] [MY-012579] [InnoDB] fallocate(19, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.630446Z 1 [Warning] [MY-012579] [InnoDB] fallocate(20, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.649691Z 1 [Warning] [MY-012579] [InnoDB] fallocate(21, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.666272Z 1 [Warning] [MY-012579] [InnoDB] fallocate(22, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.682022Z 1 [Warning] [MY-012579] [InnoDB] fallocate(23, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:55.698763Z 1 [Warning] [MY-012579] [InnoDB] fallocate(24, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.379410Z 1 [Warning] [MY-012579] [InnoDB] fallocate(25, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, 16384) returned errno: 22
2020-06-18T04:40:56.384741Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
Thanks for a Help!
",<mysql><docker><alpine-linux>,2771,0,57,161,0,1,3,39,3162,0,0,1,16,2020-06-18 4:53,2020-10-06 18:23,,110,,Advanced,39
53297872,How can I sum multiple columns in a spark dataframe in pyspark?,"I've got a list of column names I want to sum
columns = ['col1','col2','col3']
How can I add the three and put it in a new column ? (in an automatic way, so that I can change the column list and have new results)
Dataframe with result I want:
col1   col2   col3   result
 1      2      3       6
",<python><apache-spark><pyspark><apache-spark-sql>,296,0,3,2093,4,16,38,75,47307,0,2055,3,16,2018-11-14 10:21,2018-11-14 10:25,2018-11-14 10:25,0,0,Basic,10
62761424,How to create a database diagrams in visual studio code?,"I am trying Visual studio code to code the database but I cannot create the database diagram. Is there a way I can create it just like in SSMS.
Thank you.
",<sql><database><visual-studio-code><rdbms><diagram>,155,1,0,161,2,2,4,50,33892,0,208,1,16,2020-07-06 17:31,2020-11-08 3:13,,125,,Basic,3
50370683,"Room migration: ""no such table: room_table_modification_log""","Room 1.1.0 version.
I am getting this error on the first run after migration.
It runs well if I close the app and start it again. 
ROOM: Cannot run invalidation tracker. Is the db closed?
java.lang.IllegalStateException: Cannot perform this operation because the connection pool has been closed.
    at android.database.sqlite.SQLiteConnectionPool.throwIfClosedLocked(SQLiteConnectionPool.java:1182)
    at android.database.sqlite.SQLiteConnectionPool.waitForConnection(SQLiteConnectionPool.java:792)
    at android.database.sqlite.SQLiteConnectionPool.acquireConnection(SQLiteConnectionPool.java:518)
Caused By : SQL(query) error or missing database.
    (no such table: room_table_modification_log (code 1): , while compiling: DELETE FROM proposals WHERE hotel_id = ?)
#################################################################
    at android.database.sqlite.SQLiteConnection.nativePrepareStatement(Native Method)
    at android.database.sqlite.SQLiteConnection.acquirePreparedStatement(SQLiteConnection.java:1095)
    at android.database.sqlite.SQLiteConnection.prepare(SQLiteConnection.java:660)
    at android.database.sqlite.SQLiteSession.prepare(SQLiteSession.java:588)
    at android.database.sqlite.SQLiteProgram.&lt;init&gt;(SQLiteProgram.java:59)
    at android.database.sqlite.SQLiteStatement.&lt;init&gt;(SQLiteStatement.java:31)
    at android.database.sqlite.SQLiteDatabase.compileStatement(SQLiteDatabase.java:1417)
    at android.arch.persistence.db.framework.FrameworkSQLiteDatabase.compileStatement(FrameworkSQLiteDatabase.java:64)
    at android.arch.persistence.room.RoomDatabase.compileStatement(RoomDatabase.java:244)
    at android.arch.persistence.room.SharedSQLiteStatement.createNewStatement(SharedSQLiteStatement.java:65)
    at android.arch.persistence.room.SharedSQLiteStatement.getStmt(SharedSQLiteStatement.java:77)
    at android.arch.persistence.room.SharedSQLiteStatement.acquire(SharedSQLiteStatement.java:87)
    at com.hotellook.db.dao.FavoritesDao_Impl.removeProposals(FavoritesDao_Impl.java:723)
The DB initialization code looks like this (Dagger AppModule)
@Provides
@Singleton
fun provideDataBase(app: Application): AppDatabase =
  Room.databaseBuilder(app, AppDatabase::class.java, DATABASE_NAME)
      .addMigrations(MIGRATION_1_2)
      .addMigrations(MIGRATION_2_3)
      .build()
private companion object {
  const val DATABASE_NAME = ""app.db""
  val MIGRATION_1_2 = object : Migration(1, 2) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column0 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column0 TEXT;"")
    }
  }
  val MIGRATION_2_3 = object : Migration(2, 3) {
    override fun migrate(database: SupportSQLiteDatabase) {
      database.execSQL(""CREATE TABLE table0 (column1 INTEGER NOT NULL, column2 TEXT NOT NULL, PRIMARY KEY(column1))"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column2 TEXT;"")
      database.execSQL(""ALTER TABLE table1 ADD COLUMN column3 REAL;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column1 TEXT;"")
      database.execSQL(""ALTER TABLE table2 ADD COLUMN column2 REAL;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column1 INTEGER DEFAULT 0;"")
      database.execSQL(""ALTER TABLE table3 ADD COLUMN column2 TEXT;"")
    }
  } 
The app upgrades from version 2 to 3.
How can I fix it?
",<android><sqlite><database-migration><android-room>,3454,0,52,912,1,8,19,39,5597,,16,3,16,2018-05-16 12:14,2018-05-19 0:29,2018-05-19 0:29,3,3,Intermediate,31
48454336,SSIS: Code page goes back to 65001,"In an SSIS package that I'm writing, I have a CSV file as a source. On the Connection Manager General page, it has 65001 as the Code page (I was testing something). Unicode is not checked.
The columns map to a SQL Server destination table with varchar (among others) columns. 
  There's an error at the destination: The column ""columnname"" cannot be processed because more than one code page (65001 and 1252) are specified for it.
My SQL columns have to be varchar, not nvarchar due to other applications that use it.
On the Connection Manager General page I then change the Code page to 1252 (ANSI - Latin I) and OK out, but when I open it again it's back to 65001. It doesn't make a difference if (just for test) I check Unicode or not.
As a note, all this started happening after the CSV file and the SQL table had columns added and removed (users, you know.) Before that, I had no issues whatsoever. Yes, I refreshed the OLE DB destination in the Advanced Editor.
This is SQL Server 2012 and whichever version of BIDS and SSIS come with it.
",<sql-server><csv><encoding><ssis><etl>,1045,0,6,2367,15,50,68,75,62382,0,393,7,16,2018-01-26 0:56,2018-01-27 11:31,2018-01-27 11:31,1,1,Intermediate,15
56746192,Table Partitions with Entity Framework Core,"I have a large database that will use partitioned column-store tables in a redesign. Is it possible to specify the partition in the generated sql with Entity Framework Core 2.2?
This is for an Azure SQL hyperscale database with a table which currently contains about 3 billion rows. When using stored procedures to execute the requests performance is excellent but if the partition range is not specified in the query performance is less that optimal. I am hoping to move away from the inline sql we are currently using in the application layer and move to entity framework core. Being able to specify the partition for the tenant is our only blocker at the moment. 
This is an example where clause in the stored proceduure
Select @Range = $PARTITION.TenantRange(@InputTenantId)
Select ..... FROM xxx where $PARTITION.TenantRange(TenantId) = @range
The above query would provide excellent performance but I am hoping I can make the same specification of the partition using entity framework.
",<sql-server><entity-framework-core><data-partitioning>,992,0,3,161,0,1,4,66,2529,0,0,0,16,2019-06-25 2:49,,,,,Basic,3
58786791,Is there a way to directly insert data from a parquet file into PostgreSQL database?,"I'm trying to restore some historic backup files that saved in parquet format, and I want to read from them once and write the data into a PostgreSQL database.
I know that backup files saved using spark, but there is a strict restriction for me that I cant install spark in the DB machine or read the parquet file using spark in a remote device and write it to the database using spark_df.write.jdbc. Everything needs to happen on the DB machine and in the absence of spark and Hadoop only using Postgres and Bash scripting.
my files structure is something like:
foo/
    foo/part-00000-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00001-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    foo/part-00002-2a4e207f-4c09-48a6-96c7-de0071f966ab.c000.snappy.parquet
    ..
    ..
I expect to read data and schema from each parquet folder like foo, create a table using that schema and write the data into the shaped table, only using bash and Postgres CLI.
",<bash><postgresql><hdfs><parquet>,987,0,8,410,1,4,16,65,31361,0,243,2,16,2019-11-10 8:05,2019-11-10 14:41,2019-11-10 14:41,0,0,Basic,10
54026900,Check that a List parameter is null in a Spring data JPA query,"I have a Spring Boot application and use Spring Data JPA to query a MySQL database.
I need to get a list of courses filtered with some parameters.
I usually use the syntax param IS NULL or (/*do something with param*/) so that it ignores the parameter if it is null.
With simple datatypes I have no problems but when it comes to a List of objects I don't know how to check for NULL value. How can I check if the ?3 parameter is NULL in the following query ?
@Query(""SELECT DISTINCT c FROM Course c\n"" +
       ""WHERE c.courseDate &lt; CURRENT_TIME\n"" +
       ""AND (?1 IS NULL OR (c.id NOT IN (3, 4, 5)))\n"" +
       ""AND (?2 IS NULL OR (c.title LIKE ?2 OR c.description LIKE ?2))\n"" +
       ""AND ((?3) IS NULL OR (c.category IN ?3)) "")
List&lt;Course&gt; getCoursesFiltered(Long courseId, String filter, List&lt;Category&gt; categories);
Error is :
  could not extract ResultSet; SQL [n/a]; nested exception is org.hibernate.exception.DataException: could not extract ResultSet[SQL: 1241, 21000]
And in the stack trace I can see :
  Caused by: java.sql.SQLException: Operand should contain 1 column(s)
Indeed generated query would be ... AND ((?3, ?4, ?5) IS NULL OR (c.category IN (?3, ?4, ?5))) if my list contains 3 elements. But IS NULL cannot be applied to multiple elements (query works fine if my list contain only one element).
I have tried size(?3) &lt; 1, length(?3) &lt; 1, (?3) IS EMPTY, etc. but still no luck.
",<sql><jpa><spring-data-jpa><spring-data><hql>,1426,0,15,14705,11,57,84,54,36913,0,1335,4,16,2019-01-03 17:17,2019-01-03 17:49,2019-01-04 8:07,0,1,Basic,10
58623291,How to restore database from dump or sql file in docker using volume?,"I am trying to run my database in docker which already have some data into it, But when I run it in docker it gives empty .. So how to restore my database in docker PostgreSQL image.
",<postgresql><docker><docker-compose><docker-volume>,183,0,0,171,1,2,7,58,21776,,0,1,16,2019-10-30 10:27,2019-10-30 11:11,2019-10-30 11:11,0,0,Basic,10
49210401,select 30 random rows where sum amount = x,"I have a table 
items
id int unsigned auto_increment primary key,
name varchar(255)
price DECIMAL(6,2)
I want to get at least 30 random items from this table where the total sum of price equals 500, what's the best approach to accomplish this?
I have seen this solution which looks have a similar issue MySQL Select 3 random rows where sum of three rows is less than value
And I am wondering if there are any other solutions that are easier to implement and/or more efficient 
",<php><mysql>,477,1,4,534,1,5,15,52,1463,0,42,7,16,2018-03-10 14:53,2018-03-11 0:47,2018-03-18 23:20,1,8,Basic,10
60138692,sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation <<table>>,"I've read over 20 different questions with the same problem - and the suggested answers didn't solve my problem. I'm still getting sqlalchemy psycopg2.errors.InsufficientPrivilege: permission denied for relation &lt;&lt;table&gt;&gt;
Environment: EC2, debian 8, postgresql, flask, sqlalchemy
my table in postgresql:
Create table Members(
id BIGSERIAL PRIMARY KEY,
joinDate TIMESTAMPTZ DEFAULT Now(),
password TEXT NOT NULL
);
directly in postgresql: INSERT INTO members (password) VALUES('123')  RETURNING id; works perfect
I've granted my user all possible grants
grant all privileges on database &lt;my_db&gt; to &lt;my_user&gt;
grant all privileges on all table in schema public to &lt;my_user&gt;
grant all privileges on all relations in schema public to &lt;my_user&gt;
I've also created a .htaccess file under var/www/html with Header set Access-Control-Allow-Origin ""*""
my table mapped with sqlalchemy member.py
from datetime import datetime
from sqlalchemy import create_engine, Column, String, Integer, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from marshmallow import Schema, fields
from sqlalchemy.schema import UniqueConstraint
import sqlalchemy as sa
Engine = create_engine('postgresql://&lt;my_user&gt;:&lt;my_psw&gt;@&lt;my_db_url&gt;/&lt;my_dn_name&gt;')
Session = sessionmaker(bind=Engine)
Base = declarative_base()
class MemberSchema(Schema):
    id = fields.Number()    
    password = fields.Str()
class Member(Base):
    __tablename__ = 'members'
    id = Column(Integer, primary_key=True)
    password = Column(String)
    def __init__(self, password):
        self.password = password
the code receiving the POST and doing the insert in test.py:
from flask import Flask, render_template, request,jsonify
from flask_cors import CORS, cross_origin
import logging
def register_member(password):
    session = Session()
    newMember = Member(password)
    session.add(newMember)
    session.commit()
    return newMember
app = Flask(__name__)
cors = CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'
@app.route(""/register"", methods=[""GET"", ""POST""])
@cross_origin()    
def register():
    try:
        formData = request.form
        register_member(formData['password'])
        return jsonify({""message"":""Regsitrated successfully!""})
    except Exception as ex:
        logging.debug(ex)
if __name__ == ""__main__"":
    app.run(host='0.0.0.0', port=1234, debug=True)
executing this code results in exception with these errors
DEBUG:root:(psycopg2.errors.InsufficientPrivilege) permission denied for relation members
[SQL: INSERT INTO members (password) VALUES (%(password)s) RETURNING members.id]
[parameters: {'password': '4444'}]
and
TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.
Any help would be appreciated
",<python><postgresql><sqlalchemy><permissions>,2887,0,71,1798,4,20,38,39,30027,0,8,4,16,2020-02-09 16:19,2020-02-19 17:51,,10,,Basic,6
51627330,SQL to find upper case words from a column,"I have a description column in my table and its values are:
This is a EXAMPLE
This is a TEST
This is a VALUE
I want to display only EXAMPLE, TEST, and VALUE from the description column.
How do I achieve this?
",<sql><oracle>,209,0,3,275,0,2,14,80,6722,0,4,7,16,2018-08-01 7:09,2018-08-01 7:43,,0,,Basic,10
61859247,Cannot connect with SSMS to SQL Server on Docker,"I have followed the official Microsoft documentation and I have installed SQL Server Docker image 
As result I have a SQL Server image running on Docker at the IP address 172.17.0.2
I also can easily connect to it using sqlcmd with my dummy password
The problem is that I cannot connect to it through SSMS:
Login failed for user 'sa'. (Microsoft SQL Server, Error: 18456)
Of course I read other StackOverflow posts before posting this question and I have tried multiple logins: 
localhost,1433
localhost:1433
172.17.0.2,1433
etc...
How can I connect if localhost doesn't work as well as the IP address of the docker image? 
",<sql-server><docker><containers><ssms><sqlcmd>,624,5,1,10484,15,78,123,45,21064,0,705,9,16,2020-05-17 21:45,2020-05-20 5:49,,3,,Intermediate,20
50999358,Xampp MySQL not starting - “MYSQL not starting on XAMPP 3.2.1 version…”,"i installed xampp version 3.2.1 on my laptop and mysql was working fine on it before but suddenly mysql stopped working while apache and others were working.
when i click on start mysql it displays this error
i use windows 10
8:52:32 AM  [mysql]     Status change detected: running
8:52:40 AM  [mysql]     Status change detected: stopped
8:52:40 AM  [mysql]     Error: MySQL shutdown unexpectedly.
8:52:40 AM  [mysql]     This may be due to a blocked port, missing dependencies, 
8:52:40 AM  [mysql]     improper privileges, a crash, or a shutdown by another method.
8:52:40 AM  [mysql]     Press the Logs button to view error logs and check
8:52:40 AM  [mysql]     the Windows Event Viewer for more clues
8:52:40 AM  [mysql]     If you need more help, copy and post this
8:52:40 AM  [mysql]     entire log window on the forums
then i checked the log file and this is what i found there
`
2018-06-23 08:52:32 2354 InnoDB: Warning: Using innodb_additional_mem_pool_size is DEPRECATED. This option may be removed in future releases, together with the option innodb_use_sys_malloc and with the InnoDB's internal memory allocator.
180623  8:52:32 [Note] InnoDB: Using mutexes to ref count buffer pool pages
180623  8:52:32 [Note] InnoDB: The InnoDB memory heap is disabled
180623  8:52:32 [Note] InnoDB: Mutexes and rw_locks use Windows interlocked functions
180623  8:52:32 [Note] InnoDB: Memory barrier is not used
180623  8:52:32 [Note] InnoDB: Compressed tables use zlib 1.2.3
180623  8:52:32 [Note] InnoDB: Not using CPU crc32 instructions
180623  8:52:32 [Note] InnoDB: Initializing buffer pool, size = 16.0M
180623  8:52:32 [Note] InnoDB: Completed initialization of buffer pool
180623  8:52:32 [Note] InnoDB: Highest supported file format is Barracuda.
180623  8:52:32 [Note] InnoDB: 128 rollback segment(s) are active.
180623  8:52:32 [Note] InnoDB: Waiting for purge to start
180623  8:52:33 [Note] InnoDB:  Percona XtraDB (http://www.percona.com) 5.6.22-72.0 started; log sequence number 1600924
180623  8:52:33 [Note] Plugin 'FEEDBACK' is disabled.
180623  8:52:33 [Note] Server socket created on IP: '::'.
180623  8:52:33 [Warning] 'db' entry 'size of datafile is: 1907       should be: 1760
180409 15:17:15 [error] mysql.db: got error: 0 when reading datafile at record: 3
180409 15:17:15 [error] got an error from unknown thread, ha _myisam.cc:952
180409 15:17:15 [Warning] Recove@80409 15:17:15 [Warning] Checking table:   '.\mysql\db'
180409 15:17:15 [ERROR] mysql.db: 1 client is using or hasn't closed the table properly
180409 15:17:15 [ERROR] mysql.db:' had database in mixed case that has been forced to lowercase because lower_case_table_names is set. It will not be possible to remove this privilege using REVOKE.
180623  8:52:33 [ERROR] Missing system table mysql.roles_mapping; please run mysql_upgrade to create it
180623  8:52:33 [ERROR] Incorrect definition of table mysql.event: expected column 'sql_mode' at position 14 to have type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','IGNORE_BAD_TABLE_OPTIONS','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH'), found type set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_A
180623  8:52:33 [ERROR] Event Scheduler: An error occurred when initializing system tables. Disabling the Event Scheduler.
180623  8:52:33 [Warning] Failed to load slave replication state from table mysql.gtid_slave_pos: 1146: Table 'mysql.gtid_slave_pos' doesn't exist
180623  8:52:33 [Warning] Neither --relay-log nor --relay-log-index were used; so replication may break when this MySQL server acts as a slave and has his hostname changed!! Please use '--log-basename=#' or '--relay-log=mysql-relay-bin' to avoid this problem.
180623  8:52:33 [Note] Started replication for '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
'
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL thread initialized, starting replication in log 'FIRST' at position 0, relay log '.\mysql-relay-bin-180401@002018@003a51@003a15@0020@005bnote@005d@0020c@003a@005cxampp@005cmysql@005cbin@005cmysqld@002eexe@003a@0020ready@0020for@0020connections@002e@000d.000095' position: 4
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O: Fatal error: Invalid (empty) username when attempting to connect to the master server. Connection attempt terminated. Internal MariaDB error code: 1593
180623  8:52:33 [ERROR] Failed to open the relay log '.\mysql-relay-bin-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.000036' (relay_log_pos 4)
180623  8:52:33 [ERROR] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave SQL: Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist, Internal MariaDB error code: 1146
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread killed while connecting to master
180623  8:52:33 [ERROR] Could not find target log during relay log initialization
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Slave I/O thread exiting, read up to log 'FIRST', position 4
180623  8:52:33 [ERROR] Initialized Master_info from 'master-version@003a@0020@002710@002e0@002e17@002dmariadb@0027@0020@0020socket@003a@0020@0027@0027@0020@0020port@003a@00203306@0020@0020mariadb@002eorg@0020binary@0020distribution@000d.info' failed
180623  8:52:33 [Warning] Reading of some Master_info entries failed
180623  8:52:33 [ERROR] Failed to initialize multi master structures
180623  8:52:33 [ERROR] Aborting
180623  8:52:33 [Note] Master '180401 18:51:15 [Note] c:\xampp\mysql\bin\mysqld.exe: ready for connections.
': Error reading relay log event: slave SQL thread was killed
180623  8:52:33 [Note] InnoDB: FTS optimize thread exiting.
180623  8:52:33 [Note] InnoDB: Starting shutdown...
180623  8:52:35 [Note] InnoDB: Shutdown completed; log sequence number 1600934
180623  8:52:35 [Note] C:\xampp\mysql\bin\mysqld.exe: Shutdown complete`
",<mysql><xampp>,7304,1,67,161,1,1,3,41,11239,0,0,5,16,2018-06-23 8:23,2020-04-22 9:07,,669,,Basic,14
51089952,Conversion failed when converting the ****** value '******' to data type ******,"I'm getting this error from SQL Server on SSMS 17 while running a giant query:
  Conversion failed when converting the ****** value '******' to data type ******
I have never seen this with ****'s before, and google searching seems to come up with nothing. Is there a known cause for why SQL Server would provide this message with asterisks?
",<sql><sql-server>,341,0,0,32923,40,135,186,72,13685,0,260,6,16,2018-06-28 19:19,2018-10-12 20:44,,106,,Basic,9
57955765,Upgrading to MYSQL 8 - MAMP,"I have installed MAMP on my MAC machine and I am trying to upgrade it to use MySQL 8. However, I am not having any luck. I have tried following script but database migration fails. 
Also, sequelPro fails to connect to DB but, phpmyadmin has no issue connecting.
#!/bin/sh
curl -OL https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.17-macos10.14-x86_64.tar.gz
tar xfvz mysql-8.0.*
echo ""stopping mamp""
sudo /Applications/MAMP/bin/stop.sh
sudo killall httpd mysqld
echo ""creating backup""
sudo rsync -arv --progress /Applications/MAMP ~/Desktop/MAMP-Backup
echo ""copy bin""
sudo rsync -arv --progress mysql-8.0.*/bin/* /Applications/MAMP/Library/bin/ --exclude=mysqld_multi --exclude=mysqld_safe 
echo ""copy share""
sudo rsync -arv --progress mysql-8.0.*/share/* /Applications/MAMP/Library/share/
echo ""fixing access (workaround)""
sudo chmod -R o+rw  /Applications/MAMP/db/mysql/
sudo chmod -R o+rw  /Applications/MAMP/tmp/mysql/
#sudo chmod -R o+rw  ""/Library/Application Support/appsolute/MAMP PRO/db/mysql/""
echo ""starting mamp""
sudo /Applications/MAMP/bin/start.sh
echo ""migrate to new version""
/Applications/MAMP/Library/bin/mysql -u root --password=root -h 127.0.0.1
",<mysql><macos><mamp><mysql-8.0>,1174,1,28,503,1,3,14,62,12091,0,9,0,16,2019-09-16 11:21,,,,,Basic,14
55502546,Find mean of pyspark array<double>,"
In pyspark, I have a variable length array of doubles for which I would like to find the mean. However, the average function requires a single numeric type. 
Is there a way to find the average of an array without exploding the array out? I have several different arrays and I'd like to be able to do something like the following:
df.select(col(""Segment.Points.trajectory_points.longitude""))
DataFrame[longitude: array]
df.select(avg(col(""Segment.Points.trajectory_points.longitude""))).show()
org.apache.spark.sql.AnalysisException: cannot resolve
'avg(Segment.Points.trajectory_points.longitude)' due to data type
mismatch: function average requires numeric types, not
ArrayType(DoubleType,true);;
If I have 3 unique records with the following arrays, I'd like the mean of these values as the output. This would be 3 mean longitude values.  
Input:  
[Row(longitude=[-80.9, -82.9]),
 Row(longitude=[-82.92, -82.93, -82.94, -82.96, -82.92, -82.92]),
 Row(longitude=[-82.93, -82.93])]
Output:  
-81.9,
-82.931,
-82.93
I am using spark version 2.1.3.
Explode Solution:
So I've got this working by exploding, but I was hoping to avoid this step. Here's what I did
from pyspark.sql.functions import col
import pyspark.sql.functions as F
longitude_exp = df.select(
    col(""ID""), 
    F.posexplode(""Segment.Points.trajectory_points.longitude"").alias(""pos"", ""longitude"")
)
longitude_reduced = long_exp.groupBy(""ID"").agg(avg(""longitude""))
This successfully took the mean. However, since I'll be doing this for several columns, I'll have to explode the same DF several different times. I'll keep working through it to find a cleaner way to do this.
",<apache-spark><pyspark><apache-spark-sql>,1641,0,21,329,0,2,11,68,9340,0,35,2,16,2019-04-03 19:05,2019-04-03 20:40,2019-04-03 20:40,0,0,Basic,1
50676867,Executing Named Queries in Athena,"We want to execute a parameterized query in Athena using the javascript sdk by aws.
Seems Athena's named query may be the way to do, but the documentation seems very cryptic to understand how to go about doing this. 
It would be great if someone can help us do the following
What is the recommended way to avoid sql injection in athena?
Create a parameterized query like SELECT c FROM Country c WHERE c.name = :name 
Pass the name parameter's value
Execute this query
",<amazon-web-services><sql-injection><amazon-athena><named-query>,468,1,2,13452,4,61,79,74,7538,0,164,1,16,2018-06-04 9:00,2019-02-03 21:02,,244,,Basic,3
58351958,psycopg2.ProgrammingError: column cons.consrc does not exist,"I am working on a web app where I needed to update the Postgres(12) database. I am using flask-migrate for the purpose.
I have tried to fix it, but it seems it is a new issue and hence I am not getting much help. I have tried to modify the database from my terminal, that works obviously. This is what I usually type to update my db:
flask db migrate -m ""..""
This is the error message I am receiving at the terminal:
psycopg2.ProgrammingError: column cons.consrc does not exist
LINE 4:                 cons.consrc as src
                        ^
HINT:  Perhaps you meant to reference the column ""cons.conkey"" or the column ""cons.conbin"".
",<python><database><postgresql><flask>,639,0,5,165,0,1,7,40,8014,0,23,1,16,2019-10-12 7:36,2019-10-21 22:48,2019-10-21 22:48,9,9,Basic,9
59584212,How to grant Refresh permissions to the materialized view to user in POSTGRESQL?,"I am executing sql file on linux by running script.
I can see my queries are getting executed fine but I have following query to refresh view in my testData.sql file that is giving me the error
refresh MATERIALIZED VIEW view_test
Error
psql:/home/test/sql/testData.sql:111: ERROR:  must be owner of relation view_test
I have applied following permissions
grant select,update,delete,insert on view_test to &quot;user123&quot;;
How to grant refresh permissions to the View in POSTGRESQL?
",<postgresql><view><refresh>,486,0,3,565,3,9,21,35,14884,0,26,3,16,2020-01-03 19:23,2021-11-17 10:13,,684,,Basic,9
52225503,Generate C# class from SQL Server table without Store Procedure,"Is there any way to generate a class for table in SQL Server without adding table in project with ADO.net Entity Framework?
public class Approval
{
        public long Id { get; set; }
        public long? FormId { get; set; }
        public long? DesignationId { get; set; }
        public DateTime? CreatedOn { get; set; }
}
",<c#><sql><asp.net><sql-server><oop>,327,1,7,376,1,3,21,70,43628,0,63,3,16,2018-09-07 15:19,2018-10-24 17:08,2019-07-09 15:58,47,305,Basic,3
57994758,Why Parquet over some RDBMS like Postgres,"I'm working to build a data architecture for my company. A simple ETL with internal and external data with the aim to build static dashboard and other to search trend.
I try to think about every step of the ETL process one by one and now I'm questioning about the Load part.
I plan to use Spark (LocalExcecutor on dev and a service on Azure for production) so I started to think about using Parquet into a Blob service. I know all the advantage of Parquet over CSV or other storage format and I really love this piece of technology. Most of the articles I read about Spark finish with a df.write.parquet(...).
But I cannot figure it out why can I just start a Postgres and save everything here. I understand that we are not producing 100Go per day of data but I want to build something future proof in a fast growing company that gonna produce exponentially data by the business and by the logs and metrics we start recording more and more.
Any pros/cons by more experienced dev ?
EDIT : What also make me questioning this is this tweet : https://twitter.com/markmadsen/status/1044360179213651968
",<postgresql><apache-spark><parquet>,1097,2,1,2620,6,38,71,78,9357,0,682,3,16,2019-09-18 14:09,2019-09-19 7:55,2019-09-19 7:55,1,1,Intermediate,23
52360260,SQL Server The column cannot be processed because more than 1 code page (65001 and 1252) are specified for it,"I did a select * on a table. I exported the results and tried to import it and write to another table (I have to do this via SSIS, I cannot do this via SQL Server (i.e. select into) for security permission purposes, but I can do it this way).
How can I fix this error? What exactly does it mean? I tried searching StackOverflow but my search was not very helpful.
I tried to use an OLEDB source step, but my server instance does not appear for some reason, so now I'm trying to export/import out of SSMS.
",<sql-server><t-sql><ssis>,505,0,2,3043,8,48,95,67,46891,0,904,3,16,2018-09-17 3:06,2018-09-18 7:16,2018-09-18 7:16,1,1,Intermediate,18
52749377,"Google Cloud SQL Postgres, when will PG 10 be available?","I am planning on moving our main project to Postgres 10 at some point.  I like to keep the local dev's database version close to what we are running on prod.
Currently our prod database is on Google Cloud SQL PostgreSQL 9.6.  I have not heard anything from Google on when this managed cloud sql product will offer Postgres 10.x in addition to 9.6.
Does anyone know when Postgres 10 will be a supported option on GCP's managed SQL product?  I would like to start planning for it.
",<postgresql><google-cloud-platform><google-cloud-sql><postgresql-10>,479,0,0,161,0,1,3,38,3227,0,0,4,16,2018-10-10 22:01,2018-10-11 6:28,,1,,Basic,3
50254277,Connection refused when running Laravel artisan command with Docker,"I'm running Laravel 5.4 in Docker. This is my docker-compose.yml file:
version: '2'
services:
  app:
    container_name: laravel_app
    image: webdevops/php-apache-dev:ubuntu-16.04
    links:
      - mysql
    depends_on:
      - mysql
    ports:
      - 8888:80
    volumes:
      - .:/app
    environment:
      docker: 'true'
      WEB_DOCUMENT_ROOT: '/app/public'
      WEB_NO_CACHE_PATTERN: '\.(.*)$$'
      working_dir: '/app'
  mysql:
    image: mariadb:latest
    ports:
      - 8889:80
    environment:
      MYSQL_ROOT_PASSWORD: 'dev'
      MYSQL_DATABASE: 'dev'
      MYSQL_USER: 'dev'
      MYSQL_PASSWORD: 'dev'
This is the relevant part of my .env file:
DB_CONNECTION=mysql
DB_HOST=mysql
DB_PORT=8889
DB_DATABASE=dev
DB_USERNAME=dev
DB_PASSWORD=dev
I am able to see the Laravel welcome page - that side of things works. But when I run php artisan migrate I get this error:
  SQLSTATE[HY000] [2002] Connection refused (SQL: select * from information_schema.tables where table_schema = dev and table_name = migrations)
I have tried fiddling with the host and port parameters in the .env file.
",<php><mysql><laravel><docker><docker-compose>,1106,0,40,7365,20,69,124,35,28061,0,197,6,16,2018-05-09 13:11,2018-05-09 13:29,2018-05-09 13:29,0,0,Basic,14
48651369,What is the best practice of firestore data structure?,"I'm making a blog app using firebase.
I want to know the best practice of data structure.
As far as I know, there are 2 case.
(I'm using react native)
case 1:
posts
  -postID
   -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
    -favoriteList
      -postID(onlyID)
      -postID(onlyID)
In this case, for example, when we need to get favorite posts.
firebase.firestore().collection(`favorites/${userID}/favoriteList`)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((favorite) =&gt; {
        firebase.firestore().collection(`favorites/`).doc(`${favorite.id}`)
          .get()
          .then((post) =&gt; {
          myPostList.push(post.data())
        });
  });
in this case, we can't order the favorite posts by createdDate. So, need to sort client side. Even if so, we don't use limit() function.
case 2:
posts
  -postID
  -title,content,author(userID),createdDate,favoriteCount
favorites
  -userID
     -favoriteList
       -postID
         -title,content,author(userID),createdDate,favoriteCount
       -postID
         -title,content,author(userID),createdDate,favoriteCount
firebase.firestore().collection(`favorites/${userID}/favoriteList`).orderBy('createdDate','desc').limit(30)
    .get()
    .then((snapshot) =&gt; {
      snapshot.forEach((post) =&gt; {
        myPostList.push(post.data())
      });
  });
in this case, When the favorite post is modified by the author,
we have to update all of the favorite posts. (e.g. If 100 users save the post as a favorite, we have to update to 100 data.)
(And I'm not sure we can increment favoritecount by a transaction, exactly same.)
I think if we use firebase.batch(), we can manage it. But I think it seems Inefficient.
It seems that both ways are not perfect. Do you know the best practice of this case?
",<javascript><firebase><react-native><nosql><google-cloud-firestore>,1811,0,40,548,3,11,24,70,12934,0,32,2,16,2018-02-06 20:24,2018-03-26 2:19,,48,,Advanced,33
55336035,Is it possible and how create a trigger with Entity Framework Core,"Is it possible to create a SQL trigger with Entity Framework Core.
Maybe by using instruction in 
protected override void OnModelCreating(DbModelBuilder dbModelBuilder)
{
}
Or simply by executing SQL statements in Migration scripts
public override void Up()
{
}
",<sql-server><entity-framework-core><database-trigger>,262,0,6,17827,31,124,203,58,20650,,522,2,16,2019-03-25 10:46,2020-02-18 9:41,2020-02-18 9:41,330,330,Basic,10
55956801,Shutting down ExecutorService 'applicationTaskExecutor',"I have a Spring-boot application deployed in docker container within in AWS ECS cluster.
My application stack is => Spring Boot -- JPA -- MySQL RDS
Initially application is deployed and accessible through EC2 public IP.
But after few minutes only application is Shutting down ExecutorService 'applicationTaskExecutor' and restart container again. it is happening constantly in every 3/4 mins.
I am not getting this kind of error in local deployment connecting to same RDS.
Here is my application.properties
server.port=8192
spring.datasource.url=jdbc:mysql://multichannelappdatabase.cvjsdfsdfsdfsfs.us-east-1.rds.amazonaws.com:3306/multichannelappdb
spring.datasource.username=xxxxx
spring.datasource.password=xxxxx
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.jackson.serialization.FAIL_ON_EMPTY_BEANS=false
spring.jpa.database-platform=org.hibernate.dialect.MySQLDialect
spring.jpa.show-sql=true
#spring.jpa.hibernate.ddl-auto=create
spring.datasource.testWhileIdle = true
spring.datasource.timeBetweenEvictionRunsMillis = 10000
spring.datasource.tomcat.testOnBorrow=true 
spring.datasource.tomcat.validationQuery=SELECT 1
and here is my cloudwatch log

16:20:50
2019-05-02 16:20:50.514 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8192 (http) with context path ''

16:20:50
2019-05-02 16:20:50.516 INFO 1 --- [ main] c.api.app.runner.UserManagerApplication : Started UserManagerApplication in 9.338 seconds (JVM running for 10.291)

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring DispatcherServlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.117 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Initializing Servlet 'dispatcherServlet'

16:20:57
2019-05-02 16:20:57.131 INFO 1 --- [nio-8192-exec-1] o.s.web.servlet.DispatcherServlet : Completed initialization in 14 ms

16:23:19
2019-05-02 16:23:19.253 INFO 1 --- [ Thread-4] o.s.s.concurrent.ThreadPoolTaskExecutor : Shutting down ExecutorService 'applicationTaskExecutor'

16:23:19
2019-05-02 16:23:19.254 INFO 1 --- [ Thread-4] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'

16:23:19
2019-05-02 16:23:19.257 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...

16:23:19
2019-05-02 16:23:19.274 INFO 1 --- [ Thread-4] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed.
Need suggestion to resolve this issue.
Still getting the same issue .. any solution will be helpful
",<mysql><spring-boot><spring-data-jpa><amazon-rds><amazon-ecs>,2620,0,44,1196,4,14,31,64,26122,,17,4,16,2019-05-02 16:37,2019-06-06 12:58,,35,,Intermediate,31
49142684,psql how to drop current database,"Trying to drop the primary database ""foo"", I get the error
  ERROR:  cannot drop the currently open database
Whats the right way to drop the primary psql database? 
",<postgresql>,165,0,0,16955,16,138,148,81,21100,,4078,2,16,2018-03-07 1:24,2018-03-07 1:24,2018-03-07 8:36,0,0,Basic,10
53720353,"Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""","This happens even when the DB user specified in .env file is different. For the record ""ankitj"" is also the username of my system. I don't understand why this is happening.
Here's the error:
Unhandled rejection SequelizeConnectionError: password authentication failed for user ""ankitj""
    at connection.connect.err (/home/ankitj/Desktop/skillbee/node_modules/sequelize/lib/dialects/postgres/connection-manager.js:128:24)
    at Connection.connectingErrorHandler (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/client.js:140:14)
    at Connection.emit (events.js:160:13)
    at Socket.&lt;anonymous&gt; (/home/ankitj/Desktop/skillbee/node_modules/pg/lib/connection.js:124:12)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
",<node.js><postgresql><sequelize.js>,906,0,10,431,2,5,8,77,14356,0,27,6,16,2018-12-11 8:45,2019-01-23 21:36,,43,,Basic,13
49286687,"Managing database connections in Node.js, best practices?","I'm building an Node application which will query simple and more complex (multiple joins) queries. I'm looking for suggestions on how I should manage the mySQL connections. 
I have the following elements: 
server.js : Express
router1.js (fictive name) : Express Router middleware
router2.js (fictive name) : Express Router middleware
    //this is router1
    router.get('/', function (req, res){
    connection.connect(function(Err){...});
      connection.query('SELECT* FROM table WHERE id = ""blah""', function(err,results,fields){
        console.log(results);
      });
      ...
    connection.end();
    })
Should I connect to mysql everytime '/router1/' is requested, like in this example, or it's better to leave one connection open one at start up? As: connection.connect(); outside of:  router.get('/',function(req,res){
...
}); ?
",<mysql><node.js><database>,842,0,0,277,1,4,8,48,17538,,76,4,16,2018-03-14 19:59,2019-11-03 19:31,,599,,Intermediate,22
48974135,`Error: file is not a database` in SQLite,"I have created a database named database.db.
When I create a table in the database, I get the error Error: file is not a database as shown below:
nehal@nehal-Inspiron-5559:~/Desktop/UAV$ sqlite3 database.db 
SQLite version 3.20.1 2017-08-24 16:21:36
Enter &quot;.help&quot; for usage hints.
sqlite&gt; CREATE TABLE users(
   ...&gt; password varchar(10),
   ...&gt; email text,
   ...&gt; name text
   ...&gt; );
Error: file is not a database
How do I resolve the error?
",<database><sqlite>,471,0,11,407,2,5,11,54,48664,0,0,5,16,2018-02-25 13:44,2018-02-25 16:58,2018-02-25 16:58,0,0,Basic,9
56319638,EntityFrameworkCore SQLite in-memory db tables are not created,"For integration tests I am using an EntityFrameworkCore SQLite in-memory db and creating its schema as per Microsoft docs, but when I attempt to seed data an exception is thrown that tables do not exist.
The mouse-over docs for DbContext.Database.EnsureCreated(); :
  Ensure that the database for the context exists.  If it exists, no
  action is taken.  If it does not exist then the database and all its
  schema are created.  If the database exists, then no action is made to
  ensure it is compatible with the model for this context.
I've read that an EntityFrameworkCore in-memory db only exists as long as an open connection exists, and so I tried explicitly creating a var connection = new SqliteConnection(""DataSource=:memory:""); instance and wrapping the below code in a using(connection) {} block and passing the connection instance options.UseSqlite(connection);, but DbContext.Database.EnsureCreated(); still doesn't create any db-objects
public class CustomWebApplicationFactory&lt;TStartup&gt; : WebApplicationFactory&lt;Startup&gt;
{
    protected override IWebHostBuilder CreateWebHostBuilder()
    {
        return WebHost.CreateDefaultBuilder()
            .UseStartup&lt;Startup&gt;();
    }
    protected override void ConfigureWebHost(IWebHostBuilder builder)
    {
      using (var connection = new SqliteConnection(""DataSource=MySharedInMemoryDb;mode=memory;cache=shared""))
      {
          connection.Open();
          builder.ConfigureServices(services =&gt;
          {
              var serviceProvider = new ServiceCollection()
                  .AddEntityFrameworkSqlite()
                  .BuildServiceProvider();
              services.AddDbContext&lt;MyDbContext&gt;(options =&gt;
              {
                  options.UseSqlite(connection);
                  options.UseInternalServiceProvider(serviceProvider);
              });
              var contextServiceProvider = services.BuildServiceProvider();
              // we need a scope to obtain a reference to the database contexts
              using (var scope = contextServiceProvider.CreateScope())
              {
                  var scopedProvider = scope.ServiceProvider;
                  var logger = scopedProvider.GetRequiredService&lt;ILogger&lt;CustomWebApplicationFactory&lt;TStartup&gt;&gt;&gt;();
                  using (var myDb = scopedProvider.GetRequiredService&lt;MyDbContext&gt;())
                  {
                      // DEBUG CODE
                      // this returns script to create db objects as expected
                      // proving that MyDbContext is setup correctly
                      var script = myDb.Database.GenerateCreateScript();
                      // DEBUG CODE
                      // this does not create the db objects ( tables etc )
                      // this is not as expected and contrary to ms docs
                      var result = myDb.Database.EnsureCreated();
                      try
                      {
                          SeedData.PopulateTestData(myDb);
                      }
                      catch (Exception e)
                      {
                          // exception is thrown that tables don't exist
                          logger.LogError(e, $""SeedData.PopulateTestData(myDb) threw exception=[{e.Message}]"");
                      }
                  }
              }
          });
        }
        builder.UseContentRoot(""."");
        base.ConfigureWebHost(builder);
    }
Please note that in this post I'm only asking the question why doesn't DbContext.Database.EnsureCreated(); create the schema as expected. I'm not presenting the above code as a general pattern for running integration-tests.
",<c#><sqlite><integration-testing><in-memory-database><entity-framework-core-2.2>,3702,0,71,8295,17,61,91,80,15929,0,96,1,16,2019-05-27 4:20,2019-05-29 20:07,,2,,Advanced,32
54325397,How to connect MySQL database to ReactJS app?,"I have build a Todo App with create-react-app. The store I'm using is based on Local Storage(JS attribute of object window). Now I created a MySQL databases and want to connect to that database, so the state will show the values from database, and will be updated through actions. 
I've tried to connect to db and output values through 'node' console using db.js. It works.
const mysql = require('mysql');
const con = mysql.createConnection({
  host: ""localhost"",
  user: ""root"",
  password: ""root"",
  database: 'root'
});
con.connect(function(err) {
  if (err) throw err;
  con.query(""SELECT * FROM tasks"", function (err, result, fields) {
    if (err) throw err;
    console.log(result);
  });
});
Is it possible to connect the state of app to database using this script?
",<javascript><mysql><reactjs>,774,0,16,175,1,1,9,76,98021,0,3,2,16,2019-01-23 10:43,2019-01-23 10:45,2019-01-23 10:45,0,0,Basic,3
50477550,Lookup against MYSQL TEXT type column,"My table/model has TEXT type column, and when filtering for the records on the model itself, the AR where produces the correct SQL and returns correct results, here is what I mean :
MyNamespace::MyValue.where(value: 'Good Quality')
Produces this SQL :
SELECT `my_namespace_my_values`.* 
FROM `my_namespace_my_values` 
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Take another example where I m joining MyNamespace::MyValue and filtering on the same value column but from the other model (has relation on the model to my_values). See this (query #2) :
OtherModel.joins(:my_values).where(my_values: { value: 'Good Quality' })
This does not produce correct query, this filters on the value column as if it was a String column and not Text, therefore producing incorrect results like so (only pasting relevant where) :
WHERE my_namespace_my_values`.`value` = 'Good Quality'
Now I can get past this by doing LIKE inside my AR where, which will produce the correct result but slightly different query. This is what I mean :
OtherModel.joins(:my_values).where('my_values.value LIKE ?, '%Good Quality%')
Finally arriving to my questions. What is this and how it's being generated for where on the model (for text column type)?
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
Maybe most important question what is the difference in terms of performance using :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
and this :
(my_namespace_my_values.value LIKE '%Good Quality%')
and more importantly how do I get my query with joins (query #2) produce where like this :
WHERE `my_namespace_my_values`.`value` = '\\\""Good Quality\\\""'
",<mysql><ruby-on-rails><activerecord>,1670,0,18,22672,36,134,182,63,578,0,1642,4,16,2018-05-22 22:58,2018-05-25 13:44,2018-05-30 14:54,3,8,Basic,2
52109026,Firestore schema versioning and backward compatibility with android app to prevent crashes,"Firestore a NOSQL is a Document oriented database. Now how to manage versioning of the data as I use it with Firebase SDK and Android applications?
For e.g. let's say I have a JSON schema that I launch with my 1.0.0version of my android app. Later 1.0.1 comes up where I have to add some extra fields for the newer documents. Since I changed the structure to have additional information, it only applies to new documents.
Therefore, using this logic, I can see my Android application must be able to deal with all versions of JSON tree if used against this project I create in the firebase console with Firestore. But this can be very painful right, that I have carry the deadweight of backward compatibility endlessly? Is there a way to have some sort of version like in protobuf or something the android app can send to firestore server side so that automatically we can do something to prevent crashes on the android app when it sees new fields? 
See also this thread, the kind of problem the engineer has posted. You can end up with this kind of problem as new fields get discovered in your JSON tree by the android app
Add new field or change the structure on all Firestore documents
Any suggestions for how we should go about this?
In node.js architecture we handle this with default-> v1.1/update or 
default-> v1.0/update, that way we can manage the routes.
But for android+firebase SKD-> talking to Firestore NOSQL, how do I manage the versioning of the json schema. 
",<android><firebase><nosql><google-cloud-firestore><versioning>,1477,1,0,597,0,7,18,43,1602,0,0,1,16,2018-08-31 5:35,2020-04-08 9:21,,586,,Advanced,32
54332942,spark windowing function VS group by performance issue,"I have a dataframe done like
| id | date      |  KPI_1 | ... | KPI_n
| 1  |2012-12-12 |   0.1  | ... |  0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46
| 4  |2012-12-14 |   0.2  | ... |  0.45 
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
we have 
X identifiers
a row for every identifier for a given date
n KPIs
I have to calculate some derived KPI for every row, and this KPI depends on the previous values of every ID.
Let's say my derived KPI is a diff, it would be:
| id | date      |  KPI_1 | ... | KPI_n | KPI_1_diff | KPI_n_diff
| 1  |2012-12-12 |   0.1  | ... |  0.5  |   0.1      | 0.5
| 2  |2012-12-12 |   0.2  | ... |  0.4  |   0.2      |0.4
| 3  |2012-12-12 |   0.66 | ... |  0.66 |   0.66     | 0.66 
| 1  |2012-12-13 |   0.2  | ... |  0.46 |   0.2-0.1  | 0.46 - 0.66
| 4  |2012-12-13 |   0.2  | ... |  0.45  ...
| ...
| 55| 2013-03-15 |  0.5  | ... |  0.55
Now: what I would do is:
val groupedDF = myDF.groupBy(""id"").agg(
    collect_list(struct(col(""date"",col(""KPI_1""))).as(""wrapped_KPI_1""),
    collect_list(struct(col(""date"",col(""KPI_2""))).as(""wrapped_KPI_2"")
    // up until nth KPI
)
I would get aggregated data such as:
[(""2012-12-12"",0.1),(""2012-12-12"",0.2) ...
Then I would sort these wrapped data, unwrap and map over these aggregated result with some UDF and produce the output (compute diffs and other statistics).
Another approach is to use the window functions such as:
val window = Window.partitionBy(col(""id"")).orderBy(col(""date"")).rowsBetween(Window.unboundedPreceding,0L) 
and do :
val windowedDF = df.select (
  col(""id""),
  col(""date""),
  col(""KPI_1""),
  collect_list(struct(col(""date""),col(""KPI_1""))).over(window),  
  collect_list(struct(col(""date""),col(""KPI_2""))).over(window)
   )   
This way I get:
[(""2012-12-12"",0.1)]
[(""2012-12-12"",0.1), (""2012-12-13"",0.1)]
...
That look nicer to process, but I suspect that repeating the window would produce unnecessary grouping and sorting for every KPI.
So here are the questions:
I'd rather go for the grouping approach?
Would I go for the window? If so what is the most efficient approach to do it?
",<apache-spark><apache-spark-sql>,2156,0,33,851,1,10,17,38,21302,0,348,2,16,2019-01-23 17:49,2019-01-24 7:28,2019-01-24 7:28,1,1,Intermediate,23
54301624,[ERR_HTTP_HEADERS_SENT]: Cannot set headers after they are sent to the client,"I'm working with PostgreSQL and NodeJS with its ""PG Module"". 
CRUD works but sometimes doesn't update automatically the views when i save or delete some item. this is my code and I think that the error is here but i cannot find it, i tried everything :'( 
Error Message:
const controller = {};
const { Pool } = require('pg');
var connectionString = 'postgres://me:system@localhost/recipebookdb';
const pool = new Pool({
    connectionString: connectionString,
})
controller.list = (request, response) =&gt; {
    pool.query('SELECT * FROM recipes', (err, result) =&gt; {
        if (err) {
            return next(err);
        }
           return response.render('recipes', { data: result.rows });
    });
};
controller.save = (req, res) =&gt; {
    pool.query('INSERT INTO recipes(name, ingredients, directions) VALUES ($1, $2, $3)',
        [req.body.name, req.body.ingredients, req.body.directions]);
    return res.redirect('/');
};
controller.delete = (req, res) =&gt; {
    pool.query('DELETE FROM RECIPES WHERE ID = $1', [req.params.id]);
    return res.redirect('/');
}
module.exports = controller;
PD: CRUD works but sometimes appears that error.
",<javascript><node.js><postgresql>,1157,1,29,256,1,3,16,63,71152,0,40,3,16,2019-01-22 5:13,2019-01-22 5:24,2019-01-22 5:52,0,0,Basic,13
53934294,How to count null values in postgresql?,"select distinct ""column"" from table;
output:
    column
1     0.0
2     [null]
3     1.0
But when I try to count the null values
select count(""column"") from train where ""column"" is NULL;
Gives output 0 (zero)
Can you suggest where it's going wrong?
",<sql><postgresql><count><null>,249,0,6,7771,25,82,130,53,25494,0,121,6,16,2018-12-26 16:01,2018-12-26 16:03,2018-12-26 16:03,0,0,Basic,9
56977456,varchar is missing on pgadmin4 UI,"I am very new on Postgresql. I have installed pgadmin4 on my Mac OS X. I try to create a very basic table with a column which has type varchar but it doesn't seem to have ti on the list of the UI.
Am I missing something?
",<postgresql><macos><pgadmin>,221,1,1,2309,7,31,63,81,14545,0,3517,1,16,2019-07-10 19:31,2019-07-16 14:55,2019-07-16 14:55,6,6,Intermediate,15
53526505,Granting Full SQL Server Permissions for a Database,"How can I give a user (who was created WITHOUT LOGIN) full control over a contained database without having to specify that database's name, like GRANT CONTROL ON DATABASE::DatabaseName TO UserName, but without using a database name?  I figure it'll include GRANT ALTER ANY SCHEMA TO UserName, but I'm not sure what else I'd need to grant, or if there's a better way.  Thanks.
",<sql-server><permissions><roles>,377,0,0,657,1,6,17,54,38229,0,13,2,16,2018-11-28 19:10,2018-11-28 19:35,2018-11-28 19:35,0,0,Basic,5
56897416,HikariPool-1 - Exception during pool initialization,"I'm deploying Spring-Boot4 project and I have got an error called HikariPool-1 - Exception during pool initialization.
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.3.10.Final}
2019-07-05 11:29:43.600  INFO 5084 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
2019-07-05 11:29:43.792  INFO 5084 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.4.Final}
2019-07-05 11:29:43.877  INFO 5084 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2019-07-05 11:29:44.964 ERROR 5084 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Exception during pool initialization.
org.postgresql.util.PSQLException: FATAL: database ""test2"" does not exist
    at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2559) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.QueryExecutorImpl.&lt;init&gt;(QueryExecutorImpl.java:133) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:250) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.jdbc.PgConnection.&lt;init&gt;(PgConnection.java:195) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.makeConnection(Driver.java:454) ~[postgresql-42.2.5.jar:42.2.5]
    at org.postgresql.Driver.connect(Driver.java:256) ~[postgresql-42.2.5.jar:42.2.5]
    at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198) ~[HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:541) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.pool.HikariPool.&lt;init&gt;(HikariPool.java:115) [HikariCP-3.2.0.jar:na]
    at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:112) [HikariCP-3.2.0.jar:na]
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:68) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) [hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) ~[classes/:na]
2019-07-05 11:29:44.980  WARN 5084 --- [           main] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : FATAL: database ""test2"" does not exist
2019-07-05 11:29:44.980  WARN 5084 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
2019-07-05 11:29:44.980  INFO 5084 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2019-07-05 11:29:45.011  INFO 5084 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2019-07-05 11:29:45.027 ERROR 5084 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:593) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1105) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE]
    at com.example.test6.Test6Application.main(Test6Application.java:10) [classes/:na]
Caused by: org.hibernate.service.spi.ServiceException: Unable to create requested service [org.hibernate.engine.jdbc.env.spi.JdbcEnvironment]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:275) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:237) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.id.factory.internal.DefaultIdentifierGeneratorFactory.injectServices(DefaultIdentifierGeneratorFactory.java:152) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.injectDependencies(AbstractServiceRegistryImpl.java:286) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:243) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:214) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.internal.InFlightMetadataCollectorImpl.&lt;init&gt;(InFlightMetadataCollectorImpl.java:179) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:119) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:904) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:935) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) ~[spring-orm-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1837) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1774) ~[spring-beans-5.1.8.RELEASE.jar:5.1.8.RELEASE]
    ... 16 common frames omitted
Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.determineDialect(DialectFactoryImpl.java:100) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl.buildDialect(DialectFactoryImpl.java:54) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:137) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:94) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:263) ~[hibernate-core-5.3.10.Final.jar:5.3.10.Final]
    ... 33 common frames omitted
Process finished with exit code 1
I can't understand what does it means. I have already searched for the error in StackOverflow. There are some questions about HikariPool but none of them are working for me.
My application.properties is as follows,
spring.datasource.url=jdbc:postgresql://localhost/test2
spring.datasource.username=postgres
spring.datasource.password=root
spring.jpa.generate-ddl=true
My pom.xml dependancies are,
&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
            &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
",<spring><postgresql><spring-boot><hikaricp>,19330,0,147,3977,11,43,64,78,127318,0,2342,6,16,2019-07-05 6:12,2019-07-05 6:29,2019-07-05 13:50,0,0,Basic,9
49095292,Exclude primary key attributes from a sequelize query,"I have a sequelize query from multiple tables inner joined together. I need to group them by on a nested include model but the sequelize query throws the primary key every time, even if I mention the attributes as: attributes:[].
However attributes:[] is working for nested include models.
",<mysql><node.js><sequelize.js>,290,0,4,195,1,1,10,69,33371,0,26,4,16,2018-03-04 12:16,2018-03-04 18:12,,0,,Basic,10
58206572,AWS RDS Mysql kill transaction by ID (without thread_id),"I'm currently having an issue with some long running transactions that are holding locks to a single row in a table that I'm being unable to kill.
Innodb_trx holds the following information - special attention to thread_id = null;
select * from information_schema.innodb_trx\G
*************************** 2. row ***************************
                    trx_id: 153261728
                 trx_state: RUNNING
               trx_started: 2019-10-02 10:05:42
     trx_requested_lock_id: NULL
          trx_wait_started: NULL
                trx_weight: 5
       trx_mysql_thread_id: 0
                 trx_query: NULL
       trx_operation_state: NULL
         trx_tables_in_use: 0
         trx_tables_locked: 2
          trx_lock_structs: 3
     trx_lock_memory_bytes: 1136
           trx_rows_locked: 1
         trx_rows_modified: 2
   trx_concurrency_tickets: 0
       trx_isolation_level: READ COMMITTED
         trx_unique_checks: 1
    trx_foreign_key_checks: 1
trx_last_foreign_key_error: NULL
 trx_adaptive_hash_latched: 0
 trx_adaptive_hash_timeout: 0
          trx_is_read_only: 0
trx_autocommit_non_locking: 0
and
show engine innodb status\G;
------------------
---TRANSACTION 153261728, ACTIVE (PREPARED) 27716 sec recovered trx
3 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 2
---TRANSACTION 96697370, ACTIVE (PREPARED) 988082 sec recovered trx
0 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 3
Is there as way to kill/stop/rollback a transaction like this?
",<mysql><amazon-rds>,1510,0,32,168,0,0,6,49,1641,0,1,1,16,2019-10-02 17:39,2022-10-07 1:53,,1101,,Basic,10
52647648,Remove last n chars of a varchar column,"I wanted to know if there's a ""compact"" way of deleting last n chars from a column in PostGreSQL.
I have a column born as a DATE formatted like this: yyyy-MM-dd.
I altered the table to make this column a varchar, replaced all of the unnecessary dashes but I can't think about a quick and reliable way to delete the last two chars since I'd like the varchar to show only yyyyMM.
With ""quick and reliable"" I mean something that won't ask me to use a temp table.
Thanks in advance
",<postgresql>,478,0,0,665,4,10,33,53,14820,0,20,2,16,2018-10-04 13:12,2018-10-04 13:48,2018-10-04 13:48,0,0,Basic,2
54844651,I dont know how Postgresql created user on my mac,"Two days ago, i started learning postgresql. Most tutorials I followed online were either old or the codes wont just work on my mac. I followed a lot of tutorials that did a lot of totally different things.
When i switched on my system today. I noticed Postgresql created a user on my mac. I don't know what this is or maybe i used the wrong CLI code.
When I tried viewing the user, I saw this
should I delete this user or it has a function?
",<postgresql><macos><terminal>,442,2,0,423,2,5,13,40,20634,0,11,2,16,2019-02-23 18:20,2019-02-23 20:04,2019-02-23 20:04,0,0,Basic,14
50093776,"SQLITE, Create a temp table then select from it","just wondering how i can create a temp table and then select from it further down the script.
Example. 
CREATE TEMPORARY TABLE TEMP_TABLE1 AS 
Select 
L.ID,
SUM(L.cost)/2 as Costs,
from Table1 L
JOIN Table2 C on L.ID = C.ID
Where C.name  = 'mike'
Group by L.ID
Select 
Count(L.ID)
from Table1 L
JOIN TEMP_TABLE1 TT1 on L.ID = TT1.ID;
Where L.ID not in (TT1) 
And Sum(L.Cost) &gt; TT1.Costs
Ideally I want to have a temp table then use it later in the script to reference from.
Any help would be great!
",<sqlite>,502,0,17,181,1,1,6,42,52575,,1,2,16,2018-04-30 4:03,2018-04-30 4:58,,0,,Basic,10
53057786,"Woocommerce mySQL Query - List All Orders, Users and Purchased Items","I have a fully working mySQL query which pulls all of the orders, users, addresses and items purchased from Woocommerce, however it only lists the products individually, and I would like to add the quantity for each product displayed.
Currently shows 'Items Ordered'
Running Shoes
Walking Shoes
Where it should show 'Items Ordered'
3 x Running Shoes
4 x Walking Shoes
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
I believe Woocommerce stores the QTY in the following table / entries:
wp_woocommerce_order_itemmeta
  order_item_id
    SELECT wp_woocommerce_order_itemmeta.meta_value
    WHERE wp_woocommerce_order_itemmeta.meta_value = '_qty' and wp_woocommerce_order_itemmeta.order_item_id =
And I need to join it in somehow into this section:
( select group_concat( order_item_name separator '&lt;/p&gt;' ) FROM wp_woocommerce_order_items where order_id = p.ID ) AS 'Items Ordered'
Thanks in advance.
UPDATED WITH ANSWER FROM: Lucek
Thanks Lucek, absolutely perfect.
I've combined the complete query in case anyone else wants to copy it.
SELECT
  p.ID AS 'Order ID',
  p.post_date AS 'Purchase Date',
  MAX( CASE WHEN pm.meta_key = '_billing_email'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Email Address',
  MAX( CASE WHEN pm.meta_key = '_billing_first_name'  AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'First Name',
  MAX( CASE WHEN pm.meta_key = '_billing_last_name'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Last Name',
  MAX( CASE WHEN pm.meta_key = '_billing_address_1'   AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Address',
  MAX( CASE WHEN pm.meta_key = '_billing_city'        AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'City',
  MAX( CASE WHEN pm.meta_key = '_billing_state'       AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'State',
  MAX( CASE WHEN pm.meta_key = '_billing_postcode'    AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Post Code',
    CASE p.post_status
      WHEN 'wc-pending'    THEN 'Pending Payment'
      WHEN 'wc-processing' THEN 'Processing'
      WHEN 'wc-on-hold'    THEN 'On Hold'
      WHEN 'wc-completed'  THEN 'Completed'
      WHEN 'wc-cancelled'  THEN 'Cancelled'
      WHEN 'wc-refunded'   THEN 'Refunded'
      WHEN 'wc-failed'     THEN 'Failed'
    ELSE 'Unknown'
    END AS 'Purchase Status',
  MAX( CASE WHEN pm.meta_key = '_order_total'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Order Total',
  MAX( CASE WHEN pm.meta_key = '_paid_date'           AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'Paid Date',
  ( SELECT GROUP_CONCAT(CONCAT(m.meta_value, ' x ', i.order_item_name) separator '&lt;/br&gt;' )
    FROM wp_woocommerce_order_items i
    JOIN wp_woocommerce_order_itemmeta m ON i.order_item_id = m.order_item_id AND meta_key = '_qty'
    WHERE i.order_id = p.ID AND i.order_item_type = 'line_item') AS 'Items Ordered',
  MAX( CASE WHEN pm.meta_key = 'post_excerpt'         AND p.ID = pm.post_id THEN pm.meta_value END ) AS 'User Comments'
FROM  wp_posts AS p
JOIN  wp_postmeta AS pm ON p.ID = pm.post_id
JOIN  wp_woocommerce_order_items AS oi ON p.ID = oi.order_id
WHERE post_type = 'shop_order'
GROUP BY p.ID
",<mysql><wordpress><woocommerce><product>,4903,0,65,173,0,1,9,46,9943,0,0,1,16,2018-10-30 5:09,2018-10-31 8:27,2018-10-31 8:27,1,1,Basic,10
48102013,SQL Server INSERT INTO with WHERE clause,"I'm trying to insert some mock payment info into a dev database with this query:
INSERT
    INTO
        Payments(Amount)
    VALUES(12.33)
WHERE
    Payments.CustomerID = '145300';
How can adjust this to execute? I also tried something like this: 
IF NOT EXISTS(
    SELECT
        1
    FROM
        Payments
    WHERE
        Payments.CustomerID = '145300' 
) INSERT 
    INTO
        Payments(Amount)
    VALUES(12.33);
",<sql><sql-server><database><t-sql><insert>,424,0,17,1406,4,21,57,44,100145,0,208,8,16,2018-01-04 19:13,2018-01-04 19:18,2018-01-04 19:19,0,0,Basic,3
50607120,CosmosDb count distinct elements,"Is there a direct function to count distinct elements in a CosmosDb query?
This is the default count:
SELECT value count(c.id) FROM c
And distinct works without count:
SELECT distinct c.id FROM c
But this returns a Bad Request - Syntax Error:
  SELECT value count(distinct c.id) FROM c
How would count and distinct work together?
",<sql><azure><azure-cosmosdb>,330,0,5,2660,7,35,51,58,32338,0,126,7,16,2018-05-30 14:21,2018-05-31 9:08,2019-01-04 0:32,1,219,Basic,1
55406735,Can a Postgres Commit Exist in Procedure that has an Exception Block?,"I am having a difficult time understanding transactions in Postgres. I have a procedure that may encounter an exception. There are parts of the procedure where I might want to commit my work so-far so that it won't be rolled back if an exceptions ensues.
I want to have an exception handling block at the end of the procedure where I catch the exception and insert the information from the exception into a logging table.
I have boiled the problem down to a simple procedure, below, which fails on PostgreSQL 11.2 with
2D000 cannot commit while a subtransaction is active
PL/pgSQL function x_transaction_try() line 6 at COMMIT
    drop procedure if exists x_transaction_try;
    create or replace procedure x_transaction_try()
        language plpgsql
    as $$
    declare
    begin
         raise notice 'A';
         -- TODO A: do some insert or update that I want to commit no matter what
         commit;
         raise notice 'B';
         -- TODO B: do something else that might raise an exception, without rolling
         -- back the work that we did in ""TODO A"".
    exception when others then
      declare
        my_ex_state text;
        my_ex_message text;
        my_ex_detail text;
        my_ex_hint text;
        my_ex_ctx text;
      begin
          raise notice 'C';
          GET STACKED DIAGNOSTICS
            my_ex_state   = RETURNED_SQLSTATE,
            my_ex_message = MESSAGE_TEXT,
            my_ex_detail  = PG_EXCEPTION_DETAIL,
            my_ex_hint    = PG_EXCEPTION_HINT,
            my_ex_ctx     = PG_EXCEPTION_CONTEXT
          ;
          raise notice '% % % % %', my_ex_state, my_ex_message, my_ex_detail, my_ex_hint, my_ex_ctx;
          -- TODO C: insert this exception information in a logging table and commit
      end;
    end;
    $$;
    call x_transaction_try();
Why doesn't this stored procedure work? Why is it that we never see the output of raise notice 'B' and instead we go into the exception block? Is it possible to do what I have described above with a Postgres 11 stored procedure?
Edit: This is a complete code sample. Paste the above complete code sample (including both the create procedure and call statements) into a sql file and run it in a Postgres 11.2 database to repro. The desired output would be for the function to print A then B, but instead it prints A then C along with the exception information.
Also notice that if you comment out all of the exception handling block such that the function does not catch exceptions at all, then the function will output 'A' then 'B' without an exception occurring. This is why I titled the question the way that I did 'Can a Postgres Commit Exist in Procedure that has an Exception Block?'
",<postgresql><stored-procedures><plpgsql><postgresql-11>,2701,0,44,2155,2,29,60,71,16087,0,579,2,16,2019-03-28 20:56,2019-03-29 8:18,2019-03-29 19:46,1,1,Advanced,32
58936116,PyCharm warns about unexpected arguments for SQLAlchemy User model,"I'm working with Flask-SQLAlchemy in PyCharm. When I try to create instances of my User model by passing keyword arguments to the model, PyCharm highlights the arguments with an ""Unexpected argument(s)"" warning. When I create instances of other models, I don't get this warning. Why am I getting this error for my User model?
class User(db.Model, UserMixin):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String, unique=True, nullable=False)
new_user = User(username=""test"")
In the above example username=""test"" is highlighted as a warning.
",<python><sqlalchemy><pycharm><flask-login>,572,1,8,355,0,4,11,81,4193,0,16,3,16,2019-11-19 14:17,2019-11-19 15:39,2019-11-19 15:39,0,0,Basic,6
59089991,Ruby 2.6.5 and PostgreSQL pg-gem segmentation fault,"From the console I cannot do any operation that touches the database. I get a Segmentation fault.
.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56: [BUG] Segmentation fault at 0x0000000000000110
ruby 2.6.5p114 (2019-10-01 revision 67812) [x86_64-darwin18]
It is literally any operation that might need the database, including MyModel.new.
-- Control frame information -----------------------------------------------
c:0071 p:---- s:0406 e:000405 CFUNC  :initialize
c:0070 p:---- s:0403 e:000402 CFUNC  :new
c:0069 p:0016 s:0398 e:000397 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/pg-1.1.4/lib/pg.rb:56
c:0068 p:0107 s:0393 e:000392 METHOD /Users/xxx/.rbenv/versions/2.6.5/lib/ruby/gems/2.6.0/gems/activerecord-6.0.1/lib/active_record/connection_adapters/postgres
I have uninstalled and reinstalled the pg gem. And rebuilt the database. And restarted PostgreSQL.
I have seen other people reporting the problem when running under Puma, but my configuration works under Puma, fails under console!
Edit for clarity:
Yes, using bundler.
Starting the rails console either with rails c or bundle exec rails c has the same effect (segfault) with same stack trace.
Gemfile.lock has pg (1.1.4)
I re-bundled, specifying a bundle path. The stack trace now has that bundle path, so I guess by default bundler was using the rbenv path.
",<ruby><postgresql><ruby-on-rails-6>,1364,0,10,4718,4,34,52,66,7933,0,103,2,16,2019-11-28 13:28,2019-12-04 10:42,2019-12-04 10:42,6,6,Advanced,32
65035103,Sqldelight database schema not generated,"I have a KMM project and want to use SqlDelight library, but when I build the project  database schema not generated and table entities also.
actual class DatabaseDriverFactory(private val context: Context) {
    actual fun createDriver(): SqlDriver {
               //Unresolved reference: CoreDb
        return AndroidSqliteDriver(CoreDb.Schema, context, &quot;test.db&quot;)
    }
}
I have defined sqldelight folder inside my shared module and also created folder for feature generated kotlin classes as it is configured in gradle.build.kts and also have one *.sq file inside sqldelight folder
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
When I run generateSqlDelightInterface task I just see those log
&gt; Task :core:generateAndroidDebugCoreDbInterface NO-SOURCE
&gt; Task :core:generateAndroidReleaseCoreDbInterface NO-SOURCE
&gt; Task :core:generateIosMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataCommonMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateMetadataMainCoreDbInterface NO-SOURCE
&gt; Task :core:generateSqlDelightInterface UP-TO-DATE
can't register checkAndroidModules
BUILD SUCCESSFUL in 311ms
1:40:36 PM: Task execution finished 'generateSqlDelightInterface'.
Here is my full build.gradle.kts
import org.jetbrains.kotlin.gradle.plugin.mpp.KotlinNativeTarget
plugins {
  kotlin(&quot;multiplatform&quot;)
  kotlin(&quot;plugin.serialization&quot;)
  id(&quot;com.android.library&quot;)
  id(&quot;kotlin-android-extensions&quot;)
  id(&quot;koin&quot;)
  id(&quot;com.squareup.sqldelight&quot;)
}
repositories {
  gradlePluginPortal()
  google()
  jcenter()
  mavenCentral()
  maven {
    url = uri(&quot;https://dl.bintray.com/kotlin/kotlin-eap&quot;)
  }
  maven {
    url = uri(&quot;https://dl.bintray.com/ekito/koin&quot;)
  }
}
kotlin {
  android()
  val iOSTarget: (String, KotlinNativeTarget.() -&gt; Unit) -&gt; KotlinNativeTarget =
    if (System.getenv(&quot;SDK_NAME&quot;)?.startsWith(&quot;iphoneos&quot;) == true)
      ::iosArm64
    else
      ::iosX64
  iOSTarget(&quot;ios&quot;) {
    binaries {
      framework {
        baseName = &quot;core&quot;
      }
    }
  }
  val coroutinesVersion = &quot;1.3.9-native-mt&quot;
  val ktor_version = &quot;1.4.2&quot;
  val serializationVersion = &quot;1.0.0-RC&quot;
  val koin_version = &quot;3.0.0-alpha-4&quot;
  val sqlDelight = &quot;1.4.4&quot;
  sourceSets {
    val commonMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-core:$ktor_version&quot;)
        implementation(&quot;io.ktor:ktor-client-serialization:$ktor_version&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$coroutinesVersion&quot;)
        implementation(&quot;org.jetbrains.kotlinx:kotlinx-serialization-core:$serializationVersion&quot;)
        implementation(&quot;com.squareup.sqldelight:runtime:$sqlDelight&quot;)
        // SqlDelight extension
        implementation(&quot;com.squareup.sqldelight:coroutines-extensions:$sqlDelight&quot;)
        // Koin for Kotlin
        implementation(&quot;org.koin:koin-core:$koin_version&quot;)
        //shared preferences
        implementation(&quot;com.russhwolf:multiplatform-settings:0.6.3&quot;)
      }
    }
    val commonTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-common&quot;))
        implementation(kotlin(&quot;test-annotations-common&quot;))
      }
    }
    val androidMain by getting {
      dependencies {
        implementation(&quot;androidx.core:core-ktx:1.3.2&quot;)
        implementation(&quot;io.ktor:ktor-client-android:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:android-driver:$sqlDelight&quot;)
      }
    }
    val androidTest by getting {
      dependencies {
        implementation(kotlin(&quot;test-junit&quot;))
        implementation(&quot;junit:junit:4.12&quot;)
      }
    }
    val iosMain by getting {
      dependencies {
        implementation(&quot;io.ktor:ktor-client-ios:$ktor_version&quot;)
        implementation(&quot;com.squareup.sqldelight:native-driver:$sqlDelight&quot;)
      }
    }
    val iosTest by getting
  }
}
android {
  compileSdkVersion(29)
  sourceSets[&quot;main&quot;].manifest.srcFile(&quot;src/androidMain/AndroidManifest.xml&quot;)
  defaultConfig {
    minSdkVersion(23)
    targetSdkVersion(29)
    versionCode = 1
    versionName = &quot;1.0&quot;
  }
  buildTypes {
    getByName(&quot;release&quot;) {
      isMinifyEnabled = false
    }
  }
}
val packForXcode by tasks.creating(Sync::class) {
  group = &quot;build&quot;
  val mode = System.getenv(&quot;CONFIGURATION&quot;) ?: &quot;DEBUG&quot;
  val sdkName = System.getenv(&quot;SDK_NAME&quot;) ?: &quot;iphonesimulator&quot;
  val targetName = &quot;ios&quot; + if (sdkName.startsWith(&quot;iphoneos&quot;)) &quot;Arm64&quot; else &quot;X64&quot;
  val framework =
    kotlin.targets.getByName&lt;KotlinNativeTarget&gt;(&quot;ios&quot;).binaries.getFramework(mode)
  inputs.property(&quot;mode&quot;, mode)
  dependsOn(framework.linkTask)
  val targetDir = File(buildDir, &quot;xcode-frameworks&quot;)
  from({ framework.outputDirectory })
  into(targetDir)
}
tasks.getByName(&quot;build&quot;).dependsOn(packForXcode)
sqldelight {
  database(&quot;CoreDb&quot;) {
    packageName = &quot;com.example.app.core.database&quot;
    sourceFolders = listOf(&quot;sqldelight&quot;)
    dialect = &quot;sqlite:3.24&quot;
  }
}
and for top level build.gradle
classpath &quot;com.squareup.sqldelight:gradle-plugin:$sqlDelight&quot;
Update
My project folder structure
root
  app
    src
      ...
  core //(kmm shared module)
    androidMain
        com.example.app.core
            database
    commonMain
        com.example.app.core
            database
            repository
            ...
            sqldelight
    iosMain
        com.example.app.core
            database
",<android><gradle><kotlin-multiplatform><sqldelight>,6024,2,185,5287,7,38,64,77,6964,0,751,5,16,2020-11-27 9:45,2020-11-27 11:16,2020-11-27 11:16,0,0,Intermediate,23
48685606,Import-Module : The specified module 'SqlServer' was not loaded because no valid module file was found in any module directory,"Well, hello there!
I'm working on a Script to get the Sql Job History and need to use the ""SqlServer"" Module. It's installed but I can't import it due to the Error Message above. When I got to the Modules Path, the Folder ""SqlServer"" exists and isn't empty. Don't get the problem here.
Thanks in advance!
",<sql-server><powershell><module>,305,0,0,169,1,1,4,45,26433,0,0,2,16,2018-02-08 12:31,2019-07-18 19:17,,525,,Basic,14
50493398,"MySQL connector error ""The server time zone value Central European Time""","My problem
MySQL connector ""The server time zone value Central European Time"" is unrecognized or represents more than one time zone.
The project
Small web Project with:
JavaEE, Tomcat 8.5, MySQL, Maven
My attempt
Maven -> change MySQL-connector form 6.x to 5.1.39 (no change)
Edit context.xml URL change
Connection in context.xml
URL=""jdbc: mysql://127.0.0.1:3306/rk_tu_lager?useLegacyDatetimeCode=false;serverTimezone=CEST;useSSL=false;
Error:
  Caused by:
  com.mysql.cj.core.exceptions.InvalidConnectionAttributeException:  The
  server time zone value 'Mitteleurop?ische Sommerzeit' is unrecognized
  or  represents more than one time zone. You must configure either the
  server or JDBC driver (via the serverTimezone configuration property)
  to use a more specifc time zone value if you want to utilize time zone
  support.
",<java><mysql><jdbc>,831,0,0,574,1,4,15,61,43737,0,195,4,16,2018-05-23 16:35,2018-05-24 15:01,2018-05-24 15:01,1,1,Basic,14
49425985,Inserting NULL as default in SQLAlchemy?,"I have the following column in SQLAlchemy:
name = Column(String(32), nullable=False)
I want that if no value is passed onto an insertion, it should enter a default value of either completely blank or if not possible, then default as NULL.
Should I define the column as :
name = Column(String(32), nullable=False, default=NULL)
Because in python NULL object is actually None
Any suggestions?
",<python><sqlalchemy><models>,391,0,4,1795,4,25,49,49,21170,0,84,2,16,2018-03-22 10:16,2018-03-22 13:53,,0,,Basic,14
50783515,Pyspark dataframe OrderBy list of columns,"I am trying to use OrderBy function in pyspark dataframe before I write into csv but I am not sure to use OrderBy functions if I have a list of columns.
Code:
Cols = ['col1','col2','col3']
df = df.OrderBy(cols,ascending=False)
",<python-3.x><apache-spark><pyspark><apache-spark-sql><sql-order-by>,227,0,2,967,3,10,23,78,46502,0,0,1,16,2018-06-10 12:07,2018-06-10 12:26,2018-06-10 12:26,0,0,Basic,10
57082613,AWS RDS Performance Insights - view full queries,"I'm using a MySQL-Server from AWS RDS. I would like to inspect the queries made by the app to optimize them. My problem is, that almost every query is longer than 1024 chars (which is the max-size, as stated here).
So I cannot identify the query by the first 1024 chars, as thats only the SELECT-Part - the interesting parts WHERE, ORDER, and so on are truncated. Since the app uses an ORM-System, I cannot change the queries to shorten them. 
Already tried to increase the option performance_schema_max_digest_length in the parameter-group to 4096, but that has no effect (no change can be seen in the options directly on the server).
What can I do?
",<mysql><amazon-web-services>,651,2,0,817,1,7,25,60,6673,,48,2,15,2019-07-17 19:03,2019-07-17 20:15,2022-06-22 17:06,0,1071,Intermediate,23
53847917,"PostgreSQL throws ""column is of type jsonb but expression is of type bytea"" with JPA and Hibernate","This is my entity class that is mapped to a table in postgres (9.4)
I am trying to store metadata as jsonb type in the database
@Entity
@Table(name = “room_categories”)
@TypeDef(name = “jsonb”, typeClass = JsonBinaryType.class)
public class RoomCategory extends AbstractEntity implements Serializable {
    private String name;
    private String code;
    @Type(type = ""jsonb"")
    @Column(columnDefinition = ""json"")
    private Metadata metadata;
}
This is the metadata class:
public class Metadata implements Serializable {
    private String field1;
    private String field2;
}
I have used following migration file to add jsonb column:
databaseChangeLog:
– changeSet:
id: addColumn_metadata-room_categories
author: arihant
changes:
– addColumn:
schemaName: public
tableName: room_categories
columns:
– column:
name: metadata
type: jsonb
I am getting this error while creating the record in postgres:
ERROR: column “metadata” is of type jsonb but expression is of type bytea
Hint: You will need to rewrite or cast the expression.
This is the request body i am trying to persist in db:
  {
    “name”: “Test102”,
    “code”: “Code102”,
    “metadata”: {
    “field1”: “field11”,
    “field2”: “field12”
    }
    }
Please help how to convert bytea type to jsonb in java spring boot app
",<java><postgresql><hibernate><jpa><jsonb>,1289,0,44,151,1,1,3,78,33884,,0,3,15,2018-12-19 9:13,2018-12-19 9:24,,0,,Intermediate,19
53657509,"Do I need ""transactionScope.Complete();""?","As far as I understand, the ""correct"" way to use a TransactionScope is to always call transactionScope.Complete(); before exiting the using block. Like this:
using (TransactionScope transactionScope = new TransactionScope(TransactionScopeOption.Required, new TransactionOptions { IsolationLevel = IsolationLevel.ReadUncommitted }))
{
    //...
    //I'm using this as a NOLOCK-alternative in Linq2sql.
    transactionScope.Complete();
}
However, I've seen that the code works without it, and even the answer I've learnt to use it from   omits it. So my question is, must it be used or not? 
",<c#><sql><.net><sql-server><linq-to-sql>,591,1,9,26768,38,141,295,44,8010,0,2952,3,15,2018-12-06 18:21,2018-12-06 18:50,2018-12-10 0:30,0,4,Intermediate,31
52465530,"Sequelize connection timeout while using Serverless Aurora, looking for a way to increase timeout duration or retry connection","I'm having an issue at the moment where I'm trying to make use of a Serverless Aurora database as part of my application.
The problem is essentially that when the database is cold, time to establish a connection can be greater than 30 seconds (due to db spinup) - This seems to be longer than the default timeout in Sequelize (using mysql), and as far as I can see I can't find any other way to increase this timeout or perhaps I need some way of re-attempting a connection?
Here's my current config:
const sequelize = new Sequelize(DATABASE, DB_USER, DB_PASSWORD, {
    host: DB_ENDPOINT,
    dialect: ""mysql"",
    operatorsAliases: false,
    pool: {
      max: 2,
      min: 0,
      acquire: 120000, // This needs to be fairly high to account for a 
      serverless db spinup
      idle: 120000,
      evict: 120000
    }
});
A couple of extra points:
Once the database is warm then everything works perfectly.
Keeping the database ""hot"", while it would technically work, kind of defeats the point of having it as a serverless db (Cost reasons).
I'm open to simply having my client re-try the API call in the event the timeout is a connection error.
Here's the logs in case they help at all.
{
""name"": ""SequelizeConnectionError"",
""parent"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
},
""original"": {
    ""errorno"": ""ETIMEDOUT"",
    ""code"": ""ETIMEDOUT"",
    ""syscall"": ""connect"",
    ""fatal"": true
}
}
",<mysql><node.js><amazon-rds><serverless><amazon-aurora>,1462,0,28,459,1,3,10,73,18139,0,2,1,15,2018-09-23 11:10,2018-09-23 12:03,2018-09-23 12:03,0,0,Intermediate,23
64933345,BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate on ASP.NET core 2.2,"I'm using aspnetboilerplate solution developed with ASP.NET core 2.2 .
The backend is deployed on azure and it uses the SQL server provided.
Sometimes, when the backend has a lot of requests to handle, it logs this exception:
ERROR 2020-11-20 12:28:21,968 [85   ] Mvc.ExceptionHandling.AbpExceptionFilter - Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
Microsoft.AspNetCore.Server.Kestrel.Core.BadHttpRequestException: Reading the request body timed out due to data arriving too slowly. See MinRequestBodyDataRate.
I tried to solve this problem adding this code to my Program.cs
 namespace WorkFlowManager.Web.Host.Startup
    {
        public class Program
        {
            public static void Main(string[] args)
            {
                var host = new WebHostBuilder()
                    .UseKestrel(options =&gt;
                    {
                        options.Limits.MinResponseDataRate = null;
                    });
                BuildWebHost(args).Run();
            }
            public static IWebHost BuildWebHost(string[] args)
            {
                return WebHost.CreateDefaultBuilder(args)
                    .UseStartup&lt;Startup&gt;()
                    .Build();
            }
        }
    }
But the problem is not solved.
",<c#><sql-server><azure-devops><aspnetboilerplate><asp.net-core-2.2>,1321,0,23,685,4,14,25,46,20300,0,15,3,15,2020-11-20 16:38,2020-11-20 18:57,2020-11-20 18:57,0,0,Intermediate,23
62340498,Open database files (.db) using python,"I have a data base file .db in SQLite3 format and I was attempting to open it to look at the data inside it. Below is my attempt to code using python.
    import sqlite3
    # Create a SQL connection to our SQLite database
    con = sqlite3.connect(dbfile)
    cur = con.cursor()
    # The result of a ""cursor.execute"" can be iterated over by row
    for row in cur.execute(""SELECT * FROM ""):
    print(row)
    # Be sure to close the connection
    con.close()
For the line (""SELECT * FROM "") , I understand that you have to put in the header of the table after the word ""FROM"", however, since I can't even open up the file in the first place, I have no idea what header to put. Hence how can I code such that I can open up the data base file to read its contents?
",<python><sqlite>,766,0,14,199,1,1,7,65,49292,0,28,3,15,2020-06-12 8:31,2020-06-12 10:01,2020-06-12 10:01,0,0,Basic,6
56821546,Can't connect to db from docker container,"I have MySQL server installed on a server and dockerized project, The DB is not publicly accessible, only from inside the server. It's used by local non dockerized apps too. 
I want to connect to it from inside the docker with it remaining not publicly accessible, I tried 172.17.0.1 but I get connection refused.
the current bind_address is 127.0.0.1, What do you suggest the bind_address would be ??
",<mysql><docker>,402,0,0,365,2,3,10,77,52171,0,5,2,15,2019-06-29 23:48,2019-06-30 0:48,2019-06-30 0:48,1,1,Basic,14
53397531,"How to use scram-sha-256 in Postgres 10 in Debian? Getting ""FATAL: password authentication failed""","I edited pg_hba.conf:
sudo su postgres
nano /etc/postgresql/10/main/pg_hba.conf
and added this line:
local   all             username                               scram-sha-256
and changed all md5 to scram-sha-256 in that file.
As the postgres user, I created a new user with superuser rights:
sudo su postgres
psql
CREATE USER username WITH SUPERUSER PASSWORD 'password';
Then I restarted Postgres:
/etc/init.d/postgresql restart
and tried to login with pgAdmin4 where I changed the username under the database's Connection properties. But neither that nor psql -U username testdb &lt; ./testdb.sql work as I'm getting:
  FATAL: password authentication failed for user ""username""
So how can I get Postgres working with scram-sha-256 on my Debian9/KDE machine? It worked earlier when I left all the md5 in pg_hba.conf as they were.
",<postgresql><authentication><postgresql-10><sasl-scram>,833,0,15,771,2,8,27,67,18581,0,47,2,15,2018-11-20 16:36,2018-11-20 21:12,2018-11-20 21:12,0,0,Basic,14
50505042,MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.Util.getInstance(Util.java:381)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3558)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3490)
    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:919)
    at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:3996)
    at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1284)
    at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2137)
    at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:776)
    at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:46)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
    at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
    at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:352)
    at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:284)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at java.sql/java.sql.DriverManager.getConnection(Unknown Source)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriverManager(DriverManagerDataSource.java:154)
    at org.springframework.jdbc.datasource.DriverManagerDataSource.getConnectionFromDriver(DriverManagerDataSource.java:145)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnectionFromDriver(AbstractDriverBasedDataSource.java:205)
    at org.springframework.jdbc.datasource.AbstractDriverBasedDataSource.getConnection(AbstractDriverBasedDataSource.java:169)
    at org.hibernate.engine.jdbc.connections.internal.DatasourceConnectionProviderImpl.getConnection(DatasourceConnectionProviderImpl.java:122)
    at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess.obtainConnection(JdbcEnvironmentInitiator.java:180)
    at org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl.getIsolatedConnection(DdlTransactionIsolatorNonJtaImpl.java:43)
I get this error even after updating mysql password. I read in some posts that mysql has a new caching mechanism and all clients need to run the following which I already did but still the same error:
 update user set authentication_string='test' where user='root';
 flush privileges   
",<java><mysql><spring><tomcat>,3344,0,35,237,2,6,13,57,90098,0,3,8,15,2018-05-24 8:55,2018-05-24 11:03,,0,,Basic,14
54357532,What is the execution order of the PARTITION BY clause compared to other SQL clauses?,"I cannot find any source mentioning execution order for Partition By window functions in SQL.
Is it in the same order as Group By?
For example table like:
Select *, row_number() over (Partition by Name) 
from NPtable 
Where Name = 'Peter'
I understand if Where gets executed first, it will only look at Name = 'Peter', then execute window function that just aggregates this particular person instead of entire table aggregation, which is much more efficient.
But when the query is:
Select top 1 *, row_number() over (Partition by Name order by Date) 
from NPtable 
Where Date &gt; '2018-01-02 00:00:00'
Doesn't the window function need to be executed against the entire table first then applies the Date&gt; condition otherwise the result is wrong?
",<sql><sql-server>,749,2,11,643,1,5,15,51,10947,0,32,3,15,2019-01-25 0:48,2019-01-25 2:36,2019-01-25 2:36,0,0,Basic,5
50761433,How to optimize count and order by query in millions of row,"Needed help in optimizing order by and count query, I have tables having millions (approx 3 millions) rows.
I have to join 4 tables and fetch the records, When i run the simple query it takes only millisecond to complete but as I try to count or order by having left join table it get stuck for unlimited of time.
Please see the cases below.
DB Server Configuration:
CPU Number of virtual cores: 4
Memory(RAM): 16 GiB
Network Performance: High
Rows in each table:
tbl_customers -  #Rows: 20 million.
tbl_customers_address -  #Row 25 million.
tbl_shop_setting - #Rows 50k
aio_customer_tracking - #Rows 5k
Tables Schema:
CREATE TABLE `tbl_customers` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(225) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `accepts_marketing` TINYINT(1) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_name` VARCHAR(50) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `last_order_id` BIGINT(20) NULL DEFAULT NULL,
    `total_spent` DECIMAL(12,2) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL COLLATE 'latin1_swedish_ci',
    `verified_email` TINYINT(4) NULL DEFAULT NULL,
    `updated_at` DATETIME NULL DEFAULT NULL,
    `created_at` DATETIME NULL DEFAULT NULL,
    `date_updated` DATETIME NULL DEFAULT NULL,
    `date_created` DATETIME NULL DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE INDEX `shopify_customer_id_unique` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `shop_id` (`shop_id`)
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_customers_address` (
    `id` BIGINT(20) NOT NULL AUTO_INCREMENT,
    `customer_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_address_id` BIGINT(20) NULL DEFAULT NULL,
    `shopify_customer_id` BIGINT(20) NULL DEFAULT NULL,
    `first_name` VARCHAR(50) NULL DEFAULT NULL,
    `last_name` VARCHAR(50) NULL DEFAULT NULL,
    `company` VARCHAR(50) NULL DEFAULT NULL,
    `address1` VARCHAR(250) NULL DEFAULT NULL,
    `address2` VARCHAR(250) NULL DEFAULT NULL,
    `city` VARCHAR(50) NULL DEFAULT NULL,
    `province` VARCHAR(50) NULL DEFAULT NULL,
    `country` VARCHAR(50) NULL DEFAULT NULL,
    `zip` VARCHAR(15) NULL DEFAULT NULL,
    `phone` VARCHAR(20) NULL DEFAULT NULL,
    `name` VARCHAR(50) NULL DEFAULT NULL,
    `province_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_code` VARCHAR(5) NULL DEFAULT NULL,
    `country_name` VARCHAR(50) NULL DEFAULT NULL,
    `longitude` VARCHAR(250) NULL DEFAULT NULL,
    `latitude` VARCHAR(250) NULL DEFAULT NULL,
    `default` TINYINT(1) NULL DEFAULT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `customer_id` (`customer_id`),
    INDEX `shopify_address_id` (`shopify_address_id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
CREATE TABLE `tbl_shop_setting` (
    `id` INT(11) NOT NULL AUTO_INCREMENT,   
    `shop_name` VARCHAR(300) NOT NULL COLLATE 'latin1_swedish_ci',
     PRIMARY KEY (`id`),
)
COLLATE='utf8mb4_general_ci'
ENGINE=InnoDB;
CREATE TABLE `aio_customer_tracking` (
    `id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,
    `shopify_customer_id` BIGINT(20) UNSIGNED NOT NULL,
    `email` VARCHAR(255) NULL DEFAULT NULL,
    `shop_id` BIGINT(20) UNSIGNED NOT NULL,
    `domain` VARCHAR(255) NULL DEFAULT NULL,
    `web_session_count` INT(11) NOT NULL,
    `last_seen_date` DATETIME NULL DEFAULT NULL,
    `last_contact_date` DATETIME NULL DEFAULT NULL,
    `last_email_open` DATETIME NULL DEFAULT NULL,
    `created_date` DATETIME NOT NULL,
    `is_geo_fetched` TINYINT(1) NOT NULL DEFAULT '0',
    PRIMARY KEY (`id`),
    INDEX `shopify_customer_id` (`shopify_customer_id`),
    INDEX `email` (`email`),
    INDEX `shopify_customer_id_shop_id` (`shopify_customer_id`, `shop_id`),
    INDEX `last_seen_date` (`last_seen_date`)
)
COLLATE='latin1_swedish_ci'
ENGINE=InnoDB;
Query Cases Running and Not Running:
1. Running:  Below query fetch the records by joining all the 4 tables, It takes only 0.300 ms.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
2. Not running: Simply when try to get the count of these row stuk the query, I waited 10 min but still running.
SELECT 
     COUNT(DISTINCT c.shopify_customer_id)   -- what makes #2 different
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
LIMIT 20
3. Not running: In the #1 query we simply put the 1 Order by clause and it get stuck, I waited 10 min but still running. I study query optimization some article and tried by indexing, Right Join etc.. but still not working.
SELECT `c`.first_name,`c`.last_name,`c`.email, `t`.`last_seen_date`, `t`.`last_contact_date`, `ssh`.`shop_name`, ca.`company`, ca.`address1`, ca.`address2`, ca.`city`, ca.`province`, ca.`country`, ca.`zip`, ca.`province_code`, ca.`country_code`
FROM `tbl_customers` AS `c`
JOIN `tbl_shop_setting` AS `ssh` ON c.shop_id = ssh.id 
LEFT JOIN (SELECT shopify_customer_id, last_seen_date, last_contact_date FROM aio_customer_tracking GROUP BY shopify_customer_id) as t ON t.shopify_customer_id = c.shopify_customer_id
LEFT JOIN `tbl_customers_address` as ca ON (c.shopify_customer_id = ca.shopify_customer_id AND ca.default = 1)
GROUP BY c.shopify_customer_id
  ORDER BY `t`.`last_seen_date`    -- what makes #3 different
LIMIT 20
EXPLAIN QUERY #1:
EXPLAIN QUERY #2:
EXPLAIN QUERY #3:
Any suggestion to optimize the query, table structure are welcome.
WHAT I'M TRYING TO DO:
tbl_customers table contains the customer info, tbl_customer_address table contains the addresses of the customers(one customer may have multiple address), And aio_customer_tracking table contains visiting records of the customer last_seen_date is the visiting date.
Now, simply I want to fetch and count the customers, with their one of the address, and visiting info. Also, I may order by any of the column from these 3 tables, In my example i am ordering by last_seen_date (the default order). Hope this explanation helps to understand what i am trying to do. 
",<mysql><database><database-administration>,7174,3,129,688,3,13,35,57,4231,0,12,4,15,2018-06-08 12:44,2018-06-11 4:51,,3,,Intermediate,23
53663954,Trouble connecting to postgres from outside Kubernetes cluster,"I've launched a postgresql server in minikube, and I'm having difficulty connecting to it from outside the cluster.
Update
It turned out my cluster was suffering from unrelated problems, causing all sorts of broken behavior. I ended up nuking the whole cluster and vm and starting from scratch. Now I've got working. I changed the deployment to a statefulset, though I think it could work either way.
Setup and test:
kubectl --context=minikube create -f postgres-statefulset.yaml
kubectl --context=minikube create -f postgres-service.yaml
url=$(minikube service postgres --url --format={{.IP}}:{{.Port}})
psql --host=${url%:*} --port=${url#*:} --username=postgres --dbname=postgres \
     --command='SELECT refobjid FROM pg_depend LIMIT 1'
Password for user postgres:
 refobjid
----------
     1247
postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  selector:
    app: postgres
  type: NodePort
  ports:
    - name: postgres
      port: 5432
      targetPort: 5432
      protocol: TCP
postgres-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: postgres
  labels:
    app: postgres
    role: service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
      role: service
  serviceName: postgres
  template:
    metadata:
      labels:
        app: postgres
        role: service
    spec:
      containers:
        - name: postgres
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
              name: postgres
              protocol: TCP
Original question
I created a deployment running one container (postgres-container) and a NodePort (postgres-service). I can connect to postgresql from within the pod itself:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- psql --port=5432 --username=postgres --dbname=postgres
But I can't connect through the service.
$ minikube service --url postgres-service
http://192.168.99.100:32254
$ psql --host=192.168.99.100 --port=32254 --username=postgres --dbname=postgres
psql: could not connect to server: Connection refused
        Is the server running on host ""192.168.99.100"" and accepting
        TCP/IP connections on port 32254?
I think postgres is correctly configured to accept remote TCP connections:
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- tail /var/lib/postgresql/data/pg_hba.conf
host    all             all             127.0.0.1/32            trust
...
host all all all md5
$ kubectl --context=minikube exec -it postgres-deployment-7fbf655986-r49s2 \
    -- grep listen_addresses /var/lib/postgresql/data/postgresql.conf
listen_addresses = '*'
My service definition looks like:
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres-container
  type: NodePort
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
And the deployment is:
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-container
  template:
    metadata:
      labels:
        app: postgres-container
    spec:
      containers:
        - name: postgres-container
          image: postgres:9.6
          env:
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgres
            - name: POSTGRES_DB
              value: postgres
          ports:
            - containerPort: 5432
The resulting service configuration:
$ kubectl --context=minikube get service postgres-service -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-12-07T05:29:22Z
  name: postgres-service
  namespace: default
  resourceVersion: ""194827""
  selfLink: /api/v1/namespaces/default/services/postgres-service
  uid: 0da6bc36-f9e1-11e8-84ea-080027a52f02
spec:
  clusterIP: 10.109.120.251
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 32254
    port: 5432
    protocol: TCP
    targetPort: 5432
  selector:
    app: postgres-container
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
I can connect if I use port-forward, but I'd like to use the nodePort instead. What am I missing?
",<postgresql><kubernetes>,4501,1,143,6082,9,43,67,74,30418,0,135,2,15,2018-12-07 5:47,2018-12-08 8:13,,1,,Intermediate,23
50547724,How to resolve the error: SQL authentication method unknown in Laravel-MySql,"I using docker and I have a container of Laravel Framework 5.5.25 and other with mysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL). in my configuration of docker compose I have this: 
version: ""2""
services:
    mysql:
    image: mysql
        ports:
            - ""3307:3306""
        command: --sql_mode=""""
So, when Laravel try to connect to MySql I have this error: 
  SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from
",<php><docker><docker-compose><laravel-5.5><mysql-8.0>,497,0,7,351,2,3,12,46,21493,0,25,5,15,2018-05-26 23:11,2018-06-27 7:00,,32,,Basic,14
48630408,SSIS Script Tasks losing code,"I have a very strange issue happening that is causing Script Task code to clear out. I have been able to test on 2-3 different machines. We are running SSDT 15.4 preview. The steps to reproduce were as follows.
Create a script task inside of a foreach loop container. 
Create a comment in the script task. 
Change or add a variable mapping in the foreach. 
Save package. 
Close the package.
Open the package.
Open the script task and the comment will have vanished.
As my last attempt for success,I have upgraded to 15.5.1 and the problem still exists.
",<sql-server><ssis><etl><sql-server-data-tools><script-task>,553,0,0,349,0,4,12,40,3756,0,6,3,15,2018-02-05 19:50,2018-03-27 21:04,,50,,Basic,14
48087980,Annotate QuerySet with first value of ordered related model,"I have a QuerySet of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM.
Background
Let's say that I have two related models: Book, and BlogPost, each with a foreign key to an Author:
class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()
class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing.
Working solution in PostgreSQL 9.6
WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
Translating to Django's ORM
While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further
books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now.
Attempted solution
I'd first built a Q object to filter down to mystery books published after the blog post.
published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
From here, I attempted to piece together django.db.models.Min and additional F objects to acheive my desired results, but with no success.
Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.
",<python><django><postgresql><django-queryset><django-1.10>,2289,0,39,16645,14,66,76,55,2258,,2247,2,15,2018-01-04 2:47,2018-01-11 9:29,2018-01-11 9:29,7,7,Intermediate,16
51227958,SQLAlchemy - check if query found any results,"How can I check if a query found any results?
result = db.engine.execute(sql, id=foo)
// check if result has rows ...
for row in result:
  ...
Tried this:
if result is None:
    print(""reuslt is None!"")
and checked length:
print(""result len"", len(result))
",<sqlalchemy><flask-sqlalchemy>,256,0,7,11791,26,86,143,68,26581,0,880,6,15,2018-07-08 0:31,2018-07-09 5:48,,1,,Basic,9
48315442,Error while exploding a struct column in Spark,"I have a dataframe whose schema looks like this:
event: struct (nullable = true)
|    | event_category: string (nullable = true)
|    | event_name: string (nullable = true)
|    | properties: struct (nullable = true)
|    |    | ErrorCode: string (nullable = true)
|    |    | ErrorDescription: string (nullable = true)
I am trying to explode the struct column properties using the following code: 
df_json.withColumn(""event_properties"", explode($""event.properties""))
But it is throwing the following exception: 
cannot resolve 'explode(`event`.`properties`)' due to data type mismatch: 
input to function explode should be array or map type, 
not StructType(StructField(IDFA,StringType,true),
How to explode the column properties?
",<scala><apache-spark><pyspark><apache-spark-sql>,732,0,13,463,1,8,18,43,30529,0,45,3,15,2018-01-18 6:55,2018-01-18 8:12,2018-01-18 9:48,0,0,Basic,2
51083593,Parent count based on pairing of multiple children,"In the below example, I'm trying to count the number of drinks I can make based on the availability of ingredients per bar location that I have.
To further clarify, as seen in the below example: based on the figures highlighted in the chart below; I know that I can only make 1 Margarita on 6/30/2018 (in either DC or FL if I ship the supplies to the location).
Sample of data table
Please use the below code to enter the relevant data above:
    CREATE TABLE #drinks 
    (
        a_date      DATE,
        loc         NVARCHAR(2),
        parent      NVARCHAR(20),
        line_num    INT,
        child       NVARCHAR(20),
        avail_amt   INT
    );
INSERT INTO #drinks VALUES ('6/26/2018','CA','Long Island','1','Vodka','7');
INSERT INTO #drinks VALUES ('6/27/2018','CA','Long Island','2','Gin','5');
INSERT INTO #drinks VALUES ('6/28/2018','CA','Long Island','3','Rum','26');
INSERT INTO #drinks VALUES ('6/26/2018','DC','Long Island','1','Vodka','15');
INSERT INTO #drinks VALUES ('6/27/2018','DC','Long Island','2','Gin','18');
INSERT INTO #drinks VALUES ('6/28/2018','DC','Long Island','3','Rum','5');
INSERT INTO #drinks VALUES ('6/26/2018','FL','Long Island','1','Vodka','34');
INSERT INTO #drinks VALUES ('6/27/2018','FL','Long Island','2','Gin','14');
INSERT INTO #drinks VALUES ('6/28/2018','FL','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Margarita','1','Tequila','6');
INSERT INTO #drinks VALUES ('7/1/2018','DC','Margarita','2','Triple Sec','3');
INSERT INTO #drinks VALUES ('6/29/2018','FL','Margarita','1','Tequila','1');
INSERT INTO #drinks VALUES ('6/30/2018','FL','Margarita','2','Triple Sec','0');
INSERT INTO #drinks VALUES ('7/2/2018','CA','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/8/2018','CA','Cuba Libre','2','Coke','5');
INSERT INTO #drinks VALUES ('7/13/2018','CA','Cuba Libre','3','Lime','14');
INSERT INTO #drinks VALUES ('7/5/2018','DC','Cuba Libre','1','Rum','0');
INSERT INTO #drinks VALUES ('7/19/2018','DC','Cuba Libre','2','Coke','12');
INSERT INTO #drinks VALUES ('7/31/2018','DC','Cuba Libre','3','Lime','9');
INSERT INTO #drinks VALUES ('7/2/2018','FL','Cuba Libre','1','Rum','1');
INSERT INTO #drinks VALUES ('7/19/2018','FL','Cuba Libre','2','Coke','3');
INSERT INTO #drinks VALUES ('7/17/2018','FL','Cuba Libre','3','Lime','2');
INSERT INTO #drinks VALUES ('6/30/2018','DC','Long Island','3','Rum','4');
INSERT INTO #drinks VALUES ('7/7/2018','FL','Cosmopolitan','5','Triple Sec','7');
The expected results are as follows:
Please note, as seen in the expected results, children are interchangeable. For example, on 7/7/2018 Triple Sec arrived for the drink cosmopolitan; however because the child is also rum, it changes the availability of Margaritas for FL.
Also not the update to the DC region for Cuba Libre's on both 06/30 and 06/31.
Please take into consideration that parts are interchangeable and also that each time a new item arrives it makes available any item previously now.
Lastly - It would be awesome if I could add another column that shows kit availability regardless of location based only on availability of the child. For Ex. If there is a child #3 in DC and none in FL they FL can assume that they have enough inventory to make drink based on inventory in another location!
",<sql><sql-server><parent-child><cross-apply><outer-apply>,3294,2,34,165,0,0,6,37,375,0,0,4,15,2018-06-28 13:00,2018-06-29 18:15,,1,,Advanced,32
49370562,Comma separating numbers without advance knowledge of number of digits in Postgres,"In Postgres 9.5:
select to_char(111, 'FM9,999'); -- gives 111
select to_char(1111, 'FM9,999'); -- gives 1,111
select to_char(11111, 'FM9,999'); -- gives #,### !!!
How can I format my numbers with commas without advance knowledge/assumption of maximum number of possible digits preferably by using built-in stuff like above?
",<postgresql>,324,0,3,7848,5,44,48,37,14202,0,2543,2,15,2018-03-19 19:03,2018-03-19 19:08,2018-03-19 19:08,0,0,Basic,1
52243652,GCP SQL Postgres problem with privileges : can't run a query with postgres user with generated symfony db,"I am struggling to solve this problem with Google Cloud Platform's Cloud SQL component. My tech stack consists of hosting my application in a Google Kubernetes Engine (GKE) deployment, using the Cloud SQL proxy sidecar to connect to the database within the pods. The backend is a Symfony project.
I follow these steps to create and populate the database (without success):
Create Cloud SQL Postgres instance
Add proxy to k8s container to connect to the Cloud SQL instance with all credentials, as described in the GCP documentation
Enter my Symfony (phpfpm) pod and run the command php bin/console doctrine:schema:update --force to update the schema. The queries are executed in the database, so the schema is created and so on.
I try to open the database from the SQL console connection within GCP with the postgres user and try to execute a simple select * from foo; query. The response is Insufficient privilege: 7 ERROR:  permission denied for relation
How can I query the data in the database with the postgres user?
EDIT :
I have this situation about users :
     Role name     |                         Attributes                         |           Member of           
-------------------+------------------------------------------------------------+-------------------------------
 acando14          | Create role, Create DB                                     | {cloudsqlsuperuser,proxyuser}
 cloudsqladmin     | Superuser, Create role, Create DB, Replication, Bypass RLS | {}
 cloudsqlagent     | Create role, Create DB                                     | {cloudsqlsuperuser}
 cloudsqlreplica   | Replication                                                | {}
 cloudsqlsuperuser | Create role, Create DB                                     | {}
 postgres          | Create role, Create DB                                     | {cloudsqlsuperuser,acando14}
 proxyuser         | Create role, Create DB                                     | {cloudsqlsuperuser}
And I have this situation in the tables :
              List of relations
 Schema |      Name       | Type  |  Owner   
--------+-----------------+-------+----------
 public | article         | table | acando14
If I use postgres user logged in my db symfony works :
symfony =&gt; select * from article;
 id | model_id | code | size 
----+----------+------+------
(0 rows)
But if I use the server to execute the code the response is :
  SQLSTATE[42501]: Insufficient privilege: 7 ERROR: permission denied
  for relation employee at PDOException(code: 42501): SQLSTATE[42501]:
  Insufficient privilege: 7 ERROR: permission denied for relation .. at
And another problem is that I didn't generate all the tables with the command but I have to generate it executing all the queries, so strange...
Thank you, regards
",<php><postgresql><symfony><google-cloud-platform><google-cloud-sql>,2784,1,24,562,2,7,21,58,14934,0,16,1,15,2018-09-09 10:39,2018-09-09 12:47,2018-09-09 12:47,0,0,Basic,3
49945649,MySQL Error: Authentication plugin 'caching_sha2_password' cannot be loaded,"I just installed MySQL Ver 14.14 Distrib 5.7.22 with Homebrew on my macOS v10.13.4.
I ran the command:
brew install mysql 
After the installation completed, as directed by Homebrew, I ran the command:
mysql_secure_installation
and was returned the error: Error: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found   
I tried a few things like changing default_authentication_plugin to mysql_native_password in the my.cnf file but it still throws the same error. 
Next I tried running:
mysql_upgrade -u root
and I was thrown the same error again mysql_upgrade: Got error: 2059: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/Cellar/mysql/5.7.22/lib/plugin/caching_sha2_password.so, 2): image not found while connecting to the MySQL server
Upgrade process encountered error and will not continue.
Any help is appreciated.
",<mysql><macos><macos-high-sierra>,962,0,6,587,2,6,15,75,29574,0,242,4,15,2018-04-20 16:01,2018-04-20 16:57,2018-04-21 12:50,0,1,Basic,9
54944677,Sql Server LDF file taking too large space,"Why is my database log file taking to high space? Its almost taking up 30GB of my HDD. Even after deleting 1,000,000 records, its not freeing up any space.
So, 
1.Why is the log file taking this much space (30gb)?2.how can I free up the space?
",<sql-server>,244,0,5,612,2,6,32,75,43163,0,244,5,15,2019-03-01 12:27,2019-03-01 13:00,2019-03-01 13:00,0,0,Intermediate,23
50793970,Select shortest and longest string,"Is it possible to select the shortest and longest strings by characters in a table?
I have a CITY column of type VARCHAR(20) and I want to select the shortest and longest city names in alphabetical order by length.
I did like this
SELECT CITY,LENGTH(CITY) FROM STATION WHERE LENGTH(CITY) IN ( SELECT MAX(LENGTH(CITY)) FROM STATION UNION SELECT MIN(LENGTH(CITY)) FROM STATION ) ORDER BY CITY ASC;
When ordered alphabetically, Let the CITY names be listed as ABC, DEF, PQRS, and WXY, with the respective lengths 3,3,4, and 3. The longest-named city is obviously PQRS, but there are options for the shortest-named city; I have to select ABC because it comes first alphabetically.
My query ended up with all three CITY having length 3.
ABC 3 DEF 3 PQRS 4 WXY 3 
The result of SELECT must be
ABC 3 PQRS 4 
",<mysql><sql>,801,0,5,513,1,6,13,74,40990,0,314,20,15,2018-06-11 9:03,2018-06-11 9:15,2018-06-12 9:08,0,1,Basic,9
49432167,How to convert rows into a list of dictionaries in pyspark?,"I have a DataFrame(df) in pyspark, by reading from a hive table:
df=spark.sql('select * from &lt;table_name&gt;')
+++++++++++++++++++++++++++++++++++++++++++
|  Name    |    URL visited               |
+++++++++++++++++++++++++++++++++++++++++++
|  person1 | [google,msn,yahoo]           |
|  person2 | [fb.com,airbnb,wired.com]    |
|  person3 | [fb.com,google.com]          |
+++++++++++++++++++++++++++++++++++++++++++
When i tried the following, got an error
df_dict = dict(zip(df['name'],df['url']))
""TypeError: zip argument #1 must support iteration.""
type(df.name) is of 'pyspark.sql.column.Column'
How do i create a dictionary like the following, which can be iterated later on
{'person1':'google','msn','yahoo'}
{'person2':'fb.com','airbnb','wired.com'}
{'person3':'fb.com','google.com'}
Appreciate your thoughts and help. 
",<apache-spark><pyspark><apache-spark-sql>,833,0,16,169,1,1,3,55,52106,0,0,4,15,2018-03-22 15:10,2018-03-22 15:28,,0,,Basic,10
48847660,Spark + Parquet + Snappy: Overall compression ratio loses after spark shuffles data,"Commmunity!
Please help me understand how to get better compression ratio with Spark?
Let me describe case:
I have dataset, let's call it product on HDFS which was imported using Sqoop ImportTool as-parquet-file using codec snappy. As result of import, I have 100 files with total 46 GB du, files with diffrrent size (min 11MB, max 1.5GB, avg ~ 500MB). Total count of records a little bit more than 8 billions with 84 columns
I'm doing simple read/repartition/write with Spark using snappy as well and as result I'm getting:
~100 GB output size with the same files count, same codec, same count and same columns.
Code snippet:
val productDF = spark.read.parquet(""/ingest/product/20180202/22-43/"")
productDF
.repartition(100)
.write.mode(org.apache.spark.sql.SaveMode.Overwrite)
.option(""compression"", ""snappy"")
.parquet(""/processed/product/20180215/04-37/read_repartition_write/general"")
Using parquet-tools I have looked into random files from both ingest and processed and they looks as below:
ingest:
creator:                        parquet-mr version 1.5.0-cdh5.11.1 (build ${buildNumber}) 
extra:                          parquet.avro.schema = {""type"":""record"",""name"":""AutoGeneratedSchema"",""doc"":""Sqoop import of QueryResult"",""fields""
and almost all columns looks like
AVAILABLE: OPTIONAL INT64 R:0 D:1
row group 1:                    RC:3640100 TS:36454739 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:172743 SZ:370515/466690/1.26 VC:3640100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 126518400000, max: 1577692800000, num_nulls: 2541633]
processed:
creator:                        parquet-mr version 1.5.0-cdh5.12.0 (build ${buildNumber}) 
extra:                          org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields""
AVAILABLE:                      OPTIONAL INT64 R:0 D:1
...
row group 1:                    RC:6660100 TS:243047789 OFFSET:4 
AVAILABLE:                       INT64 SNAPPY DO:0 FPO:4122795 SZ:4283114/4690840/1.10 VC:6660100 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE ST:[min: -2209136400000, max: 10413820800000, num_nulls: 4444993]
In other hand, without repartition or using coalesce - size remains close to ingest data size.
Going forward, I did following:
read dataset and write it back with   
productDF
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithoutshuffle"")
read dataset, repartition and write it back with 
productDF
  .repartition(500)
  .write.mode(org.apache.spark.sql.SaveMode.Overwrite)
  .option(""compression"", ""none"")
  .parquet(""/processed/product/20180215/04-37/read_repartition_write/nonewithshuffle"")
As result: 80 GB without and  283 GB with repartition with same # of output files
80GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:456753 SZ:1452623/1452623/1.00 VC:11000100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -1735747200000, max: 2524550400000, num_nulls: 7929352]
283 GB parquet meta example:
AVAILABLE:                       INT64 UNCOMPRESSED DO:0 FPO:2800387 SZ:2593838/2593838/1.00 VC:3510100 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: -2209136400000, max: 10413820800000, num_nulls: 2244255]
It seems, that parquet itself (with encoding?) much reduce size of data even without uncompressed data. How ? :)
I tried to read  uncompressed 80GB, repartition and write back - I've got my 283 GB
The first question for me is why I'm getting bigger size after spark repartitioning/shuffle?
The second is how to efficiently shuffle data in spark to benefit parquet encoding/compression if there any?
In general, I don't want that my data size growing after spark processing, even if I didn't change anything.
Also, I failed to find, is there any configurable compression rate for snappy, e.g. -1 ... -9? As I know, gzip has this, but what is the way to control this rate in Spark/Parquet writer?
Appreciate for any help!
Thanks!
",<apache-spark><apache-spark-sql><parquet><snappy>,3982,0,36,1223,1,12,16,53,15878,0,15,2,15,2018-02-18 1:43,2019-06-04 20:54,,471,,Intermediate,23
60636473,How to read/write Timestamp in Doobie (Postgres),"How to read/write Timestamp in Doobie?
I have a record class that contains a timestamp field. When I am trying to write it to the database or read it using doobie I get an error Cannot find or construct a Read instance for type.
case class ExampleRecord(data: String, created_at: Timestamp)
val create = sql""create table if not exists example_ts (data TEXT NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP)"".update.run
val insert = Update[ExampleRecord](""insert into example_ts (data, created_at) values (?, ?)"")
  .updateMany(List(
    ExampleRecord(""one"", Timestamp.valueOf(LocalDateTime.now())),
    ExampleRecord(""two"", Timestamp.valueOf(LocalDateTime.now()))
  ))
val select = sql""select data, created_at from example_ts"".query[ExampleRecord].stream
val app = for {
  _ &lt;- create.transact(xa).compile.drain
  _ &lt;- insert.transact(xa).compile.drain
  _ &lt;- select.transact(xa).compile.drain
} yield ()
app.unsafeRunSync()
",<postgresql><scala><doobie>,957,0,18,2021,0,16,26,75,5582,0,50,1,15,2020-03-11 12:51,2020-03-11 12:51,2020-03-11 12:51,0,0,Basic,3
55347251,Cannot select where ip=inet_pton($ip),"I have a unique column in database which is named ip
IP addresses are stored in this column as BINARY(16) (with no collation) after converting them using the PHP function 
$store_ip = inet_pton($ip);
When I try to insert the same IP twice it works fine and fails because it is unique,
But when I try to select the IP it doesn't work and always returns FALSE (not found)
&lt;?php
try {
    $ip = inet_pton($_SERVER['REMOTE_ADDR']);
    $stmt = $db-&gt;prepare(""SELECT * FROM `votes` WHERE ip=?"");
    $stmt-&gt;execute([$ip]);
    $get = $stmt-&gt;fetch();
    if( ! $get){
        echo 'Not found';
    }else{
        echo 'Found';
    }
    // close connection
    $get = null;
    $stmt = null;
} catch (PDOException $e) {
    error_log($e-&gt;getMessage());
}
The part where I insert the IP:
&lt;?php
if( ! filter_var($ip, FILTER_VALIDATE_IP)){
        return FALSE;
}
$ip = inet_pton($_SERVER['REMOTE_ADDR']);
try {
    $stmt = $db-&gt;prepare(""INSERT INTO votes(ip, answer) VALUES(?,?)"");
    $stmt-&gt;execute([$ip, $answer]);
    $stmt = null;
} catch (PDOException $e) {
    return FALSE;
}
",<php><mysql><pdo>,1099,0,39,832,1,15,34,57,1700,0,198,3,15,2019-03-25 22:21,2019-03-30 20:50,2019-03-30 20:50,5,5,Basic,1
54351783,duplicate key value violates unique constraint - postgres error when trying to create sql table from dask dataframe,"Following on from this question, when I try to create a postgresql table from a dask.dataframe with more than one partition I get the following error:
IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""pg_type_typname_nsp_index""
DETAIL:  Key (typname, typnamespace)=(test1, 2200) already exists.
 [SQL: '\nCREATE TABLE test1 (\n\t""A"" BIGINT, \n\t""B"" BIGINT, \n\t""C"" BIGINT, \n\t""D"" BIGINT, \n\t""E"" BIGINT, \n\t""F"" BIGINT, \n\t""G"" BIGINT, \n\t""H"" BIGINT, \n\t""I"" BIGINT, \n\t""J"" BIGINT, \n\tidx BIGINT\n)\n\n']
You can recreate the error with the following code:
import numpy as np
import dask.dataframe as dd
import dask
import pandas as pd
import sqlalchemy_utils as sqla_utils
import sqlalchemy as sqla
DATABASE_CONFIG = {
    'driver': '',
    'host': '',
    'user': '',
    'password': '',
    'port': 5432,
}
DBNAME = 'dask'
url = '{driver}://{user}:{password}@{host}:{port}/'.format(
        **DATABASE_CONFIG)
db_url = url.rstrip('/') + '/' + DBNAME
# create db if non-existent
if not sqla_utils.database_exists(db_url):
    print('Creating database \'{}\''.format(DBNAME))
    sqla_utils.create_database(db_url)
conn = sqla.create_engine(db_url)
# create pandas df with random numbers
df = pd.DataFrame(np.random.randint(0,40,size=(100, 10)), columns=list('ABCDEFGHIJ'))
# add index so that it can be used as primary key later on
df['idx'] = df.index
# create dask df
ddf = dd.from_pandas(df, npartitions=4)
# Write to psql
dto_sql = dask.delayed(pd.DataFrame.to_sql)
out = [dto_sql(d, 'test', db_url, if_exists='append', index=False, index_label='idx')
       for d in ddf.to_delayed()]
dask.compute(*out)
The code doesn't produce an error if npartitions is set to 1. So I'm guessing it has to do with postgres not being able to handle parallel requests to write to a same sql table...? How can I fix this?
",<python><postgresql><pandas><dask><pandas-to-sql>,1856,1,36,2367,2,30,59,52,27798,0,175,3,15,2019-01-24 16:56,2019-03-21 17:39,2019-03-21 17:39,56,56,Basic,9
55066509,Error in phpmyadmin Warning in ./libraries/plugin_interface.lib.php#551,"Error:
  Warning in ./libraries/plugin_interface.lib.php#551 count(): Parameter
  must be an array or an object that implements Countable
Backtrace:
./libraries/display_export.lib.php#381: PMA_pluginGetOptions(
string 'Export',
array,
)
./libraries/display_export.lib.php#883: PMA_getHtmlForExportOptionsFormat(array)
./libraries/display_export.lib.php#1099: PMA_getHtmlForExportOptions(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
array,
integer 0,
)
./tbl_export.php#143: PMA_getExportDisplay(
string 'table',
string 'bpapluswpdb',
string 'wp_commentmeta',
string '',
integer 0,
integer 0,
string '',
)
How can I fix it?
",<mysql><sql><phpmyadmin>,664,0,23,153,1,1,7,55,20468,0,1,3,15,2019-03-08 15:40,2019-03-08 15:50,2019-12-08 20:41,0,275,Basic,13
48999379,"psycopg2.OperationalError: FATAL: password authentication failed for user ""<my UNIX user>""","I am a fairly new to web developement.
First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it.
It works well locally with PostgreSQL but I can't make it work on my server.
It seems like it tries to connect to Postgres with my Unix user. 
Why would my server try to do that?
I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations:
Settings.py + python-decouple:
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
This is the error message I get each time I try to ./manage.py migrate
'myportfolio' is my Unix user name, the database username is different:
Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
I tried to: 
delete my django code, re install 
delete/purge postgres and reinstall
modify pg_hba.conf local to trust
At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?
",<python><django><postgresql><ubuntu-16.04>,5269,0,67,1560,4,18,27,42,48293,0,137,8,15,2018-02-27 0:41,2018-02-27 1:47,2018-02-27 12:36,0,0,Basic,13
64905397,Sequelize missing FROM-clause entry for table Postgres,"I am trying to filter my DataRow objects by where the included DataPoints data is loc and sortOrder is 2. Below is the query I am trying. I keep getting the following error.
SequelizeDatabaseError: missing FROM-clause entry for table &quot;dataPoints&quot;
I have tried setting required: true and duplicating: false with no luck. I tried these in both the DataRows include as well as DataPoints.
    attributes: ['id', 'name', 'labId'],
    include: [{
      as: 'dataColumns',
      model: DataColumn,
      attributes: ['id', 'name', 'sortOrder', 'dataType', 'isDate', 'isLocation'],
    }, {
      as: 'dataSets',
      model: Study,
      attributes: ['id', 'name'],
    }, {
      as: 'dataRows',
      model: DataRow,
      attributes: ['id'],
      where: {
        [Op.and]: [
          {
            '$dataPoints.data$': 'loc',
          },
          {
            '$dataPoints.sortOrder$': 2,
          },
        ],
      },
      include: [{
        as: 'dataPoints',
        model: DataPoint,
        attributes: ['id', 'sortOrder', 'data'],
      }],
    },],
    order: [
      [{ model: DataColumn, as: 'dataColumns' }, 'sortOrder', 'ASC'],
      [{ model: DataRow, as: 'dataRows' }, { model: DataPoint, as: 'dataPoints' }, 'sortOrder', 'ASC'],
    ],
  }
",<node.js><database><postgresql><sequelize.js>,1272,0,39,183,1,1,5,53,14105,,0,5,15,2020-11-19 4:23,2021-06-24 9:00,,217,,Basic,10
50388490,Analyze SQL query in DBeaver,"I would like to get some info about a running query to analyze it. In PgAdmin 3 I could at least use 'Explain Query', while in DBeaver, after clicking 'Explain Execution Plan', nothing happens.
How to obtain any information about query in DBeaver?
@Edit
Sorry if this question seems too broad. I don't expect explanation of how to analyze the query, I just would like to know if it is possible to open an analyzer in DBeaver and how to do it.
",<sql><postgresql><dbeaver>,443,0,0,5196,7,50,85,41,46892,,454,2,15,2018-05-17 9:53,2021-01-26 7:10,,985,,Basic,3
52589849,"""Create database if not exists"" in postgres","I am new to postgreSQL and I am trying to create a schema file which will contain all the scripts required to create the database and required tables. The way I used to do this for SQl server was to check if the database exists and then run the necessary scripts. 
The following script in postgreSQL throws an error saying, ""CREATE DATABASE cannot be executed from a function or multi-command string""
do $$
begin
If not exists (select 1 from pg_database where datname = 'TestDB')
Then
 CREATE DATABASE ""TestDB"";
end if;
end
$$
I created a postgres database dump file by exporting a backup of the database but that contains,
Drop Database ""TestDB""
Create Database ""TestDB""
which means everytime I run the schema file, the database would be dropped and recreated and it would be a problem if there is data present in the database.
How do I check if the database exists in postgreSQL without having to drop the database and recreate it everytime I run the file please? 
Thanks in Advance 
",<postgresql><postgresql-9.3><pgadmin-4>,986,0,11,313,1,4,21,79,23684,,15,1,15,2018-10-01 11:11,2020-12-16 11:15,,807,,Basic,9
57059458,SQL: Most Overdue pair of numbers?,"We have a this table and random data load: 
CREATE TABLE [dbo].[webscrape](
    [id] [int] IDENTITY(1,1) NOT NULL,
    [date] [date] NULL,
    [value1] [int] NULL,
    [value2] [int] NULL,
    [value3] [int] NULL,
    [value4] [int] NULL,
    [value5] [int] NULL,
    [sumnumbers] AS ([value1]+[value2]+[value3]+[value4]+[value5])
) ON [PRIMARY]
declare @date date = '1990-01-01',
@endDate date = Getdate()
while @date&lt;=@enddate
begin
insert into [dbo].[webscrape](date,value1,value2,value3,value4,value5)
SELECT @date date,FLOOR(RAND()*(36-1)+1) value1,
FLOOR(RAND()*(36-1)+1) value2,
FLOOR(RAND()*(36-1)+1) value3,
FLOOR(RAND()*(36-1)+1) value4,
FLOOR(RAND()*(36-1)+1) value5
set @date = DATEADD(day,1,@date)
end
select * from [dbo].[webscrape] 
In SQL how can we return pair of values that have gone the longest without occurring on a given date?
And (if you happen to know) in Power BI Q&amp;A NLP, how do we map so that so we can ask in natural language ""when have the most overdue pairs occurred?""
Overdue being the pair of numbers with the longest stretch of time since occurring as of the given date. 
UPDATE:  I am trying this very ugly code.  Any ideas: 
  select *
    from (
      select date,value1 number1,value2 number2 from webscrape union all  
      select date,value1,value3 from webscrape union all
      select date,value1,value4 from webscrape union all
      select date,value1,value5 from webscrape union all
      select date,value2,value3 from webscrape union all
      select date,value2,value4 from webscrape union all
      select date,value2,value5 from webscrape union all
      select date,value3,value4 from webscrape union all
      select date,value3,value5 from webscrape union all
      select date,value4,value5 from webscrape 
    ) t order by date
    ----------------------------------
    select t.number1,t.number2, count(*)
     as counter
    from (
      select value1 number1,value2 number2 from webscrape union all  
      select value1,value3 from webscrape union all
      select value1,value4  from webscrape union all
      select value1,value5 from webscrape union all
      select value2,value3 from webscrape union all
      select value2,value4  from webscrape union all
      select value2,value5 from webscrape union all
      select value3,value4  from webscrape union all
      select value3,value5 from webscrape union all
      select value4,value5 from webscrape 
    ) t
group by t.number1,number2
order by counter
Thanks for any help.
",<sql><sql-server><t-sql><nlp><powerbi>,2503,0,63,1667,9,38,73,44,425,0,82,1,15,2019-07-16 14:20,2019-07-19 18:09,2019-07-19 18:09,3,3,Basic,9
54515875,How can I update a table to insert decimal points at a fixed position in numbers?,"I am using Microsoft SQL Server 2014 and have a table with three columns and the field data type is Decimal(38,0).
I want to update each row of my table to insert a decimal point after the first two digits. For example, I want 123456 to become 12.3456. The numbers are different lengths; some are five digits, some are seven digits, etc.
My table is:
+-------------+-------+-------+
| ID          |   X   |   Y   |
+-------------+-------+-------+
| 1200        | 321121| 345000|
| 1201        | 564777| 4145  |
| 1202        | 4567  | 121444|
| 1203        | 12747 | 789887|
| 1204        | 489899| 124778|
+-------------+-------+-------+
And I want to change this to:
+-------------+--------+--------+
| ID          |   X    |   Y    |
+-------------+--------+--------+
| 1200        | 32.1121| 34.5000|
| 1201        | 56.4777| 41.45  |
| 1202        | 45.67  | 12.1444|
| 1203        | 12.747 | 78.9887|
| 1204        | 48.9899| 12.4778|
+-------------+--------+--------+
My code is:
Update [dbo].[UTM]
     SET [X] = STUFF([X],3,0,'.')
         [Y] = STUFF([X],3,0,'.')
And I tried this:
BEGIN
DECLARE @COUNT1 int;
DECLARE @COUNT2 int;
DECLARE @TEMP_X VARCHAR(255);
DECLARE @TEMP_Y VARCHAR(255);
DECLARE @TEMP_main VARCHAR(255);
SELECT @COUNT1 = COUNT(*) FROM [UTM];
SET @COUNT2 = 0;
    WHILE(@COUNT2&lt;@COUNT1)
    BEGIN
        SET @TEMP_main = (SELECT [id] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_X = (SELECT [X] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        SET @TEMP_Y = (SELECT [Y] from [UTM] order by [id] desc offset @COUNT2 rows fetch next 1 rows only);
        UPDATE [dbo].[UTM]
           SET [X] = CONVERT(decimal(38,0),STUFF(@TEMP_X,3,0,'.'))
              ,[Y] = CONVERT(decimal(38,0),STUFF(@TEMP_Y,3,0,'.'))
           WHERE [id] = @TEMP_main;
        SET @COUNT2 = @COUNT2  +  1
    END
END
",<sql><sql-server><numbers><decimal><sql-server-2014>,1904,0,49,492,1,6,21,74,4952,0,126,4,15,2019-02-04 12:13,2019-02-04 12:15,2019-02-04 12:32,0,0,Basic,9
55760416,In OLAP cube wrong Grand Total when attribute is filtered,"A user trying to check the Sales Amount per Salesperson. Sample data:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750
Grand Total:          2250
It looks fine, but we have the following hierarchy Company &gt; Class &gt; Group &gt; Subgroup in the cube and if a user tries to use this hierarchy in filters - Grand Total fails (if any attribute is unchecked in this hierarchy). Sample:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
I've noticed the same problem before when we tried to filter Date attribute, if not every day of the month was selected it shown wrong Grand Total too.
Have you an idea why it happens and how to fix it?
Sales Amount is physical measure (not calculated measure), it is selected from SQL view (the same happens with every fact). 
I've asked the same question here, but nobody could answer it.
I've tried to delete all MDX calculations (scopes), but still Grand Total was incorrect. 
EDIT
I've noticed that the problem occurs when filtering like that:
1 element selected from the first level of the hierarchy, 1 element from 2nd level and 1 element from the 3rd level of hierarchy as in the image above.
If the 3rd level isn't filtered it shows good Grand Total.
EDIT 2
I've tried to trace on SSAS, it returns exactly the same output as in Excel. It generated the following MDX when using Salesperson dimension on the rows:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS, 
NON EMPTY { ([Salesperson].[Salesperson].[Salesperson].ALLMEMBERS ) } 
DIMENSION PROPERTIES MEMBER_CAPTION, 
MEMBER_UNIQUE_NAME ON ROWS FROM ( 
SELECT ( {  [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
            [Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
            [Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } 
) ON COLUMNS FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) 
WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
This MDX generated without Salesperson dimension:
SELECT NON EMPTY { [Measures].[Sales Amount] } ON COLUMNS 
FROM ( SELECT ( { [Item].[Class - Group - Subgroup].[Class].&amp;[XXX]&amp;[1.], 
[Item].[Class - Group - Subgroup].[Group].&amp;[XXX]&amp;[2.]&amp;[2.2.], 
[Item].[Class - Group - Subgroup].[Subgroup].&amp;[XXX]&amp;[2.]&amp;[2.3.]&amp;[2.3.1.] } ) ON COLUMNS 
FROM ( SELECT ( { [Company].[Company].&amp;[XXX] } ) ON COLUMNS 
FROM [Sales])) WHERE ( [Company].[Company].&amp;[XXX], [Item].[Class - Group - Subgroup].CurrentMember ) CELL PROPERTIES VALUE, BACK_COLOR, FORE_COLOR, FORMATTED_VALUE, FORMAT_STRING, FONT_NAME, FONT_SIZE, FONT_FLAGS
I've noticed even if I'm not using any dimension on the rows (in samples above I've used Salesperson dimension) it shows wrong Grand Total.
For example it shows:
Sales Amount 
350
And when using Salesperson dimension on the rows:
Salesperson   Sales Amount    
001                   1000    
002                    500    
003                    750    
Grand Total:           350
",<sql-server><excel><sql-server-2008-r2><pivot-table><ssas>,3329,5,34,848,4,15,42,74,2853,0,68,1,15,2019-04-19 10:27,2019-05-03 11:13,,14,,Advanced,32
64065118,What extra one gets by selecting Azure SQL Managed Instance vis-a-vis Azure SQL DB PaaS,"I would like to know what extra benefits one get by choosing Azure SQL Managed Instance compared to Azure SQL DB PaaS. I know SQL Managed Instance is offered as a vCore based purchasing model only. Apart from this what is the extra add on and benefits that one gets over the other. Any reply would be appreciated.
",<azure><azure-sql-database><azure-sql-managed-instance>,314,0,0,1967,2,24,46,45,7678,,0,3,15,2020-09-25 13:29,2020-09-26 17:43,,1,,Basic,3
60989709,How to convert a class instance to JsonDocument?,"Let's say we have an entity class that looks like this:
public class SerializedEntity
{
    public JsonDocument Payload { get; set; }
    public SerializedEntity(JsonDocument payload)
    {
        Payload = payload;
    }
}
According to npsql this generates a table with column payload of type jsonb for this class which is correct. 
Now what I would like to do is take any class instance and store it as payload in this table e.g.: 
public class Pizza {
    public string Name { get; set; }
    public int Size { get; set; }
}
should then be possible to be retrieved as an object with following structure:
{Name: ""name"", Size: 10}
So I need something like this:
var pizza = new Pizza(""Margharita"", 10);
var se = new SerializedEntity(someConverter.method(pizza))
",<c#><entity-framework><asp.net-core><npgsql>,764,1,19,2506,2,26,39,66,11557,0,257,5,15,2020-04-02 10:43,2020-04-02 11:24,2020-04-02 11:24,0,0,Basic,6
54257933,Calling flush() in @Transactional method in Spring Boot application,"Is it possible that calling Hibernate flush() in the middle of @Transactional method will save incomplete results in the database?
For example, is it possible that this function could save ""John"" to the database?
@Transactional
public void tt() {
    Student s = new Student(""John"");
    em.persist(s);
    em.flush();
    // Perform some calculations that change some attributes of the instance
    s.setName(""Jeff"");
}
I tried it with H2 in-memory database and it didn't save incomplete transaction changes. But is it possible under certain conditions and maybe with another DB engine?
",<java><sql><spring><hibernate><jpa>,588,0,8,501,1,6,29,55,26628,0,146,1,15,2019-01-18 16:28,2019-01-18 17:17,2019-01-18 17:17,0,0,Basic,9
48069425,Converting Between Timezones in Postgres,"I am trying to understand the timestamps and timezones in Postgre. I think I got it, until I red this article.  Focus on the ""Converting Between Timezones"" part. It has two examples. 
(Consider the default timezone configuration to be UTC.)
Example 1
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'); outputs 2015-12-31 16:00:00
According to the article and what I understand, because the '2016-01-01 00:00' part of the timezone function is just a string, it is silently converted to the default UTC. So from '2016-01-01 00:00' UTC it is then converted to US/Pacific as asked by the timezone function, that is 2015-12-31 16:00:00.  
Example 2 
db=# SELECT timezone('US/Pacific', '2016-01-01 00:00'::timestamp); outputs 2016-01-01 08:00:00+00
Excuse me, I dont see why and the explanation there does not help. Ok, the '2016-01-01 00:00'::timestamp part of the timezone function is no longer a string, but an actual timestamp. In what timezone? If it is UTC, the output would have to be the same as the Example 1. So it is automatically converted to US/Pacific? Then the output is in UTC? But why? I asked for a US/Pacific in my timezone  not a UTC.
Please explain how the timezone behaves when gets a timestamp and gets asked to transform it. Thank you.
",<postgresql><timezone><timestamp><timezone-offset><timestamp-with-timezone>,1259,1,14,4312,20,69,130,52,33126,0,468,2,15,2018-01-02 23:51,2018-01-03 17:46,2018-01-16 8:08,1,14,Basic,9
53591359,postgresql: Filter in JSON array,"Let's say we have a table items which has columns name and attributes:
CREATE TABLE students (
  name VARCHAR(100),
  attributes JSON
)
where attributes is an array of (always equally-structured) JSON documents such as
[{""name"":""Attribute 1"",""value"":""Value 1""},{""name"":""Attribute 2"",""value"":""Value 2""}]
I now want to find all students where any attribute value matches something (such as Foo%). Here's a playground example.
I realize that this isn't exactly the most straight-forward design, but for now it's what I have to work with, though performance of such a search being categorically terribly inefficient would of course be a valid concern.
",<sql><postgresql>,648,1,9,19523,7,68,100,73,37808,0,1059,3,15,2018-12-03 10:02,2018-12-03 10:43,2018-12-03 10:43,0,0,Basic,9
49971903,Converting epoch to datetime in PySpark data frame using udf,"I have a PySpark dataframe with this schema:
root
 |-- epoch: double (nullable = true)
 |-- var1: double (nullable = true)
 |-- var2: double (nullable = true)
Where epoch is in seconds and should be converted to date time. In order to do so, I define a user defined function (udf) as follows:
from pyspark.sql.functions import udf    
import time
def epoch_to_datetime(x):
    return time.localtime(x)
    # return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x))
    # return x * 0 + 1
epoch_to_datetime_udf = udf(epoch_to_datetime, DoubleType())
df.withColumn(""datetime"", epoch_to_datetime(df2.epoch)).show()
I get this error:
---&gt; 21     return time.localtime(x)
    22     # return x * 0 + 1
    23 
    TypeError: a float is required
If I simply return x + 1 in the function, it works. Trying float(x) or float(str(x)) or numpy.float(x) in time.localtime(x) does not help and I still get an error. Outside of udf, time.localtime(1.514687216E9) or other numbers works fine. Using datetime package to convert epoch to datetim results in similar errors. 
It seems that time and datetime packages do not like to fed with DoubleType from PySpark. Any ideas how I can solve this issue? Thanks.
",<python><apache-spark><pyspark><apache-spark-sql>,1201,0,29,1350,3,17,31,66,38149,0,506,3,15,2018-04-23 0:14,2018-04-23 3:07,2018-04-23 3:07,0,0,Basic,9
57546833,How to take a dump from Mysql 8.0 into 5.7?,"I would like to take a dump from Mysql 8.0.11 and restore it into 5.7.27.
When I tried to restore it I got the error:
  ERROR 1273 (HY000) at line 25: Unknown collation: 'utf8mb4_0900_ai_ci'
Then I tried to use the compatible flag to make it easier on an older MySQL DB.
mysqldump --compatible=mysql4 --add-drop-table -u r00t -h xxx.eu-north-1.rds.amazonaws.com -p radius_db &gt; ~/radius.sql
But that doesn't seem to work either:
  mysqldump: Couldn't execute '/*!40100 SET @@SQL_MODE='MYSQL40' */':
  Variable 'sql_mode' can't be set to the value of 'MYSQL40' (1231)
Any advice would be appreciated.
",<mysql>,602,0,1,64585,90,280,468,69,15424,0,2772,2,15,2019-08-18 17:26,2019-08-22 10:35,2019-08-22 10:35,4,4,Basic,9
54665992,"Error ""Could not find any index named [IX_MyIndex]"" upon creating it","I came before this very weird error:
  Msg 7999, Level 16, State 9, Line 12
  Could not find any index named 'IX_MyIndex' for table 'dbo.MyTable'.
When running the script to create it!!
CREATE NONCLUSTERED INDEX [IX_MyIndex] ON [dbo].[MyTable] (
    [Field1]
    ,[Field2]
    ) INCLUDE (
    Fields3
    ,Fields4
    ,Fields5
    )
    WITH (
         MAXDOP = 4
         ,DATA_COMPRESSION = PAGE
         ,DROP_EXISTING = ON
        )
What am I missing?
",<sql-server>,456,0,13,3111,4,31,60,53,6255,0,363,2,15,2019-02-13 8:52,2019-02-13 8:58,2019-02-13 8:58,0,0,Basic,9
58976719,How to convert string column type to integer using Laravel migration?,"I am trying to alter the column datatype using Laravel migration. But I am facing following error. Please help me out.
Schema::table('files', function(Blueprint $table) {
    $table-&gt;integer('app_id')-&gt;change();
    $table-&gt;index(['app_id', 'filename']);
});
  SQLSTATE[42000]: Syntax error or access violation: 1064 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci' at line 1 (SQL: ALTER TABLE files CHANGE app_id app_id INT CHARACTER SET utf8 DEFAULT 0 NOT NULL COLLATE utf8_unicode_ci) 
",<php><mysql><laravel>,647,0,6,183,1,2,6,71,11810,,1,2,15,2019-11-21 14:00,2019-11-25 12:42,2019-11-25 12:42,4,4,Basic,9
53537229,MSSQL at Docker exits instantly,"I'm trying to setup MSSQL at Docker at Windows 10, but for some reason it started shutting down my container
I've been using it like that for months, but now I have no idea what's happening
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
    C:\Users\user\
    λ docker login
    Authenticating with existing credentials...
    Login Succeeded
    C:\Users\user\
    λ docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=&lt;YourStrong!Passw0rd&gt;123' -p 1433:1433 --name sql -d mcr.microsoft.com/mssql/server:2017-latest
    337e5efb35f0bf4b465181a0f8be4851b12f353a3a8710ddf817d2f501e5fea
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   3 seconds ago       Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                     PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   6 seconds ago       Exited (1) 2 seconds ago                       sql
    C:\Users\user\
    λ docker start sql
    sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS              PORTS                    NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   14 seconds ago      Up 2 seconds        0.0.0.0:1433-&gt;1433/tcp   sql
    C:\Users\user\
    λ docker ps -a
    CONTAINER ID        IMAGE                                        COMMAND                  CREATED             STATUS                    PORTS               NAMES
    347q5effb3cf0        mcr.microsoft.com/mssql/server:2017-latest   ""/opt/mssql/bin/sqls…""   16 seconds ago      Exited (1) 1 second ago                       sql
  docker logs sql
shows
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
  The SQL Server End-User License Agreement (EULA) must be accepted before SQL
  Server can start. The license terms for this product can be downloaded from
  http://go.microsoft.com/fwlink/?LinkId=746388.
  You can accept the EULA by specifying the --accept-eula command line option,
  setting the ACCEPT_EULA environment variable, or using the mssql-conf tool.
Anybody has an idea what may be wrong?
",<sql-server>,3024,2,36,1821,6,23,65,80,13734,0,141,1,15,2018-11-29 10:47,2018-11-29 13:52,2018-11-29 13:52,0,0,Basic,9
58597639,How to make multiple query in a very short interval / simultaneously,"Hey I'm getting an error message : conn busy from pgx
I don't know how to solve this. Here is my function :
func (r *proverbRepo) SelectPendingProverbs(table string) (proverbs []domain.Proverb, err error) {
    query := fmt.Sprintf(""SELECT id, proverb literal FROM %s"", table)
    rows, err := r.Db.Query(context.Background(), query)
    defer rows.Close()
    if err != nil {
        return
    }
    for rows.Next() {
        var prov domain.Proverb
        if err = rows.Scan(&amp;prov.ID, &amp;prov.Literal); err != nil {
            return
        }
        proverbs = append(proverbs, prov)
    }
    return
}
r.Db is pgx.Connect(context.Background(), os.Getenv(""PSQL_URL""))
I'm fetching two different table in a very short interval from two separate front end requests.
The first request goes through, the other one returns the conn busy error message.
I really don't know what to look for, would somebody help me ?
",<go><psql><pgx>,923,0,22,3691,5,22,38,36,9220,0,63,2,15,2019-10-28 20:01,2019-10-28 21:28,2019-10-28 21:28,0,0,Intermediate,23
50885572,UseSqlite with Entity Framework Core in ASP.NET Core 2.1 not working,"I am starting a Razor pages project in ASP.NET Core 2.1. I am trying to use SQLite but when configuring the database only SQL Server seems to be an option.
Startup.cs
using Microsoft.AspNetCore.Builder;
using Microsoft.AspNetCore.Hosting;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Application.Models;
using Microsoft.EntityFrameworkCore;
namespace Application
{
    public class Startup
    {
        public Startup(IConfiguration configuration)
        {
            Configuration = configuration;
        }
        public IConfiguration Configuration { get; }
        // This method gets called by the runtime. Use this method to add services to the container.
        public void ConfigureServices(IServiceCollection services)
        {
            services.AddDbContext&lt;ApplicationContext&gt;(options =&gt;
               options.UseSqlite(""Data Source=Database.db""));
            services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1);
        }
        // This method gets called by the runtime. Use this method to configure the HTTP request pipeline.
        public void Configure(IApplicationBuilder app, IHostingEnvironment env)
        {
            app.UseStaticFiles();
            app.UseMvc();
        }
    }
}
Intellisense does not recognize options.UseSqlite and builds fail. This was not/ is not an issue with .net core 2.0 projects.
Is this not supported yet? Reading through the documentation makes it seem that it is. I'm not sure what else is going wrong here.
",<c#><sqlite><entity-framework-core>,1593,0,36,467,1,5,20,49,32447,0,21,3,15,2018-06-16 6:37,2018-06-16 7:05,2018-06-16 7:05,0,0,Basic,14
56265904,Reading UUID from result set in Postgres JDBC,"I'm using UUID for my id column, I'm looking for a way to retrieve the data inside my Java application. I can't find a  method in ResultSet for getting a UUID. How would I go about getting the UUID?
",<postgresql><jdbc>,199,0,0,8557,27,108,176,64,8613,0,554,1,15,2019-05-22 22:44,2019-05-23 3:37,2019-05-23 3:37,1,1,Basic,3
60797825,Why mysql explain analyze is not working?,"Besides having mariadb 10.1.36-MariaDB I get following error.
EXPLAIN ANALYZE select 1
MySQL said: Documentation
1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'ANALYZE select 1' at line 1
What additional I need to do here. My PHP version is 7.2.11.
",<php><mysql><mariadb><query-optimization>,346,0,4,153,0,1,4,48,9362,,0,2,15,2020-03-22 9:44,2020-03-22 9:50,2020-03-22 9:50,0,0,Basic,14
64974214,AWS RDS - Access denied to admin user when using GRANT ALL PRIVILEGES ON the_db.* TO 'the_user'@'%',"When we try to GRANT ALL permissions to a user for a specific database, the admin (superuser) user of database receives the following error.
Access denied for user 'admin'@'%' to database 'the_Db'
After looking other questions in stackoverflow I could not find the solution. I already tried to change * -&gt; % without success, that is the approach suggested in the following source:
http://www.fidian.com/problems-only-tyler-has/using-grant-all-with-amazons-mysql-rds
I think there is an underlying configuration on RDS so I can't grant all permissions for the users, but I don't know how to detect what is happening.
Update
After doing some workarounds I noticed that the &quot;Delete versioning rows&quot; permissions is the one that causes the problem. I can add all permissions but that one.
https://mariadb.com/kb/en/grant/
So the only &quot;way&quot; I could grant other permissions was to specific each one of those with a script like this.
GRANT Alter ON *.* TO 'user_some_app'@'%';
GRANT Create ON *.* TO 'user_some_app'@'%';  
GRANT Create view ON *.* TO 'user_some_app'@'%';
GRANT Delete ON *.* TO 'user_some_app'@'%';
GRANT Drop ON *.* TO 'user_some_app'@'%';
GRANT Grant option ON *.* TO 'user_some_app'@'%';
GRANT Index ON *.* TO 'user_some_app'@'%';
GRANT Insert ON *.* TO 'user_some_app'@'%';
GRANT References ON *.* TO 'user_some_app'@'%';
GRANT Select ON *.* TO 'user_some_app'@'%';
GRANT Show view ON *.* TO 'user_some_app'@'%';
GRANT Trigger ON *.* TO 'user_some_app'@'%';
GRANT Update ON *.* TO 'user_some_app'@'%';
GRANT Alter routine ON *.* TO 'user_some_app'@'%';
GRANT Create routine ON *.* TO 'user_some_app'@'%';
GRANT Create temporary tables ON *.* TO 'user_some_app'@'%';
GRANT Execute ON *.* TO 'user_some_app'@'%';
GRANT Lock tables ON *.* TO 'user_some_app'@'%';
",<mysql><amazon-web-services><mariadb><amazon-rds>,1796,4,19,1394,2,12,16,49,14713,0,106,1,15,2020-11-23 18:28,2021-03-02 11:30,2021-03-02 11:30,99,99,Basic,14
48206047,How to return all the columns with flask-sqlalchemy query join from two tables,"I'm trying to do a join from two tables in flask-sqlalchemy and I want all the columns from both tables but if I execute:
Company.query.join(Buyer, Buyer.buyer_id == Company.id).all()
I have only the columns from Company (It returns, in fact, a Company object). 
I know I can do something like:
Company.query.join(Buyer, Buyer.buyer_id == Company.id) \
             .add_columns(Buyer.id, Buyer.name, etc..).all()
It returns in this case:
(&lt;Company 2&gt;, 1, 'S67FG', etc..)
the problem is that I have a lot of columns and also I don't know how to marshmallow the returned obj with flask-marshmallow (with nested fields does not work).
Is there a way to return a new obj with columns from the two tables?
What is for you the best way to manage these situations?
Any suggestion is highly appreciated. Thanks
",<python><sqlalchemy>,810,0,4,713,3,6,20,66,10857,0,78,1,15,2018-01-11 11:31,2018-01-11 11:47,2018-01-11 11:47,0,0,Basic,9
59249332,How to solve 'Cannot authenticate using Kerberos' issue doing EF Core database scaffolding in Linux(Ubuntu 18.04)? Are there any solutions?,"I've been trying to develop a simple AspNetCore application with EntityFrameworkCore to connect and work with the MSSQL server database. And manage all this by Rider IDE, a tool for Database client (DBeaver) and dotnet command line interface(dotnet ef). I'm using the database first approach(create a database on the MSSQL server, fill it with tables, and then build Models based on tables).
My STEP-by-STEP actions:
1)install and set up MSSQL server for my machine working on Ubuntu 18.04. Install the command line tool &quot;SQLCMD&quot;. ///
Link to guide - https://learn.microsoft.com/en-gb/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-ver15
2)locally connected to my MSSQLServer instance.
sqlcmd -S localhost -U SA -P 'MyPasswd'
3)Using Transact-SQL created a Database and installed a DB client (DBeaver) to quickly manage my databases now and in the future.
The next step, as I supposed, was to use tutorials about connecting my Web Application to a database that was found here https://blog.jetbrains.com/dotnet/2017/08/09/running-entity-framework-core-commands-rider/ and here https://www.entityframeworktutorial.net/efcore/create-model-for-existing-database-in-ef-core.aspx
My ASP.NET Core project's package references:
Microsoft.EntityFrameworkCore
Microsoft.EntityFrameworkCore.SqlServer
Microsoft.EntityFrameworkCore.Tools
After typing in the CLI command
dotnet ef dbcontext scaffold &quot;Server=localhost;Database=WebAppDB;Integrated Security=true;&quot; Microsoft.EntityFrameworkCore.SqlServer -c RsvpContext (
to build &quot;RsvpContext&quot; context to connect to my database WebAppDB.)
I see what I see:
Build started...
Build succeeded.
Microsoft.Data.SqlClient.SqlException (0x80131904): **Cannot authenticate using 
Kerberos. Ensure Kerberos has been initialized on the client with 'kinit' and a 
Service Principal Name has been registered for the SQL Server to allow Kerberos 
authentication.**
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
   at Microsoft.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, SqlCredential credential, Object providerInfo, String newPassword, SecureString newSecurePassword, Boolean redirectedUserInstance, SqlConnectionString userConnectionOptions, SessionData reconnectSessionData, Boolean applyTransientFaultHandling, String accessToken, DbConnectionPool pool, SqlAuthenticationProviderManager sqlAuthProviderManager)
   at Microsoft.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, DbConnectionPoolKey poolKey, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnectionPool pool, DbConnection owningObject, DbConnectionOptions options, DbConnectionPoolKey poolKey, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject, DbConnectionOptions userOptions, DbConnectionInternal oldConnection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, UInt32 waitForMultipleObjectsTimeout, Boolean allowCreate, Boolean onlyOneCheckConnection, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionPool.TryGetConnection(DbConnection owningObject, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal&amp; connection)
at Microsoft.Data.ProviderBase.DbConnectionFactory.TryGetConnection(DbConnection owningConnection, TaskCompletionSource`1 retry, DbConnectionOptions userOptions, DbConnectionInternal oldConnection, DbConnectionInternal&amp; connection)
   at Microsoft.Data.ProviderBase.DbConnectionInternal.TryOpenConnectionInternal(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.ProviderBase.DbConnectionClosed.TryOpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory, TaskCompletionSource`1 retry, DbConnectionOptions userOptions)
   at Microsoft.Data.SqlClient.SqlConnection.TryOpen(TaskCompletionSource`1 retry)
   at Microsoft.Data.SqlClient.SqlConnection.Open()
   at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(DbConnection connection, DatabaseModelFactoryOptions options)
at Microsoft.EntityFrameworkCore.SqlServer.Scaffolding.Internal.SqlServerDatabaseModelFactory.Create(String connectionString, DatabaseModelFactoryOptions options)
   at Microsoft.EntityFrameworkCore.Scaffolding.Internal.ReverseEngineerScaffolder.ScaffoldModel(String connectionString, DatabaseModelFactoryOptions databaseOptions, ModelReverseEngineerOptions modelOptions, ModelCodeGenerationOptions codeOptions)
   at Microsoft.EntityFrameworkCore.Design.Internal.DatabaseOperations.ScaffoldContext(String provider, String connectionString, String outputDir, String outputContextDir, String dbContextClassName, IEnumerable`1 schemas, IEnumerable`1 tables, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContextImpl(String provider, String connectionString, String outputDir, String outputDbContextDir, String dbContextClassName, IEnumerable`1 schemaFilters, IEnumerable`1 tableFilters, Boolean useDataAnnotations, Boolean overwriteFiles, Boolean useDatabaseNames)
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.ScaffoldContext.&lt;&gt;c__DisplayClass0_0.&lt;.ctor&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.&lt;&gt;c__DisplayClass3_0`1.&lt;Execute&gt;b__0()
   at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.Execute(Action action)
ClientConnectionId:38f805bc-5879-458b-9256-d6a201d7ce99
Cannot authenticate using Kerberos. Ensure Kerberos has been initialized on the 
client with 'kinit' and a Service Principal Name has been registered for the SQL 
Server to allow Kerberos authentication.
ErrorCode=InternalError, Exception=Interop+NetSecurityNative+GssApiException: 
GSSAPI operation failed with error - Unspecified GSS failure.  Minor code may 
provide more information (SPNEGO cannot find mechanisms to negotiate).
   at System.Net.Security.NegotiateStreamPal.GssInitSecurityContext(SafeGssContextHandle&amp; context, SafeGssCredHandle credential, Boolean isNtlm, SafeGssNameHandle targetName, GssFlags inFlags, Byte[] buffer, Byte[]&amp; outputBuffer, UInt32&amp; outFlags, Int32&amp; isNtlmUsed)
   at System.Net.Security.NegotiateStreamPal.EstablishSecurityContext(SafeFreeNegoCredentials credential, SafeDeleteContext&amp; context, String targetName, ContextFlagsPal inFlags, SecurityBuffer inputBuffer, SecurityBuffer outputBuffer, ContextFlagsPal&amp; outFlags)
   at Microsoft.Data.SqlClient.SNI.SNIProxy.GenSspiClientContext(SspiClientContextStatus sspiClientContextStatus, Byte[] receivedBuff, Byte[]&amp; sendBuff, Byte[] serverName)
   at Microsoft.Data.SqlClient.SNI.TdsParserStateObjectManaged.GenerateSspiClientContext(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength, Byte[] _sniSpnBuffer)
   at Microsoft.Data.SqlClient.TdsParser.SNISSPIData(Byte[] receivedBuff, UInt32 receivedLength, Byte[]&amp; sendBuff, UInt32&amp; sendLength)
If someone, preferably working on Linux, had the same issue, please let me know and share your solutions(guide on what to do in this situation).
",<asp.net-mvc><ubuntu><kerberos><sql-server-2017><ef-core-3.0>,9027,6,78,155,1,1,8,59,15262,,31,2,15,2019-12-09 12:53,2020-06-19 9:45,2020-06-19 9:45,193,193,Basic,9
54030469,Host 'X' is not allowed to connect to this MySQL server,"I wanna deploy MySQL+PHPMyAdmin. My docker-compose.yml:
version: ""3""
services:
  db:
    image: mysql:5.7
    restart: always
    container_name: db
    volumes:
      - ./~mysql:/var/lib/mysql
      - ./mysql.cnf:/etc/mysql/conf.d/my.cnf
    environment:
      MYSQL_DATABASE: ""dbtest""
      MYSQL_ROOT_PASSWORD: ""123456""
      MYSQL_ROOT_HOST: ""%""
    networks:
      - db
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ""mysqladmin ping -h localhost""
      interval: 1s
      timeout: 1s
      retries: 60
  phpmyadmin:
    image: phpmyadmin/phpmyadmin:4.7
    restart: always
    container_name: phpmyadmin
    ports:
      - 8080:80
    networks:
      - external-net
      - db
    environment:
      PMA_HOST: db
    depends_on:
      - db
networks:
  external-net:
    external:
      name: external-net
  db:
    driver: bridge
After some time later I getting subject error. MYSQL_ROOT_HOST don't helped. When I trying to connect to mysql from db-container:
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
I really don't know what to do with this magic... Thx.
",<mysql><docker><docker-compose>,1154,0,43,365,2,3,12,35,36082,0,18,5,15,2019-01-03 22:09,2019-02-07 19:36,,35,,Basic,14
53952383,add condition to mysql json_arrayagg function,"I have a json query that gives me json of a joined table of person and pets:
SELECT json_object(
  'personId', p.id,
  'pets', json_arrayagg(json_object(
    'petId', pt.id,
    'petName', pt.name
  ))
  )
FROM person p LEFT JOIN pets pt
ON p.id = pt.person_id
GROUP BY p.id;
my issue is that person can have 0 or more pets, and when a person have 0 pets I get list with 1 empty pet, and what I would like to get in that case is empty list.
this is what I get:
{
  ""personId"": 1,
  ""pets"": [
    {
      ""petId"": null,
      ""petName"": """"
    }
  ]
}
and I need:
{
  ""personId"": 1,
  ""pets"": []
}
is that possible?
",<mysql>,615,0,23,1428,3,17,34,80,10295,,44,3,15,2018-12-28 0:32,2018-12-28 0:50,2018-12-28 0:50,0,0,Basic,2
49557144,How to extract values from a numeric-keyed nested JSON field in MySQL,"I have a MySQL table with a JSON column called sent. The entries in the column have information like below:
{
 ""data"": {
  ""12"":""1920293""
 }
}
I'm trying to use the mysql query:
select sent-&gt;""$.data.12"" from mytable
but I get an exception: 
Invalid JSON path expression. The error is around character position 9.
Any idea How I can extract the information? The query works fine for non-numeric subfields.
",<mysql><json><mysql-json>,408,0,8,1168,0,16,31,42,4434,0,539,1,15,2018-03-29 13:18,2018-03-29 15:06,2018-03-29 15:06,0,0,Basic,2
59437636,Sequelize find in JSON field,"I have PostgreSQL database with JSON type field named ""data"" that has following content structure:
{
  ""requestData"" : {
    ""url"": ""some url""
    ""body"": {
      ""page_id"": 12
    }
  }
}
I try to make findAll query request with filter by page_id using Sequelize, but don't get some results. 
The question is: could I search by nested field in JSON type, or only in JSONB type? And how?
",<javascript><json><postgresql><sequelize.js>,388,0,10,349,2,4,11,81,18023,,9,1,15,2019-12-21 15:49,2019-12-22 19:00,2019-12-22 19:00,1,1,Basic,3
53704187,Connecting to an Azure database using SQLAlchemy in Python,"I am trying to connect to an Azure database using SQLAlchemy in Python.
My code is the following:
engine_azure = \
create_engine('mssql+pyodbc://{Server admin login}:{password}@{Server name}.database.windows.net:1433/{AdventureWorksLT}', echo=True)
I get the following message:
C:\ProgramData\Anaconda3\lib\site-packages\sqlalchemy\connectors\pyodbc.py:92: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Then I run the following code:
print(engine_azure.table_names())
I get the following message:
DBAPIError: (pyodbc.Error) ('01S00', '[01S00] [Microsoft][ODBC Driver Manager] Invalid connection string attribute (0) (SQLDriverConnect)')
",<python><sql-server><azure><sqlalchemy>,718,1,6,4701,18,78,143,63,33052,0,35,6,15,2018-12-10 10:54,2018-12-11 2:33,,1,,Basic,3
48171611,difference between pandas read sql query and read sql table,"Is there a difference in relation to time execution between this two commands :
import pandas as pd
df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
Thank you for your help 
",<python><sql><pandas><dataframe><sqlite>,208,0,4,674,1,7,18,52,22574,0,9,4,15,2018-01-09 15:33,2018-01-09 15:49,2018-01-09 15:49,0,0,Intermediate,19
50069427,Create Unique constraint for 'true' only in EF Core,"I have a class for tracking attachments to a Record. Each Record can have multiple RecordAttachments, but there is a requirement that there can only be one RecordAttachment per-Record that is marked as IsPrimary. 
public class RecordAttachment
{
    public int Id { get; set; }
    public int RecordId { get; set; }
    public string Details { get; set; }
    public bool IsPrimary { get; set; }
    public Record Record { get; set; }
}
I can't just use .HasIndex(e =&gt; new { e.RecordId, e.IsPrimary }).IsUnique(true) because there can be multiple false values per Record.
Basically I need a unique constraint on RecordId and IsPrimary == true, although this didn't work:
entity.HasIndex(e =&gt; new { e.RecordId, IsPrimary = (e.IsPrimary == true) }).IsUnique(true)
Edit:
Looking at answers like this: Unique Constraint for Bit Column Allowing Only 1 True (1) Value it appears this would be possible creating the constraint directly with SQL, but then it wouldn't be reflected in my Model.
",<c#><sql-server><entity-framework-core>,992,1,15,3322,2,29,58,73,7529,0,341,1,15,2018-04-27 19:53,2018-04-27 21:07,2018-04-27 21:07,0,0,Intermediate,19
52603131,How to optimize partitioning when migrating data from JDBC source?,"I am trying to move data from a table in PostgreSQL table to a Hive table on HDFS. To do that, I came up with the following code:
  val conf  = new SparkConf().setAppName(""Spark-JDBC"").set(""spark.executor.heartbeatInterval"",""120s"").set(""spark.network.timeout"",""12000s"").set(""spark.sql.inMemoryColumnarStorage.compressed"", ""true"").set(""spark.sql.orc.filterPushdown"",""true"").set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"").set(""spark.kryoserializer.buffer.max"",""512m"").set(""spark.serializer"", classOf[org.apache.spark.serializer.KryoSerializer].getName).set(""spark.streaming.stopGracefullyOnShutdown"",""true"").set(""spark.yarn.driver.memoryOverhead"",""7168"").set(""spark.yarn.executor.memoryOverhead"",""7168"").set(""spark.sql.shuffle.partitions"", ""61"").set(""spark.default.parallelism"", ""60"").set(""spark.memory.storageFraction"",""0.5"").set(""spark.memory.fraction"",""0.6"").set(""spark.memory.offHeap.enabled"",""true"").set(""spark.memory.offHeap.size"",""16g"").set(""spark.dynamicAllocation.enabled"", ""false"").set(""spark.dynamicAllocation.enabled"",""true"").set(""spark.shuffle.service.enabled"",""true"")
  val spark = SparkSession.builder().config(conf).master(""yarn"").enableHiveSupport().config(""hive.exec.dynamic.partition"", ""true"").config(""hive.exec.dynamic.partition.mode"", ""nonstrict"").getOrCreate()
  def prepareFinalDF(splitColumns:List[String], textList: ListBuffer[String], allColumns:String, dataMapper:Map[String, String], partition_columns:Array[String], spark:SparkSession): DataFrame = {
        val colList                = allColumns.split("","").toList
        val (partCols, npartCols)  = colList.partition(p =&gt; partition_columns.contains(p.takeWhile(x =&gt; x != ' ')))
        val queryCols              = npartCols.mkString("","") + "", 0 as "" + flagCol + "","" + partCols.reverse.mkString("","")
        val execQuery              = s""select ${allColumns}, 0 as ${flagCol} from schema.tablename where period_year='2017' and period_num='12'""
        val yearDF                 = spark.read.format(""jdbc"").option(""url"", connectionUrl).option(""dbtable"", s""(${execQuery}) as year2017"")
                                                                      .option(""user"", devUserName).option(""password"", devPassword)
                                                                      .option(""partitionColumn"",""cast_id"")
                                                                      .option(""lowerBound"", 1).option(""upperBound"", 100000)
                                                                      .option(""numPartitions"",70).load()
        val totalCols:List[String] = splitColumns ++ textList
        val cdt                    = new ChangeDataTypes(totalCols, dataMapper)
        hiveDataTypes              = cdt.gpDetails()
        val fc                     = prepareHiveTableSchema(hiveDataTypes, partition_columns)
        val allColsOrdered         = yearDF.columns.diff(partition_columns) ++ partition_columns
        val allCols                = allColsOrdered.map(colname =&gt; org.apache.spark.sql.functions.col(colname))
        val resultDF               = yearDF.select(allCols:_*)
        val stringColumns          = resultDF.schema.fields.filter(x =&gt; x.dataType == StringType).map(s =&gt; s.name)
        val finalDF                = stringColumns.foldLeft(resultDF) {
          (tempDF, colName) =&gt; tempDF.withColumn(colName, regexp_replace(regexp_replace(col(colName), ""[\r\n]+"", "" ""), ""[\t]+"","" ""))
        }
        finalDF
  }
    val dataDF = prepareFinalDF(splitColumns, textList, allColumns, dataMapper, partition_columns, spark)
    val dataDFPart = dataDF.repartition(30)
    dataDFPart.createOrReplaceTempView(""preparedDF"")
    spark.sql(""set hive.exec.dynamic.partition.mode=nonstrict"")
    spark.sql(""set hive.exec.dynamic.partition=true"")
    spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"")
The data is inserted into the hive table dynamically partitioned based on prtn_String_columns: source_system_name, period_year, period_num
Spark-submit used:
SPARK_MAJOR_VERSION=2 spark-submit --conf spark.ui.port=4090 --driver-class-path /home/fdlhdpetl/jars/postgresql-42.1.4.jar  --jars /home/fdlhdpetl/jars/postgresql-42.1.4.jar --num-executors 80 --executor-cores 5 --executor-memory 50G --driver-memory 20G --driver-cores 3 --class com.partition.source.YearPartition splinter_2.11-0.1.jar --master=yarn --deploy-mode=cluster --keytab /home/fdlhdpetl/fdlhdpetl.keytab --principal fdlhdpetl@FDLDEV.COM --files /usr/hdp/current/spark2-client/conf/hive-site.xml,testconnection.properties --name Splinter --conf spark.executor.extraClassPath=/home/fdlhdpetl/jars/postgresql-42.1.4.jar
The following error messages are generated in the executor logs:
Container exited with a non-zero exit code 143.
Killed by external signal
18/10/03 15:37:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[SIGTERM handler,9,system]
java.lang.OutOfMemoryError: Java heap space
    at java.util.zip.InflaterInputStream.&lt;init&gt;(InflaterInputStream.java:88)
    at java.util.zip.ZipFile$ZipFileInflaterInputStream.&lt;init&gt;(ZipFile.java:393)
    at java.util.zip.ZipFile.getInputStream(ZipFile.java:374)
    at java.util.jar.JarFile.getManifestFromReference(JarFile.java:199)
    at java.util.jar.JarFile.getManifest(JarFile.java:180)
    at sun.misc.URLClassPath$JarLoader$2.getManifest(URLClassPath.java:944)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:450)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:99)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:745)
I see in the logs that the read is being executed properly with the given number of partitions as below:
Scan JDBCRelation((select column_names from schema.tablename where period_year='2017' and period_num='12') as year2017) [numPartitions=50]
Below is the state of executors in stages:
The data is not being partitioned properly. One partition is smaller while the other one becomes huge. There is a skew problem here.
While inserting the data into Hive table the job fails at the line:spark.sql(s""INSERT OVERWRITE TABLE schema.hivetable PARTITION(${prtn_String_columns}) select * from preparedDF"") but I understand this is happening because of the data skew problem.
I tried to increase number of executors, increasing the executor memory, driver memory, tried to just save as csv file instead of saving the dataframe into a Hive table but nothing affects the execution from giving the exception:
java.lang.OutOfMemoryError: GC overhead limit exceeded
Is there anything in the code that I need to correct ? Could anyone let me know how can I fix this problem ?
",<apache-spark><jdbc><hive><apache-spark-sql><partitioning>,7269,4,58,2009,9,60,136,41,14545,0,193,3,15,2018-10-02 6:38,2018-10-06 10:20,2018-10-06 10:20,4,4,Intermediate,23
57646511,What is the use of __table_args__ = {'extend_existing': True} in SQLAlchemy?,"I have a flask application which I'm trying to convert into Django. In one of the models which inherit an abstract base model, it is mentioned as
__table_args__ = {'extend_existing': True}
Can someone please explain what this means in SQLAlchemy with a small example.
I have gone through few articles but as I worked on Django and new to Flask-SQLAlchemy I couldn't understand it properly.
https://hackersandslackers.com/manage-database-models-with-flask-sqlalchemy/
https://docs.sqlalchemy.org/en/13/orm/extensions/declarative/table_config.html
",<python><django><sqlalchemy>,546,4,1,4832,9,43,89,44,10524,,1152,1,15,2019-08-25 13:39,2021-12-13 4:30,2021-12-13 4:30,841,841,Basic,4
63282351,Django and Azure SQL key error 'deferrable' when start migrate command,"I try to connect Django to Azure SQL and have the error KeyError: deferrable when I start to migrate command.
I can't find a resolution for this issue.
I use this application:
asgiref==3.2.10
Django==3.1
django-mssql-backend==2.8.1
pyodbc==4.0.30
pytz==2020.1
sqlparse==0.3.1
and this is my config in settings.py:
DATABASES = {
    'default': {
        'ENGINE': 'sql_server.pyodbc',
        'NAME': 'DBNAME',
        'USER': 'DBUSER',
        'PASSWORD': 'PASSWORD',
        'HOST': 'databasename.database.windows.net',
        'PORT': '1433',
        'OPTIONS': {
            'driver': 'ODBC Driver 17 for SQL Server',
        },
    },
}
The error is when i try to run 'manage.py migrate`. Everything runs fine until the 8th step. Here's the output:
(venv) C:\Users\...\...\&gt;python manage.py migrate     
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, sessions
Running migrations:
  Applying contenttypes.0001_initial... OK
  Applying auth.0001_initial... OK
  Applying admin.0001_initial... OK
  Applying admin.0002_logentry_remove_auto_add... OK
  Applying admin.0003_logentry_add_action_flag_choices... OK
  Applying contenttypes.0002_remove_content_type_name... OK
  Applying auth.0002_alter_permission_name_max_length... OK
  Applying auth.0003_alter_user_email_max_length... OK
  Applying auth.0004_alter_user_username_opts... OK
  Applying auth.0005_alter_user_last_login_null... OK
  Applying auth.0006_require_contenttypes_0002... OK
  Applying auth.0007_alter_validators_add_error_messages... OK
  Applying auth.0008_alter_user_username_max_length...Traceback (most recent call last):
  File &quot;manage.py&quot;, line 22, in &lt;module&gt;
    main()
  File &quot;manage.py&quot;, line 18, in main
    execute_from_command_line(sys.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 401, in execute_from_command_line
    utility.execute()
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\__init__.py&quot;, line 395, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 330, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 371, in execute
    output = self.handle(*args, **options)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\base.py&quot;, line 85, in wrapped
    res = handle_func(*args, **kwargs)
  File &quot;C:\Users\...\...\lib\site-packages\django\core\management\commands\migrate.py&quot;, line 243, in handle
    post_migrate_state = executor.migrate(
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 117, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 147, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\executor.py&quot;, line 227, in apply_migration
    state = migration.apply(state, schema_editor)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\migration.py&quot;, line 124, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)    
  File &quot;C:\Users\...\...\lib\site-packages\django\db\migrations\operations\fields.py&quot;, line 236, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\base\schema.py&quot;, line 571, in alter_field
    self._alter_field(model, old_field, new_field, old_type, new_type,
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 479, in _alter_field
    self.execute(self._create_unique_sql(model, columns=[old_field.column]))
  File &quot;C:\Users\...\...\lib\site-packages\sql_server\pyodbc\schema.py&quot;, line 861, in execute
    sql = str(sql)
  File &quot;C:\Users\...\...\lib\site-packages\django\db\backends\ddl_references.py&quot;, line 200, in __str__
    return self.template % self.parts
KeyError: 'deferrable'
(venv) C:\Users\...\...&gt;
but if I connect to DB I see a table created a table from Django.
please help
Thanks!
",<python><django><azure><django-models><azure-sql-database>,4477,0,74,176,0,1,6,55,4415,0,1,2,15,2020-08-06 11:12,2020-08-06 18:52,,0,,Basic,3
58628889,ASP.NET Core Testing - get NullReferenceException when initializing InMemory SQLite dbcontext in fixture,"I have a test fixture in which I initialize my SQLite in-memory dbcontext, shown below:
public static MYAPPDBContext Create()
{
    var options = new DbContextOptionsBuilder&lt;MYAPPDBContext&gt;()
                    .UseSqlite(""DataSource=:memory:"")
                    .Options;
    var context = new MYAPPDBContext(options);
    context.Database.OpenConnection(); // this is where exception is thrown
    context.Database.EnsureCreated();
    return context;
}
When I call the Create() method, I get the following NullReferenceException:
System.NullReferenceException
  HResult=0x80004003
  Message=Object reference not set to an instance of an object.
  Source=Microsoft.Data.Sqlite
  StackTrace:
   at Microsoft.Data.Sqlite.SqliteConnection.Open()
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.Sqlite.Storage.Internal.SqliteRelationalConnection.Open(Boolean errorsExpected)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.&lt;&gt;c.&lt;OpenConnection&gt;b__15_0(DatabaseFacade database)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, Func`2 operation, Func`2 verifySucceeded, TState state)
   at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation)
   at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.OpenConnection(DatabaseFacade databaseFacade)
   at MYAPPPlus.UnitTests.TestInfrastructure.MYAPPContextFactory.Create() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\MYAPPContextFactory.cs:line 26
   at MYAPPPlus.UnitTests.TestInfrastructure.QueryTestFixture..ctor() in C:\websites\MYAPPPremier\tests\MYAPPPlus.UnitTests\TestInfrastructure\QueryTestFixture.cs:line 24
Any ideas on what might be happening?
FYI: I'm basing my code on the blog post at https://garywoodfine.com/entity-framework-core-memory-testing-database/, among other resources. 
Also, my fixture works just fine when using basic ef core inmemory database.
",<c#><sqlite><asp.net-core><xunit><in-memory-database>,2250,2,27,816,1,11,21,36,6548,0,360,4,15,2019-10-30 15:34,2019-10-30 16:12,2019-10-30 20:27,0,0,Basic,12
52283751,Calculating percentage of total count for groupBy using pyspark,"I have the following code in pyspark, resulting in a table showing me the different values for a column and their counts. I want to have another column showing what percentage of the total count does each row represent. How do I do that?
difrgns = (df1
           .groupBy(""column_name"")
           .count()
           .sort(desc(""count""))
           .show())
Thanks in advance!
",<apache-spark><pyspark><apache-spark-sql>,379,0,5,353,2,3,10,45,31200,0,5,4,15,2018-09-11 20:27,2018-09-11 22:44,2018-09-11 22:44,0,0,Basic,2
59437429,"Error ""collection was modified enumeration operation may not execute"" when restoring database backup in Azure Data Studio","I'm extremely new to databases so please bear with me. 
I've set up local SQL Server running on a Docker container (using a Mac). I'm trying to restore SQL database using Azure Data Studio (v1.14.0) but it's not working. 
I used the guide on database.guide but keep getting errors. I have no clue what it means.
  Restore database failed: collection was modified; enumeration
  operation may not execute
I have tried restoring .bak-file from a backup made on my school computer (used SQL Server Management Studio on a PC), tried restoring with the bak-file from Database.guide. I also made a backup from my current DB in Azure and tried restoring that one - didn't work either. 
",<sql-server><azure><docker>,679,1,0,421,0,4,9,49,3747,0,0,1,15,2019-12-21 15:25,2019-12-22 11:07,2019-12-22 11:07,1,1,Basic,14
55288840,get parents and children of tree folder structure in my sql < 8 and no CTEs,"I have a folder table that joins to itself on an id, parent_id relationship:
CREATE TABLE folders (
  id int(10) unsigned NOT NULL AUTO_INCREMENT,
  title nvarchar(255) NOT NULL,
  parent_id int(10) unsigned DEFAULT NULL,
  PRIMARY KEY (id)
);
INSERT INTO folders(id, title, parent_id) VALUES(1, 'root', null);
INSERT INTO folders(id, title, parent_id) values(2, 'one', 1);
INSERT INTO folders(id, title, parent_id) values(3, 'target', 2);
INSERT INTO folders(id, title, parent_id) values(4, 'child one', 3);
INSERT INTO folders(id, title, parent_id) values(5, 'child two', 3);
INSERT INTO folders(id, title, parent_id) values(6, 'root 2', null);
INSERT INTO folders(id, title, parent_id) values(7, 'other child one', 6);
INSERT INTO folders(id, title, parent_id) values(8, 'other child two', 6);
I want a query that returns all the parents of that record, right back to the route and any children.
So if I ask for folder with id=3, I get records: 1, 2, 3, 4, 5. I am stuck how to get the parents.
The version of MYSQL is 5.7 and there are no immediate plans to upgrade so sadly CTEs are not an option.
I have created this sql fiddle
",<mysql><sql><hierarchical-data><recursive-query>,1134,1,19,27236,60,241,456,35,5198,0,526,8,15,2019-03-21 20:34,2019-03-21 21:30,,0,,Intermediate,17
55248938,TypeORM and Postgres competing naming styles,"We are using TypeORM and Postgresql and I'm curious about naming conventions.
Given that there are perfectly appropriate styles and naming conventions for databases that are separate from the perfectly good ones used for Javascript, is it considered better practice to force databases to use the code convention or to force code to use the database convention, or to translate everything?
For example:
It's common practice to use the SQl style defined in Joe Celko's SQL Programming Style for the database. This advocates for snake_case for the column names.
It's also common practice to name variables in camelCase when programming in JavaScript and all the documentation on typeorm.
So, when these two worlds collide, is it best practice to force one to the other or to translate every multi-word entity in the definitions to do the mapping.
This isn't really a question of how to do that but rather if there is a common practice one way of the other.
The three possibilities for a column representing User Id are:
1: Translate everything
@Column( { name: user_id } )
userId: number;
2: Use the database convention in the code
@Column()
user_id: number;
3: Use the coding convention in the database
@Column()
userId: number
",<postgresql><typeorm>,1226,1,11,369,1,2,9,40,15065,0,11,1,15,2019-03-19 19:46,2019-12-16 12:26,2019-12-16 12:26,272,272,Intermediate,20
49149511,Is Microsoft Sync Framework alive?,"According to the MS documentation Sync Framework Toolkit (https://code.msdn.microsoft.com/Sync-Framework-Toolkit-4dc10f0e) is a legacy open source product which MS no longer support:
https://msdn.microsoft.com/en-us/library/jj839436(v=sql.110).aspx
That's fine, but how about Microsoft Sync SDK which is not open source? 
Does it mean that open source part useless because server part can be removed by MS in the future?
The question is does it mean that Sync Framework SDK (Server side library) is dead? (Green Part)
",<c#><sql-server><synchronization><microsoft-sync-framework>,518,5,0,8903,4,43,73,67,14769,,908,3,15,2018-03-07 10:24,2019-01-30 6:58,,329,,Intermediate,20
58722202,What are the different use cases for using QueryBuilder vs. Repository in TypeORM?,"I'm building an API using NestJS with TypeORM. I've been querying a MySQL database using the TypeORM Repository API mostly because the NestJS Database documentation section provided an example using this.photoRepository.find(). As I get further along, I've noticed many of my exploratory search results recommending using the TypeORM QueryBuilder API for performance and flexibility reasons.
I'm getting the sense that the Repository approach is easier to use for simple needs and a great abstraction if I ever decide to switch my database framework. On the other hand, it also seems to me that QueryBuilder is more performant and customizable.
Could we outline the different use cases for QueryBuilder vs. Repository in TypeORM?
",<mysql><database><nestjs><typeorm>,730,3,1,173,0,1,10,53,7889,0,9,1,15,2019-11-06 2:22,2019-11-06 8:35,2019-11-06 8:35,0,0,Intermediate,20
59624695,Entity Framework Core 3.1 Return value (int) from stored procedure,"this returns -1, how can i get the actual return value from stored procedure?
here is my stored procedure
ALTER PROCEDURE [Production].[Select_TicketQuantity]
    @Ticket NVARCHAR(25),
    @Reference NVARCHAR(20)
AS
BEGIN
    declare @SQL nvarchar (4000)
    SET @SQL = 'select QARTCOL as Quantidade from D805DATPOR.GCARCCR1 where NCOLGIA = ' + @Ticket + ' AND NARTCOM = ''' + @Reference + ''''
    SET @SQL = N'select CONVERT(int,Quantidade) as Quantidade from OpenQuery(MACPAC, ''' + REPLACE(@SQL, '''', '''''') + ''')'
    PRINT @SQL
    EXEC (@SQL)
END   
C# code
int? quantity= 0;
try
{
    quantity= await _context.Database.ExecuteSqlRawAsync(""EXEC Production.Select_TicketQuantity @p0, @p1"", parameters: new[] { ticket, reference});
}
catch (Exception ex)
{
    _logger.LogError($""{ex}"");
    return RedirectToPage(""Index"");
}
",<c#><sql-server><entity-framework-core-3.1>,834,0,23,3390,4,35,80,75,27905,0,366,7,15,2020-01-07 8:30,2020-01-07 9:57,,0,,Basic,10
62828259,Laravel Slow queries,"public function delete( ReportDetailRequest $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = CustomerInfo::find($id);
        $customerRecord-&gt;delete();
    }
I currently have the above in a laravel application where a DELETE request is sent to this controller. At the moment, as you can see, its very simple, but the query seems super slow. It comes back in postman in 2.23 seconds. What should I try to speed this up? The database layer (mysql) does have an index on ID from what I can tell and the application isn't running in debug. Is this typical?
edit:
Good thinking that the request validation may be doing something (it is validating that this user has auth to delete).
class ReportDetailRequest extends FormRequest
{
    /**
     * Determine if the user is authorized to make this request.
     *
     * @return bool
     */
    public function authorize()
    {
        $id = (int)$this-&gt;route('id');
        $customerInfo = CustomerInfo::find($id)-&gt;first();
        $company = $customerInfo-&gt;company_id;
        return (auth()-&gt;user()-&gt;company-&gt;id  == $company );
    }
    /**
     * Get the validation rules that apply to the request.
     *
     * @return array
     */
    public function rules()
    {
        return [
            //
        ];
    }
}
Show create table:
CREATE TABLE &quot;customer_info&quot; (
  &quot;id&quot; int(11) NOT NULL AUTO_INCREMENT,
  &quot;user_id&quot; int(11) DEFAULT NULL,
  &quot;report_guid&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  &quot;customer_email&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;created_at&quot; timestamp NULL DEFAULT NULL,
  &quot;updated_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_read&quot; tinyint(1) NOT NULL,
  &quot;customer_name&quot; varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;customer_support_issue&quot; longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  &quot;company_id&quot; int(11) NOT NULL,
  &quot;archived&quot; tinyint(1) NOT NULL,
  &quot;archived_at&quot; timestamp NULL DEFAULT NULL,
  &quot;report_active&quot; tinyint(4) DEFAULT NULL,
  &quot;customer_screenshot&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  &quot;video_url&quot; varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (&quot;id&quot;),
  KEY &quot;indexReportLookup&quot; (&quot;report_guid&quot;),
  KEY &quot;guid&quot; (&quot;report_guid&quot;),
  KEY &quot;customer_info_id_index&quot; (&quot;id&quot;)
)
Baseline:
 public function delete( Request $request )
    {
        // $id = (int)$request-&gt;id;
        // $customerRecord = CustomerInfo::find($id);
        // $foo_sql = $customerRecord-&gt;delete()-&gt;toSql();
        // echo($foo_sql);
        return 'test';
        //$customerRecord-&gt;delete();
    }
Ok so a brand new table, with a brand new Request. with a single ID in it, looks like this:
The controller looks like:
public function deleteTest( Request $request )
    {
        $id = (int)$request-&gt;id;
        $customerRecord = NewTable::where('id', '=', $id)-&gt;first();
        $customerRecord-&gt;delete();
        return response(null, 200);
    }
Postman version is :Version 7.27.1 (7.27.1)
1630 ms. WTF. 1.6 seconds for a simple request on a new table.
EXPLAIN DELETE:
1   DELETE  new_tables      range   PRIMARY PRIMARY 8   const   1   100 Using where
EXPLAIN SELECT
1   SIMPLE  new_tables      const   PRIMARY PRIMARY 8   const   1   100 Using index
MYSQL version   8.0.18
innodb_version  8.0.18
So now to add to the fun.
A framework free PHP file. Simple GET request. 100ms.
&lt;?php
echo('tester');
?&gt;
Edit. Just to reiterate.
A Laravel GET method (with authentication) returning test, returns 1.6s.
A no framework &quot;sample.php&quot; file returns in 100ms.
A Laravel GET method (without authentication) returning test, returns in 430ms.
A Laravel GET method (without authentication but with DB access), returns in 1483ms.
It look like there is indeed something holding up requests once the application starts using the database.
Route::middleware('auth:api')-&gt;get('/test1','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest1.api');
Route::middleware('auth:api')-&gt;get('/test2','Api\NewTableController@index')-&gt;name('report.deleteTest2.api');
Route::get('/test3','Api\CustomerInfoController@deleteTest')-&gt;name('report.deleteTest3.api');
Route::get('/test4','Api\NewTableController@index')-&gt;name('report.deleteTest4.api');
 Route::get('/test5','Api\NewTableController@dbTest')-&gt;name('report.deleteTest5.api');
NewTableController:
&lt;?php
namespace App\Http\Controllers\Api;
class NewTableController extends Controller
{
    public function index()
    {
        return &quot;test2&quot;;
    }
}
CustomerInfoController ( with some stuff removed, but method is pretty similar conceptually to NewTableController albeit with some dependency injection going on ).
&lt;?php
namespace App\Http\Controllers\Api;
use App\Http\Controllers\Controller;
use Illuminate\Http\Request;
use App\Http\Requests\ReportDetailRequest;
use App\Services\CustomerInfoService;
use Auth;
use App\LookupParent;
use App\LookupChild;
use App\CustomerInfo;
use App\Http\Resources\CustomerInfoResourceCollection;
use App\Http\Resources\CustomerInfoResource;
use App\Http\Resources\CustomerInfoResourceDetail;
use Carbon\Carbon;
use App\NewTable;
class CustomerInfoController extends Controller
{
    protected $customerInfoService;
    public function __construct(
        CustomerInfoService $customerInfoService
        )
    {
        $this-&gt;customerInfoService = $customerInfoService;
    }
    public function deleteTest()
    {
        return 'deleteTest';
    }
   public function dbTest()
   {   
     tap(NewTable::find(1))-&gt;delete();
   }
}
Results:
/test1 (with authentication 1380ms)
/test2 (with authentication 1320ms)
/test3 (without authentication 112ms)
/test4 (without authentication 124ms)
/test5 (db without authentication 1483ms)
In other words, authentication talks to the database as does a simple delete query without authentication. These take at least a second to complete each. This leads to the roughly two second request mentioned above which has both elements (authentication and database access).
Edit. For those of you reading from Google. Problem was to do with the managed database provided by Digital Ocean. Setup a localised database on MySQL on the same box, and problem resolved itself. think it was either latency from datacenters between web server and database across the world somewhere or a misconfiguration on the part of the db admins at DigitalOcean. Resolved myself, problem wasn't Laravel.
",<mysql><laravel><performance>,6782,4,144,4349,6,51,89,76,9288,0,182,5,15,2020-07-10 6:09,2020-07-21 12:53,,11,,Intermediate,23
59654712,Invalid value for key 'authentication',"I have a .NET Core 3.0 app where am trying to connect to a Azure SQL database using EF Core and Active directory integrated authentication.
I have verified that I have access to this database from my machine as I can connect to it just fine using SQL server management studio and 'Azure Active Directory-Integrated' authentication.
However, when I try to read data in my app (using EF Core), I always get a System.Argument exception with the following statement:
Invalid value for key 'authentication'
Exception details point to the Db connection string.
So, here is my connection string from my dev appsettings.json file:
&quot;ConnectionStrings&quot;: {
&quot;MCDB&quot;: &quot;Server=tcp:dev-media-center-sql.database.windows.net,1433;Initial
Catalog=MediaCenter;Persist Security Info=False;User
ID={my User ID};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Authentication=Active Directory Integrated;&quot;   },
I have tried hard coding the connection string directly in my code, thinking that there might be a problem with my JSON but I still get the same exception.
Is &quot;Active Directory Integrated&quot; not a valid value for the 'Authentication' keyword? If not, what is it then? I have already tried &quot;ActiveDirectoryIntegrated&quot; (with no spaces) and /&quot;Active Directory Integrated&quot;/ (escaping double quotes) but to no avail.
",<json><azure><.net-core><azure-active-directory><azure-sql-database>,1383,0,0,735,2,7,16,52,14531,0,43,6,15,2020-01-08 22:30,2020-09-25 23:43,,261,,Basic,2
59849262,Postgresql full text search with TypeOrm,"There is some way to handle full text search with Postgres and TypeOrm. I've seen some examples but they only work with Mysql. How can I get the equivalent of this but with Postgresql?
@Entity()
export class User {
    @PrimaryGeneratedColumn()
    id: string;
    @Index({ fulltext: true })
    @Column(""varchar"")
    name: string;
}
And use query builder:
const searchTerm = ""John"";
const result = await connection.manager.getRepository(User)
            .createQueryBuilder()
            .select()
            .where(`MATCH(name) AGAINST ('${searchTerm}' IN BOOLEAN MODE)`)
            .getMany();
",<postgresql><typeorm>,601,0,17,795,2,10,30,78,18538,0,256,3,15,2020-01-21 21:09,2020-01-23 5:14,2020-01-23 5:14,2,2,Basic,3
48305081,Spark' Dataset unpersist behaviour,"Recently I saw some strange behaviour of Spark.
I have a pipeline in my application in which I'm manipulating one big Dataset - pseudocode:
val data = spark.read (...)
data.join(df1, ""key"") //etc, more transformations
data.cache(); // used to not recalculate data after save
data.write.parquet() // some save
val extension = data.join (..) // more transformations - joins, selects, etc.
extension.cache(); // again, cache to not double calculations
extension.count();
// (1)
extension.write.csv() // some other save
extension.groupBy(""key"").agg(some aggregations) //
extension.write.parquet() // other save, without cache it will trigger recomputation of whole dataset
However when I call data.unpersist() i.e. in place (1), Spark deletes from Storage all datasets, also the extension Dataset which is not the dataset I tried to unpersist.
Is that an expected behaviour? How can I free some memory by unpersist on old Dataset without unpersisting all Dataset that was ""next in chain""?
My setup:
Spark version: current master, RC for 2.3
Scala: 2.11
Java: OpenJDK 1.8
Question looks similar to Understanding Spark&#39;s caching, but here I'm doing some actions before unpersist. At first I'm counting everything and then save into storage - I don't know if caching works the same in RDD like in Datasets
",<apache-spark><apache-spark-sql>,1303,1,17,15806,4,47,62,67,6579,0,2301,2,15,2018-01-17 15:54,2018-01-17 17:41,2018-01-17 17:41,0,0,Intermediate,23
51089008,"An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue","Any reason why this error popped up when I tried bundling an application:
I have tried installing gem install mysql2 -v '0.3.21' as they recommend but it cant install properly. Also I am running this on macOS High Sierra. Sorry for my bad wording for this question because its my first time working with ruby.
 To see why this extension failed to compile, please check the mkmf.log which can be found here:
  /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/mkmf.log
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR="" clean
current directory: /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21/ext/mysql2
make ""DESTDIR=""
compiling infile.c
compiling client.c
client.c:439:3: error: use of undeclared identifier 'my_bool'
  my_bool res = mysql_read_query_result(client);
  ^
client.c:441:19: error: use of undeclared identifier 'res'
  return (void *)(res == 0 ? Qtrue : Qfalse);
                  ^
client.c:762:3: error: use of undeclared identifier 'my_bool'
  my_bool boolval;
  ^
client.c:793:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:794:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:797:10: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
    case MYSQL_SECURE_AUTH:
         ^~~~~~~~~~~~~~~~~
         MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
client.c:798:7: error: use of undeclared identifier 'boolval'
      boolval = (value == Qfalse ? 0 : 1);
      ^
client.c:799:17: error: use of undeclared identifier 'boolval'
      retval = &amp;boolval;
                ^
client.c:830:38: error: use of undeclared identifier 'boolval'
        wrapper-&gt;reconnect_enabled = boolval;
                                     ^
client.c:1185:38: error: use of undeclared identifier 'MYSQL_SECURE_AUTH'; did you mean 'MYSQL_DEFAULT_AUTH'?
  return _mysql_client_options(self, MYSQL_SECURE_AUTH, value);
                                     ^~~~~~~~~~~~~~~~~
                                     MYSQL_DEFAULT_AUTH
/usr/local/Cellar/mysql/8.0.11/include/mysql/mysql.h:188:3: note: 'MYSQL_DEFAULT_AUTH' declared here
  MYSQL_DEFAULT_AUTH,
  ^
10 errors generated.
make: *** [client.o] Error 1
make failed, exit code 2
Gem files will remain installed in /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/gems/mysql2-0.3.21 for inspection.
Results logged to /Users/yamanshrestha/Desktop/Dorsata/vendor/bundle/ruby/2.3.0/extensions/universal-darwin-17/2.3.0/mysql2-0.3.21/gem_make.out
An error occurred while installing mysql2 (0.3.21), and Bundler cannot continue.
Make sure that `gem install mysql2 -v '0.3.21' --source 'http://rubygems.org/'` succeeds before bundling.
",<mysql><ruby-on-rails><bundle-install>,3020,1,59,250,1,2,11,73,14099,0,4,5,15,2018-06-28 18:01,2018-06-29 20:41,,1,,Intermediate,19
59264410,How to connect to Traefik TCP Services with TLS configuration enabled?,"I am trying to configure Traefik so that I would have access to services via domain names, and that I would not have to set different ports. For example, two MongoDB services, both on the default port, but in different domains, example.localhost and example2.localhost. Only this example works. I mean, other cases probably work, but I can't connect to them, and I don't understand what the problem is. This is probably not even a problem with Traefik.
I have prepared a repository with an example that works. You just need to generate your own certificate with mkcert. The page at example.localhost returns the 403 Forbidden error but you should not worry about it, because the purpose of this configuration is to show that SSL is working (padlock, green status). So don't focus on 403.
Only the SSL connection to the mongo service works. I tested it with the Robo 3T program. After selecting the SSL connection, providing the host on example.localhost and selecting the certificate for a self-signed (or own) connection works. And that's the only thing that works that way. Connections to redis (Redis Desktop Manager) and to pgsql (PhpStorm, DBeaver, DbVisualizer) do not work, regardless of whether I provide certificates or not. I do not forward SSL to services, I only connect to Traefik. I spent long hours on it. I searched the internet. I haven't found the answer yet. Has anyone solved this?
PS. I work on Linux Mint, so my configuration should work in this environment without any problem. I would ask for solutions for Linux.
If you do not want to browse the repository, I attach the most important files:
docker-compose.yml
version: ""3.7""
services:
    traefik:
        image: traefik:v2.0
        ports:
            - 80:80
            - 443:443
            - 8080:8080
            - 6379:6379
            - 5432:5432
            - 27017:27017
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock:ro
            - ./config.toml:/etc/traefik/traefik.config.toml:ro
            - ./certs:/etc/certs:ro
        command:
            - --api.insecure
            - --accesslog
            - --log.level=INFO
            - --entrypoints.http.address=:80
            - --entrypoints.https.address=:443
            - --entrypoints.traefik.address=:8080
            - --entrypoints.mongo.address=:27017
            - --entrypoints.postgres.address=:5432
            - --entrypoints.redis.address=:6379
            - --providers.file.filename=/etc/traefik/traefik.config.toml
            - --providers.docker
            - --providers.docker.exposedByDefault=false
            - --providers.docker.useBindPortIP=false
    apache:
        image: php:7.2-apache
        labels:
            - traefik.enable=true
            - traefik.http.routers.http-dev.entrypoints=http
            - traefik.http.routers.http-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.entrypoints=https
            - traefik.http.routers.https-dev.rule=Host(`example.localhost`)
            - traefik.http.routers.https-dev.tls=true
            - traefik.http.services.dev.loadbalancer.server.port=80
    pgsql:
        image: postgres:10
        environment:
            POSTGRES_DB: postgres
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: password
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.pgsql.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.pgsql.tls=true
            - traefik.tcp.routers.pgsql.service=pgsql
            - traefik.tcp.routers.pgsql.entrypoints=postgres
            - traefik.tcp.services.pgsql.loadbalancer.server.port=5432
    mongo:
        image: mongo:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.mongo.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.mongo.tls=true
            - traefik.tcp.routers.mongo.service=mongo
            - traefik.tcp.routers.mongo.entrypoints=mongo
            - traefik.tcp.services.mongo.loadbalancer.server.port=27017
    redis:
        image: redis:3
        labels:
            - traefik.enable=true
            - traefik.tcp.routers.redis.rule=HostSNI(`example.localhost`)
            - traefik.tcp.routers.redis.tls=true
            - traefik.tcp.routers.redis.service=redis
            - traefik.tcp.routers.redis.entrypoints=redis
            - traefik.tcp.services.redis.loadbalancer.server.port=6379
config.toml
[tls]
[[tls.certificates]]
certFile = ""/etc/certs/example.localhost.pem""
keyFile = ""/etc/certs/example.localhost-key.pem""
Build &amp; Run
mkcert example.localhost # in ./certs/
docker-compose up -d
Prepare step by step
Install mkcert (run also mkcert -install for CA)
Clone my code
In certs folder run mkcert example.localhost
Start container by docker-compose up -d
Open page https://example.localhost/ and check if it is secure connection
If address http://example.localhost/ is not reachable, add 127.0.0.1 example.localhost to /etc/hosts
Certs:
Public: ./certs/example.localhost.pem
Private: ./certs/example.localhost-key.pem
CA: ~/.local/share/mkcert/rootCA.pem
Test MongoDB
Install Robo 3T
Create new connection:
Address: example.localhost
Use SSL protocol
CA Certificate: rootCA.pem (or Self-signed Certificate)
Test tool:
Test Redis
Install RedisDesktopManager
Create new connection:
Address: example.localhost
SSL
Public Key: example.localhost.pem
Private Key: example.localhost-key.pem
Authority: rootCA.pem
Test tool:
So far:
Can connect to Postgres via IP (info from Traefik)
jdbc:postgresql://172.21.0.4:5432/postgres?sslmode=disable
jdbc:postgresql://172.21.0.4:5432/postgres?sslfactory=org.postgresql.ssl.NonValidatingFactory
Try telet (IP changes every docker restart):
&gt; telnet 172.27.0.5 5432
Trying 172.27.0.5...
Connected to 172.27.0.5.
Escape character is '^]'.
^]
Connection closed by foreign host.
&gt; telnet example.localhost 5432
Trying ::1...
Connected to example.localhost.
Escape character is '^]'.
^]
HTTP/1.1 400 Bad Request
Content-Type: text/plain; charset=utf-8
Connection: close
400 Bad RequestConnection closed by foreign host.
If I connect directly to postgres, the data is nice. If I connect to via Traefik then I have Bad Request when closing the connection. I have no idea what this means and whether it must mean something.
",<postgresql><docker><ssl><tcp><traefik>,6320,12,120,1882,1,23,30,60,8244,0,377,2,15,2019-12-10 9:51,2020-01-29 1:34,,50,,Advanced,37
48277519,how to use COMMIT and ROLLBACK in a PostgreSQL function,"I am using three insert statements, and if there is an error in the third statement, I want to rollback the first and the second one. If there is no way to do this, please tell me a different approach to handle this in PostgresqQL.
If I use COMMIT or ROLLBACK, I get an error.
CREATE OR REPLACE FUNCTION TEST1 ()
   RETURNS VOID
   LANGUAGE 'plpgsql'
   AS $$
BEGIN 
    INSERT INTO table1 VALUES (1);
    INSERT INTO table1 VALUES (2);
    INSERT INTO table1 VALUES ('A');
    COMMIT;
EXCEPTION
   WHEN OTHERS THEN
   ROLLBACK;
END;$$;
The above code is not working; COMMIT and ROLLBACK are not supported by PostgreSQL functions.
",<postgresql><postgresql-10>,631,0,20,487,3,7,23,60,47949,0,10,3,15,2018-01-16 9:05,2018-01-16 9:16,2018-01-16 9:16,0,0,Basic,10
63816057,How do i close database instance in gorm 1.20.0,"As i have not found in Close() function with *gorm instance, any help would be appreciated
dbURI := fmt.Sprintf(&quot;user=%s password=%s dbname=%s port=%s sslmode=%s TimeZone=%s&quot;,
    &quot;username&quot;, &quot;password&quot;, &quot;dbname&quot;, &quot;5432&quot;, &quot;disable&quot;, &quot;Asia/Kolkata&quot;)
fmt.Println(dbURI)
connection, err := gorm.Open(postgres.Open(dbURI), &amp;gorm.Config{})
if err != nil {
    fmt.Println(&quot;Error connecting database&quot;)
    panic(err.Error())
} else {
    fmt.Println(&quot;Connected to database&quot;)
}
Note: connection.Close() is not available for GORM 1.20.0
",<postgresql><go><go-gorm><glide-golang>,623,0,11,497,1,4,16,43,26825,0,25,3,15,2020-09-09 16:46,2020-09-09 18:24,,0,,Basic,10
50474156,java.sql.SQLException: Unknown initial character set index '255' received from server for connector 8.0.11,"While establishing the connection to a MySQL database, I'm getting the following error
java.sql.SQLException: Unknown initial character set index '255' received from 
server. Initial client character set can be forced via the 'characterEncoding' 
property.
Upon googling, I got to know that we need to modify 2 params in my.ini or my.cnf.
I am using MySQL version 8.0.11 and it does not have this file.
Hence I modified these parameters using the SQL commands:
Please note name and duration are column name in the table.    
ALTER TABLE courses MODIFY name VARCHAR(50) COLLATE utf8_unicode_ci;    
ALTER TABLE courses MODIFY duration VARCHAR(50) COLLATE utf8_unicode_ci;
ALTER TABLE courses MODIFY name VARCHAR(50) CHARACTER SET utf8;
ALTER TABLE courses MODIFY duration VARCHAR(50) CHARACTER SET utf8;
Hence my table looks like this 
After this, I restarted MySQL server, but I'm still getting the above error.
Please note I'm deploying my application in tomcat and running rest API call which will connect to the database. While connecting to the database, I'm getting the above error.
",<java><mysql><jdbc>,1088,1,8,445,2,6,13,76,52338,0,34,15,15,2018-05-22 18:20,2018-05-22 18:32,2018-08-14 5:25,0,84,Basic,10
48562745,Select from multiple tables in one call,"In my code I have a page that includes information from 3 different tables. To show this information I make 3 SQL select calls and unite them in one list to pass as Model to my view. Can I do it with one SQL call? Data has no connection with one another. 
My code:
public ActionResult Index()
{
    StorePageData PageData = new StorePageData();
    return View(PageData);
}
public class StorePageData
{
     public List&lt;Table1Data&gt; Table1 { get; set; }
     public List&lt;Table2Data&gt; Table2 { get; set; }
     public List&lt;Table3Data&gt; Table3 { get; set; }
     public StorePageData()
     {
          Table1  = //loading from Database1
          Table2  = //loading from Database2
          Table3  = //loading from Database3
     }
}
public class Table1Data
{
     public int Id { get; set; }
     public double Info1 { get; set; }
     public string Info2 { get; set; }
}
public class Table2Data
{
     public int Id { get; set; }
     public List&lt;int&gt; Info1 { get; set; }
     public List&lt;int&gt; Info2 { get; set; }
}
public class Table3Data
{
     public int Id { get; set; }
     public List&lt;string&gt; Info1 { get; set; }
     public List&lt;string&gt; Info2 { get; set; }
}
If there is a way to load all 3 tables in one SQL request it will improve significantly the load time of this page.
Thank you.
",<c#><asp.net><.net><sql-server><asp.net-mvc>,1336,0,36,2700,2,25,32,52,16257,0,403,7,15,2018-02-01 12:25,2018-02-01 12:32,2018-02-05 0:07,0,4,Basic,10
58894875,How to delete a database in pgadmin,"When i try to create other database with the name ""eCommerce"" in pgadmin 4 this message appears 
ERROR: source database ""template1"" is being accessed by other users
DETAIL: There are 2 other sessions using the database.
I try to delete the others databases but is not working and appears
ERROR: cannot drop a template database
What should i do?
",<java><sql><database>,345,0,0,155,1,2,6,35,35059,0,4,6,15,2019-11-16 20:26,2019-11-16 22:26,2019-11-16 22:26,0,0,Basic,10
51183321,How to use Paging with SQLite?,"I want to integrate Paging with SQLite in existing application. There are around 90 tables in my database so its time consuming to convert to Room. I also tried LIMIT...OFFSET but that takes time to process data every time. 
Thanks.
",<android><sqlite><paging>,233,2,1,4928,9,34,64,35,24595,0,113,0,15,2018-07-05 4:15,,,,,Basic,10
56542036,"pgloader - Failed to connect to mysql at ""localhost"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled","I am trying to migrate my rails application from mysql to postgres. Since we have already running application so I am moving mysql data to postgres database using pgloader. But when I do 
pgloader mysql://root:root_password@127.0.0.1/mysql_database postgresql://postgres_user:postgres_pass@127.0.0.1/postgres_database
I get error - Failed to connect to mysql at ""127.0.0.1"" (port 3306) as user ""root"": Condition QMYND:MYSQL-UNSUPPORTED-AUTHENTICATION was signalled. I can easily log in to mysql from terminal though.
Thanks in advance.
",<mysql><ruby-on-rails><postgresql><pgloader>,536,0,1,279,1,2,12,77,11056,,10,1,15,2019-06-11 10:49,2020-03-21 15:19,,284,,Intermediate,18
51693126,"Why does DynamoDB seem to inconsistently return LastEvaluatedKey when no records remain, depending on the record limit of the query?","Using javascript aws-sdk and querying a dynamodb table, with document client. the table contains 10 elements and the query limit = 5. 
For each request I use the LastEvaluatedKey to build a new query and send a fresh request, here are the results :
first request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {…}}
second request --&gt; {Items: Array(5), Count: 5, ScannedCount: 5, LastEvaluatedKey: {…}}
third request --&gt; {Items: Array(0), Count: 0, ScannedCount: 0}
According to this doc 
  If the result contains a LastEvaluatedKey element, proceed to step 2. 
  If there is not a LastEvaluatedKey in the result, then there are no
  more items to be retrieved
It is supposed to not return LastEvaluatedKey in the second request, cause there is no more elements, but it returns one which send to an empty result in the third request.
When i try with limit = 4, every things works as expected
first request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {…}}
second request --&gt; {Items: Array(4), Count: 4, ScannedCount: 4, LastEvaluatedKey: {…}}
third request --&gt; {Items: Array(2), Count: 2, ScannedCount: 2} &lt;--- there is no LastEvaluatedKey as expected
So what is happening here ?
",<amazon-web-services><pagination><nosql><amazon-dynamodb>,1242,1,6,1779,4,17,38,48,19219,0,90,3,15,2018-08-05 9:34,2018-08-05 12:39,2018-08-05 12:44,0,0,Advanced,33
51947581,Select all columns except for some PostgreSQL,"I have to compare tables but there are some columns which I don't need to compare and I only know them (not the ones I have to compare) so I want to select all columns from table except the ones that I don't need to compare.
I thought of something like:
SELECT 'SELECT ' || array_to_string(ARRAY(SELECT 'o' || '.' || c.column_name
    FROM information_schema.columns As c
        WHERE table_name = 'office' 
        AND  c.column_name NOT IN('id', 'deleted')
), ',') || ' FROM officeAs o' As sqlstmt
however the output was SELECT * FROM office As o 
instead of being select a,b,c from office without id and deleted columns.
Does anyone have any ideas what's wrong with this query?
",<sql><postgresql>,682,0,8,2219,4,35,82,53,32424,,232,4,15,2018-08-21 11:21,2018-08-21 11:29,2018-08-21 11:29,0,0,Basic,10
53921336,An error happened while reading data from the provider. The remote certificate is invalid according to the validation procedure,"I'm trying to connect Postgres Database on AWS EC2 instance to Microsoft PowerBI. I tried various method available on internet but its showing the above error. Although I've done this connection on AWS RDS. I installed required dependencies (GAC) and all the certificates required for PowerBI.
",<windows><postgresql><amazon-ec2><powerbi>,294,1,0,333,1,2,13,67,26312,0,9,6,15,2018-12-25 10:10,2018-12-25 10:33,2018-12-25 10:33,0,0,Basic,13
50129411,Why is predicate pushdown not used in typed Dataset API (vs untyped DataFrame API)?,"I always thought that dataset/dataframe API's are the same.. and the only difference is that dataset API will give you compile time safety. Right ?
So.. I have very simple case:
 case class Player (playerID: String, birthYear: Int)
 val playersDs: Dataset[Player] = session.read
  .option(""header"", ""true"")
  .option(""delimiter"", "","")
  .option(""inferSchema"", ""true"")
  .csv(PeopleCsv)
  .as[Player]
 // Let's try to find players born in 1999. 
 // This will work, you have compile time safety... but it will not use predicate pushdown!!!
 playersDs.filter(_.birthYear == 1999).explain()
 // This will work as expected and use predicate pushdown!!!
 // But you can't have compile time safety with this :(
 playersDs.filter('birthYear === 1999).explain()
Explain from first example will show that it's NOT doing predicate pushdown (Notice empty PushedFilters):
== Physical Plan ==
*(1) Filter &lt;function1&gt;.apply
+- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
While the second sample will do it correctly (Notice PushedFilters):
== Physical Plan ==
*(1) Project [.....]
+- *(1) Filter (isnotnull(birthYear#11) &amp;&amp; (birthYear#11 = 1999))
   +- *(1) FileScan csv [...] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:People.csv], PartitionFilters: [], PushedFilters: [IsNotNull(birthYear), EqualTo(birthYear,1999)], ReadSchema: struct&lt;playerID:string,birthYear:int,birthMonth:int,birthDay:int,birthCountry:string,birthState:s...
So the question is.. how can I use DS Api, and have compile time safety.., and predicate pushdown working as expected ????
Is it possible ? If not.. does this mean that DS api gives you compile time safety.. but at the cost of performance!! ??? (DF will be much faster in this case.. especially when processing large parquet files)
",<apache-spark><dataframe><apache-spark-sql><apache-spark-dataset>,1998,0,23,165,0,1,6,81,3471,0,2,1,15,2018-05-02 7:40,2018-05-02 9:57,2018-05-02 9:57,0,0,Basic,3
51800800,Results in Grid is not showing in SQL Server,"I couldn't see the results in grid pane, so I did the following: 
Tools &gt; Options &gt; Query Results &gt; Results to Grid 
      &gt; uncheck the ""Discard results after execution"" check box
But still I couldn't see the results in grid. I even hit the ""Reset to Default"" button in same window yet no luck
Please shed some light on this.
",<sql-server><sql-server-2012><ssms><ssms-2012>,339,0,2,751,1,11,28,72,33458,,108,3,15,2018-08-11 14:46,2018-08-11 14:57,,0,,Basic,14
62881953,Redis vs SQL Server performance,"Application performance is one of the main reason of using cache over relational database. Because it stores data in memory in the form of key value pair, we can store frequently accessed data in cache which are not changes very frequently. Reading from cache is much faster than database. Redis is one of the best solution in distributed cache market.
I was doing a performance test between Azure Redis cache and Azure SQL Server. I have created a simple ASP.NET Core application and inside that I have read data from SQL Server database as well as Redis multiple times and compare the read time duration between them. For database reading I have used Entity Framework Core and for Redis reading I have used 'Microsoft.Extensions.Caching.StackExchangeRedis'.
Model
using System;
namespace WebApplication2.Models
{
    [Serializable]
    public class Student
    {
        public int Id { get; set; }
        public string Name { get; set; }
        public int Age { get; set; }
        public string Subject { get; set; }
        public Student()
        {
            Name = string.Empty;
            Subject = string.Empty;
        }
    }
}
Entity Framework Core data context.
using Microsoft.EntityFrameworkCore;
using WebApplication2.Models;
namespace WebApplication2.Data
{
    public class StudentContext : DbContext
    {
        public StudentContext(DbContextOptions&lt;StudentContext&gt; options)
            : base(options)
        {
        }
        public DbSet&lt;Student&gt;? Students { get; set; }
    }
}
Startup class
public void ConfigureServices(IServiceCollection services)
{
    services.AddControllersWithViews();
    string studentDbConnectionString = Configuration.GetConnectionString(&quot;StudentDbConnectionString&quot;);
    services.AddDbContext&lt;StudentContext&gt;(option =&gt; option.UseSqlServer(studentDbConnectionString));
    string redisConnectionString = Configuration.GetConnectionString(&quot;RedisConnectionString&quot;);
    services.AddStackExchangeRedisCache(options =&gt;
    {
        options.Configuration = redisConnectionString;
    });
}
appsettings.json
{
  &quot;Logging&quot;: {
    &quot;LogLevel&quot;: {
      &quot;Default&quot;: &quot;Information&quot;,
      &quot;Microsoft&quot;: &quot;Warning&quot;,
      &quot;Microsoft.Hosting.Lifetime&quot;: &quot;Information&quot;
     }
  },
  &quot;AllowedHosts&quot;: &quot;*&quot;,
  &quot;ConnectionStrings&quot;: {
    &quot;StudentDbConnectionString&quot;: &quot;[Azure SQL Server connection string]&quot;,
    &quot;RedisConnectionString&quot;: &quot;[Azure Redis cache connection string]&quot;
  }
}
Home controller
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Caching.Distributed;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Runtime.Serialization.Formatters.Binary;
using WebApplication2.Data;
using WebApplication2.Models;
namespace WebApplication2.Controllers
{
    public class HomeController : Controller
    {
        private readonly StudentContext _studentContext;
        private readonly IDistributedCache _cache;
        public HomeController(StudentContext studentContext, IDistributedCache cache)
        {
            _studentContext = studentContext;
            _cache = cache;
        }
        public IActionResult Index()
        {
            List&lt;Student&gt;? students = null;
            var counter = 10000;
            var sw = Stopwatch.StartNew();
            for (var i = 0; i &lt; counter; i++)
            {
                students = _studentContext.Students.OrderBy(student =&gt; student.Id).ToList();
            }
            sw.Stop();
            ViewData[&quot;DatabaseDuraion&quot;] = $&quot;Database: {sw.ElapsedMilliseconds}&quot;;
            if (students != null &amp;&amp; students.Count &gt; 0)
            {
                List&lt;Student&gt; studentsFromCache;
                var key = &quot;Students&quot;;
                _cache.Set(key, ObjectToByteArray(students));
                sw.Restart();
                for (var i = 0; i &lt; counter; i++)
                {
                    studentsFromCache = (List&lt;Student&gt;)ByteArrayToObject(_cache.Get(key));
                }
                sw.Stop();
                ViewData[&quot;RedisDuraion&quot;] = $&quot;Redis: {sw.ElapsedMilliseconds}&quot;;
            }
            return View();
        }
        private byte[] ObjectToByteArray(object obj)
        {
            var bf = new BinaryFormatter();
            using var ms = new MemoryStream();
            bf.Serialize(ms, obj);
            return ms.ToArray();
        }
        private object ByteArrayToObject(byte[] arrBytes)
        {
            using var memStream = new MemoryStream();
            var binForm = new BinaryFormatter();
            memStream.Write(arrBytes, 0, arrBytes.Length);
            memStream.Seek(0, SeekOrigin.Begin);
            object obj = binForm.Deserialize(memStream);
            return obj;
        }
    }
}
Home\Index.cshtml view
@{
    ViewData[&quot;Title&quot;] = &quot;Home Page&quot;;
}
&lt;div class=&quot;text-center&quot;&gt;        
    &lt;p&gt;@ViewData[&quot;DatabaseDuraion&quot;]&lt;/p&gt;
    &lt;p&gt;@ViewData[&quot;RedisDuraion&quot;]&lt;/p&gt;
&lt;/div&gt;
I have found SQL Server is faster than Redis.
The ASP.NET Core application is hosted in Azure App Service with the same location with Azure SQL Server and Azure Redis.
Please let me know why Redis is slower than SQL Server?
",<sql-server><azure><redis>,5510,1,142,494,1,4,14,77,15095,0,94,3,15,2020-07-13 18:21,2020-07-17 6:37,2020-07-17 6:37,4,4,Intermediate,23
55093267,How to resolve writing configuration error while installing mysql?,"I am new to MySQL. I downloaded the MySQL windows installer and selected I guess everything I thought I would need for x64 and x86 products especially utilities, workbench and MySQL Server for MYSQL 8.0.15 
When I get to the Configuration Steps section I select Execute and this is what happens: 
  Writing Configuration file turns red. 
Next I checked the Log File: 
The Log File states: 
  Beginning configuration step: Writing Configuration File 
  Invalid server template 
  Ended configuration step: Writing configuration file. 
",<mysql><windows>,534,0,0,151,0,1,4,64,2806,,14,1,15,2019-03-10 23:01,2022-03-31 2:01,,1117,,Basic,14
55162939,BigQuery: Return First Value from Different Groups in a Group By,"I am currently having a problem with a Standard SQL query. I have a list of emails where every email can have multiple functions. See the example below on how the table looks like.
Email                         Function
peter@gmail.com               engineer
peter@gmail.com               specialist
dave@gmail.com                analyst
dave@gmail.com                tester
dave@gmail.com                manager
michael@gmail.com             intern
What I want is a query that returns every email once with the first function it finds. So the above table should return the following:
Email                         Function
peter@gmail.com               engineer
dave@gmail.com                analyst
michael@gmail.com             intern
How do I do this?
What I have right now is a simplified version of the query.
SELECT Email, Function
FROM database
GROUP BY Email, Function
The issue is here is that I have to put both Email and Function in the GROUP BY. If I only put Email in the Group By the query cannot run even though I only want the query to GROUP BY Email.
Thanks!
",<sql><google-bigquery><standards>,1077,0,14,171,1,1,6,60,22249,,3,6,15,2019-03-14 12:47,2019-03-14 12:49,,0,,Basic,10
54095648,Using Dapper to get nvarchar(max) returns a string trimmed to 4000 characters. Can this behaviour be changed?,"I have a SQL Server data table which stores a JSON string in one of its columns.  The JSON string is a serialised .net object and the data typically exceeds 4000 characters.
I have a simple stored procedure which I use to retrieve the data:
    @StageID int,
    @Description varchar(250) = null OUTPUT,
    @Program nvarchar(max) = null OUTPUT
AS
BEGIN
    SET NOCOUNT ON;
    SELECT @Program = StageProgram, @Description = Description 
    FROM StageProgram 
    WHERE StageID = @StageID;
    RETURN 0;
END 
I am using the data type nvarchar(max) for the column. When I serialise the .net object to JSON and write it to the database using Dapper, I find that the full string is correctly stored in the database.
However, when I attempt to retrieve the string I find that it is trimmed to 4000 characters, discarding the rest of the data.
Here is the relevant code:
DynamicParameters p = new DynamicParameters();
p.Add(""@StageID"", Properties.Settings.Default.StageID, DbType.Int32, ParameterDirection.Input);
p.Add(""@Description"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@Program"", """", DbType.String, direction: ParameterDirection.Output);
p.Add(""@ReturnValue"", DbType.Int32, direction: ParameterDirection.ReturnValue);               
try
{
     int stageID = Properties.Settings.Default.StageID;
     connection.Execute(sql, p, commandType: CommandType.StoredProcedure);                 
     json = p.Get&lt;string&gt;(""@Program"");
     int r = p.Get&lt;int&gt;(""@ReturnValue"");                    
}
When I run this, the string json is trimmed to 4000 characters.
If I use the built in .net SQL Server connection to retrieve it instead (using a query rather than a stored procedure for simplicity), the full data is correctly returned:
SqlCommand getProgram = new SqlCommand(""SELECT StageProgram FROM StageProgram WHERE StageID = 1;"");
getProgram.Connection = connection;
string json = Convert.ToString(getProgram.ExecuteScalar());
Is an experienced Dapper user able to provide an explanation for this behaviour? 
Can it be changed?
",<c#><json><sql-server><dapper>,2062,0,31,409,0,6,15,66,8476,0,7,1,15,2019-01-08 16:13,2019-01-08 17:08,2019-01-08 17:08,0,0,Basic,14
57482120,"In Apache Spark, how to convert a slow RDD/dataset into a stream?","I'm investigating an interesting case that involves wide transformations (e.g. repartition &amp; join) on a slow RDD or dataset, e.g. the dataset defined by the following code:
val ds = sqlContext.createDataset(1 to 100)
  .repartition(1)
  .mapPartitions { itr =&gt;
    itr.map { ii =&gt;
      Thread.sleep(100)
      println(f""skewed - ${ii}"")
      ii
    }
  }
The slow dataset is relevant as it resembles a view of a remote data source, and the partition iterator is derived from a single-threaded network protocol (http, jdbc etc.), in this case, the speed of download > the speed of single-threaded processing, but &lt;&lt; the speed of distributed processing.
Unfortunately the conventional Spark computation model won't be efficient on a slow dataset because we are confined to one of the following options:
Use only narrow transformations (flatMap-ish) to pipe the stream with data processing end-to-end in a single thread, obviously the data processing will be a bottle neck and resource utilisation will be low.
Use a wide operation (repartitioning included) to balance the RDD/dataset, while this is essential for parallel data processing efficiency, the Spark coarse-grained scheduler demands that the download to be fully completed, which becomes another bottleneck.
Experiment
The following program represents a simple simulation of such case:
val mapped = ds
val mapped2 = mapped
  .repartition(10)
  .map { ii =&gt;
    println(f""repartitioned - ${ii}"")
    ii
  }
mapped2.foreach { _ =&gt;
  }
When executing the above program it can be observed that line println(f""repartitioned - ${ii}"") will not be executed before line println(f""skewed - ${ii}"") in RDD dependency.
I'd like to instruct Spark scheduler to start distributing/shipping data entries generated by the partition iterator before its task completion (through mechanisms like microbatch or stream). Is there a simple way of doing this? E.g. converting the slow dataset into a structured stream would be nice, but there should be alternatives that are better integrated.
Thanks a lot for your opinion
UPDATE: to make your experimentation easier I have appended my scala tests that can be ran out of the box:
package com.tribbloids.spookystuff.spike
import org.apache.spark.SparkContext
import org.apache.spark.sql.{SQLContext, SparkSession}
import org.scalatest.{FunSpec, Ignore}
@Ignore
class SlowRDDSpike extends FunSpec {
  lazy val spark: SparkSession = SparkSession.builder().master(""local[*]"").getOrCreate()
  lazy val sc: SparkContext = spark.sparkContext
  lazy val sqlContext: SQLContext = spark.sqlContext
  import sqlContext.implicits._
  describe(""is repartitioning non-blocking?"") {
    it(""dataset"") {
      val ds = sqlContext
        .createDataset(1 to 100)
        .repartition(1)
        .mapPartitions { itr =&gt;
          itr.map { ii =&gt;
            Thread.sleep(100)
            println(f""skewed - $ii"")
            ii
          }
        }
      val mapped = ds
      val mapped2 = mapped
        .repartition(10)
        .map { ii =&gt;
          Thread.sleep(400)
          println(f""repartitioned - $ii"")
          ii
        }
      mapped2.foreach { _ =&gt;
        }
    }
  }
  it(""RDD"") {
    val ds = sc
      .parallelize(1 to 100)
      .repartition(1)
      .mapPartitions { itr =&gt;
        itr.map { ii =&gt;
          Thread.sleep(100)
          println(f""skewed - $ii"")
          ii
        }
      }
    val mapped = ds
    val mapped2 = mapped
      .repartition(10)
      .map { ii =&gt;
        Thread.sleep(400)
        println(f""repartitioned - $ii"")
        ii
      }
    mapped2.foreach { _ =&gt;
      }
  }
}
",<scala><apache-spark><apache-spark-sql><spark-streaming>,3646,0,95,3756,14,65,106,80,971,0,490,2,15,2019-08-13 16:47,2020-06-26 10:13,,318,,Basic,10
51028387,Full Text Search in EF Core 2.1?,"I am looking at using Full Text Search but not 100% clear on how to get it up with EF Core 2.1. 
It seems that EF Core 2.1 might have implemented partial support for Full Text Search but I am not finding any tutorials on how actually use it.
My understanding is that I will have to add an Full Text index to one of my columns.
So if I have this table
public class Company {
    public string Name {get; set;}
}
public class CompanyConfig : IEntityTypeConfiguration&lt;Company&gt;
{
  public void Configure(EntityTypeBuilder&lt;Company&gt; builder)
        {
            builder.HasKey(x =&gt; x.Id);
            builder.Property(x =&gt; x.Name).HasMaxLength(100).IsRequired();
        }
}
How would I add full text index to my Name property?
",<c#><sql-server><entity-framework><entity-framework-core><full-text-search>,742,0,13,83739,198,532,839,75,13506,0,391,1,15,2018-06-25 16:53,2018-07-23 11:10,2018-07-23 11:10,28,28,Basic,14
50122955,check for duplicates in Pyspark Dataframe,"Is there a simple and efficient way to check a python dataframe just for duplicates (not drop them) based on column(s)?
I want to check if a dataframe has dups based on a combination of columns and if it does, fail the process.
TIA.
",<python-2.7><dataframe><pyspark><apache-spark-sql>,233,0,0,599,1,4,17,81,75844,0,29,6,15,2018-05-01 19:55,2018-05-01 20:05,2018-05-01 20:05,0,0,Basic,10
55839111,Installing psycopg2 fails on MacOS with unclear error message,"Trying to install psycopg2 using pip 19.1 on MacOS 10.14.4 returns the lengthy error message below. I understand there are warnings related to gcc, but given the actual error messages I cannot find any clues what the underlying problem is. 
I have tried the following actions without any luck:
Upgraded Xcode to the latest version (10.2.1)
Upgrade Postgresql to 11.2.1
Uninstalled psycopg2-binary to prevent any dependency issues
Cleared all files left by a previously successful install at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (Yes, I had it installed at some point and it worked, not sure why I uninstalled)
There seem to be quite some issues around pip and psycopg2 but no question I found on Stackoverflow showed similar error messages, nor did any of the suggested fixes help. Any pointers to potential fixes are very much appreciated!
pip install psycopg2
Collecting psycopg2
  Using cached https://files.pythonhosted.org/packages/23/7e/93c325482c328619870b6cd09370f6dbe1148283daca65115cd63642e60f/psycopg2-2.8.2.tar.gz
Installing collected packages: psycopg2
  Running setup.py install for psycopg2 ... error
    ERROR: Complete output from command /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.9-x86_64-3.7
    creating build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_json.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extras.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errorcodes.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/tz.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_range.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_ipaddress.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/_lru_cache.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/extensions.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/errors.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/sql.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    copying lib/pool.py -&gt; build/lib.macosx-10.9-x86_64-3.7/psycopg2
    running build_ext
    building 'psycopg2._psycopg' extension
    creating build/temp.macosx-10.9-x86_64-3.7
    creating build/temp.macosx-10.9-x86_64-3.7/psycopg
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/psycopgmodule.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/green.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/pqpath.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o
    psycopg/pqpath.c:135:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(conn-&gt;pgconn) : PQresultStatus(*pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~
    psycopg/pqpath.c:1710:11: warning: code will never be executed [-Wunreachable-code]
        ret = 1;
              ^
    psycopg/pqpath.c:1815:17: warning: implicit conversion from enumeration type 'ConnStatusType' to different enumeration type 'ExecStatusType' [-Wenum-conversion]
                    PQstatus(curs-&gt;conn-&gt;pgconn) : PQresultStatus(curs-&gt;pgres)));
                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
    3 warnings generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/utils.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/bytes_format.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o
    In file included from psycopg/bytes_format.c:81:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/libpq_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o
    In file included from psycopg/libpq_support.c:29:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/win32_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o
    In file included from psycopg/win32_support.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/solaris_support.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o
    In file included from psycopg/solaris_support.c:28:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/column_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o
    In file included from psycopg/column_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_connection_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o
    In file included from psycopg/replication_connection_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_cursor_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/replication_message_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o
    In file included from psycopg/replication_message_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/diagnostics_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o
    In file included from psycopg/diagnostics_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/error_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o
    In file included from psycopg/error_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/conninfo_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o
    In file included from psycopg/conninfo_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_int.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/lobject_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/notify_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o
    In file included from psycopg/notify_type.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/xid_type.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_asis.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_binary.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_datetime.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_list.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pboolean.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pdecimal.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pint.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_pfloat.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/adapter_qstring.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/microprotocols_proto.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o
    In file included from psycopg/microprotocols_proto.c:27:
    In file included from ./psycopg/psycopg.h:37:
    ./psycopg/config.h:81:13: warning: unused function 'Dprintf' [-Wunused-function]
    static void Dprintf(const char *fmt, ...) {}
                ^
    1 warning generated.
    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -DPSYCOPG_VERSION=2.8.2 (dt dec pq3 ext lo64) -DPG_VERSION_NUM=110002 -DHAVE_LO64=1 -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -I. -I/usr/local/Cellar/postgresql/11.2_1/include -I/usr/local/Cellar/postgresql/11.2_1/include/server -c psycopg/typecast.c -o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o
    gcc -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-3.7/psycopg/psycopgmodule.o build/temp.macosx-10.9-x86_64-3.7/psycopg/green.o build/temp.macosx-10.9-x86_64-3.7/psycopg/pqpath.o build/temp.macosx-10.9-x86_64-3.7/psycopg/utils.o build/temp.macosx-10.9-x86_64-3.7/psycopg/bytes_format.o build/temp.macosx-10.9-x86_64-3.7/psycopg/libpq_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/win32_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/solaris_support.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/column_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_connection_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_cursor_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/replication_message_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/diagnostics_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/error_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/conninfo_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_int.o build/temp.macosx-10.9-x86_64-3.7/psycopg/lobject_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/notify_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/xid_type.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_asis.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_binary.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_datetime.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_list.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pboolean.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pdecimal.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pint.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_pfloat.o build/temp.macosx-10.9-x86_64-3.7/psycopg/adapter_qstring.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols.o build/temp.macosx-10.9-x86_64-3.7/psycopg/microprotocols_proto.o build/temp.macosx-10.9-x86_64-3.7/psycopg/typecast.o -L/usr/local/lib -lpq -lssl -lcrypto -o build/lib.macosx-10.9-x86_64-3.7/psycopg2/_psycopg.cpython-37m-darwin.so
    ld: library not found for -lssl
    clang: error: linker command failed with exit code 1 (use -v to see invocation)
    error: command 'gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command ""/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-record-ztnpuu7u/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/y4/3pcgz9d54zj29hfq1lmlxpk80000gn/T/pip-install-ci93cz6u/psycopg2/
",<postgresql><macos><pip><psycopg2>,26899,1,153,362,0,3,13,65,7143,0,129,3,15,2019-04-24 21:54,2019-04-24 22:29,2019-04-24 22:29,0,0,Basic,13
56583738,How to connect to a sqlite db from a React app?,"I'm trying to build a page using Create React App, plus the sqlite3 module. In their default configurations, the two things don't seem to be compatible out of the box. I'm new to React and JS in general, so I'm hoping there's something obvious I'm missing here.
Using npx v6.9.0 and Node v12.4.0, I can reproduce this by typing npx create-react-app test, cd test, npm start.
So far so good. I type: npm install sqlite3, and receive an npm warning that I should install typescript. OK, I type npm install typescript. All good. I start up the app, and it compiles so far. Great!
I open up App.js, and per sqlite3's readme doc, under the import lines, I type var sqlite3 = require('sqlite3').verbose();
This is probably where I'm doing something wrong. My app now fails to compile, telling me:
./node_modules/sqlite3/node_modules/node-pre-gyp/lib/info.js
Module not found: Can't resolve 'aws-sdk' in '/Users/brendanlandis/Desktop/test/node_modules/sqlite3/node_modules/node-pre-gyp/lib'
I try npm install aws-sdk, which gets me a little farther. My app still won't compile, but now the error is:
TypeError: stream is undefined
./node_modules/set-blocking/index.js/&lt;/module.exports/&lt;
node_modules/set-blocking/index.js:3
  1 | module.exports = function (blocking) {
  2 |   [process.stdout, process.stderr].forEach(function (stream) {
&gt; 3 |     if (stream._handle &amp;&amp; stream.isTTY &amp;&amp; typeof stream._handle.setBlocking === 'function') {
  4 |       stream._handle.setBlocking(blocking);
  5 |     }
  6 |   });
In googling around, I haven't yet figured out what I'm doing wrong. Any help would be appreciated. Thanks!
",<reactjs><sqlite><npm>,1637,0,20,151,1,1,3,63,24404,0,0,2,15,2019-06-13 15:19,2020-07-29 11:45,,412,,Basic,14
52682336,Async SQLite python,"I write asynchronous telegram bot using the aiogram library. I decided to use SQLite as a database for storing immutable values. How do I implement asynchronous reads from my database?
",<python><python-3.x><sqlite><python-asyncio><telegram-bot>,185,0,0,151,1,1,7,58,27282,0,4,1,15,2018-10-06 18:52,2018-10-06 20:17,2018-10-06 20:17,0,0,Basic,13
60306594,Failed to start PostgreSQL Cluster 10-main when booting,"when I try to boot Ubuntu, it never finishes the boot process because it appears the message ""Failed to start PostgreSQL Cluster 10-main."" I also get the same message with 9.5-main. But lets focus on 10.
When I execute:
systemctl status postgresql@10-main.service
I get the following message:
postgresql@10-main.service - PostgreSQL Cluster 10-main
  Loaded: loaded (/lib/systemd/system/postgresql@.service; indirect; vendor preset: enabled)
  Active: failed (Result: protocol) since Wed 2020-02-19 17:57:22 CET; 30 min ago
 Process: 1602 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 10-main start (code_exited, status=1/FAILURE)
PC_info systemd[1]: Starting PostgreSQL Cluster 10-main...
PC_info postgresql@10-main[1602]: Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-19-main.log -s -o -c config_file=""/etc/postgresql/10/main/postgresql.conf"" exit with status 1:
PC_info systemd[1]: postgresql@10-main.service: Can't open PID file /var/run/postgresql/10-main.pid (yet?) after start: No such file or directory
PC_info systemd[1]: postgresql@10-main.service: Failed with result 'protocol'.
PC_info systemd[1]: Failed to start PostgreSQL Cluster 10-main.
PC_info is information about my computer (user, date..) not relevant
I got this error from one day to an other without touching anything related to Database Servers.
I tried to fix it by my self but nothing worked
Writing the command 
service postgresql@10-main start
I get
Job for postgresql@10-main.service failed because the service did not take the steps required by its unit configuration
See ""systemctl status postgresql@10-main.service"" and ""journalctl -xe""  for details.
Running this two command I get the message from the beginning.
Anyone has an idea of what is happening? How I can fix it?
",<postgresql><ubuntu><pid><boot>,1868,0,13,230,1,2,10,64,32630,0,43,5,15,2020-02-19 18:02,2020-05-30 4:14,,101,,Basic,14
51913783,Removing Duplicate Rows in PostgreSQL with multiple columns,"I have a table ""votes"" with the following columns: 
voter, election_year, election_type, party
I need to remove all duplicate rows of the combination of voter and election_year, and I'm having trouble figuring out how to do this.
I ran the following:
WITH CTE AS(
SELECT voter, 
       election_year,
       ROW_NUMBER()OVER(PARTITION BY voter, election_year ORDER BY voter) as RN
FROM votes
)
DELETE
FROM CTE where RN&gt;1
based on another StackOverflow answer, but it seems this is specific to SQL Server.  I've seen ways to do this using unique ID's, but this particular table doesn't have that luxury.  How can I adopt the above script to remove the duplicates I need?  Thanks!
EDIT: Per request, creation of the table with some example data:
CREATE TABLE public.votes
(
    voter varchar(10),
    election_year smallint,
    election_type varchar(2),
    party varchar(3)
);
INSERT INTO votes
    (voter, election_year, election_type, party)
VALUES
    ('2435871347', 2018, 'PO', 'EV'),
    ('2435871347', 2018, 'RU', 'EV'),
    ('2435871347', 2018, 'GE', 'EV'),
    ('2435871347', 2016, 'PO', 'EV'),
    ('2435871347', 2016, 'GE', 'EV'),
    ('10215121/8', 2016, 'GE', 'ED')
;
",<sql><postgresql>,1183,0,33,813,2,9,23,49,18245,0,89,3,15,2018-08-19 1:38,2018-08-19 2:34,2018-08-19 2:34,0,0,Basic,10
52075642,"How to handle unique data in SQLAlchemy, Flask, Python","How do you usually handle unique database entries in Flask? I have the following column in my db model:
bank_address = db.Column(db.String(42), unique=True)
The problem is, that even before I can make a check whether it is already in the database or not, I get an error: 
Check if it is unique and THEN write into db:
if request.method == 'POST':
    if user.bank_address != request.form['bank_address_field']:
        user.bank_address = request.form['bank_address_field']
        db.session.add(user)
        db.session.commit()
The error I get:
  sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE
  constraint failed: user.bank_address_field [SQL: 'UPDATE user SET
  bank_address_field=? WHERE user.id = ?']
",<python><sqlite><flask><sqlalchemy><unique>,724,0,6,4510,8,59,112,44,20853,0,671,2,15,2018-08-29 10:50,2018-08-29 10:57,2018-08-29 10:57,0,0,Basic,13
58600623,MySQL UUID function produces the same value when used as a function parameter,"The UUID() function by itself produces a different value each time it is called, as I would expect it to do:
SELECT UUID() from INFORMATION_SCHEMA.TABLES LIMIT 3;
3bb7d468-f9c5-11e9-8349-d05099466715
3bb7d482-f9c5-11e9-8349-d05099466715
3bb7d492-f9c5-11e9-8349-d05099466715
However, as soon as we use it within the REPLACE() function, it begins producing the same value:
SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
e0f2d47a-f9c5-11e9-8349-d05099466715
This 'breaks' Insert From Select statements like this where we expect each inserted row to have a unique value:
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(UUID(),'-','')), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
Note, I am using the information schema's list of tables for convenience.  It shouldn't matter, but for those that are curious, our PK's are UUIDs in binary(16) form.  I can't change that; please don't focus on that.
The UUID() function is non-deterministic, while the REPLACE() function is deterministic.  I would have expected the non-deterministic characteristic of the UUID() function to result in the REPLACE() function behaving as if it had a different argument for each row, but it seems as though the DB engine is over optimizing by assuming the UUID() to be constant.
I also tested this behavior with another non-deterministic function, RAND(), and in this case the REPLACE() function worked as we'd expect!
SELECT REPLACE(RAND(),' ',' ') from INFORMATION_SCHEMA.TABLES LIMIT 3;
0.911571646026868
0.626416072832808
0.6977608461843439
Questions:
Is there a way to perform an ""Insert From Select"" and generate a unique UUID in binary 16 form per row in the select?
Why is this happening?  Is this a bug?
Updates
I am using 5.7.27 locally:
mysql  Ver 14.14 Distrib 5.7.27, for Linux (x86_64)
But this will end up deploying to an AWS RDS instance.  lol... The Terraform (scripted deploy) spins up an AWS RDS instance with engine version 5.7.16.
Looking in the AWS console, I see support up to version 5.7.26 (in the 5.7 vein) and 8.0.16 (in the 8.0 vein).  I'll discuss upgrading the deployed engine version.  I'd love to change the PK column definitions to default the values as @Schwern has suggested.
Work Around
Until I can get others to agree to a version change, I'm moving forward by using a temporary table as intermediate storage for generated id values.
CREATE TEMPORARY TABLE GeneratedIds (
    generatedId varchar(36) NOT NULL,
    tableName text NOT NULL
);
INSERT INTO GeneratedIds (generatedId, tableName)
SELECT UUID(), TABLE_NAME from INFORMATION_SCHEMA.TABLES;
INSERT INTO MyTable (uid, tableName) -- uid is binary(16)
SELECT UNHEX(REPLACE(generatedId,'-','')), tableName FROM GeneratedIds;
DROP TABLE GeneratedIds;
This is not very elegant, but it does work.  In my case I am working within a sql migration file where I can string together this kind of sequence of sql in a cohesive manner.  I wouldn't recommend doing this in code; it smells.
Conclusion
This does appear to be a bug in MySQL.  I did a quick search of their bug DB but I did not find a mention of it.  Regardless, the SQL statements above illustrate the defect, and @Schwern and I have shown that this bug has been fixed in version 5.7.27 (exactly) and version 8.0.16 (possibly all 8.., only tested 8.0.16 and 8.0.18).
Version 8.0.16 test:
Server version: 8.0.16 MySQL Community Server - GPL
Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql&gt; SELECT REPLACE(UUID(),'-','-') from INFORMATION_SCHEMA.TABLES LIMIT 3;
+--------------------------------------+
| REPLACE(UUID(),'-','-')              |
+--------------------------------------+
| 96f9205a-fdc6-11e9-87de-d05099466715 |
| 96f920f9-fdc6-11e9-87de-d05099466715 |
| 96f9213e-fdc6-11e9-87de-d05099466715 |
+--------------------------------------+
3 rows in set (0.00 sec)
",<mysql>,4172,0,55,697,0,6,22,37,4332,0,8,1,15,2019-10-29 2:10,2019-10-29 3:27,2019-10-29 3:27,0,0,Basic,10
51573145,Formula `mysql` is not installed,"I'm trying to install mysql using brew services start mysql as per the website instructions but it gives me an error:
Formula `mysql` is not installed.
I already did a full uninstall on my XAMPP server.
Terminal:
Dylans-Macbook:~ dylandude$ brew install mysql@5.7
Updating Homebrew...
==&gt; Auto-updated Homebrew!
Updated 1 tap (homebrew/core).
==&gt; Updated Formulae
ansible                    jpeg-turbo                 phplint
ccrypt                     mapcrafter                 skinny
dxpy                       mikutter
jenkins-job-builder        pgcli
Warning: mysql@5.7 5.7.23 is already installed and up-to-date
To reinstall 5.7.23, run `brew reinstall mysql@5.7`
Dylans-Macbook:~ dylandude$ brew services start mysql
Error: Formula `mysql` is not installed.
",<mysql><homebrew>,771,0,16,161,1,1,4,58,21732,0,0,1,15,2018-07-28 16:18,2018-07-29 19:56,,1,,Basic,14
50996079,"""Login timeout expired"" error when accessing MS SQL db via sqlalchemy and pyodbc","So I have some trouble getting sqlalchemy and pyodbc working with a remote MS SQL Server. Local sqlcmd worked properly but not when I try to read the db via python code. Any help would be appreciated. 
Environment:
Centos 7
SQLCmd version: Version 17.1.0000.1 Linux
MS SQL Server 6.01.7601.17514
Python 2.7
The following sqlcmd worked properly
sqlcmd -S {Host},{Port} -U {USER} -P {PWD} -Q ""use {Database};""
Attempts to work with sqlalchemy or pyodbc directly didn't work. Error:
pyodbc.OperationalError: ('HYT00', u'[HYT00] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')
Code:
Attempt with pyodbc
conn = pyodbc.connect(
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    r'SERVER=HOST,PORT;'
    r'DATABASE=DATABASE;'
    r'UID=UID;'
    r'PWD=PWD'
    )
Attempt with sqlalchemy:
create_engine('mssql+pyodbc://{user}:{password}@{host}:{port}/{database}?driver={driver}'.format(
        user=user,
        password=password,
        host=host,
        database=database,
        port=port,
        driver=""ODBC+Driver+17+for+SQL+Server""
    )).connect()
I can reproduce the error with sqlcmd if I remove the port from the command, so maybe the conn_string I am passing to pyodbc is not in the correct format?
",<sql-server><sqlalchemy><pyodbc><sqlcmd>,1261,0,16,2061,5,27,40,39,20837,,11,3,15,2018-06-22 21:59,2018-09-14 11:10,2018-09-14 11:10,84,84,Basic,14
48927271,Count number of words in a spark dataframe,"How can we find the number of words in a column of a spark dataframe without using REPLACE() function of SQL ? Below is the code and input I am working with but the replace() function does not work.
from pyspark.sql import SparkSession
my_spark = SparkSession \
    .builder \
    .appName(""Python Spark SQL example"") \
    .enableHiveSupport() \
    .getOrCreate()
parqFileName = 'gs://caserta-pyspark-eval/train.pqt'
tuesdayDF = my_spark.read.parquet(parqFileName)
tuesdayDF.createOrReplaceTempView(""parquetFile"")
tuesdaycrimes = spark.sql(""SELECT LENGTH(Address) - LENGTH(REPLACE(Address, ' ', ''))+1 FROM parquetFile"")
print(tuesdaycrimes.show())
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|          X|        Y|
+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-----------+---------+
|2015-05-14 03:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST| -122.42589|37.774597|
|2015-05-14 03:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...| -122.42436|37.800415|
",<python><apache-spark><pyspark><apache-spark-sql>,1473,0,22,159,1,1,6,76,48389,0,0,4,14,2018-02-22 12:20,2018-02-22 13:15,,0,,Basic,2
57539722,How to clear a Cosmos DB database or delete all items using Azure portal,"If go to https://portal.azure.com, open our Azure Cosmos DB account (1) --&gt; Data Explorer (2) --&gt; Click on users (3) --&gt; Click on New SQL Query:
Azure will open a text box to enter a Query:
I've found that Cosmos DB does not allow the usage of DELETE instead of SELECT: https://stackoverflow.com/a/48339202/1198404, so I should do something like:
SELECT * FROM c DELETE c
SELECT * FROM c DELETE *
But none of my attempts worked.
",<azure><azure-cosmosdb><azureportal><azure-cosmosdb-sqlapi>,438,5,2,3849,6,58,90,42,76294,0,216,5,14,2019-08-17 20:28,2019-08-17 22:30,2019-08-18 13:17,0,1,Basic,1
58880998,"Communications link failure , Spring Boot + MySql +Docker + Hibernate","I'm working with spring boot, hibernate &amp; MySql. While running the application it is running well as per expectation . But while making the docker-compose file and running the app docker image with mysql docker image it gives this error.
Error
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
java.net.ConnectException: Connection refused.
private Connection createConnection() throws SQLException 
{
        DriverManager.registerDriver(new com.mysql.jdbc.Driver());
        String mysqlUrl = &quot;jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false&quot;;
        Connection connection = DriverManager.getConnection(mysqlUrl, &quot;root&quot;, &quot;root&quot;);
        return connection;
}
Application.properties
spring.datasource.url=jdbc:mysql://localhost/database?autoReconnect=true&amp;useSSL=false
spring.datasource.username=root
spring.datasource.password=root
Please guide me how to tackle this.
docker-compose.yml
version: '3'
services:
  docker-mysql:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=database
      - MYSQL_USER=root
      - MYSQL_PASSWORD=root
    ports:
      - 3307:3306
  app:
    image: app:latest
    ports:
       - 8091:8091
    depends_on:
       - docker-mysql
",<mysql><spring><spring-boot><docker><docker-compose>,1303,0,26,627,5,15,33,72,43054,0,436,6,14,2019-11-15 16:21,2019-11-15 16:50,2019-11-15 17:17,0,0,Advanced,37
57803604,Homebrew Mariadb Mysql installation root access denied,"So I basically am installing mariadb with mysql on my mac using homebrew.
These are the steps I made:
brew doctor -> worked 
brew update -> worked 
brew install mariadb -> worked 
mysql_install_db -> Failed
  WARNING: The host 'Toms-MacBook-Pro.local' could not be looked up
  with /usr/local/Cellar/mariadb/10.4.6_1/bin/resolveip. This probably
  means that your libc libraries are not 100 % compatible with this
  binary MariaDB version. The MariaDB daemon, mysqld, should work
  normally with the exception that host name resolving will not work.
  This means that you should use IP addresses instead of hostnames when
  specifying MariaDB privileges ! mysql.user table already exists!
Running mysql_upgrade afterwards gave me following error:
  Version check failed. Got the following error when calling the 'mysql'
  command line client ERROR 1698 (28000): Access denied for user
  'root'@'localhost' FATAL ERROR: Upgrade failed
I can't enter mysql like this:
mysql -uroot
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
but like this:
sudo mysql -u root
The user table returns this:
MariaDB [(none)]&gt; USE mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
MariaDB [mysql]&gt; SELECT User, Host, plugin FROM mysql.user;
+---------------+-------------------------+-----------------------+
| User          | Host                    | plugin                |
+---------------+-------------------------+-----------------------+
| root          | localhost               | mysql_native_password |
| toms          | localhost               | mysql_native_password |
|               | localhost               |                       |
|               | toms-macbook-pro.local |                       |
+---------------+-------------------------+-----------------------+
4 rows in set (0.004 sec)
",<mysql><macos><mariadb><homebrew>,1945,0,19,259,1,3,10,77,14310,0,1,5,14,2019-09-05 10:38,2019-09-05 12:29,2019-09-05 12:41,0,0,Advanced,39
49400284,Count number of rows in golang,"I want to display the number of rows from database using Go. How do I display number of rows?
count, err := db.Query(""SELECT COUNT(*) FROM main_table"")
",<mysql><go>,152,0,1,205,1,3,9,51,27443,0,17,2,14,2018-03-21 7:12,2018-03-21 7:42,2018-03-21 7:42,0,0,Basic,9
55304197,Array difference in postgresql,"I have two arrays [1,2,3,4,7,6] and [2,3,7] in PostgreSQL which may have common elements. What I am trying to do is to exclude from the first array all the elements that are present in the second. 
So far I have achieved the following:
SELECT array
  (SELECT unnest(array[1, 2, 3, 4, 7, 6])
   EXCEPT SELECT unnest(array[2, 3, 7]));
However, the ordering is not correct as the result is {4,6,1} instead of the desired {1,4,6}.
How can I fix this ? 
I finally created a custom function with the following definition (taken from here) which resolved my issue:
create or replace function array_diff(array1 anyarray, array2 anyarray)
returns anyarray language sql immutable as $$
    select coalesce(array_agg(elem), '{}')
    from unnest(array1) elem
    where elem &lt;&gt; all(array2)
$$;
",<arrays><postgresql>,788,1,13,1251,2,18,38,49,8540,0,111,2,14,2019-03-22 16:41,2019-03-22 17:09,2019-03-22 17:09,0,0,Intermediate,18
63066240,Setup postgres in Github Actions for Django,"I'm currently working on a website right now on Django. On my computer, I am running it on Docker with a postgres database. Here's the docker-compose file I have:
version: '3'
services:
    db:
        image: postgres
        environment:
            - POSTGRES_DB=postgres
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
    web:
        build: .
        volumes:
            - .:/usr/src/app
        ports:
            - &quot;8000:8000&quot;
And here's the relevant part in settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'postgres',
        'USER': 'postgres',
        'PASSWORD': 'postgres',
        'HOST': 'db',
        'PORT': 5432,
    }
}
When I run my tests in the docker container with this setup, it works find and the tests run. However, in github actions, it doesn't work. Here's my workflow file:
name: Django CI
on: push
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4
      matrix:
        python-version: [3.7, 3.8]
    services:
      db:
        image: postgres
        env:
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v1
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Run Tests
      run: |
        python manage.py test
When this runs in github actions, I get the following error:
django.db.utils.OperationalError: could not translate host name &quot;db&quot; to address: Temporary failure in name resolution
Could someone please help me with this, and please let me know if you need anymore code.
",<django><github-actions><django-testing><django-postgresql>,1931,0,66,195,0,2,7,76,4544,0,1,2,14,2020-07-24 3:17,2020-08-10 18:49,2020-08-10 18:49,17,17,Intermediate,23
49546011,Unable to connect mysql to spring boot project,"i am following this https://spring.io/guides/gs/accessing-data-mysql/ guide to connect mysql db to spring boot project
but getting following error when running the application, i am generating spring starter project and only selecting web, mysql and jpa boxes while creating project via spring tool suite
2018-03-28 16:48:42.125 ERROR 15452 --- [           main] com.zaxxer.hikari.HikariConfig           : Failed to load driver class com.mysql.jdbc.Driver from HikariConfig class classloader jdk.internal.loader.ClassLoaders$AppClassLoader@782830e
2018-03-28 16:48:42.128  WARN 15452 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.context.properties.bind.BindException: Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource
2018-03-28 16:48:42.130  INFO 15452 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 16:48:42.142  INFO 15452 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 16:48:42.143 ERROR 15452 --- [           main] o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to bind properties under '' to com.zaxxer.hikari.HikariDataSource:
    Property: driverclassname
    Value: com.mysql.jdbc.Driver
    Origin: ""driverClassName"" from property source ""source""
    Reason: Unable to set value for property driver-class-name
Action:
Update your application's configuration
following is application.properties 
spring.jpa.hibernate.ddl-auto=create
spring.datasource.url=jdbc:mysql://localhost:3306/world
spring.datasource.username=root
spring.datasource.password=admin
and pom.xml
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;proj.mine&lt;/groupId&gt;
    &lt;artifactId&gt;training-app-2&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;name&gt;training-app-2&lt;/name&gt;
    &lt;description&gt;traning practive app&lt;/description&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
EDIT: Added spring.datasource.driver-class-name=com.mysql.jdbc.Driver in application.properties, error still persists
2018-03-28 17:55:05.641  WARN 3140 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
2018-03-28 17:55:05.643  INFO 3140 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2018-03-28 17:55:05.656  INFO 3140 --- [           main] ConditionEvaluationReportLoggingListener : 
Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
2018-03-28 17:55:05.662 ERROR 3140 --- [           main] o.s.boot.SpringApplication               : Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:587) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1250) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1099) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:502) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:312) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:310) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:760) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:868) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549) ~[spring-context-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:388) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:327) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1246) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1234) [spring-boot-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at proj.mine.TrainingApp2Application.main(TrainingApp2Application.java:10) [classes/:na]
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:579) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 18 common frames omitted
Caused by: java.lang.IllegalStateException: Cannot load driver class: com.mysql.jdbc.Driver
    at org.springframework.util.Assert.state(Assert.java:94) ~[spring-core-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:224) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:81) ~[spring-boot-autoconfigure-2.0.0.RELEASE.jar:2.0.0.RELEASE]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Unknown Source) ~[na:na]
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.0.4.RELEASE.jar:5.0.4.RELEASE]
    ... 19 common frames omitted
",<mysql><spring><spring-boot><spring-data-jpa><spring-tool-suite>,11921,6,130,147,1,3,9,39,44053,0,2,7,14,2018-03-29 0:00,2018-03-29 12:51,2018-03-29 12:51,0,0,Basic,12
56246710,How to resolve DB connection invalidated warning in Airflow Scheduler?,"I am upgrading our Airflow instance from 1.9 to 1.10.3 and whenever the scheduler runs now I get a warning that the database connection has been invalidated and it's trying to reconnect. A bunch of these errors show up in a row. The console also indicates that tasks are being scheduled but if I check the database nothing is ever being written.
The following warning shows up where it didn't before
[2019-05-21 17:29:26,017] {sqlalchemy.py:81} WARNING - DB connection invalidated. Reconnecting...
Eventually, I'll also get this error
FATAL: remaining connection slots are reserved for non-replication superuser connections
I've tried to increase the SQL Alchemy pool size setting in airflow.cfg but that had no effect
# The SqlAlchemy pool size is the maximum number of database connections in the pool.
sql_alchemy_pool_size = 10
I'm using CeleryExecutor and I'm thinking that maybe the number of workers is overloading the database connections.
I run three commands, airflow webserver, airflow scheduler, and airflow worker, so there should only be one worker and I don't see why that would overload the database.
How do I resolve the database connection errors? Is there a setting to increase the number of database connections, if so where is it? Do I need to handle the workers differently?
Update:
Even with no workers running, starting the webserver and scheduler fresh, when the scheduler fills up the airflow pools the DB connection warning starts to appear.
Update 2:
I found the following issue in the Airflow Jira: https://issues.apache.org/jira/browse/AIRFLOW-4567
There is some activity with others saying they see the same issue. It is unclear whether this directly causes the crashes that some people are seeing or whether this is just an annoying cosmetic log. As of yet there is no resolution to this problem.
",<database><postgresql><sqlalchemy><airflow><airflow-scheduler>,1829,2,8,411,0,3,14,63,3873,0,257,1,14,2019-05-21 21:39,2019-08-16 16:55,2019-08-16 16:55,87,87,Advanced,39
54920939,Parsing FB-Purity's Firefox idb (Indexed Database API) object_data blob from Linux bash,"From a Linux bash script, I want to read the structured data stored by a particular Firefox add-on called FB-Purity.
I have found a folder called .mozilla/firefox/b8eab5j0.default/storage/default/moz-extension+++37a9788c-671d-4cae-ba5c-fbdb8788499a^userContextId=4294967295/ that contains a .metadata file which contains the string moz-extension://37a9788c-671d-4cae-ba5c-fbdb8788499a, an URL which when opened in Firefox shows the add-on's details, so I am pretty sure that this folder belongs to the add-on.
That folder contains an idb directory, which sounds like Indexed Database API, a W3C standard apparently used since last year by Firefox it to store add-ons data.
The idb folder only contains an empty folder and an SQLite file.
The SQLite file, unfortunately, does not contain much application structured data, but the object_data table contains a 95KB blob which probably contains the real structured data:
INSERT INTO `object_data` VALUES (1,'0pmegsjfoetupsf.742612367',NULL,NULL,
X'e08b0d0403000101c0f1ffe5a201000400ffff7b00220032003100380035003000320022003a002
2005300610074006f0072007500200055007205105861006e00690022002c00220036003100350036
[... 95KB ...]
00780022007d00000000000000');
Question: Any clue what this blob's format is? How to extract it (using command line or any library or Linux tool) to JSON or any other readable format?
",<sqlite><firefox-addon><blob><indexeddb><firefox-addon-webextensions>,1355,2,11,58957,59,224,377,52,2601,0,2729,1,14,2019-02-28 8:02,2020-01-26 22:23,2020-01-26 22:23,332,332,Intermediate,26
51625671,Routing to Different SQL Server Instances Running through Docker on Default Port,"I can use Traefik for web sites since they use headers when they are connecting.
But I want to have multiple different instances of SQL Server running through docker which will be externally available (outside the docker host, potentially outside the local network)
So, is there anything which allows connecting to different sql server instances running on the same docker instance WITHOUT having to give them different ports or external ip addresses such that someone could access
sql01.docker.local,1433 AND sql02.docker.local,1433 from SQL Tools.
Start Additional Question
Since there has been no replies perhaps there is a way to have different instances like: sql.docker.local\instance1 and sql.docker.local\instance2 though I imagine that may also not be possible
End  Additional Question
This is an example of the docker-compose file I was trying to use (before I realised that queries to sql server don't send through a host header - or am I wrong about that?)
version: '2.1'
services:
  traefik:
    container_name: traefik
    image: stefanscherer/traefik-windows
    command: --docker.endpoint=tcp://172.28.80.1:2375 --logLevel=DEBUG
    ports:
      - ""8080:8080""
      - ""80:80""
      - ""1433:1433""
    volumes:
      - ./runtest:C:/etc/traefik
      - C:/Users/mvukomanovic.admin/.docker:C:/etc/ssl
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.enable=false""
  whoami:
    image: stefanscherer/whoami
    labels:
      - ""traefik.backend=whoami""
      - ""traefik.frontend.entryPoints=http""
      - ""traefik.port=8080""
      - ""traefik.frontend.rule=Host:whoami.docker.local""
    networks:
      - default
    restart: unless-stopped
  sql01:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql01
    domainname: sql01.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql01.docker.local,sql01,sql01.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
  sql02:
    image: microsoft/mssql-server-windows-developer
    environment:
      - ACCEPT_EULA=Y
    hostname: sql02
    domainname: sql02.local
    networks:
      - default
    restart: unless-stopped
    labels:
      - ""traefik.frontend.rule=Host:sql02.docker.local,sql02,sql02.local""
      - ""traefik.frontend.entryPoints=mssql""
      - ""traefik.port=1433""
      - ""traefik.frontend.port=1433""
    networks:
      - default
    restart: unless-stopped    
networks:
  default:
    external:
      name: nat
",<sql-server><docker><docker-compose><traefik>,2667,0,74,1402,1,16,24,70,2660,0,907,3,14,2018-08-01 5:05,2018-08-07 14:40,,6,,Advanced,32
60869614,Pyspark: how to extract hour from timestamp,"I have a table like the following
    df
 +------------------------------------+-----------------------+
|identifier                          |timestamp              |
+------------------------------------+-----------------------+
|86311425-0890-40a5-8950-54cbaaa60815|2020-03-18 14:41:55 UTC|
|38e121a8-f21f-4d10-bb69-26eb045175b5|2020-03-13 15:19:21 UTC|
|1a69c9b0-283b-4b6d-89ac-66f987280c66|2020-03-16 12:59:51 UTC|
|c7b5c53f-bf40-498f-8302-4b3329322bc9|2020-03-18 22:05:06 UTC|
|0d3d807b-9b3a-466e-907c-c22402240730|2020-03-17 18:40:03 UTC|
+------------------------------------+-----------------------+
tmp.printSchema()
root
 |-- identifier: string (nullable = true)
 |-- timestamp: string (nullable = true)
I would like to have a column that take only the day and the hours from the timestamp.
I am trying the following:
from pyspark.sql.functions import hour
df = df.withColumn(""hour"", hour(col(""timestamp"")))
but I get the following
+--------------------+--------------------+----+
|          identifier|           timestamp|hour|
+--------------------+--------------------+----+
|321869c3-71e5-41d...|2020-03-19 03:34:...|null|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|null|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|null|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|null|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|null|
+--------------------+--------------------+----+
while I would like to have
+--------------------+--------------------+-------------------+
|          identifier|           timestamp|hour               |
+--------------------+--------------------+-------------------+
|321869c3-71e5-41d...|2020-03-19 03:00:...|2020-03-19 03:00:00|
|226b8d50-2c6a-471...|2020-03-19 02:59:...|2020-03-19 02:00:00|
|47818b7c-34b5-43c...|2020-03-19 01:41:...|2020-03-19 01:00:00|
|f5ca5599-7252-49d...|2020-03-19 04:25:...|2020-03-19 04:00:00|
|add2ae24-aa7b-4d3...|2020-03-19 01:50:...|2020-03-19 01:00:00|
+--------------------+--------------------+-------------------+
",<python><sql><pyspark>,1987,0,36,7043,20,76,142,58,33584,0,107,5,14,2020-03-26 14:32,2020-03-26 15:05,,0,,Basic,2
56093317,"Google Query - ""NOT LIKE"" Statement Doesn't work","The following line doesn't work:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB NOT LIKE '%Info%'&quot;,1)
Throws:
Unable to parse query string for Function QUERY parameter 2: PARSE_ERROR: Encountered &quot;  &quot;AH &quot;&quot; at line 1, column 26. Was expecting one of: &quot;(&quot; ... &quot;(&quot;
However, this one does:
=QUERY(AB:AE,&quot;select AB,AC,AD,AE where AB LIKE '%Info%'&quot;,1)
",<sql><google-sheets><google-sheets-formula><google-query-language>,401,0,2,482,1,10,18,43,17326,,342,2,14,2019-05-11 18:47,2019-05-11 18:51,2019-05-27 9:07,0,16,Basic,2
56329093,Memory leaks when using pandas_udf and Parquet serialization?,"I am currently developing my first whole system using PySpark and I am running into some strange, memory-related issues. In one of the stages, I would like to resemble a Split-Apply-Combine strategy in order to modify a DataFrame. That is, I would like to apply a function to each of the groups defined by a given column and finally combine them all. Problem is, the function I want to apply is a prediction method for a fitted model that ""speaks"" the Pandas idiom, i.e., it is vectorized and takes a Pandas Series as an input. 
I have then designed an iterative strategy, traversing the groups and manually applying a pandas_udf.Scalar in order to solve the problem. The combination part is done using incremental calls to DataFrame.unionByName(). I have decided not to use the GroupedMap type of pandas_udf because the docs state that the memory should be managed by the user, and you should have special care whenever one of the groups might be too large to keep it in memory or be represented by a Pandas DataFrame.
The main problem is that all the processing seems to run fine, but in the end I want to serialize the final DataFrame to a Parquet file. And it is at this point where I receive a lot of Java-like errors about DataFrameWriter, or out-of-memory exceptions.
I have tried the code in both Windows and Linux machines. The only way I have managed to avoid the errors has been to increase the --driver-memory value in the machines. The minimum value is different in every platform, and is dependent on the size of the problem, which somehow makes me suspect on memory leaks.
The problem did not happen until I started using pandas_udf. I think that there is probably a memory leak somewhere in the whole process of pyarrow serialization taking place under the hood when using a pandas_udf.
I have created a minimal reproducible example. If I run this script directly using Python, it produces the error. Using spark-submit and increasing a lot the driver memory, it is possible to make it work. 
import pyspark
import pyspark.sql.functions as F
import pyspark.sql.types as spktyp
# Dummy pandas_udf -------------------------------------------------------------
@F.pandas_udf(spktyp.DoubleType())
def predict(x):
    return x + 100.0
# Initialization ---------------------------------------------------------------
spark = pyspark.sql.SparkSession.builder.appName(
        ""mre"").master(""local[3]"").getOrCreate()
sc = spark.sparkContext
# Generate a dataframe ---------------------------------------------------------
out_path = ""out.parquet""
z = 105
m = 750000
schema = spktyp.StructType(
    [spktyp.StructField(""ID"", spktyp.DoubleType(), True)]
)
df = spark.createDataFrame(
    [(float(i),) for i in range(m)],
    schema
)
for j in range(z):
    df = df.withColumn(
        f""N{j}"",
        F.col(""ID"") + float(j)
    )
df = df.withColumn(
    ""X"",
    F.array(
        F.lit(""A""),
        F.lit(""B""),
        F.lit(""C""),
        F.lit(""D""),
        F.lit(""E"")
    ).getItem(
        (F.rand()*3).cast(""int"")
    )
)
# Set the column names for grouping, input and output --------------------------
group_col = ""X""
in_col = ""N0""
out_col = ""EP""
# Extract different group ids in grouping variable -----------------------------
rows = df.select(group_col).distinct().collect()
groups = [row[group_col] for row in rows]
print(f""Groups: {groups}"")
# Split and treat the first id -------------------------------------------------
first, *others = groups
cur_df = df.filter(F.col(group_col) == first)
result = cur_df.withColumn(
    out_col,
    predict(in_col)
)
# Traverse the remaining group ids ---------------------------------------------
for i, other in enumerate(others):
    cur_df = df.filter(F.col(group_col) == other)
    new_df = cur_df.withColumn(
        out_col,
        predict(in_col)
    )
    # Incremental union --------------------------------------------------------
    result = result.unionByName(new_df)
# Save to disk -----------------------------------------------------------------
result.write.mode(""overwrite"").parquet(out_path)
Shockingly (at least for me), the problem seems to vanish if I put a call to repartition() just before the serialization statement.
result = result.repartition(result.rdd.getNumPartitions())
result.write.mode(""overwrite"").parquet(out_path)
Having put this line into place, I can lower a lot the driver memory configuration, and the script runs fine. I can barely understand the relationship among all those factors, although I suspect lazy evaluation of the code and pyarrow serialization might be related.
This is the current environment I am using for development:
arrow-cpp                 0.13.0           py36hee3af98_1    conda-forge
asn1crypto                0.24.0                py36_1003    conda-forge
astroid                   2.2.5                    py36_0
atomicwrites              1.3.0                      py_0    conda-forge
attrs                     19.1.0                     py_0    conda-forge
blas                      1.0                         mkl
boost-cpp                 1.68.0            h6a4c333_1000    conda-forge
brotli                    1.0.7             he025d50_1000    conda-forge
ca-certificates           2019.3.9             hecc5488_0    conda-forge
certifi                   2019.3.9                 py36_0    conda-forge
cffi                      1.12.3           py36hb32ad35_0    conda-forge
chardet                   3.0.4                 py36_1003    conda-forge
colorama                  0.4.1                    py36_0
cryptography              2.6.1            py36hb32ad35_0    conda-forge
dill                      0.2.9                    py36_0
docopt                    0.6.2                    py36_0
entrypoints               0.3                      py36_0
falcon                    1.4.1.post1     py36hfa6e2cd_1000    conda-forge
fastavro                  0.21.21          py36hfa6e2cd_0    conda-forge
flake8                    3.7.7                    py36_0
future                    0.17.1                py36_1000    conda-forge
gflags                    2.2.2                ha925a31_0
glog                      0.3.5                h6538335_1
hug                       2.5.2            py36hfa6e2cd_0    conda-forge
icc_rt                    2019.0.0             h0cc432a_1
idna                      2.8                   py36_1000    conda-forge
intel-openmp              2019.3                      203
isort                     4.3.17                   py36_0
lazy-object-proxy         1.3.1            py36hfa6e2cd_2
libboost                  1.67.0               hd9e427e_4
libprotobuf               3.7.1                h1a1b453_0    conda-forge
lz4-c                     1.8.1.2              h2fa13f4_0
mccabe                    0.6.1                    py36_1
mkl                       2018.0.3                      1
mkl_fft                   1.0.6            py36hdbbee80_0
mkl_random                1.0.1            py36h77b88f5_1
more-itertools            4.3.0                 py36_1000    conda-forge
ninabrlong                0.1.0                     dev_0    &lt;develop&gt;
nose                      1.3.7                 py36_1002    conda-forge
nose-exclude              0.5.0                      py_0    conda-forge
numpy                     1.15.0           py36h9fa60d3_0
numpy-base                1.15.0           py36h4a99626_0
openssl                   1.1.1b               hfa6e2cd_2    conda-forge
pandas                    0.23.3           py36h830ac7b_0
parquet-cpp               1.5.1                         2    conda-forge
pip                       19.0.3                   py36_0
pluggy                    0.11.0                     py_0    conda-forge
progressbar2              3.38.0                     py_1    conda-forge
py                        1.8.0                      py_0    conda-forge
py4j                      0.10.7                   py36_0
pyarrow                   0.13.0           py36h8c67754_0    conda-forge
pycodestyle               2.5.0                    py36_0
pycparser                 2.19                     py36_1    conda-forge
pyflakes                  2.1.1                    py36_0
pygam                     0.8.0                      py_0    conda-forge
pylint                    2.3.1                    py36_0
pyopenssl                 19.0.0                   py36_0    conda-forge
pyreadline                2.1                      py36_1
pysocks                   1.6.8                 py36_1002    conda-forge
pyspark                   2.4.1                      py_0
pytest                    4.5.0                    py36_0    conda-forge
pytest-runner             4.4                        py_0    conda-forge
python                    3.6.6                hea74fb7_0
python-dateutil           2.8.0                    py36_0
python-hdfs               2.3.1                      py_0    conda-forge
python-mimeparse          1.6.0                      py_1    conda-forge
python-utils              2.3.0                      py_1    conda-forge
pytz                      2019.1                     py_0
re2                       2019.04.01       vc14h6538335_0  [vc14]  conda-forge
requests                  2.21.0                py36_1000    conda-forge
requests-kerberos         0.12.0                   py36_0
scikit-learn              0.20.1           py36hb854c30_0
scipy                     1.1.0            py36hc28095f_0
setuptools                41.0.0                   py36_0
six                       1.12.0                   py36_0
snappy                    1.1.7                h777316e_3
sqlite                    3.28.0               he774522_0
thrift-cpp                0.12.0            h59828bf_1002    conda-forge
typed-ast                 1.3.1            py36he774522_0
urllib3                   1.24.2                   py36_0    conda-forge
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.15.26706          h3a45250_0
wcwidth                   0.1.7                      py_1    conda-forge
wheel                     0.33.1                   py36_0
win_inet_pton             1.1.0                    py36_0    conda-forge
wincertstore              0.2              py36h7fe50ca_0
winkerberos               0.7.0                    py36_1
wrapt                     1.11.1           py36he774522_0
xz                        5.2.4                h2fa13f4_4
zlib                      1.2.11               h62dcd97_3
zstd                      1.3.3                hfe6a214_0
Any hint or help would be much appreciated.
",<python><pandas><pyspark><apache-spark-sql><pyarrow>,10600,0,176,259,0,3,13,43,3318,0,48,2,14,2019-05-27 15:45,2019-06-05 15:59,,9,,Intermediate,23
48197134,How to pull specific version of Docker Postgres Image?,"I am new to docker area. I did docker pull postgres and docker pull postgres:9.5.4 , in both cases it pulling latest image as postgres 10.1 (see below). 
PostgreSQL 10.1 on x86_64-pc-linux-gnu, compiled by gcc (Debian 6.3.0-18) 6.3.0 20170516, 64-bit 
I would like to pull only 9.5.4 version of postgres image from dockers hub.
",<postgresql><docker>,328,3,0,5071,8,37,58,51,23078,0,50,1,14,2018-01-10 22:27,2018-01-10 23:00,,0,,Basic,3
48312246,How to Install MYSQL Preference Pane?,"Quick question.
I have been trying to make MySQL work to no luck. When I downloaded MySQL from https://dev.mysql.com/downloads/mysql/ (the latest version for Mac, 5.7.21), I followed all the instructions. When I go to System Preferences and click on the MySQL icon, it tells me:
Preferences Error: Could not load MySQL Preference Pane. Any solution to this? 
Thanks in advance!
",<mysql><macos>,378,2,0,151,1,1,3,43,25110,0,0,3,14,2018-01-18 0:48,2018-01-18 11:35,,0,,Basic,14
57123453,How to use Diesel with SQLite connections and avoid `database is locked` type of errors,"In my Rust application I am using Diesel to interact with an SQLite database. I have multiple threads that may query at the same time the database, and I am using the crate r2d2 to create a pool of connections.
The issue that I am seeing is that I am not able to concurrently query the database. If I try to do that, I always get the error database is locked, which is unrecoverable (any following request will fail from the same error even if only a single thread is querying).
The following code reproduces the issue.
# Cargo.toml
[dependencies]
crossbeam = { version = ""0.7.1"" }
diesel = { version = ""1.4.2"", features = [""sqlite"", ""r2d2""] }
-- The database table
CREATE TABLE users (
    name TEXT PRIMARY KEY NOT NULL
);
#[macro_use]
extern crate diesel;
mod schema;
use crate::schema::*;
use crossbeam;
use diesel::r2d2::{ConnectionManager, Pool};
use diesel::RunQueryDsl;
use diesel::{ExpressionMethods, SqliteConnection};
#[derive(Insertable, Queryable, Debug, Clone)]
#[table_name = ""users""]
struct User {
    name: String,
}
fn main() {
    let db_url = ""test.sqlite3"";
    let pool = Pool::builder()
        .build(ConnectionManager::&lt;SqliteConnection&gt;::new(db_url))
        .unwrap();
    crossbeam::scope(|scope| {
        let pool2 = pool.clone();
        scope.spawn(move |_| {
            let conn = pool2.get().unwrap();
            for i in 0..100 {
                let name = format!(""John{}"", i);
                diesel::delete(users::table)
                    .filter(users::name.eq(&amp;name))
                    .execute(&amp;conn)
                    .unwrap();
            }
        });
        let conn = pool.get().unwrap();
        for i in 0..100 {
            let name = format!(""John{}"", i);
            diesel::insert_into(users::table)
                .values(User { name })
                .execute(&amp;conn)
                .unwrap();
        }
    })
    .unwrap();
}
This is the error as shown when the application panics:
thread '&lt;unnamed&gt;' panicked at 'called `Result::unwrap()` on an `Err` value: DatabaseError(__Unknown, ""database is locked"")'
AFAIK, I should be able to use the connection pool with multiple threads (that is, multiple connections for multiple threads), as shown in the r2d2_sqlite crate example.
Moreover, the sqlite3 library I have installed in my system supports the Serialized threading model, which from here:
  In serialized mode, SQLite can be safely used by multiple threads with
  no restriction.
How can I avoid the database is locked errors? Also, if these errors are not avoidable for any reason, how can I unlock the database?
",<database><multithreading><sqlite><rust><rust-diesel>,2612,2,62,8838,6,45,80,50,6703,0,2767,2,14,2019-07-20 9:35,2019-08-29 20:44,2019-08-29 20:44,40,40,Advanced,32
51575004,TimescaleDB: efficiently select last row,"I have a postgres database with the timescaledb extension.
My primary index is a timestamp, and I would like to select the latest row.
If I happen to know the latest row happened after a certain time, then I can use a query such as:
query = 'select * from prices where time &gt; %(dt)s'
Here I specify a datetime, and execute the query using psycopg2:
# 2018-01-10 11:15:00
dt = datetime.datetime(2018,1,10,11,15,0)
with psycopg2.connect(**params) as conn:
    cur = conn.cursor()
    # start timing
    beg = datetime.datetime.now()
    # execute query
    cur.execute(query, {'dt':dt})
    rows = cur.fetchall()
    # stop timing
    end = datetime.datetime.now()
print('took {} ms'.format((end-beg).total_seconds() * 1e3))
The timing output:
took 2.296 ms
If, however, I don't know the time to input into the above query, I can use a query such as:
query = 'select * from prices order by time desc limit 1'
I execute the query in a similar fashion
with psycopg2.connect(**params) as conn:
    cur = conn.cursor()
    # start timing
    beg = datetime.datetime.now()
    # execute query
    cur.execute(query)
    rows = cur.fetchall()
    # stop timing
    end = datetime.datetime.now()
print('took {} ms'.format((end-beg).total_seconds() * 1e3))
The timing output:
took 19.173 ms
So that's more than 8 times slower.
I'm no expert in SQL, but I would have thought the query planner would figure out that ""limit 1"" and ""order by primary index"" equates to an O(1) operation.
Question:
Is there a more efficient way to select the last row in my table?
In case it is useful, here is the description of my table:
# \d+ prices
                                           Table ""public.prices""
 Column |            Type             | Collation | Nullable | Default | Storage | Stats target | Description 
--------+-----------------------------+-----------+----------+---------+---------+--------------+-------------
 time   | timestamp without time zone |           | not null |         | plain   |              | 
 AAPL   | double precision            |           |          |         | plain   |              | 
 GOOG   | double precision            |           |          |         | plain   |              | 
 MSFT   | double precision            |           |          |         | plain   |              | 
Indexes:
    ""prices_time_idx"" btree (""time"" DESC)
Child tables: _timescaledb_internal._hyper_12_100_chunk,
              _timescaledb_internal._hyper_12_101_chunk,
              _timescaledb_internal._hyper_12_102_chunk,
              ...
",<sql><postgresql><psycopg2><timescaledb>,2547,0,44,249,0,3,10,70,9788,0,7,4,14,2018-07-28 20:25,2018-07-30 5:20,,2,,Advanced,39
59232753,How to change a table ID from serial to identity?,"I have the following table in Postgres 10.10:
  Table ""public.client""
       Column        |  Type   | Collation | Nullable |                 Default                  
---------------------+---------+-----------+----------+------------------------------------------
 clientid            | integer |           | not null | nextval('client_clientid_seq'::regclass)
 account_name        | text    |           | not null | 
 last_name           | text    |           |          | 
 first_name          | text    |           |          | 
 address             | text    |           | not null | 
 suburbid            | integer |           |          | 
 cityid              | integer |           |          | 
 post_code           | integer |           | not null | 
 business_phone      | text    |           |          | 
 home_phone          | text    |           |          | 
 mobile_phone        | text    |           |          | 
 alternative_phone   | text    |           |          | 
 email               | text    |           |          | 
 quote_detailsid     | integer |           |          | 
 invoice_typeid      | integer |           |          | 
 payment_typeid      | integer |           |          | 
 job_typeid          | integer |           |          | 
 communicationid     | integer |           |          | 
 accessid            | integer |           |          | 
 difficulty_levelid  | integer |           |          | 
 current_lawn_price  | numeric |           |          | 
 square_meters       | numeric |           |          | 
 note                | text    |           |          | 
 client_statusid     | integer |           |          | 
 reason_for_statusid | integer |           |          | 
Indexes:
    ""client_pkey"" PRIMARY KEY, btree (clientid)
    ""account_name_check"" UNIQUE CONSTRAINT, btree (account_name)
Foreign-key constraints:
    ""client_accessid_fkey"" FOREIGN KEY (accessid) REFERENCES access(accessid)
    ""client_cityid_fkey"" FOREIGN KEY (cityid) REFERENCES city(cityid)
    ""client_client_statusid_fkey"" FOREIGN KEY (client_statusid) REFERENCES client_status(client_statusid)
    ""client_communicationid_fkey"" FOREIGN KEY (communicationid) REFERENCES communication(communicationid)
    ""client_difficulty_levelid_fkey"" FOREIGN KEY (difficulty_levelid) REFERENCES difficulty_level(difficulty_levelid)
    ""client_invoice_typeid_fkey"" FOREIGN KEY (invoice_typeid) REFERENCES invoice_type(invoice_typeid)
    ""client_job_typeid_fkey"" FOREIGN KEY (job_typeid) REFERENCES job_type(job_typeid)
    ""client_payment_typeid_fkey"" FOREIGN KEY (payment_typeid) REFERENCES payment_type(payment_typeid)
    ""client_quote_detailsid_fkey"" FOREIGN KEY (quote_detailsid) REFERENCES quote_details(quote_detailsid)
    ""client_reason_for_statusid_fkey"" FOREIGN KEY (reason_for_statusid) REFERENCES reason_for_status(reason_for_statusid)
    ""client_suburbid_fkey"" FOREIGN KEY (suburbid) REFERENCES suburb(suburbid)
Referenced by:
    TABLE ""work"" CONSTRAINT ""work_clientid_fkey"" FOREIGN KEY (clientid) REFERENCES client(clientid)
I would like to change clientid from a serial id (nextval('client_clientid_seq'::regclass)) to not null generated always as identity primary key. 
The table has 107 records which were manually entered including clientids.
How could this be done without destroying existing data?
",<sql><postgresql><auto-increment><ddl>,3346,0,48,401,0,3,10,65,6525,0,19,5,14,2019-12-08 5:38,2019-12-08 5:43,2019-12-08 7:01,0,0,Basic,10
49644232,How to set timezone to UTC in Apache Spark?,"In Spark's WebUI (port 8080) and on the environment tab there is a setting of the below:
user.timezone   Zulu
Do you know how/where I can override this to UTC?
Env details:
Spark 2.1.1
jre-1.8.0-openjdk.x86_64
no jdk
EC2 Amazon Linux
",<java><apache-spark><pyspark><apache-spark-sql><jvm>,234,0,1,264,3,18,45,58,47760,0,89,5,14,2018-04-04 6:29,2018-04-04 7:36,2018-04-04 7:36,0,0,Intermediate,20
51349719,MySQL Workbench Import NULL from CSV,"I can't for the life of me make MySQL Workbench import NULL for blank cells in a CSV. 
I've tried: 
blank
NULL
\N
Each with and without """"
How the does one signify a cell is 'null' inside a CSV file I want to import into MySQL via Workbench?
",<mysql><mysql-workbench><workbench>,242,0,0,1286,2,21,48,61,9046,0,536,4,14,2018-07-15 15:20,2019-04-29 18:21,2019-04-29 18:21,288,288,Basic,9
58475174,How do I query an array in TypeORM,"I want to create user permissions management. I use TypeORM with PostgreSQL. This is the column definition for the permissions within the user entity:
@Column({
  type: 'text',
  array: true
})
permissions: UserPermission[] = [];
This is the UserPermission enum:
export enum UserPermission {
  APP_USER = 'APP_USER',
  USER_ADMIN = 'USER_ADMIN',
  SUPERADMIN = 'SUPERADMIN'
}
I want to find one user who has the 'SUPERADMIN' permission but I cannot find the right spot in the documentation / github issues which explains how to do this. I already spent over an hour on this and I suppose this is a simple task.
Is there something like &quot;Includes&quot; to check if the permissions array includes a specific element and/or includes multiple elements?
const user = await this.userRepository.findOne({
  where: {
    permissions: Includes('SUPERADMIN')
  }
});
I would be very thankful if someone could point me to the correct documentation page :)
Edit:
The following works for me but I think it is not optimal yet:
@Column('simple-json')
permissions: string[];
let user = await this.userRepository.createQueryBuilder('user')
  .where('user.permissions like :permissions', { permissions: `%&quot;${UserPermission.SUPERADMIN}&quot;%` })
  .getOne();
",<node.js><postgresql><typescript><typeorm>,1250,0,22,191,1,1,6,56,60641,0,7,5,14,2019-10-20 16:47,2020-04-16 20:31,,179,,Basic,3
62632590,Command 01_migrate failed on Amazon Linux 2 AMI,"I have a Django project which is deployed to Elastic Beanstalk Amazon Linux 2 AMI. I installed PyMySQL for connecting to the db and i added these lines to settings.py such as below;
import pymysql
pymysql.version_info = (1, 4, 6, &quot;final&quot;, 0)
pymysql.install_as_MySQLdb()
And also i have a .config file for migrating the db;
container_commands:
  01_migrate:
    command: &quot;django-admin.py migrate&quot;
    leader_only: true
option_settings:
  aws:elasticbeanstalk:application:environment:
    DJANGO_SETTINGS_MODULE: mysite.settings
Normally, i was using mysqlclient on my Linux AMI with this .config file but it doesn't work on Linux 2 AMI so i installed the PyMySQL. Now, i'm trying to deploy the updated version of my project but i'm getting an error such as below;
Traceback (most recent call last):
  File &quot;/opt/aws/bin/cfn-init&quot;, line 171, in &lt;module&gt;
    worklog.build(metadata, configSets)
  File &quot;/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py&quot;, line 129, in build
    Contractor(metadata).build(configSets, self)
  File &quot;/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py&quot;, line 530, in build
    self.run_config(config, worklog)
  File &quot;/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py&quot;, line 542, in run_config
    CloudFormationCarpenter(config, self._auth_config).build(worklog)
  File &quot;/usr/lib/python2.7/site-packages/cfnbootstrap/construction.py&quot;, line 260, in build
    changes['commands'] = CommandTool().apply(self._config.commands)
  File &quot;/usr/lib/python2.7/site-packages/cfnbootstrap/command_tool.py&quot;, line 117, in apply
    raise ToolError(u&quot;Command %s failed&quot; % name)
ToolError: Command 01_migrate failed
How can i fix this issue?
",<django><amazon-elastic-beanstalk><pymysql><amazon-linux><amazon-linux-2>,1786,0,26,616,0,8,19,53,5968,0,35,4,14,2020-06-29 6:44,2020-07-24 13:45,2020-07-24 13:45,25,25,Advanced,40
50503394,Cannot create commands from unopened database,"I've searched around quite a lot and I cannot find any answers to this.
I am writing a Xamarin Forms Mobile application, it seems when I minimise the application and then reopen it or one of my activities get launched the following exception gets thrown:
SQLiteConnection.CreateCommand (System.String cmdText, System.Object[] ps)
SQLite.SQLiteException: Cannot create commands from unopened database
SQLiteConnection.CreateCommand (System.String cmdText, System.Object[] ps)
TableQuery`1[T].GenerateCommand (System.String selectionList)
TableQuery`1[T].GetEnumerator ()
System.Collections.Generic.List`1[T]..ctor (System.Collections.Generic.IEnumerable`1[T] collection) [0x00062] in :0
Enumerable.ToList[TSource] (System.Collections.Generic.IEnumerable`1[T] source)
AsyncTableQuery`1[T].&lt;ToListAsync&gt;b__9_0 ()
Task`1[TResult].InnerInvoke ()
Task.Execute ()
Here is my code:
Generic Repository (Where the Sqlite instance gets created)
public class Repository&lt;T&gt; : IRepository&lt;T&gt; where T : Entity, new()
{
     private readonly SQLiteAsyncConnection _db;
    public Repository(string dbPath)
    {
        _db = new SQLiteAsyncConnection(dbPath);
        _db.CreateTableAsync&lt;T&gt;().Wait();
    }
}
The IOC registration
FreshIOC.Container.Register&lt;IRepository&lt;Settings&gt;&gt;(new Repository&lt;Settings&gt;(dbPath)); // FreshIOC is a wrapper around TinyIOC
In my App.xaml.cs OnResume
protected override void OnResume()
{
    SQLiteAsyncConnection.ResetPool();
}
The above with ResetPool I put that in to see if it would make a difference but it did not.
URL Activity
protected override void OnCreate(Bundle bundle)
{
    base.OnCreate(bundle);
    var url = Intent.Data.ToString();
    var split = url.Split(new[] { ""ombi://"", ""_"" }, StringSplitOptions.RemoveEmptyEntries);
    if (split.Length &gt; 1)
    {
        var dbLocation = new FileHelper().GetLocalFilePath(""ombi.db3"");
        var repo = new Repository&lt;OmbiMobile.Models.Entities.Settings&gt;(dbLocation);
        var settings = repo.Get().Result;
        foreach (var s in settings)
        {
            var i = repo.Delete(s).Result;
        }
        repo.Save(new Settings
        {
            AccessToken = split[1],
            OmbiUrl = split[0]
        });
    }
    Intent startup = new Intent(this, typeof(MainActivity));
    StartActivity(startup);
    Finish();
}
I am not sure what else to do or look for, I can't seem to find any information about this sort of error.
Update:
After more debugging it seems to only happen after the Url activity has finished.
I have removed the DB code from the Activity and it still seems to happen. Once the Activity has launched the main App() then runs this code:
var repo = FreshIOC.Container.Resolve&lt;IRepository&lt;Settings&gt;&gt;();
try
{
    Task.Run(async () =&gt;
    {
        settings = (await repo.Get()).FirstOrDefault();
    }).Wait();
}
catch (Exception e)
{
    Debug.WriteLine(e.Message);
    throw;
}
This where the error is happening. It happens when the Get() is called which calls return _db.Table&lt;T&gt;().ToListAsync();
I have tried making everything async (didn't help), making the repository, connection and where we do CreateTableAsync async and still no luck.
",<c#><.net><sqlite><dependency-injection><xamarin.forms>,3234,0,69,8053,2,46,84,75,3628,0,448,3,14,2018-05-24 7:24,2018-05-30 3:56,2018-06-04 7:39,6,11,Advanced,32
54272997,Access Docker postgres container from another container,"I am trying to make a portable solution to having my application container connect to a postgres container.  By 'portable' I mean that I can give the user two docker run commands, one for each container, and they will always work together.
I have a postgres docker container running on my local PC, and I run it like this,
docker run -p 5432:5432 -v $(pwd)/datadir:/var/lib/postgresql/data -e POSTGRES_PASSWORD=qwerty -d postgres:11
and I am able to access it from a python flask app, using the address 127.0.0.1:5432.
I put the python app in a docker container as well, and I am having trouble connecting to the postgres container.
Address 127.0.0.1:5432 does not work.
Address 172.17.0.2:5432 DOES work (172.17.0.2 is the address of the docker container running postgres).  However I consider this not portable because I can't guarantee what the postgres container IP will be.
I am aware of the --add-host flag, but it is also asking for the host-ip, which I want to be the localhost (127.0.0.1).  Despite several hits on --add-host I wasn't able to get that to work so that the final docker run commands can be the same on any computer they are run on.
I also tried this: docker container port accessed from another container
My situation is that the postgres and myApp will be containers running on the same computer.  I would prefer a non-Docker compose solution.
",<postgresql><docker>,1369,1,7,497,0,4,14,55,5683,0,13,1,14,2019-01-20 2:08,2019-01-20 4:46,2019-01-20 4:46,0,0,Intermediate,22
48690890,How does columnar Databases do indexing?,"I understand that the columnar databases put column data together on the disk rather than rows. I also understand that in traditional row-wise RDBMS, leaf index node of B-Tree contains pointer to the actual row. 
But since columnar doesn't store rows together, and they are particularly designed for columnar operations, how do they differ in the indexing techniques?
Do they also use B-tress?
How do they index inside whatever datastructure they use?
Or there is no accepted format, every vendor have their own indexing scheme to cater their needs?
I have been searching, but unable to find any text. Every text I found is for row-wise DBMS.
",<mysql><database>,643,0,0,5451,8,46,75,43,4768,0,760,3,14,2018-02-08 16:59,2018-02-16 21:14,,8,,Intermediate,23
48922384,How to implement created_at and updated_at column using Room Persistence ORM tools in android,"How can I implement created_at and updated_at columns using Room Persistence ORM tools in Android, that can update the timestamp automatically when creating or updating a row in a table?
",<java><android><orm><android-room><sql-timestamp>,187,0,2,423,1,3,12,67,13721,0,16,2,14,2018-02-22 8:08,2018-05-02 19:37,,69,,Basic,9
53755125,Insert the current date time using Laravel,"I want to store the current date time in MySQL using the following Laravel function. Actually, I stored a static date. Instead of this, how can I store the current date time in the created_at and updated_at fields in the database?
function insert(Request $req)
{
    $name = $req-&gt;input('name');
    $address = $req-&gt;input('address');
    $data = array(&quot;name&quot; =&gt; $name, &quot;address&quot; =&gt; $address, &quot;created_at&quot; =&gt; '2017-04-27 10:29:59', &quot;updated_at&quot; =&gt; '2017-04-27 10:29:59');
    DB::table('student')-&gt;insert($data);
    echo &quot;Record inserted successfully.&lt;br/&gt;&quot;;
    return redirect('/');
}
",<php><mysql><laravel><php-carbon>,665,0,11,344,1,3,11,65,65820,0,22,5,14,2018-12-13 4:43,2018-12-13 4:45,2018-12-13 4:45,0,0,Basic,9
53910835,Using Async/Await with node-postgres,"I am using node-postgres to query my database and would like to know how to use async/await and handle errors correctly
An example of my use is here with a very simple query
const { Pool } = require('pg');
let config;
if (process.env.NODE_ENV === 'production' || process.env.NODE_ENV === 'staging') {
  config = { connectionString: process.env.DATABASE_URL, ssl: true };
} else {
  config = {
    host: 'localhost',
    user: 'myuser',
    database: 'mydatabase',
  };
}
const pool = new Pool(config);
async function getAllUsers() {
  let response;
  try {
    response = await pool.query('select * FROM users');
  } catch (error) {
    throw error;
  }
  return response.rows;
}
Then in my routes.js I have
app.get('/all_users', async (req, res) =&gt; {
  const users = await queries.getAllUsers();
  console.log(users); // returns all users fine
});
This is my understanding so far, but i don't think I am approaching this correctly as when it comes to errors my app will freeze and throw UnhandledPromiseRejectionWarning. So if I provide an incorrect table for example
async function getAllUsers() {
  let response;
  try {
    response = await pool.query('select * FROM notable');
  } catch (error) {
    throw error;
  }
  return response.rows;
}
UnhandledPromiseRejectionWarning: error: relation &quot;notable&quot; does not exist
The app will crash after 30 seconds and I have not handled this error gracefully. What am I missing?
",<node.js><postgresql><node-postgres>,1438,0,41,15150,37,125,289,40,22122,0,1415,1,14,2018-12-24 8:13,2018-12-24 8:19,2018-12-24 8:19,0,0,Basic,13
53631015,Why SQL primary key index begin at 1 and not at 0?,"was wondering, why does the SQL primary key index begin at 1, and why not at 0 ? Is there a reason ? 
I don't know if this is the good place to ask a question like this, but found no answer on the web.
",<mysql><sql><database>,202,0,0,268,1,2,14,54,11796,0,23,3,14,2018-12-05 11:14,2018-12-05 11:16,2018-12-05 11:51,0,0,Basic,4
51171289,Unable to acquire JDBC Connection on integration test when using Docker bridge network,"When I run maven test locally is passed. But got this error when I run it on CI server. 
Error Message
Could not open JPA EntityManager for transaction; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection
Stacktrace
org.springframework.transaction.CannotCreateTransactionException: Could not open JPA EntityManager for transaction; nested exception is org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection
Caused by: org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: 
Communications link failure
The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
Caused by: java.net.UnknownHostException: mysql
When running local test, they all passed, maven test default setting provided by IntelliJ IDEA is used.
Since the error complains about database connection, so I checked by Jenkins Audit to Database Plugin. Connection Successful!
The connection parameter in my application.properties also corresponds to this 
spring.datasource.url=jdbc:mysql://mysql:3306/database?useLegacyDatetimeCode=false&amp;serverTimezone=Asia/Shanghai
spring.datasource.username=root
spring.datasource.password=password
spring.datasource.maxActive=5
The MySQL in the URL is the MySQL docker container name. If change it with localhost or private IP in docker container inspect mysql the error message is the same, while the Stacktrace is a little different on last two lines.
for localhost
The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
Caused by: java.net.ConnectException: Connection refused (Connection refused)
for private IP
The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server. 
Caused by: java.net.SocketTimeoutException: connect timed out
The different I think is the host in URL, localhost is used for the local test.
While the Jenkins server used Docker bridge network.
The container status is:  
docker container ls
CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS
                                          NAMES
51ea7c7864a4        mysql:5.7             ""docker-entrypoint.s…""   19 hours ago        Up 19 hours         0.0.0.0:3306-&gt;3306/tcp                             mysql
de364f7b5eaf        maven:3-jdk-8         ""/usr/local/bin/mvn-…""   21 hours ago        Up 21 hours
                                          optimistic_stallman
a6545591e358        jenkinsci/blueocean   ""/sbin/tini -- /usr/…""   43 hours ago        Up 43 hours         0.0.0.0:50000-&gt;50000/tcp, 0.0.0.0:2048-&gt;8080/tcp   frosty_cray
When I run the JUnit test in IntelliJ, it fails sometimes on the local environment. The error log is like:
Caused by: org.h2.jdbc.JdbcSQLException: Schema ""DATABASE"" not found; SQL statement:
TRUNCATE TABLE database.data_log 
I have searched the issue, it's said h2 database use upper case by default.
After run maven test, this issue will go if run JUnit test in IDE again. But this should be not related to the root cause.
Search on the error message, find some similar question but with different nested exception:  
  Could not open JPA EntityManager for transaction; nested exception is javax.persistence.PersistenceException  
  SpingREST: Could not open JPA EntityManager for transaction; nested exception is org.hiberna   
  Could not open JPA EntityManager for transaction; org.hibernate.exception.GenericJDBCException: Could not open connection  
  Could not open JPA EntityManager for transaction in spring  
All of them is about nested exception is javax.persistence.PersistenceException
But nested exception is org.hibernate.exception.JDBCConnectionException: is my situation.
Read Connect Java to a MySQL database
however since that plugin connects OK, means the connection from Jenkins container to MySQL container is fine.
Summarise:
1. local test with maven passed
2. Jenkins plugin connect to MySQL success
3. Integration test fails when run from Jenkins
4. local test environment is WIN10 64bit; Jenkins run in docker container on Ubuntu 16.04 64bit server, with MySQL 5.7 container connects to the same bridge network.
",<java><mysql><spring><docker><jenkins>,4416,6,34,2671,5,37,57,62,9051,0,563,2,14,2018-07-04 10:17,2018-08-10 5:26,,37,,Advanced,32
53607322,Laravel: SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client,"after installing a new laravel app 5.7 and trying to migrate I get this error:
  Illuminate\Database\QueryException : SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from information_schema.tables where table_schema = xxx_db and table_name = migrations)
  at
  C:\xampp\htdocs\xxxxx\vendor\laravel\framework\src\Illuminate\Database\Connection.php:664
  660| // If an exception occurs when attempting to run a query, we'll
  format the error 661| // message to include the bindings with SQL,
  which will make this exception a 662| // lot more helpful to the
  developer instead of just the database's errors. 663| catch (Exception
  $e) {
  664| throw new QueryException( 665| $query,
  $this->prepareBindings($bindings), $e 666| ); 667| } 668|
  Exception trace:
  1 PDOException::(""PDO::__construct(): The server requested
  authentication method unknown to the client [caching_sha2_password]"")
  C:\xampp\htdocs\xxxxx\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  2 PDO::__construct(""mysql:host=127.0.0.1;port=3306;dbname=xxx_db "",
  ""root"", ""**********"", [])
  C:\xampp\htdocs\xxxxx\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  Please use the argument -v to see more details.
",<php><mysql><laravel><laravel-artisan><migrate>,1302,0,0,1860,7,34,57,49,43054,0,64,11,14,2018-12-04 6:59,2018-12-04 7:04,,0,,Intermediate,31
58901356,Docker container can't connect to SQL Server on a remote server,"I have a container hosted ASP.NET Core application but it can't connect to SQL Server on a remote server.
The error is:
  A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections.
This is what I already checked:
Try to disable firewall on host machine and DB server. : still get the error
Edit connection string to use IP and port : still get the error
Ping from application container to DB server: I can ping to DB server normally
Connect to database server via SQL Server Management Studio: I can connect normally
So I think that the container can see the DB server but can't connect. What's the other thing should I check?
Thank you very much for your help.
Update
Dockerfile:
FROM mcr.microsoft.com/dotnet/core/aspnet:3.0 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 1433
#our sql server was use this port for connect
EXPOSE 64608
FROM mcr.microsoft.com/dotnet/core/sdk:3.0 AS build
WORKDIR /src
COPY Web.API.sln ./
COPY MyProject.Core/*.csproj ./MyProject.Core/
COPY MyProject.API/*.csproj ./MyProject.API/
RUN dotnet restore
COPY . .
WORKDIR /src/MyProject.API
RUN dotnet build -c Release -o /app
FROM build AS publish
RUN dotnet publish -c Release -o /app
FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT [""dotnet"", ""MyProject.API.dll""]
Run Docker Command:
docker build -t myproject-api:latest .
docker run -d -p 7991:80 --name myproject-api myproject-api:latest
Connection String:
""data source=172.16.0.88\\SQL_DEV,64608; initial catalog=MyProject; persist security info=True; user id=myuser; password=mypassword;""
",<sql-server><docker><asp.net-core>,1732,0,32,677,3,10,28,49,27215,,56,3,14,2019-11-17 14:17,2019-11-18 3:59,2019-11-18 3:59,1,1,Advanced,39
59036323,Azure SQL stored procedure ridiculously slow called from C#,"Summary:
We have two identical databases, one on a local server, one on Azure.
We have a C# system that accesses these databases, calling stored procedures.
The stored procedures are running very, very slowly when called from the C# system to the Azure database. They're running fine from C# to the local server, and from SSMS to both the Azure and the local databases.
As an example, calling the stored procedure 'usp_DevelopmentSearch_Select'
Local database, SSMS : 1 second
Local database, C# : 1 second
Azure database, SSMS : 1 second
Azure database, C# : 17 minutes
This is happening on multiple stored procedures, I'm just using usp_DevelopmentSearch_Select as an example, to test solutions and to trace the execution plan.
I've ruled out ARITHABORT (the usual suspect), and it seems that running usp_DevelopmentSearch_Select in SSMS and from the C# system generate a functionally identical execution plan.
Details:
We write a very large C# system, which accesses SQL Server databases.
Currently all our clients host their own databases locally on their own servers, however we are looking into the option of hosting the databases on Azure. So I set up some small Azure test databases, ironed out the kinks, and got an Azure-hosted system going.
Then I copied one of our client's databases up, to compare performance hosted locally vs hosted on Azure.
The actual client database is performing unusably badly on Azure!
The first screen calls a stored procedure 'usp_DevelopmentSearch_Select'
Connection to the database on their server:-
In SSMS, calling the stored procedure (below) returns the values in about 1 second
EXEC usp_DevelopmentSearch_Select @MaxRecord = 100, @SearchType = 'CUR'
In our C# program, calling the stored procedure returns the values in about 1 second
Connection to the database on Azure:-
In SSMS, calling the stored procedure returns the values in about 1 second
In our C# program, calling the stored procedure returns the values in about 17 minutes!
Fast in SSMS and slow from C# usually means ARITHABORT, so I turned it on at the start of the stored procedure :
SET ARITHABORT ON; 
That didn't make any difference, so I updated it to convert the passed parameters to local variables.
ALTER PROCEDURE [dbo].[usp_DevelopmentSearch_Select]
     (@MAXRECORD INT,
      @SEARCHTYPE VARCHAR(3))
AS
BEGIN
    SET ARITHABORT ON; 
    DECLARE @MAXRECORD_Var INT = @MAXRECORD
    DECLARE @SEARCHTYPE_Var VARCHAR(3) = @SEARCHTYPE
    ... (Updated all references to @MAXRECORD and @SEARCHTYPE to @MAXRECORD_Var and @SEARCHTYPE_Var)
END
Still no joy, so I got the Execution Plan details for both:-
select o.object_id, s.plan_handle, h.query_plan 
from sys.objects o 
inner join sys.dm_exec_procedure_stats s on o.object_id = s.object_id
cross apply sys.dm_exec_query_plan(s.plan_handle) h
where o.object_id = object_id('usp_DevelopmentSearch_Select')
And just to check, I reloaded the screen in the C# program, and checked the running query:-
SELECT sqltext.TEXT,
req.session_id,
req.status,
req.command,
req.cpu_time,
req.total_elapsed_time,
req.plan_handle
FROM sys.dm_exec_requests req
CROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltext
It is definitely using one of the two execution plans returned above.
So, check the settings for the Execution Plans
SELECT * FROM sys.dm_exec_plan_attributes (0x05002D00D1A1EA5510E66E783602000001);
SELECT * FROM sys.dm_exec_plan_attributes (0x05002D00D1A1EA55E0FC6E783602000001);
Set_Options is 4345 for both, so they're definitely both using ARITHABORT.
The only differences are the localisation bits: Language and Date Format. The Azure database is stuck in American, can't seem to change that, while the C# program forces it to British.
I tried the C# program without forcing it to British, and still got the same issue. It also used exactly the same Execution Plan, so clearly localisation doesn't affect that.
So, I called up the info on the Execution Plans:-
SELECT * FROM sys.dm_exec_query_plan (0x05002D00D1A1EA5510E66E783602000001);
SELECT * FROM sys.dm_exec_query_plan (0x05002D00D1A1EA55E0FC6E783602000001);
Saved them both, and compared the results:-
The two columns far left show the overall comparison: yellow being different, white being the same.
As you can see, the two Execution Plans are almost identical, just with a handful of differences at the top.
The first differences can be seen in the above screenshot: the 'StatementCompId' is one higher in the SSMS (left) pane than the C# (right) pane. Google doesn't want to tell me what StatementCompId is, but given they're in sequence I'm guessing it's the order to do them in, and the SSMS is one higher because the EXEC command that called the SP counts as one.
For ease, I've compiled all the remaining differences into a single screenshot:-
Compile times and CPU usages, free memory, and a couple more 'StatementCompId'
So, the two Execution Plans are functionally identical, with identical settings (except localisation which doesn't seem to have an effect).
So why does it take around 17 minutes calling the Azure SP from C# compared to around 1 second calling the Azure SP from SSMS or the local SP from the locally-hosted database either way?
The Stored Procedure itself is just a SELECT FROM, with a few LEFT JOINs to other tables, nothing fancy and it's never given us any trouble on locally-hosted databases.
SELECT TOP (@MAXRECORD_Var) &lt;FieldList&gt;
FROM (
    SELECT DISTINCT &lt;FieldList&gt;
    FROM &lt;TableName&gt; WITH (NOLOCK)
    LEFT JOIN &lt;TableName&gt; WITH (NOLOCK) ON &lt;Link&gt;
    LEFT JOIN &lt;TableName&gt; WITH (NOLOCK) ON &lt;Link&gt;
    LEFT JOIN &lt;TableName&gt; WITH (NOLOCK) ON &lt;Link&gt;
    LEFT JOIN &lt;TableName&gt; WITH (NOLOCK) ON &lt;Link&gt;
    LEFT JOIN &lt;TableName&gt; WITH (NOLOCK) ON &lt;Link&gt;
    WHERE (
        &lt;Conditions&gt;
    ) AS Base
ORDER BY &lt;FieldName&gt;
Edit: Some Progress
I tried several things that came up from Googling:-
1) WITH RECOMPILE
I tried adding this to the Stored Procedure, didn't make any difference
2) OPTION (OPTIMIZE FOR (@MAXRECORD_Var UNKNOWN, @SEARCHTYPE_Var UNKNOWN))
I tried adding this to the Stored Procedure, didn't make any difference
3) Explicitly setting all options
This one made a noticeable (but still far too small) difference!
I wrote a query to tell me the current options
DECLARE @options INT
SELECT @options = @@OPTIONS
PRINT @options
PRINT 'SET DISABLE_DEF_CNST_CHK ' + CASE WHEN ( (1 &amp; @options) = 1 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET IMPLICIT_TRANSACTIONS ' + CASE WHEN ( (2 &amp; @options) = 2 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET CURSOR_CLOSE_ON_COMMIT ' + CASE WHEN ( (4 &amp; @options) = 4 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_WARNINGS ' + CASE WHEN ( (8 &amp; @options) = 8 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_PADDING ' + CASE WHEN ( (16 &amp; @options) = 16 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULLS ' + CASE WHEN ( (32 &amp; @options) = 32 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ARITHABORT ' + CASE WHEN ( (64 &amp; @options) = 64 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ARITHIGNORE ' + CASE WHEN ( (128 &amp; @options) = 128 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET QUOTED_IDENTIFIER ' + CASE WHEN ( (256 &amp; @options) = 256 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET NOCOUNT ' + CASE WHEN ( (512 &amp; @options) = 512 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULL_DFLT_ON ' + CASE WHEN ( (1024 &amp; @options) = 1024 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULL_DFLT_OFF ' + CASE WHEN ( (2048 &amp; @options) = 2048 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET CONCAT_NULL_YIELDS_NULL ' + CASE WHEN ( (4096 &amp; @options) = 4096 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET NUMERIC_ROUNDABORT ' + CASE WHEN ( (8192 &amp; @options) = 8192 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET XACT_ABORT ' + CASE WHEN ( (16384 &amp; @options) = 16384 ) THEN 'ON' ELSE 'OFF' END + ';'
This produced a set of SET statements, and the current Options value
5496
SET DISABLE_DEF_CNST_CHK OFF;
SET IMPLICIT_TRANSACTIONS OFF;
SET CURSOR_CLOSE_ON_COMMIT OFF;
SET ANSI_WARNINGS ON;
SET ANSI_PADDING ON;
SET ANSI_NULLS ON;
SET ARITHABORT ON;
SET ARITHIGNORE OFF;
SET QUOTED_IDENTIFIER ON;
SET NOCOUNT OFF;
SET ANSI_NULL_DFLT_ON ON;
SET ANSI_NULL_DFLT_OFF OFF;
SET CONCAT_NULL_YIELDS_NULL ON;
SET NUMERIC_ROUNDABORT OFF;
SET XACT_ABORT OFF;
Note: Running SET DISABLE_DEF_CNST_CHK OFF; throws an error, so I commented that one out.
'DISABLE_DEF_CNST_CHK' is not a recognized SET option.
Adding this to the start of the Stored Procedure brought the time down from 17 minutes to 40 seconds.
Still far more than 1 second it takes to run in SSMS, and still not enough to be usable, but progress none the less.
However, I noticed that the Options value it returned (5496) was different to the value I got from the Execution Plan details above (4345), and also some of the settings where different from the settings for that database.
So, I re-ran the query hard-coded to 4345
DECLARE @options INT
SELECT @options = 4345 --@@OPTIONS
PRINT @options
PRINT 'SET DISABLE_DEF_CNST_CHK ' + CASE WHEN ( (1 &amp; @options) = 1 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET IMPLICIT_TRANSACTIONS ' + CASE WHEN ( (2 &amp; @options) = 2 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET CURSOR_CLOSE_ON_COMMIT ' + CASE WHEN ( (4 &amp; @options) = 4 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_WARNINGS ' + CASE WHEN ( (8 &amp; @options) = 8 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_PADDING ' + CASE WHEN ( (16 &amp; @options) = 16 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULLS ' + CASE WHEN ( (32 &amp; @options) = 32 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ARITHABORT ' + CASE WHEN ( (64 &amp; @options) = 64 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ARITHIGNORE ' + CASE WHEN ( (128 &amp; @options) = 128 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET QUOTED_IDENTIFIER ' + CASE WHEN ( (256 &amp; @options) = 256 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET NOCOUNT ' + CASE WHEN ( (512 &amp; @options) = 512 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULL_DFLT_ON ' + CASE WHEN ( (1024 &amp; @options) = 1024 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET ANSI_NULL_DFLT_OFF ' + CASE WHEN ( (2048 &amp; @options) = 2048 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET CONCAT_NULL_YIELDS_NULL ' + CASE WHEN ( (4096 &amp; @options) = 4096 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET NUMERIC_ROUNDABORT ' + CASE WHEN ( (8192 &amp; @options) = 8192 ) THEN 'ON' ELSE 'OFF' END + ';'
PRINT 'SET XACT_ABORT ' + CASE WHEN ( (16384 &amp; @options) = 16384 ) THEN 'ON' ELSE 'OFF' END + ';'
This returned
4345
SET DISABLE_DEF_CNST_CHK ON;
SET IMPLICIT_TRANSACTIONS OFF;
SET CURSOR_CLOSE_ON_COMMIT OFF;
SET ANSI_WARNINGS ON;
SET ANSI_PADDING ON;
SET ANSI_NULLS ON;
SET ARITHABORT ON;
SET ARITHIGNORE ON;
SET QUOTED_IDENTIFIER OFF;
SET NOCOUNT OFF;
SET ANSI_NULL_DFLT_ON OFF;
SET ANSI_NULL_DFLT_OFF OFF;
SET CONCAT_NULL_YIELDS_NULL ON;
SET NUMERIC_ROUNDABORT OFF;
SET XACT_ABORT OFF;
Again, the line SET DISABLE_DEF_CNST_CHK ON; says it's not an option you can set, so I commented it out.
Updated the Stored Procedure with those SET values, and tried again.
It still takes 40 seconds, so no further progress.
Running it in SSMS still takes 1 second, so at least it didn't break that, not that it's any help but nice to know!
Edit #2: Or not...
Seems yesterday's apparent progress was a blip: it's back to taking 17 minutes again! (With nothing changed)
Tried combining all three options: WITH RECOMPILE, OPTION OPTIMIZE and explicitly setting the SET OPTIONS. Still takes 17 minutes.
Edit 3: Parameter Sniffing Setting
In SQL Azure, you can turn off Parameter Sniffing from the database options screen.
And check them using
SELECT * FROM sys.database_scoped_configurations
Tried SSMS and C# twice each after setting this to OFF.
As before, SSMS takes 1 second, C# still takes 15+ minutes.
Of course, given C# forces a load of parameters to a specific state when it connects, it's entirely possible that it's overriding it.
So, just to say I tried it, I added turning it off to the Stored Procedure
ALTER DATABASE SCOPED CONFIGURATION SET PARAMETER_SNIFFING = OFF;
Still 15+ minutes.
Ah well, was worth a try!
Plus, lots of new parameters to look up and test.
Edit #4 : Azure Staging Pool Configurations and Automatic Tuning
I tried out several different configurations on the Staging Pool, to see if that made a difference. I didn't try the worst query, as it was costing us money to up the eDTUs, but I tried several others, twice each (going down the list each time, so not the same one twice straight away).
Going from 50 eDTUs to 100 eDTUs made a bit of a difference, so I guess on our Test Elastic Pool we use all of the 50, but after that it didn't make any difference. Oddly, the Premium gave worse performance than Standard in places.
I then posted this on the Azure MSDN site (when they finally got round to Verifying my account), and they suggested going through all the Performance options on the Azure Portal and see if that recommends anything.
It suggested a couple of indexes, which I enabled, but that was all.
Then I flipped the Automatic Tuning over from 'Server' to 'Azure Defaults'
I re-ran most of the same timing tests, just to see what difference it had made.
The query that had been taking 17 minutes now generally took 13 seconds, a massive improvement! Yay!
The rest were a mixed bag. C was generally quicker, most still took around the same time, and E now takes nearly twice as long (26s up from 14s).
There results also seemed to have a lot more variance than they did before, although it's possible that changing the eDTU size resets the tunings. The second run was usually better than the first, often noticeably so.
Still all a lot slower than running the same system against a database on a local server, but a huge improvement for the slowest Stored Procedure at least.
",<c#><sql><performance><azure><stored-procedures>,13931,9,117,662,0,6,22,74,2298,0,5,3,14,2019-11-25 16:35,2020-09-08 11:46,,288,,Intermediate,23
49051959,postgresql crosstab simple example,"I got a key-value based table where each key-value pair is assigned to an entity which is identified by an id:
| id  | key       | value |
|-----|-----------|-------|
| 123 | FIRSTNAME | John  |
| 123 | LASTNAME  | Doe   |
And I want to transform it a structure like this:
| id  | firstName | lastName |
|-----|-----------|----------|
| 123 | John      | Doe      |
I suppose one can use postgres built-in crosstab function to do it.
Can you show me how to do it and explain why it works?
",<postgresql><pivot-table>,489,0,8,16272,13,59,99,79,23582,0,895,1,14,2018-03-01 14:34,2018-03-01 14:34,2018-03-01 14:34,0,0,Intermediate,23
49142197,Google Cloud SQL Postgres - randomly slow queries from Google Compute / Kubernetes,"I've been testing Google Cloud SQL with Postgresql, but I have random queries taking ~3s instead of a few ms. 
Troubleshooting I did:
The queries themselves aren't problems, rerunning the same query will work. 
Indexes are properly set. The database is also very very small, it shouldn't do this, even if there weren't any index.
The Kubernetes container is connecting to the database through SQL Proxy (I followed this https://cloud.google.com/sql/docs/postgres/connect-kubernetes-engine). It is not the problem though as I tried to connect directly to the database, with the same issue.
I configured net.ipv4.tcp_keepalive_time to 60 to make sure the connection weren't dropping.
I also have a pool of connection that are never disconnected to make sure it wasn't from that.
When I run queries directly through my local Postgresql client, I never have the problem.
I don't have this issue when developing locally either and connecting to my local database.
What I'm getting at is: I feel there's some weird connection/link issue between my Google Compute instances and my Google SQL instance that I can't seem to figure out. 
Any idea?
Edit: 
I also noticed these logs in my SQL Cloud instance every 30s:
ERROR:  recovery is not in progress 
HINT:  Recovery control functions can only be executed during recovery. 
STATEMENT:  SELECT pg_is_xlog_replay_paused(), current_timestamp
",<postgresql><google-cloud-sql>,1382,2,4,289,0,1,6,60,1436,0,3,1,14,2018-03-07 0:25,2018-04-03 8:20,,27,,Intermediate,23
62472371,psycopg2.errors.InsufficientPrivilege: permission denied for relation django_migrations,"What my settings.py for DB looks like:
ALLOWED_HOSTS = ['*']
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': 'fishercoder',
        'USER': 'fishercoderuser',
        'PASSWORD': 'password',
        'HOST': 'localhost',
        'PORT': '5432',
    }
}
I have created a new and empty db named ""fishercoder"" this way:
psql -U postgres
create database fishercoder; 
ALTER USER postgres with password 'badpassword!'; 
CREATE USER fishercoderuser WITH PASSWORD 'password';
ALTER ROLE fishercoderuser SET client_encoding TO 'utf8';
ALTER ROLE fishercoderuser SET default_transaction_isolation TO 'read committed';
ALTER ROLE fishercoderuser SET timezone TO 'PST8PDT';
GRANT ALL PRIVILEGES ON DATABASE fishercoder TO fishercoderuser;
Then I've imported my other SQL dump into this new DB successfully by running:psql -U postgres fishercoder &lt; fishercoder_dump.sql
Then I tried to run ./manage.py makemigrations on my Django project on this EC2 instance, but got this error:
Traceback (most recent call last):
  File ""/home/ubuntu/myprojectdir/myprojectenv/lib/python3.6/site-packages/django/db/backends/utils.py"", line 86, in _execute
    return self.cursor.execute(sql, params)
psycopg2.errors.InsufficientPrivilege: permission denied for relation django_migrations
I found these three related posts on SO: 
One, two and three
I tried the commands they suggested:
postgres=# GRANT ALL ON ALL TABLES IN SCHEMA public to fishercoderuser;
GRANT
postgres=# GRANT ALL ON ALL SEQUENCES IN SCHEMA public to fishercoderuser;
GRANT
postgres=# GRANT ALL ON ALL FUNCTIONS IN SCHEMA public to fishercoderuser;
GRANT
no luck, I then restarted my postgresql db: sudo service postgresql restart
when I tried to run migrations again, still faced w/ the same error.
More debug info below:
ubuntu@ip-xxx-xxx-xx-xx:~$ psql -U postgres
Password for user postgres:
psql (10.12 (Ubuntu 10.12-0ubuntu0.18.04.1))
Type ""help"" for help.
postgres=# \dt django_migrations
Did not find any relation named ""django_migrations"".
postgres=# \d django_migrations
Did not find any relation named ""django_migrations"".
postgres=#  \dp django_migrations
                            Access privileges
 Schema | Name | Type | Access privileges | Column privileges | Policies
--------+------+------+-------------------+-------------------+----------
(0 rows)
postgres=# SHOW search_path; \dt *.django_migrations
   search_path
-----------------
 ""$user"", public
(1 row)
Did not find any relation named ""*.django_migrations"".
postgres=# \dn+ public.
                                     List of schemas
        Name        |  Owner   |  Access privileges   |           Description
--------------------+----------+----------------------+----------------------------------
 information_schema | postgres | postgres=UC/postgres+|
                    |          | =U/postgres          |
 pg_catalog         | postgres | postgres=UC/postgres+| system catalog schema
                    |          | =U/postgres          |
 pg_temp_1          | postgres |                      |
 pg_toast           | postgres |                      | reserved schema for TOAST tables
 pg_toast_temp_1    | postgres |                      |
 public             | postgres | postgres=UC/postgres+| standard public schema
                    |          | =UC/postgres         |
(6 rows)
Any ideas how to fix this?
",<python><django><database><postgresql><amazon-ec2>,3403,3,70,3288,12,51,86,58,14759,0,3653,3,14,2020-06-19 14:25,2021-06-03 7:39,,349,,Basic,10
53287215,Retry failed sqlalchemy queries,"Every time I'm restarting mysql service, my app is receiving the following error on any query:
result = self._query(query)
  File ""/usr/local/lib/python3.6/site-packages/pymysql/cursors.py"", line 328, in _query
    conn.query(q)
  File ""/usr/local/lib/python3.6/site-packages/pymysql/connections.py"", line 516, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File ""/usr/local/lib/python3.6/site-packages/pymysql/connections.py"", line 727, in _read_query_result
    result.read()
  File ""/usr/local/lib/python3.6/site-packages/pymysql/connections.py"", line 1066, in read
    first_packet = self.connection._read_packet()
  File ""/usr/local/lib/python3.6/site-packages/pymysql/connections.py"", line 656, in _read_packet
    packet_header = self._read_bytes(4)
  File ""/usr/local/lib/python3.6/site-packages/pymysql/connections.py"", line 702, in _read_bytes
    CR.CR_SERVER_LOST, ""Lost connection to MySQL server during query"")
sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2013, 'Lost connection to MySQL server during query') [SQL: ...] [parameters: {...}] (Background on this error at: http://sqlalche.me/e/e3q8)
Any query after that will succeed as usual.
This is just a common use case for example, in general I might want to retry any query depending on the error.
Is there any way to catch and retry the query in some low level sqlalchemy api? Doing try-except or a custom query method in my code is not reasonable as I use it too many times and its not maintainable.
",<python><mysql><sqlalchemy><flask-sqlalchemy><retry-logic>,1528,1,17,8501,10,63,144,80,14014,0,8263,4,14,2018-11-13 18:16,2018-11-14 12:16,2018-11-14 12:16,1,1,Basic,10
62656356,Is it possible to connect an existing database with Strapi CMS?,"I am trying to create an API using Strapi CMS. I have an existing PostgreSQL + postgis database and I would like to connect-use this database in a Strapi project.
Do you know if it is possible to do something like this?
",<postgresql><postgis><strapi>,220,0,0,303,2,3,11,50,12009,0,6,4,14,2020-06-30 11:24,2020-08-02 20:32,,33,,Basic,3
49231031,Stored procedure is not returning data,"I am in the progress of transferring a script from a (discontinued) windows server to our Linux one. One of the scripts I need to transfer is a connection with a MSSQL-server. 
The connection with the server is established and I am able to fetch ""regular"" data from any of the tables, but when I execute a stored procedure, I don't receive any of the desired data. The procedure just returns false when executed. 
Testing the prepared statement for errors with $stmt-&gt;errorInfo() does not show me any relevant information, it just returns error code 00000, which should indicate everything (should) work fine.
Array
(
    [0] =&gt; 00000
    [1] =&gt; 0
    [2] =&gt; (null) [0] (severity 0) [(null)]
    [3] =&gt; 0
    [4] =&gt; 0
)
php
$con = new \PDO('dblib:charset=UTF-8;host=freedts;dbname=database', 'user', 'password');
/** ------------------------------------------------------**/
$sql = 'SELECT * FROM prgroepen';
$stmt = $con-&gt;prepare($sql);
if ($stmt) {
    try {
        $stmt-&gt;execute();
        $data = $stmt-&gt;fetch(\PDO::FETCH_ASSOC);
        if ($data) echo '&lt;pre&gt;'.print_r($data, true).'&lt;/pre&gt;';
        else var_dump($data);
    }catch(\Exception $e) {
        echo $e-&gt;getMessage();
    }
}
/** ------------------------------------------------------**/
$SP = &lt;&lt;&lt;SQL
    DECLARE @return_value int,
            @soort nvarchar(1),
            @dagen money
    EXEC    @return_value = [dbo].[web_voorraadstatus] @produkt = N'ABEC24_9002', @aantal = 1, @soort = @soort OUTPUT, @dagen = @dagen OUTPUT
    SELECT  @soort as N'@soort', @dagen as N'@dagen'
SQL;
$stmt = $con-&gt;prepare($SP);
if ($stmt) {
    try {
        $stmt-&gt;execute();
        $data = $stmt-&gt;fetch(\PDO::FETCH_ASSOC);
        if ($data) echo '&lt;pre&gt;'.print_r($data, true).'&lt;/pre&gt;';
        else var_dump($data);
    }catch(\Exception $e) {
        echo $e-&gt;getMessage();
    }
}
output
Array
(
    [kode] =&gt; A
    [omschrijving] =&gt; ACCESSOIRE DISPLAYS
    [aeenheid] =&gt; ST
    [agb] =&gt; 604006
    [veenheid] =&gt; ST
    [vgb] =&gt; 700011
    [coefaank] =&gt; 
    [coefverk] =&gt; 
    [internet] =&gt; 1
    [foto] =&gt; #\\serverpc\fws$\GROEPEN\A.jpg#
    [vader] =&gt; 
    [produkt_niveau] =&gt; 0
    [bs_kode] =&gt; 
    [bs_vader] =&gt; 
    [web_volgorde] =&gt; 6
    [pdfcataloog] =&gt; 
)
bool(false) 
I also tried to call the SP in different ways, but with no avail as well.
The exact same code runs perfectly on the windows server, with the only difference is that the windows server uses the sqlsrv-driver
/** ============================== **/
/*  @produkt as nvarchar(15),
/*  @aantal as money,
/*  @soort as nvarchar(1) output,
/*  @dagen as money output
/** ============================== **/
$stmt = $con-&gt;prepare('execute web_voorraadstatus ?, ?, ?, ?');
$stmt-&gt;bindParam(1, $produkt, PDO::PARAM_STR);
$stmt-&gt;bindParam(2, $aantal, PDO::PARAM_STR);
$stmt-&gt;bindParam(3, $soort, PDO::PARAM_STR, 1);
$stmt-&gt;bindParam(4, $dagen, PDO::PARAM_STR, 10);
var_dump($stmt-&gt;execute()); # true
var_dump($soort, $dagen);   # NULL, NULL
So is dblib actually able to execute stored procedures and retrieving the data returned by it?
note: the client charset is already set to UTF-8 in the FreeDTS config file
Here is a partial from the freeDTS log, it's seems I'm receiving data from the MSSQL-server just fine?
dblib.c:4639:dbsqlok(0x7fcfd8acc530)
dblib.c:4669:dbsqlok() not done, calling tds_process_tokens()
token.c:540:tds_process_tokens(0x7fcfd78d7bd0, 0x7ffe281bec38, 0x7ffe281bec3c, 0x6914)
util.c:156:Changed query state from PENDING to READING
net.c:555:Received header
0000 04 01 00 5c 00 37 01 00-                        |...\.7..|
net.c:609:Received packet
0000 04 01 00 5c 00 37 01 00-79 00 00 00 00 fe 01 00 |...\.7.. y.......|
0010 e0 00 00 00 00 00 81 02-00 00 00 21 00 e7 02 00 |........ ...!....|
0020 09 04 d0 00 34 06 40 00-73 00 6f 00 6f 00 72 00 |....4.@. s.o.o.r.|
0030 74 00 00 00 21 00 6e 08-06 40 00 64 00 61 00 67 |t...!.n. .@.d.a.g|
0040 00 65 00 6e 00 d1 02 00-56 00 08 00 00 00 00 90 |.e.n.... V.......|
0050 d0 03 00 fd 10 00 c1 00-01 00 00 00             |........ ....|
",<php><sql-server><linux>,4178,0,106,16480,6,47,60,37,2712,,1605,3,14,2018-03-12 8:38,2018-03-20 14:49,,8,,Intermediate,17
60727318,Add generated column to an existing table Postgres,"I am trying to add a generated column to an existing table with this script.   
alter table Asset_Store add column
md5_hash VARCHAR(100) GENERATED ALWAYS AS 
(CAST(UPPER(    
        case
             when OR_ID is not null then MD5(cast(OR_ID as varchar(100)))
             when Asset_ID is not null then MD5(Asset_ID)
             else null
        end 
) as VARCHAR(100)))
STORED
;
but I am getting an error: 
SQL Error [42601]: ERROR: syntax error at or near ""(""
 Position: 88
 ERROR: syntax error at or near ""(""
 Position: 88
 ERROR: syntax error at or near ""(""
 Position: 88
What is the issue? I don't get it. 
In the schema of my Asset_Store table the column
OR_ID is int and Asset_ID is varchar(100).     
I guess it expects a slightly different syntax... but what is the right syntax?   
",<sql><postgresql>,797,0,23,38523,19,99,168,45,11960,0,1306,2,14,2020-03-17 17:26,2020-03-17 19:37,2020-03-17 19:37,0,0,Basic,2
51517262,Laravel Bulk Update for multiple record ids,"I want to mass update my records in Laravel but the records not getting updated. I have a different record for each Id. Below is what I am trying.
$ids = [5,6,8,9],
$updated_array = [
  ['name' =&gt; 'tarun'],
  ['name' =&gt; 'Akash'],
  ['name' =&gt; 'Soniya'],
  ['name' =&gt; 'Shalu'],
];
Model::whereIn('id', $ids)-&gt;update($updated_array);
",<php><mysql><laravel><eloquent>,347,0,9,141,1,1,4,55,27199,0,0,8,14,2018-07-25 10:59,2018-07-25 11:26,,0,,Basic,10
61197228,"flask, gunicorn (gevent), sqlalchemy (postgresql): too many connections","I created Flask WSGI-application which uses gunicorn as WSGI-server, for DB it uses PostgreSQL through Flask SQLAlchemy extension. That's all hosted on Heroku.
gunicorn configuration
number of workers: 2;
number of workers connections: 1024;
number of threads: 1;
worker class: gevent.
Heroku PostgreSQL configuration
maximum number of connections: 20.
For everything else default configuration is used.
I'm getting this error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) FATAL: too many connections for role &lt;id&gt;. Obviously, i'm exceeded allowed number of DB connections.
I tried these things to fix it:
for SQLAlchemy set poolclass to NullPool;
for SQLAlchmey set pool_recycle to 2. Number of connections still the same even after more than 2 seconds;
session.close() with engine.dispose();
number of workers - 2, number of worker connections - 9;
number of workers - 1, number of worker connections - 18;
number of workers - 1, number of worker connections - 10, SQLAlchemy max_overflow = 0, SQLALchmey pool_size = 10 (i'm getting this error: sqlalchemy.exc.TimeoutError: QueuePool limit of size 10 overflow 0 reached, connection timed out, timeout 30).
Nothing of this works. I'm still getting this error even with minimum gunicorn configuration (1 worker with 18 connections). I'm really started not to understand what is really going on. 
I thought it worked like this: each worker have it's own instance of engine, and each engine have it's own pool size. So, if there is 2 workers with engine default config (pool size is 5), then we have 2 * 5 = 10 maximum connections to DB. But it looks like this is really not like this.
Questions
how to fix this error?
how SQLAlchemy pooling works with gevent workers? i.e., how can i count maximum number of DB connections?
how should I configure it correctly so that it works as expected?
Sorry for too much questions, but it is really frustrating me.
",<python><postgresql><flask><sqlalchemy><gunicorn>,1924,0,16,817,0,16,29,72,2727,0,802,2,14,2020-04-13 21:37,2022-08-24 13:27,,863,,Intermediate,23
52715797,What's the driver class name for SQL Server JDBC,"I want to connect my Java SpringBoot app to SQL Server and I get the information that spring cannot load driver class. I tried:
spring.datasource.driver-class-name=com.microsoft.sqlserver.jdbc.SQLServerDriver
and 
spring.datasource.driver-class-name=com.microsoft.jdbc.sqlserver.SQLServerDriver
But both did not work, here is my maven dependency 
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.sqlserver&lt;/groupId&gt;
    &lt;artifactId&gt;mssql-jdbc&lt;/artifactId&gt;
    &lt;version&gt;7.0.0.jre8&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
",<java><sql-server><spring>,579,0,8,1974,9,30,62,63,36484,0,40,2,14,2018-10-09 7:46,2018-10-09 7:51,2018-10-09 7:51,0,0,Basic,3
64230223,SQL: What is the equivalent of json_pretty() in postgreSQL,"What is the equivalent of this MySQL function
SELECT JSON_PRETTY('{&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3}') AS Result
  FROM table;
Formatted JSON:
+------------------------------+
| Result                       |
+------------------------------+
| {                            |
|   &quot;a&quot;: 1,                    |
|   &quot;b&quot;: 2,                    |
|   &quot;c&quot;: 3                     |
| }                            |
+------------------------------+
I've tried jsonb_pretty() as mentioned in the document but nothing is available
",<sql><json><postgresql><jsonb><dataformat>,570,0,12,149,0,1,4,52,7245,,0,1,14,2020-10-06 16:40,2020-10-06 17:26,,0,,Basic,3
56254895,Hive partitioned table reads all the partitions despite having a Spark filter,"I'm using spark with scala to read a specific Hive partition. The partition is year, month, day, a and b
scala&gt; spark.sql(""select * from db.table where year=2019 and month=2 and day=28 and a='y' and b='z'"").show
But I get this error:
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 236 in stage 0.0 failed 4 times, most recent failure: Lost task 236.3 in stage 0.0 (TID 287, server, executor 17): org.apache.hadoop.security.AccessControlException: Permission denied: user=user, access=READ, inode=""/path-to-table/table/year=2019/month=2/day=27/a=w/b=x/part-00002"":user:group:-rw-rw----
As you can see, spark is trying to read a different partition and I don't have permisions there.
It shouldn't be, because I created a filter and this filter is my partition.
I tried the same query with Hive and it's works perfectly (No access problems)
Hive&gt; select * from db.table where year=2019 and month=2 and day=28 and a='y' and b='z';
Why is spark trying to read this partition and Hive doesn't?
There is a Spark configuration that am I missing?
Edit: More information
Some files were created with Hive, others were copied from one server and pasted to our server with different permissions (we can not change the permissions), then they should have refreshed the data. 
We are using:
cloudera 5.13.2.1
hive 1.1.0
spark 2.3.0
hadoop 2.6.0
scala 2.11.8
java 1.8.0_144
Show create table
|CREATE EXTERNAL TABLE Columns and type
PARTITIONED BY (`year` int COMMENT '*', `month` int COMMENT '*', `day` int COMMENT '*', `a` string COMMENT '*', `b` string COMMENT '*')
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
WITH SERDEPROPERTIES (
 'serialization.format' = '1'
)
STORED AS
 INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
 OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION 'hdfs://path'
TBLPROPERTIES (
 'transient_lastDdlTime' = '1559029332'
)
|
",<scala><apache-spark><hive><apache-spark-sql>,1969,0,27,700,0,4,16,80,7031,0,49,3,14,2019-05-22 10:36,2019-05-28 1:58,2019-05-28 11:11,6,6,Intermediate,23
51324788,result of prepared select statement as array,"I would like to get the complete result of a prepared statement as an array (key/value pairs) in order to later use it in a str_replace() function.
My table has three columns, an index and the fields ""x1"" and ""x2"". I used the following successfully:
$db = new mysqli(""servername"", ""username"", ""pw"", ""dbname"");
if($ps1 = $db-&gt;prepare(""SELECT x1, x2 FROM my_table"")) {
  $ps1-&gt;execute();
  $ps1-&gt;bind_result($search, $replace);
    $result = array();
    while ($ps1-&gt;fetch()) {
      $result[$search] = $replace;
    }
    $ps1-&gt;close();
}
However, I am thinking that there must be a simpler way, without a while loop, getting the complete result, not added up from single rows one by one. 
I looked at other questions, and I came up with the following, but it doesn't work (""Warning: mysqli_fetch_assoc() expects parameter 1 to be mysqli_result""):
if($ps1 = $db-&gt;prepare(""SELECT x1, x2 FROM my_table"")) {
  $ps1-&gt;execute();
  $result = mysqli_fetch_assoc($ps1);
  return $result;
  $ps1-&gt;close();
}
I also tried $result = mysqli_fetch_all($ps1); with no success ( getting ""Call to undefined function mysqli_fetch_all()""). 
BTW, I am using PHP 5.6.
ADDITION after some answers and discussion in comments concerning MYSQLND:
phpinfo() displays the following information in its mysqlnd section:
  Loaded plugins:
  mysqlnd,debug_trace,auth_plugin_mysql_native_password,auth_plugin_mysql_clear_password,auth_plugin_sha256_password
",<php><sql><arrays><mysqli><prepared-statement>,1451,0,22,65283,19,73,131,65,6117,0,3129,7,14,2018-07-13 12:09,2018-07-17 15:03,2018-07-17 15:03,4,4,Basic,1
52457637,Postgres cannot cast type jsonb to integer,"Threre are two tables.
Table1                           
 id integer
 color_name character(64) 
Table2
 id integer
 jdata jsonb
Json data looks like:
{""price"": 4500, ""colorId"": 5}
I need output colors and count of items grouped by colors, so i tried to use this query:
SELECT Table1.color_name, Table2.jdata -&gt;&gt; 'colorId', count(*)
FROM Table1
INNER JOIN Table2
ON Table1.id = Table2.jdata -&gt;&gt; 'colorId'
group by Table2.jdata -&gt;&gt; 'colorId';
I get an error:
  error: operator does not exist: integer = jsonb
Also i tried exec this:
select Table1.color_name, count(*) 
from Table1
join Table2
on (Table2.jdata-&gt;&gt;'colorId')::int = Table1.id
group by Table1.color_name
What i get:
  error: cannot cast type jsonb to integer
",<json><postgresql><join>,744,0,15,141,1,1,5,79,27511,,0,3,14,2018-09-22 14:32,2018-09-22 14:48,,0,,Basic,10
48612353,How to change the attributes order in Apache SparkSQL `Project` operator?,"This is a Catalyst specific problem
See below my queryExecution.optimizedPlan before apply my Rule.
01 Project [x#9, p#10, q#11, if (isnull(q#11)) null else UDF(q#11) AS udfB_10#28, if (isnull(p#10)) null else UDF(p#10) AS udfA_99#93]
02 +- InMemoryRelation [x#9, p#10, q#11], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
03    :  +- *SerializeFromObject [assertnotnull(input[0, eic.R0, true], top level non-flat input object).x AS x#9, unwrapoption(IntegerType, assertnotnull(input[0, eic.R0, true], top level non-flat input object).p) AS p#10, unwrapoption(IntegerType, assertnotnull(input[0, eic.R0, true], top level non-flat input object).q) AS q#11]
04    :     +- *MapElements &lt;function1&gt;, obj#8: eic.R0
05    :        +- *DeserializeToObject newInstance(class java.lang.Long), obj#7: java.lang.Long
05    :           +- *Range (0, 3, step=1, splits=Some(2))
In line 01 I need swap the position of udfA and udfB this way:
01 Project [x#9, p#10, q#11, if (isnull(p#10)) null else UDF(p#10) AS udfA_99#93, if (isnull(q#11)) null else UDF(q#11) AS udfB_10#28]
when I try to change the order of the attributes in a Projection operation in SparkSQL via Catalyst optimization the result of the query is modified to an invalid value. Maybe I'm not doing everything is needed. I'm just changing the order of NamedExpression objects in fields parameter:
object ReorderColumnsOnProjectOptimizationRule extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {
    case Project(fields: Seq[NamedExpression], child) =&gt; 
      if (checkCondition(fields)) Project(newFieldsObject(fields), child) else Project(fields, child)
    case _ =&gt; plan
  }
  private def newFieldsObject(fields: Seq[NamedExpression]): Seq[NamedExpression] = {
    // compare UDFs computation cost and return the new NamedExpression list
    . . .
  }
  private def checkCondition(fields: Seq[NamedExpression]): Boolean = {
    // compare UDFs computation cost and return Boolean for decision off change order on field list.
    . . . 
  }
  . . .
}
Note: I'm adding my Rule on extraOptimizations SparkSQL object:
spark.experimental.extraOptimizations = Seq(ReorderColumnsOnProjectOptimizationRule)
Any suggestions will be of great help.
EDIT 1
By the way, I created a notebook on Databricks for testing purposes.  See this link for more detail
Commenting on line 60 the optimization is invoked and an error occurs.
. . .
58     // Do UDF with less cost before, so I need change the fields order
59     myPriorityList.size == 2 &amp;&amp; myPriorityList(0) &gt; myPriorityList(1)
60     false
61   }
What did I miss ? 
EDIT 2
Consider the following piece of code from compiler optimisation, which is almost analogous :
if ( really_slow_test(with,plenty,of,parameters)
     &amp;&amp; slower_test(with,some,parameters)
     &amp;&amp; fast_test // with no parameters
   )
 {
  ...then code...
 }
This code first evaluates an expensive function then, on success, proceeds to evaluate the remainder of the expression. But even if the first test fails and the evaluation is short-cut, there’s a significant performance penalty because the fat really_slow_test(...) is always evaluated. While retaining program correctness, one can rearrange the expression as follows:
if ( fast_test
     &amp;&amp; slower_test(with,some,parameters)
     &amp;&amp; (really_slow_test(with,plenty,of,parameters))
 {
  ...then code...
 }
My goal is to run the fastest UDFs first
",<scala><apache-spark><apache-spark-sql>,3493,1,49,1029,1,9,18,80,768,0,269,2,14,2018-02-04 19:52,2018-02-08 9:28,2018-02-13 17:08,4,9,Intermediate,23
52387021,Spark read parquet with custom schema,"I'm trying to import data with parquet format with custom schema but it returns :
TypeError: option() missing 1 required positional argument: 'value'
   ProductCustomSchema = StructType([
        StructField(""id_sku"", IntegerType(), True),
        StructField(""flag_piece"", StringType(), True),
        StructField(""flag_weight"", StringType(), True),
        StructField(""ds_sku"", StringType(), True),
        StructField(""qty_pack"", FloatType(), True)])
def read_parquet_(path, schema) : 
    return spark.read.format(""parquet"")\
                             .option(schema)\
                             .option(""timestampFormat"", ""yyyy/MM/dd HH:mm:ss"")\
                             .load(path)
product_nomenclature = 'C:/Users/alexa/Downloads/product_nomenc'
product_nom = read_parquet_(product_nomenclature, ProductCustomSchema)
",<apache-spark><pyspark><apache-spark-sql>,834,0,15,441,1,4,15,57,46143,0,13,2,14,2018-09-18 12:51,2018-09-18 13:25,,0,,Basic,2
54428037,Installing MySQL and checking root password fails?,"I recently uninstalled and reinstalled MySQL (on Windows 7) using the installer. When I try to set up the MySQL Server, it won't let me continue unless I enter the ""current"" root password??? I don't understand how there can be a current root password if I completely uninstalled and reinstalled the program.
I've tried with a blank password as well as every password I can possibly think of that I would have used and nothing works. Google is completely unhelpful as every result I've found either refers to a ""homebrew"" installation, whatever that is, or refers to installations on Linux. Is there some folder of config files that the uninstallation refuses to delete that I need to remove manually? Or am I missing something else?
",<mysql><mysql-workbench>,733,1,0,307,1,2,11,38,41594,0,10,5,14,2019-01-29 19:05,2019-01-29 19:33,2019-01-29 19:33,0,0,Basic,14
49570306,Proper way to deal with database connectivity issue,"I getting below error on trying to connect with the database :
  A network-related or instance-specific error occurred while
  establishing a connection to SQL Server. The server was not found or
  was not accessible. Verify that the instance name is correct and that
  SQL Server is configured to allow remote connections. (provider: Named
  Pipes Provider, error: 40 - Could not open a connection to SQL Server)
Now sometimes i get this error and sometimes i dont so for eg:When i run my program for the first time,it open connection successfully and when i run for the second time i get this error and the next moment when i run my program again then i dont get error.
When i try to connect to same database server through SSMS then i am able to connect successfully but i am getting this network issue in my program only.
Database is not in my LOCAL.Its on AZURE.
I dont get this error with my local database.
Code :
public class AddOperation
{
    public void Start()
    {
          using (var processor = new MyProcessor())
          {
              for (int i = 0; i &lt; 2; i++)
              {
                  if(i==0)
                  {
                     var connection = new SqlConnection(""Connection string 1"");
                     processor.Process(connection);
                  }
                  else
                  {
                      var connection = new SqlConnection(""Connection string 2"");
                      processor.Process(connection);
                  }   
              }
          }
    }       
}
public class MyProcessor : IDisposable
{
    public void Process(DbConnection cn)
        {
            using (var cmd = cn.CreateCommand())
            {
                cmd.CommandText = ""query"";
                cmd.CommandTimeout = 1800;
                cn.Open();//Sometimes work sometimes dont
                using (var reader = cmd.ExecuteReader(CommandBehavior.CloseConnection))
                { 
                   //code
                }
            }
        }
}
So i am confused with 2 things :
1) ConnectionTimeout : Whether i should increase connectiontimeout and will this solve my unusual connection problem ?
2) Retry Attempt Policy : Should i implement retry connection mechanism like below :
public static void OpenConnection(DbConnection cn, int maxAttempts = 1)
        {
            int attempts = 0;
            while (true)
            {
                try
                {
                    cn.Open();
                    return;
                }
                catch
                {
                    attempts++;
                    if (attempts &gt;= maxAttempts) throw;
                }
            }
        }
I am confused with this 2 above options.
Can anybody please suggest me what would be the better way to deal with this problem?
",<c#><ado.net><azure-sql-database><sqlconnection>,2825,0,56,6736,21,98,218,58,7262,,2232,7,14,2018-03-30 7:32,2018-03-30 15:32,,0,,Basic,13
53829952,Merge Schema with int and double cannot be resolved when reading parquet file,"I've got two parquet files, one contains an integer field myField and another contains a double field myField. When attempting to read both the files at once
val basePath = ""/path/to/file/""
val fileWithInt = basePath + ""intFile.snappy.parquet""
val fileWithDouble = basePath + ""doubleFile.snappy.parquet""
val result = spark.sqlContext.read.option(""mergeSchema"", true).option(""basePath"", basePath).parquet(Seq(fileWithInt, fileWithDouble): _*).select(""myField"")
I get the following error
Caused by: org.apache.spark.SparkException: Failed to merge fields 'myField' and 'myField'. Failed to merge incompatible data types IntegerType and DoubleType
When passing an explicit schema 
val schema = StructType(Seq(new StructField(""myField"", IntegerType)))
val result = spark.sqlContext.read.schema(schema).option(""mergeSchema"", true).option(""basePath"", basePath).parquet(Seq(fileWithInt, fileWithDouble): _*).select(""myField"")
It fails with the following 
java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary
    at org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:48)
When casting up to a double
val schema = StructType(Seq(new StructField(""myField"", DoubleType)))
I get
java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainIntegerDictionary
    at org.apache.parquet.column.Dictionary.decodeToDouble(Dictionary.java:60)
Does anyone know any ways around this problem other than reprocessing the source data.
",<scala><apache-spark><apache-spark-sql>,1552,0,14,201,1,2,4,68,20664,0,0,1,14,2018-12-18 9:29,2018-12-26 16:09,,8,,Intermediate,16
55837863,"How can I perform version control of Procedures, Views, and Functions in Postgres sql","I want to do a versioning control of my database.
I currently control my front-end applications through git, however I am creating my database and would like to have a versioning of my tables, function and procedures, how can I accomplish this for database?
That is, I will make a change in a function but I would like to save the previous one I was executing in case there is any problem I can put the previous one again.
",<database><postgresql>,423,0,0,157,1,1,7,35,10062,0,4,2,14,2019-04-24 20:11,2019-04-24 21:35,2019-04-24 21:35,0,0,Intermediate,20
49757487,UnresolvedException: Invalid call to dataType on unresolved object when using DataSet constructed from Seq.empty (since Spark 2.3.0),"The following snippet works fine in Spark 2.2.1 but gives a rather cryptic runtime exception in Spark 2.3.0:
import sparkSession.implicits._
import org.apache.spark.sql.functions._
case class X(xid: Long, yid: Int)
case class Y(yid: Int, zid: Long)
case class Z(zid: Long, b: Boolean)
val xs = Seq(X(1L, 10)).toDS()
val ys = Seq(Y(10, 100L)).toDS()
val zs = Seq.empty[Z].toDS()
val j = xs
  .join(ys, ""yid"")
  .join(zs, Seq(""zid""), ""left"")
  .withColumn(""BAM"", when('b, ""B"").otherwise(""NB""))
j.show()
In Spark 2.2.1 it prints to the console
+---+---+---+----+---+
|zid|yid|xid|   b|BAM|
+---+---+---+----+---+
|100| 10|  1|null| NB|
+---+---+---+----+---+
In Spark 2.3.0 it results in:
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'BAM
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:105)
  at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
  at org.apache.spark.sql.types.StructType$$anonfun$fromAttributes$1.apply(StructType.scala:435)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:296)
  at org.apache.spark.sql.types.StructType$.fromAttributes(StructType.scala:435)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.schema$lzycompute(QueryPlan.scala:157)
  ...
The culprit really seems to be Dataset being created from an empty Seq[Z]. When you change that to something that will also result in an empty Dataset[Z] it works as in Spark 2.2.1, e.g.
val zs = Seq(Z(10L, true)).toDS().filter('zid === 999L)
In the migration guide from 2.2 to 2.3 is mentioned:
  Since Spark 2.3, the Join/Filter’s deterministic predicates that are after the first non-deterministic predicates are also pushed down/through the child operators, if possible. In prior Spark versions, these filters are not eligible for predicate pushdown.
Is this related, or a (known) bug?
",<scala><apache-spark><apache-spark-sql>,2225,0,38,149,1,1,5,62,15170,0,2,2,14,2018-04-10 15:25,2018-05-02 21:13,,22,,Advanced,33
50177658,NullReferenceException inside .NET code of SqlConnection.CacheConnectionStringProperties(),"I'm facing really strange issue. Given the code below:
static void Main()
{
    var c = new System.Data.SqlClient.SqlConnection();
    c.ConnectionString = ""Data Source=SOME_NAME;Initial Catalog=SOME_DB;Integrated Security=True"";
    c.ConnectionString = """"; //null also triggers exception
    Console.WriteLine(""Success"");
}
It worked fine for quite a time, but on newest version of Windows 10 (1803) which has .NET Version 4.7.03056 Release 461808 (seems to be 4.7.2) it crashes with following exception:
Unhandled Exception: System.NullReferenceException: Object reference not set to an instance of an object.
   at System.Data.SqlClient.SqlConnection.CacheConnectionStringProperties()
   at System.Data.SqlClient.SqlConnection.set_ConnectionString(String value)
   at TestCacheConnectionStringProperties.Program.Main()
This crashes on the second assignment, if I remove any of the assignments of the ConnectionString it works ok.   
I've looked at the sources and didn't find a place where NullReferenceException could happen (however sources seems to be for .NET Framework 4.7.1 so smth may change).
Now the question is - what causes this issue? Is this a .NET bug? If yes - how to address it?
UPDATE: According to the comments - thanks very much guys - the issue is caused by these lines (decompiled):
private void CacheConnectionStringProperties()
{
  SqlConnectionString connectionOptions = this.ConnectionOptions as SqlConnectionString;
  if (connectionOptions != null)
    this._connectRetryCount = connectionOptions.ConnectRetryCount;
  //Seems like this is causing the bug because it is not inside of null-check-if for connectionOptions variable
  if (this._connectRetryCount != 1 || !ADP.IsAzureSqlServerEndpoint(connectionOptions.DataSource))
    return;
  this._connectRetryCount = 2;
}
It is somehow related to Azure and is quite different from what is available in sources.
I've posted the issue here and will wait for response.
",<c#><.net><sqlconnection><.net-4.7.2>,1946,2,26,3559,2,29,44,51,1664,0,1448,1,14,2018-05-04 14:49,2018-05-10 20:54,,6,,Basic,12
52539100,Why did this postgres database upgrade fail?,"I am trying to upgrade from Postgresql 9.6 to 10 unsuccessfully.
I ran brew upgrade postgresql with success, then ran brew postgresql-upgrade-database with failure message. The issue seems to be this line:
lc_collate values for database ""postgres"" do not match:  old ""en_GB.UTF-8"", new ""en_US.UTF-8""
The whole message was:
    ==&gt; Upgrading postgresql data from 9.6 to 10...
Stopping `postgresql`... (might take a while)
==&gt; Successfully stopped `postgresql` (label: homebrew.mxcl.postgresql)
==&gt; Moving postgresql data from /usr/local/var/postgres to /usr/local/var/postgres.o
The files belonging to this database system will be owned by user ""jbkimac"".
This user must also own the server process.
The database cluster will be initialized with locale ""en_US.UTF-8"".
The default database encoding has accordingly been set to ""UTF8"".
The default text search configuration will be set to ""english"".
Data page checksums are disabled.
fixing permissions on existing directory /usr/local/var/postgres ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
WARNING: enabling ""trust"" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    /usr/local/opt/postgresql/bin/pg_ctl -D /usr/local/var/postgres -l logfile start
Performing Consistency Checks
-----------------------------
Checking cluster versions                                   ok
Checking database user is the install user                  ok
Checking database connection settings                       ok
Checking for prepared transactions                          ok
Checking for reg* data types in user tables                 ok
Checking for contrib/isn with bigint-passing mismatch       ok
Checking for invalid ""unknown"" user columns                 ok
Creating dump of global objects                             ok
Creating dump of database schemas
                                                            ok
lc_collate values for database ""postgres"" do not match:  old ""en_GB.UTF-8"", new ""en_US.UTF-8""
Failure, exiting
Error: Upgrading postgresql data from 9.6 to 10 failed!
==&gt; Removing empty postgresql initdb database...
==&gt; Moving postgresql data back from /usr/local/var/postgres.old to /usr/local/var/p
==&gt; Successfully started `postgresql` (label: homebrew.mxcl.postgresql)
Error: Failure while executing; `/usr/local/opt/postgresql/bin/pg_upgrade -r -b /usr/local/Cellar/postgresql/9.6.1/bin -B /usr/local/opt/postgresql/bin -d /usr/local/var/postgres.old -D /usr/local/var/postgres -j 8` exited with 1.
Can anyone help advise me as to how to fix this ""en_GB.UTF-8"", new ""en_US.UTF-8"" conflict issue?
",<postgresql>,3032,0,55,1930,0,19,39,61,8427,0,188,3,14,2018-09-27 14:13,2019-02-27 9:53,,153,,Basic,14
53385230,Laravel Eloquent join vs with,"I see that join is (by default inner join) and its returning all columns but it takes almost the same time as with keyword for just 1000 data.
$user->join(‘profiles’, ‘users.id’, ‘=’, ‘profiles.user_id’) - generates the below query.
select * from `users` inner join `profiles` on `users`.`id` = `profiles`.`user_id` where `first_name` LIKE '%a%'`
User::with(‘profile’) -  this eager loading outputs the below query 
select * from `users` where exists (select * from `profiles` where `users`.`id` = `profiles`.`user_id` and `first_name` LIKE '%a%')
which is the best way to return a list of users with a pagination for a REST API ?  eager loading seems promising but its with a sub query.
if do with eager loading, this is how i will be filtering. need to use whereHas
if($request-&gt;filled('first_name')){
        $query-&gt;whereHas('profile',function($q) use ($request){
            $q-&gt;where('first_name','like','%'.request('first_name').'%');
        });
    }
but if used Join, its less lines of code.
  if ($request-&gt;filled('first_name')) {
            $users = $users-&gt;where('first_name', 'LIKE', ""%$request-&gt;first_name%"");
        }
laravel version is 5.7
",<php><mysql><laravel><laravel-5><eloquent>,1177,0,10,5496,15,57,116,37,13011,0,2239,1,14,2018-11-20 2:09,2018-11-20 7:53,2018-11-20 7:53,0,0,Basic,8
50287558,How to rename duplicated columns after join?,"I want to use join with 3 dataframe, but there are some columns we don't need or have some duplicate name with other dataframes, so I want to drop some columns like below:
result_df = (aa_df.join(bb_df, 'id', 'left')
  .join(cc_df, 'id', 'left')
  .withColumnRenamed(bb_df.status, 'user_status'))
Please note that status column is in two dataframes, i.e. aa_df and bb_df.
The above doesn't work. I also tried to use withColumn, but the new column is created, and the old column is still existed.
",<apache-spark><pyspark><apache-spark-sql>,496,0,7,997,3,14,35,37,37148,0,6,3,14,2018-05-11 7:51,2018-05-11 7:59,2018-05-11 8:08,0,0,Basic,10
57040784,SQLAlchemy: foreign key to multiple tables,"Let's consider 3 tables:
books
American authors
British authors
Each book has a foreign key to its author, which can either be in the American table, or the British one.
How can I implement such foreign key condition in SQLAlchemy?
I'd like to have a single column to handle the link.
My approach so far was to create an abstract class Author, from which both AmericanAuthor and BritishAuthor inherit, and have the foreign key of Book point to the parent.
class Author(Model):
    __abstract__ = True
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String)
class AmericanAuthor(Author):
    __tablename__ = 'american_author'
    # some other stuff
class BritishAuthor(Author):
    __tablename__ = 'british_author'
    # some other stuff
class Book(Model):
    __tablename__ = 'book'
    title = db.Column(db.String)
    author_id = db.Column(db.Integer, db.ForeignKey(""author.id""))
It fails with the error:
sqlalchemy.exc.NoReferencedTableError: Foreign key associated with column 'books.author_id' could not find table 'author' with which to generate a foreign key to target column 'id'
Which completely makes sense, considering author is abstract...
",<python><postgresql><sqlalchemy>,1177,0,23,2553,1,17,29,65,6087,,158,2,14,2019-07-15 13:26,2019-07-15 13:52,2020-02-04 8:34,0,204,Intermediate,18
49147149,Does PESSIMISTIC_WRITE lock the whole table?,"Just to be sure that I correctly understand how things work.
If I do em.lock(employee, LockModeType.PESSIMISTIC_WRITE); - will it block only this entity (employee) or the whole table Employees?
If it matters, I am talking about PostgreSQL.
",<java><postgresql><hibernate><jpa><locking>,240,0,4,34221,20,137,244,62,8595,0,9088,1,14,2018-03-07 8:22,2018-03-07 12:40,2018-03-07 12:40,0,0,Advanced,32
58077193,Why does Django drop the SQL DEFAULT constraint when adding a new column?,"In the latest Django (2.2), when I add a new field to a model like this:
new_field= models.BooleanField(default=False)
Django runs the following commands for MySQL:
ALTER TABLE `app_mymodel` ADD COLUMN `new_field` bool DEFAULT b'0' NOT NULL;
ALTER TABLE `app_mymodel` ALTER COLUMN `new_field` DROP DEFAULT;
COMMIT;
While this works when everything is updated, this is very problematic because old versions of the application can no longer create models after this migration is run (they do not know about new_field). Why not just keep the DEFAULTconstraint?
",<mysql><django><django-models><database-migration><django-migrations>,558,0,6,37451,7,47,62,78,1987,0,741,2,14,2019-09-24 9:30,2019-09-27 8:08,2019-10-02 10:58,3,8,Basic,3
63850520,Multiple SQL query not working with delimiter on DBeaver,"I can't execute MySQL statement when using delimiter (default ';'). I mean, when I run query like:
select * from mdw.dim_date dd limit 10;
select * from mdw.dim_order do limit 5;
I've got such error:
SQL Error [1064] [42000]: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'select * from mdw.dim_order do limit 5' at line 2
I don't want to execute this per Alt+X (I am using Dbeaver) as I want to put query into pentaho. I need to execute around 50 deletes, so I don't want to divide it on 50 SQL scripts.
I am using MySQL (version: 5.6.36-82.0).
",<mysql><sql><pentaho><dbeaver>,634,0,2,151,1,1,5,56,28714,0,0,3,14,2020-09-11 16:00,2020-09-11 16:09,,0,,Basic,10
49097346,How to speed up a SQLAlchemy Query?,"I have a table of over 10 million rows. There are roughly 50+ columns. The table stores sensors data/parameters. Let's say that I need to query data for the whole day or 86,400 seconds. It would need take roughly 20 or more seconds to complete this query.
I have added individual indices on a few columns such as recordTimestamp(that store when the data is captured), deviceId(the identification of the sensor), positionValid(whether GPS geolocation is valid). Then I added a composite index which includes all three columns. 
Below is my query:
t1 = time.time()
conn = engine.connect()
select_statement = select([Datatable]).where(and_(
    Datatable.recordTimestamp &gt;= start_date,
    Datatable.recordTimestamp &lt;= end_date,
    Datatable.deviceId == device_id,
    Datatable.positionValid != None,
    Datatable.recordTimestamp % query_interval == 0))
lol_data = conn.execute(select_statement).fetchall()    
conn.close() 
t2 = time.time()
time_taken = t2 - t1
print('Select: ' + time_taken)
Below is my EXPLAIN ANALYZE statement:
EXPLAIN ANALYZE SELECT datatable.id, datatable.""createdAt"", datatable.""analogInput01"", datatable.""analogInput02"", datatable.""analogInput03"", datatable.""analogInput04"", datatable.""analogInput05"", datatable.""analogInput06"", datatable.""analogInput07"", datatable.""canEngineRpm"", datatable.""canEngineTemperature"", datatable.""canFuelConsumedLiters"", datatable.""canFuelLevel"", datatable.""canVehicleMileage"", datatable.""deviceId"", datatable.""deviceTemperature"", datatable.""deviceInternalVoltage"", datatable.""deviceExternalVoltage"", datatable.""deviceAntennaCut"", datatable.""deviceEnum"", datatable.""deviceVehicleMileage"", datatable.""deviceSimSignal"", datatable.""deviceSimStatus"", datatable.""iButton01"", datatable.""iButton02"", datatable.""recordSequence"", datatable.""recordTimestamp"", datatable.""accelerationAbsolute"", datatable.""accelerationBrake"", datatable.""accelerationBump"", datatable.""accelerationTurn"", datatable.""accelerationX"", datatable.""accelerationY"", datatable.""accelerationZ"", datatable.""positionAltitude"", datatable.""positionDirection"", datatable.""positionSatellites"", datatable.""positionSpeed"", datatable.""positionLatitude"", datatable.""positionLongitude"", datatable.""positionHdop"", datatable.""positionMovement"", datatable.""positionValid"", datatable.""positionEngine"" FROM datatable WHERE datatable.""recordTimestamp"" &gt;= 1519744521 AND datatable.""recordTimestamp"" &lt;= 1519745181 AND datatable.""deviceId"" = '864495033990901' AND datatable.""positionValid"" IS NOT NULL AND datatable.""recordTimestamp"" % 1 = 0;
Below is the result from EXPLAIN ANALYZE of the SELECT:
Index Scan using ""ix_dataTable_recordTimestamp"" on dataTable (cost=0.44..599.35 rows=5 width=301) (actual time=0.070..10.487 rows=661 loops=1)
Index Cond: ((""recordTimestamp"" &gt;= 1519744521) AND (""recordTimestamp"" &lt;= 1519745181))
Filter: ((""positionValid"" IS NOT NULL) AND ((""deviceId"")::text = '864495033990901'::text) AND ((""recordTimestamp"" % 1) = 0))
Rows Removed by Filter: 6970
Planning time: 0.347 ms
Execution time: 10.658 ms
Whereas below is the result from time taken calculated by Python:
Select:  47.98712515830994 
JSON:  0.19731807708740234
Below is my code profiling:
10302 function calls (10235 primitive calls) in 12.612 seconds
Ordered by: cumulative time
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:882(execute)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/sql/elements.py:267(_execute_on_connection)
    1    0.000    0.000   12.595   12.595 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1016(_execute_clauseelement)
    1    0.000    0.000   12.592   12.592 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/base.py:1111(_execute_context)
    1    0.000    0.000   12.590   12.590 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/default.py:506(do_execute)
    1   12.590   12.590   12.590   12.590 {method 'execute' of 'psycopg2.extensions.cursor' objects}
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1113(fetchall)
    1    0.000    0.000    0.017    0.017 /Users/afeezaziz/Projects/Bursa/backend/env/lib/python3.6/site-packages/sqlalchemy/engine/result.py:1080(_fetchall_impl)
    1    0.008    0.008    0.017    0.017 {method 'fetchall' of 'psycopg2.extensions.cursor' objects}
",<python><postgresql><sqlalchemy><psycopg2>,4692,0,36,1347,2,12,26,45,15193,,37,3,14,2018-03-04 16:04,2018-04-03 7:06,,30,,Intermediate,23
49395898,Does MySQL support partial indexes?,"Partial indexes only include a subset of the rows of a table.
I've been able to create partial indexes in Oracle, DB2, PostgreSQL, and SQL Server. For example, in SQL Server I can create the index as:
create index ix1_case on client_case (date) 
  where status = 'pending';
This index is cheap since it does not include all 5 million rows of the table, but only the pending cases, that should not exceed a thousand rows. 
How do I do it in MySQL?
",<mysql><sql><database><indexing><relational-database>,447,0,2,46447,10,41,77,46,12817,0,2923,3,14,2018-03-20 23:44,2018-03-21 2:03,2018-04-18 15:58,1,29,Basic,3
59943384,Datatype for phone numbers in postgresql,"I am new to postgresql so can anyone tell me that is there any specific datatype to store phone numbers in postgresql while creating table in pgadmin or is it just string?
",<postgresql><database-design><sqldatatypes>,172,0,0,393,2,6,14,45,40259,0,4,2,14,2020-01-28 6:43,2020-01-28 7:35,2020-01-28 7:52,0,0,Basic,1
48865416,How to use the latest sqlite3 version in python,"I need to use sqlite version 3.8 or higher with python in Amazon Linux. 
I updated my sqlite installation to the latest version:
$ sqlite3 -version
3.22.0 2018-01-22 18:45:57 0c55d179733b46d8d0ba4d88e01a25e10677046ee3da1d5b1581e86726f2171d
I also updated my pysqlite version
pip install --upgrade pysqlite
However, my pysqlite still only seems to support sqlite version 3.7:
$ python
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt;
&gt;&gt;&gt; from pysqlite2 import dbapi2 as sqlite
&gt;&gt;&gt; sqlite.version
'2.8.3'
&gt;&gt;&gt; sqlite.sqlite_version
'3.7.17'
How can I update the sqlite python API to support a newer version of sqlite?
",<python><sqlite><pysqlite>,716,0,15,331,1,2,11,73,13026,0,2,4,14,2018-02-19 11:40,2018-07-17 15:37,,148,,Basic,3
62602720,String to Date migration from Spark 2.0 to 3.0 gives Fail to recognize 'EEE MMM dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter,"I have a date string from a source in the format 'Fri May 24 00:00:00 BST 2019' that I would convert to a date and store in my dataframe as '2019-05-24' using code like my example which works for me under spark 2.0
from pyspark.sql.functions import to_date, unix_timestamp, from_unixtime
df = spark.createDataFrame([(&quot;Fri May 24 00:00:00 BST 2019&quot;,)], ['date_str'])
df2 = df.select('date_str', to_date(from_unixtime(unix_timestamp('date_str', 'EEE MMM dd HH:mm:ss zzz yyyy'))).alias('date'))
df2.show(1, False)
In my sandbox environment I've updated to spark 3.0 and now get the following error for the above code, is there a new method of doing this in 3.0 to convert my string to a date
: org.apache.spark.SparkUpgradeException: You may get a different
result due to the upgrading of Spark 3.0: Fail to recognize 'EEE MMM
dd HH:mm:ss zzz yyyy' pattern in the DateTimeFormatter.
You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the
behavior before Spark 3.0.
You can form a valid datetime pattern with the guide from
https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
",<apache-spark><pyspark><apache-spark-sql>,1113,2,4,191,1,1,5,46,59095,0,0,4,14,2020-06-26 20:55,2020-06-27 7:46,,1,,Basic,3
53579444,"Efficient text preprocessing using PySpark (clean, tokenize, stopwords, stemming, filter)","Recently, I began to learn the spark on the book ""Learning Spark"". In theory, everything is clear, in practice, I was faced with the fact that I first need to preprocess the text, but there were no actual tips on this topic.
The first thing that I took into account is that it is now preferable to use Dataframe instead of RDD, so my preprocessing attempt was made on dataframes.
Required operations:
Clearing text from punctuation (regexp_replace)
Tokenization (Tokenizer)
Delete stop words (StopWordsRemover)
Stematization (SnowballStemmer)
Filtering short words (udf)
My code is:
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, lower, regexp_replace
from pyspark.ml.feature import Tokenizer, StopWordsRemover
from nltk.stem.snowball import SnowballStemmer
spark = SparkSession.builder \
    .config(""spark.executor.memory"", ""3g"") \
    .config(""spark.driver.cores"", ""4"") \
    .getOrCreate()
df = spark.read.json('datasets/entitiesFull/full').select('id', 'text')
# Clean text
df_clean = df.select('id', (lower(regexp_replace('text', ""[^a-zA-Z\\s]"", """")).alias('text')))
# Tokenize text
tokenizer = Tokenizer(inputCol='text', outputCol='words_token')
df_words_token = tokenizer.transform(df_clean).select('id', 'words_token')
# Remove stop words
remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')
df_words_no_stopw = remover.transform(df_words_token).select('id', 'words_clean')
# Stem text
stemmer = SnowballStemmer(language='english')
stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))
df_stemmed = df_words_no_stopw.withColumn(""words_stemmed"", stemmer_udf(""words_clean"")).select('id', 'words_stemmed')
# Filter length word &gt; 3
filter_length_udf = udf(lambda row: [x for x in row if len(x) &gt;= 3], ArrayType(StringType()))
df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))
Processing takes a very long time, the size of the entire document is 60 GB. Does it make sense to use RDD? Will caching help? How can I optimize preprocessing?
First I tested the implementation on the local computer, then I will try on the cluster. Local computer - Ubuntu RAM 6Gb, 4 CPU. Any alternative solution is also welcome. Thanks!
",<python><apache-spark><pyspark><apache-spark-sql><text-processing>,2270,0,30,169,1,1,7,64,15602,0,0,1,14,2018-12-02 10:40,2020-07-08 21:59,,584,,Intermediate,23
55455166,Special character (Hawaiian 'Okina) leads to weird string behavior,"The Hawaiian quote has some weird behavior in T-SQL when using it in conjunction with string functions. What's going on here? Am I missing something? Do other characters suffer from this same problem?
SELECT UNICODE(N'ʻ') -- Returns 699 as expected.
SELECT REPLACE(N'""ʻ', '""', '_') -- Returns ""ʻ, I expected _ʻ
SELECT REPLACE(N'aʻ', 'a', '_') -- Returns aʻ, I expected _ʻ
SELECT REPLACE(N'""ʻ', N'ʻ', '_') -- Returns __, I expected ""_
SELECT REPLACE(N'-', N'ʻ', '_') -- Returns -, I expected -
Also, strange when used in a LIKE for example:
DECLARE @table TABLE ([Name] NVARCHAR(MAX))
INSERT INTO
    @table
VALUES
    ('John'),
    ('Jane')
SELECT
    *
FROM
    @table
WHERE
    [Name] LIKE N'%ʻ%' -- This returns both records. I expected none.
",<sql-server><t-sql><unicode><collation>,746,1,23,2601,3,24,41,74,826,0,49,2,14,2019-04-01 12:29,2019-04-01 13:44,2019-04-04 20:23,0,3,Basic,2
63609570,Mysql 'VALUES function' is deprecated,"This is my python code which prints the sql query.
def generate_insert_statement(column_names, values_format, table_name, items, insert_template=INSERT_TEMPLATE, ):
    return insert_template.format(
        column_names=&quot;,&quot;.join(column_names),
        values=&quot;,&quot;.join(
            map(
                lambda x: generate_raw_values(values_format, x),
                items
            )
        ),
        table_name=table_name,
        updates_on=create_updates_on_columns(column_names)
    )
query = generate_insert_statement(table_name=property['table_name'],
        column_names=property['column_names'],
        values_format=property['values_format'], items=batch)
        print(query) #here
        execute_commit(query)
When printing the Mysql query my Django project shows following error in the terminal:
'VALUES function' is deprecated and will be removed in a future release. Please use an alias (INSERT INTO ... VALUES (...) AS alias) and replace VALUES(col) in the ON DUPLICATE KEY UPDATE clause with alias.col instead
Mysql doumentation does not say much about it.What does this mean and how to can i rectify it.
INSERT_TEMPLATE = &quot;INSERT INTO {table_name} ({column_names}) VALUES {values} ON DUPLICATE KEY UPDATE {updates_on};&quot;
",<python><mysql>,1276,0,19,187,0,1,9,80,9766,0,10,1,14,2020-08-27 5:09,2020-08-27 5:18,2020-08-27 5:18,0,0,Basic,2
52915923,Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured. SPRING,"I've already checked all the similar questions and every answer says that I need to specify a driverClassName which I already do. Here is my application.yml:
spring:
  application:
    name: cibus-backend
  datasource:
    driverClassName: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/Cibus?useSSL=true
    username: root
    password: 1234567890
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: update
      properties:
      hibernate:
        format_sql: true
        type: trace
    database-platform: org.hibernate.dialect.MySQL5InnoDBDialect
    database: mysql
logging:
  level:
    org:
      hibernate:
        type: trace
Am I missing something? The weird thing is that a classmate of mine who has the same code can start the app perfectly well. This is why I think it has something to do with the path. Maybe spring isn't accessing the yml file. I included it in src.main.resources which is the default place where Spring looks for it.
Here is the stacktrace:
Error starting ApplicationContext. To display the conditions report re-run         
your application with 'debug' enabled.
2018-10-21 10:13:15.657 ERROR 10356 --- [JavaFX-Launcher]             
o.s.b.d.LoggingFailureAnalysisReporter   : 
***************************
APPLICATION FAILED TO START
***************************
Description:
Failed to configure a DataSource: 'url' attribute is not specified and no embedded datasource could be configured.
Reason: Failed to determine a suitable driver class
Action:
Consider the following:
    If you want an embedded database (H2, HSQL or Derby), please put it on the classpath.
    If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active).
Exception in Application init method
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:389)
    at com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:328)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:767)
Caused by: java.lang.RuntimeException: Exception in Application init method
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:912)
    at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$155(LauncherImpl.java:182)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:732)
    at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:197)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1267)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1124)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:372)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1089)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:859)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:780)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:412)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:333)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1277)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1265)
    at labtic.AppStarter.init(AppStarter.java:25)
    at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:841)
    ... 2 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:590)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1247)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:251)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1135)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1062)
    at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:818)
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:724)
    ... 30 more
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:582)
    ... 43 more
Caused by: org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Failed to determine a suitable driver class
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.determineDriverClassName(DataSourceProperties.java:236)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceProperties.initializeDataSourceBuilder(DataSourceProperties.java:176)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration.createDataSource(DataSourceConfiguration.java:43)
    at org.springframework.boot.autoconfigure.jdbc.DataSourceConfiguration$Hikari.dataSource(DataSourceConfiguration.java:83)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)
    ... 44 more
Exception running application labtic.AppStarter
Here is the Gradle Build as I was asked to include it:
buildscript {
    ext {
        springBootVersion = '2.0.5.RELEASE'
    }
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath(""org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}"")
    }
}
apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'org.springframework.boot'
apply plugin: 'io.spring.dependency-management'
group = ''
version = '0.0.1-SNAPSHOT'
sourceCompatibility = 1.8
repositories {
    mavenCentral()
    jcenter()
}
dependencies {
    compile('org.springframework.boot:spring-boot-starter-data-jpa')
    runtime('mysql:mysql-connector-java')
    testCompile('org.springframework.boot:spring-boot-starter-test')
    // https://mvnrepository.com/artifact/mysql/mysql-connector-java
    compile group: 'mysql', name: 'mysql-connector-java', version: '8.0.12'
    compileOnly 'org.projectlombok:lombok:1.18.2'
    annotationProcessor ""org.projectlombok:lombok:1.18.2""
}
Any ideas? Thank you in advance.
",<java><mysql><spring><spring-boot>,12522,1,161,604,1,10,19,59,40228,0,135,6,14,2018-10-21 13:42,2018-10-21 15:13,2018-10-21 15:13,0,0,Advanced,39
53433285,MySQL Update or Rename a Key in JSON,"I'm having this json stored in db
{
    ""endDate"": ""2018-10-10"",
    ""startDate"": ""2017-09-05"", 
    ""oldKeyValue"": {
        ""foo"": 1000, 
        ""bar"": 2000, 
        ""baz"": 3000
    },
    ""anotherValue"": 0
}
How can I rename ""oldKeyValue"" key to ""newKeyValue"" without knowing the index of the key in an UPDATE query? I'm looking for something like this
UPDATE `my_table` SET `my_col` = JSON()
NOTE: only the key needs to change, the values (i.e. {""foo"": 1000, ""bar"": 2000, ""baz"": 3000}) should remain the same
",<mysql><json>,515,0,15,4117,20,72,131,53,13753,0,170,3,14,2018-11-22 14:39,2018-11-22 14:56,2018-11-22 14:56,0,0,Basic,2
51933189,Character encoding (UTF-8) in PowerShell session,"Hei all,
as a console/terminal enthusiast and database administrator (PostgreSQL) it is essential for me to work with the correct charcater encoding.
Therefore, I want my client console/terminal window always set to e.g. UTF-8.
Back with Windows' CMD.EXE this attempt was as easy as typing the command chcp 65001 to set the desired code page identifier.
Now, I am in the process of switching to PowerShell and setting the character encoding seems very odd, IMHO.
I've done some research on how to set the PowerShell session to UTF-8 and I figured out, that I need three steps/commmnds to accomplish that.
PS C:\&gt; $OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; [Console]::OutputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; chcp 65001
Despite the fact that the first two commands are not intuitive and hard to remember...
Leaving out one of them leads to something not working out properly!
Also, setting just one of them seems to have no effect to the others.
So, I must set all three for working with the PostgreSQL's psql database client.
Otherwise I run into encoding issues while exporting/importing data.
Now my question is: ""Why the heck? Isn't there an easier way to simply set the character encoding in PowerShell?""
Unfortunately, I did not find any plausible documentation myself about setting the character enconding!
Thanks in advance
/EDIT
The second comment by TheIncorrigible1 led me to the best answer fo far: Displaying Unicode in Powershell
- So one can set the whole PowerShell with two separated statements to the desired encoding (UTF-8).
PS C:\&gt; $OutputEncoding = [System.Console]::OutputEncoding = [System.Console]::InputEncoding = [System.Text.Encoding]::UTF8
PS C:\&gt; $PSDefaultParameterValues['*:Encoding'] = 'utf8'
Explanation:
$OutputEncoding sets the encoding for e.g. | (piping) and/or communication between programs and/or processes.
[System.Console]::OutputEncoding sets the encoding for STDOUT and the console/terminal output.
[System.Console]::InputEncoding sets the encoding for STDIN or keyboard input.
$PSDefaultParameterValues['*:Encoding'] sets the encoding for all cmdlets that support the -Encoding option like e.g. Out-File -Encoding.
",<windows><postgresql><powershell><character-encoding>,2200,3,14,358,1,4,14,70,19962,0,922,1,14,2018-08-20 14:40,2020-11-04 12:01,,807,,Basic,2
51950129,Execute raw query in migration - Sequelize 3.30,"I want to execute a raw query in my migrations up and down functions. 
When I try to do: Sequelize.query, it says ERROR: Sequelize.query is not a function.
This is my migration skeleton file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize, migration) =&gt; {
     return Sequelize.query(...);   //ERROR: Sequelize.query is not a Function
  },
  down: (queryInterface, Sequelize) =&gt; {
     return Sequelize.query(...);  //ERROR: Sequelize.query is not a Function
  }
};
",<node.js><postgresql><sequelize.js>,491,0,17,10327,20,80,123,50,13394,,1192,1,14,2018-08-21 13:42,2018-08-22 1:56,2018-08-22 1:56,1,1,Basic,3
55998961,RSConfig generates a Dsn Connection String doesn't work,"TL;DR. 
Repro steps, take a backup of your C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config 
Run this command to update the connection string in SSRS's config:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s &lt;ServerName&gt; -i &lt;instanceNameIfNotDefault&gt; -d ""reportserver$ssrs"" -a SQL -u sa -p ""YourSAPassword"" -t
Now browse to the SSRS website and it doesn't work! To fix it either restore your config file or run through the SSRS GUI tool and it works!
How does the RsConfig utility work?
Background
After I install SSRS on an Windows 2016 Server and restore the 2 databases I need to change the Connection String in SSRS configuration file to point to the new SQL server name/instance.
Problem
When I try to change the encrypted Connection String in C:\Program Files\Microsoft SQL Server\MSRS13.SSRS\Reporting Services\ReportServer\RsReportServer.config file using the RSConfig utility:
C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn&gt;rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!"" -t
It changes the Dsn Connection String in the RsReportServer.config. 
Before:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAC2DBxZFsfVB16r0e3......
*
After:
&lt;Dsn>AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAE+tJc/4Vs0a0fdH0tCY8kgQAAAAiAAAAUgBlAHAAbwByAHQAaQBuAGcAIABTAGUAcgB2AGUAcgAAABBmAAAAAQAAIAAAAO2nOjFDJMo........
*
However after this change, browsing to the SSRS Website results in the error:
  The report server can’t connect to its database. Make sure the database is running and accessible. You can also check the report server trace log for details.
If I run the SQL Reporting Services Configuration Tool (GUI) and change the Dsn Connection String browsing to the SSRS Website works! 
Obviously it changes the Dsn but I can't work out what else it does whilst the GUI tool is running. I've used ProcessMonitor and I've seen that the GUI tool does NOT use   RSConfig.exe utility, it uses itself RsConfigTool.exe! So I can't even capture command-line arguments of what the actual command/connection string should be. Also each time we change the connection string a new random one is generated so not sure how to do comparison's of actual vs expected.
I did a WinDiff of Registry keys and apart from some encrypted hexadecimal diffs, nothing stood out.
I run SQLProfiler and there were a bunch of grants that I have emulated in my PowerShell script, eg: 
$sqls += @""
USE [ReportServer`$SSRSTempDB]
if not exists (select * from sysusers where issqlrole = 1 and name = 'RSExecRole')
BEGIN
 EXEC sp_addrole 'RSExecRole'
END;
GO
My hunch is the $ sign in the SQL Database Name and the @ in the ""made up/simulated"" password are not getting escaped when I run the commands, eg:
$MachineName = ""server0012""
$instanceName = ""SSRS""
$saPassword = ""P@ssw0rd!""
$rsConfigPath = ""C:\Program Files (x86)\Microsoft SQL Server\130\Tools\Binn\rsconfig.exe""
$setupArgs = -join('-c -s ""', $MachineName,'"" -i ""', $instanceName,'"" -d ','""ReportServer`$SSRS"" -t -a SQL -u ""sa"" -p ""', $saPassword,"""""""")
Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope Process
Write-Host $rsConfigPath $setupArgs
$args = $setupArgs.Split("" "")
&amp; ""$rsConfigPath"" $args
Restart-Service -Force ""SQL Server ($instanceName)""
When I run these vanilla commands in Command Prompt (no need to escape PowerShell characters):
rsconfig -c -s Server0012 -i SSRS -d ""reportserver$ssrs"" -a SQL -u sa -p ""P@ssw0rd!""
It changes the Dsn Connection String but browsing to SSRS Website gives same error (above).
How can I find out what else RsConfigTool.exe does when changing the Current Report Server Database? Or any guesses why the Connection String generated using the RSConfig Utility is out of whack - I've tried many different combinations, seems like only the RSConfigTool can actually do it?
Note 1:
I'm scripting this all up as a DevOps project and we are baking these images with packer, so nothing can be done manually. 
Note 2:
The Machine is joined to the domain and renamed after SQL installed. So using a Configuration.ini file I don't think will work.
",<sql><powershell><encryption><reporting-services><connection-string>,4235,4,25,62781,37,202,325,79,3721,,6536,2,14,2019-05-06 4:53,2019-05-15 17:44,2019-05-30 2:06,9,24,Advanced,32
50037975,Wordpress cannot connect to mysql server,"I have run a mysql server in my macbook, where I can access via both mysql command mysql -u root and navicat application. However, when I open the install page of a brand new wordpress app in my macbook. During the installation, I had got：
",<mysql><wordpress>,240,1,1,339,1,3,11,40,14936,0,10,10,14,2018-04-26 8:11,2018-04-26 8:32,,0,,Basic,9
52146191,Why can MySQL not use a partial primary key index?,"The MySQL documentation describing the use of index extensions, gives the following table as an example, followed by the query below:
CREATE TABLE t1 (
    i1 INT NOT NULL DEFAULT 0,
    i2 INT NOT NULL DEFAULT 0,
    d DATE DEFAULT NULL,
    PRIMARY KEY (i1, i2),
    INDEX k_d (d)
) ENGINE = InnoDB;
SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = '2000-01-01';
InnoDB internally will convert the index k_d to include the primary key at the end.  That is, the actual index k_d will be on (d, i1, i2), three columns.
The documentation goes on to explain that (emphasis mine):
  The optimizer cannot use the primary key in this case because that comprises columns (i1, i2) and the query does not refer to i2. Instead, the optimizer can use the secondary index k_d on (d), and the execution plan depends on whether the extended index is used.
I am confused by the above statement.  First it says that i1 is not enough to use the primary key index of two columns (i1, i2).  Then, in the second sentence, it says that the index k_d on (d, i1, i2) can be used, despite that only d and i1 are being used, with i2 absent.
My general understanding of indices in MySQL, and in other flavors of SQL, is that a left portion of an index can be used if a subset of all columns in the index are present, starting from the left.
What is different about a primary key (clustered) index and a non clustered secondary index which allows the latter to use a partial index, but the former cannot?
",<mysql><indexing><clustered-index>,1475,1,19,505816,28,292,366,49,2049,0,17194,2,14,2018-09-03 8:47,2018-09-06 15:48,2018-09-06 15:48,3,3,Basic,5
48460910,Electron App Getting Exception While requiring SQLITE3,"package.json 
""name"": ""billingapp"",
""version"": ""1.0.0"",
""description"": """",
""main"": ""index.js"",
""scripts"": {
""rebuild"": ""electron-rebuild -f -w billingapp""
},
""author"": ""S Kundu"",
""license"": ""ISC"",
""dependencies"": {
""electron"": ""^1.7.11"",
""sqlite3"": ""^3.1.13""
}
""devDependencies"": {
""electron-rebuild"": ""^1.7.3""
}
index.js 
const electron  = require('electron');
const path      = require('path');
const url       = require('url');
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
const {app, BrowserWindow, Menu, ipcMain} = electron;
let mainWindow;
app.on('ready', function(){
// Create the login window
mainWindow = new BrowserWindow({
  resizable: true,
  fullscreen: false
});
// Load html in window
mainWindow.loadURL(url.format({
  pathname: path.join(__dirname, 'login.html'),
  protocol: 'file:',
  slashes: true
}));
});
login.html 
&lt;h1&gt;Welcome to billing system&lt;/h1&gt;
These are the code files.
Steps to install NPM Packages
npm install electron
npm install sqlite3
Its working perfect when I remove bellow code:
var sqlite3 = require('sqlite3').verbose();
var db = new sqlite3.Database(path.join(__dirname, 'sample.db'));
But  with this code , while running 
npm start
is getting bellow error:
App threw an error during load
  Error: Cannot find module
  'C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64\node_sqlite3.node'
      at Module._resolveFilename (module.js:470:15)
      at Function.Module._resolveFilename (C:\Users\sintu\Desktop\BillingSystem\node_modules\electron\dist\resources\electron.asar\common\reset-search-paths.js:35:12)
      at Function.Module._load (module.js:418:25)
      at Module.require (module.js:498:17)
      at require (internal/module.js:20:19)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:4:15)
      at Object. (C:\Users\sintu\Desktop\BillingSystem\node_modules\sqlite3\lib\sqlite3.js:190:3)
      at Module._compile (module.js:571:32)
      at Object.Module._extensions..js (module.js:580:10)
      at Module.load (module.js:488:32)
when I run npm run rebuild , I get bellow error 
× Rebuild Failed
An unhandled error occurred inside electron-rebuild
Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
Error: Building the projects in this solution one at a time. To enable parallel build, please add the ""/m"" switch.
C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\build\deps\action_before_build.vcxproj(20,3): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the  declaration is correct, and that the file exists on disk.
gyp ERR! build error
gyp ERR! stack Error: C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\lib\build.js:258:23)
gyp ERR! stack     at emitTwo (events.js:126:13)
gyp ERR! stack     at ChildProcess.emit (events.js:214:7)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:198:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""C:\Program Files\nodejs\node.exe"" ""C:\Users\sintu\Desktop\billingApp\node_modules\node-gyp\bin\node-gyp.js"" ""rebuild"" ""--target=1.7.11"" ""--arch=x64"" ""--dist-url=https://atom.io/download/electron"" ""--build-from-source"" ""--module_name=node_sqlite3"" ""--module_path=C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3\lib\binding\electron-v1.7-win32-x64"" ""--host=https://mapbox-node-binary.s3.amazonaws.com"" ""--remote_path=./{name}/v3.1.13/{toolset}/"" ""--package_name=electron-v1.7-win32-x64.tar.gz""
gyp ERR! cwd C:\Users\sintu\Desktop\billingApp\node_modules\sqlite3
gyp ERR! node -v v8.9.1
gyp ERR! node-gyp -v v3.6.2
gyp ERR! not ok
Failed with exit code: 1
    at SafeSubscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:277:84)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:198:26)
    at Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at MapSubscriber.Subscriber._error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:129:26)
    at MapSubscriber.Subscriber.error (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:103:18)
    at SafeSubscriber._next (C:\Users\sintu\Desktop\billingApp\node_modules\spawn-rx\lib\src\index.js:251:65)
    at SafeSubscriber.__tryOrUnsub (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:239:16)
    at SafeSubscriber.next (C:\Users\sintu\Desktop\billingApp\node_modules\rxjs\Subscriber.js:186:22)
npm ERR! code ELIFECYCLE
npm ERR! errno 4294967295
npm ERR! billingapp@1.0.0 rebuild: electron-rebuild -f -w billingapp
npm ERR! Exit status 4294967295
npm ERR!
npm ERR! Failed at the billingapp@1.0.0 rebuild script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
npm ERR! A complete log of this run can be found in:
npm ERR!     C:\Users\sintu\AppData\Roaming\npm-cache_logs\2018-01-30T15_36_46_678Z-debug.log
",<javascript><sqlite><electron>,6886,4,51,165,0,1,11,66,1369,,9,1,14,2018-01-26 11:56,2018-07-23 15:52,,178,,Advanced,37
53560489,"EF Core SQLite in memory exception: SQLite Error 1: 'near ""MAX"": syntax error'","I am creating SQLite In Memory database for unit testing:
        var connection = new SqliteConnection(""DataSource=:memory:"");
        connection.Open();
        try
        {
            var options = new DbContextOptionsBuilder&lt;BloggingContext&gt;()
                .UseSqlite(connection)
                .Options;
            // Create the schema in the database
            using (var context = new BloggingContext(options))
            {
                context.Database.EnsureCreated();
            }
            // Run the test against one instance of the context
            using (var context = new BloggingContext(options))
            {
                var service = new BlogService(context);
                service.Add(""http://sample.com"");
            }
            // Use a separate instance of the context to verify correct data was saved to database
            using (var context = new BloggingContext(options))
            {
                Assert.AreEqual(1, context.Blogs.Count());
                Assert.AreEqual(""http://sample.com"", context.Blogs.Single().Url);
            }
        }
context.Database.EnsureCreated(); fails with with exception:
Message: Microsoft.Data.Sqlite.SqliteException : SQLite Error 1: 'near ""MAX"": syntax error'.
There is github issue saying: 
The issue here is varchar(max) is SqlServer specific type. The scaffolding should not add it as relational type which gets passed to migration in other providers which is prone to generate invalid sql at migration.
But how then can I use SQLite in Memory for unit tests if my database contains many varchar(max) columns?
",<c#><sqlite><entity-framework-core><ef-core-2.0><ef-core-2.1>,1619,3,29,2103,3,20,40,58,7400,,160,4,14,2018-11-30 15:33,2018-12-30 20:05,,30,,Intermediate,18
53386872,Is it possible to use StringFormat or Constant Variable in android Room Query,"I want to query the user associations list with the following room query using public constant variable Association.MEMBER_STATUS_APPROVED.
@Query(""SELECT * FROM Association WHERE memberStatus = "" + Association.MEMBER_STATUS_APPROVED)
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations();
But, room gives me [SQLITE_ERROR] when build.
 It is possible to re-write that query by replacing the constant variable with parameter like the following.
@Query(""SELECT * FROM Association WHERE memberStatus = :statusApproved"")
LiveData&lt;List&lt;Association&gt;&gt; loadUserAssociations(String statusApproved);
I would like to know that does Room support such kind of string concatenation or String Format? (or) May be I missing something?
",<android><database><android-sqlite><android-room>,741,0,4,571,1,5,8,39,3871,,6,3,14,2018-11-20 5:39,2018-11-20 6:12,,0,,Intermediate,18
65481470,Connect to remote db with ssh tunneling in DBeaver,"I know this question was already asked before (like here), but still I could not find a solution and those posts are quite old.
So I am able to connect to the remote db with an ssh connection and then use the command line like this:
// Putty SSH Connection
host: ssh.strato.de
port: 22
username: xxxxxxx 
password: xxxxxxx 
// connect to mysql with terminal
mysql -h rdbms -u xxxxxxx -p xxxxxxxx
If I try the same with ssh-tunneling in DBeaver I get an connection error
The ssh-tunneling itself seems to work. If I use the same credentials as above and press &quot;Test tunnel configuration&quot; I get a success message.
I tried several other options for port and host (localhost, rdbms.strato.de, etc), which I found via mysql show variables; show processlist; show user();, but none of them worked.
The Strato Support told me that I can only connect to the db internally with phpmyadmin or remotely wiht putty and mysql, but since the last method is working, shouldn't ssh-tunneling also work?
",<mysql><ssh><ssh-tunnel><dbeaver>,997,3,11,511,1,4,14,45,37688,0,14,2,14,2020-12-28 17:23,2021-04-27 18:20,,120,,Basic,10
51835172,Sequelize Error: Relation does not exist,"I am able to use sequelize.js to do an INSERT INTO command for a table in my development database, but not in my test database.
Despite researching thoroughly, I have not been able to resolve the issue.
A similar question has been posted here, though I have not been able to answer my question with the answers:
sequelize with postgres database not working after migration from mysql
Here is my relevant migration file:
'use strict';
module.exports = {
  up: (queryInterface, Sequelize) =&gt; {
    return queryInterface.createTable('Trees', {
      id: {
        allowNull: false,
        autoIncrement: true,
        primaryKey: true,
        type: Sequelize.INTEGER
      },
      title: {
        type: Sequelize.STRING,
        allowNull: false
      },
      content: {
        type: Sequelize.STRING(10000),
        allowNull: false
      },
      createdAt: {
        allowNull: false,
        type: Sequelize.DATE
      },
      updatedAt: {
        allowNull: false,
        type: Sequelize.DATE
      }
    });
  },
  down: (queryInterface, Sequelize) =&gt; {
    return queryInterface.dropTable('Trees');
  }
};
Here is the model file:
'use strict';
module.exports = (sequelize, DataTypes) =&gt; {
  var Tree = sequelize.define('Tree', {
    title: {
      type: DataTypes.STRING,
      allowNull: false
    },
    content: {
      type: DataTypes.STRING(10000),
      allowNull: false
    }
  }, {});
  Tree.associate = function(models) {
    // associations can be defined here
  };
  return Tree;
};
Here is the code that accesses the database:
const setTree = (treeVal, callback) =&gt; {
  console.log(`this would be the part`);
  Tree.create({
    title: 'Tree',
    content: JSON.stringify(treeVal)
  })
  .then((treeStr) =&gt; {
    let primaryTopics = JSON.parse(treeStr.content);
    callback(null, primaryTopics);
  })
  .catch((err) =&gt; {
    callback(err);
  });
}
This is exported in the module.exports method:
  callTree(callback) {
    return Tree.findOne({
      where: {
        title: 'Tree'
      }
    })
    .then((treeStr) =&gt; {
      if (treeStr === null) {
        return callback(`not defined yet`);
      }
      let primaryTopics = treeStr.content;
      primaryTopics = JSON.parse(primaryTopics);
      callback(null, primaryTopics);
    })
    .catch((err) =&gt; {
      callback(err);
    });
  }
And I'm pulling this method for an integration test here (the PrimaryTopic table is in the same database, and I receive no errors trying to run it):
  beforeEach((done) =&gt; {
    this.primaryTopic;
    sequelize.sync({force: true}).then((res) =&gt; {
      PrimaryTopic.create({
        title: 'Title: Hello World',
        content: '&lt;p&gt;Content: Hello World&lt;/p&gt;'
      })
      .then((primaryTopic) =&gt; {
        this.primaryTopic = primaryTopic;
        treeQueries.buildTree((err, res) =&gt; {
          if (err) {
            console.error(err);
          }
        });
        done();
      })
      .catch((err) =&gt; {
        console.log(err);
        done();
      });
    });
  });
I've searched through all the code for possible errors, but haven't found anything yet. 
I can use psql to access the Trees table in the test database, though it is empty.
I can use the same code to insert a value into the Trees table in the development database with no issues.
Here is the error I receive when I try to run a test (using jasmine.js for testing):
{ SequelizeDatabaseError: relation ""Trees"" does not exist
    at Query.formatError (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:363:16)
    at query.catch.err (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/sequelize/lib/dialects/postgres/query.js:86:18)
    at tryCatcher (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/util.js:16:23)
    at Promise._settlePromiseFromHandler (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:512:31)
    at Promise._settlePromise (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:569:18)
    at Promise._settlePromise0 (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:614:10)
    at Promise._settlePromises (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/promise.js:689:18)
    at Async._drainQueue (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:133:16)
    at Async._drainQueues (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:143:10)
    at Immediate.Async.drainQueues [as _onImmediate] (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/bluebird/js/release/async.js:17:14)
    at runCallback (timers.js:756:18)
    at tryOnImmediate (timers.js:717:5)
    at processImmediate [as _immediateCallback] (timers.js:697:5)
  name: 'SequelizeDatabaseError',
  parent: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  original: 
   { error: relation ""Trees"" does not exist
    at Connection.parseE (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:553:11)
    at Connection.parseMessage (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:378:19)
    at Socket.&lt;anonymous&gt; (/home/siddhartha/Documents/01-Studio/01-Commercial-Public/01-Komodo/2018-resources/node_modules/pg/lib/connection.js:119:22)
    at Socket.emit (events.js:160:13)
    at addChunk (_stream_readable.js:269:12)
    at readableAddChunk (_stream_readable.js:256:11)
    at Socket.Readable.push (_stream_readable.js:213:10)
    at TCP.onread (net.js:599:20)
     name: 'error',
     length: 104,
     severity: 'ERROR',
     code: '42P01',
     detail: undefined,
     hint: undefined,
     position: '13',
     internalPosition: undefined,
     internalQuery: undefined,
     where: undefined,
     schema: undefined,
     table: undefined,
     column: undefined,
     dataType: undefined,
     constraint: undefined,
     file: 'parse_relation.c',
     line: '1160',
     routine: 'parserOpenTable',
     sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' },
  sql: 'INSERT INTO ""Trees"" (""id"",""title"",""content"",""createdAt"",""updatedAt"") VALUES (DEFAULT,\'Tree\',\'[{""title"":""Title: Hello World"",""id"":1,""secondaryTopics"":[]}]\',\'2018-08-14 06:38:37.243 +00:00\',\'2018-08-14 06:38:37.243 +00:00\') RETURNING *;' }
Here's a link to the full repository.
",<javascript><node.js><postgresql><sequelize.js>,8517,2,185,221,2,3,7,49,36600,,0,3,14,2018-08-14 6:42,2020-10-14 14:19,,792,,Basic,1
49592794,Postgres: Are There Downsides to Using a JSON Column vs. an integer[] Column?,"
TLDR: If I want to save arrays of integers in a Postgres table, are there any pros or cons to using an array column (integer[]) vs. using a JSON column (eg. does one perform better than the other)?
Backstory:
I'm using a PostgreSQL database, and Node/Knex to manage it.  Knex doesn't have any way of directly defining a PostgreSQL integer[] column type, so someone filed a Knex bug asking for it ... but one of the Knex devs closed the ticket, essentially saying that there was no need to support PostgreSQL array column types when anyone can instead use the JSON column type.
My question is, what downsides (if any) are there to using a JSON column type to hold a simple array of integers?  Are there any benefits, such as improved performance, to using a true array column, or am I equally well off by just storing my arrays inside a JSON column?
EDIT: Just to be clear, all I'm looking for in an answer is either of the following:
A) an explanation of how JSON columns and integer[] columns in PostgreSQL work, including either how one is better than the other or how the two are (at least roughly) equal.
B) no explanation, but at least a reference to some benchmarks that show that one column type or the other performs better (or that the two are equal)
",<postgresql><knex.js>,1261,0,2,34011,31,163,237,52,4026,0,2733,1,14,2018-03-31 21:55,2018-04-01 16:37,2018-04-01 16:37,1,1,Intermediate,23
63354909,Is it possible to use Traefik to proxy PostgreSQL over SSL?,"Motivations
I am a running into an issue when trying to proxy PostgreSQL with Traefik over SSL using Let's Encrypt.
I did some research but it is not well documented and I would like to confirm my observations and leave a record to everyone who faces this situation.
Configuration
I use latest versions of PostgreSQL v12 and Traefik v2. I want to build a pure TCP flow from tcp://example.com:5432 -&gt; tcp://postgresql:5432 over TLS using Let's Encrypt.
Traefik service is configured as follow:
  version: &quot;3.6&quot;
    services:
      traefik:
        image: traefik:latest
        restart: unless-stopped
        volumes:
          - &quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot;
          - &quot;./configuration/traefik.toml:/etc/traefik/traefik.toml:ro&quot;
          - &quot;./configuration/dynamic_conf.toml:/etc/traefik/dynamic_conf.toml&quot;
          - &quot;./letsencrypt/acme.json:/acme.json&quot;
        networks:
          - backend
        ports:
          - &quot;80:80&quot;
          - &quot;443:443&quot;
          - &quot;5432:5432&quot;
    networks:
      backend:
        external: true
With the static setup:
[entryPoints]
  [entryPoints.web]
    address = &quot;:80&quot;
    [entryPoints.web.http]
      [entryPoints.web.http.redirections.entryPoint]
        to = &quot;websecure&quot;
        scheme = &quot;https&quot;
  [entryPoints.websecure]
    address = &quot;:443&quot;
    [entryPoints.websecure.http]
      [entryPoints.websecure.http.tls]
        certresolver = &quot;lets&quot;
  [entryPoints.postgres]
    address = &quot;:5432&quot;
PostgreSQL service is configured as follow:
version: &quot;3.6&quot;
services:
  postgresql:
    image: postgres:latest
    environment:
      - POSTGRES_PASSWORD=secret
    volumes:
      - ./configuration/trial_config.conf:/etc/postgresql/postgresql.conf:ro
      - ./configuration/trial_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./configuration/initdb:/docker-entrypoint-initdb.d
      - postgresql-data:/var/lib/postgresql/data
    networks:
      - backend
    #ports:
    #  - 5432:5432
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`example.com`)&quot;
      - &quot;traefic.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
networks:
  backend:
    external: true
volumes:
  postgresql-data:
It seems my Traefik configuration is correct. Everything is OK in the logs and all sections in dashboard are flagged as Success (no Warnings, no Errors). So I am confident with the Traefik configuration above. The complete flow is about:
EntryPoint(':5432') -&gt; HostSNI(`example.com`) -&gt; TcpRouter(`postgres`) -&gt; Service(`postgres@docker`)
But, it may have a limitation at PostgreSQL side.
Debug
The problem is that I cannot connect the PostgreSQL database. I always get a Timeout error.
I have checked PostgreSQL is listening properly (main cause of Timeout error):
# - Connection Settings -
listen_addresses = '*'
port = 5432
And I checked that I can connect PostgreSQL on the host (outside the container):
psql --host 172.19.0.4 -U postgres
Password for user postgres:
psql (12.2 (Ubuntu 12.2-4), server 12.3 (Debian 12.3-1.pgdg100+1))
Type &quot;help&quot; for help.
postgres=#
Thus I know PostgreSQL is listening outside its container, so Traefik should be able to bind the flow.
I also have checked external traefik can reach the server:
sudo tcpdump -i ens3 port 5432
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens3, link-type EN10MB (Ethernet), capture size 262144 bytes
09:02:37.878614 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [S], seq 1027429527, win 64240, options [mss 1452,nop,wscale 8,nop,nop,sackOK], length 0
09:02:37.879858 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [S.], seq 3545496818, ack 1027429528, win 64240, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
09:02:37.922591 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [.], ack 1, win 516, length 0
09:02:37.922718 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [P.], seq 1:9, ack 1, win 516, length 8
09:02:37.922750 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [.], ack 9, win 502, length 0
09:02:47.908808 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [F.], seq 9, ack 1, win 516, length 0
09:02:47.909578 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [P.], seq 1:104, ack 10, win 502, length 103
09:02:47.909754 IP example.com.postgresql &gt; x.y-z-w.isp.com.61229: Flags [F.], seq 104, ack 10, win 502, length 0
09:02:47.961826 IP x.y-z-w.isp.com.61229 &gt; example.com.postgresql: Flags [R.], seq 10, ack 104, win 0, length 0
So, I am wondering why the connection cannot succeed. Something must be wrong between Traefik and PostgreSQL.
SNI incompatibility?
Even when I remove the TLS configuration, the problem is still there, so I don't expect the TLS to be the origin of this problem.
Then I searched and I found few posts relating similar issue:
Introducing SNI in TLS handshake for SSL connections
Traefik 2.0 TCP routing for multiple DBs;
As far as I understand it, the SSL protocol of PostgreSQL is a custom one and does not support SNI for now and might never support it. If it is correct, it will confirm that Traefik cannot proxy PostgreSQL for now and this is a limitation.
By writing this post I would like to confirm my observations and at the same time leave a visible record on Stack Overflow to anyone who faces the same problem and seek for help. My question is then: Is it possible to use Traefik to proxy PostgreSQL?
Update
Intersting observation, if using HostSNI('*') and Let's Encrypt:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Everything is flagged as success in Dashboard but of course Let's Encrypt cannot perform the DNS Challenge for wildcard *, it complaints in logs:
time=&quot;2020-08-12T10:25:22Z&quot; level=error msg=&quot;Unable to obtain ACME certificate for domains \&quot;*\&quot;: unable to generate a wildcard certificate in ACME provider for domain \&quot;*\&quot; : ACME needs a DNSChallenge&quot; providerName=lets.acme routerName=postgres@docker rule=&quot;HostSNI(`*`)&quot;
When I try the following configuration:
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.routers.postgres.tls=true&quot;
      - &quot;traefik.tcp.routers.postgres.tls.domains[0].main=example.com&quot;
      - &quot;traefik.tcp.routers.postgres.tls.certresolver=lets&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
The error vanishes from logs and in both setups the dashboard seems ok but traffic is not routed to PostgreSQL (time out). Anyway, removing SSL from the configuration makes the flow complete (and unsecure):
    labels:
      - &quot;traefik.enable=true&quot;
      - &quot;traefik.docker.network=backend&quot;
      - &quot;traefik.tcp.routers.postgres.entrypoints=postgres&quot;
      - &quot;traefik.tcp.routers.postgres.rule=HostSNI(`*`)&quot;
      - &quot;traefik.tcp.services.postgres.loadBalancer.server.port=5432&quot;
Then it is possible to connect PostgreSQL database:
time=&quot;2020-08-12T10:30:52Z&quot; level=debug msg=&quot;Handling connection from x.y.z.w:58389&quot;
",<postgresql><tcp><proxy><traefik><sni>,8147,3,122,7550,3,39,59,73,10855,0,2039,3,14,2020-08-11 9:00,2020-08-12 7:16,2023-01-13 8:21,1,885,Basic,9
55485858,Using SQLite3 with Django 2.2 and Python 3.6.7 on Centos7,"I am moving my Django code from 2.1.7 directly to the new Django 2.2.  The only problem I encountered in my Centos7 development environment was that my local development database (sqlite3) version was incompatible using my Python 3.6.7.
The error I was getting from ""manage.py runserver"" was:
django.core.exceptions.ImproperlyConfigured: SQLite 3.8.3 or later
I am unable to use another version of Python because this is the maximum supported by AWS elasticbeanstalk.  The Python 3.6.7 seems to come with sqlite module of version:
&gt;&gt;&gt; import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.7.17'
&gt;&gt;&gt; 
I use a seperate development account on my local Centos7 workstation and issue pipenv shell to begin my code development and IDE.  
The only workaround I've found is to manually download SQLite3 autoconf version 3.27.2 and manually compile into that development account home folder using the following commands: 
wget https://www.sqlite.org/2019/sqlite-autoconf-3270200.tar.gz
gzip -d sqlite-autoconf-3270200.tar.gz
tar -xvf sqlite-autoconf-3270200.tar
cd sqlite-autoconf-3270200/
./configure --prefix=/home/devuser/opt/
make
make install
Following that, I have modified my .bashrc to reflect the following: 
export LD_LIBRARY_PATH=""${HOME}/opt/lib""
This seems to do the trick when I log back into my devuser account.  My app seems to run correctly using my local development database.
Python 3.6.7 (default, Dec  5 2018, 15:02:05)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux
&gt;&gt;&gt;import sqlite3
&gt;&gt;&gt; sqlite3.version
'2.6.0'
&gt;&gt;&gt; sqlite3.sqlite_version
'3.27.2'
My local development database is SQLite, but my settings.py does not load any SQLite3 database backend when it senses it's in production on AWS (uses Mysql production database as backend when environment variable flag PRODUCTION is checked).
Is my understanding of the problem correct and is my approach and implementation acceptable? 
I felt that recompiling python was a huge waste of time, and to be honest it may have been faster to stand up a local mysql version and stop wasting time with sqlite... but it's so nice to just copy or dump a file, migrate, and loaddata for a fresh start.
",<django><sqlite><python-3.6><pipenv><django-2.2>,2237,1,22,153,0,3,12,56,7109,0,28,1,14,2019-04-03 1:45,2019-06-14 8:47,,72,,Basic,9
51968981,"Merging duplicated records together with ""Merge"" syntax","I am using SQL Server 2014. I am currently trying to combine millions of personnel application records in to a single personnel record.  
The records contain the following columns:
ID, First_Name, Last_Name, DOB, Post_Code, Mobile, Email
A person can enter their details numerous times but due to fat fingers or fraud they can sometimes put in, incorrect details.  
In my example Christopher has filled his details in 5 times, First_Name, Last_Name, DOB are always correct, Post_Code, Mobile and Email contain various connotations.  
What I want to do is take the min(id) associated with this group in this case 84015283 and put it in to a new table, this will be the primary key and then you will see the other id's that are associated with it.
Examples
NID       CID
------------------
84015283  84015283
84015283  84069198
84015283  84070263
84015283  84369603
84015283  85061159
Where it gets a little complicated is, where 2 different people can have the same First_Name, Last_Name and DOB, at least one of the other fields must match ""post_code, mobile or email"" as per my example to another record within the group.
Though first_name, last_name, DoB match between ID's 84015283, 84069198, 84070263. 84015283, 84069198 are identical so they would match without an issue, 84070263 matches on the postcode, 84369603 matches on the mobile to a previous record and 85061159 matches on a previous mobile/email but not post_code.
If putting the NID within the original dataset is easier I can go with this rather than putting it all in a separate table.
After some googling and trying to get my head around this, I believe that using ""Merge"" might be a good way to achieve what I am after but I am concerned it will take a very long time due to the number of records involved.
Also going forward any routine would have to be run on subsequent new records.
I have listed the code for the example if anyone can help
DROP TABLE customer_dist
CREATE TABLE [dbo].customer_dist
(
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL,
)
INSERT INTO customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('84015283', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84069198', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559829', 'CH@hotmail.com'),
       ('84070263', 'Christopher', 'Higg', '1956-01-13', 'CH2 3AZ', '07089559822', 'CHigg@AOL.com'),
       ('84369603', 'Christopher', 'Higg', '1956-01-13', 'CH2 3ZA', '07089559829', 'Higg@emailme.com'),
       ('85061159', 'CHRISTOPHER', 'Higg', '1956-01-13', 'CH2 3RA', '07089559829', 'CH@hotmail.com'),
       ('87065122', 'Matthew', 'Davis', '1978-05-10', 'CH5 1TS', '07077084692', 'Matt@gamil.com')
SELECT * FROM customer_dist
Below is the expected results, sorry I should of made it clearer what I wanted at the end.
Output Table Results
    NID         id          First_Name  Last_Name   DoB         post_code   mobile          Email
    84015283    84015283    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84069198    Christopher Higg            1/13/1956   CH2 3AZ         7089559829  CH@hotmail.com
    84015283    84070263    Christopher Higg            1/13/1956   CH2 3AZ         7089559822  CHigg@AOL.com
    84015283    84369603    Christopher Higg            1/13/1956   CH2 3ZA         7089559829  Higg@emailme.com
    84015283    85061159    CHRISTOPHER Higg            1/13/1956   CH2 3RA         7089559829  CH@hotmail.com
    78065122    87065122    Matthew Davis               05/10/1978  CH5 1TS
7077084692  Matt@gamil.com
OR                          
NID         id
84015283    84015283
84015283    84069198
84015283    84070263
84015283    84369603
84015283    85061159
87065122    87065122
Apologies for the slow response.
I have updated my required output, I was asked to include an extra record that was not a match to the other records but did not include this in my required output.
HABO's response was the closest to what was needed unfortunately on further testing with other sample data, duplicates were created and the logic broke down.  Other Sample data would be :-
declare @customer_dist as Table (
    [id] [int] NOT NULL,
    [First_Name] [varchar](50) NULL,
    [Last_Name] [varchar](50) NULL,
    [DoB] [date] NULL,
    [post_code] [varchar](50) NULL,
    [mobile] [varchar](50) NULL,
    [Email] [varchar](100) NULL );
INSERT INTO @customer_dist (id, First_Name, Last_Name, DoB, post_code, mobile, Email)
VALUES ('32006455', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07706212920',  'nastie220@yahoo.com'),
       ('35963960', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863324',  'nastie@hotmail.com'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com'),
       ('46653041', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie2010@yahoo.com'),
       ('48023677', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07483888179',  'nastie@hotmail.com'),
       ('49560434', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie@hotmail.com'),
       ('49861032', 'Mary', 'WILSON',   '1983-09-20',   'BT62JA',   '07849727199',  'nastie2001@yahoo.com'),
       ('53130969', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07849727199',  'Nastie@hotmail.cm'),
       ('33843283', 'Mary', 'Wilson',   '1983-09-20',   'BT148HU',  '07484863478',  'nastie2010@yahoo.co.uk'),
       ('38627975', 'Mary', 'Wilson',   '1983-09-20',   'BT62JA',   '07484863478',  'nastie2001@yahoo.com')
SELECT * FROM @customer_dist;
",<sql><sql-server><t-sql><sql-server-2014>,5802,1,84,743,2,13,38,58,976,,8,9,14,2018-08-22 14:20,2018-08-22 15:05,,0,,Basic,10
55006231,Query to get nextval from sequence with Spring JPA,"I have a repository with which I am trying to get next value from sequence. The sequence name I need to be parameterized.
The repository query looks a like:
  @Query(value = ""SELECT ?1.NEXTVAL from dual;"", nativeQuery = true)
  String getSeqID(@Param(""seqName"") String seqName);
But, I am getting following exception:
org.hibernate.QueryException: JPA-style positional param was not an integral ordinal
    at org.hibernate.engine.query.spi.ParameterParser.parse(ParameterParser.java:187) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.ParamLocationRecognizer.parseLocations(ParamLocationRecognizer.java:59) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.internal.NativeQueryInterpreterStandardImpl.getParameterMetadata(NativeQueryInterpreterStandardImpl.java:34) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
    at org.hibernate.engine.query.spi.QueryPlanCache.getSQLParameterMetadata(QueryPlanCache.java:125) ~[hibernate-core-5.0.12.Final.jar:5.0.12.Final]
",<java><sql><spring><jpa><spring-data-jpa>,1036,0,5,486,2,8,23,76,60988,0,13,5,14,2019-03-05 15:28,2019-03-05 15:40,2020-04-01 20:04,0,393,Basic,10
54518722,MySQL Connector could not process parameters,"I'm trying to loop through an array and insert each element into a table.  As far as I can see my syntax is correct and I took this code straight from Microsoft Azure's documentation.  
try:
   conn = mysql.connector.connect(**config)
   print(""Connection established"")
except mysql.connector.Error as err:
  if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
    print(""Something is wrong with the user name or password"")
  elif err.errno == errorcode.ER_BAD_DB_ERROR:
    print(""Database does not exist"")
  else:
    print(err)
else:
  cursor = conn.cursor()
data = ['1','2','3','4','5']
for x in data:
   cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(x))
   print(""Inserted"",cursor.rowcount,""row(s) of data."")
conn.commit()
cursor.close()
conn.close()
print(""Done."")
When I run this is gets to cursor.execute(...) and then fails.  Here is the stack trace.
  Traceback (most recent call last):
    File ""test.py"", line 29, in 
      cursor.execute(""INSERT INTO test (serial) VALUES (%s)"",(""test""))
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\cursor_cext.py"", line 248, in execute
      prepared = self._cnx.prepare_for_mysql(params)
    File ""C:\Users\AlexJ\AppData\Local\Programs\Python\Python37\lib\site-packages\mysql\connector\connection_cext.py"", line 538, in prepare_for_mysql
      raise ValueError(""Could not process parameters"")
  ValueError: Could not process parameters
",<python><mysql><mysql-connector-python>,1446,1,24,514,2,7,21,43,39576,,29,3,14,2019-02-04 14:52,2019-02-04 15:03,2019-02-04 15:03,0,0,Basic,14
61081569,No exception being thrown when opening MySqlConnection?,"I'm just starting out with async and Task's and my code has stopped processing. It happens when I have an incoming network packet and I try and communicate with the database inside the packet handler.
public class ClientConnectedPacket : IClientPacket
{
    private readonly EntityFactory _entityFactory;
    public ClientConnectedPacket(EntityFactory entityFactory)
    {
        _entityFactory= entityFactory;
    }
    public async Task Handle(NetworkClient client, ClientPacketReader reader)
    {
        client.Entity = await _entityFactory.CreateInstanceAsync( reader.GetValueByKey(""unique_device_id""));
        // this Console.WriteLine never gets reached
        Console.WriteLine($""Client [{reader.GetValueByKey(""unique_device_id"")}] has connected"");
    }
}
The Handle method gets called from an async task
if (_packetRepository.TryGetPacketByName(packetName, out var packet))
{
    await packet.Handle(this, new ClientPacketReader(packetName, packetData));
}
else
{
    Console.WriteLine(""Unknown packet: "" + packetName);
}
Here is the method which I think is causing the issue
public async Task&lt;Entity&gt; CreateInstanceAsync(string uniqueId)
{
    await using (var dbConnection = _databaseProvider.GetConnection())
    { 
        dbConnection.SetQuery(""SELECT COUNT(NULL) FROM `entities` WHERE `unique_id` = @uniqueId"");
        dbConnection.AddParameter(""uniqueId"", uniqueId);
        var row = await dbConnection.ExecuteRowAsync();
        if (row != null)
        {
            return new Entity(uniqueId, false);
        }
    }
    return new Entity(uniqueId,true);
}
DatabaseProvider's GetConnection method:
public DatabaseConnection GetConnection()
{
    var connection = new MySqlConnection(_connectionString);
    var command = connection.CreateCommand();
    return new DatabaseConnection(_logFactory.GetLogger(), connection, command);
}
DatabaseConnection's constructor:
public DatabaseConnection(ILogger logger, MySqlConnection connection, MySqlCommand command)
{
    _logger = logger;
    _connection = connection;    
    _command = command;
    _connection.Open();
}
When I comment out this line, it reaches the Console.WriteLine
_connection.Open();
",<c#><mysql>,2182,0,60,1,8,31,101,36,340,,81,2,14,2020-04-07 13:49,2020-04-12 18:20,,5,,Basic,13
54714594,Google CloudSQLAdmin - The service account does not have the required permissions for the bucket,"I am writing a python function which uses service account credentials to call the Google cloudSQLAdmin api to export a database to a bucket. 
The service account has been given project owner permissions, and the bucket has permissions set for project owners. The sqlAdmin api has been enabled for our project. 
Python code:
from google.oauth2 import service_account
from googleapiclient.discovery import build
import googleapiclient
import json
def main():
    SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin', 'https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/devstorage.full_control']
    SERVICE_ACCOUNT_FILE = './creds/service-account-credentials.json'
    PROJECT = ""[REDACTED]""
    DB_INSTANCE = ""[REDACTED]""
    BUCKET_PATH = ""gs://[REDACTED]/[REDACTED].sql""
    DATABASES = [REDACTED]
    BODY = { # Database instance export request.
    ""exportContext"": { # Database instance export context. # Contains details about the export operation.
      ""kind"": ""sql#exportContext"", # This is always sql#exportContext.
      ""fileType"": ""SQL"", # The file type for the specified uri.
          # SQL: The file contains SQL statements.
          # CSV: The file contains CSV data.
      ""uri"": BUCKET_PATH, # The path to the file in Google Cloud Storage where the export will be stored. The URI is in the form gs://bucketName/fileName. If the file already exists, the requests succeeds, but the operation fails. If fileType is SQL and the filename ends with .gz, the contents are compressed.
      ""databases"": DATABASES,
    },
  }
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    sqladmin = googleapiclient.discovery.build('sqladmin', 'v1beta4', credentials=credentials)
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
    print(json.dumps(response, sort_keys=True, indent=4))
Running this code nets the following error:
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""[REDACTED]/main.py1"", line 47, in hello_pubsub
    response = sqladmin.instances().export(project=PROJECT, instance=DB_INSTANCE, body=BODY).execute()
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://www.googleapis.com/sql/v1beta4/projects/[REDACTED]/instances/[REDACTED]/export?alt=json returned ""The service account does not have the required permissions for the bucket.""&gt;
I have tried this across 2 GCP projects, with multiple service accounts with varying permissions. 
Related questions:
Access denied for service account (permission issue?) when importing a csv from cloud storage to cloud sql - This issue was caused by incorrect permissions, which shouldn't be the case here as the account has project owner permissions
",<python><google-api><google-cloud-platform><google-oauth><google-cloud-sql>,3123,5,36,141,1,1,4,68,12673,0,0,3,14,2019-02-15 17:49,2019-03-29 10:31,,42,,Basic,13
48239668,Fails to initialize MySQL database on Windows 10,"Using Laradock
System Info:
Docker version: 17.10.0-ce, build f4ffd25
OS: Windows 10 Home
When I run docker-compose up -d mysql I'm getting error. Following is the docker logs
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[ERROR] --initialize specified but the data directory has files in it. Aborting.
[ERROR] Aborting
I have tried deleting mysql folder under ~/.laradock\data and didn't work.
Update 1
MySQL Container under laradock Dockerfile
mysql:
  build:
    context: ./mysql
    args:
      - MYSQL_VERSION=${MYSQL_VERSION}
  environment:
    - MYSQL_DATABASE=${MYSQL_DATABASE}
    - MYSQL_USER=${MYSQL_USER}
    - MYSQL_PASSWORD=${MYSQL_PASSWORD}
    - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    - TZ=${WORKSPACE_TIMEZONE}
  volumes:
    - ${DATA_SAVE_PATH}/mysql:/var/lib/mysql
    - ${MYSQL_ENTRYPOINT_INITDB}:/docker-entrypoint-initdb.d
  ports:
    - &quot;${MYSQL_PORT}:3306&quot;
  networks:
    - backend
MySQL Dockerfile
ARG MYSQL_VERSION=8.0
FROM mysql:${MYSQL_VERSION}
MAINTAINER Mahmoud Zalt &lt;mahmoud@zalt.me&gt;
#####################################
# Set Timezone
#####################################
ARG TZ=UTC
ENV TZ ${TZ}
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone
RUN chown -R mysql:root /var/lib/mysql/
ADD my.cnf /etc/mysql/conf.d/my.cnf
CMD [&quot;mysqld&quot;]
EXPOSE 3306
Update 2
After I delete mysql folder under ~/.laradock/data I'm getting following error. After the command it generates the files in below image. When I rerun giving back the previous error mentioned above.
[Note] Basedir set to /usr/
[Warning] The syntax '--symbolic-links/-s' is deprecated and will be removed in a future release
[Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.
[Warning] Setting lower_case_table_names=2 because file system for /var/lib/mysql/ is case insensitive
[Warning] You need to use --log-bin to make --log-slave-updates work.
libnuma: Warning: /sys not mounted or invalid. Assuming one node: No such file or directory
mbind: Operation not permitted
[ERROR]
InnoDB: Operating system error number 22 in a file operation.
[ERROR] InnoDB: Error number 22 means
'Invalid argument'
[ERROR] InnoDB: File
./ib_logfile101: 'aio write' returned OS error 122. Cannot continue
operation
[ERROR] InnoDB: Cannot
continue operation.
** I tried in a windows 7 machine and its working.
",<mysql><docker><laradock>,2741,2,43,8479,4,43,70,49,20058,0,527,4,14,2018-01-13 12:02,2018-01-13 12:18,2018-02-05 21:38,0,23,Basic,14
64068518,Postgres race condition involving subselect and foreign key,"We have 2 tables defined as follows
CREATE TABLE foo (
  id BIGSERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);
CREATE TABLE bar (
  foo_id BIGINT UNIQUE,
  foo_name TEXT NOT NULL UNIQUE REFERENCES foo (name)
);
I've noticed that when executing the following two queries concurrently
INSERT INTO foo (name) VALUES ('BAZ')
INSERT INTO bar (foo_name, foo_id) VALUES ('BAZ', (SELECT id FROM foo WHERE name = 'BAZ'))
it is possible under certain circumstances to end up inserting a row into bar where foo_id is NULL. The two queries are executed in different transactions, by two completely different processes.
How is this possible? I'd expect the second statement to either fail due to a foreign key violation (if the record in foo is not there), or succeed with a non-null value of foo_id (if it is).
What is causing this race condition? Is it due to the subselect, or is it due to the timing of when the foreign key constraint is checked?
We are using isolation level &quot;read committed&quot; and postgres version 10.3.
EDIT
I think the question was not particularly clear on what is confusing me. The question is about how and why 2 different states of the database were being observed during the execution of a single statement. The subselect is observing that the record in foo as being absent, whereas the fk check sees it as present. If it's just that there's no rule preventing this race condition, then this is an interesting question in itself - why would it not be possible to use transaction ids to ensure that the same state of the database is observed for both?
",<sql><postgresql><concurrency><foreign-keys><subquery>,1578,0,16,522,0,6,23,44,935,0,136,5,14,2020-09-25 17:08,2020-09-25 19:10,2020-09-25 19:10,0,0,Advanced,32
51294893,How to connect Ms SQL from a Flutter App?,"Some context: The Db already exists and the app is for internal use of the company, that's why I'm not doing an API.
I need to connect my app to an SQL server to execute a query and retreive data from it.
I've already tried with this plugin but no succes SqlJocky5
Someone have done something similar already with flutter? How you did it? there's another library for connecting the app with a sql server?
So, What I'm looking for is if there's a library to do it like in Xamarin Forms (SqlClient) or in Android Studio Java (JDBC Driver).
",<android><sql><dart><flutter>,538,1,0,1116,1,10,26,48,47670,0,21,1,14,2018-07-11 22:14,2018-07-16 19:06,2018-07-16 19:06,5,5,Intermediate,20
49554728,Jenkins Job - DatabaseError: file is encrypted or is not a database,"When running this code for connecting to a db through cmd - locally and on the actual server it works fine. But I have set it up on Jenkins and receive the error: 
DatabaseError: file is encrypted or is not a database
It seems to be happening on this line:
  self.cursor.execute(*args)
The database class is:
class DatabaseManager(object):
    def __init__(self, db):
        self.conn = sqlite3.connect(db)
        self.cursor = self.conn.cursor()
    def query(self, *args):
        self.cursor.execute(*args)
        self.conn.commit()
        return self.cursor
    def __del__(self):
        self.conn.close()
",<python><jenkins><sqlite>,615,0,14,1207,4,27,50,47,442,0,11,3,14,2018-03-29 11:18,2018-03-29 11:23,,0,,Basic,14
62324520,How to use SSH Tunnel to connect to an RDS instance via an EC2 instance?,"So this is really new to me, so apologies if this is a dumb question.
I have a RDS instance that is not publicly accessible and is sitting in its own private VPC. I have an EC2 instance that is allowed to connect to RDS, but nothing else is allowed to connect to the instance.
I now want PgAdmin to be able to show data from my RDS instance.
I went through the wizard in PgAdmin, I put in the EC2 Instance's Public IP as Tunnel host, the username is ec2-user and the authentication is by identity file (using the pem file that I use to ssh into the instance).
However, I still can't connect. In the Advanced tab, PGAdmin asks for a Host address, but complains when I put in my RDS instance's endpoint.
How do I get my local pgAdmin to now access my DB which is no longer accessible to the public internet?
--- forgot to add the error message
Unable to connect to server:
Failed to create the SSH tunnel.
Error: Could not establish session to SSH gateway
",<postgresql><amazon-web-services><amazon-rds><pgadmin>,954,0,5,7366,32,91,157,76,24188,0,22,2,14,2020-06-11 12:33,2020-06-12 0:32,,1,,Intermediate,20
58378708,SQLAlchemy: Can't reconnect until invalid transaction is rolled back,"I have a weird problem.
I have a simple py3 app, which uses sqlalchemy.
But several hours later, there is an error:
  (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back
My init part:
self.db_engine = create_engine(self.db_config, pool_pre_ping=True) # echo=True if needed to see background SQL
Session = sessionmaker(bind=self.db_engine)
self.db_session = Session()
The query (this is the only query that happens):
while True:
    device_id = self.db_session.query(Device).filter(Device.owned_by == msg['user_id']).first()
    sleep(20)
The whole script is in infinite loop, single threaded (SQS reading out). Does anybody cope with this problem?
",<python><python-3.x><sqlalchemy>,690,0,6,423,1,3,13,47,19864,,9,1,13,2019-10-14 14:17,2019-11-22 10:01,2019-11-22 10:01,39,39,Basic,13
59034719,Spark: Prevent shuffle/exchange when joining two identically partitioned dataframes,"I have two dataframes df1 and df2 and I want to join these tables many times on a high cardinality field called visitor_id.  I would like to perform only one initial shuffle and have all the joins take place without shuffling/exchanging data between spark executors.  
To do so, I have created another column called visitor_partition that consistently assigns each visitor_id a random value between [0, 1000).  I have used a custom partitioner to ensure that the df1 and df2 are exactly partitioned such that each partition contains exclusively rows from one value of visitor_partition.  This initial repartition is the only time I want to shuffle the data.
I have saved each dataframe to parquet in s3, paritioning by visitor partition -- for each data frame, this creates 1000 files organized in df1/visitor_partition=0, df1/visitor_partition=1...df1/visitor_partition=999.
Now I load each dataframe from the parquet and register them as tempviews via df1.createOrReplaceTempView('df1') (and the same thing for df2) and then run the following query
SELECT
   ...
FROM
  df1 FULL JOIN df1 ON
    df1.visitor_partition = df2.visitor_partition AND
    df1.visitor_id = df2.visitor_id
In theory, the query execution planner should realize that no shuffling is necessary here.  E.g., a single executor could load in data from df1/visitor_partition=1 and df2/visitor_partition=2 and join the rows in there.  However, in practice spark 2.4.4's query planner performs a full data shuffle here.
Is there some way I can prevent this shuffle from taking place?
",<apache-spark><join><pyspark><apache-spark-sql>,1552,0,20,13135,18,58,93,39,3962,0,729,1,13,2019-11-25 15:05,2019-11-25 17:16,2019-11-25 17:16,0,0,Intermediate,23
49152718,DISTINCT ON() in jOOQ,"I would like to make a query in PostgreSQL
select 
  distinct on(uuid) 
  (select nr_zew from bo_get_sip_cti_polaczenie_info(uuid)) as nr_zew 
from bo_sip_cti_event_day
where data_ins::date = current_date
and kierunek like 'P'
and (hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') 
in Java as far I have
Result&lt;Record&gt; result = create
    .select()
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
and it works but as I try to add
Result&lt;Record&gt; result = create
    .selectDistinct(""uuid"")
    .from(""bo_sip_cti_event_day"")
    .where(""data_ins::date = current_date"")
    .and(""kierunek like 'P'"")
    .and(""(hangup_cause like 'NO_ANSWER' or hangup_cause like 'NO_USER_RESPONSE') "")
    .fetch();
then it says that cannot do selectDistinct(String). How can I use distinct in jOOQ?
",<java><postgresql><jooq>,973,0,22,5196,7,50,85,52,9395,0,454,1,13,2018-03-07 13:06,2018-03-08 8:27,2018-03-08 8:27,1,1,Basic,3
55536681,What happens with returning IEnumerable if used with async/await (streaming data from SQL Server with Dapper)?,"I am using Dapper to stream data from a very large set in SQL Server. It works fine with returning IEnumerable and calling Query(), but when I switch to QueryAsync(), it seems that the program tries to read all of the data from SQL Server instead of streaming.
According to this question, it should work fine with buffered: false, which I am doing, but the question says nothing about async/await.
Now according to this question, it's not straightforward to do what I want with QueryAsync().
Do I understand correctly that enumerables are iterated when the context is switched for async/await?
Another question if this is something that will be possible to do when the new C#8 async streaming is available?
",<c#><sql-server><async-await><dapper><c#-8.0>,707,2,7,28437,27,124,212,35,4718,0,3887,3,13,2019-04-05 13:38,2019-04-05 14:04,2019-04-05 14:04,0,0,Advanced,33
55395326,How to debug T-SQL with SQL Server Management Studio 2017?,"The latest changelog (18.0 Preview 7) of SQL Server Management Studio announced, that the T-SQL Debugger is deprecated.
What are the alternatives for the future? Can someone understand this decision? I fear that removing a fundamental development tool like this will effect many developers.
",<sql-server><t-sql><debugging><ssms><ssms-2017>,291,2,0,9394,10,64,93,65,12881,,1122,3,13,2019-03-28 10:28,2019-05-22 1:08,2019-05-22 1:08,55,55,Intermediate,21
52426656,Track last modification timestamp of a row in Postgres,"In Postgres I want to store table's last update/insert time. Microsoft SQL Server offers a type timestamp which is automatically maintained by the database. 
But timestamp in Postgres works differently, it is not updated automatically and the column is always null.
",<postgresql><timestamp>,266,0,2,135,1,1,7,35,20813,0,6,2,13,2018-09-20 13:47,2019-06-06 15:40,2019-06-06 15:40,259,259,Basic,9
60384146,Capture the user who deleted the row in Temporal table,"I understand that temporal tables are intended to give you a point in time view of the data. I am using temporal tables for auditing purpose. I have the following Temporal table.
Lets assume this is the current state of the Temporal table:
ID  RoleID  UserID      ModifiedBy
------------------------------------------
1   11      1001        foo@example.com
2   22      1001        foo@example.com
3   33      1002        bar@example.com
4   11      1003        foo@example.com
I have a web application using EF Core. My EF code always sets the ModifiedBy to currently logged in user. I logged into the application as bar@example.com and deleted a record with ID 2. SQL Server will automatically insert the deleted record into the history table as expected and keep ModifiedBy as foo@example.com because that was the point in time value of ModifiedBy column.
However now the system does not know who deleted the row. In this scenario bar@example.com is the one who actually deleted the row. How do I capture the user who deleted the record? What are my options here?
",<sql-server><t-sql><ef-core-2.2><temporal-tables>,1067,0,10,31334,54,220,415,77,1833,0,249,2,13,2020-02-24 21:27,2021-07-17 13:21,,509,,Basic,9
50809120,Postgres INSERT INTO with SELECT ordering,"When inserting into Postgres via a select statement, are the rows guaranteed to be inserted in the same order that the select statement returns them?
That is, given a table bar (where id is SERIAL PRIMARY KEY, and name is TEXT):
id | name
---+-----
 0 | A
 1 | B
 2 | C
And another table, foo (empty and with the same schema), if I INSERT INTO foo (name) SELECT name FROM bar ORDER BY id DESC will foo be guaranteed to have:
id | name
---+-----
 0 | C
 1 | B
 2 | A
This seems to be the case, but I'd like to confirm that it isn't an implementation detail that may not hold with larger selects.
I read through section 13.8 in the SQL-92 standard and general rule #3 claims that ""The query expression is effectively evaluated before inserting any rows into B."", but it doesn't explicitly say anything about ordering. Is the standard purposefully vague (perhaps to allow parallel insertions?) and ordering is an implementation detail?
",<sql><postgresql>,933,1,19,1555,1,10,15,64,6563,,19,2,13,2018-06-12 4:09,2018-06-12 5:11,,0,,Basic,9
53129719,Semantics of INSERT SELECT FOR UPDATE ON CONFLICT DO NOTHING RETURNING,"We have encountered a very peculiar issue with our production system. Unfortunately despite a lot of effort, I have not been able to reproduce the issue locally, so I cannot provide a minimal, complete and verifiable example. Also, as this is production code, I have had to change the names of the tables in the following example. However I believe I am presenting all the relevant facts.
We have four tables bucket_holder, bucket, item and bucket_total created as follows:
CREATE TABLE bucket_holder (
  id SERIAL PRIMARY KEY,
  bucket_holder_uid UUID NOT NULL
);
CREATE TABLE bucket ( 
  id SERIAL PRIMARY KEY, 
  bucket_uid UUID NOT NULL, 
  bucket_holder_id INTEGER NOT NULL REFERENCES bucket_holder (id), 
  default_bucket BOOLEAN NOT NULL
);
CREATE TABLE item ( 
  id SERIAL PRIMARY KEY, 
  item_uid UUID NOT NULL, 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
CREATE TABLE bucket_total ( 
  bucket_id INTEGER NOT NULL REFERENCES bucket (id), 
  amount NUMERIC NOT NULL 
);
There are also indexes on appropriate columns as follows:
CREATE UNIQUE INDEX idx1 ON bucket_holder (bucket_holder_uid);
CREATE UNIQUE INDEX idx2 ON bucket (bucket_uid);
CREATE UNIQUE INDEX idx3 ON item (item_uid);
CREATE UNIQUE INDEX idx4 ON bucket_total (bucket_id);
The idea is that a bucket_holder holds buckets, one of which is a default_bucket, buckets hold items and each bucket has a unique bucket_total record containing the sum of the amounts of all the items.
We are trying to do bulk inserts into the item table as follows:
WITH
unnested AS ( 
  SELECT * 
  FROM UNNEST(
    ARRAY['00000000-0000-0000-0000-00000000001a', '00000000-0000-0000-0000-00000000002a']::UUID[], 
    ARRAY['00000000-0000-0000-0000-00000000001c', '00000000-0000-0000-0000-00000000002c']::UUID[], 
    ARRAY[1.11, 2.22]::NUMERIC[]
  ) 
  AS T(bucket_holder_uid, item_uid, amount) 
), 
inserted_item AS ( 
  INSERT INTO item (bucket_id, item_uid, amount) 
  SELECT bucket.id, unnested.item_uid, unnested.amount 
  FROM unnested 
  JOIN bucket_holder ON unnested.bucket_holder_uid = bucket_holder.bucket_holder_uid 
  JOIN bucket ON bucket.bucket_holder_id = bucket_holder.id 
  JOIN bucket_total ON bucket_total.bucket_id = bucket.id 
  WHERE bucket.default_bucket 
  FOR UPDATE OF bucket_total 
  ON CONFLICT DO NOTHING 
  RETURNING bucket_id, amount 
), 
total_for_bucket AS ( 
  SELECT bucket_id, SUM(amount) AS total 
  FROM inserted_item 
  GROUP BY bucket_id 
) 
UPDATE bucket_total 
SET amount = amount + total_for_bucket.total 
FROM total_for_bucket 
WHERE bucket_total.bucket_id = total_for_bucket.bucket_id
In reality the arrays passed in are dynamic and have length up to 1000, but all 3 arrays have the same length. The arrays are always sorted so that the bucket_holder_uids are in order in order to ensure that deadlock cannot occur. The point of the ON CONFLICT DO NOTHING is that we should be able to handle the situation where some of the items were already present (the conflict is on item_uid). In this case the bucket_total should of course not be updated.
This query assumes that appropriate bucket_holder, bucket and bucket_total records already exist. It is ok for the query to fail otherwise as in practice this situation will not occur. Here is an example of setting up some sample data:
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000001a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000001b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000001a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000001b'), 0);
INSERT INTO bucket_holder (bucket_holder_uid) VALUES ('00000000-0000-0000-0000-00000000002a');
INSERT INTO bucket (bucket_uid, bucket_holder_id, default_bucket) VALUES ('00000000-0000-0000-0000-00000000002b', (SELECT id FROM bucket_holder WHERE bucket_holder_uid = '00000000-0000-0000-0000-00000000002a'), TRUE);
INSERT INTO bucket_total (bucket_id, amount) VALUES ((SELECT id FROM bucket WHERE bucket_uid = '00000000-0000-0000-0000-00000000002b'), 0);
This query appears to have done the correct thing for hundreds of thousands of items, but for a handful of items, the bucket_total has been updated by twice the amount of the item. I don't know if it's been updated twice or if it was updated once by twice the amount of the item. However in these cases, only one item has been inserted (inserting twice would be impossible anyway as there is a uniqueness constraint on item_uid). Our logs suggest that for the affected buckets, two threads were executing the query simultaneously. 
Can anyone see and explain any issue with this query and indicate how it could be rewritten?
We are using version PG9.6.6
UPDATE
We've spoken to a core postgres developer about this, who apparently doesn't see a concurrency issue here. We're now investigating really nasty possibilities such as index corruption, or the (remote) chance of a pg bug.
",<sql><postgresql><concurrency><common-table-expression>,5103,0,94,522,0,6,23,64,819,0,136,1,13,2018-11-03 8:40,2018-11-08 4:17,,5,,Basic,9
48249783,Search Query not accurate enough,"I have a search query done by me to the best of my knowledge in PHP but there are some improvements required:
When I search say 'what is food' and I have 'what is food' in the database all results containing one of the keywords 'what', 'is', 'food' are shown. The desired behaviour is to display results containing the exact phrase 'what is food' (first)
Only the last word in the query is highlighted and I want to highlight all words
Desired behaviour: The right answer shows at the top, regardless of its position in the database.
My current code is like this:
if (isset($_GET[""mainSearch""]))
{
  $condition = '';
  $mainSearch = SQLite3::escapeString($_GET['mainSearch']);
  $keyword = $_GET['mainSearch'];
  $query = explode("" "", $keyword);
  $perpageview=7;
  if ($_GET[""pageno""])
  {
      $page=$_GET[""pageno""];
  }
  else
  {
      $page=1;
  }
  $frompage = $page*$perpageview-$perpageview;
  foreach ($query as $text)
  {
      $condition .= ""question LIKE '%"".SQLite3::escapeString($text).""%' OR answer LIKE '%"".SQLite3::escapeString($text).""%' OR "";
  }
  foreach ($query as $text_2)
  {
      $condition_2 .= ""bname LIKE '%"".SQLite3::escapeString($text_2).""%' OR bankreq LIKE '%"".SQLite3::escapeString($text_2).""%' OR "";
  }
  $condition = substr($condition, 0, -4);
  $condition_2 = substr($condition_2, 0, -4);
  $order = "" ORDER BY quiz_id DESC "";
  $order_2 = "" ORDER BY id DESC "";
  $sql_query = ""SELECT * FROM questions WHERE "" . $condition . ' '. $order.' LIMIT '.$frompage.','.$perpageview;
  $sql_query_count = ""SELECT COUNT(*) as count FROM questions WHERE "" . $condition .' '. $order;
  //$mainAnswer = ""SELECT * FROM questions WHERE question LIKE '%$mainSearch%' or answer LIKE '%$mainSearch%'"";
  $bank_query = ""SELECT * FROM banks WHERE "" . $condition_2 . ' LIMIT 1';
  $result = $db-&gt;query($sql_query);
  $resultCount = $db-&gt;querySingle($sql_query_count);
  $bankret = $db-&gt;query($bank_query);
  //$mainAnsRet = $db-&gt;query($mainAnswer);
  $pagecount = ceil($resultCount/$perpageview);
  if ($resultCount &gt; 0)
  {
  if ($result &amp;&amp; $bankret)
  {
      while ($row = $result-&gt;fetchArray(SQLITE3_ASSOC))
      {
          $wording = str_replace($text, ""&lt;span style='font-weight: bold; color: #1a0dab;'&gt;"".$text.""&lt;/span&gt;"", $row['answer']);
           echo '&lt;div class=""quesbox_3""&gt;
            &lt;div class=""questitle""&gt;
                &lt;h2&gt;'.$row[""question""].'&lt;/h2&gt;
            &lt;/div&gt;
            &lt;div class=""quesanswer""&gt;'.$wording.'&lt;/div&gt;
        &lt;/div&gt;';
      }
      while ($brow = $bankret-&gt;fetchArray(SQLITE3_ASSOC))
      {
            $bname = $brow['bname'];
            $bankbrief = $brow['bankbrief'];
            $bankreq = $brow['bankreq'];
            $bankaddress = $brow['bankaddress'];
            $banklogo = $brow['banklogo'];
            $founded = $brow['founded'];
            $owner = $brow['owner'];
            $available = $brow['available'];
           echo '&lt;div class=""modulecontent""&gt;
            &lt;div class=""modulename""&gt;
                &lt;div class=""mname""&gt;'.$bname.'&lt;/div&gt;
                &lt;div class=""mlogo""&gt;&lt;img src=""'.$banklogo.'""&gt;&lt;/div&gt;
            &lt;/div&gt;';
            if (strlen($bankreq) &gt; 300)
            {
                $bankcut = substr($bankreq, 0, 300);
                $bankreq = substr($bankcut, 0, strrpos($bankcut, ' ')).'... &lt;a href=""bankprofile.php?bname='.$bname.'""&gt;Read More&lt;/a&gt;';
                echo '&lt;div class=""modulebrief""&gt;'.$bankreq.'&lt;/div&gt;';
            }
            echo '&lt;div class=""modulelinks""&gt;
                &lt;div class=""mfound""&gt;Founded: &lt;span&gt;'.$founded.'&lt;/span&gt;&lt;/div&gt;
                &lt;div class=""mowned""&gt;Ownd By: &lt;span&gt;'.$owner.'&lt;/span&gt;&lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;';
               // &lt;div class=""mavailable""&gt;Available for Export Loan: &lt;span&gt;'.$available.'&lt;/span&gt;&lt;/div&gt;
      }
      ?&gt;
      &lt;div class=""page_num""&gt;
      &lt;?php
      for ($i=1; $i &lt;= $pagecount; $i++) {
         echo '&lt;a href=""searchresult.php?mainSearch='.$mainSearch.'&amp;pageno='.$i.'""&gt;'.$i.'&lt;/a&gt;';
      }
      ?&gt;
      &lt;/div&gt;
      &lt;?php
  }
  }
  else
  {
      $session_n = $_SESSION['log_id'];
      $sesdate = date('d/M/Y');
      echo ""&lt;div class='searchNone'&gt;&lt;p&gt;No results found&lt;/p&gt;&lt;/div&gt;
      &lt;div class='sendSearchQ'&gt;
      &lt;p&gt;Please send us your question.&lt;/p&gt;
      &lt;form action='sendquestion.php' method='post' encytype='multipart/form-data'&gt;
      &lt;div class='searchQinputs'&gt;
          &lt;input type='text' name='searchQuestion' id='searchQuestion'placeholder='Whats your question'&gt;&lt;br&gt;
          &lt;input type='submit' name='sendQuestion' id='sendQuestion' value='Send'&gt;
          &lt;input type='text' name='user' id='user' value='$session_n' style='display: none'&gt;
          &lt;input type='text' name='qDate' id='qDate' value='$sesdate' style='display: none'&gt;
          &lt;input type='text' name='status' id='status' value='0' style='display: none'&gt;
          &lt;/div&gt;
      &lt;/form&gt;
      &lt;/div&gt;"";
  }
}
",<php><search><sqlite>,5282,0,124,493,1,7,29,68,444,0,25,5,13,2018-01-14 12:55,2018-01-18 18:59,2018-01-18 18:59,4,4,Basic,9
51033689,How to fix error on postgres install ubuntu,"I'm struggling to fix an error on install of the postgres client. I'm installing this on a Continuous Integration build, so I need it to install without error. The thing is, the client is installed, and I can even run psql commands, if I ssh into the server, but I need this to run without my touch, which means the install has to happen without error. 
I've done all the google-foo, and none of the suggestions I've seen on Ubuntu forums, or here seem to point in the right direction. This is all on ubuntu 14.04.
Alternatively, maybe I can just silence the errors, as long as the client is usable.
Following is the error I run into:
sudo apt-get install postgresql-client
    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    The following additional packages will be installed:
      libpq5 postgresql-client-9.6 postgresql-client-common
    Suggested packages:
      postgresql-9.6 postgresql-doc-9.6
    The following NEW packages will be installed:
      libpq5 postgresql-client postgresql-client-9.6 postgresql-client-common
    0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.
    Need to get 1494 kB of archives.
    After this operation, 6121 kB of additional disk space will be used.
    Get:1 http://deb.debian.org/debian stretch/main amd64 libpq5 amd64 9.6.7-0+deb9u1 [132 kB]
    Get:2 http://deb.debian.org/debian stretch/main amd64 postgresql-client-common all 181+deb9u1 [79.0 kB]
    Get:3 http://deb.debian.org/debian stretch/main amd64 postgresql-client-9.6 amd64 9.6.7-0+deb9u1 [1228 kB]
    Get:4 http://deb.debian.org/debian stretch/main amd64 postgresql-client all 9.6+181+deb9u1 [55.7 kB]
    Fetched 1494 kB in 0s (55.5 MB/s)
    debconf: delaying package configuration, since apt-utils is not installed
    Selecting previously unselected package libpq5:amd64.
    (Reading database ... 31433 files and directories currently installed.)
    Preparing to unpack .../libpq5_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking libpq5:amd64 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client-common.
    Preparing to unpack .../postgresql-client-common_181+deb9u1_all.deb ...
    Unpacking postgresql-client-common (181+deb9u1) ...
    Selecting previously unselected package postgresql-client-9.6.
    Preparing to unpack .../postgresql-client-9.6_9.6.7-0+deb9u1_amd64.deb ...
    Unpacking postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    Selecting previously unselected package postgresql-client.
    Preparing to unpack .../postgresql-client_9.6+181+deb9u1_all.deb ...
    Unpacking postgresql-client (9.6+181+deb9u1) ...
    Setting up libpq5:amd64 (9.6.7-0+deb9u1) ...
    Processing triggers for libc-bin (2.24-11+deb9u3) ...
    Setting up postgresql-client-common (181+deb9u1) ...
    Setting up postgresql-client-9.6 (9.6.7-0+deb9u1) ...
    update-alternatives: using /usr/share/postgresql/9.6/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode
    update-alternatives: error: error creating symbolic link '/usr/share/man/man7/ABORT.7.gz.dpkg-tmp': No such file or directory
    dpkg: error processing package postgresql-client-9.6 (--configure):
     subprocess installed post-installation script returned error exit status 2
    dpkg: dependency problems prevent configuration of postgresql-client:
     postgresql-client depends on postgresql-client-9.6; however:
      Package postgresql-client-9.6 is not configured yet.
    dpkg: error processing package postgresql-client (--configure):
     dependency problems - leaving unconfigured
    Errors were encountered while processing:
     postgresql-client-9.6
     postgresql-client
    E: Sub-process /usr/bin/dpkg returned an error code (1)
    Exited with code 100
I've tried the following to fix:
    sudo apt-get purge postgr*
    sudo apt-get autoremove
    sudo apt-get install synaptic
    sudo apt-get update
from: https://ubuntuforums.org/showthread.php?t=2277582
    which psql
    /usr/bin/psql
And
    more /etc/apt/sources.list
    deb http://deb.debian.org/debian stretch main
    deb http://deb.debian.org/debian stretch-updates main
    deb http://security.debian.org/debian-security stretch/updates main
I'm stumped on how to move forward. 
",<postgresql><ubuntu><debian>,4276,4,2,1049,1,14,40,68,10181,0,51,1,13,2018-06-26 1:20,2018-10-04 20:55,2018-10-04 20:55,100,100,Basic,9
56017410,Pyspark Error:- dataType <class 'pyspark.sql.types.StringType'> should be an instance of <class 'pyspark.sql.types.DataType'>,"I need to extract some data from a pipelinedRDD but while converting it to Dataframe it is giving the following error:
Traceback (most recent call last):
  File ""/home/karan/Desktop/meds.py"", line 42, in &lt;module&gt;
    relevantToSymEntered(newrdd)
  File ""/home/karan/Desktop/meds.py"", line 26, in relevantToSymEntered
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed 
medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms 
Recorded"",""Severeness""],ArrayType)]))
  File ""/home/karan/Downloads/spark-2.4.2-bin-
hadoop2.7/python/pyspark/sql/types.py"", line 409, in __init__
    ""dataType %s should be an instance of %s"" % (dataType, DataType)
AssertionError: dataType &lt;class 'pyspark.sql.types.StringType'&gt; should be an 
instance of &lt;class 'pyspark.sql.types.DataType'&gt;
1. Thing my error is of different type it is TypeError while I got problems with AssertionError.
My problem has nothing to do with casting of data types.
I've already tried using toDF() but it changes the column names which is undesirable.
import findspark
findspark.init('/home/karan/Downloads/spark-2.4.2-bin-hadoop2.7')
from pyspark.sql import SQLContext
from pyspark.sql.types import StructType, StringType, IntegerType, StructField, ArrayType
from pyspark import SparkConf, SparkContext
import pandas as pd
def reduceColoumns(self):
    try:
        filtered=self.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""Disease""],x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return filtered
def cleanData(self,s):
    try:
        self.zipWithIndex
    except Exception as e:
        print(""Error in CleanData:- "")
        print(e)
    return self.filter(lambda x: x[1][0]==s)
def relevantToSymEntered(self):
    mat = spark.createDataFrame(self,StructType([StructField(""Prescribed medicine"",StringType), StructField([""Disease"",""ID"",""Symptoms Recorded"",""Severeness""],ArrayType)]))
    #mat = mat.rdd.map(lambda x: (x[""Prescribed medicine""],list([x[""ID""],x[""Symptoms Recorded""],x[""Severeness""]])))
    print(type(mat))
conf = SparkConf().setMaster(""local[*]"").setAppName(""MovieSimilarities"")
sc = SparkContext(conf = conf)
spark=SQLContext(sc)
rdd = spark.read.csv(""/home/karan/Desktop/ExportExcel2.csv"",header=True,sep="","",multiLine=""True"")
print(rdd)
newrdd=reduceColoumns(rdd)
x=input(""Enter the disease-"")
newrdd=cleanData(newrdd,x)
relevantToSymEntered(newrdd)
",<python><apache-spark><pyspark><apache-spark-sql>,2474,0,61,149,1,2,7,59,24530,0,0,1,13,2019-05-07 7:16,2020-05-05 3:53,,364,,Basic,9
48189015,How to perform case insensitive ORDER BY in mysql?,"I want to perform case-insensitive ORDER BY in MySQL.
I have the data in my database like 
A, C, b, e, D etc
I'm getting the result as 
A, C, D, b, e 
But, I want the result as
A, b, C, D, e 
How can I get that?
",<mysql>,212,0,0,668,1,9,22,76,10626,0,313,4,13,2018-01-10 13:55,2018-01-10 13:57,2018-01-10 13:57,0,0,Basic,10
53986727,"""Insert Into"" statement causing errors due to ""Parameter 7 (""""): The supplied value is not a valid instance of data type float.""","I'm loading a batch of CSV files into a SQL Server table using Python one row at a time.  The files each contain a number of free text fields and erroneous data which I trim and rename before attempting to insert.  
In general (about 95% of the time), the code seems to work however exceptions appear with the error message described below.
I'm confused as a) I only have four columns in my table, and can't understand why it would be looking for Parameter 7, and b) the text columns are being loaded into nvarchar(max) formatted columns, so I wouldn't expect a data type error.  
I've checked the source files to see which rows threw an error, there seems to be no discernible difference between the problem rows and others that are successfully loaded.
I've trimmed the process right back to only insert the JobID (as a bigint) and it works without issue, but as soon as I bring in the text fields, it causes an error.
I'm using Python 3.7.0 and loading into SQL Server 14.0
import numpy as np
import pyodbc
import os
import glob
import pandas as pd
import csv
import config
import urllib
import shutil
import codecs
path = ""C:\\myFilePath""
allFiles = glob.glob(os.path.join(path, ""*.csv""))
for file_ in allFiles:
    df = pd.concat((pd.read_csv(f, encoding='utf8') for f in allFiles))
cnxn = pyodbc.connect(""Driver={ODBC Driver 13 for SQL Server};""                                             
                  ""Server=myServer;""
                  ""Database=myDatabase;""
                  ""Trusted_Connection=yes;""
                  ""SelectMethod=cursor;""
                  )
df2 = df[['JobID', 'NPS_score', 'Obtuse_Column_Name_1', 'Obtuse_Column_Name_2']].copy()
df2.columns = ['JobID', 'Score','Q1', 'Q2']
cursor = cnxn.cursor()
for index,row in df2.iterrows():
    try:
        counter = counter + 1
        cursor.execute(""""""insert into [myDB].[dbo].[input_test]( [JobID], [Score], [Q1], [Q2]) VALUES (?, ?, ?, ?)"""""", row['JobID'],row['Score'],row['Q1'], row['Q2'])
        cursor.commit()
        print(counter)
    except Exception as e:
        print(e) 
        continue    
cursor.close()  
cnxn.close()
I expect the data to be loaded but on some lines get the following error code: 
  ('42000', '[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL
  Server]The incoming tabular data stream (TDS) remote procedure call
  (RPC) protocol stream is incorrect. Parameter 7 (""""): The supplied
  value is not a valid instance of data type float. Check the source
  data for invalid values. An example of an invalid value is data of
  numeric type with scale greater than precision. (8023)
  (SQLExecDirectW)')
",<python><sql-server><pandas>,2620,0,41,379,1,2,10,71,27292,0,0,3,13,2018-12-31 11:06,2018-12-31 13:12,2018-12-31 13:12,0,0,Basic,9
53529974,What does df.repartition with no column arguments partition on?,"In PySpark the repartition module has an optional columns argument which will of course repartition your dataframe by that key.
My question is - how does Spark repartition when there's no key? I couldn't dig any further into the source code to find where this goes through Spark itself.
def repartition(self, numPartitions, *cols):
    """"""
    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The
    resulting DataFrame is hash partitioned.
    :param numPartitions:
        can be an int to specify the target number of partitions or a Column.
        If it is a Column, it will be used as the first partitioning column. If not specified,
        the default number of partitions is used.
    .. versionchanged:: 1.6
       Added optional arguments to specify the partitioning columns. Also made numPartitions
       optional if partitioning columns are specified.
    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()
    10
    &gt;&gt;&gt; data = df.union(df).repartition(""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  5|  Bob|
    |  5|  Bob|
    |  2|Alice|
    |  2|Alice|
    +---+-----+
    &gt;&gt;&gt; data = data.repartition(7, ""age"")
    &gt;&gt;&gt; data.show()
    +---+-----+
    |age| name|
    +---+-----+
    |  2|Alice|
    |  5|  Bob|
    |  2|Alice|
    |  5|  Bob|
    +---+-----+
    &gt;&gt;&gt; data.rdd.getNumPartitions()
    7
    """"""
    if isinstance(numPartitions, int):
        if len(cols) == 0:
            return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)
        else:
            return DataFrame(
                self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)
    elif isinstance(numPartitions, (basestring, Column)):
        cols = (numPartitions, ) + cols
        return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)
    else:
        raise TypeError(""numPartitions should be an int or Column"")
For example: it's totally fine to call these lines but I have no idea what it's actually doing. Is it a hash of the entire line? Perhaps the first column in the dataframe?
df_2 = df_1\
       .where(sf.col('some_column') == 1)\
       .repartition(32)\
       .alias('df_2')
",<python><apache-spark><pyspark><apache-spark-sql>,2260,0,54,177,0,1,11,47,4970,0,19,1,13,2018-11-29 0:04,2018-11-29 0:31,,0,,Basic,2
53536206,Android Room database query does not return id column,"The problem is that the query returns all columns except  'id'
I use fts4 and in docs it says:
  FTS-enabled tables always use a primary key of type INTEGER and with
  the column name ""rowid"". If your FTS-table-backed entity defines a
  primary key, it must use that type and column name.
here is my entity class:
@Fts4
@Entity(tableName = ""projects"")
public class Project {
    @ColumnInfo(name = ""rowid"")
    @PrimaryKey(autoGenerate = true)
    private int id;
    private String name;
    @ColumnInfo(name = ""start_date"")
    private String startDate;
    @ColumnInfo(name = ""end_date"")
    private String endDate;
    private String description;
    @ColumnInfo(name = ""icon_path"")
    private String iconPath;
    private long budget;
public Project(String name, String startDate, String endDate, String description, String iconPath, long budget) {
    this.name = name;
    this.startDate = startDate;
    this.endDate = endDate;
    this.description = description;
    this.iconPath = iconPath;
    this.budget = budget;
}
public int getId() {
    return id;
}
public void setId(int id) {
    this.id = id;
}
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getStartDate() {
    return startDate;
}
public void setStartDate(String startDate) {
    this.startDate = startDate;
}
public String getEndDate() {
    return endDate;
}
public void setEndDate(String endDate) {
    this.endDate = endDate;
}
public String getDescription() {
    return description;
}
public void setDescription(String description) {
    this.description = description;
}
public String getIconPath() {
    return iconPath;
}
public void setIconPath(String iconPath) {
    this.iconPath = iconPath;
}
public long getBudget() {
    return budget;
}
public void setBudget(long budget) {
    this.budget = budget;
}
and here is my simple query:
@Query(""SELECT * FROM projects"")
public LiveData&lt;List&lt;Project&gt;&gt; getAllProjectsI);
I got a warning :
  app.aarsham.projeno.data.Model.Project has some fields [rowid] which
  are not returned by the query. If they are not supposed to be read
  from the result, you can mark them with @Ignore annotation. You can
  suppress this warning by annotating the method with
  @SuppressWarnings(RoomWarnings.CURSOR_MISMATCH). Columns returned by
  the query: name, start_date, end_date, description, icon_path, budget.
  Fields in app.aarsham.projeno.data.Model.Project: rowid, name,
  start_date, end_date, description, icon_path, budget.
and an error:
  The columns returned by the query does not have the fields [id] in
  app.aarsham.projeno.data.Model.Project even though they are annotated
  as non-null or primitive. Columns returned by the query:
  [name,start_date,end_date,description,icon_path,budget]
can anyone help about this?
",<android><sqlite><android-room><fts4>,2833,0,85,161,0,2,9,76,4006,0,67,1,13,2018-11-29 9:55,2018-12-30 3:09,2018-12-30 3:09,31,31,Basic,10
53554458,SQLAlchemy: Get database name from engine,"After creating an SQLALchemy engine like this
engine = create_engine('mssql+pyodbc://user:pass@dbserver:port/db_name?driver=ODBC+Driver+13+for+SQL+Server)
Is there a way to get db_name from the engine-object? I know I can parse the name from the connection string but is there a better way of doing this? I had a look at the SQLAlchemy-API but couldn't find an answer.
",<python><sql-server>,369,1,3,317,1,2,13,72,13350,0,18,1,13,2018-11-30 9:13,2019-04-22 15:35,2019-04-22 15:35,143,143,Basic,3
48308826,Wamp Server Error [Local Server - 2 of 3 services running],"I am new to wamp servers and trying to install wampServer 3.1.0 on my windows 10 machine . 
Somehow it is not installed properly and is having configuration error .
At present ""Wamp server is still in orange state and is throwing the error"" 
  2 of 3 services running
As of my understanding either of Apache,MySQl orPHP is not working .
On further investigation I found that Apache is ok.
But on running mysql.exe(C:\wamp64\bin\mysql\mysql5.7.19\bin) it is throwing :
  ERROR 2003 (HY000): Can't connect to MySQL server on 'localhost'
  (10061)
which lands me to SO-32519474 ,
I tried following the steps ,but it looks good to me in my case .
On further searching I find that wampmysqld64 is stopped in the services.
when I am trying to restart it I am getting the error 
I am stuck up here and have no further clue how to get it fixed 
Any help is highly appreciated.
",<mysql><windows-services><wamp><wampserver>,869,2,0,553,1,10,29,61,126418,0,43,18,13,2018-01-17 19:44,2018-01-18 18:00,2018-01-18 18:00,1,1,Basic,14
53763417,Number of unique elements in all columns of a pyspark dataframe,"How it is possible to calculate the number of unique elements in each column of a pyspark dataframe:
import pandas as pd
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = pd.DataFrame([[1, 100], [1, 200], [2, 300], [3, 100], [4, 100], [4, 300]], columns=['col1', 'col2'])
df_spark = spark.createDataFrame(df)
print(df_spark.show())
# +----+----+
# |col1|col2|
# +----+----+
# |   1| 100|
# |   1| 200|
# |   2| 300|
# |   3| 100|
# |   4| 100|
# |   4| 300|
# +----+----+
# Some transformations on df_spark here
# How to get a number of unique elements (just a number) in each columns?
I know only the following solution which is very slow, both of these lines are calculated in the same amount of time:
col1_num_unique = df_spark.select('col1').distinct().count()
col2_num_unique = df_spark.select('col2').distinct().count()
There are about 10 millions rows in df_spark.
",<python><apache-spark><dataframe><pyspark><apache-spark-sql>,907,0,24,3017,10,41,60,72,19737,,5507,3,13,2018-12-13 13:53,2018-12-13 15:08,2018-12-13 15:08,0,0,Basic,9
51038591,Error installing mysql,"Beginning configuration step: Initializing Database
Attempting to run MySQL Server with --initialize-insecure option...
Starting process for MySQL Server 8.0.11...
Starting process with command: C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe --defaults-file=""C:\ProgramData\MySQL\MySQL Server 8.0\my.ini"" --console --initialize-insecure=on...
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-011071] [Server] Unknown suffix '.' used for variable 'lower_case_table_names' (value '0.0')
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-011071] [Server] C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe: Error while setting value '0.0' to 'lower_case_table_names'
2018-06-26T08:44:42.036600Z 0 [ERROR] [MY-010119] [Server] Aborting
2018-06-26T08:44:42.036600Z 0 [Note] [MY-010120] [Server] Binlog end
Process for mysqld, with ID 3232, was run successfully and exited with code 1.
Failed to start process for MySQL Server 8.0.11.
Database initialization failed.
Ended configuration step: Initializing Database
I am having this error in log during the installing of MySQL on the application configuration part.
",<mysql><installation>,1106,0,12,351,1,2,12,54,36564,0,4,12,13,2018-06-26 8:45,2018-07-03 15:15,,7,,Basic,13
57943166,Flutter Syncing LocalDB With RemoteDB,"I have data stored locally in sqlite.  I have a remote MySql server.  In android I could setup SyncAdapter to handle the syncing between the localdb and the remotedb.
When a record is saved locally and there is an internet connection it should push the data to the server in the background.  It should also periodically update the data stored in SqLite.
Now I'm trying to find the equivalent of SyncAdapter in flutter to do this but I can't seem to find one.  How would I implement this functionality in flutter without having to use firebase?
",<mysql><flutter><sqlite><android-syncadapter>,544,0,0,7999,17,66,124,77,1345,0,520,2,13,2019-09-15 10:19,2021-09-16 0:07,,732,,Basic,9
50967722,room error: The columns returned by the query does not have the fields fieldname,"Here is a sample POJO
public class Product{
  private long id;
  private String name;
  private double price;
 ... constructor for all fields
 ... getters and setters
}
Now, in my productDAO if I have a query like this
@Query(select id, name from products)
LiveData&lt;List&lt;Product&gt;&gt; getProducts()
I get an error like:
  The columns returned by the query does not have the fields [price] in
  ... Product even though they are annotated as non-null or primitive.
  Columns returned by the query: [id,name]
a) If I go in my Products and set
@Nullable
private double price;
the error remains.
b) If I go in my Products and set
@Ignore
private double price;
the error goes away but if I have another query for instance
 @Query(select p.id, p.name, p.price, t.someField from products p inner join table t)
    LiveData&lt;List&lt;Product&gt;&gt; getJoinQueryResponse()
because @Ignore is set the price is returned as 0.0. 
So how to solve this? Hopefully I don't need to make a POJO for each different response from room...
",<android><android-sqlite><android-room><android-architecture-components>,1028,0,17,14879,40,131,218,43,14673,,908,1,13,2018-06-21 11:41,2018-06-21 11:54,2018-06-21 11:54,0,0,Basic,1
52722671,How to make GROUP BY in a cypher query?,"I want to translate a SQL query to cypher. Please, is there any solution to make GROUP BY in cypher?
    SELECT dt.d_year, 
           item.i_brand_id          brand_id, 
           item.i_brand             brand, 
           Sum(ss_ext_discount_amt) sum_agg 
    FROM   date_dim dt, 
   store_sales, 
   item 
    WHERE  dt.d_date_sk = store_sales.ss_sold_date_sk 
    AND store_sales.ss_item_sk = item.i_item_sk 
    AND item.i_manufact_id = 427 
    AND dt.d_moy = 11 
    GROUP  BY dt.d_year, 
      item.i_brand, 
      item.i_brand_id 
   ORDER  BY dt.d_year, 
      sum_agg DESC, 
      brand_id;
",<sql><neo4j><group-by><cypher>,604,0,17,135,1,1,6,50,16169,0,0,2,13,2018-10-09 13:47,2018-10-09 16:32,2018-10-09 16:32,0,0,Basic,10
53378568,SSIS vs. Oracle Data Integrator,"Currently I am a Data Engineer that works mainly with SSIS. While reading about the ETL tools available in the market, i found that Oracle has its own ETL tool called ODI (Oracle Data integrator). I searched for an unbiased comparison between the Oracle Data Integrator and SSIS. I didn't find any article about that. There are some biased article such as :
ETL Tools Comparison of Oracle ODI &amp; Microsoft SSIS Tool -Dec 2014
Competitive Comparison of SQL Server 2008 Integration Services
Based on Stackoverflow questions, there are about 16000 questions about SSIS while ODI has about 200 questions. Which mean that SSIS is more popular.
Is there any unbiased comparison between both technologies? And what are the services that ODI provides and that are not found in SSIS?
Please I am not looking for personal opinions, I need an unbiased answer
",<sql-server><oracle><ssis><etl><oracle-data-integrator>,851,2,0,36491,13,67,124,52,4754,0,4879,1,13,2018-11-19 16:07,2018-11-20 6:59,2018-11-20 6:59,1,1,Intermediate,19
53462775,How to determine if postgis is enabled on a database?,"I wanted to know if there is a way to determine that PostGis was enabled on a database. 
I am trying to replicate my production server with my dev machine and I am not sure if the database on my dev machine had either PostGIS or postgis_topology enabled or both. 
I tried looking around for a solution but could not come up with anything. 
Any suggestions in this regard would be helpful.
",<postgresql><postgis>,389,0,0,16573,40,141,245,63,13297,0,503,5,13,2018-11-24 22:09,2018-11-24 23:59,2018-11-24 23:59,0,0,Basic,10
50973091,Exploding column with index,"I know that I can ""explode"" a column of type array like this:
import org.apache.spark.sql._
import org.apache.spark.sql.functions.explode
val explodedDf = 
    payloadLegsDf.withColumn(""legs"", explode(payloadLegsDf.col(""legs"")))
Now I have multiple rows; one for each item in the array.
Is there a way I can ""explode with index""?  So that there will be a new column that contains the index of the item in the original array?
(I can think of hacks to do this.  First make the array field into an array of tuples of the original value and the index.  Then do the explode.  Then unpack the tuples.  But is there a more elegant way?)
",<scala><apache-spark-sql>,630,0,4,8646,33,119,206,54,6767,0,723,1,13,2018-06-21 16:09,2018-06-21 16:40,2018-06-21 16:40,0,0,Basic,10
63760689,Estimating size of Postgres indexes,"I'm trying to get a better understanding of the tradeoffs involved in creating Postgres indexes. As part of that, I'd love to understand how much space indexes usually use. I've read through the docs, but can't find any information on this. I've been doing my own little experiments creating tables and indexes, but it would be amazing if someone could offer an explanation of why the size is what it is. Assume a common table like this with 1M rows, where each row has a unique id and a unique outstanding.
CREATE TABLE account (
    id integer,
    active boolean NOT NULL,
    outstanding double precision NOT NULL,
);
and the indexes created by
CREATE INDEX id_idx ON account(id)
CREATE INDEX outstanding_idx ON account(outstanding)
CREATE INDEX id_outstanding_idx ON account(id, outstanding)
CREATE INDEX active_idx ON account(active)
CREATE INDEX partial_id_idx ON account(id) WHERE active
What would you estimate the index sizes to be in bytes and more importantly, why?
",<postgresql><indexing><storage>,978,1,12,5483,14,63,104,53,6478,0,1781,2,13,2020-09-06 4:24,2020-09-06 12:00,2020-09-11 1:57,0,5,Basic,10
49991865,Node.js & MySQL - Error: 1251 - Client does not support authentication protocol requested by server; consider upgrading MySQL client,"Right now I have only this code:
const Sequelize = require('sequelize');
const sequelize = new Sequelize('database', 'root', 'passwd', {
  host: 'localhost',
  dialect: 'mysql',
    // http://docs.sequelizejs.com/manual/tutorial/querying.html#operators
  operatorsAliases: false
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
But when I try to run the .js I get this error. I have already tried a lot of solutions out there, including the one I found more often but it didn't work. So right now I don't know what to do. Can somebody help me?
Thank you
",<javascript><mysql><node.js><sequelize.js>,716,3,16,409,1,5,10,80,23565,0,23,4,13,2018-04-24 0:48,2018-04-24 11:41,,0,,Basic,9
60228723,Cannot connect to local postgresql DB using DBeaver,"I'm trying to connect to a postgresql database which is in localhost:5432 but I keep getting the error: 
FATAL: Ident authentication failed for user """".
I installed Postgres11 on virtual machine running Centos7. Created a database through command line, with the name business_db.
I've checked and postgresql is running in localhost:5432.
My pg_hba.conf file is like this:
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             all                                     peer
host    all             all             127.0.0.1/32           ident
host    all             all             ::1/128                 ident
local   replication     all                                     peer
host    replication     all             127.0.0.1/32            ident
host    replication     all             ::1/128                 ident
The pg_ident.conf file doesn't hold any configurations:
# Put your actual configuration here
# ----------------------------------
# MAPNAME       SYSTEM-USERNAME         PG-USERNAME
The database exists as shown by the command:
I'm logged into the system as ""dev"" user but., whatever I try while testing connection whit DBeaver, I allways get the error:
I also tried to set User as postgres and use my system password but get the same error. What am I missing?
",<postgresql><jdbc><dbeaver>,1320,2,11,187,1,1,9,57,32946,,6,4,13,2020-02-14 14:59,2020-02-14 15:19,2020-02-14 15:19,0,0,Basic,14
60278766,Best way to Insert Python NumPy array into PostgreSQL database,"Our team uses software that is heavily reliant on dumping NumPy data into files, which slows our code quite a lot. If we could store our NumPy arrays directly in PostgreSQL we would get a major performance boost.
Other performant methods of storing NumPy arrays in any database or searchable database-like structure are welcome, but PostgresSQL would be preferred.  
My question is very similar to one asked previously. However, I am looking for a more robust and performant answer and I wish to store any arbitrary NumPy array. 
",<python><sql><postgresql><numpy>,530,1,0,604,0,9,27,51,8284,0,198,2,13,2020-02-18 10:29,2020-02-21 16:45,2020-02-22 1:37,3,4,Intermediate,23
63742915,EF: Passing a table valued parameter to a user-defined function from C#,"I have a user-defined function in SQL Server that accepts a TVP (table valued parameter) as parameter. In EF, how do I call such a function from C# ?
I tried using the method ObjectContext.CreateQuery&lt;&gt; but got the following error:
The parameter 'param' of function 'QueryByParam' is invalid. Parameters can only be of a type that can be converted to an Edm scalar type.
Also tried method ObjectContext.ExecuteStoreQuery&lt;&gt; and got the same error. It doesn't return an IQueryable anyway.
Sample code
[DbFunction(nameof(SampleDbContext), &quot;QueryByParam&quot;)]
public IQueryable&lt;SecurityQueryRow&gt; QueryByParam(IEnumerable&lt;ProfileType&gt; profiles, bool isActive = false)
{
    DataTable dataTable = ....
    ObjectParameter profilesParam = new ObjectParameter(&quot;profileTypeIds&quot;, dataTable);
    ObjectParameter isActiveParam = new ObjectParameter(&quot;isActive &quot;, isActive);
    return ((IObjectContextAdapter)this).ObjectContext.CreateQuery&lt;SecurityQueryRow&gt;(
            string.Format(&quot;[{0}].[{1}](@profileTypeIds, @isActive)&quot;, GetType().Name, &quot;QueryByParam&quot;),
            profilesParam,
            isActiveParam);
}
The requirement is that we need an IQueryable back, not the consumed result.
",<c#><sql-server><entity-framework><entity-framework-6><table-valued-parameters>,1261,0,16,131,0,1,4,65,3967,0,0,1,13,2020-09-04 14:16,2020-09-07 17:09,,3,,Basic,9
56382010,Why does MS SQL allow you to create an illegal column?,"I recently saw a tweet stating that you could prevent other developers from reading from a table using the SELECT * FROM TableName by building your table in the following way:
CREATE TABLE [TableName]
(
   [ID] INT IDENTITY NOT NULL,
   [Name] VARCHAR(50) NOT NULL,
   [DontUseStar] AS (1 / 0)
);
It's easy to see that using the SELECT * here would try to read the blank column name as 1 divided by 0 (thus causing a divide by zero error), but without a datatype assigned to the column.
Why does SQL allow you to create a column with no assigned data type, with a name it knows will be illegal?
",<sql><sql-server>,595,0,8,1365,0,12,24,43,550,0,443,3,13,2019-05-30 16:21,2019-05-30 16:26,2019-05-30 16:26,0,0,Basic,9
52127228,"Docker, Flask, SQLAlchemy: ValueError: invalid literal for int() with base 10: 'None'","I have a flask app that can be initialized successfully and connects to Postgresql database. However, when i try to dockerize this app, i get the below error message. ""SQLALCHEMY_DATABASE_URI"" is correct and i can connect to it, so i can't figure where I have gone wrong. 
docker-compose logs
app_1       |   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/url.py"", line 60, in __init__
app_1       |     self.port = int(port)
app_1       | ValueError: invalid literal for int() with base 10: 'None'
Postgres database connects successfully in Docker container 
postgres_1  | LOG:  database system is ready to accept connections
config.py
from os import environ
import os
RDS_USERNAME = environ.get('RDS_USERNAME')
RDS_PASSWORD = environ.get('RDS_PASSWORD')
RDS_HOSTNAME = environ.get('RDS_HOSTNAME')
RDS_PORT = environ.get('RDS_PORT')
RDS_DB_NAME = environ.get('RDS_DB_NAME')
SQLALCHEMY_DATABASE_URI = ""postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dbname}""\
                          .format(username = RDS_USERNAME, password = RDS_PASSWORD, \
                           hostname = RDS_HOSTNAME, port = RDS_PORT, dbname = RDS_DB_NAME)
flask_app.py (entry point)
def create_app():
    app = Flask(__name__, static_folder=""./static"", template_folder=""./static"")
    app.config.from_pyfile('./app/config.py', silent=True)
    register_blueprint(app)
    register_extension(app)
    with app.app_context():
        print(db) -&gt; This prints the correct path for SQLALCHEMY_DATABASE_URI
        db.create_all()
        db.session.commit()
    return app
def register_blueprint(app):
    app.register_blueprint(view_blueprint)
    app.register_blueprint(race_blueprint)
def register_extension(app):
    db.init_app(app)
    migrate.init_app(app)
app = create_app()
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
Dockerfile
FROM ubuntu
RUN apt-get update &amp;&amp; apt-get -y upgrade
RUN apt-get install -y python-pip &amp;&amp; pip install --upgrade pip
RUN mkdir /home/ubuntu
WORKDIR /home/ubuntu/celery-scheduler
ADD requirements.txt /home/ubuntu/celery-scheduler/
RUN pip install -r requirements.txt
COPY . /home/ubuntu/celery-scheduler
EXPOSE 5000
CMD [""python"", ""flask_app.py"", ""--host"", ""0.0.0.0""]
docker-compose.yml
version: '2' 
services:
  app:
    restart: always
    build: 
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app
    depends_on:
      - postgres
  postgres:
    restart: always
      image: postgres:9.6
    environment:
      - POSTGRES_USER=${RDS_USERNAME}
      - POSTGRES_PASSWORD=${RDS_PASSWORD}
      - POSTGRES_HOSTNAME=${RDS_HOSTNAME}
      - POSTGRES_DB=${RDS_DB_NAME}
    ports:
      - ""5432:5432""
",<python><postgresql><docker><flask><sqlalchemy>,2717,0,85,896,2,18,43,35,4479,0,317,2,13,2018-09-01 10:30,2018-09-01 11:09,2018-09-15 7:22,0,14,Basic,9
51264240,rake db:migrate error with mysql2 gem - Library not loaded: libssl.1.0.0.dylib,"Getting the following error after running rake db:migrate
rake aborted!
LoadError: dlopen(/Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle, 9): Library not loaded: libssl.1.0.0.dylib
  Referenced from: /Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle
  Reason: image not found - /Users/scott/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/mysql2-0.4.10/lib/mysql2/mysql2.bundle
/Users/scott/Google Drive/playground/myApp/myApp/config/application.rb:21:in `&lt;top (required)&gt;'
/Users/scott/Google Drive/playground/myApp/myApp/Rakefile:4:in `&lt;top (required)&gt;'
What does the libssl refer to? 
",<ruby-on-rails><ruby><rubygems><rake><mysql2>,700,0,6,1613,3,25,41,52,7385,0,50,7,13,2018-07-10 11:39,2018-07-10 15:34,2018-07-10 15:34,0,0,Basic,14
57319206,What is the meaning of && in PostGIS?,"In PostGIS, what is the result of the &amp;&amp; operation between two geometries? In my mind, &amp;&amp; returns a boolean, but does return geometry this time. In the following example, the operation is between a LineString and a Polygon.
Firstly, I guess this is the relationship between inclusion and being included. Until I do the following example, I think this should be a relationship of type ""intersection"". Am I right?
select ST_geomfromtext('linestring(0.1 0.1,1.9 1.9)', 4326) &amp;&amp; st_geomfromtext('POLYGON((0 0,0 1,1 1,1 0,0 0))', 4326)
The result is t which represents true.
",<postgresql><postgis>,594,0,9,173,1,1,6,73,10650,,8,2,13,2019-08-02 2:32,2019-08-02 2:35,2019-08-02 2:35,0,0,Basic,2
54816169,How to keep null values when writing to csv,"I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command. The issue is that Python's csv writer automatically converts Nulls into an empty string """" and it fails my job when the column is an int or float datatype and it tries to insert this """" when it should be a None or null value.
  To make it as easy as possible to interface with modules which
  implement the DB API, the value None is written as the empty string.
  https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer
What is the best way to keep the null value? Is there a better way to write csvs in Python? I'm open to all suggestions.
Example:
I have lat and long values:
42.313270000    -71.116240000
42.377010000    -71.064770000
NULL    NULL
When writing to csv it converts nulls to """":
with file_path.open(mode='w', newline='') as outfile:
    csv_writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
    if include_headers:
        csv_writer.writerow(col[0] for col in self.cursor.description)
    for row in self.cursor:
        csv_writer.writerow(row)
.
42.313270000,-71.116240000
42.377010000,-71.064770000
"""",""""
  NULL
  Specifies the string that represents a null value. The default is \N
  (backslash-N) in text format, and an unquoted empty string in CSV
  format. You might prefer an empty string even in text format for cases
  where you don't want to distinguish nulls from empty strings. This
  option is not allowed when using binary format.
  https://www.postgresql.org/docs/9.2/sql-copy.html
ANSWER:
What solved the problem for me was changing the quoting to csv.QUOTE_MINIMAL.
  csv.QUOTE_MINIMAL Instructs writer objects to only quote those fields
  which contain special characters such as delimiter, quotechar or any
  of the characters in lineterminator.
Related questions:
- Postgresql COPY empty string as NULL not work
",<python><python-3.x><postgresql><csv>,1943,5,12,1416,7,35,63,62,45790,0,1300,5,13,2019-02-21 21:02,2019-02-21 21:11,2019-03-08 18:23,0,15,Basic,3
58951334,"aiopg + sqlalchemy: how to ""drop table if exists"" without raw sql?","I am looking at examples of aiopg usage with sqlalchemy and these lines scare me:
async def create_table(conn):
    await conn.execute('DROP TABLE IF EXISTS tbl')
    await conn.execute(CreateTable(tbl))
I do not want to execute raw sql queries when using sqlalchemy. However I can't find any other way to implement the same logic. My attempts were:
1)
await conn.execute(tbl.drop(checkfirst=True))
This raises:
  sqlalchemy.exc.UnboundExecutionError: Table object 'tbl' is not
  bound to an Engine or Connection.  Execution can not proceed without a
  database to execute against.
Also I can't find a way to bind the table to engine because aiopg doesn't support metadata.create_all
2)
await conn.execute(DropTable(tbl))
This raises:
  psycopg2.errors.UndefinedTable: table ""tbl"" does not exist
Seems like DropTable construct doesn't support IF EXISTS part in any way.
So, the question is, is there any way to rewrite await conn.execute('DROP TABLE IF EXISTS tbl') statement into something without raw sql when using aiopg + sqlalchemy?
",<python><postgresql><sqlalchemy><drop-table><aiopg>,1038,2,8,8250,13,37,74,74,3493,0,1175,1,13,2019-11-20 9:51,2021-10-16 14:02,2021-10-16 14:02,696,696,Basic,3
55027706,PhpStorm - How to connect to MySQL via Docker,"I am working with PhpStorm 2018.3.4, Docker, MySQL and Ubuntu.
I tried unsuccessfully to configure MySQL with the Docker container network_mysql.
First, I have tried this configuration :
It gave me this error :
Then, I tried this :
This one gave me this other error.
Am I missing something? Is there another place where I must configure something?
docker ps output : 
Here docker network ls :
For the command docker inspect network_mysql, here is a link to the description :
https://pastebin.com/9LmeAkc8
Here is a docker-compose.yml configuration : 
https://pastebin.com/DB4Eye4y
I tried to put - ""3306:3306"" in addition to the wex_server_proxy section with no avail.
The file to modify was this one :
https://pastebin.com/TPBQNCDZ
I added the ports section, opening the 3306 port :) And then, it works.
",<mysql><docker><phpstorm>,805,12,7,463,1,8,21,45,23650,0,39,3,13,2019-03-06 16:20,2019-03-06 23:14,2019-03-06 23:14,0,0,Basic,3
50972190,Room Database Migration Failed: ALTER TABLE to add multiple columns,"I'm upgrading my Database from version 3 to version 4 by providing migration from 3 to 4.
Here's my code for migration:
private static Migration MIGRATION_3_4 = new Migration(3, 4) {
    @Override
    public void migrate(@NonNull SupportSQLiteDatabase database) {
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN localVideoUrl TEXT;"");
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN postType TEXT"");
      database.execSQL(""ALTER TABLE caption_table ADD COLUMN videoUrl TEXT"");
    }
};
Here's the code which create room database
this.mAppDataBase = Room.databaseBuilder(getApplicationContext(), AppDataBase.class, ""my_db"")
                        .addMigrations(MIGRATION_2_3, MIGRATION_3_4)
                        .build();
Here's the piece of code that I have added on my PostModel
@Expose
private String postType;
@Expose
private String videoUrl;
@Expose
private String localVideoUrl;
public String getPostType() {
    return postType;
}
public void setPostType(String postType) {
    this.postType = postType;
}
public String getVideoUrl() {
    return videoUrl;
}
public void setVideoUrl(String videoUrl) {
    this.videoUrl = videoUrl;
}
public String getLocalVideoUrl() {
    return localVideoUrl;
}
public void setLocalVideoUrl(String localVideoUrl) {
    this.localVideoUrl = localVideoUrl;
}
And below is the error I'm getting. The error is not related to the notNull property of room entity. 
  java.lang.IllegalStateException: Migration didn't properly handle
  posts(com.myapp.Database.PostModel).
  Expected:
      TableInfo{name='posts', columns={imageWidth=Column{name='imageWidth', type='INTEGER',
  affinity='3', notNull=true, primaryKeyPosition=0},
  localVideoUrl=Column{name='localVideoUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  authorImageLocalUrl=Column{name='authorImageLocalUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  videoUrl=Column{name='videoUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  imageLocalUrl=Column{name='imageLocalUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, postType=Column{name='postType',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  authorName=Column{name='authorName', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, imageUrl=Column{name='imageUrl',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  id=Column{name='id', type='INTEGER', affinity='3', notNull=true,
  primaryKeyPosition=1}, title=Column{name='title', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  authorImageUrl=Column{name='authorImageUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageHeight=Column{name='imageHeight', type='INTEGER', affinity='3',
  notNull=true, primaryKeyPosition=0}}, foreignKeys=[], indices=[]}
  Found:
      TableInfo{name='posts', columns={imageWidth=Column{name='imageWidth', type='INTEGER',
  affinity='3', notNull=true, primaryKeyPosition=0},
  authorImageLocalUrl=Column{name='authorImageLocalUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageLocalUrl=Column{name='imageLocalUrl', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0},
  authorName=Column{name='authorName', type='TEXT', affinity='2',
  notNull=false, primaryKeyPosition=0}, imageUrl=Column{name='imageUrl',
  type='TEXT', affinity='2', notNull=false, primaryKeyPosition=0},
  id=Column{name='id', type='INTEGER', affinity='3', notNull=true,
  primaryKeyPosition=1}, title=Column{name='title', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  authorImageUrl=Column{name='authorImageUrl', type='TEXT',
  affinity='2', notNull=false, primaryKeyPosition=0},
  imageHeight=Column{name='imageHeight', type='INTEGER', affinity='3',
  notNull=true, primaryKeyPosition=0}}, foreignKeys=[], indices=[]}
",<sqlite><android-sqlite><android-room>,3926,0,43,380,2,4,16,72,4538,0,9,1,13,2018-06-21 15:23,2019-09-06 8:35,,442,,Intermediate,26
54115837,Column does not allow DBNull.Value - No KeepNulls - Proper Column Mappings,"I am using c# with .NET 4.5.2, pushing to SQL Server 2017 14.0.1000.169
In my database, I have a table with a DateAdded field, of type DateTimeOffset.
I am attempting to BulkCopy with the following code:
private Maybe BulkCopy(SqlSchemaTable table, System.Data.DataTable dt, bool identityInsertOn)
{
    try
    {
        var options = SqlBulkCopyOptions.TableLock | SqlBulkCopyOptions.FireTriggers | SqlBulkCopyOptions.UseInternalTransaction; //  | SqlBulkCopyOptions.CheckConstraints; // Tried CheckConstraints, but it didn't change anything.
        if (identityInsertOn) options |= SqlBulkCopyOptions.KeepIdentity;
        using (var conn = new SqlConnection(_connString))
        using (var bulkCopy = new SqlBulkCopy(conn, options, null))
        {
            bulkCopy.DestinationTableName = table.TableName;
            dt.Columns.Cast&lt;System.Data.DataColumn&gt;().ToList()
                .ForEach(x =&gt; bulkCopy.ColumnMappings.Add(new SqlBulkCopyColumnMapping(x.ColumnName, x.ColumnName)));
            try
            {
                conn.Open();
                bulkCopy.WriteToServer(dt);
            }
            catch (Exception ex)
            {
                return Maybe.Failure(ex);
            }
        }
    }
    catch (Exception ex)
    {
        return Maybe.Failure(ex);
    }
    return Maybe.Success();
}
The two possible reasons I know of for the does not allow DBNull error are:
Columns are in the wrong order, which is solved by either putting them in the same order as their Database Ordinal, or by performing a Column Mapping.
KeepNulls is enabled, and DBNull.Value (or null?) are set in the DataTable.
But I am Mapping correctly and NOT ever setting KeepNulls.
Yet I am receiving the error: 
  Column DateAdded does not allow DBNull.Value
EDIT I also tried just NOT SETTING anything, including null, DBNull.Value, and DefaultValue... just literally not setting that column at all.
Also, if I Remove the DateAdded column from the DataTable, it Works.  But I don't want that.  Of the 100,000 records, maybe 20 of them have data.  So in my batches of 500, sometimes None have data in the DateAdded field, sometimes one or two have data.
So I'd like to keep the column in my DataTable but let it use the DefaultValue.
One last note: I have alternated between setting the DataColumn's Value to DBNull.Value versus dt.Columns[x.ColumnName].DefaultValue.  Both ways give the same error.
Edit 2
This is the code I'm using to populate the data in my Data Table:
foreach (var column in table)
{
    System.Data.DataRow newRow = dt.NewRow();
    foreach (var field in column)
    {
        if (!IsNull(field.Value) &amp;&amp; !IsEmptyDateOrNumber(field.ColumnType, field.Value))
        {
            // For DateAdded, this is not hit on the first batch, though there are columns Before and After DateAdded on the same row which do have value.
            // But it WILL be hit once or twice a few batches later.  So I don't want to completely remove the definition from the DataTable.
            newRow[field.ColumnName] = field.Value;
        }
        else
        {
            // newRow[field.ColumnName] = dt.Columns[field.ColumnName].DefaultValue;
            // newRow[field.ColumnName] = DBNull.Value;
            // dt.Columns[field.ColumnName].AllowDBNull = true;
        }
    }
    dt.Rows.Add(newRow);
}
IsNull() returns TRUE if the value is null or the string ""null"", as is required for my business requirements.
IsEmptyDateOrNumber() will return TRUE if the field is a numeric or date type, and the value is null or empty """".  Because while empty is valid for many string-like fields, it is never a valid numeric value.
The condition to assign the field a value is hit exactly 0 percent of the time for this particular column.  Thus nothing is set.
",<c#><sql-server><datatable>,3799,0,67,5878,2,46,58,39,19028,0,668,3,13,2019-01-09 17:55,2019-01-09 19:15,2019-01-16 16:59,0,7,Intermediate,31
64861331,How can I install or upgrade to sqlite 3.33.0 on Ubuntu 18.04?,"I'm currently running Ubuntu 18.04 with SQLite3. SQLite 3 is at version 3.22.0 and I need to upgrade it to version 3.33.0 to take advantage of new functionality that is available. If I remove and reinstall SQLite3 with apt-get, it just re=installs 3.22.0. How can I upgrade to the latest version of SQLite3?
",<sqlite><upgrade><ubuntu-18.04><apt-get>,308,0,0,432,0,3,13,51,9754,0,66,1,13,2020-11-16 15:58,2021-02-15 23:57,,91,,Intermediate,15
54060265,How to list files in S3 bucket using Spark Session?,"Is it possible to list all of the files in given S3 path (ex: s3://my-bucket/my-folder/*.extension) using a SparkSession object?
",<amazon-web-services><apache-spark><amazon-s3><apache-spark-sql>,129,0,0,5384,16,63,116,49,30042,0,1797,4,13,2019-01-06 9:34,2019-01-06 11:52,2019-01-06 20:03,0,0,Basic,3
52624097,GIN Index has O(N^2) complexity for array overlap operator?,"I ran into an issue with using the &amp;&amp; array operator on a GIN index of mine. Basically I have a query that looks like this:
SELECT *
FROM example
WHERE keys &amp;&amp; ARRAY[1,2,3,...]
This works fine for a small number of array elements (N) in the array literal, but gets really slow as N gets bigger in what appears to be O(N^2) complexity.
However, from studying the GIN data structure as described by the docs, it seems that the performance for this could be O(N). In fact, it's possible to coerce the query planner into an O(N) plan like this:
SELECT DISTINCT ON (example.id) *
FROM unnest(ARRAY[1,2,3,...]) key
JOIN example ON keys &amp;&amp; ARRAY[key]
In order to illustrate this better, I've created a jupyter notebook that populates an example table, show the query plans for both queries, and most importantly benchmarks them and plots a time vs array size (N) graph.
https://github.com/felixge/pg-slow-gin/blob/master/pg-slow-gin.ipynb
Please help me understand what causes the O(N^2) performance for query 1 and if query 2 is the best way to work around this issue.
Thanks
Felix Geisendörfer
PS: I'm using Postgres 10, but also verified that this problem exists with Postgres 11.
I've also posted this question on the postgres performance mailing list, but unfortunately didn't get any answer.
",<sql><postgresql><indexing>,1315,4,6,2932,5,27,36,50,1363,0,46,1,13,2018-10-03 9:39,2022-01-25 16:15,,1210,,Advanced,33
53989887,How do I configure PyMySQL connect for SSL?,"I'm trying to connect my database using SSL with PyMySQL, but I can't find good documentation on what the syntax is.
These credentials work in Workbench and with the CLI, but I get this error when using PyMySQL.
Can't connect to MySQL server on 'server.domain.com' ([WinError 10061] No connection could be made because the target machine actively refused it)&quot;)
db_conn = pymysql.connect(
    host=db_creds['host'],
    user=db_creds['user'],
    passwd=db_creds['passwd'],
    db=db_creds['db'],
    charset=db_creds['charset'],
    ssl={'ssl':{'ca': 'C:/SSL_CERTS/ca-cert.pem',
                'key' : 'C:/SSL_CERTS/client-key.pem',
                'cert' : 'C:/SSL_CERTS/client-cert.pem'
                }
        }
)
If I shut SSL off and drop the SSL parameter, I can connect unsecured just fine.   What am I doing wrong with the SSL parameter?
Edit: PyMySQL now wants ssl parameters listed like this instead of in a dict.
db_conn = pymysql.connect(
     host=db_creds['host'],
     user=db_creds['user'],
     passwd=db_creds['passwd'],
     db=db_creds['db'],
     charset=db_creds['charset'],
     ssl_ca='C:/SSL_CERTS/ca-cert.pem',
     ssl_key='C:/SSL_CERTS/client-key.pem',
     ssl_cert='C:/SSL_CERTS/client-cert.pem'
                          )
",<python><ssl><pymysql>,1262,0,22,403,1,4,14,56,14190,0,9,2,13,2018-12-31 17:20,2019-01-05 2:11,2019-01-05 2:11,5,5,Basic,14
56717592,Dangerous query method deprecation warning on Rails 5.2.3,"I am in the process of upgrading my Rails app to 5.2.3
I am using the following code in my app.
MyModel.order('LOWER(name) ASC')
It raises the following deprecation warning:
DEPRECATION WARNING: Dangerous query method (method whose arguments are used as raw SQL) called with non-attribute argument(s): ""LOWER(name)"". Non-attribute arguments will be disallowed in Rails 6.0. This method should not be called with user-provided values, such as request parameters or model attributes. Known-safe values can be passed by wrapping them in Arel.sql()
I have changed the above as the deprecation warning recommends and the warning gone away:
MyModel.order(Arel.sql('LOWER(name) ASC'))
I have surfed about related discussion here. It seems this change is introduced to disallow the SQL injections. 
But the order clause LOWER(name) ASC doesn't contains any user input. Why this ordering is considered as insecure? Is this the intended behavior or Am I missing anything here?
",<ruby-on-rails><sql-injection><ruby-on-rails-5.2>,967,1,4,1367,1,6,22,61,3109,0,47,1,13,2019-06-22 17:20,2019-06-23 11:26,2019-06-23 11:26,1,1,Basic,3
49047779,Google Data Studio & AWS MySQL SSL Connection,"I am trying to remotely connect Google Data Studio with our MySQL Database, which is hosted on an AWS instance. To allow for a secure connection, we added SSL access to the AWS's MySQL database user as recommended in the documentation:
GRANT USAGE ON *.* TO 'encrypted_user'@'%' REQUIRE SSL;
The problem here is that AWS, unlike GOOGLE CloudSQL, only generates a Server certificate, and not a Client certificate, nor a Client private key (as far as I can tell). Both the latter is needed to enable SSL for Google Data Studio &amp; MySQL connection. 
Just to add a side-note, we also white-listed Google's recommended IPs as listed here. There are a lot of users in this thread complaining that white-listing specific IPs does not work, they had to add wildcard on the subnets. So we have also added addresses of the /16 subnets for each IP: 
64.18.%.%
64.233.%.%
66.102.%.%
66.249.%.%
72.14.%.%
74.125.%.%
108.177.%.%
173.194.%.%
207.126.%.%
209.85.%.%
216.58.%.%
216.239.%.%
Finally, one does not need to restart the AWS firewall after white-listing new IPs, it is immediately in-effect. 
My Questions:
Is there absolutely no way to create a client certificate and a client private key on MySQL hosted on AWS ?
I would really want to use SSL between Google Data Studio (GDS) and our MySQL-DB, but the GDS-UI does not allow us to connect without filling in the client certificate and client private key. Is there any work around at the moment for me to allow this secure connection ? 
Thanks in advance!
",<mysql><amazon-web-services><ssl><looker-studio>,1504,5,13,1401,2,20,37,50,7823,0,433,1,13,2018-03-01 10:35,2018-06-08 13:31,,99,,Intermediate,29
52728319,Can't set lower_case_table_names in MySQL 8.x on Windows 10,"In MySQL 8.0.12 running on Windows 10, it seems impossible to set lower_case_table_names to 2, so as to achieve the appearance of mixed case DB and table names in Workbench. I realize that under the hood these objects may remain lower case, which is fine. But I want it to look right in Workbench, and I could always achieve this in previous versions of MySQL. When I attempt to do that and restart the service so it takes effect, the service crashes and stops. In the mysql logs I see this:
  Different lower_case_table_names settings for server ('2') and data
  dictionary ('1'). 
  Data Dictionary initialization failed.
This seems to be a common problem for a lot of people.
I read here that the solution is: 
  So lower_case_table_names needs to be set together with
  --initialize.
But I have no idea what that means, or how to set it at startup. I have googled all over and read several forum articles but I can't find clear instructions on how to resolve this.
",<mysql><windows><mysql-workbench>,969,1,2,6916,24,81,159,77,11801,,1220,2,13,2018-10-09 19:47,2018-10-11 23:56,2018-10-16 23:35,2,7,Advanced,39
50477946,Detect duplicate items in recursive CTE,"I have a set of dependencies stored in my database. I'm looking to find all the objects that depend on the current one, whether directly or indirectly.  Since objects can depend zero or more other objects, it's perfectly reasonable that object 1 is depended on by object 9 twice (9 depends on 4 and 5, both of which depend on 1).  I'd like to get the list of all the objects that depend on the current object without duplication.
This gets more complex if there are loops.  Without loops, one could use DISTINCT, though going through long chains more than once only to cull them at the end is still a problem.  With loops, however, it becomes important that the RECURSIVE CTE doesn't union with something it has already seen.
So what I have so far looks like this:
WITH RECURSIVE __dependents AS (
  SELECT object, array[object.id] AS seen_objects
  FROM immediate_object_dependents(_objectid) object
  UNION ALL
  SELECT object, d.seen_objects || object.id
  FROM __dependents d
  JOIN immediate_object_dependents((d.object).id) object
    ON object.id &lt;&gt; ALL (d.seen_objects)
) SELECT (object).* FROM __dependents;
(It's in a stored procedure, so I can pass in _objectid)
Unfortunately, this just omits a given object when I've seen it before in the current chain, which would be fine if a recursive CTE was being done depth-first, but when it's breadth-first, it becomes problematic.
Ideally, the solution would be in SQL rather than PLPGSQL, but either one works.
As an example, I set this up in postgres:
create table objectdependencies (
  id int,
  dependson int
);
create index on objectdependencies (dependson);
insert into objectdependencies values (1, 2), (1, 4), (2, 3), (2, 4), (3, 4);
And then I tried running this:
with recursive rdeps as (
  select dep
  from objectdependencies dep
  where dep.dependson = 4 -- starting point
  union all
  select dep
  from objectdependencies dep
  join rdeps r
    on (r.dep).id = dep.dependson
) select (dep).id from rdeps;
I'm expecting ""1, 2, 3"" as output.
However, this somehow goes on forever (which I also don't understand).  If I add in a level check (select dep, 0 as level, ... select dep, level + 1, on ... and level &lt; 3), I see that 2 and 3 are repeating.  Conversely, if I add a seen check:
with recursive rdeps as (
  select dep, array[id] as seen
  from objectdependencies dep
  where dep.dependson = 4 -- starting point
  union all
  select dep, r.seen || dep.id
  from objectdependencies dep
  join rdeps r
    on (r.dep).id = dep.dependson and dep.id &lt;&gt; ALL (r.seen)
) select (dep).id from rdeps;
then I get 1, 2, 3, 2, 3, and it stops. I could use DISTINCT in the outer select, but that only reasonably works on this data because there is no loop. With a larger dataset and more loops, we will continue to grow the CTE's output only to have the DISTINCT pare it back down.  I would like the CTE to simply stop that branch when it's already seen that particular value somewhere else.
Edit: this is not simply about cycle detection (though there can be cycles). It's about uncovering everything referenced by this object, directly and indirectly.  So if we have 1->2->3->5->6->7 and 2->4->5, we can start at 1, go to 2, from there we can go to 3 and 4, both of those branches will go to 5, but I don't need both branches to do so - the first one can go to 5, and the other can simply stop there. Then we go on to 6 and 7. Most cycle detection will find no cycles and return 5, 6, 7 all twice. Given that I expect most of my production data to have 0-3 immediate references, and most of those to be likewise, it will be very common for there to be multiple branches from one object to another, and going down those branches will be not only redundant but a huge waste of time and resource.
",<sql><postgresql><common-table-expression>,3772,0,43,21814,5,42,69,76,3894,0,1133,4,13,2018-05-22 23:47,2018-05-25 23:35,2018-05-29 6:00,3,7,Intermediate,17
53197922,Difference between .query() and .execute() in MySQL,"I'm having difficulty comprehending the implementation of prepared statements. I've done a fair amount of research but most of the information I found is either out of context or contain examples far more complex than what I'm trying to accomplish. Can anyone clarify for me why the execute method in the second example below is throwing a syntax error?
NOTE: I'm using the node-mysql2 package here.
controller.js  (using query mysql method)
  const db = require(""../lib/database"");
  async addNewThing(req, res, next) {
    let data = req.body
    const queryString = 'INSERT INTO table SET ?'
    try {
      await db.query(queryString, data)
      res.status(201).json({
        message: 'Record inserted',
        data
      })
    } catch (error) {
      next(error)
    }
  }
Record is successfully inserted into the database
controller.js    (using execute mysql method)
  const db = require(""../lib/database"");
  async addNewThing(req, res, next) {
    let data = req.body
    const queryString = 'INSERT INTO table SET ?'
    try {
      await db.execute(queryString, [data])
      res.status(201).json({
        message: 'Record inserted',
        data
      })
    } catch (error) {
      next(error)
    }
  }
Results in the following error:
  You have an error in your SQL syntax; check the manual that
  corresponds to your MySQL server version for the right syntax to use
  near '?' at line 1
data
{ thing_id: '987654', thing_name: 'thing' }
",<mysql><node.js><express><syntax-error>,1457,0,37,472,1,4,13,69,13077,0,6,1,13,2018-11-07 21:15,2018-11-09 3:08,2018-11-09 3:08,2,2,Intermediate,19
52791121,Is Java Spring JPA native query SQL injection proof?,"I'm writing this question because I didn't find any useful article about how to prevent SQL Injection in Spring Data JPA. All the tutorials are showing how to use these queries but they don't mentioned anything about these possible attacks.
I'm having the following query:
@Repository
public interface UserRepository extends CrudRepository&lt;User, Integer&gt; {
    @Query(nativeQuery = true, value = ""SELECT * FROM users WHERE email LIKE %:emailAddress%"")
    public ResponseList&lt;User&gt; getUsers(@Param(""emailAddress"") String emailAddress);
}
The rest controller to deliver the request:
@RequestMapping(value = ""/get-users"", method = RequestMethod.POST)
public Response&lt;StringResponse&gt; getUsers(WebRequest request) {
    return userService.getUsers(request.getParameter(""email""));
}
Are JPQL or native query parameters escaped before executing them?
This is the query with SQL injection executed in my MySQL console which drops the users table:
SELECT * FROM users WHERE email LIKE '%'; DROP TABLE users; -- %';
I have tried to execute the SQL attack by sending a POST request to the server:
http://localhost:8080/get-users
POST: key/value: ""email"" : ""'; DROP TABLE users; --""
I have enabled Hibernate's sql logging and this is what the above request produced:
[http-nio-8080-exec-8] DEBUG org.hibernate.SQL - SELECT * FROM users WHERE email LIKE ?
Hibernate: SELECT * FROM users WHERE email LIKE ?
[http-nio-8080-exec-8] DEBUG org.hibernate.loader.Loader - bindNamedParameters() %'; DROP TABLE users; -- % -&gt; emailAddress [1]
[http-nio-8080-exec-8] TRACE o.h.type.descriptor.sql.BasicBinder - binding parameter [1] as [VARCHAR] - [%'; DROP TABLE users; -- %]
The table wasn't dropped (which is good) but why the parameter isn't escaped?
What if I don't annotate the @Param(""emailAddress"") and I use indexed parameters?:
@Query(nativeQuery = true, value = ""SELECT * FROM users WHERE email LIKE ?1"")
public ResponseList&lt;User&gt; getUsers(String email);
",<spring-data-jpa><sql-injection>,1971,1,21,9447,22,84,137,39,12261,0,261,1,13,2018-10-13 8:37,2018-10-14 12:42,2018-10-14 12:42,1,1,Advanced,43
50414906,"Why am I getting an ""Invalid use of Nulls"" error in my query?","I use a userform in Excel to run some internal queries between my spreadsheets.
However I am getting Invalid Use of Null. I am aware of the nz null syntax (SQL MS Access - Invalid Use of Null), however my queries can be quite large, and I am wondering if there is anything I can add to my VBA code to allow nulls. 
",<sql><excel><vba><ms-access>,315,1,2,1584,6,33,69,37,3362,,303,1,13,2018-05-18 15:40,2018-05-23 19:56,2018-05-23 19:56,5,5,Basic,13
52644981,Spark: Return empty column if column does not exist in dataframe,"As shown in the below code, I am reading a JSON file into a dataframe and then selecting some fields from that dataframe into another one.
df_record = spark.read.json(""path/to/file.JSON"",multiLine=True)
df_basicInfo = df_record.select(col(""key1"").alias(""ID""), \
                                col(""key2"").alias(""Status""), \
                                col(""key3.ResponseType"").alias(""ResponseType""), \
                                col(""key3.someIndicator"").alias(""SomeIndicator"") \
                                )
Issue is that some times, the JSON file does not have some of the keys that I try to fetch - like ResponseType. So it ends up throwing errors like:
org.apache.spark.sql.AnalysisException: No such struct field ResponseType
How can I get around this issue without forcing a schema at the time of read? is it possible to make it return a NULL under that column when it is not available?
how do I detect if a spark dataframe has a column Does mention how to detect if a column is available in a dataframe. This question, however, is about how to use that function. 
",<apache-spark><pyspark><apache-spark-sql>,1086,1,9,833,3,10,26,39,32979,0,23,6,13,2018-10-04 10:55,2018-10-04 12:06,2018-10-04 12:06,0,0,Basic,9
56776974,Connect to a database in cloud,"I have an SQLite database (110kb) in an S3 bucket. I want to connect to that database every time I run my Python application.
An option is to download database everytime I run the Python application and connect it. But I want to know if there exists a way to connect to that SQLite database through memory, using S3FileSystem and open.
I'm using SQLite3 library in Python 3.6.
",<python><python-3.x><sqlite><amazon-s3><in-memory-database>,377,0,2,2063,3,15,26,71,18444,0,476,5,13,2019-06-26 16:04,2019-06-26 18:18,2019-12-21 6:48,0,178,Basic,3
54124262,"How to fix ""Illuminate\Database\QueryException: SQLSTATE[HY000] [1044] Access denied for user""","I tried to run: php artisan migrate
Also to connect to MySQL using Xampp on Windows.
I Got this error:
Illuminate\Database\QueryException  : SQLSTATE[HY000] [1044] Access
denied for user ''@'localhost' to database 'homestead' (SQL: select *
from information_schema.tables where table_schema = homestead and
table_name = migrations)
  at
C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|  Exception trace:
  1   PDOException::(&quot;SQLSTATE[HY000] [1044] Access denied for user
''@'localhost' to database 'homestead'&quot;)
      C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  2  
PDO::__construct(&quot;mysql:host=127.0.0.1;port=3306;dbname=homestead&quot;,
&quot;homestead&quot;, &quot;&quot;, [])
      C:\Users\harsh\Laravel1\vendor\laravel\framework\src\Illuminate\Database\Connectors\Connector.php:70
  Please use the argument -v to see more details.
.env file:
DB_CONNECTION=mysql 
DB_HOST=127.0.0.1 
DB_PORT=3306 
DB_DATABASE=homestead 
DB_USERNAME=homestead 
DB_PASSWORD=
",<php><mysql><laravel><laravel-5>,1581,0,34,349,1,5,15,54,144061,0,5,10,13,2019-01-10 8:07,2019-01-10 8:38,2019-01-10 8:38,0,0,Basic,9
53382161,"message -""could not read a hi value - you need to populate the table: hibernate_sequence""","My problem is as follows: When i create POST request in ""Postman"" app. This is what i try to POST 
  {""name"": ""John Doe"", ""email"":""jdoe@test.com"", ""city"": ""London""}
I am getting the following error:
{
""timestamp"": ""2018-11-19T20:16:00.486+0000"",
""status"": 500,
""error"": ""Internal Server Error"",
""message"": ""could not read a hi value - you need to populate the table: hibernate_sequence; nested exception is org.hibernate.id.IdentifierGenerationException: could not read a hi value - you need to populate the table: hibernate_sequence"",
""path"": ""/api/ver01/product""
}
I was looking for answer in search box but none of them helped me. So i think that the problem is in sql code but I am not sure. Whole project is written in intelliJ IDE.
This is my Product class.
package com.hubertkulas.webstore.store.archetype;
import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import javax.persistence.Entity;
import javax.persistence.GeneratedValue;
import javax.persistence.GenerationType;
import javax.persistence.Id;
import java.math.BigDecimal;
import java.sql.Date;
@Entity
@JsonIgnoreProperties({""hibernateLazyInitializer"", ""handler""})
public class Product {
@Id
@GeneratedValue(strategy = GenerationType.AUTO)
private Long id;
private boolean contact;
private String email;
private String category;
private String name;
private String city;
private String model;
private BigDecimal price;
@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = ""MM-dd-yyyy"")
private Date date;
public String getName() {
    return name;
}
public void setName(String name) {
    this.name = name;
}
public String getEmail() {
    return email;
}
public void setEmail(String email) {
    this.email = email;
}
public String getCity() {
    return city;
}
public void setCity(String city) {
    this.city = city;
}
public String getCategory() {
    return category;
}
public void setCategory(String category) {
    this.category = category;
}
public String getModel() {
    return model;
}
public void setModel(String model) {
    this.model = model;
}
public BigDecimal getPrice() {
    return price;
}
public void setPrice(BigDecimal price) {
    this.price = price;
}
public Date getDate() {
    return date;
}
public void setDate(Date date) {
    this.date = date;
}
public boolean isContact() {
    return contact;
}
public void setContact(boolean contact) {
    this.contact = contact;
}
public Long getId() {
    return id;
}
// setter for id because Jackson will use it
public void setId(Long id) {
    this.id = id;
}
}
This is my ProductController class
package com.hubertkulas.webstore.store.controllers;
import com.hubertkulas.webstore.store.archetype.Product;
import com.hubertkulas.webstore.store.jparepository.ProductRepository;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.*;
import java.util.List;
@RestController
@RequestMapping(""api/ver01/product"")
public class ProductController {
//injecting ProductRepository when ProductController is called
@Autowired
private ProductRepository productRepository;
@GetMapping
public List&lt;Product&gt; list() {
    //finds all of the records and returns it
   return productRepository.findAll();
}
@PostMapping
@ResponseStatus(HttpStatus.OK)
public void create(@RequestBody Product product){
    productRepository.save(product);
}
@GetMapping(""/{id}"")
public Product get(@PathVariable(""id"") long id){
    // return specific record with added id
    return productRepository.getOne(id);
}
}
This is my ProductRepository Interface
package com.hubertkulas.webstore.store.jparepository;
import com.hubertkulas.webstore.store.archetype.Product;
import org.springframework.data.jpa.repository.JpaRepository;
//Using Jpa for CRUD operations
public interface ProductRepository extends JpaRepository&lt;Product, Long&gt; {
}
And this is my database
CREATE TABLE
product
(
    id BIGINT NOT NULL,
    contact BOOLEAN NOT NULL,
    email VARCHAR,
    category VARCHAR,
    name VARCHAR,
    city VARCHAR,
    date DATETIME,
    price NUMERIC,
    model VARCHAR,
    PRIMARY KEY (id)
);
CREATE TABLE
hibernate_sequence
(
    next_val BIGINT
);
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (1, 1, 'abraham@site.com', 'Electronics', 'Abraham Westbrom', 'New 
York', 4419619200000, '3250');
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (2, 1, 'udon@site.com', 'Electronics', 'Udon Hon', 'London', 
4419619200000, '799');
INSERT INTO product (id, contact, email, category, name, city, date, price)
VALUES (3, 0, 'mateuszsinus@site.com', 'Software', 'Mateusz Sinus', 
'Warsaw', 4419619200000, '10000');
INSERT INTO hibernate_sequence (next_val) VALUES (4);
",<java><sql><hibernate><spring-boot><postman>,4821,0,193,168,1,2,10,47,25192,0,8,4,13,2018-11-19 20:28,2018-11-19 20:40,2018-11-19 20:40,0,0,Basic,9
54781017,How to Map Input and Output Columns dynamically in SSIS?,"I Have to Upload Data in SQL Server from .dbf Files through SSIS.
My Output Column is fixed but the input column is not fixed because the files come from the client and the client may have updated data by his own style. there may be some unused columns too or the input column name can be different from the output column.
One idea I had in my mind was to map files input column with output column in SQL Database table and use only those column which is present in the row for file id.
But I am not getting how to do that. Any idea?
Table Example
FileID
InputColumn
OutputColumn
Active
1
CustCd
CustCode
1
1
CName
CustName
1
1
Address
CustAdd
1
2
Cust_Code
CustCode
1
2
Customer Name
CustName
1
2
Location
CustAdd
1
",<sql><sql-server><ssis><etl><ssis-2012>,717,0,0,623,3,11,25,46,14738,0,57,1,13,2019-02-20 7:33,2019-02-20 23:59,2019-02-20 23:59,0,0,Basic,9
59389911,sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine,"I am trying to push the excel xlsx data to mySQl Alchemy by using this simple code...
import pandas as pd
import os
import sqlalchemy
mydir = (os.getcwd()).replace('\\', '/') + '/'
# MySQL Connection
MYSQL_USER = 'xxxxxxx'
MYSQL_PASSWORD = 'xxxxxxxx'
MYSQL_HOST_IP = '127.0.0.1'
MYSQL_PORT = 3306
MYSQL_DATABASE = 'xlsx_test_db'
# connect db
engine = sqlalchemy.create_engine('mysql+mysqlconnector://' + MYSQL_USER + ':' + MYSQL_PASSWORD + '@' + MYSQL_HOST_IP + ':' + str(
    MYSQL_PORT) + '/' + MYSQL_DATABASE, echo=False)
engine.connect()
# reading and insert one file at a time
for file in os.listdir('.'):
    # only process excels files
    file_basename, extension = file.split('.')
    if extension == 'xlsx':
        df = pd.read_excel(r'' + mydir + 'MNM_Rotterdam_5_Daily_Details-20191216081027.xlsx', sheet_name='Report')
        df.to_sql(file_basename, con=engine, if_exists='replace')
But I found this error
Traceback (most recent call last):
  File ""C:/Users/DELL/PycharmProjects/automateDB/myWatchDog.py"", line 28, in &lt;module&gt;
    df.to_sql(file_basename, con=engine, if_exists='replace')
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\core\generic.py"", line 2532, in to_sql
    dtype=dtype, method=method)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 460, in to_sql
    chunksize=chunksize, dtype=dtype, method=method)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 1173, in to_sql
    table.create()
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 577, in create
    self.pd_sql.drop_table(self.name, self.schema)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\pandas\io\sql.py"", line 1222, in drop_table
    self.meta.reflect(only=[table_name], schema=schema)
  File ""C:\Users\DELL\PycharmProjects\MyALLRefProf\venv\lib\site-packages\sqlalchemy\sql\schema.py"", line 3956, in reflect
    (bind.engine, s, ', '.join(missing)))
sqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine(mysql+mysqlconnector://root:***@127.0.0.1:3306/xlsx_test_db): (MNM_Rotterdam_5_Daily_Details-20191216081027)
So any one could me to solve this...
Thank you...
I hope it would be clear enough....
",<python><pandas><sqlalchemy>,2358,0,45,2279,7,36,80,37,16097,0,328,4,13,2019-12-18 10:23,2020-03-28 19:39,,101,,Basic,9
57822113,More than 24 hours in a day in postgreSQL,"Assuming I have this schema:
create table rental (
    id           integer,
    rental_date  timestamp,
    customer_id  smallint,
    return_date  timestamp,
);
Running this query returns strange results:
select customer_id, avg(return_date - rental_date) as ""avg""
from rental
group by customer_id
order by ""avg"" DESC
It displays:
customer_id|avg_rent_duration     |
-----------|----------------------|
        315|     6 days 14:13:22.5|
        187|5 days 34:58:38.571428|
        321|5 days 32:56:32.727273|
        539|5 days 31:39:57.272727|
        436|       5 days 31:09:46|
        532|5 days 30:59:34.838709|
        427|       5 days 29:27:05|
        555|5 days 26:48:35.294118|
...
599 rows
Why is there values like 5 days 34:58:38, 5 days 32:56:32 and so on? I thought there where only 24 hours in a day, maybe I'm wrong.
EDIT
Demo here: http://sqlfiddle.com/#!17/caa7a/1/0
Sample data:
insert into rental (rental_date, customer_id, return_date)
values
('2007-01-02 13:10:06', 1, '2007-01-03 01:01:01'),
('2007-01-02 01:01:01', 1, '2007-01-09 15:10:06'),
('2007-01-10 22:10:06', 1, '2007-01-11 01:01:01'),
('2007-01-30 01:01:01', 1, '2007-02-03 22:10:06');
",<sql><postgresql><intervals>,1173,2,31,30632,39,173,267,73,2400,0,1001,2,13,2019-09-06 12:31,2019-09-06 14:43,2019-09-06 14:43,0,0,Basic,1
49408290,Django + Docker + SQLite3 How to mount database to the container?,"I had sqlite database with some sort of tables on my localhost and i wanted to copy and paste that database to my server which runs on docker. I created paths like this:
db_data: there is my sqlite database which i want to run in my django project.
web: there is my whole django project
in my docker-compose.yml i writed this volume:
version: ""3""
services:
  web:
    build: ./web/
    ports:
      - ""8001:8001""
    volumes:
      - ./web:/code
      - /home/cosmo/db_data/db.sqlite3:/code/db.sqlite3
    command: python manage.py runserver 0.0.0.0:8001
So i thik that docker will get database in my db_data and will make volume inside my web folder (in my project. There i had database on my localhost so it wouldnt be problem.) But i will paste here settings.py:
# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# Database
# https://docs.djangoproject.com/en/2.0/ref/settings/#databases
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}
So when i will open my db.sqlite3 inside db_data every tables and content are there, but when i will run container the db.sqlite3 in my project folder (web) is empty.
When i will run docker ps command there is no database container maybe this is the problem i dont know. I have there only:
in the red circle is my django container. So when i will run my server and try to login every account from the database is unknown. So I think that container works with that empty db in my project. Someone please has any solution? Thanks.
",<django><sqlite><docker><mount>,1654,3,25,491,0,7,23,49,9856,0,25,1,13,2018-03-21 13:56,2021-02-17 4:36,2021-02-17 4:36,1064,1064,Basic,9
62911496,postgresql-client-13 : Depends: libpq5 (>= 13~beta2) but 12.3-1.pgdg18.04+1 is to be installed,"I want to try new PostgreSQL and follow this instruction. But installation fails:
$ sudo apt install postgresql-client-13
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:
The following packages have unmet dependencies:
 postgresql-client-13 : Depends: libpq5 (&gt;= 13~beta2) but 12.3-1.pgdg18.04+1 is to be installed
E: Unable to correct problems, you have held broken packages.
I also tried this instruction to resolve unmet dependencies
What did I wrong and how to install psql 13?
UPD
Content of my sources.list.d:
kes@kes-X751SA /etc/apt/sources.list.d $ cat pgdg.list 
deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg main
kes@kes-X751SA /etc/apt/sources.list.d $ cat pgdg-testing.list 
deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg-testing main 13
Also:
$ sudo apt-cache policy postgresql-13
postgresql-13:
  Installed: (none)
  Candidate: 13~beta2-1.pgdg18.04+1
  Version table:
     13~beta2-1.pgdg18.04+1 100
        100 http://apt.postgresql.org/pub/repos/apt bionic-pgdg-testing/13 amd64 Packages
",<postgresql><linux-mint-19><postgresql-13>,1367,4,28,22762,17,111,163,43,12564,0,4593,3,13,2020-07-15 9:15,2020-08-07 9:51,2020-08-07 9:51,23,23,Basic,9
58562024,"Docker password authentication failed for user ""postgres""","I'm writing a docker-compose file to launch some services. But the db service is a trouble maker, I always get this error:
FATAL:  password authentication failed for user ""postgres""
DETAIL:  Password does not match for user ""postgres"".
Connection matched pg_hba.conf line 95: ""host all all all md5""
I've read a lot of threads, and I've correctly set the POSTGRES_USER and POSTGRES_PASSWORD. I have also remove the previous volumes and container to force postgresql to re-init the password. But I can't figure out why it's still not working.
So what is the correct way to force the re-initialization of the postgresql image. So I would be able to connect to my database.
I've seen that this error: Connection matched pg_hba.conf line 95: ""host all all all md5"", and I've heard about the postgres conf file. But it's an official container it's supposed to work, isn't it ?
version: '3'
services:
  poll:
    build: poll
    container_name: ""poll""
    ports:
      - ""5000:80""
    networks:
      - poll-tier
    environment:
      - REDIS_HOST=redis
    depends_on:
      - redis
  worker:
    build: worker
    container_name: ""worker""
    networks:
      - back-tier
    environment:
      - REDIS_HOST=redis
      - POSTGRES_HOST=db
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
    depends_on:
      - redis
      - db
  redis:
    image: ""redis:alpine""
    container_name: ""redis""
    networks:
      - poll-tier
      - back-tier
  result:
    build: result
    container_name: ""result""
    ports:
      - ""5001:80""
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
      - POSTGRES_HOST=db
      - RESULT_PORT=80
    networks:
      - result-tier
    depends_on:
      - db
  db:
    image: ""postgres:alpine""
    container_name: ""db""
    restart: always
    networks:
      - back-tier
      - result-tier
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=postgres
volumes:
  db-data:
    driver: local
networks:
  poll-tier: {}
  back-tier: {}
  result-tier: {}
I'm expected to get the db connected, and not password authentication failed for user ""postgres"".
",<postgresql><docker><docker-compose>,2189,0,76,991,1,8,22,68,26693,0,173,2,13,2019-10-25 16:00,2019-10-25 19:41,2019-10-25 19:41,0,0,Basic,9
48633900,Presto: Cast timestamp w/TZ to plain timestamp WITHOUT converting to UTC,"This query in Presto:
select *, 
  cast(ts_w_tz as timestamp) as ts, 
  cast(substr(cast(ts_w_tz as varchar), 1, 23) as timestamp) as local_ts_workaround 
from (select timestamp '2018-02-06 23:00:00.000 Australia/Melbourne' as ts_w_tz);
Returns:
                   ts_w_tz                   |           ts            |   local_ts_workaround   
---------------------------------------------+-------------------------+-------------------------
 2018-02-06 23:00:00.000 Australia/Melbourne | 2018-02-06 12:00:00.000 | 2018-02-06 23:00:00.000
As you can see, the act of casting the timestamp with timezone to a timestamp has resulted in the timestamp being converted back to UTC time (eg ts). IMO the correct behaviour should be to return the 'wall reading' of the timestamp, as per local_ts_workaround.
I realise there are many posts about how Presto's handling of this is wrong and doesn't conform to the SQL standard, and that there is a fix in the works. But in the meantime this is a major pain since the effect is that there appears to be no built in way to get a localised timestamp withOUT timezone (as per local_ts_workaround).
Obviously, I have the string conversion workaround for now, but this seems horrible. I am wondering if anyone has a better workaround or can point out something that I have missed?
Thanks.
",<sql><presto>,1322,0,10,8162,10,35,32,45,12762,0,15,2,13,2018-02-06 0:59,2020-05-07 6:28,2021-02-15 18:07,821,1105,Advanced,33
53513963,How to restore a database from bak file from azure data studio on Mac,"Previously on Mac I use mysql operation studio and I click on database and click restore then browse to my bak file, but now they change to azure data studio and when I repeat the same steps I got this error:
""You must enable preview features in order to use restore"" 
but I cannot figure out hot to enable that. I have googled and tried few things even open my azure, microsoft account on website but I do not see that option. 
Can some one help please !
",<sql><azure-sql-database>,456,1,0,1259,3,22,52,53,13507,0,84,4,13,2018-11-28 7:10,2018-12-11 15:49,2018-12-11 15:49,13,13,Basic,9
48396535,How can I modify the time zone in Azure SQL Database?,"Can you please tell me how to change the time zone in Azure SQL Database?
",<azure-sql-database>,74,0,0,141,1,1,4,63,24596,0,0,1,13,2018-01-23 7:44,2018-01-23 12:14,,0,,Basic,9
59488379,AWS Athena partition fetch all paths,"Recently, I've experienced an issue with AWS Athena when there is quite high number of partitions.
The old version had a database and tables with only 1 partition level, say id=x. Let's take one table; for example, where we store payment parameters per id (product), and there are not plenty of IDs. Assume its around 1000-5000. Now while querying that table with passing id number on where clause like "".. where id = 10"". The queries were returned pretty fast actually. Assume we update the data twice a day.
Lately, we've been thinking to add another partition level for day like, ""../id=x/dt=yyyy-mm-dd/.."". This means that partition number grows  xID times per day if a month passes and if we have 3000 IDs, we'd approximately get 3000x30=90000 partitions a month. Thus, a rapid grow in number of partitions.
On, say 3 months old data (~270k partitions), we'd like to see a query like the following would return in at most 20 seconds or so.
select count(*) from db.table where id = x and dt = 'yyyy-mm-dd'
This takes like a minute.
The Real Case
It turns out Athena first fetches the all partitions (metadata) and s3 paths (regardless the usage of where clause) and then filter those s3 paths that you would like to see on where condition. The first part (fetching all s3 paths by partitions lasts long proportionally to the number of partitions)
The more partitions you have, the slower the query executed.
Intuitively, I expected that Athena fetches only s3 paths stated on where clause, I mean this would be the one way of magic of the partitioning. Maybe it fetches all paths 
Does anybody know a work around, or do we use Athena in a wrong way ?
Should Athena be used only with small number of partitions ?
Edit
In order to clarify the statement above, I add a piece from support mail.
from Support
  ...
  You mentioned that your new system has 360000 which is a huge number.
  So when you are doing select * from &lt;partitioned table&gt;, Athena first download all partition metadata and searched S3 path mapped with
  those partitions. This process of fetching data for each partition
  lead to longer time in query execution.
  ...
Update
An issue opened on AWS forums. The linked issue raised on aws forums is here.
Thanks.
",<amazon-web-services><nosql><aws-glue><presto><amazon-athena>,2239,1,2,2037,1,16,25,61,1429,0,110,1,13,2019-12-26 12:18,2020-03-28 16:34,2020-03-28 16:34,93,93,Advanced,33
57603349,Error:mysqld.service: Start request repeated too quickly. On manjaro,"Yesterday I updated my manjaro. I had many problems since then.
Firstly, I type
systemctl status mysqld.service
to start MySQL, but it errors out with
mysqld.service: Start request repeated too quickly.
I has found many suggestions but they doesn't work.
I already have tried:
Check the permission of the MySQL data directory using the below command. The ownership should be mysql:mysql and the directory permission should be 700.
ls -ld /var/lib/mysql/
Check the permission of databases inside the MySQL data directory using the below command. The ownership should be mysql:mysql for all the files inside that directory.
ls -lh /var/lib/mysql/
Check the listening network TCP ports using the command
netstat -ntlp
Check the MySQL log files for any error using:
cat /var/log/mysql/mysqld.log
Try to start MySQL using
mysqld_safe --defaults-file=/etc/my.cf
My Error:
dong@dong-manjaro  /home/dong   systemctl status mysqld.service                                                                          13:30:33 
● mysqld.service - MySQL Server
   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Thu 2019-08-22 13:30:29 CST; 6s ago
     Docs: man:mysqld(8)
           http://dev.mysql.com/doc/refman/en/using-systemd.html
  Process: 8006 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)
  Process: 8027 ExecStart=/usr/bin/mysqld $MYSQLD_OPTS (code=exited, status=127)
 Main PID: 8027 (code=exited, status=127)
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Service RestartSec=100ms expired, scheduling restart.
8月 22 13:30:29 dong-manjaro systemd[1]: mysqld.service: Scheduled restart job, restart counter is at 5.
8月 22 13:30:29 dong-manjaro systemd[1]: Stopped MySQL Server.
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Start request repeated too quickly.**
8月 22 13:30:29 dong-manjaro systemd[1]: **mysqld.service: Failed with result 'exit-code'.**
8月 22 13:30:29 dong-manjaro systemd[1]: **Failed to start MySQL Server.**
",<mysql><linux><archlinux><manjaro>,2063,1,25,131,1,1,5,73,34795,0,3,6,13,2019-08-22 6:39,2020-12-12 8:22,,478,,Basic,13
62257900,How to list all the stored procedure in AWS RedShift,"I was checking this, but not find the proper one. So I prepared one and sharing that query here.
",<amazon-web-services><plsql><amazon-redshift>,97,0,0,2832,6,40,91,72,16515,0,57,2,13,2020-06-08 8:22,2020-06-08 8:23,2020-06-08 8:23,0,0,Basic,10
48145384,How to disable only_full_group_by option in Laravel,"I am new to laravel and I am having an issue with DB problem.
I have disabled 'only_full_group_by' sql_mode by editing /etc/mysql/my.cnf file. And I checked sql_mode for both global and session using SELECT @@GLOBAL.sql_mode; and SELECT @@SESSION.sql_mode; and confirmed that sql_mode no longer has only_full_group_by.
However, when I make a request through postman, it gives me the error saying this is incompatible with sql_mode=only_full_group_by.
I am so confused. Why do I get this error even after I changed sql_mode? Am I doing something wrong?
Any suggestion or advice would be appreciated.
Thank you.
SQL using toSql()
select A.* 
from `A` 
inner join `B` on `A`.`id` = `B`.`a_id` 
inner join `C` on `C`.`id` = `B`.`c_id` 
group by `A`.`id` having COUNT(A.id) &gt; 0;
",<php><mysql><laravel-5><mysql-5.7>,777,0,13,1045,3,17,40,47,16047,0,114,5,13,2018-01-08 6:38,2018-01-08 6:44,2018-01-08 6:44,0,0,Basic,10
54446734,Is it possible to issue CREATE statements using sp_executesql with parameters?,"I'm trying to dynamically create triggers, but ran into a confusing issue around using sp_executesql and passing parameters into the dynamic SQL.  The following simple test case works:
DECLARE @tableName sysname = 'MyTable';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT 1
        END';
EXEC sp_executesql @sql
However, I want to be able to use @tableName (and other values) as variables within the script, so I passed it along to the sp_executesql call:
DECLARE @tableName sysname = 'ContentItems';
DECLARE @sql nvarchar(max) = N'
    CREATE TRIGGER TR_' + @tableName + N' ON ' + @tableName + N' FOR INSERT
        AS
        BEGIN
            PRINT @tableName
        END';
EXEC sp_executesql @sql, N'@tableName sysname', @tableName=@tableName
When running the above, I get an error:
  Msg 156, Level 15, State 1, Line 2
  Incorrect syntax near the keyword 'TRIGGER'.
After trying I few things, I've discovered that even if I don't use @tableName in the dynamic SQL at all, I still get this error.  And I also get this error trying to create a PROCEDURE (except, obviously, the message is Incorrect syntax near the keyword 'PROCEDURE'.)
Since the SQL runs fine either directly or when not supplying parameters to sp_executesql, this seems like I'm running into a true limitation in the SQL engine, but I don't see it documented anywhere.  Does anyone know if there is a way to accept to a dynamic CREATE script, or at least have insight into the underlying limitation that's being run into?
Update
I can add a PRINT statement, and get the below SQL, which is valid, and runs successfully (when run directly).  I still get the error if there's nothing dynamic in the SQL (it's just a single string with no concatenation).
CREATE TRIGGER TR_ContentItems ON ContentItems FOR INSERT
    AS
    BEGIN
        PRINT @tableName
    END
I also get the same error whether using sysname or nvarchar(max) for the parameter.
",<sql-server><dynamic-sql><sp-executesql>,2020,0,31,152787,23,148,175,79,1581,0,3899,7,13,2019-01-30 17:56,2019-01-30 18:36,,0,,Basic,10
62802173,Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call,"I have an application based on F#, and I use EF-Core and MySQL (Pomelo.EntityFrameworkCore.MySql).
I have an async method which updates data in DB(MySql)
let updatePlayerAchievementsAsync (logger:ILogger) (ctx:ReportCacheDbContext) (id: int) = async {
  let! account = ctx.AccountCaches.FirstOrDefaultAsync(fun e -&gt; e.AccountId = id) |&gt; Async.AwaitTask
  if account &lt;&gt; null then
    account.State &lt;- &quot;Closed&quot;
    do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
    logger.LogInformation(&quot;Account{0} updated&quot;, id)        
}
when this method comes to the 99th element, the following errors occurred:
|ERROR|System.InvalidOperationException:An exception has been raised that is likely due to a transient failure. Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call. 
---&gt; MySql.Data.MySqlClient.MySqlException (0x80004005): Connect Timeout expired. All pooled connections are in use.
I tried to follow 1st error's recomendation and tried to add EnableRetryOnFailure()
member this.ConfigureServices(services: IServiceCollection) =
    services.AddOptions() |&gt; ignore
    services.AddCors() |&gt; ignore
    services
        .AddDbContext&lt;ApplicationDbContext&gt;(
            fun (service:IServiceProvider) (dbContext:DbContextOptionsBuilder) -&gt;
                dbContext.UseMySql(profile.DbConnectionToAdmin /*HERE*/)|&gt; ignore)
    ...
And I can't find any documentation about this adding options for F# &amp; MySQL, cause all found info written on C#.
Maybe problem in used pools (default max=100) and I wrote next:
...
do! ctx.SaveChangesAsync true |&gt; Async.AwaitTask |&gt; Async.Ignore
ctx.Database.CloseConnection()
logger.LogInformation(&quot;Account{0} updated&quot;, id)  
But anyway problem wasn't solved.
This is my new experience in F# and async and I cant understand what I did incorrectly.
Could anyone help me with it?
",<mysql><entity-framework><asynchronous><async-await><f#>,1956,0,26,315,1,3,9,75,48466,,1,3,13,2020-07-08 19:19,2020-11-03 11:34,,118,,Basic,10
49782240,Can I do case insensitive search with JSON_EXTRACT in MySQL?,"I am running SELECT * FROM mytable WHERE LOWER(JSON_EXTRACT(metadata, ""$.title"")) = 'hello world' with the intent that hello world is data from a user that I will flatten to all lowercase. The actual value in my db is ""Hello World"", but this search comes back empty every time.
If I do a SELECT LOWER(JSON_EXTRACT(metadata, ""$.title"")) FROM mytable, it certainly comes back lowercase as hello world. Not sure what I'm missing here.
Queries to get actual values:
SELECT JSON_EXTRACT(metadata, ""$.title"")  FROM mytable gets me ""Hello World""
SELECT LOWER(JSON_EXTRACT(metadata, ""$.title""))  FROM mytable gets me ""hello world""
Queries trying to find the right row
Gets me value
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""Hello World""
SELECT * FROM mytable WHERE metadata-&gt;""$.title"" = ""Hello World""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""%hello world%""
Gets me nothing
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") = ""hello world""
SELECT * FROM mytable WHERE JSON_EXTRACT(metadata, ""$.title"") LIKE  ""%hello world%""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"") = ""hello world""
SELECT * FROM ututs WHERE LOWER(metadata-&gt;""$.title"")  LIKE ""hello""
So it looks like the result is giving back the value, including quotes. That doesn't appear to be the issue though, given I get a result when I match the case. I am also confused why the % at the start is solving my issue. There is no space between the "" and H. I typed the JSON out myself.
I also updated metadata column straight to {""title"":""Hello World""} by manually typing. MySQL automatically adds a space after colon to make it {""title"": ""Hello World""}, which is fine, but was just sanity checking any spaces.
",<mysql><json>,1733,0,21,8822,14,58,107,69,10018,,151,3,13,2018-04-11 18:35,2018-04-11 20:35,2018-04-11 20:35,0,0,Basic,10
51151773,Best practice calling scalar functions with Entity Framework Core (2.1),"I often need to call scalar functions that are defined on a SQL Server from my web applications (ASP.NET Core / EF Core). Since these functions are just simple helper functions and I also use a lot of them I use a general pattern for calling these scalar functions - with the help of the new query types available from EF Core 2.1.
Since I am relatively new to EF Core my question is if this pattern might cause problems and/or if there is a better solution or best practice for calling scalar functions. The solution works and I cannot observe any problems so far but for example I wondered if using the same query type for different functions might lead to unexpected values or weird behaviour due to caching/tracking behaviour, etc. within EF Core - it's more of a gut feeling.
So here's the pattern:
Instead of defining different entity types for every single scalar function I simply define one generic type:
public class PrimitiveDto&lt;T&gt;
{
    public T Value { get; set; }
}
In my context class I register these types for every return type I expect from the scalar functions I want to use - so for all scalar functions returning 'int' the context class would have one additional entry like this:
public virtual DbQuery&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
For EF Core &gt;= 3 it is:
public virtual DbSet&lt;PrimitiveDto&lt;int&gt;&gt; BasicIntDto { get; set; }
In every part of the application where I want to call a scalar function returning 'int' I simply use the same following pattern:
context.BasicIntDto.FromSql(&quot;SELECT &lt;FUNCTION&gt; AS Value&quot;)
By using this pattern I can call any number of functions the same way without defining additional types or extending the context class.
Please let me know if I could run into a trap through this pattern. Thank you very much.
",<c#><sql-server><entity-framework><asp.net-core><entity-framework-core>,1820,0,7,700,0,8,18,54,8974,0,305,1,13,2018-07-03 10:03,2019-01-15 16:03,,196,,Basic,10
48557948,Where should I register my DBAL type?,"I am using Doctrine's enum types to track the status of an entity that I am using in a Symfony application. I am using (roughly) the methods described here: 
http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/cookbook/mysql-enums.html
My problem comes when I try to update the database schema. I get the following error:
  [Doctrine\DBAL\DBALException]
  Unknown column type ""EnumStatusType"" requested. Any Doctrine type that you use has to be registered
   with \Doctrine\DBAL\Types\Type::addType(). You can get a list of all the known types with \Doctrin
  e\DBAL\Types\Type::getTypesMap(). If this error occurs during database introspection then you might
   have forgot to register all database types for a Doctrine Type. Use AbstractPlatform#registerDoctr
  ineTypeMapping() or have your custom types implement Type#getMappedDatabaseTypes(). If the type nam
  e is empty you might have a problem with the cache or forgot some mapping information.
This error is very helpful in a way -- as is the documentation -- but both of those resources leave out two pieces of information: In which file should I use addType() to register my new type? 
Two secondary questions: Should I call the addType() method statically, as shown in the examples? If not, how should I retrieve an object in order to call the method non-statically?
",<php><mysql><symfony><doctrine-orm><doctrine>,1342,2,7,1034,5,35,70,70,10124,0,843,1,13,2018-02-01 8:03,2018-02-01 8:07,2018-02-01 8:07,0,0,Basic,3
53694089,ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data,"Problem
I am not able to connect to my Cloud SQL postgres instance with the command line, which has been working previously:
gcloud sql connect &lt;instance_name&gt; --user=&lt;username&gt;
This is the error I'm getting:
ERROR: (gcloud.sql.connect) HTTPError 400: The incoming request contained invalid data.
Version
Running macOS Mojave 10.14 (18A391) with a tethered 4G hotspot via my Samsung Galaxy S8.
$ gcloud --version
Google Cloud SDK 227.0.0
bq 2.0.39
core 2018.11.30
gsutil 4.34
Log
Running the command with the --log-http flag, it returns:
{
 ""error"": {
  ""errors"": [
   {
    ""domain"": ""global"",
    ""reason"": ""invalidRequest"",
    ""message"": ""The incoming request contained invalid data.""
   }
  ],
  ""code"": 400,
  ""message"": ""The incoming request contained invalid data.""
 }
}
Question
Why is this happening and what can I do to fix it?
",<google-cloud-platform><google-cloud-sql>,851,0,21,698,1,6,18,59,11343,0,9,3,13,2018-12-09 16:00,2018-12-09 18:07,2018-12-09 18:07,0,0,Basic,3
48837393,Scala doobie fragment with generic type parameter,"I am trying to abstract inserting objects of different types into sql tables of similar structure. Here's what I'm trying to do:
class TableAccess[A : Meta](table: String) {
  def insert(key: String, a: A): ConnectionIO[Unit] = {
    (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
  }
}
But I get this compile error:
[error] diverging implicit expansion for type doobie.util.param.Param[A]
[error] starting with method fromMeta in object Param
[error]     (fr""insert into "" ++ Fragment.const(table) ++ fr"" values ($key, $a);"").update.run.map(_ =&gt; ())
All I can find in the documentation is:
  doobie allows you to interpolate values of any type (and options
  thereof) with an Meta instance, which includes...
But it seems that is not enough in this case; what's the right typeclass/imports/conversions I need?
",<sql><scala><doobie>,865,0,8,18284,2,37,59,58,1581,0,334,3,13,2018-02-17 2:47,2018-03-18 8:42,2020-01-06 19:10,29,688,Basic,5
57987355,Remove sensitive information from environment variables in postgres docker container,"I want to make a postgres database image but don't want to expose password and username which are stored as environment variable when produced using docker-compose.yml file. Basically, I don't want anyone to exec into the container and find out the variables.
One way is to use docker-secrets, but I don't want to to use docker swarm because my containers would be running on a single host.
my docker-compose file -
    version: ""3""
    services:
       db:
         image: postgres:10.0-alpine
      environment:
         POSTGRES_USER: 'user'
         POSTGRES_PASSWORD: 'pass'
         POSTGRES_DB: 'db'
Things I have tried -
1) unset the environment variable at the end of entrypoint-entrypoint.sh 
        for f in /docker-entrypoint-initdb.d/*; do
            case ""$f"" in
            *.sh)     echo ""$0: running $f""; . ""$f"" ;;
            *.sql)    echo ""$0: running $f""; ""${psql[@]}"" -f ""$f""; echo ;;
            *.sql.gz) echo ""$0: running $f""; gunzip -c ""$f"" | ""${psql[@]}""; echo ;;
            *)        echo ""$0: ignoring $f"" ;;
            esac
            echo
        done
        unset POSTGRES_USER
nothing happened though. :(
2) init.sql inside docker-entrypoint-initdb.d, to create db, user and pass without using env.
I shared the volume, as - 
```
   volumes:
       - ./docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
```
and, on my host, inside docker-entrypoint-initdb.d, I saved an init.sql as -
CREATE DATABASE docker_db;CREATE USER docker_user with encrypted password 'pass';GRANT ALL PRIVILEGES ON DATABASE docker_db TO docker_user;
I moved inside the running container and this file was there but, no user or database was created as mentioned in the file.
I have been stuck on this for past two days, any help is much appreciated.
",<postgresql><docker><docker-compose><dockerfile><docker-swarm>,1767,0,23,608,1,5,16,43,8922,0,83,5,13,2019-09-18 7:32,2019-09-18 7:38,2019-09-19 6:48,0,1,Basic,14
49391212,PostgreSQL: compare jsons,"As known, at the moment PostgreSQL has no method to compare two json values. The comparison like json = json doesn't work. But what about casting json to text before?
Then
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""b""}')::json::text
returns true
while
select ('{""x"":""a"", ""y"":""b""}')::json::text = 
('{""x"":""a"", ""y"":""d""}')::json::text
returns false
I tried several variants with more complex objects and it works as expected.
Are there any gotchas in this solution?
UPDATE:
The compatibility with v9.3 is needed
",<json><postgresql>,528,0,9,351,1,3,15,59,38607,0,4,3,13,2018-03-20 17:59,2018-03-20 19:51,2018-03-20 19:51,0,0,Basic,2
48554917,Getting Sequelize.js library to work on Amazon Lambda,"So I'm trying to run a lambda on amazon and narrowed down the error finally by testing the lambda in amazons testing console.
The error I got is this.
{
  ""errorMessage"": ""Please install mysql2 package manually"",
  ""errorType"": ""Error"",
  ""stackTrace"": [
    ""new MysqlDialect (/var/task/node_modules/sequelize/lib/dialects/mysql/index.js:14:30)"",
    ""new Sequelize (/var/task/node_modules/sequelize/lib/sequelize.js:234:20)"",
    ""Object.exports.getSequelizeConnection (/var/task/src/twilio/twilio.js:858:20)"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:679:25)"",
    ""__webpack_require__ (/var/task/src/twilio/twilio.js:20:30)"",
    ""/var/task/src/twilio/twilio.js:63:18"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:66:10)"",
    ""Module._compile (module.js:570:32)"",
    ""Object.Module._extensions..js (module.js:579:10)"",
    ""Module.load (module.js:487:32)"",
    ""tryModuleLoad (module.js:446:12)"",
    ""Function.Module._load (module.js:438:3)"",
    ""Module.require (module.js:497:17)"",
    ""require (internal/module.js:20:19)""
  ]
}
Easy enough, so I have to install mysql2.  So I added it to my package.json file.
{
  ""name"": ""test-api"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""handler.js"",
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 0""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""devDependencies"": {
    ""aws-sdk"": ""^2.153.0"",
    ""babel-core"": ""^6.26.0"",
    ""babel-loader"": ""^7.1.2"",
    ""babel-plugin-transform-runtime"": ""^6.23.0"",
    ""babel-preset-es2015"": ""^6.24.1"",
    ""babel-preset-stage-3"": ""^6.24.1"",
    ""serverless-domain-manager"": ""^1.1.20"",
    ""serverless-dynamodb-autoscaling"": ""^0.6.2"",
    ""serverless-webpack"": ""^4.0.0"",
    ""webpack"": ""^3.8.1"",
    ""webpack-node-externals"": ""^1.6.0""
  },
  ""dependencies"": {
    ""babel-runtime"": ""^6.26.0"",
    ""mailgun-js"": ""^0.13.1"",
    ""minimist"": ""^1.2.0"",
    ""mysql"": ""^2.15.0"",
    ""mysql2"": ""^1.5.1"",
    ""qs"": ""^6.5.1"",
    ""sequelize"": ""^4.31.2"",
    ""serverless"": ""^1.26.0"",
    ""serverless-plugin-scripts"": ""^1.0.2"",
    ""twilio"": ""^3.10.0"",
    ""uuid"": ""^3.1.0""
  }
}
I noticed when I do sls deploy however, it seems to only be packaging some of the modules?
Serverless: Package lock found - Using locked versions
Serverless: Packing external modules: babel-runtime@^6.26.0, twilio@^3.10.0, qs@^6.5.1, mailgun-js@^0.13.1, sequelize@^4.31.2, minimi
st@^1.2.0, uuid@^3.1.0
Serverless: Packaging service...
Serverless: Uploading CloudFormation file to S3...
Serverless: Uploading artifacts...
Serverless: Validating template...
Serverless: Updating Stack...
Serverless: Checking Stack update progress...
................................
Serverless: Stack update finished...
I think this is why it's not working.  In short, how do I get mysql2 library to be packaged correctly with serverless so my lambda function will work with the sequelize library?
Please note that when I test locally my code works fine.
My serverless file is below
service: testapi
# Use serverless-webpack plugin to transpile ES6/ES7
plugins:
  - serverless-webpack
  - serverless-plugin-scripts
  # - serverless-domain-manager
custom:
  #Define the Stage or default to Staging.
  stage: ${opt:stage, self:provider.stage}
  webpackIncludeModules: true
  #Define Databases Here
  databaseName: ""${self:service}-${self:custom.stage}""
  #Define Bucket Names Here
  uploadBucket: ""${self:service}-uploads-${self:custom.stage}""
  #Custom Script setup
  scripts:
    hooks:
      #Script below will run schema changes to the database as neccesary and update according to stage.
      'deploy:finalize':  node database-schema-update.js --stage ${self:custom.stage}
  #Domain Setup
  # customDomain:
  #    basePath: ""/""
  #    domainName: ""api-${self:custom.stage}.test.com""
  #    stage: ""${self:custom.stage}""
  #    certificateName: ""*.test.com""
  #    createRoute53Record: true
provider:
  name: aws
  runtime: nodejs6.10
  stage: staging
  region: us-east-1
  environment:
    DOMAIN_NAME: ""api-${self:custom.stage}.test.com""
    DATABASE_NAME: ${self:custom.databaseName}
    DATABASE_USERNAME: ${env:RDS_USERNAME}
    DATABASE_PASSWORD: ${env:RDS_PASSWORD}
    UPLOAD_BUCKET: ${self:custom.uploadBucket}
    TWILIO_ACCOUNT_SID: """"
    TWILIO_AUTH_TOKEN: """"
    USER_POOL_ID: """"
    APP_CLIENT_ID: """"
    REGION: ""us-east-1""
    IDENTITY_POOL_ID: """"
    RACKSPACE_API_KEY: """"
  #Below controls permissions for lambda functions.
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:UpdateTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource: ""arn:aws:dynamodb:us-east-1:*:*""
functions:
  create_visit:
    handler: src/visits/create.main
    events:
      - http:
          path: visits
          method: post
          cors: true
          authorizer: aws_iam
  get_visit:
    handler: src/visits/get.main
    events:
      - http:
          path: visits/{id}
          method: get
          cors: true
          authorizer: aws_iam
  list_visit:
    handler: src/visits/list.main
    events:
      - http:
          path: visits
          method: get
          cors: true
          authorizer: aws_iam
  update_visit:
    handler: src/visits/update.main
    events:
      - http:
          path: visits/{id}
          method: put
          cors: true
          authorizer: aws_iam
  delete_visit:
    handler: src/visits/delete.main
    events:
      - http:
          path: visits/{id}
          method: delete
          cors: true
          authorizer: aws_iam
  twilio_send_text_message:
    handler: src/twilio/twilio.send_text_message
    events:
      - http:
          path: twilio/sendtextmessage
          method: post
          cors: true
          authorizer: aws_iam
  #This function handles incoming calls and where to route it to.
  twilio_incoming_call:
    handler: src/twilio/twilio.incoming_calls
    events:
      - http:
          path: twilio/calls
          method: post
  twilio_failure:
    handler: src/twilio/twilio.twilio_failure
    events:
      - http:
          path: twilio/failure
          method: post
  twilio_statuschange:
    handler: src/twilio/twilio.statuschange
    events:
      - http:
          path: twilio/statuschange
          method: post
  twilio_incoming_message:
    handler: src/twilio/twilio.incoming_message
    events:
      - http:
          path: twilio/messages
          method: post
  twilio_whisper:
    handler: src/twilio/twilio.whisper
    events:
      - http:
          path: twilio/whisper
          method: post
      - http:
          path: twilio/whisper
          method: get
  twilio_start_call:
    handler: src/twilio/twilio.start_call
    events:
      - http:
          path: twilio/startcall
          method: post
      - http:
          path: twilio/startcall
          method: get
resources:
  Resources:
    uploadBucket:
       Type: AWS::S3::Bucket
       Properties:
         BucketName: ${self:custom.uploadBucket}
    RDSDatabase:
      Type: AWS::RDS::DBInstance
      Properties:
        Engine : mysql
        MasterUsername: ${env:RDS_USERNAME}
        MasterUserPassword: ${env:RDS_PASSWORD}
        DBInstanceClass : db.t2.micro
        AllocatedStorage: '5'
        PubliclyAccessible: true
        #TODO: The Value of Stage is also available as a TAG automatically which I may use to replace this manually being put here..
        Tags:
          -
            Key: ""Name""
            Value: ${self:custom.databaseName}
      DeletionPolicy: Snapshot
    DNSRecordSet:
      Type: AWS::Route53::RecordSet
      Properties:
        HostedZoneName: test.com.
        Name: database-${self:custom.stage}.test.com
        Type: CNAME
        TTL: '300'
        ResourceRecords:
        - {""Fn::GetAtt"": [""RDSDatabase"",""Endpoint.Address""]}
      DependsOn: RDSDatabase
UPDATE:: So I confirmed that running sls package --stage dev seems to create this in the zip folder that would eventually upload to AWS.  This confirms that serverless is not creating the package correctly with the mysql2 reference for some reason? Why is this?
webpack config file as requested
const slsw = require(""serverless-webpack"");
const nodeExternals = require(""webpack-node-externals"");
module.exports = {
  entry: slsw.lib.entries,
  target: ""node"",
  // Since 'aws-sdk' is not compatible with webpack,
  // we exclude all node dependencies
  externals: [nodeExternals()],
  // Run babel on all .js files and skip those in node_modules
  module: {
    rules: [
      {
        test: /\.js$/,
        loader: ""babel-loader"",
        include: __dirname,
        exclude: /node_modules/
      }
    ]
  }
};
",<amazon-web-services><aws-lambda><amazon-rds><serverless-framework><node-mysql2>,8812,1,276,8781,12,86,154,77,9087,0,2051,2,13,2018-02-01 3:33,2018-02-01 8:02,2018-02-01 8:02,0,0,Basic,6
48462011,Alembic migration: How to set server_onupdate in Alembic's alter_column function,"I'm trying to change a table column in PostgreSQL using Alembic but I don't know how to perform the needed update to apply the SQLAlchemy's server_onupdate property.
The column is:
changed = Column(ArrowType(timezone=True), server_default=utcnow(), primary_key=True)
I'm using the Arrowtype column type from SQLAlchemy_utils package (this is not a problem).
My intention is to create something like this:
changed = Column(ArrowType(timezone=True), **server_onupdate=utcnow()**, primary_key=True)
But using the Alembic function: alter_column
In the documentation there are only references to the server_default property but nothing about server_onupdate
Is there a way to achieve this?
Thanks
",<python><postgresql><sqlalchemy><alembic>,692,0,2,305,0,1,8,45,1575,0,46,1,13,2018-01-26 13:10,2023-02-22 20:59,,1853,,Basic,3
57795044,spring data JPA - mysql - findById() empty unless findAll() called before,"I'm struggling with this strange error: the findById() method of a CrudRepository returns Optional.empty, unless findAll() is called before when using mysql.
e.g.
User
@Entity
public class User {
    @Id
    @GeneratedValue
    private UUID id;
    public UUID getId() {
        return id;
    }
}
UserRepository
public interface UserRepository extends CrudRepository&lt;User, UUID&gt; { }
UserService
@Service
public class UserService {
    @Autowired
    private UserRepository userRepository;
    @Transactional
    public UUID create() {
        final User user = new User();
        userRepository.save(user);
        return user.getId();
    }
    @Transactional
    public User find(@PathVariable UUID userId) {
        // userRepository.findAll(); TODO without this functin call, Optinoal.empty is returned by the repo
        return userRepository.findById(userId).orElseThrow(() -&gt; new IllegalArgumentException(String.format(""missing user:%s"", userId)));
    }
}
UserApp
@SpringBootApplication
public class UserApp {
    private static final Logger LOG = LoggerFactory.getLogger(UserApp.class);
    @Autowired
    private UserService userService;
    @EventListener
    public void onApplicationEvent(ContextRefreshedEvent event) {
        final UUID userId = userService.create();
        final User user = userService.find(userId);
        LOG.info(""found user: {}"", user.getId());
    }
    public static void main(String[] args) {
        SpringApplication.run(UserApp.class, args);
    }
}
application.properties
spring.datasource.url=jdbc:mysql://localhost:3306/db_test
spring.datasource.username=springuser
spring.datasource.password=ThePassword
spring.jpa.hibernate.ddl-auto=create-drop
spring.jpa.show-sql=true
spring.jpa.database=mysql
Why does the findAll() method call change the result of findById()?
Edit: Hibernate logs with findAll:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_ from user user0_
Without:
Hibernate: drop table if exists user
Hibernate: create table user (id binary(255) not null, primary key (id)) engine=MyISAM
Hibernate: insert into user (id) values (?)
Hibernate: select user0_.id as id1_0_0_ from user user0_ where user0_.id=?
",<mysql><spring><spring-data-jpa><spring-data>,2337,0,74,6382,14,56,96,57,3551,0,68,4,13,2019-09-04 19:55,2019-09-25 22:01,,21,,Basic,9
48748070,SQLAlchemy connection via proxy,"I need to connect to existing database from SQLAlchemy via proxy.
        self.DB = {
            'drivername': 'oracle',
            'host': url,
            'port': port,
            'username': username,
            'password': password,
            'database': dbname
        }
        _engine = create_engine(URL(**self.DB))
        self.connection = _engine.connect()
I'm getting:
cx_Oracle.DatabaseError: ORA-12170: TNS:Connect timeout occurred
And I'm pretty sure I just need proxy because of my company policy. I couldn't find any tips in documentation how can I create connection via proxy.
",<python><proxy><sqlalchemy>,601,0,12,1271,0,14,35,66,2051,0,206,0,13,2018-02-12 13:51,,,,,Basic,3
54845280,How to interpret mysqldump output?,"My intent is to extract the triggers, functions, and stored procedures from a database, edit them, and add them to another database.
Below is a partial output from mysqldump.  I understand how the database is updated with the DROP, CREATE, andINSERT INTO statements, but don't understand the triggers.  I expected the following:
CREATE TRIGGER users_BINS BEFORE INSERT ON users
FOR EACH ROW
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
What does /*!50003 mean?  I thought it was some comment which would mean the CREATE for the trigger isn't present, but I must be misinterpreting the output. 
 How should one interpret a mysqldump output?
mysqldump -u username-ppassword --routines mydb
--
-- Table structure for table `users`
--
DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `users` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `idPublic` int(11) NOT NULL,
  `accountsId` int(11) NOT NULL,
  `firstname` varchar(45) NOT NULL,
  `lastname` varchar(45) NOT NULL,
  `email` varchar(45) NOT NULL,
  `username` varchar(45) NOT NULL,
  `password` char(255) NOT NULL COMMENT 'Password currently uses bcrypt and only requires 60 characters, but may change over time.',
  `tsCreated` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp(),
  `osTicketId` int(11) NOT NULL,
  `phone` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `uniqueEmail` (`accountsId`,`email`),
  UNIQUE KEY `uniqueUsername` (`accountsId`,`username`),
  KEY `fk_users_accounts1_idx` (`accountsId`),
  CONSTRAINT `fk_users_accounts1` FOREIGN KEY (`accountsId`) REFERENCES `accounts` (`id`) ON DELETE CASCADE ON UPDATE NO ACTION
) ENGINE=InnoDB AUTO_INCREMENT=35 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;
--
-- Dumping data for table `users`
--
LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES (xxx
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;
/*!50003 SET @saved_cs_client      = @@character_set_client */ ;
/*!50003 SET @saved_cs_results     = @@character_set_results */ ;
/*!50003 SET @saved_col_connection = @@collation_connection */ ;
/*!50003 SET character_set_client  = utf8 */ ;
/*!50003 SET character_set_results = utf8 */ ;
/*!50003 SET collation_connection  = utf8_general_ci */ ;
/*!50003 SET @saved_sql_mode       = @@sql_mode */ ;
/*!50003 SET sql_mode              = 'STRICT_TRANS_TABLES,STRICT_ALL_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,TRADITIONAL,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' */ ;
DELIMITER ;;
/*!50003 CREATE*/ /*!50017 DEFINER=`michael`@`12.34.56.78`*/ /*!50003 TRIGGER `users_BINS` BEFORE INSERT ON `users` FOR EACH ROW
BEGIN
if(IFNULL(NEW.idPublic, 0) = 0) THEN
   INSERT INTO _inc_accounts (type, accountsId, idPublic) values (""users"",NEW.accountsId,1)
   ON DUPLICATE KEY UPDATE idPublic = idPublic + 1;
   SET NEW.idPublic=(SELECT idPublic FROM _inc_accounts WHERE accountsId=NEW.accountsId AND type=""users"");
END IF;
END */;;
DELIMITER ;
/*!50003 SET sql_mode              = @saved_sql_mode */ ;
/*!50003 SET character_set_client  = @saved_cs_client */ ;
/*!50003 SET character_set_results = @saved_cs_results */ ;
/*!50003 SET collation_connection  = @saved_col_connection */ ;
",<mysql><triggers>,3622,0,72,25165,68,221,392,74,1604,0,1099,1,13,2019-02-23 19:21,2019-03-05 3:57,2019-03-05 3:57,10,10,Basic,5
58249954,JSON stringify and PostgreSQL bigint compliance,"I am trying to add BigInt support within my library, and ran into an issue with JSON.stringify.
The nature of the library permits not to worry about type ambiguity and de-serialization, as everything that's serialized goes into the server, and never needs any de-serialization.
I initially came up with the following simplified approach, just to counteract Node.js throwing TypeError: Do not know how to serialize a BigInt at me:
// Does JSON.stringify, with support for BigInt:
function toJson(data) {
    return JSON.stringify(data, (_, v) =&gt; typeof v === 'bigint' ? v.toString() : v);
}
But since it converts each BigInt into a string, each value ends up wrapped into double quotes.
Is there any work-around, perhaps some trick within Node.js formatting utilities, to produce a result from JSON.stringify where each BigInt would be formatted as an open value? This is what PostgreSQL understands and supports, and so I'm looking for a way to generate JSON with BigInt that's compliant with PostgreSQL.
Example
const obj = {
    value: 123n
};
console.log(toJson(obj));
// This is what I'm getting: {""value"":""123""}
// This is what I want: {""value"":123}
Obviously, I cannot just convert BigInt into number, as I would be losing information then. And rewriting the entire JSON.stringify for this probably would be too complicated.
UPDATE
At this point I have reviewed and played with several polyfills, like these ones:
polyfill-1
polyfill-2
But they all seem like an awkward solution, to bring in so much code, and then modify for BigInt support. I am hoping to find something more elegant.
",<node.js><postgresql><stringify><bigint>,1595,3,23,24653,15,116,141,43,15274,,1325,3,13,2019-10-05 15:54,2019-10-06 0:11,2019-10-06 0:11,1,1,Basic,9
53405317,Postgres changeset with column TEXT not working with Liquibase 3.6.2 and Postgres 9.6,"I am working with the new Spring Boot 2.1.0 version.  In Spring Boot 2.1.0, Liquibase was updated from 3.5.5 to 3.6.2.  I've noticed several things in my change sets are no long working.  
-- test_table.sql
CREATE TABLE test_table (
   id             SERIAL PRIMARY KEY,
   --Works fine as TEXT or VARCHAR with Liquibase 3.5 which is bundled with Spring Boot version 2.0.6.RELEASE
   --Will only work as VARCHAR with Liquibase 3.6.2 which is bundled with Spring Boot version 2.1.0.RELEASE and above
   worksheet_data TEXT
);
-- test_table.csv
id,worksheet_data
1,fff
-- Liquibase Changeset
    &lt;changeSet id=""DATA_01"" author=""me"" runOnChange=""false""&gt;
    &lt;loadData
            file=""${basedir}/sql/data/test_table.csv""
            tableName=""test_table""/&gt;
    &lt;/changeSet&gt;
This will not work.  I am presented with this odd stacktrace.  It complains it can't find liquibase/changelog/fff which I'm not referencing at all in the changeset.  The ""fff"" coincidentally matches the data value in table_test.csv.    
    org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: Invocation of init method failed; nested exception is liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
     Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1745) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:576) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:498) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:307) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1083) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:853) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:546) ~[spring-context-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142) ~[spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:316) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248) [spring-boot-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    at net.migov.amar.MiAmarApiApplication.main(MiAmarApiApplication.java:33) [classes/:na]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.1.RELEASE.jar:2.1.1.RELEASE]
    Caused by: liquibase.exception.MigrationFailedException: Migration failed for change set liquibase/changelog/data_nonprod.xml::DATA_NONPROD_02::scott_winters:
         Reason: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:78) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:202) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.Liquibase.update(Liquibase.java:179) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:353) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:305) ~[liquibase-core-3.6.2.jar:na]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1804) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1741) ~[spring-beans-5.1.3.RELEASE.jar:5.1.3.RELEASE]
        ... 23 common frames omitted
    Caused by: liquibase.exception.DatabaseException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
        at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:191) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.attachParams(ExecutablePreparedStatementBase.java:110) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.BatchDmlExecutablePreparedStatement.attachParams(BatchDmlExecutablePreparedStatement.java:51) ~[liquibase-core-3.6.2.jar:na]
        at liquibase.statement.ExecutablePreparedStatementBase.execute(ExecutablePreparedStatementBase.java:81) ~[liquibase-core-3.6.2.jar:na]
        at liquib
ase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:115) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600) ~[liquibase-core-3.6.2.jar:na]
    ... 31 common frames omitted
Caused by: java.io.FileNotFoundException: class path resource [liquibase/changelog/fff] cannot be resolved to URL because it does not exist
    at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:195) ~[spring-core-5.1.3.RELEASE.jar:5.1.3.RELEASE]
    at liquibase.integration.spring.SpringLiquibase$SpringResourceOpener.getResourcesAsStream(SpringLiquibase.java:556) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.getResourceAsStream(ExecutablePreparedStatementBase.java:281) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.toCharacterStream(ExecutablePreparedStatementBase.java:241) ~[liquibase-core-3.6.2.jar:na]
    at liquibase.statement.ExecutablePreparedStatementBase.applyColumnParameter(ExecutablePreparedStatementBase.java:184) ~[liquibase-core-3.6.2.jar:na]
    ... 38 common frames omitted
If I change TEXT to VARCHAR it works.  From my understanding these column types are the same in postgres, so I can work around this.  However, this is frustrating, and I don't see this new behavior documented.  From this link 3.6.2 is advertised as a ""drop in"" change (http://www.liquibase.org/2018/04/liquibase-3-6-0-released.html).
I would like to use the new features of Spring Boot 2.1.0, but I cannot specify liquibase 3.5.5 in my build because Spring Boot will complain about incompatible versions.  This is just one issue I'm seeing with changesets that worked in 3.5.5.  Maybe the folks at Spring should consider rolling back the version of liquibase.
Any advice on this matter would be greatly appreciated.  Thanks.
UPDATED
I have created a sample Spring Boot project to demonstrate this:  https://github.com/pcalouche/postgres-liquibase-text
",<postgresql><liquibase>,9651,4,74,1615,2,17,19,47,5159,0,153,1,13,2018-11-21 4:34,2018-12-27 10:26,2018-12-27 10:26,36,36,Advanced,32
52485862,How to use HASHBYTES function in SQL Server for multiple columns,"I have a requirement wherein I have to create hashvalue which consist of all columns of a table. With Checksum this can be done easily, but Checksum is not recommended as per Microsoft: 
  If at least one of the values in the expression list changes, the list checksum will probably change. However, this is not guaranteed. Therefore, to detect whether values have changed, we recommend the use of CHECKSUM only if your application can tolerate an occasional missed change. Otherwise, consider using HashBytes instead. With a specified MD5 hash algorithm, the probability that HashBytes will return the same result, for two different inputs, is much lower compared to CHECKSUM.
HASHBYTES accepts only 2 parameters (algorithm type, column)
Now the problem is even though HASHBYTES is more reliable compared to checksum but there doesn't seem to be an easy way to create it on multiple columns.
An example in the checksum,
create table dbo.chksum_demo1
(
    id int not null,
    name varchar(25),
    address varchar(250),
    HashValue as Checksum (id,name,address)
    CONSTRAINT PK_chksum_demo1 PRIMARY KEY (Id)
)
How can we do the above using Hashbytes instead of checksum? 
",<sql-server>,1178,0,13,813,3,15,32,54,38627,0,42,4,13,2018-09-24 19:12,2018-09-24 19:40,,0,,Basic,2
48409610,Share a dict with multiple Python scripts,"I'd like a unique dict (key/value) database to be accessible from multiple Python scripts running at the same time.
If script1.py updates d[2839], then script2.py should see the modified value when querying d[2839] a few seconds after.
I thought about using SQLite but it seems that concurrent write/read from multiple processes is not SQLite's strength (let's say script1.py has just modified d[2839], how would script2.py's SQLite connection know it has to reload this specific part of the database?)
I also thought about locking the file when I want to flush the modifications (but it's rather tricky to do), and use json.dump to serialize, then trying to detect the modifications, use json.load to reload if any modification, etc. ... oh no I'm reinventing the wheel, and reinventing a particularly inefficient key/value database!
redis looked like a solution but it does not officially support Windows, the same applies for leveldb.
multiple scripts might want to write at exactly the same time (even if this is a very rare event), is there a way to let the DB system handle this (thanks to a locking parameter? it seems that by default SQLite can't do this because ""SQLite supports an unlimited number of simultaneous readers, but it will only allow one writer at any instant in time."")
What would be a Pythonic solution for this?
Note: I'm on Windows, and the dict should have maximum 1M items (key and value both integers).
",<python><sqlite><dictionary><key-value-store>,1432,5,10,42385,103,394,707,54,3096,0,3760,8,13,2018-01-23 19:42,2018-01-23 20:09,2018-01-27 8:19,0,4,Intermediate,22
50310979,How to use SQLAlchemy's `one_or_none` query method?,"I'm trying to use the one_or_none query method to retrieve a record from my database but when I pass in a kwargs like I normally would with the filter_by method, it says it doesn't expect that keyword.
I tried going through the doc, but there's not description of the method's argument or an example.
",<python><sqlalchemy>,301,1,2,1679,6,18,31,50,20017,0,12,1,13,2018-05-12 21:54,2018-05-12 22:58,,0,,Basic,3
48777206,DROP TABLE IF EXISTS not working on Azure SQL Data Warehouse,"I used the SQL Server management studio to generate script against Azure Data Warehouse. I selected Edition Azure Data Warehouse, it generates below the script to drop table if it exists and create table. However, the script cannot pass validation. Please see below for the error message.
DROP TABLE IF EXISTS Table1
GO
Error message:  
  Parse error at line: 2, column: 12: Incorrect syntax near 'IF'.
",<azure-sql-database><azure-synapse>,403,0,2,317,1,6,18,77,17584,0,24,4,13,2018-02-13 23:12,2018-02-14 9:18,,1,,Basic,3
53473804,Build sqlproj on Azure DevOps,"I'm trying to use Azure DevOps Pipelines to build my .NET Core 2.1 solution from GitHub.  It includes a SQL project that has a TargetFrameworkVersion of v4.6.2.  This project always fails to build.
Build FAILED.
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj : warning NU1503: Skipping restore for project '/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj'. The project file may be invalid or missing targets required for restore. [/home/vsts/work/1/s/MySolution/MySolution.sln]
/home/vsts/work/1/s/MySolution/MyDatabase/MyDatabase.sqlproj(57,3): error MSB4019: The imported project ""/usr/share/dotnet/sdk/2.1.403/Microsoft/VisualStudio/v15.0/SSDT/Microsoft.Data.Tools.Schema.SqlTasks.targets"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
1 Warning(s)
1 Error(s)
How do I reference or include those targets for the build server?  It builds fine in VS2017.  I've spent more than a day hunting and cannot find any information on this problem.
",<build><azure-devops><azure-pipelines><sqlproj>,1039,0,7,12416,9,45,81,36,8624,,1275,3,13,2018-11-26 1:47,2018-11-27 5:24,2018-11-27 5:24,1,1,Intermediate,22
55566459,"C# Dynamic Linq: Implement ""Like"" in The Where Clause","So I want to make a general sorter for my data. I have this code to get data from the database which will extract the data only which contains value.
using System.Linq.Dynamic;
public static IQueryable&lt;object&gt; SortList(string searchString, Type modelType, 
    IQueryable&lt;object&gt; model)
{
    ....
    string toStringPredicate = type == typeof(string) ? propertyName + 
        "".Contains(@0)"" : propertyName + "".ToString().Contains(@0)"";
    model = model.Where(propertyName + "" != NULL AND "" + toStringPredicate, value);
}
The model is this:
public class ManageSubscriberItems
{
    public int? UserId { get; set; }
    public string Email { get; set; }
    public Guid SubscriberId { get; set; }
}
When I call:
models = (IQueryable&lt;ManageSubscriberItems&gt;)EcommerceCMS.Helpers.FilterHelper
    .SortList(searchString, typeof(ManageSubscriberItems), models);
if(models.Any())
It throws this error:
  ""LINQ to Entities does not recognize the method 'System.String
  ToString()' method, and this method cannot be translated into a store
  expression.""
EDIT
I found the problem, but I still cannot fix it. So if the property is not string, it will throw an error when calling .ToString().Contains(). 
model = model.Where(propertyName + "" != NULL AND "" + propertyName + 
    "".ToString().Contains(@0)"", value);
What I want is to implement LIKE in the query. Can anyone help me?
",<c#><sql-server><linq><dynamic><dynamic-linq>,1393,0,27,1953,2,23,64,38,6249,0,60,4,13,2019-04-08 4:53,2019-04-12 6:36,,4,,Basic,2
53257270,SQL over clause - dividing partition into numbered sub-partitions,"I have a challenge, that I've come across at multiple occasions but never been able to find an efficient solution to. Imagine I have a large table with data regarding e.g. bank accounts and their possible revolving moves from debit to credit:
AccountId DebitCredit AsOfDate
--------- ----------- ----------
aaa       d           2018-11-01
aaa       d           2018-11-02
aaa       c           2018-11-03
aaa       c           2018-11-04
aaa       c           2018-11-05
bbb       d           2018-11-02
ccc       c           2018-11-01
ccc       d           2018-11-02
ccc       d           2018-11-03
ccc       c           2018-11-04
ccc       d           2018-11-05
ccc       c           2018-11-06
In the example above I would like to assign sub-partition numbers to the combination of AccountId and DebitCredit where the partition number is incremented each time DebitCredit shifts. In other words in the example above I would like this result:
AccountId DebitCredit AsOfDate   PartNo
--------- ----------- ---------- ------
aaa       d           2018-11-01      1
aaa       d           2018-11-02      1
aaa       c           2018-11-03      2
aaa       c           2018-11-04      2
aaa       c           2018-11-05      2
bbb       d           2018-11-02      1
ccc       c           2018-11-01      1
ccc       d           2018-11-02      2
ccc       d           2018-11-03      2
ccc       c           2018-11-04      3
ccc       d           2018-11-05      4
ccc       c           2018-11-06      5
I cannot really figure out how to do it quickly and efficiently. The operation has to be done daily on a tables with millions of rows.
In this example it is guaranteed that we will have consecutive rows for all accounts. However, of course the customer might open an account the 15th in the month and/or close his account the 26th.
The challenge is to be solved on an MSSQL 2016 server, but a solution that would work on 2012 (and maybe even 2008r2) would be nice.
As you can imagine there's no way of telling whether there will only be debit or credit rows or whether the account will be revolving each day.
",<sql><sql-server><t-sql><sql-server-2016><ranking-functions>,2120,0,30,133,0,0,6,56,973,0,2,3,13,2018-11-12 7:02,2018-11-12 7:17,2018-11-12 8:17,0,0,Intermediate,17
50073853,Cannot upgrade server earlier than 5.7 to 8.0 in centos. server start fail,"I try to configure Mysql server on centos 7.4. After installing Mysql 8.0 to my system, systemctl restart mysqld failed.
See the error log /var/log/mysqld.log.
  [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.11) starting as process 320
  [ERROR] [MY-013168] [InnoDB] Cannot upgrade server earlier than 5.7 to 8.0
  [ERROR] [MY-011013] [Server] Failed to initialize DD Storage Engine.[ERROR] [MY-010020] [Server] Data Dictionary initialization failed.
  [ERROR] [MY-010119] [Server] Aborting
  [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.11)  MySQL Community Server - GPL.
",<mysql><centos>,619,0,0,139,0,1,4,78,6343,0,0,1,13,2018-04-28 7:02,2019-05-01 22:31,,368,,Basic,14
52038200,"Can not persist data model's field into database, but can retrieve it","I have a problem when trying to persist a data model class into a database. I have a class like this: 
class DataModelClass{
    //some more field etc.
    @Column(name = ""number1"", nullable = true)
    private Integer number1;
    @Column(name = ""number2"", nullable = true)
    private Integer number2;
    public DataModelClass(){}
    (...)
    public Integer getNumber2() {
        return number2;
    }
    public void setNumber2( Integer number2 ) {
        this.number2= number2;
    }
}
The second field was added after first one. When to persist object created with this class via:
em.persist(dataModelClass);
A new row in database is created, but only with first field added. The second one is empty. When I am debugging the object dataModelClass has every field set with some integer value.
When I am adding a value for number2 through pgAdmin, and then retrieving this row with java code via:
DataModelClass dmc = em.find(DataModelClass.class, 1);
Than dmc.getNumber2() is not empty/null.
Anyone have any ideas what is wrong?
[Edit]
Maybe it will help a little more, On data model (DataModelClass) class i got this annotation:
@Entity
@Table(name = ""custom_table"",
       uniqueConstraints=@UniqueConstraint(name=""UK_example_foreign_id"", columnNames={""example_foreign_id""})
)
@SequenceGenerator(name = DataModelClass.SEQ_NAME, sequenceName = DataModelClass.SEQ_NAME, allocationSize = 1)
Obviously this field exist in my class
",<java><postgresql><hibernate><jpa>,1438,0,31,2218,4,17,43,64,568,,80,2,13,2018-08-27 11:22,2018-09-04 11:17,2018-09-04 11:39,8,8,Basic,9
64354878,Convert UTF-8 varbinary(max) to varchar(max),"I have a varbinary(max) column with UTF-8-encoded text that has been compressed. I would like to decompress this data and work with it in T-SQL as a varchar(max) using the UTF-8 capabilities of SQL Server.
I'm looking for a way of specifying the encoding when converting from varbinary(max) to varchar(max). The only way I've managed to do that is by creating a table variable with a column with a UTF-8 collation and inserting the varbinary data into it.
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
SELECT SUBSTRING(Decompressed, 4, DATALENGTH(Decompressed) - 3) WithoutBOM
FROM
    (SELECT DECOMPRESS(RawResource) AS Decompressed FROM Resource) t
I'm wondering if there is a more elegant and efficient approach that does not involve inserting into a table variable.
UPDATE:
Boiling this down to a simple example that doesn't deal with byte order marks or compression:
I have the string &quot;Hello 😊&quot; UTF-8 encoded without a BOM stored in variable @utf8Binary
DECLARE @utf8Binary varbinary(max) = 0x48656C6C6F20F09F988A
Now I try to assign that into various char-based variables and print the result:
DECLARE @brokenVarChar varchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenVarChar = ' + @brokenVarChar
DECLARE @brokenNVarChar nvarchar(max) = CONVERT(varchar(max), @utf8Binary)
print '@brokenNVarChar = ' +  @brokenNVarChar 
DECLARE @rv TABLE(
    Res varchar(max) COLLATE Latin1_General_100_CI_AS_SC_UTF8 
)
INSERT INTO @rv
select @utf8Binary
DECLARE @working nvarchar(max)
Select TOP 1 @working = Res from @rv
print '@working = ' + @working
The results of this are:
@brokenVarChar = Hello ðŸ˜Š
@brokenNVarChar = Hello ðŸ˜Š
@working = Hello 😊
So I am able to get the binary result properly decoded using this indirect method, but I am wondering if there is a more straightforward (and likely efficient) approach.
",<sql-server><t-sql>,1891,0,30,148,0,1,6,55,4092,0,5,4,13,2020-10-14 13:54,2020-10-14 16:00,2020-10-14 16:00,0,0,Basic,9
54946697,psycopg2 - Inserting list of dictionaries into PosgreSQL database. Too many executions?,"I am inserting a list of dictionaries to a PostgreSQL database. The list will be growing quickly and the number of dict values (columns) is around 30. The simplified data:
projects = [
{'name': 'project alpha', 'code': 12, 'active': True},
{'name': 'project beta', 'code': 25, 'active': True},
{'name': 'project charlie', 'code': 46, 'active': False}
]
Inserting the data into the PostgreSQL database with the following code does work (as in this answer), but I am worried about executing too many queries.
for project in projects:
    columns = project.keys()
    values = project.values()
    query = &quot;&quot;&quot;INSERT INTO projects (%s) VALUES %s;&quot;&quot;&quot;
    # print(cursor.mogrify(query, (AsIs(','.join(project.keys())), tuple(project.values()))))
    cursor.execute(query, (AsIs(','.join(columns)), tuple(values)))
conn.commit()
Is there a better practice? Thank you so much in advance for your help!
",<python><postgresql><psycopg2>,924,1,16,2570,3,20,40,53,8627,0,1616,3,13,2019-03-01 14:32,2019-03-01 15:13,2019-03-01 17:47,0,0,Basic,9
50799157,Use a CTE to UPDATE or DELETE in MySQL,"The new version of MySQL, 8.0, now supports Common Table Expressions.
According to the manual:
A WITH clause is permitted at the beginning of SELECT, UPDATE, and DELETE statements:
WITH ... SELECT ...
WITH ... UPDATE ...
WITH ... DELETE ...
So, I thought, given the following table:
ID lastName firstName
----------------------
1  Smith    Pat
2  Smith    Pat
3  Smith    Bob
I can use the following query:
WITH ToDelete AS 
(
   SELECT ID,
          ROW_NUMBER() OVER (PARTITION BY lastName, firstName ORDER BY ID) AS rn
   FROM mytable
)   
DELETE FROM ToDelete;
in order to delete duplicates from the table, just like I could do in SQL Server.
It turns out I was wrong. When I try to execute the DELETE stament from MySQL Workbench I get the error:
Error Code: 1146. Table 'todelete' doesn't exist
I also get an error message when I try to do an UPDATE using the CTE.
So, my question is, how could one use a WITH clause in the context of an UPDATE or DELETE statement in MySQL (as cited in the manual of version 8.0)?
",<mysql><sql-update><common-table-expression><sql-delete><mysql-8.0>,1021,2,20,71571,9,63,98,50,13195,0,4808,2,13,2018-06-11 13:41,2018-06-11 13:48,2018-06-11 13:48,0,0,Basic,9
52375300,Count(*) vs Count(id) speed,"I know they return different results (the first counts nulls, the latter not). That's not my question. Just imagine a case where I don't care (either because there are no nulls, or because there are only a few and I only want a general sense of the number of rows in the database).
My question is about the following (presumable) contradiction:
Here  one of the highest rep users in the SQL tag says 
  Your use of COUNT(*) or COUNT(column) should be based on the
  desired output only.
On the other hand, here is a 47 times upvoted comment saying 
  ... if you have a non-nullable column such as ID, then count(ID) will
  significantly improve performance over count(*).
The two seem to contradict each other. So can someone please explain to me why is whatever the correct one correct?
",<sql><sql-server>,788,2,2,26768,38,141,295,50,14150,0,2952,2,13,2018-09-17 20:33,2018-09-17 20:41,,0,,Intermediate,23
59283754,STRING_AGG with line break,"DROP TABLE IF EXISTS items;
CREATE TABLE items (item varchar(20));
INSERT INTO items VALUES ('apple'),('raspberry');
SELECT STRING_AGG(item, CHAR(13)) AS item_list FROM items;
How do I get a line break between items ?
",<sql><sql-server><t-sql><string-aggregation>,218,1,4,9734,4,17,28,50,16293,,245,4,13,2019-12-11 10:33,2019-12-11 11:02,2019-12-11 11:40,0,0,Basic,2
61576670,Databases in psql Don't Show up in PgAdmin4,"I installed Postgres 
and followed the instruction. I create a database and logged in by the master password but I don't find the database even the + mark is not shown in the servers. can anyone help, please?
",<postgresql><pgadmin>,209,1,0,151,1,2,8,62,15936,0,3,3,13,2020-05-03 15:04,2020-06-28 5:22,,56,,Basic,14
52302676,"Hibernate entity query for finding the most recent, semi-unique row, in a single table","I have a Hibernate database with a single table that looks like:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     4            Tape        09-10-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
     7           Pencil       09-08-2018         Allen           Supplies
And I want to return only the newest purchases, based on some other limitations. For example:
List&lt;Purchase&gt; getNewestPurchasesFor(Array&lt;String&gt; productNames, Array&lt;String&gt; purchaserNames) { ... }
Could be called using:
List&lt;Purchase&gt; purchases = getNewestPurchasesFor([""Notebook"", ""Pencil""], [""Bob"", ""Steve""]);
In English, ""Give me the newest purchases, for either a Notebook or Pencil, by either Bob or Steve.""
And would provide:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME
-----------------------------------------------------------
     1          Notebook      09-07-2018          Bob            
     3           Pencil       09-06-2018          Bob            
     5           Pencil       09-09-2018         Steve           
So it's like a ""distinct"" lookup on multiple columns, or a ""limit"" based on some post-sorted combined-column unique key, but all the examples I've found show using the SELECT DISTINCT(PRODUCT_NAME, PURCHASER_NAME) to obtain those columns only, whereas I need to use the format:
from Purchases as entity where ... 
So that the model types are returned with relationships intact.
Currently, my query returns me all of the old purchases as well:
PURCHASE_ID | PRODUCT_NAME | PURCHASE_DATE | PURCHASER_NAME | PRODUCT_CATEGORY
------------------------------------------------------------------------------
     1          Notebook      09-07-2018          Bob            Supplies
     2          Notebook      09-06-2018          Bob            Supplies
     3           Pencil       09-06-2018          Bob            Supplies
     5           Pencil       09-09-2018         Steve           Supplies
     6           Pencil       09-06-2018         Steve           Supplies
Which, for repeat purchases, causes quite the performance drop.
Are there any special keywords I should be using to accomplish this? Query languages and SQL-fu are not my strong suits.
Edit:
Note that I'm currently using the Criteria API, and would like to continue doing so.
Criteria criteria = session.createCriteria(Purchase.class);
criteria.addOrder(Order.desc(""purchaseDate""));
// Product names
Criterion purchaseNameCriterion = Restrictions.or(productNames.stream().map(name -&gt; Restrictions.eq(""productName"", name)).toArray(Criterion[]::new));
// Purchaser
Criterion purchaserCriterion = Restrictions.or(purchaserNames.stream().map(name -&gt; Restrictions.eq(""purchaser"", name)).toArray(Criterion[]::new));
// Bundle the two together
criteria.add(Restrictions.and(purchaseNameCriterion, purchaserCriterion));
criteria.list(); // Gives the above results
If I try to use a distinct Projection, I get an error:
ProjectionList projections = Projections.projectionList();
projections.add(Projections.property(""productName""));
projections.add(Projections.property(""purchaser""));
criteria.setProjection(Projections.distinct(projections));
Results in:
17:08:39 ERROR Order by expression ""THIS_.PURCHASE_DATE"" must be in the result list in this case; SQL statement:
Because, as mentioned above, adding a projection/distinct column set seems to indicate to Hibernate that I want those columns as a result/return value, when what I want is to simply limit the returned model objects based on unique column values.
",<java><sql><hibernate>,3998,0,41,31434,32,138,234,47,1029,0,899,10,13,2018-09-12 20:06,2018-09-12 20:56,,0,,Advanced,32
54479941,How to init MySql Database in Docker Compose,"Scenario:
I developed a microservice in Spring which uses a mysql 8 database.
This db has to be initalized (create a Database, some tables and data).
On my host machine I initialized the database with data.sql and schema.sql script.
The Problem is, that I have to set:
spring.datasource.initialization-mode=always
for the first execute. This initializes my db the way I want to.
For future runs I have to comment this command. Very ugly soltion but I could not find a better one and I got no reponse right now to this question.
I thought for testing it is ok but I definetly have to improve that.
Currently I want to run my service with docker by a docker compose.
Expected:
This is the docker-compose file. Fairly simple. I'm totally new in the world of docker and so I want to go on step by step.
version: '3' 
services:usermanagement-service:
build:
  ./UserManagementService
restart:
  on-failure
ports:
  - ""7778:7778""
links:
  - mysqldb
depends_on:
  - mysqldb   mysqldb:
build:
  ./CustomMySql
volumes:
  - ./mysql-data:/var/lib/mysql
restart:
  on-failure
environment:
  MYSQL_ROOT_PASSWORD: root
  MYSQL_DATABASE: userdb
  MYSQL_USER: testuser
  MYSQL_PASSWORD: testuser
expose:
  - ""3600""
I was expecting, that my database gets initialized with a user and that in the first run my microservice initializes the db with data.
So before the next start of compose I have to comment the command and rebuild the image. (I know , ugly)
Problem:
So besides this ugly solution I run into runtime problems. 
On docker-compose up my Microservice is faster than the init of the database. So it tries to call the db what results in en error.
Because of the restart on failure the microservice comes up again.
Now it works because the init of the db has finished.
Solution:
I searched the www and for it seems like a known problem which might be solved within a wait-for-it.sh file. This has to be included with COPY in the Dockerfile.
So I'm no expert but I am searching for a good solution to either:
init database from within spring und make my service wait till mysql is ready
or init the database from withn my container via a volume on the first run and of course solve this init problem.
I don't know what is best practice here I and I would be very thankful for some help how to build this up.
",<mysql><spring-boot><docker><docker-compose><init>,2298,1,29,2102,3,30,53,57,21435,0,490,1,13,2019-02-01 12:52,2019-02-01 13:38,2019-02-01 13:38,0,0,Advanced,32
48382457,MYSQL JSON column change array order after saving,"I am using JSON column type in MySQL database table. When I try to save JSON values in table column, the JSON array automatically re-order(shuffle) 
I have following JSON:
{""TIMER_HEADER"": [{""XX!TIMERHDR"": ""XXTIMERHDR"", ""VER"": "" 7"", ""REL"": "" 0"", ""COMPANYNAME"": ""XXX"", ""IMPORTEDBEFORE"": ""N"", ""FROMTIMER"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""XX!HDR"": ""XXHDR"", ""PROD"": ""Qics for Wnows"", ""VER"": ""Version 6.0"", ""REL"": ""Release R"", ""IIFVER"": ""1"", ""DATE"": ""2018-01-20"", ""TIME"": ""1516520267"",   ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
After saving in JSON-column of MySql table, this becomes:
{""TIMER_HEADER"": [{""REL"": "" 0"", ""VER"": "" 7"", ""FROMTIMER"": ""N"", ""COMPANYNAME"": ""XXX"", ""XX!TIMERHDR"": ""XXTIMERHDR"", ""IMPORTEDBEFORE"": ""N"", ""COMPANYCREATETIME"": ""12423426""}, {""REL"": ""Release R"", ""VER"": ""Version 6.0"", ""DATE"": ""2018-01-20"", ""PROD"": ""Qics for Wnows"", ""TIME"": ""1516520267"", ""IIFVER"": ""1"", ""XX!HDR"": ""XXHDR"", ""ACCNTNT"": ""N"", ""ACCNTNTSPLITTIME"": ""0""}], ""COLUMN_HEADER"": [""!TIMEACT"", ""DATE"", ""JOB"", ""EMP"", ""ITEM"", ""PITEM"", ""DURATION"", ""PROJ"", ""NOTE"", ""BILLINGSTATUS""]}
I need the same order as I have original, because there is an validation on 3rd party.
Please help. Thanks.
",<mysql><json><mysql-json>,1292,0,2,1211,0,19,32,70,7443,0,45,2,13,2018-01-22 13:15,2018-01-22 13:31,,0,,Basic,9
57165310,Create a DATETIME column in SQLite FLutter database?,"I create a TABLE with some columns and everything's ok until I tried to insert a new DATETIME column:
_onCreate(Database db, int version) async {
await db.
execute(""CREATE TABLE $TABLE ($ID INTEGER PRIMARY KEY, $NAME TEXT, $COLOUR 
TEXT, $BREED TEXT, $BIRTH DATETIME)"");
}
My Model Class to store is:
class Pet {
 int id;
 String name;
 DateTime birth;
 String breed;
 String colour;
 Pet({this.id, this.name, this.birth, this.colour, this.breed});
 }
In the controller, I tried to store a new instance of Pet, instantiating a new variable DateTime _date = new DateTime.now(); and saving all in
Pet newPet = Pet(
      id: null,
      name: _name,
      colour: _colour,
      breed: _breed,
      birth: _date
  );
But when I insert in the database I receive:
  Unhandled Exception: Invalid argument: Instance of 'DateTime'
",<sqlite><datetime><flutter><instance>,825,0,24,523,3,7,18,54,25011,0,22,1,13,2019-07-23 13:30,2019-07-23 13:33,2019-07-23 13:33,0,0,Basic,3
61144337,How to get insert id after save to database in CodeIgniter 4,"I'm using Codeigniter 4. 
And inserting new data like this,
$data = [
        'username' =&gt; 'darth',
        'email'    =&gt; 'd.vader@theempire.com'
];
$userModel-&gt;save($data);
Which is mentioned here: CodeIgniter’s Model reference
It's doing the insertion. 
But I haven't found any reference about to get the inserted id after insertion.
Please help! Thanks in advance. 
",<php><mysql><codeigniter-4>,379,1,6,636,1,10,22,47,49589,0,42,10,13,2020-04-10 16:19,2020-04-11 9:32,2020-04-11 9:32,1,1,Basic,3
51619861,Sql left join on left key with null values,"I have a question about join with key with null value. 
Suppose I have a table t, which is going to be on left side. (id is primary key and sub_id is the key to join with the right table.)
    id sub_id value
    1  3       23
    2  3       234
    3  2       245
    4  1       12
    5  null    948
    6  2       45
    7  null    12
and I have another table m which is on right side. (t.sub_id = m.id)
    id feature
    1  9       
    2  8       
    3  2       
    4  1       
    5  4    
    6  2       
    7  null
Now I want to use 
    select * from t left join m on t.sub_id = m.id   
What result will it return? Is Null value in left key influence the result? I want all null left key rows not to shown in my result.
Thank you!  
",<sql><null><left-join>,746,0,24,2707,5,18,30,79,85700,0,34,4,13,2018-07-31 18:27,2018-07-31 18:30,2018-07-31 18:30,0,0,Basic,10
60421158,Would it be possible to have multiple database connection pools in rails to switch between?,"A little background
I have been using the Apartment gem for running a multi-tenancy app for years. Now recently the need to scale the database out into separate hosts has arrived, the db server simply can't keep up any more (both reads and writes are getting too too much) - and yes, I scaled the hardware to the max (dedicated hardware, 64 cores, 12 Nvm-e drives in raid 10, 384Gb ram etc.). 
I was considering doing this per-tenant (1 tenant = 1 database connection config / pool) as that would be a ""simple"" and efficient way to get up to number-of-tenants-times more capacity without doing loads of application code changes. 
Now, I am running rails 4.2 atm., soon upgrading to 5.2. I can see that rails 6 adds support for a per-model connection definitions, however that is not really what I need, as I have a completely mirrored database schema for each of my 20 tenants. Typically I switch ""database"" per request (in middleware) or per background job (sidekiq middleware), however this is currently trivial and handled ny the Apartment gem, as it just sets the search_path in Postgresql and does not really change the actual connection. When switching to a per-tenant hosting strategy I will need to switch the entire connection per request.
Questions:
I understand that I could do an ActiveRecord::Base.establish_connection(config) per request / background job - however, as I also understand, that triggers an entirely new database connection handshake to be made and a new db pool to spawn in rails - right? I guess that would be a performance suicide to make that kind of overhead on every single request to my application.
I am therefore wondering if anyone can see the option with rails of e.g. pre-establishing multiple (total of 20) database connections/pools from the beginning (e.g. on boot of the application), and then just switch between those pools per request? So that he db connections are already made and ready to be used.
Is all this just a poor poor idea, and should I instead look for a different approach? E.g. 1 app instance = one specific connection to one specific tenant. Or something else. 
",<ruby-on-rails><ruby><postgresql><multi-tenant>,2125,1,3,8613,11,59,118,44,2129,0,228,3,13,2020-02-26 19:28,2020-02-27 16:52,2020-03-03 5:06,1,6,Intermediate,21
53226642,SQLite3 database is Locked in Azure,"I have a Flask server Running on Azure provided by Azure App services with sqlite3 as a database. I am unable to update sqlite3 as it is showing that database is locked 
    2018-11-09T13:21:53.854367947Z [2018-11-09 13:21:53,835] ERROR in app: Exception on /borrow [POST]
    2018-11-09T13:21:53.854407246Z Traceback (most recent call last):
    2018-11-09T13:21:53.854413046Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 2292, in wsgi_app
    2018-11-09T13:21:53.854417846Z     response = self.full_dispatch_request()
    2018-11-09T13:21:53.854422246Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    2018-11-09T13:21:53.854427146Z     rv = self.handle_user_exception(e)
    2018-11-09T13:21:53.854431646Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1718, in handle_user_exception
    2018-11-09T13:21:53.854436146Z     reraise(exc_type, exc_value, tb)
    2018-11-09T13:21:53.854440346Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/_compat.py"", line 35, in reraise
    2018-11-09T13:21:53.854444746Z     raise value
    2018-11-09T13:21:53.854448846Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    2018-11-09T13:21:53.854453246Z     rv = self.dispatch_request()
    2018-11-09T13:21:53.854457546Z   File ""/home/site/wwwroot/antenv/lib/python3.7/site-packages/flask/app.py"", line 1799, in dispatch_request
    2018-11-09T13:21:53.854461846Z     return self.view_functions[rule.endpoint](**req.view_args)
    2018-11-09T13:21:53.854466046Z   File ""/home/site/wwwroot/application.py"", line 282, in borrow
    2018-11-09T13:21:53.854480146Z     cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    2018-11-09T13:21:53.854963942Z sqlite3.OperationalError: database is locked
Here is the route - 
@app.route('/borrow',methods=[""POST""])
def borrow():
    # import pdb; pdb.set_trace()
    body = request.get_json()
    user_id = body[""userid""]
    bookid = body[""bookid""]
    conn = sqlite3.connect(""database.db"")
    cursor = conn.cursor()
    date = datetime.now()
    expiry_date = date + timedelta(days=30)
    cursor.execute(""UPDATE books SET stock = stock - 1 WHERE bookid = ?"",(bookid,))
    # conn.commit()
    cursor.execute(""INSERT INTO borrowed (issuedate,returndate,memberid,bookid) VALUES (?,?,?,?)"",(""xxx"",""xxx"",user_id,bookid,))
    conn.commit()
    cursor.close()
    conn.close()
    return json.dumps({""status"":200,""conn"":""working with datess update""})
I tried checking the database integrity using pragma. There was no integrity loss. So I don't know what might be causing that error. Any help is Appreciated :)
",<sql><azure><sqlite>,2796,0,35,363,0,2,12,67,6387,0,146,6,13,2018-11-09 13:29,2019-01-05 10:20,,57,,Basic,9
53783736,What is the difference between Azure SQL Database Elastic Pools and Azure SQL Database Managed Instance?,"Azure SQL Database has two similar flavors - Managed Instance and Elastic pools. Both flavors enables placing multiple databases that share the same resources and in both cases can be changed cpu/storage for entire group of database within the instance/pool. What is the difference between them?
",<azure><azure-sql-database><azure-sql-managed-instance>,296,0,0,13452,4,40,56,55,19771,0,21,1,13,2018-12-14 16:41,2018-12-14 16:41,2018-12-14 16:41,0,0,Basic,8
63256680,Adding an array of integers as a data type in a Gorm Model,"I am trying to save an array of numbers in a single postgresql field using Gorm.
The array needs to be a list with between 2 &amp; 13 numbers: [1, 2, 3, 5, 8, 13, 21, 40, 1000]
Everything was working when saving a single int64. When I tried changing the model to account for an array of int64's it gives me the following error:
&quot;panic: invalid sql type  (slice) for postgres&quot;
my Gorm model is:
type Game struct {
    gorm.Model
    GameCode    string
    GameName    string
    DeckType    []int64
    GameEndDate string
}
Update based on answer from @pacuna. I tried the suggested code and I get a similar error.
&quot;panic: invalid sql type Int64Array (slice) for postgres&quot;
Here is the full code block:
package main
import (
    &quot;fmt&quot;
    &quot;github.com/jinzhu/gorm&quot;
    _ &quot;github.com/jinzhu/gorm/dialects/postgres&quot;
    pq &quot;github.com/lib/pq&quot;
)
var db *gorm.DB
// Test -- Model for Game table
type Test struct {
    gorm.Model                                           
    GameCode    string                                      
    GameName    string                                      
    DeckType    pq.Int64Array    
    GameEndDate string   
}
func main() {
    db, err := gorm.Open(&quot;postgres&quot;, &quot;host=localhost port=5432 user=fullstack dbname=scratch_game sslmode=disable&quot;)
    if err != nil {
        fmt.Println(err.Error())
        panic(&quot;Failed to connect to database...&quot;)
    }
    defer db.Close()
    dt := []int64{1, 2, 3}
    db.AutoMigrate(&amp;Test{})
    fmt.Println(&quot;Table Created&quot;)
    db.Create(&amp;Test{GameCode: &quot;xxx&quot;, GameName: &quot;xxx&quot;, DeckType: pq.Int64Array(dt), GameEndDate: &quot;xxx&quot;})
    fmt.Println(&quot;Record Added&quot;)
}
",<postgresql><go><go-gorm>,1783,0,47,192,1,1,12,54,21620,0,5,1,13,2020-08-05 0:12,2020-08-05 1:04,2020-08-05 1:04,0,0,Basic,8
48243934,Mocking Postgresql now() function for testing,"I have the following stack
Node/Express backend
Postgresql 10 database
Mocha for testing
Sinon for mocking
I have written a bunch of end-to-end tests to test all my webservices. The problem is that some of them are time dependent (as in ""give me the modified records of the last X seconds""). 
sinon is pretty good at mocking all the time/dated related stuff in Node, however I have a modified field in my Postgresql tables that is populated with a trigger:
CREATE FUNCTION update_modified_column()
  RETURNS TRIGGER AS $$
BEGIN
  NEW.modified = now();
  RETURN NEW;
END;
$$ LANGUAGE 'plpgsql';
The problem of course is that sinon can't override that now() function.
Any idea on how I could solve this? The problem is not setting a specific date at the start of the test, but advancing time faster than real-time (in one of my tests I want to change some stuff in the database, advance the 'current time' with one day, change some more stuff in the database and do webservice calls to see the result).
I can figure out a few solutions myself, but they all involve changing the application code and making it less elegant. I don't think your application code should be impacted by the fact that you want to test it.
",<node.js><postgresql><mocha.js><sinon>,1214,0,11,6044,6,43,70,72,4163,,692,4,13,2018-01-13 20:18,2018-01-13 22:01,,0,,Basic,10
53295322,PostgreSQL statement timeout,"PostgreSQL Version: 9.3
We have online system which gets transnational data (approximately 15000 records per day).  
We have table partitioning on date &amp; time and have a PostgreSQL function to load the incoming request into the table.
Sometimes we see the error message
  ERROR: 57014: canceling statement due to statement timeout
The client sends the request again after some time if not successful and on second try it gets recorded successfully.  It seems this has to be something with table locks but I am not sure.
",<postgresql>,524,0,0,131,1,1,5,76,38728,0,0,2,13,2018-11-14 7:50,2018-11-14 11:33,,0,,Basic,9
52066085,System.Linq.Expressions exception thrown when using FirstOrDefault in .Net Core 2.1,"I am receiving ~300+ exceptions that are spammed in my server output labeled:
Exception thrown: 'System.ArgumentException' in System.Linq.Expressions.dll
The query I am using is as follows:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
Eventually the exceptions stop generating, it shows a large query in the output window, and everything continues as normal.
If I change the query to the following I do not experience the exception:
IQueryable&lt;Account&gt; account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     Where(a =&gt; a.FacebookUserID == facebookUserID);
However, if I call anything such as First, FirstOrDefault, Single, etc on the IQueryable&lt;Account&gt; variable the exceptions start up again and then stop after ~300.
These exceptions are stalling user logins upwards of 30 seconds or more. The duration of exceptions grows with the amount of data being returned from the database.
I use the Account object by passing it around the server to perform varying maintenance tasks on it and then eventually sending the object client-side where I have it being deserialized into the client-side version of the Account class.
Does anyone know what could be causing these internal exceptions and how I might be able to eliminate or mitigate them? 
Here is my output log:
Below is the exception message:
The AccountStatistics isn't listed in the query above because there are about 20 some includes and I shorthanded the include list for brevity.
Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountStatistics]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountStatistics]'
There is no inner exception.
I double checked my database and I have an entry for the user and all of their fields are filled with valid data.
Account Class (Edited for brevity)
public class Account
    {
        [Key]
        public int ID { get; set; }
        public DateTime CreationDate { get; set; }
        public AccountCurrency Currency { get; set; }
        public AccountProgression Progression { get; set; }
        public AccountSettings Settings { get; set; }
        public AccountStatistics Statistics { get; set; }
        public ICollection&lt;AccountFriendEntry&gt; Friends { get; set; }
        public ICollection&lt;AccountUnlockedGameEntry&gt; Unlocks{ get; set; }
    }
Account Statistics class
public class AccountStatistics
{
    [Key]
    public int AccountID { get; set; }
    public int LoginCount { get; set; }
    public DateTime LastLoginTime { get; set; }
    public DateTime LastActivityTime { get; set; }
}
Edit
Keys for the Account Statistics table
   migrationBuilder.CreateTable(
            name: ""AccountStatistics"",
            columns: table =&gt; new
            {
                AccountID = table.Column&lt;int&gt;(nullable: false),
                LoginCount = table.Column&lt;int&gt;(nullable: false),
                LastLoginTime = table.Column&lt;DateTime&gt;(nullable: false),
                CreationDate = table.Column&lt;DateTime&gt;(nullable: false)
            },
            constraints: table =&gt;
            {
                table.PrimaryKey(""PK_AccountStatistics"", x =&gt; x.AccountID);
                table.ForeignKey(
                    name: ""FK_AccountStatistics_Accounts_AccountID"",
                    column: x =&gt; x.AccountID,
                    principalTable: ""Accounts"",
                    principalColumn: ""ID"",
                    onDelete: ReferentialAction.Cascade);
            });
Edit 9001
After doing some testing I've realized the exception only occurs when chaining includes.
This will cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     Include(i =&gt; i.Unlocks).
     Include(i =&gt; i.Settings).
     Include(i =&gt; i.Friends).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
This will NOT cause an exception:
Account account = _accountContext.Account.
     Include(i =&gt; i.Currency).
     FirstOrDefault(a =&gt; a.FacebookUserID == facebookUserID);
It does not matter if its currency and unlock, friends and currency, settings, and statistics. Any combination of includes (2 or more) causes the exception to happen.
Edit 9002
Here are my results of the following query:
var acct = _accountContext.Account
     .Where(a =&gt; a.FacebookUserID == facebookUserID)
     .Select(x =&gt; new { Account = x, x.Currency, x.Settings }).ToList();
Exception:
System.ArgumentException: 'Field 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,System.Collections.Generic.IEnumerable`1[Project.Models.AccountSettings]].Inner' is not defined for type 'Microsoft.EntityFrameworkCore.Query.EntityQueryModelVisitor+TransparentIdentifier`2[Project.Models.Account,Project.Models.AccountSettings]''
I feel like this is treating AccountSettings as a collection when it's a single field reference.
Edit Final:
I never found a fix for this issue. I re-created all the tables and such in another environment and it works just fine. Not a very ideal solution to blow away all tables, classes, and migrations, but it's the only thing that fixed the issue.
",<c#><sql-server><entity-framework>,5685,2,74,301,0,2,10,60,5118,0,0,4,13,2018-08-28 20:56,2018-11-01 10:31,,65,,Basic,10
51632735,JDBC result set retrieve LocalDateTime,"I run a simple query to retrieve a row from a MySQL database.
I get ResultSet and I need to retrieve a LocalDateTime object from it.
My DB table.
CREATE TABLE `some_entity` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `title` varchar(45) NOT NULL,
  `text` varchar(255) DEFAULT NULL,
  `created_date_time` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `id_UNIQUE` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
I need to retrieve some entity by id.
String SELECT = ""SELECT ID, TITLE, TEXT, CREATED_DATE_TIME FROM some_entity WHERE some_entity.id = ?"";
PreparedStatement selectPreparedStatement = connection.prepareStatement(SELECT);
try {
    selectPreparedStatement.setLong(1, id);
    ResultSet resultSet = selectPreparedStatement.executeQuery();
    if (resultSet.next()) {
        Long foundId = resultSet.getLong(1);
        String title = resultSet.getString(2);
        String text = resultSet.getString(3);
        LocalDateTime createdDateTime = null;// How do I retrieve it???
    }
} catch (SQLException e) {
    throw new RuntimeException(""Failed to retrieve some entity by id."", e);
}
",<java><mysql><jdbc><java-time>,1123,0,23,12375,15,79,114,50,22758,0,3438,1,13,2018-08-01 11:59,2018-08-01 12:05,2018-08-01 12:05,0,0,Advanced,35
52043874,Limiting maximum size of dataframe partition,"When I write out a dataframe to, say, csv, a .csv file is created for each partition.  Suppose I want to limit the max size of each file to, say, 1 MB.  I could do the write multiple times and increase the argument to repartition each time.  Is there a way I can calculate ahead of time what argument to use for repartition to ensure the max size of each file is less than some specified size.
I imagine there might be pathological cases where all the data ends up on one partition. So make the weaker assumption that we only want to ensure that the average file size is less than some specified amount, say 1 MB. 
",<scala><apache-spark><apache-spark-sql>,615,0,0,8646,33,119,206,72,14117,0,723,2,13,2018-08-27 16:59,2018-08-30 19:29,2018-08-30 19:29,3,3,Intermediate,22
52827853,Type double does not exist in PostgreSQL,"I have table like this:
CREATE TABLE workingtime_times
(
  workingno serial NOT NULL,
  checktime character(6) NOT NULL,
  timeoffset double precision DEFAULT 9
)
I create function like this:
CREATE OR REPLACE FUNCTION MyFName()
    RETURNS TABLE(
        CheckTime character varying,
        TimeOffset double 
    ) AS
$BODY$
BEGIN 
    RETURN QUERY 
    SELECT  t.CheckTime, t.TimeOffset
    FROM WorkingTime_Times t
    ORDER BY t.WorkingNo DESC
    limit 1;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION MyFName()
  OWNER TO postgres;
It make an error like this: 
  type double does not exist
Why we can create an table with column datatype double but return in the function fail. What type we can return in this case?
",<postgresql>,757,0,25,2914,3,19,43,75,16874,,417,1,13,2018-10-16 4:11,2018-10-16 4:21,2018-10-16 4:21,0,0,Intermediate,15
57078064,How to POST relation in Strapi,"I'm trying to do a POST to the Strapi API and can't seem to figure out how to attach a 'has and belongs to many' (many to many) relationship.
I've already tried the following body's:
events: [&quot;ID&quot;, &quot;ID&quot;]
name: &quot;name&quot;
&amp;
events: [ID, ID]
name: &quot;name&quot;
Which regarding the docs should be right, I think.
There's no error, I get a '200 OK' response. It adds the record but without the relations.
Event.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;events&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;event&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;artists&quot;: {
      &quot;collection&quot;: &quot;artist&quot;,
      &quot;via&quot;: &quot;events&quot;,
      &quot;dominant&quot;: true
    }
  }
}
Artist.settings.json:
{
  &quot;connection&quot;: &quot;default&quot;,
  &quot;collectionName&quot;: &quot;artists&quot;,
  &quot;info&quot;: {
    &quot;name&quot;: &quot;artist&quot;,
    &quot;description&quot;: &quot;&quot;
  },
  &quot;options&quot;: {
    &quot;increments&quot;: true,
    &quot;timestamps&quot;: [
      &quot;created_at&quot;,
      &quot;updated_at&quot;
    ],
    &quot;comment&quot;: &quot;&quot;
  },
  &quot;attributes&quot;: {
    &quot;name&quot;: {
      &quot;required&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    &quot;events&quot;: {
      &quot;collection&quot;: &quot;event&quot;,
      &quot;via&quot;: &quot;artists&quot;
    }
  }
}
I'm using the standard SQLite database, strapi version 3.0.0-beta.13 and tried the request through Postman, HTML &amp; curl.
I would love to know how to attach the relation on POST
Update 23-07:
Did a fresh install of Strapi and now everything is working.
",<javascript><sqlite><strapi>,2083,1,56,213,2,3,12,80,17801,0,8,3,13,2019-07-17 14:18,2019-07-18 12:48,,1,,Advanced,33
50952467,COLLATE=utf8_unicode_ci getting removed from schema.rb after migrate,"Running rails 5.0.2
The tables in our schema.rb in source control seem to mostly have the format: 
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci"" 
do |t|
Note the COLLATE=utf8_unicode_ci"" at the end.
When I run migrations the generated schemaa.rb is mostly the same but chops off COLLATE=utf8_unicode_ci"" from those lines so it now looks like:
create_table ""app_files"", 
    force: :cascade, 
    options: ""ENGINE=InnoDB DEFAULT CHARSET=utf8"" 
do |t|
Based on other SO posts I've tried two things to fix this
1) in my /etc/mysql/my.cnf, I added:
[mysqld]
character-set-server  = utf8
collation-server = utf8_unicode_ci
2) in my database.yml ive added collation: utf8_general_ci to all of the relevant environments
I then restarted mysql, dropped, created and migrated my db but still the collate line disappears.
Any thoughts on what configuration I need to change to have that bit autogenerated ?
",<mysql><sql><ruby-on-rails><rails-migrations>,974,0,17,953,0,11,27,75,3573,,61,2,13,2018-06-20 15:58,2019-09-24 14:58,,461,,Advanced,34
50745313,Check constraint that calls function does not work on update,"I created a constraint that prevents allocations in one table to exceed inventory in another table (see the details in my previous question). But for some reason the constraint works as expected only when I insert new allocations, but on update it does not prevent violating.
Here is my constraint:
([dbo].[fn_AllocationIsValid]([Itemid]) = 1)
And here is the function:
CREATE FUNCTION [dbo].[fn_AllocationIsValid] (@itemId as int)  
RETURNS int  AS  
BEGIN 
DECLARE @isValid bit;
SELECT @isValid = CASE WHEN ISNULL(SUM(Allocation), 0) &lt;= MAX(Inventory) THEN 1 ELSE 0 END
FROM Allocations A 
JOIN Items I ON I.Id = A.ItemId
WHERE I.Id = @itemId
GROUP BY I.Id;
RETURN @isValid;
END
Updated
Here are my tables:
CREATE TABLE [allocations] (
[userID] [bigint] NOT NULL ,
[itemID] [int] NOT NULL ,
[allocation] [bigint] NOT NULL ,
CONSTRAINT [PK_allocations] PRIMARY KEY  CLUSTERED 
(
    [userID],
    [itemID]
)  ON [PRIMARY] ,
CONSTRAINT [FK_allocations_items] FOREIGN KEY 
(
    [itemID]
) REFERENCES [items] (
    [id]
) ON DELETE CASCADE  ON UPDATE CASCADE ,
CONSTRAINT [CK_allocations] CHECK ([dbo].[fn_AllocationIsValid]([Itemid], [Allocation]) = 1)
) ON [PRIMARY]
CREATE TABLE [dbo].[Items](
[Id] [int] NOT NULL,
[Inventory] [int] NOT NULL
) ON [PRIMARY]
GO
INSERT INTO Items (Id, Inventory) VALUES (2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(4340, 2692, 336)
INSERT INTO Allocations (UserId, ItemId, Allocation) VALUES(5895, 2692, 0)
The following statement execution should fail, but it does not:
update allocations set allocation = 5
where userid = 5895 and itemid = 2692
",<sql-server><constraints>,1610,1,44,5121,11,60,108,36,3505,0,244,1,13,2018-06-07 15:42,2018-06-07 18:06,2018-06-07 18:06,0,0,Advanced,33
59540432,How to Mock postgresql (pg) in node.js using jest,"I am new in node.js. I am writing code in node.js for postgresql using pg and pg-native for serverless app. I need to write unit test for it. I am unable to mock pg client using jest or sinon. 
My actual code is like this
const { Client } = require('pg');
export const getAlerts = async (event, context) =&gt; {
  const client = new Client({
    user: process.env.DB_USER,
    host: process.env.DB_HOST,
    database: process.env.DB_DATABASE,
    password: process.env.DB_PASSWORD,
    port: process.env.PORT
  });
  await client.connect();
  try {
    const result = await client.query(`SELECT * FROM public.alerts;`);
    console.log(result.rows);
    client.end();
    return success({ message: `${result.rowCount} item(s) returned`, data: result.rows, status: true });
  } catch (e) {
    console.error(e.stack);
    client.end();
    return failure({ message: e, status: false });
  }
};
How to mock pg client here?
",<node.js><postgresql><unit-testing><jestjs><sinon>,921,0,30,1145,7,17,44,44,33648,0,34,4,13,2019-12-31 6:28,2019-12-31 15:17,2019-12-31 15:17,0,0,Advanced,32
56977209,Azure Function creating too many connections to PostgreSQL,"I have an Azure Durable Function that interacts with a PostgreSQL database, also hosted in Azure.
The PostgreSQL database has a connection limit of 50, and furthermore, my connection string limits the connection pool size to 40, leaving space for super user / admin connections.
Nonetheless, under some loads I get the error
  53300: remaining connection slots are reserved for non-replication superuser connections
This documentation from Microsoft seemed relevant, but it doesn't seem like I can make a static client, and, as it mentions, 
  because you can still run out of connections, you should optimize connections to the database.
I have this method
private IDbConnection GetConnection()
{
    return new NpgsqlConnection(Environment.GetEnvironmentVariable(""PostgresConnectionString""));
}
and when I want to interact with PostgreSQL I do like this
using (var connection = GetConnection())
{
    connection.Open();
    return await connection.QuerySingleAsync&lt;int&gt;(settings.Query().Insert, settings);
}
So I am creating (and disposing) lots of NpgsqlConnection objects, but according to this, that should be fine because connection pooling is handled behind the scenes. But there may be something about Azure Functions that invalidates this thinking.
I have noticed that I end up with a lot of idle connections (from pgAdmin):
Based on that I've tried fiddling with Npgsql connection parameters like Connection Idle Lifetime, Timeout, and Pooling, but the problem of too many connections seems to persist to one degree or another. Additionally I've tried limiting the number of concurrent orchestrator and activity functions (see this doc), but that seems to partially defeat the purpose of Azure Functions being scalable. It does help - I get fewer of the too many connections error). Presumably If I keep testing it with lower numbers I may even eliminate it, but again, that seems like it defeats the point, and there may be another solution.
How can I use PostgreSQL with Azure Functions without maxing out connections?
",<postgresql><azure><azure-functions><npgsql><azure-durable-functions>,2037,5,13,6482,7,43,95,71,3816,0,1337,3,13,2019-07-10 19:10,2019-07-11 7:24,,1,,Advanced,33
52963079,"I can't start server PostgreSQL 11: ""pg_ctl: could not start server""","I am in CentOS Linux release 7.5.1804 (Core)
When I login as postgres and run:
bash-4.2$ /usr/pgsql-11/bin/initdb -D /var/lib/pgsql/11/data
The files belonging to this database system will be owned by user ""postgres"".
This user must also own the server process.
The database cluster will be initialized with locale ""en_US.UTF-8"".
The default database encoding has accordingly been set to ""UTF8"".
The default text search configuration will be set to ""english"".
Data page checksums are disabled.
creating directory /var/lib/pgsql/11/data ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
selecting dynamic shared memory implementation ... posix
creating configuration files ... ok
running bootstrap script ... ok
performing post-bootstrap initialization ... ok
syncing data to disk ... ok
WARNING: enabling ""trust"" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.
Success. You can now start the database server using:
    /usr/pgsql-11/bin/pg_ctl -D /var/lib/pgsql/11/data -l logfile start
bash-4.2$
then I run
bash-4.2$ /usr/pgsql-11/bin/pg_ctl start -l logfile -D /var/lib/pgsql/11/data
waiting for server to start..../bin/sh: logfile: Permission denied
 stopped waiting
pg_ctl: could not start server
Examine the log output.
bash-4.2$ date
Wed Oct 24 01:50:44 -05 2018
bash-4.2$
I search in GOOGLE for ""waiting for server to start..../bin/sh: logfile: Permission denied"" but this error only happened in MAC and no solutions is displayed...
Also I run
bash-4.2$ postgres --version;postmaster --version;
postgres (PostgreSQL) 11.0
postgres (PostgreSQL) 11.0
bash-4.2$
then I believe PostgreSQL 11 is fine installed, but I can't start server.
I install with this line:
yum install postgresql-jdbc.noarch postgresql-jdbc-javadoc.noarch postgresql-unit11.x86_64 postgresql-unit11-debuginfo.x86_64 postgresql11.x86_64 postgresql11-contrib.x86_64 postgresql11-debuginfo.x86_64 postgresql11-devel.x86_64 postgresql11-docs.x86_64 postgresql11-libs.x86_64 postgresql11-odbc.x86_64 postgresql11-plperl.x86_64 postgresql11-plpython.x86_64 postgresql11-pltcl.x86_64 postgresql11-server.x86_64 postgresql11-tcl.x86_64 postgresql11-test.x86_64
I didn't add [postgresql11-llvmjit.x86_64] because this requires many dependences.
CentOS Linux release 7.5.1804 + PostgreSQL 11 ?
Do I need install aditional software?
",<postgresql><centos7><postgresql-11>,2490,0,43,211,1,3,13,49,46847,0,11,4,13,2018-10-24 7:20,2018-10-24 17:07,,0,,Advanced,32
50451566,SQL Server In Memory Equivalent,"My team has recently decided to move from EF to Dapper. As such we are moving a lot of the logic that was done in EF into Stored Procedures as part of our SQL Server DB. This means that a lot of the Unit Tests that we have for EF are now Integration Level tests as they involve the DB. I am looking for a way to run these tests using an In-Memory DB so I don't have to stand up a DB externally as part of the tests. I looked into SQLite but without the SP support, it would not be a fair comparison. Are there any other In-Memory DBs that would be similar to SQL Server that can be used for testing?
",<sql-server><entity-framework><integration-testing><dapper>,600,0,0,733,1,8,20,59,7632,0,49,1,13,2018-05-21 15:01,2018-05-21 15:50,,0,,Advanced,34
51267470,"Unhandled rejection SequelizeDatabaseError: relation ""users"" does not exist","I am getting started with Sequelize.  I am following the documentation they are providing on their website :http://docs.sequelizejs.com/manual/installation/getting-started.html 
const Sequelize = require('sequelize');
const sequelize = new Sequelize('haha', 'postgres', 'postgres', {
  host: 'localhost',
  dialect: 'postgres',
  operatorsAliases: false,
  pool: {
    max: 5,
    min: 0,
    acquire: 30000,
    idle: 10000
  },
  // SQLite only
  storage: 'path/to/database.sqlite'
});
sequelize
  .authenticate()
  .then(() =&gt; {
    console.log('Connection has been established successfully.');
  })
  .catch(err =&gt; {
    console.error('Unable to connect to the database:', err);
  });
  const User = sequelize.define('user', {
    firstName: {
      type: Sequelize.STRING
    },
    lastName: {
      type: Sequelize.STRING
    }
  });
  // force: true will drop the table if it already exists
  User.sync({force: true}).then(() =&gt; {
    // Table created
    return User.create({
      firstName: 'John',
      lastName: 'Hancock'
    });
  });
Up until here, everything works perfectly. And the table ""user"" is correctly built and populated. (Although I do not understand Sequelize appends an ""s"" automatically to ""user"", any explanation.)
However when I add the following portion of code:
User.findAll().then(users =&gt; {
  console.log(users)
})
I get this error :  
  Unhandled rejection SequelizeDatabaseError: relation ""users"" does not
  exist
So my questions are:  
Why does Sequelize add an ""s"" to user. (I know it makes sense but shouldn't the developer decide that)  
What is causing that error? I followed the documentation but it still didn't work?
",<javascript><node.js><postgresql><sequelize.js><pgadmin>,1675,4,48,1743,6,33,77,47,48448,0,1026,8,13,2018-07-10 14:11,2018-09-11 2:08,,63,,Advanced,33
55538252,How to create a database backup in DBeaver and restore it?,"I need to create a SQL server database backup in DBeaver and restore it. Is that possible?
Using SQL Management Studio would not be a turnaround solution in this case, as we are not allowed to use it here.
",<sql-server><dbeaver>,206,0,0,1451,3,18,31,57,25277,,281,1,13,2019-04-05 14:58,2021-03-02 10:52,,697,,Intermediate,30
52783712,SQL Server Management Studio 2017 Database Diagram Folder Missing,"I have SSMS 2017 installed and I want to use it to create an ERD for some of my databases. The documentation online says you just need to right click on the ""Database Diagrams"" folder under your database in the navigation panel. However, that folder is simply not there for any of my databases. I cannot find any fix or work around. Does anyone have any ideas on how to fix this?
",<sql-server><ssms-2017>,380,0,0,465,1,4,18,56,12434,0,18,3,12,2018-10-12 16:38,2019-01-23 18:36,,103,,Advanced,32
52910437,Installed mysql@5.6 using brew mysql.server not a command,"I installed mysql@5.6 using brew. Below are the comands I ran
brew install mysql@5.6
sudo chmod -R 777 /usr/local/var/mysql
sudo ln -s /usr/local/Cellar/mysql\@5.6/5.6.41/bin/mysql /usr/local/bin/mysql
sudo cp /usr/local/Cellar/mysql\@5.6/5.6.41/homebrew.mxcl.mysql.plist /Library/LaunchAgents/
sudo chown root /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod 600 /Library/LaunchAgents/homebrew.mxcl.mysql.plist
sudo chmod +x /Library/LaunchAgents/homebrew.mxcl.mysql.plist
launchctl load -w /Library/LaunchAgents/homebrew.mxcl.mysql.plist
mysql.server start
  sh: mysql.server: command not found
this is the output I am getting.
mysql --version is giving output
  mysql  Ver 14.14 Distrib 5.6.41, for osx10.13 (x86_64) using  EditLine
  wrapper
If I start service via brew its starting
brew services start mysql@5.6
But when I run mysql -uroot I am getting 
  ERROR 2002 (HY000): Can't connect to local MySQL server through socket
  '/tmp/mysql.sock' (2)
",<mysql><macos><homebrew>,966,0,18,987,4,16,48,37,15874,0,189,2,12,2018-10-20 21:53,2018-10-20 21:59,2018-10-20 21:59,0,0,Advanced,34
64478510,Not able to take backup of hypertable TimescaleDB database using pg_dump PostgreSQL,"command used to take backup
C:\Program Files\PostgreSQL\12\bin&gt;pg_dump  -h localhost -U postgres -p 5432  -Fc -f &quot;D:\Database Backup\temp_10.bak&quot; GESEMS_Performace_Test.
Error :
pg_dump: NOTICE:  hypertable data are in the chunks, no data will be copied.
DETAIL:  Data for hypertables are stored in the chunks of a hypertable so COPY TO of a hypertable will
not copy any data.
Any suggestions to take backup of TimescaleDB hypertables?
",<postgresql><timescaledb>,449,1,1,153,1,2,7,68,6159,,2,1,12,2020-10-22 8:43,2020-10-22 10:22,2020-10-22 10:22,0,0,Advanced,33
53160535,Rails 4.2 Postgres 9.4.4 statement_timeout doesn't work,"I am trying to set a statement_timeout. I tried both setting in database.yml file like this
variables:
  statement_timeout: 1000
And this
ActiveRecord::Base.connection.execute(""SET statement_timeout = 1000"")
Tested with
ActiveRecord::Base.connection.execute(""select pg_sleep(4)"")
And they both don't have any effect.
I am running postgres 10 in my local and the statement_timeouts works just expected. But on my server that is running postgres 9.4.4, it simply doesn't do anything.
I've check Postgres' doc for 9.4 and statement_timeout is available. Anyone can shed some light?
",<ruby-on-rails><postgresql>,579,1,5,2544,0,21,29,72,1160,,705,1,12,2018-11-05 19:00,2020-04-04 11:09,,516,,Advanced,33
54515682,How to give ranks to multiple columns data in SQL Server?,"I have input table as shown below -
ID  Name q1     q2      q3      q4
1   a    2621   2036    1890    2300
2   b    18000  13000   14000   15000
3   c    100    200     300     400
I want ranking of columns(q1, q2, q3 and q4) data for each row. For example, if I consider last row of above input, then q4 column contains 400 value which is higher than other columns, so rank to q4 column will be 1, q3 rank will be 2, q2 rank will be 3 and q1 rank will be 4.
I am looking for output like -
id  name  q1  q2  q3  q4
1   a     1   3   4   2
2   b     1   4   3   2
3   c     4   3   2   1
There are more than 100,000 records present in input table.
I have created small SQL script for input table i.e.,
declare @temp table (ID int, Name varchar(10), q1 int, q2 int, q3 int, q4 int)
insert into @temp
select 1, 'a', 2621, 2036, 1890, 2300
union all
select 2, 'b', 18000, 13000, 14000, 15000
union all
select 3, 'c', 100, 200, 300, 400
select * from @temp
Please help me to find efficient way to solve this problem.
",<sql-server><sql-server-2012>,1013,0,29,2014,2,13,22,48,1478,,191,1,12,2019-02-04 12:02,2019-02-04 12:13,2019-02-04 12:13,0,0,Advanced,33
49702144,How do you properly handle SQL_VARIANT in Entity Framework Core?,"It looks like support has recently been added to Entity Framework Core in .NET Core 2.1 (preview) to allow the mapping of SQL_VARIANT columns (https://github.com/aspnet/EntityFrameworkCore/issues/7043).
It looks like the way to go about doing this is using the new HasConversion() method (https://learn.microsoft.com/en-us/ef/core/modeling/value-conversions).
So, in order to map my SQL_VARIANT column and treat the underlying datatype as being a VARCHAR of any length (I only care about reading it at this point), I can do the following (where the Value property here is of type object in the model):
entity.Property(e =&gt; e.Value).HasConversion(v =&gt; v.ToString(),
                                                            v =&gt; v.ToString());
This works, if the SQL_VARIANT's underlying datatype is a VARCHAR of any length.
However, being a SQL_VARIANT, the column could contain data of other types, such as DATETIME values.
For simplicity I've only specified DateTime and string here, but in theory I'd probably want to support the datatypes necessary to map whatever could be stored in a SQL_VARIANT column, if possible.
How would I go about determining which one of those two types (string and DateTime) I'd want to map to at runtime?  Is there a way to do this?
",<c#><.net-core><entity-framework-core><sql-variant>,1277,4,18,1338,3,21,52,35,5893,0,9,1,12,2018-04-06 23:15,2018-04-09 9:20,2018-04-09 9:20,3,3,Advanced,33
54387084,How to safely reindex primary key on postgres?,"We have a huge table that contains bloat on the primary key index. We constantly archive old records on that table. 
We reindex other columns by recreating the index concurrently and dropping the old one. This is to avoid interfering with production traffic.
But this is not possible for a primary key since there are foreign keys depending on it. At least based on what we have tried.
What's the right way to reindex the primary key safely without blocking DML statements on the table?
",<postgresql>,487,0,0,7418,5,41,78,48,8594,0,303,3,12,2019-01-27 10:21,2019-01-28 12:36,2020-07-24 10:30,1,544,Advanced,34
53133498,flask-sqlalchemy intellisense/autocomplete,"I'm searching for a way to actually get intellisense for flask-sqlalchemy. All the answers I have found online seem to tell us how to suppress the errors (no Column or query instance members)(switch linters, ignore the class in pylintrc, etc...), but I find it very hard to belive that the most widely used ORM in python does not actually have linting support. 
I am open to using pycharm or intellij community editions, but I'd rather stick with vs code, since I am a js developer who recently decided to learn python, and I'd like to stick with the tools I know and love. I hear that vs code has great python support, but sqlalchemy linting is a must!
",<python><sqlalchemy><visual-studio-code><flask-sqlalchemy>,654,0,0,469,0,3,12,75,5166,0,7,0,12,2018-11-03 16:53,,,,,Advanced,36
49016650,How to configure Spring boot for work with two databases?,"I am using Spring Boot 2.X with Hibernate 5 to connect two different MySQL databases (Bar and Foo) on different servers. I am trying to list all the information of an entity (own attributes and @OneToMany and @ManyToOne relations) from a method in a REST Controller. 
I have followed several tutorials to do this, thus, I am able to get all the information for my @Primary database (Foo), however, I always get an exception for my secondary database (Bar) when retrieving the @OneToMany sets. If I swap the @Primary annotation to the Bar database, I able to get the data from the Bar database but not for the Foo database . Is there a way to resolve this?
This is the exception I am getting:
...w.s.m.s.DefaultHandlerExceptionResolver :
Failed to write HTTP message: org.springframework.http.converter.HttpMessageNotWritableException: 
    Could not write JSON document: failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-com.foobar.bar.domain.Bar[""manyBars""]); 
    nested exception is com.fasterxml.jackson.databind.JsonMappingException:
        failed to lazily initialize a collection of role: 
        com.foobar.bar.domain.Bar.manyBars, could not initialize proxy - no Session (through reference chain: java.util.ArrayList[0]-&gt;com.foobar.bar.domain.Bar[""manyBars""])
My application.properties:
# MySQL DB - ""foo""
spring.datasource.url=jdbc:mysql://XXX:3306/foo?currentSchema=public
spring.datasource.username=XXX
spring.datasource.password=XXX
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
# MySQL DB - ""bar""
bar.datasource.url=jdbc:mysql://YYYY:3306/bar?currentSchema=public
bar.datasource.username=YYYY
bar.datasource.password=YYYY
bar.datasource.driver-class-name=com.mysql.jdbc.Driver
# JPA
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
My @Primary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""entityManagerFactory"",
        transactionManagerRef = ""transactionManager"",
        basePackages = {""com.foobar.foo.repo""})
public class FooDbConfig {
    @Primary
    @Bean(name = ""dataSource"")
    @ConfigurationProperties(prefix = ""spring.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Primary
    @Bean(name = ""entityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean entityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""dataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.foo.domain"")
                .persistenceUnit(""foo"")
                .build();
    }
    @Primary
    @Bean(name = ""transactionManager"")
    public PlatformTransactionManager transactionManager(
            @Qualifier(""entityManagerFactory"") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}
My secondary DataSource configuration:
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(entityManagerFactoryRef = ""barEntityManagerFactory"",
        transactionManagerRef = ""barTransactionManager"", basePackages = {""com.foobar.bar.repo""})
public class BarDbConfig {
    @Bean(name = ""barDataSource"")
    @ConfigurationProperties(prefix = ""bar.datasource"")
    public DataSource dataSource() {
        return DataSourceBuilder.create().build();
    }
    @Bean(name = ""barEntityManagerFactory"")
    public LocalContainerEntityManagerFactoryBean barEntityManagerFactory(
            EntityManagerFactoryBuilder builder, @Qualifier(""barDataSource"") DataSource dataSource) {
        return builder
                .dataSource(dataSource)
                .packages(""com.foobar.bar.domain"")
                .persistenceUnit(""bar"")
                .build();
    }
    @Bean(name = ""barTransactionManager"")
    public PlatformTransactionManager barTransactionManager(
            @Qualifier(""barEntityManagerFactory"") EntityManagerFactory barEntityManagerFactory) {
        return new JpaTransactionManager(barEntityManagerFactory);
    }
}
The REST Controller class:
@RestController
public class FooBarController {
    private final FooRepository fooRepo;
    private final BarRepository barRepo;
    @Autowired
    FooBarController(FooRepository fooRepo, BarRepository barRepo) {
        this.fooRepo = fooRepo;
        this.barRepo = barRepo;
    }
    @RequestMapping(""/foo"")
    public List&lt;Foo&gt; listFoo() {
        return fooRepo.findAll();
    }
    @RequestMapping(""/bar"")
    public List&lt;Bar&gt; listBar() {
        return barRepo.findAll();
    }
    @RequestMapping(""/foobar/{id}"")
    public String fooBar(@PathVariable(""id"") Integer id) {
        Foo foo = fooRepo.findById(id);
        Bar bar = barRepo.findById(id);
        return foo.getName() + "" "" + bar.getName() + ""!"";
    }
}
The Foo/Bar repositories:
@Repository
public interface FooRepository extends JpaRepository&lt;Foo, Long&gt; {
  Foo findById(Integer id);
}
@Repository
public interface BarRepository extends JpaRepository&lt;Bar, Long&gt; {
  Bar findById(Integer id);
}
The entities for the @Primary datasource. The entities of the second datasource are the same (only changing the class names):
@Entity
@Table(name = ""foo"")
public class Foo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @OneToMany(fetch = FetchType.LAZY, mappedBy = ""foo"")
    @JsonIgnoreProperties({""foo""})
    private Set&lt;ManyFoo&gt; manyFoos = new HashSet&lt;&gt;(0);
    // Constructors, Getters, Setters
}
@Entity
@Table(name = ""many_foo"")
public class ManyFoo {
    @Id
    @GeneratedValue(strategy = IDENTITY)
    @Column(name = ""id"", unique = true, nullable = false)
    private Integer id;
    @Column(name = ""name"")
    private String name;
    @ManyToOne(fetch = FetchType.LAZY)
    @JsonIgnoreProperties({""manyFoos""})
    private Foo foo;
    // Constructors, Getters, Setters
}  
Finally, my application main:
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
It is important to remark that the solution should keep the Lazy property for both databases in order to maintain an optimal performance.
Edit 1: If both catalogs (""databases"" in MySQL terminology) are in same database (""server"") the Rick James solution works!!
The problem remains when catalogs (MySQL databases) are in different databases (servers) and it is tried to keep Lazy  the property
Many thanks.
",<java><mysql><spring><hibernate><spring-boot>,6853,0,171,1280,1,15,43,49,4611,0,707,2,12,2018-02-27 19:38,2018-03-07 18:13,,8,,Advanced,34
53025787,Mysql Calculate Growth based on Quarters,"I have a database with two tables - companies and reports. I want to calculate the change from q1 (quarter 1) to q2 (quarter 2). I have tried to use the (following) sub-query, but then the main query fails...
FROM
    (SELECT revenue FROM reports WHERE quarter = 'q2' AND fiscal_year = 2018) AS q,
    (SELECT revenue FROM reports WHERE quarter = 'q1' AND fiscal_year = 2017) AS lq
Here is DB Fiddle to help you understand the problem and schema:
https://www.db-fiddle.com/f/eE8SNRojn45h7Rc1rPCEVN/4
Current Simple Query.
SELECT 
    c.name, r.quarter, r.fiscal_year, r.revenue, r.taxes, r.employees
FROM 
    companies c
JOIN
    reports r 
ON
    r.company_id = c.id
WHERE
    c.is_marked = 1;
Expected Results (this is what i need):
+---------+----------+----------------+----------+--------------+-----------+------------------+
|  Name   | Revenue  | Revenue_change |  Taxes   | Taxes_change | Employees | Employees_change |
+---------+----------+----------------+----------+--------------+-----------+------------------+
| ABC INC |    11056 | +54.77         | 35000.86 | -28.57%      |       568 | -32              |
| XYZ INC |     5000 | null           | null     | null         |        10 | +5               |
+---------+----------+----------------+----------+--------------+-----------+------------------+
I would really appreciate your help to build this query. Thanks in advance.
",<mysql><laravel>,1394,2,19,2756,7,44,76,76,1113,0,63,6,12,2018-10-27 19:59,2018-10-27 20:56,,0,,Intermediate,15
56180225,Keycloak server in docker fails to start in standalone mode?,"
Well, as the title suggests, this is more of an issue record. I was trying to follow the instructions on this README file of Keycloak docker server images, but encountered a few blockers. 
After pulling the image, below command to start a standalone instance failed. 
docker run jboss/keycloak
The error stack trace: 
-b 0.0.0.0
=========================================================================
  Using PostgreSQL database
=========================================================================
...
04:45:06,084 INFO  [io.smallrye.metrics] (MSC service thread 1-5) Converted [2] config entries and added [4] replacements
04:45:06,096 ERROR [org.jboss.as.controller.management-operation] (ServerService Thread Pool -- 33) WFLYCTL0013: Operation (""add"") failed - address: ([
    (""subsystem"" =&gt; ""datasources""),
    (""data-source"" =&gt; ""KeycloakDS"")
]) - failure description: ""WFLYCTL0113: '' is an invalid value for parameter user-name. Values must have a minimum length of 1 characters""
...
Caused by: java.lang.RuntimeException: Failed to connect to database
    at org.keycloak.connections.jpa.DefaultJpaConnectionProviderFactory.getConnection(DefaultJpaConnectionProviderFactory.java:382)
...
Caused by: javax.naming.NameNotFoundException: datasources/KeycloakDS -- service jboss.naming.context.java.jboss.datasources.KeycloakDS
    at org.jboss.as.naming.ServiceBasedNamingStore.lookup(ServiceBasedNamingStore.java:106)
...
I was wondering how it uses a PostgreSQL database, and assumed it might spin up its own instance. But the error looks like it has a problem connecting to the database. 
Changing to the embedded H2 DB made it work. 
docker run -e DB_VENDOR=""h2"" --name docker-keycloak-h2 jboss/keycloak
The docker-entrypoint.sh file shows that it uses below logic to determine what DB to use. 
if (getent hosts postgres &amp;&gt;/dev/null); then
        export DB_VENDOR=""postgres""
...
And further down the flow, this change-database.cli file indicates that it's actually expecting a running PostgreSQL instance to use. 
connection-url=jdbc:postgresql://${env.DB_ADDR:postgres}:${env.DB_PORT:5432}/${env.DB_DATABASE:keycloak}${env.JDBC_PARAMS:}
So I began wondering how PostgreSQL was chosen as a default initially. Executing below commands in a running Keycloak docker container revealed some interesting things. 
[root@71961b81189c bin]# getent hosts postgres
69.172.201.153  postgres.mbox.com
[root@71961b81189c bin]# echo $?
0
Not sure what this postgres.mbox.com is but apparently it's not an expected PostgreSQL server to be resolved by getent. Not sure whether this is a recent linux issue either. The hosts entry in the Name Service Switch Configuration file /etc/nsswitch.conf looks like below inside the container.
hosts:      files dns myhostname
It is the dns data source that resolved postgres to postgres.mbox.com. 
This is why the DB vendor determination logic failed which eventually caused the container failing to start. The instructions on this README file do not work as of the day this post is published. 
Below are the working commands to start a Keycloak server in docker properly with PostgreSQL as the database. 
docker network create keycloak-network
docker run -d --name postgres --net keycloak-network -e POSTGRES_DB=keycloak -e POSTGRES_USER=keycloak -e POSTGRES_PASSWORD=password postgres
docker run --name docker-keycloak-postgres --net keycloak-network -e DB_USER=keycloak -e DB_PASSWORD=password jboss/keycloak
",<postgresql><docker><keycloak>,3468,3,44,2467,1,24,42,81,17819,0,499,3,12,2019-05-17 5:36,2019-06-04 9:24,2019-06-04 9:24,18,18,Advanced,37
53469793,E_WARNING: Error while sending STMT_PREPARE packet. PID=*,"My Laravel 5.7 website has been experiencing a few problems that I think are related to each other (but happen at different times):
PDO::prepare(): MySQL server has gone away
E_WARNING: Error while sending STMT_PREPARE packet. PID=10
PDOException: SQLSTATE[23000]: Integrity constraint violation: 1062 Duplicate entry (My database often seems to try to write the same record twice in the same second. I've been unable to figure out why or how to reproduce it; it doesn't seem to be related to user behavior.)
Somehow, those first 2 types of errors only ever appear in my Rollbar logs but not on the text logs on the server or in my Slack notifications, as all errors are supposed to (and all others do).
For months, I've continued to see scary log messages like these, and I've been completely unable to reproduce these errors (and have been unable to diagnose and solve them).
I haven't yet found any actual symptoms or heard of any complaints from users, but the error messages seem non-trivial, so I really want to understand and fix the root causes.
I've tried changing my MySQL config to use max_allowed_packet=300M (instead of the default of 4M) but still get these exceptions frequently on days when I have more than a couple of visitors to my site.
I've also set (changed from 5M and 10M) the following because of this advice:
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
As further background:
My site has a queue worker that runs jobs (artisan queue:work --sleep=3 --tries=3 --daemon).
There are a bunch of queued jobs that can be scheduled to happen at the same moment based on the signup time of visitors. But the most I see that have happened simultaneously is 20.
There are no entries in the MySQL Slow Query Log.
I have a few cron jobs, but I doubt they're problematic. One runs every minute but is really simple. Another runs every 5 minutes to send certain scheduled emails if any are pending. And another runs every 30 minutes to run a report.
I've run various mysqlslap queries (I'm completely novice though) and haven't found anything slow even when simulating hundreds of concurrent clients.
I'm using Laradock (Docker).
My server is DigitalOcean 1GB RAM, 1 vCPU, 25GB SSD. I've also tried 2GB RAM with no difference.
The results from SHOW VARIABLES; and SHOW GLOBAL STATUS; are here.
My my.cnf is:
[mysql]
[mysqld]
sql-mode=&quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&quot;
character-set-server=utf8
innodb_buffer_pool_chunk_size=218M
innodb_buffer_pool_size = 218M
max_allowed_packet=300M
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow_query_log.log
long_query_time = 10
log_queries_not_using_indexes = 0
Any ideas about what I should explore to diagnose and fix these problems? Thanks.
",<mysql><laravel><performance><laravel-5><laradock>,2793,3,23,22262,31,180,366,46,3476,0,13224,4,12,2018-11-25 17:02,2018-12-10 6:09,,15,,Advanced,33
59578211,MySQL to return records with a date/time of now minus 1 hour?,"can anybody help with a MySQL command to try and select all records within a table with a date/time equal to or more than now - 1hour?
Now I'm not 100% sure that that is the best way of describing this.
I basically have records with a date/time field (e.g. 2019-07-13 13:00:00) and I want to perform a MySQL select to find all records with a date/time of one hour ago. This is to trigger a function one hour after an event.
I currently have this, but not sure if it is along the right lines at all:
SELECT * FROM database.table_name
WHERE (NOW() - INTERVAL 1 HOUR) &gt;= 'date_of_event'
AND 'status' = 'Scheduled';
Any thoughts would be great!
",<mysql><datetime>,644,0,3,2813,8,29,32,38,15524,0,37,2,12,2020-01-03 11:56,2020-01-03 12:08,2020-01-03 12:09,0,0,Basic,10
52284555,FOR JSON PATH vs FOR JSON AUTO SQL Server,"I'm having an issue creating nested JSON in SQL Server.  I'm trying to create an output that looks like this:
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
        ""address1"": ""1 Oak Grove"",
        ""address2"": ""London"",
        ""address3"": ""UK""
    },
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
I've tried both FOR JSON AUTO and FOR JSON PATH:
SELECT
  d.SCHOOL_WEBSITE AS websiteURL
  ,d.SCHOOL_EMAIL AS email 
 ,d.SCHOOL_TELEPHONE AS phone
 ,d.[Address 1] AS 'address.address1'
 ,d.[Address 2] AS 'address.address2'
 ,d.[Address 3] AS 'address.address3'
 ,accreditations.[IndiUID] as name   
 ,accreditations.Value as value 
 FROM [TESTDB].[dbo].[DataValues] as d,[TESTDB].[dbo].[accreditations] as accreditations
 WHERE d.Code = accreditations.SchoolCode
 FOR JSON AUTO --PATH
FOR JSON AUTO creates this (address section is not nested (but accredidation is):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address.address1"": ""1 Oak Grove"",
    ""address.address2"": ""London"",
    ""address.address3"": ""UK"",
    ""accreditations"": [
      {
        ""name"": ""Indicator1"",
        ""value"": ""True""
      },
      {
        ""name"": ""Indicator2"",
        ""value"": ""False""
      },
      {
        ""name"": ""Indicator3"",
        ""value"": ""False""
      }
    ]
  }
]
FOR JSON PATH creates this (address section is nested, but accreditation is not - the whole block repeats):
[
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator1"",
    ""value"": ""True""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator2"",
    ""value"": ""False""
  },
  {
    ""websiteURL"": ""www.test.edu"",
    ""email"": ""hello@test.edu"",
    ""phone"": 123456798,
    ""address"": {
      ""address1"": ""1 Oak Grove"",
      ""address2"": ""London"",
      ""address3"": ""UK""
    },
    ""name"": ""Indicator3"",
    ""value"": ""False""
    }
]
I think the key to it is some sort of FOR JSON sub query around the accreditations but I haven't had any success with this.
Create sample data with the following:
    /****** Object:  Table [dbo].[accreditations]    Script Date: 11/09/2018 22:29:43 ******/
CREATE TABLE [dbo].[accreditations](
    [SchoolCode] [nvarchar](255) NULL,
    [IndiUID] [nvarchar](255) NULL,
    [Value] [nvarchar](255) NULL
) ON [PRIMARY]
GO
/****** Object:  Table [dbo].[DataValues]    Script Date: 11/09/2018 22:29:44 ******/
CREATE TABLE [dbo].[DataValues](
    [Code] [nvarchar](255) NULL,
    [SCHOOL_NAME_FORMAL] [nvarchar](255) NULL,
    [SCHOOL_WEBSITE] [nvarchar](255) NULL,
    [SCHOOL_EMAIL] [nvarchar](255) NULL,
    [SCHOOL_TELEPHONE] [float] NULL,
    [Address 1] [nvarchar](255) NULL,
    [Address 2] [nvarchar](255) NULL,
    [Address 3] [nvarchar](255) NULL
) ON [PRIMARY]
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'ABC', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator2', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'DEF', N'Indicator3', N'False')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator1', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator2', N'True')
GO
INSERT [dbo].[accreditations] ([SchoolCode], [IndiUID], [Value]) VALUES (N'GHI', N'Indicator3', N'True')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'ABC', N'test', N'www.test.edu', N'hello@test.edu', 123456798, N'1 Oak Grove', N'London', N'UK')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'DEF', N'something', N'https://something.edu/fulltime', N'hello@something.edu', 987654321, N'23 Tree Road', N'Paris', N'France')
GO
INSERT [dbo].[DataValues] ([Code], [SCHOOL_NAME_FORMAL], [SCHOOL_WEBSITE], [SCHOOL_EMAIL], [SCHOOL_TELEPHONE], [Address 1], [Address 2], [Address 3]) VALUES (N'GHI', N'university', N'http://www.university.ac.uk/', N'hello@university.ac.uk/', 123123123, N'57 Bonsai Lane', N'London', N'UK')
GO
",<json><sql-server><for-json>,5145,4,143,125,1,1,6,47,17562,,1,1,12,2018-09-11 21:39,2018-09-11 21:48,2018-09-11 21:48,0,0,Intermediate,23
51810460,Is proper event-time sessionization possible with Spark Structured Streaming?,"Been playing around Spark Structured Streaming and mapGroupsWithState (specifically following the StructuredSessionization example in the Spark source). I want to confirm some limitations I believe exist with mapGroupsWithState given my use case.
A session for my purposes is a group of uninterrupted activity for a user such that no two chronologically ordered (by event time, not processing time) events are separated by more than some developer-defined duration (30 minutes is common).
An example will help before jumping into code:
{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}
{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}
For the stream above, a session is defined with a 30 minute period of inactivity. In a streaming context, we should end up with one session (the second has yet to complete):
[
  {
    ""user_id"": ""mike"",
    ""startTimestamp"": ""2018-01-01T00:00:00"",
    ""endTimestamp"": ""2018-01-01T00:05:00""
  }
]
Now consider the following Spark driver program:
import java.sql.Timestamp
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.execution.streaming.MemoryStream
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout}
object StructuredSessionizationV2 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master(""local[2]"")
      .appName(""StructredSessionizationRedux"")
      .getOrCreate()
    spark.sparkContext.setLogLevel(""WARN"")
    import spark.implicits._
    implicit val ctx = spark.sqlContext
    val input = MemoryStream[String]
    val EVENT_SCHEMA = new StructType()
      .add($""event_time"".string)
      .add($""user_id"".string)
    val events = input.toDS()
      .select(from_json($""value"", EVENT_SCHEMA).alias(""json""))
      .select($""json.*"")
      .withColumn(""event_time"", to_timestamp($""event_time""))
      .withWatermark(""event_time"", ""1 hours"")
    events.printSchema()
    val sessionized = events
      .groupByKey(row =&gt; row.getAs[String](""user_id""))
      .mapGroupsWithState[SessionState, SessionOutput](GroupStateTimeout.EventTimeTimeout) {
      case (userId: String, events: Iterator[Row], state: GroupState[SessionState]) =&gt;
        println(s""state update for user ${userId} (current watermark: ${new Timestamp(state.getCurrentWatermarkMs())})"")
        if (state.hasTimedOut) {
          println(s""User ${userId} has timed out, sending final output."")
          val finalOutput = SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = true
          )
          // Drop this user's state
          state.remove()
          finalOutput
        } else {
          val timestamps = events.map(_.getAs[Timestamp](""event_time"").getTime).toSeq
          println(s""User ${userId} has new events (min: ${new Timestamp(timestamps.min)}, max: ${new Timestamp(timestamps.max)})."")
          val newState = if (state.exists) {
            println(s""User ${userId} has existing state."")
            val oldState = state.get
            SessionState(
              startTimestampMs = math.min(oldState.startTimestampMs, timestamps.min),
              endTimestampMs = math.max(oldState.endTimestampMs, timestamps.max)
            )
          } else {
            println(s""User ${userId} has no existing state."")
            SessionState(
              startTimestampMs = timestamps.min,
              endTimestampMs = timestamps.max
            )
          }
          state.update(newState)
          state.setTimeoutTimestamp(newState.endTimestampMs, ""30 minutes"")
          println(s""User ${userId} state updated. Timeout now set to ${new Timestamp(newState.endTimestampMs + (30 * 60 * 1000))}"")
          SessionOutput(
            userId = userId,
            startTimestampMs = state.get.startTimestampMs,
            endTimestampMs = state.get.endTimestampMs,
            durationMs = state.get.durationMs,
            expired = false
          )
        }
      }
    val eventsQuery = sessionized
      .writeStream
      .queryName(""events"")
      .outputMode(""update"")
      .format(""console"")
      .start()
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:00:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:01:00"", ""user_id"": ""mike""}"""""",
      """"""{""event_time"": ""2018-01-01T00:05:00"", ""user_id"": ""mike""}""""""
    )
    input.addData(
      """"""{""event_time"": ""2018-01-01T00:45:00"", ""user_id"": ""mike""}""""""
    )
    eventsQuery.processAllAvailable()
  }
  case class SessionState(startTimestampMs: Long, endTimestampMs: Long) {
    def durationMs: Long = endTimestampMs - startTimestampMs
  }
  case class SessionOutput(userId: String, startTimestampMs: Long, endTimestampMs: Long, durationMs: Long, expired: Boolean)
}
Output of that program is:
root
 |-- event_time: timestamp (nullable = true)
 |-- user_id: string (nullable = true)
state update for user mike (current watermark: 1969-12-31 19:00:00.0)
User mike has new events (min: 2018-01-01 00:00:00.0, max: 2018-01-01 00:05:00.0).
User mike has no existing state.
User mike state updated. Timeout now set to 2018-01-01 00:35:00.0
-------------------------------------------
Batch: 0
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514783100000|    300000|  false|
+------+----------------+--------------+----------+-------+
state update for user mike (current watermark: 2017-12-31 23:05:00.0)
User mike has new events (min: 2018-01-01 00:45:00.0, max: 2018-01-01 00:45:00.0).
User mike has existing state.
User mike state updated. Timeout now set to 2018-01-01 01:15:00.0
-------------------------------------------
Batch: 1
-------------------------------------------
+------+----------------+--------------+----------+-------+
|userId|startTimestampMs|endTimestampMs|durationMs|expired|
+------+----------------+--------------+----------+-------+
|  mike|   1514782800000| 1514785500000|   2700000|  false|
+------+----------------+--------------+----------+-------+
Given my session definition, the single event in the second batch should trigger an expiry of session state and thus a new session. However, since the watermark (2017-12-31 23:05:00.0) has not passed the state's timeout (2018-01-01 00:35:00.0), state isn't expired and the event is erroneously added to the existing session despite the fact that more than 30 minutes have passed since the latest timestamp in the previous batch.
I think the only way for session state expiration to work as I'm hoping is if enough events from different users were received within the batch to advance the watermark past the state timeout for mike. 
I suppose one could also mess with the stream's watermark, but I can't think of how I'd do that to accomplish my use case.
Is this accurate? Am I missing anything in how to properly do event time-based sessionization in Spark?
",<apache-spark><apache-spark-sql><spark-structured-streaming>,7348,1,149,3780,3,24,31,54,2295,0,43,3,12,2018-08-12 15:56,2018-08-29 18:14,,17,,Intermediate,23
52864853,How to get MySql 8 to run with laravel?,"I'm having difficulty getting MySQL 8 to work. This is the error that appears everytime I attempt a php artisan migrate. I've reinstalled MySQL only once so far because I didn't want to hurt my head anymore on what was going on. I've edited the database.php from other possible answers, but that also doesn't seem to work. I saw a possible answer that it's because of MySQL 8's sha256 encryption of the root password, which is why I want to go back to MySQL 5.7 which I've looked up works with laravel just fine. Though, I want to keep the packages up to date and keep MySQL 8 only if i can get it to work with laravel.
PHP 7.2
How do I get MySQL 8 to work with Laravel?
 'mysql' =&gt; [
            'driver' =&gt; 'mysql',
            'host' =&gt; env('DB_HOST', '127.0.0.1'),
            'port' =&gt; env('DB_PORT', '3306'),
            'database' =&gt; env('DB_DATABASE', 'forge'),
            'username' =&gt; env('DB_USERNAME', 'forge'),
            'password' =&gt; env('DB_PASSWORD', ''),
            'unix_socket' =&gt; env('DB_SOCKET', ''),
            'charset' =&gt; 'utf8',
            'collation' =&gt; 'utf8_unicode_ci',
            'prefix' =&gt; '',
            'prefix_indexes' =&gt; true,
            'strict' =&gt; true,
            'engine' =&gt; null,
            'version' =&gt; 8,
            'modes' =&gt; [
                'ONLY_FULL_GROUP_BY',
                'STRICT_TRANS_TABLES',
                'NO_ZERO_IN_DATE',
                'NO_ZERO_DATE',
                'ERROR_FOR_DIVISION_BY_ZERO',
                'NO_ENGINE_SUBSTITUTION',
            ],
        ],
``
Illuminate\Database\QueryException  : SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client (SQL: select * from information_schema.tables where table_schema = laravel_tut and table_name = migrations)
  at /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connection.php:664
    660|         // If an exception occurs when attempting to run a query, we'll format the error
    661|         // message to include the bindings with SQL, which will make this exception a
    662|         // lot more helpful to the developer instead of just the database's errors.
    663|         catch (Exception $e) {
  &gt; 664|             throw new QueryException(
    665|                 $query, $this-&gt;prepareBindings($bindings), $e
    666|             );
    667|         }
    668|
  Exception trace:
  1   PDOException::(""PDO::__construct(): The server requested authentication method unknown to the client [caching_sha2_password]"")
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
  2   PDO::__construct(""mysql:host=127.0.0.1;port=3306;dbname=laravel_tut"", ""root"", ""fdgkadgaf9g7ayaig9fgy9ad8fgu9adfg9adg"", [])
      /Users/home/Projects/laravel_tut/vendor/laravel/framework/src/Illuminate/Database/Connectors/Connector.php:70
UPDATE ANOTHER FIX I DID TO FIX THIS:
With a fresh install of MySQL, i selected NO for encrypting passwords in the set up ( using legacy encryption, not the SHA encryption ) and it started to work with Laravel without any problems - Just use a long and strong password.
reference of the installation step:
https://www.percona.com/blog/wp-content/uploads/2018/05/Installing-MySQL-8.0-on-Ubuntu-2.png
",<php><mysql><laravel><ubuntu><server>,3347,2,46,1747,3,27,52,76,29783,0,307,2,12,2018-10-17 23:17,2018-10-18 0:26,2018-10-18 0:26,1,1,Intermediate,21
49074070,How to make sure my DataFrame frees its memory?,"I have a Spark/Scala job in which I do this:
1: Compute a big DataFrame df1 + cache it into memory
2: Use df1 to compute dfA
3: Read raw data into df2 (again, its big) + cache it
When performing (3), I do no longer need df1. I want to make sure its space gets freed. I cached at (1) because this DataFrame gets used in (2) and its the only way to make sure I do not recompute it each time but only once.
I need to free its space and make sure it gets freed. What are my options?
I thought of these, but it doesn't seem to be sufficient:
df=null
df.unpersist()
Can you document your answer with a proper Spark documentation link?
",<scala><apache-spark><garbage-collection><apache-spark-sql>,629,0,10,1500,1,19,31,77,17492,0,419,3,12,2018-03-02 17:11,2018-03-02 17:20,2018-03-02 20:56,0,0,Intermediate,24
55324144,Entity Framework Core SQLite Connection String Keyword not supported: version,"I created a ASP.NET MVC website using .NET Core 2.2 using a SQLite database. So far it's working well. Trouble begins when I want to add SQLite-specific keywords to the connection string, such as
Data Source=~\\App_Data\\MyDb.db; Version=3; DateTimeFormat=UnixEpoch; DateTimeKind=Utc
Now I get
  Keyword not supported: 'version'
I register the database like this
// ConfigureServices(IServiceCollection services)
var conn = Configuration.GetConnectionString(""MyDB"").Replace(""~"", _env.ContentRootPath);
services.AddDbContext&lt;MyDBContext&gt;(options =&gt; options.UseSqlite(conn));
Then MyDBContext has
public partial class MyDBContext : DbContext
{
    public MyDBContext() { }
    public SatrimonoContext(DbContextOptions&lt;MyDBContext&gt; options)
        : base(options) { }
    public virtual DbSet&lt;Book&gt; Book { get; set; }
}
Then I use it in my page Model
private SatrimonoContext _db;
public BookAccuracyListModel(SatrimonoContext dbContext)
{
    _db = dbContext ?? throw new ArgumentNullException(nameof(dbContext));
}
and from there I can access _db normally via LINQ.
Before posting here, I did plenty of research on the topic, and the best responses I found were this
  This provider is Microsoft.Data.Sqlite. Those connection strings are
  for System.Data.SQLite.
  We support the following keywords: Cache, Data Source, Mode.
and this
  The issue I was having was because I was trying to create a
  SqlConnection instead of a SQLiteConnection. Making that change solved
  my issue.
However, if I'm doing it ""right"", I'm not creating the SqlConnection and thus can't change it to SQLiteConnection. The other response doesn't include a solution.
So how do I get this to work the right way?
",<c#><sqlite><entity-framework-core>,1710,2,19,3544,6,29,61,62,13385,,72,2,12,2019-03-24 13:11,2019-03-24 13:45,2019-03-24 13:45,0,0,Intermediate,19
48105051,Docker - How to take a look at the Tables inside MySQL volume?,"I have imported an SQL file contains my schema and all its tables, By using:
services:
  db:
    image: mysql:5.7
    volumes:
      - db_data:/var/lib/mysql
      - ./resources/file.sql:/docker-entrypoint-initdb.d/file.sql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: db
The problem is, when I trying to retrieve data from some tables an exception in the backend appear:
  throws exception:
  com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table
  'db.Configuration' doesn't exist
      com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'db.Configuration' doesn't exist
And some tables work perfectly like user table.
Although I have tested the SQL file in MySQL Workbench.
The question is, Is there a way I can see what tables are inside the db_data volume?
",<mysql><docker><docker-compose>,813,0,13,2140,3,24,42,36,37761,0,211,4,12,2018-01-04 23:26,2018-01-05 11:14,2018-01-05 11:35,1,1,Basic,14
51298622,AWS Athena (Presto) OFFSET support,"I would like to know if there is support for OFFSET in AWS Athena. For mysql the following query is running but in athena it is giving me error. Any example would be helpful.
select * from employee where empSal >3000 LIMIT 300 OFFSET 20
",<sql><amazon-web-services><presto><amazon-athena>,237,0,0,301,1,7,19,73,13396,0,0,5,12,2018-07-12 6:26,2018-07-13 6:16,2018-07-13 9:42,1,1,Intermediate,23
52307790,Phpmyadmin export issue: count(): Parameter must be an array or an object that implements Countable,"I'm getting issue with PhpMyAdmin when exporting any database. It is coming every time.
Please help me if anyone has solution to resolve all these types of issues in PhpMyAdmin
",<mysql><phpmyadmin><php-7.2>,177,1,0,5554,5,28,52,49,11393,0,320,6,12,2018-09-13 6:29,2018-09-13 6:29,2018-09-13 6:29,0,0,Intermediate,15
59323938,How to automatically set timestamp in room SQLite database?,"I am trying to have SQLite create automatic timestamps with CURRENT_TIMESTAMP.
I took the liberty of using Google's code:
// roomVersion = '2.2.2'
@Entity
public class Playlist {
    @PrimaryKey(autoGenerate = true)
    long playlistId;
    String name;
    @Nullable
    String description;
    @ColumnInfo(defaultValue = ""normal"")
    String category;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String createdTime;
    @ColumnInfo(defaultValue = ""CURRENT_TIMESTAMP"")
    String lastModifiedTime;
}
@Dao
interface PlaylistDao {
    @Insert(onConflict = OnConflictStrategy.REPLACE)
    suspend fun insert(playlist: Playlist): Long
}
This translates into an SQLite-Statement:
CREATE TABLE `Playlist` (
    `playlistId` INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, 
    `name` TEXT, 
    `description` TEXT, 
    `category` TEXT DEFAULT 'normal', 
    `createdTime` TEXT DEFAULT CURRENT_TIMESTAMP, 
    `lastModifiedTime` TEXT DEFAULT CURRENT_TIMESTAMP
)
I did make one insert: 
mDb.playListDao().insert(Playlist().apply { name = ""Test 1"" })
But the timestamps are always Null.
With the DB Browser for SQLite I added another entry, here I get timestamps.
How do I insert without a Null-Timestamp in room?
(Info: createdTime is also always the same as lastModifiedTime. I think this has to be done with triggers in SQLite, but that is a different problem not to be discussed here).
",<sqlite><android-room>,1391,2,31,6094,2,47,67,46,8770,0,101,3,12,2019-12-13 13:53,2019-12-13 14:17,2019-12-13 16:38,0,0,Advanced,33
52817777,Google Big Query: alternatives to browser SQL editor?,"I'm currently working a lot with Google Big Query and I absolutely hate querying in-browser. I'm used to connect to regular DB's through editors like Toad, Microsoft SQL Studio, Teradata Studio Express or Databeaver. I'm look for a similar tool that you guys would recommend for using on Google Big Query.
Only alternative I've found so far are razorsql and jetbrains datagrip (whereas the latter requires a custom connection (https://blog.jetbrains.com/datagrip/2018/07/10/using-bigquery-from-intellij-based-ide/) 
Any idea's on alternatives out there? 
Thanks in advance
",<google-bigquery><datagrip><razorsql>,573,2,0,121,1,1,6,46,17610,0,2,4,12,2018-10-15 13:23,2018-10-22 23:19,,7,,Intermediate,21
61727582,How to avoid unresolved symbol with clj-kond when using hugSQL def-db-fns macro?,"I write Clojure using the VS Code Calva extension, which uses clj-kondo to perform static analysis of my code.
I'm using HugSQL to create Clojure functions from SQL queries and statements.
I'm aware that I could handle the database connection and the HugSQL integration with a library like conman, infact I used it in the past and I like it, but this time I wanted to keep things vanilla and talk to HugSQL myself.
HugSQL's def-db-fns macro takes a SQL file and creates Clojure functions based on the SQL queries and statements contained in that file.
My code below works, but clj-kondo complains that seed-mytable! is an unresolved symbol.
(ns my-app.db
  ""This namespace represents the bridge between the database world and the clojure world.""
  (:require [environ.core :refer [env]]
            [hugsql.core :as hugsql]
            [nano-id.core :refer [nano-id]]))
;; This create the function seed-mytable!, but clj-kondo doesn't (cannot?) know it.
(hugsql/def-db-fns ""sql/mytable.sql"")
;; The functions created by HugSQL can accept a db-spec, a connection, a connection pool,
;; or a transaction object. Let's keep it simple and use a db-spec for a SQLite database.
(def db-spec {:classname ""org.sqlite.JDBC""
              :subprotocol ""sqlite""
              :subname (env :database-subname)})
(defn db-seed
  ""Populate the table with some fakes.""
  []
  (let [fakes [[(nano-id) ""First fake title"" ""First fake content""]
               [(nano-id) ""Second fake title"" ""Second fake content""]]]
    ;; clj-kondo complains that seed-my-table! is an unresolved symbol
    (seed-mytable! db-spec {:fakes fakes})))
I understand why clj-kondo complains: seed-mytable! is not defined anywhere, it's ""injected"" in this namespace when calling the def-db-fns macro.
Is there a way to tell clj-kondo that after calling the hugsql/def-db-fns macro the symbol does indeed exist?
Probably it's not that useful, but this is the SQL file I'm loading with HugSQL.
-- :name seed-mytable!
-- :command :execute
-- :result :affected
-- :doc Seed the `mytable` table with some fakes.
INSERT INTO mytable (id, title, content)
VALUES :t*:fakes;
",<clojure><hugsql><vscode-calva><clj-kondo>,2123,4,34,4641,3,28,37,65,3712,,438,1,12,2020-05-11 10:32,2020-05-11 10:58,2020-05-11 10:58,0,0,Advanced,32
50735318,MySQL default value on NULL,"I have the following table: 
CREATE TABLE `Foo` (
  `id`         int NOT NULL,
  `FirstName`  varchar(255) NULL,
  `LastName`   varchar(255) NOT NULL DEFAULT 'NONE',
  PRIMARY KEY (`id`)
);
When I run the following query it take the default value of 'NONE':
INSERT INTO Foo (`FirstName`) VALUES('FOO');
When I run the following query: 
INSERT INTO Foo (`FirstName`, `LastName`) VALUES('FOO', NULL);
it gives an error:
  [Err] 1048 - Column 'LastName' cannot be null
What I want to achieve is that if a value is NULL then MySQL should use the DEFAULT value.
Does anybody know the solution?
",<mysql><default-value>,589,0,11,3001,17,69,124,47,18808,0,103,3,12,2018-06-07 7:29,2018-06-07 7:33,,0,,Advanced,33
49319731,PhalconPHP Database transactions fail on server,"I have developed a website using PhalconPHP. the website works perfectly fine on my local computer with the following specifications:
PHP Version 7.0.22
Apache/2.4.18
PhalconPHP 3.3.1
and also on my previous Server (with DirectAdmin):
PHP Version 5.6.26
Apache 2
PhalconPHP 3.0.1
But recently I have migrated to a new VPS. with cPanel:
CENTOS 7.4 vmware [server]
cPanel v68.0.30
PHP Version 5.6.34 (multiple versions available, this one selected by myself)
PhalconPHP 3.2.2
On the new VPS my website always gives me Error 500.
in my Apache Error logs file: [cgi:error] End of script output before headers: ea-php70, referer: http://mywebsitedomain.net
What I suspect is the new database System. the new one is not mySql. it is MariaDB 10.1. I tried to downgrade to MySQL 5.6 but the WHM says there is no way I could downgrade to lower versions.
this is my config file:
[database]
adapter  = Mysql
host     = localhost
username = root
password = XXXXXXXXXXXX
dbname   = XXXXXXXXXXXX
charset  = utf8
and my Services.php:
protected function initDb()
{
    $config = $this-&gt;get('config')-&gt;get('database')-&gt;toArray();
    $dbClass = 'Phalcon\Db\Adapter\Pdo\\' . $config['adapter'];
    unset($config['adapter']);
    return new $dbClass($config);
}
And in my controllers...
for example this code throws Error 500:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""id ASC"")-&gt;execute();
but changing id to fname fixes the problem:
$this-&gt;view-&gt;files = Patients::query()-&gt;orderBy(""fname ASC"")-&gt;execute();
or even the following code throws error 500:
$user = Users::findFirst(array(
                         ""conditions"" =&gt; ""id = :id:"",
                         ""bind"" =&gt; array(""id"" =&gt; $this-&gt;session-&gt;get(""userID""))
                        ));
is there a problem with the compatibility of PhalconPHP and MariaDB?
",<php><mysql><mariadb><cpanel><phalcon>,1852,1,40,514,1,6,22,59,489,0,15,2,12,2018-03-16 11:30,2018-03-18 14:42,2018-03-18 14:42,2,2,Advanced,33
61630911,Microsoft SQL Server Management Studio error at startup,"I get this error when I try to run Microsoft SQL Server Management Studio: 
  The application has failed to start because its side-by-side configuration is incorrect. Please see the application event log or use the command-line sxstrace.exe tool for more detail.
SxSTrace detail:
1, 2 
What I did for solving the problem: 
reinstalled SQL Server
reinstalled Microsoft SQL Server Management Studio
updated at recent version of .NET Framework
reinstalled Visual C++ Redistributable 
And I still get that same error.
What should I do?
",<sql-server><configuration><ssms><startup-error>,532,3,0,605,2,6,21,46,19223,0,68,5,12,2020-05-06 8:41,2020-08-18 2:49,,104,,Advanced,33
49557830,Understanding ILIKE ANY element of an array - postgresql,"I have to select all the lines in a table (let's call it mytable) for which the value in a given column (let's call it mycolumn) is not equal to 'A' and not equal to 'S'.
So I tried something like
SELECT * FROM mytable WHERE mycolumn NOT ILIKE ANY(ARRAY['A','S'])
I prefer the use of ILIKE instead of the use of = to test string equalities because the values 'A' and 'S' may come in lower-case in my data, so I want the values 's' and 'a' to be excluded as well.
Strangely enough, the query above did return some lines for which the value inside mycolumn was equal to 'A'. I was very surprised.
Therefore, to understand what was happening I tried to carry out a very simple logical test:
SELECT ('A' ILIKE ANY(ARRAY['A','S'])) as logical_test ;
The statement above returns TRUE, which was expected.
But the following statement also returns TRUE and this is where I'm lost:
SELECT ('A' NOT ILIKE ANY(ARRAY['A','S'])) as logical_test ;
Could someone explain why 'A' NOT ILIKE ANY(ARRAY['A','S']) is considered TRUE by PostgreSQL?
",<sql><postgresql>,1028,0,9,973,2,9,26,55,19881,,46,1,12,2018-03-29 13:49,2018-03-29 13:53,2018-03-29 13:53,0,0,Intermediate,15
56162109,Data Truncation issue while importing excel from Azure Blob storage to Sql Server,"I'm trying to import the below excel file present in the azure blob storage into sql server
EXCEL File
Query
SELECT * 
    FROM OPENROWSET(
        BULK 'container/testfile.xlsx', 
        DATA_SOURCE = 'ExternalSrcImport',
        FORMATFILE='container/test.fmt', FORMATFILE_DATA_SOURCE = 'ExternalSrcImport',
        codepage = 1252,
        FIRSTROW = 1
        ) as data
Format file 
10.0  
4  
1       SQLCHAR       0       7       ""\t""     1     DepartmentID     """"  
2       SQLCHAR       0       100     ""\t""     2     Name             SQL_Latin1_General_CP1_CI_AS  
3       SQLCHAR       0       100     ""\t""     3     GroupName        SQL_Latin1_General_CP1_CI_AS  
4       SQLCHAR       0       24      ""\r\n""   4     ModifiedDate     """"  
Illustration of Format File 
when I execute the query, I'm getting the below error
  Msg 4863, Level 16, State 1, Line 210 Bulk load data conversion error
  (truncation) for row 1, column 1 (DepartmentID).
looks like field terminator in the format file is not working, any ideas to import the file ?
",<sql><sql-server><azure><azure-blob-storage><openrowset>,1051,2,14,92178,19,132,173,78,634,,2567,1,12,2019-05-16 6:27,2019-05-21 17:39,,5,,Advanced,32
60590746,Update jsonb object in postgres,"One of my column is jsonb and have value in the format. The value of a single row of column is below.
{
    ""835"": {
        ""cost"": 0, 
        ""name"": ""FACEBOOK_FB1_6JAN2020"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }, 
    ""876"": {
        ""cost"": 0, 
        ""name"": ""MARVEL_BLACK_WIDOW_4DEC2019"", 
        ""email"": ""test.user@silverpush.co"", 
        ""views"": 0, 
        ""clicks"": 0, 
        ""impressions"": 0, 
        ""campaign_state"": ""paused"", 
        ""processed"":""in_progress"", 
        ""modes"":[""obj1"",""obj2""]
    }
}
I want to update campaign_info(column name) column's the inner key ""processed""  and ""models"" of the campaign_id is ""876"". 
I have tried this query:
update safe_vid_info 
set campaign_info -&gt; '835' --&gt; 'processed'='completed' 
where cid = 'kiywgh'; 
But it didn't work.
Any help is appreciated. Thanks.
",<sql><json><postgresql><sql-update><jsonb>,1024,0,27,395,1,3,13,55,18052,0,104,1,12,2020-03-08 18:31,2020-03-08 23:07,2020-03-08 23:07,0,0,Intermediate,21
53171685,@CreationTimestamp and @UpdateTimestamp is not working with LocalDateTime,"I am trying to use @CreationTimestamp and @UpdateTimestamp with LocalDateTime type, but it is giving me org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp exception.
I am using 5.0.12.hibernate version with java 8 LocalDataTime, 
Is there any way to use @UpdateTimestamp and @CreationTimestamp with Java 8 LocalDateTime ?
 org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in class path resource [org/springframework/boot/autoconfigure/orm/jpa/HibernateJpaAutoConfiguration.class]: Invocation of init method failed; nested exception is org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1080)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)
    at com.godigit.MotorDataService.main(MotorDataService.java:35)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.hibernate.HibernateException: Unsupported property type for generator annotation @CreationTimestamp
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:44)
    at org.hibernate.tuple.CreationTimestampGeneration.initialize(CreationTimestampGeneration.java:22)
    at org.hibernate.cfg.annotations.PropertyBinder.instantiateAndInitializeValueGeneration(PropertyBinder.java:415)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotation(PropertyBinder.java:383)
    at org.hibernate.cfg.annotations.PropertyBinder.getValueGenerationFromAnnotations(PropertyBinder.java:348)
    at org.hibernate.cfg.annotations.PropertyBinder.determineValueGenerationStrategy(PropertyBinder.java:323)
    at org.hibernate.cfg.annotations.PropertyBinder.makeProperty(PropertyBinder.java:269)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyAndValue(PropertyBinder.java:189)
    at org.hibernate.cfg.annotations.PropertyBinder.makePropertyValueAndBind(PropertyBinder.java:199)
    at org.hibernate.cfg.AnnotationBinder.processElementAnnotations(AnnotationBinder.java:2225)
    at org.hibernate.cfg.AnnotationBinder.processIdPropertiesIfNotAlready(AnnotationBinder.java:911)
    at org.hibernate.cfg.AnnotationBinder.bindClass(AnnotationBinder.java:738)
    at org.hibernate.boot.model.source.internal.annotations.AnnotationMetadataSourceProcessorImpl.processEntityHierarchies(AnnotationMetadataSourceProcessorImpl.java:245)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess$1.processEntityHierarchies(MetadataBuildingProcess.java:222)
    at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:265)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:847)
    at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:874)
    at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:60)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:360)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:382)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:371)
    at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:336)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1687)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1624)
    ... 21 common frames omitted
",<java><postgresql><hibernate><spring-boot><java-8>,6078,0,58,1053,1,12,16,36,20151,0,73,2,12,2018-11-06 12:13,2018-11-06 13:00,2018-11-06 13:00,0,0,Intermediate,17
52253791,Pytest: setup testclient and DB,"I'm trying to learn something about testing my flask app. In order to do that, I am using pytest and sqlalchemy.
I want to test a template, whose delivers route some SQL content. So in my opinion I need a testClient for testing the route itself and a DB fixture to manage the DB stuff included in the route.
Here is my fixture:
import pytest
from config import TestingConfig
from application import create_app, db
# ###########################
# ## functional tests
# ###########################
@pytest.fixture(scope='module')
def test_client():
    app = create_app(TestingConfig)
    # Flask provides a way to test your application by exposing the Werkzeug 
    # test Client and handling the context locals for you.
    testing_client = app.test_client()
    with app.app_context():
        db.create_all()
        yield testing_client  # this is where the testing happens!
        db.drop_all()
And this is my basic test:
def test_home_page(test_client):
    """"""
    GIVEN a Flask application
    WHEN the '/' page is requested (GET)
    THEN check the response is valid and contains rendered content
    """"""
    response = test_client.get('/')
    assert response.status_code == 200
    assert ""SOME CONTENT"" in response.data
Running my test fails with:
=================================================================================================== test session starts ===================================================================================================
platform linux -- Python 3.5.2, pytest-3.8.0, py-1.5.4, pluggy-0.7.1
rootdir: /home/dakkar/devzone/private/, inifile:
collected 2 items                                                                                                                                                                                                         
tests/test_main.py 
    SETUP    M test_client
        tests/test_main.py::test_home_page (fixtures used: test_client)F
        tests/test_main.py::test_valid_order_message (fixtures used: test_client).
    TEARDOWN M test_client
======================================================================================================== FAILURES =========================================================================================================
_____________________________________________________________________________________________________ test_home_page ______________________________________________________________________________________________________
self = &lt;sqlalchemy.engine.base.Connection object at 0x7f1c3f29b630&gt;, dialect = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;
constructor = &lt;bound method DefaultExecutionContext._init_compiled of &lt;class 'sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext'&gt;&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
args = (&lt;sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x7f1c3f29b6d8&gt;, [immutabledict({})]), conn = &lt;sqlalchemy.pool._ConnectionFairy object at 0x7f1c3f29b550&gt;
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def _execute_context(self, dialect, constructor,
                         statement, parameters,
                         *args):
        """"""Create an :class:`.ExecutionContext` and execute, returning
            a :class:`.ResultProxy`.""""""
        try:
            try:
                conn = self.__connection
            except AttributeError:
                # escape ""except AttributeError"" before revalidating
                # to prevent misleading stacktraces in Py3K
                conn = None
            if conn is None:
                conn = self._revalidate_connection()
            context = constructor(dialect, self, conn, *args)
        except BaseException as e:
            self._handle_dbapi_exception(
                e,
                util.text_type(statement), parameters,
                None, None)
        if context.compiled:
            context.pre_exec()
        cursor, statement, parameters = context.cursor, \
            context.statement, \
            context.parameters
        if not context.executemany:
            parameters = parameters[0]
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                statement, parameters = \
                    fn(self, cursor, statement, parameters,
                       context, context.executemany)
        if self._echo:
            self.engine.logger.info(statement)
            self.engine.logger.info(
                ""%r"",
                sql_util._repr_params(parameters, batches=10)
            )
        evt_handled = False
        try:
            if context.executemany:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        statement,
                        parameters,
                        context)
            elif not parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor,
                        statement,
                        context)
            else:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(cursor, statement, parameters, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor,
                        statement,
                        parameters,
&gt;                       context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlite3.OperationalError: no such table: order
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
The above exception was the direct cause of the following exception:
test_client = &lt;FlaskClient &lt;Flask 'application'&gt;&gt;
    def test_home_page(test_client):
        """"""
        GIVEN a Flask application
        WHEN the '/' page is requested (GET)
        THEN check the response is valid and contains rendered content
        """"""
&gt;       response = test_client.get('/')
tests/test_main.py:7: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
venv/lib/python3.5/site-packages/werkzeug/test.py:830: in get
    return self.open(*args, **kw)
venv/lib/python3.5/site-packages/flask/testing.py:200: in open
    follow_redirects=follow_redirects
venv/lib/python3.5/site-packages/werkzeug/test.py:803: in open
    response = self.run_wsgi_app(environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:716: in run_wsgi_app
    rv = run_wsgi_app(self.application, environ, buffered=buffered)
venv/lib/python3.5/site-packages/werkzeug/test.py:923: in run_wsgi_app
    app_rv = app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2309: in __call__
    return self.wsgi_app(environ, start_response)
venv/lib/python3.5/site-packages/flask/app.py:2295: in wsgi_app
    response = self.handle_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1741: in handle_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:2292: in wsgi_app
    response = self.full_dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1815: in full_dispatch_request
    rv = self.handle_user_exception(e)
venv/lib/python3.5/site-packages/flask/app.py:1718: in handle_user_exception
    reraise(exc_type, exc_value, tb)
venv/lib/python3.5/site-packages/flask/_compat.py:35: in reraise
    raise value
venv/lib/python3.5/site-packages/flask/app.py:1813: in full_dispatch_request
    rv = self.dispatch_request()
venv/lib/python3.5/site-packages/flask/app.py:1799: in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
application/main/routes.py:20: in index
    func.count(Order.id).label(""orders_count"")
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2947: in one
    ret = self.one_or_none()
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2917: in one_or_none
    ret = list(self)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:2988: in __iter__
    return self._execute_and_instances(context)
venv/lib/python3.5/site-packages/sqlalchemy/orm/query.py:3011: in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:948: in execute
    return meth(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/sql/elements.py:269: in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1060: in _execute_clauseelement
    compiled_sql, distilled_params
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1200: in _execute_context
    context)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1413: in _handle_dbapi_exception
    exc_info
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:265: in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
venv/lib/python3.5/site-packages/sqlalchemy/util/compat.py:248: in reraise
    raise value.with_traceback(tb)
venv/lib/python3.5/site-packages/sqlalchemy/engine/base.py:1193: in _execute_context
    context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x7f1c3f2c4ba8&gt;, cursor = &lt;sqlite3.Cursor object at 0x7f1c3f2c2ce0&gt;
statement = 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""', parameters = ()
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x7f1c3f29b6a0&gt;
    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: order [SQL: 'SELECT sum(""order"".col2_count) AS orders_col2, sum(""order"".col1_count) AS orders_col1, count(""order"".id) AS orders_count \nFROM ""order""'] (Background on this error at: http://sqlalche.me/e/e3q8)
venv/lib/python3.5/site-packages/sqlalchemy/engine/default.py:509: OperationalError
=========================================================================================== 1 failed, 1 passed in 0.52 seconds ============================================================================================
which tells me: db.create_all() does not create all tables in my testing database.
Any hint, what I am doing wrong here?
Some additional info:
using sqlite at the moment
the database file itself gets created in the filesystem with 0byte
More Debugging:
I followed this guide here: https://xvrdm.github.io/2017/07/03/testing-flask-sqlalchemy-database-with-pytest/
this is where thing become strange:
Link from above:
&gt;&gt;&gt; db.engine.table_names()  # Check the tables currently on the engine
[]                           # no table found
&gt;&gt;&gt; db.create_all()          # Create the tables according to defined models
&gt;&gt;&gt; db.engine.table_names()
['users']                    # Now table 'users' is found
What happenes in my project:
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; db.engine.table_names()
[]
&gt;&gt;&gt;
Snipplet from models.py:
from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy()
class Order(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(120), index=True, unique=True)
",<python><python-3.x><flask><sqlalchemy><pytest>,13612,3,252,5702,5,24,28,37,12551,0,30,2,12,2018-09-10 8:26,2018-09-16 12:53,2018-09-16 12:53,6,6,Intermediate,17
49490623,Datetime filtering with SQLAlchemy isn't working,"Basically, I need to build a function that will filter a query according to given dates and return a new query. I'm new to SQLAlchemy, I looked up similar questions but I still got the same error:
`Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 8, 3, 1, 438278)`
Here's my code:
def filter_dates(query_obj, datecols, start = None, end = None):
        if end is None:
            end = datetime.datetime.now()
        if start is None:
            start = end - datetime.timedelta(weeks=12)
        print(""%s to %s"" % (start, end))
        for datecol in datecols:
            print(""Filtrando datas!"")
            query_obj = query_obj.filter(datecol &gt;= start)
            query_obj = query_obj.filter(datecol &lt;= end)
            ReevTable.print_query(query_obj)
        return query_obj
datecols is an orm.attributes object. Suppose I have an Object called User with a Datetime attribute named created_at. This is the expected behaviour:
query = session.query(Company.name, Company.created_at, Company.number_of_employees, Company.email_bounce_rate)
query = filter_dates(query_obj=query, datecols = [Company.created_at, Company.email_events.created_at])
query.all()
Expected output is a table with Companies that were only created within the date range, and the bounce rate should only be calculated during that specified date range. This might seem weird, but I calculate a not just emails, but other kinds of interactions too, so I need to input a list of attributes instead of just a single one. This is why I need to separate this filtering with a method.
I've tried using pandas datetime and timedelta, the built-in python datetime module, and simple strings with pd.to_datetime, but without success. The same error gets raised everytime. My Company column is in DateTime, so I don't know what else to do.
class Company(Base)
    created_at = Column(DateTime, nullable=False)
I'm completely new to SQLAlchemy, what am I doing wrong?
Full traceback:
`Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""reev-data-science/tables/reevtable.py"", line 128, in import_data
    self.print_query(query_obj)
  File ""reev-data-science/tables/reevtable.py"", line 107, in print_query
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)
  File ""&lt;string&gt;"", line 1, in &lt;lambda&gt;
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 442, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/elements.py"", line 448, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 453, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 219, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 245, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/annotation.py"", line 80, in _compiler_dispatch
    self, visitor, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1815, in visit_select
    text, select, inner_columns, froms, byfrom, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1899, in _compose_select_body
    t = select._whereclause._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in visit_clauselist
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 826, in &lt;genexpr&gt;
    s for s in
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 829, in &lt;genexpr&gt;
    for c in clauselist.clauses)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1080, in visit_binary
    return self._generate_generic_binary(binary, opstring, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1113, in _generate_generic_binary
    self, eager_grouping=eager_grouping, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/visitors.py"", line 81, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1244, in visit_bindparam
    bindparam, within_columns_clause=True, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1277, in render_literal_bindparam
    return self.render_literal_value(value, bindparam.type)
  File ""/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/compiler.py"", line 1295, in render_literal_value
    ""Don't know how to literal-quote value %r"" % value)
NotImplementedError: Don't know how to literal-quote value datetime.datetime(2018, 1, 1, 9, 24, 46, 54634)`
The print_query() method:
`def print_query(query_obj):
    print(bcolors.OKBLUE + bcolors.BOLD + str(query_obj.statement.compile(compile_kwargs={""literal_binds"":True})) + bcolors.ENDC)`
",<python><datetime><sqlalchemy>,6262,0,79,321,0,2,11,68,5960,,115,2,12,2018-03-26 11:42,2021-07-16 0:58,,1208,,Advanced,32
56016700,How to create an empty array of struct in hive?,"I have a view in Hive 1.1.0, based on a condition, it should return an empty array or an array of struct&lt;name: string, jobslots: int&gt;
Here is my code:
select
      case when &lt;condition&gt; 
             then array()
           else array(struct(t1.name, t1.jobslots))
       end
from table t1;
The problem here is, that the empty array array() is of type array&lt;string&gt;. So when I try to insert it into a table, it throws an error.
How can I change this to return an empty array of type array&lt;struct&lt;name: string, jobslots:int&gt;&gt; so that Hive's size() function returns 0 on this array?
",<sql><arrays><struct><hive><hiveql>,611,0,12,945,0,12,32,62,7759,0,53,1,12,2019-05-07 6:27,2020-12-19 19:22,,592,,Advanced,32
53552983,How to generate datasets dynamically based on schema?,"I have multiple schema like below with different column names and data types. 
I want to generate test/simulated data using DataFrame with Scala for each schema and save it to parquet file.
Below is the example schema (from a sample json) to generate data dynamically with dummy values in it.
val schema1 = StructType(
  List(
    StructField(""a"", DoubleType, true),
    StructField(""aa"", StringType, true)
    StructField(""p"", LongType, true),
    StructField(""pp"", StringType, true)
  )
)
I need rdd/dataframe like this with 1000 rows each based on number of columns in the above schema.
val data = Seq(
  Row(1d, ""happy"", 1L, ""Iam""),
  Row(2d, ""sad"", 2L, ""Iam""),
  Row(3d, ""glad"", 3L, ""Iam"")
)
Basically.. like this 200 datasets are there for which I need to generate data dynamically, writing separate programs for each scheme is merely impossible for me.
Pls. help me with your ideas or impl. as I am new to spark.
Is it possible to generate dynamic data based on schema of different types? 
",<scala><apache-spark><apache-spark-sql>,997,0,14,900,0,13,27,50,5013,0,423,3,12,2018-11-30 7:21,2018-12-14 20:20,2018-12-17 15:03,14,17,Basic,9
48326311,Sequelize: overlapping - Checking if any value in array matches any value in the passed array,"Using sequelize to check if the the db input property, which is array, has a given item.
Have a Postgres database with data Events.
Want to get one Event that will have any of these weekDays.
Type of weekDays is ARRAY(integer).
Events.findOne({
  where: {
    weekDays: {
      $contains: [2, 3],
    },
  },
});
Tried to do with $contains, $any, or $like $any but all the time got the same error message.
  TypeError: values.map is not a function
Sincerely thanks
",<sql><node.js><database><postgresql><sequelize.js>,465,0,7,515,1,5,17,81,7328,,13,1,12,2018-01-18 16:44,2018-01-19 10:45,2018-01-19 10:45,1,1,Advanced,32
53881132,How to change the search action in wordpress search bar?,"Create a new post and publish it.
The title is my test for search, content in it is as below:
no host route
Check what happen in wordpress database.
 select post_title from wp_posts
     where post_content like ""%no%""
       and post_content like ""%route%""
       and post_content like ""%to%""
       and post_content like ""%host%"";
The post named my test for search will not be in the select's result.
Type no route to host in wordpress search bar,and click enter.
The post named my test for search shown as result.
I found the reason that the webpage contain to ,in the left upper side corner ,there is a word Customize which contains the searched word to.
How to change such search action in wordpress serach bar?
I want to make the search behavior in wordpress saerch bar, for example ,when you type no route to host, equal to the following sql command.
select post_title from wp_posts where post_content like ""%no%route%to%host%"";
All the plugins in my wordpress.
CodePen Embedded Pens Shortcode
Crayon Syntax Highlighter
Disable Google Fonts
Quotmarks Replacer
SyntaxHighlighter Evolved
",<mysql><wordpress><search>,1092,1,20,474,42,145,298,74,988,,387,1,12,2018-12-21 8:01,2018-12-23 18:31,2018-12-23 18:31,2,2,Basic,9
54448139,Microsoft.SqlServer.Types incompatible with .NET Standard,"I'm attempting to convert all of our C# class libraries from .NET Framework to .NET Standard projects, as we are starting to leverage .NET Core so need these to be consumable by both .NET Core and .NET Framework apps (with the latter being ported over to Core in the upcoming months.)
I'm having trouble converting our data access layer code because we leverage Microsoft.SqlServer.Types extensively and the official NuGet package doesn't support .NET Standard. I tried an unofficial NuGet package by dotmorten but it's missing a lot of functionality. Below is a list of everything missing that we would need (thrown together to get the code building...)
public static class SqlMockExtensions
{
    public static SqlBytes STAsBinary(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography MakeValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static int STDimension(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static bool STIsValid(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static Nullable&lt;double&gt; EnvelopeAngle(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography ReorientObject(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static SqlGeography BufferWithTolerance(this SqlGeography geography, double arg1, int arg2, bool arg3) =&gt; throw new NotImplementedException();
    public static SqlGeography Reduce(this SqlGeography geography, double tolerance) =&gt; throw new NotImplementedException();
    public static SqlGeography EnvelopeCenter(this SqlGeography geography) =&gt; throw new NotImplementedException();
    public static double STDistance(this SqlGeography geography, SqlGeography point2) =&gt; throw new NotImplementedException();
    public static SqlBytes STAsBinary(this SqlGeometry geometry) =&gt; throw new NotImplementedException();
}
When I search SO for others trying to integrate Microsoft.SqlServer.Types into their .NET Core and Standard projects, I see mentions of including the official NuGet package and then doing something like this:
SqlServerTypes.Utilities.LoadNativeAssemblies(AppDomain.CurrentDomain.BaseDirectory);
However, it errors when you try to add a non-.NET Standard compliant NuGet package into a .NET Standard project, so I'm not clear how this is a solution.
This seems like a very common problem to have, there have to be a lot of developers out there who leverage Microsoft.SqlServer.Types for SqlGeography, SqlGeometry, etc... and are porting over to .NET Standard. So how are all of you accomplishing this?
",<c#><sql-server><.net-standard><sqlgeography><sqlgeometry>,2717,3,25,17807,38,132,203,58,4761,0,7782,1,12,2019-01-30 19:28,2021-07-25 13:26,,907,,Intermediate,18
48350843,How to connect from docker-compose to Host PostgreSQL?,"I have a server with installed PostgreSQL. All my services work in containers (docker-compose). I want to use my Host PostgreSQL from containers. Buy I have the error:
  Unable to obtain Jdbc connection from DataSource (jdbc:postgresql://localhost:5432/shop-bd) for user 'shop-bd-user': Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  SQL State  : 08001
  Error Code : 0
  Message    : Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.
  Connection refused (Connection refused)
My docker-compose is using host network_mode, like this:
version: '3'
services:
  shop:  
    container_name: shop
    build: ./shop
    hostname: shop
    restart: always   
    ports:
      - 8084:8084
    network_mode: ""host""
My database connection URL is: jdbc:postgresql://localhost:5432/shop-bd
",<postgresql><docker><networking><localhost><docker-compose>,1360,0,18,1813,3,26,39,68,19448,0,57,4,12,2018-01-19 23:11,2018-01-20 8:32,2018-04-15 7:25,1,86,Basic,9
50467598,Multi-homed SQL Server with High Availability Groups,"We have two servers (SQL-ATL01, SQL-ATL02) that make up a Failover Cluster, each running as part of a  SQL Server High Availability Group (HAG). Each server has two network cards. One is a 10Gbit card that is directly connected to the other server and is used for Synchronizing the HAG on a 192.168.99.x subnet. The other is a 1Gbit card that is used to connect the DB servers to a switch to communicate with the application servers on a 10.0.0.x subnet. The Listener is pointed to the  192.168.99.x subnet.
We want to add a third server (SQL-NYC01) in another physical location to the cluster and run it as an Async replica part of the HAG, but the VPN only routes traffic on the subnet on the 1Gbit network.
Is there any way to set up the Failover Cluster and High Availability Group to tell it:
Send synchronous replica traffic for SQL-ATL01 &lt;--> SQL-ATL02 over 192.168.99.x
Send asynchronous replica traffic for (SQL-ATL01, SQL-ATL02) &lt;--> SQL-NYC01 over 10.0.0.x
Or do we have to have all replica traffic going in and out on the same IP address/subnet?  
",<sql-server><high-availability>,1066,0,0,7461,3,32,33,72,479,,327,1,12,2018-05-22 12:23,2018-05-30 9:31,,8,,Advanced,32
51088034,Hibernate @Formula doesn't include Schema,"I have an entity with a property @Formula like this:
@Entity
@Table(name = ""areasAuxiliar"")
public final class AreaAuxiliar implements Serializable {
    @Id
    @Column(name = ""idArea"")
    private Integer idArea;
    @Formula(""RUTAAREA(idArea)"")
    private String ruta;
when I configure my hibernate to point to an Oracle DB I have no problem,
BUT, when I switch to an SQLServer, hibernate is not including the shema and the query fails,
the query generated for hibernate looks like this:
select
    areaauxili4_.idArea as idArea1_6_4_,
    rutaArea(areaauxili4_.idArea) as formula2_4_
from
    SIGAP.areasAuxiliar areaauxili4_ 
the param hibernate.default_schema=SIGAP is being read and included in the table but not in the function,
is there an option/annotation to force the shema in in that function?
I have tried hibernate 5.1 and 5.2 with the same result :(
",<java><sql-server><hibernate><jpa><hql>,867,0,15,568,3,7,27,68,2683,,221,4,12,2018-06-28 16:54,2018-07-25 16:15,2018-07-31 23:30,27,33,Advanced,35
57681970,PSQLException and lock issue when trigger added on table,"UPDATE: I eliminated Hibernate from the problem. I completely reworked description of problem to simplify it as much as possible.
I have master table with noop trigger and detail table with two relations between master and detail table:
create table detail (
  id bigint not null,
  code varchar(255) not null,
  primary key (id)
);
create table master (
  id bigint not null,
  name varchar(255),
  detail_id bigint, -- ""preferred"" detail is one-to-one relation
  primary key (id),
  unique (detail_id),
  foreign key (detail_id) references detail(id)
);
create table detail_candidate ( -- ""candidate"" details = many-to-many relation modeled as join table
  master_id bigint not null,
  detail_id bigint not null,
  primary key (master_id, detail_id),
  foreign key (detail_id) references detail(id),
  foreign key (master_id) references master(id)
);
create or replace function trgf() returns trigger as $$
begin
  return NEW;
end;
$$ language 'plpgsql';
create trigger trg
  before insert or update
  on master
  for each row execute procedure trgf();
insert into master (id, name) values (1000, 'x'); -- this is part of database setup
insert into detail (code, id) values ('a', 1);    -- this is part of database setup
In such setup, I open two terminal windows with psql and perform following steps:
in first terminal, change master (leave transaction open)
begin;
update master set detail_id=null, name='y' where id=1000;
in second terminal, add detail candidate to master in own transaction
begin;
set statement_timeout = 4000;
insert into detail_candidate (master_id, detail_id) values (1000, 1);
Last command in second terminal timeouts with message
ERROR:  canceling statement due to statement timeout
CONTEXT:  while locking tuple (0,1) in relation ""master""
SQL statement ""SELECT 1 FROM ONLY ""public"".""master"" x WHERE ""id"" OPERATOR(pg_catalog.=) $1 FOR KEY SHARE OF x""
My observation and questions (changes are independent):
when the db is setup without trigger, i.e. drop trigger trg on master; is called after initial setup, everything works fine.
Why the presence of noop trigger has such an influence? I don't get it.
when the db is setup without unique constraint on master.detail_id (i.e. alter table master drop constraint master_detail_id_key; is called after initial setup), everything works fine too. Why?
when I omit explicit detail=null assignment in update statement in first terminal (since there's null value from setup anyway), everything works fine too. Why?
Tried on Postgres 9.6.12 (embedded), 9.6.15 (in Docker), 11.5 (in Docker).
Problem is reproducible in Docker image tomaszalusky/trig-example which is available on DockerHub or can be built from this Dockerfile (instructions inside).
UPDATE 2: I found common behaviour of three observation above. I spawned the query select * from pgrowlocks('master') from pgrowlocks extension in second transaction. The row-level lock of updated row in master is FOR UPDATE in failing case but FOR NO KEY UPDATE in all three working cases. This is in perfect compliance with mode match table in documentation since FOR UPDATE mode is the stronger one and mode requested by insert statement is FOR KEY SHARE (which is apparent from error message, also invoking the select ... for key share command has same effect as insert command).
The documentation on FOR UPDATE mode says:
  The FOR UPDATE lock mode is also acquired by (...) an UPDATE that modifies the values on certain columns. Currently, the set of columns considered for the UPDATE case are those that have a unique index on them that can be used in a foreign key (...)
It is true for master.detail_id column. However, still it's not clear why FOR UPDATE mode isn't chosen independently on trigger presence and why trigger presence caused it.
",<postgresql><triggers><locking>,3772,4,65,11000,2,39,64,39,3384,,1413,1,12,2019-08-27 20:40,2019-09-05 21:42,2019-09-05 21:42,9,9,Advanced,35
55145223,SSIS job failed - An item with the same key has already been added,"Our SSIS package fails 2 seconds into the run with the following error:
An item with the same key has already been added.;   at System.ThrowHelper.ThrowArgumentException(ExceptionResource resource)
   at System.Collections.Generic.Dictionary`2.Insert(TKey key, TValue value, Boolean add)
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.StartPackage()
   at Microsoft.SqlServer.IntegrationServices.Server.ISServerExec.ProjectOperator.PerformOperation()    Transact-SQL stored procedure
We deploy our SSIS packages (2016) to our SSISDB on MSSQL: 13.0.4001.0, we also have environment variables in that same folder
The strange thing is that in this project I have 5 packages that run ok (different job) but only 1 fails (it has its’ own job)
I already deleted a recreated the folder/ environment variables/jobs  - same result
I made sure I have Different environment variables folder name
We run the job with different users and 2 of them were admins in the DB
We have other servers which have the same config and project (2 QA environments) and they work the same but OK!
Do I need to go directly to MSDB and delete a row? Where?
Appreciate any ideas – thank you all
I found the error and solution 😊
It seems I deployed a package with 2 same named SMTP connection (one as a project connection and the other with same name as a package connection)
I deleted the unnecessary (in my case deleted the package one) and deployed the fixed package.
Now the job run successfully calling on one only connection.
I was missing the correct error handling from the error itself since it did not direct me to the specific connection
The package failed at runtime on validations – so no error logs to assist
I run the query from [SSISDB].[catalog].[execution_parameter_values] and compared between a successful run to a failed then I noticed the same named connections   
Thank you for your comments!
Yoni 
",<sql-server><ssis>,1925,0,0,341,1,2,10,35,17411,0,2,3,12,2019-03-13 15:11,2019-03-14 14:05,,1,,Basic,13
53855219,"MySQL not updating information_schema, unless I manually run ANALYZE TABLE `myTable`","I have the need to get last id (primary key) of a table (InnoDB), and to do so I perform the following query:
SELECT (SELECT `AUTO_INCREMENT` FROM `information_schema`.`TABLES` WHERE `TABLE_SCHEMA` = 'mySchema' AND `TABLE_NAME` = 'myTable') - 1;
which returns the wrong AUTO_INCREMENT. The problem is the TABLES table of information_schema is not updated with the current value, unless I run the following query:
ANALYZE TABLE `myTable`;
Why doesn't MySQL update information_schema automatically, and how could I fix this behavior?
Running MySQL Server 8.0.13 X64.
",<mysql><innodb><information-schema><mysql-8.0>,565,0,2,928,2,13,33,64,15108,,32,3,12,2018-12-19 16:17,2018-12-19 16:44,2018-12-19 16:44,0,0,Intermediate,16
54821962,Connect to On Prem SQL server from Azure Web app,"I have .Net application at on prim. I want to host it at Azure but don't want to move database. I publish the application on Azure but it is not able to connect to on prim database. 
SQL server is in private netwrok.
For POC purpose, I am using MSDN subscription. I am facing below error,
A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error: 40 - Could not open a connection to SQL Server)
Thanks,
Umesh
",<c#><sql-server><azure><azure-hybrid-connections>,644,0,0,2915,5,16,18,43,18362,0,10,2,12,2019-02-22 7:19,2019-02-22 7:25,2019-02-22 7:51,0,0,Intermediate,22
56110263,pgAdmin argument formats can't be mixed,"Background
Ubuntu 18.04
Postgresql 11.2 in Docker 
pgAdmin4 3.5
Have a column named alias with type character varying[](64). Values have already been set on some rows before using psycopg2. Everything was alright then.
SQL = 'UPDATE public.""mytable"" SET alias=%s WHERE id=%s'
query = cursor.mogrify(SQL, ([values] , id))
cursor.execute(query)
conn.commit()
Recently, when I want to add more value using pgAdmin GUI as shown in the first figure, the error in the second figure happens, which says Argument formats can't be mixed:
Well, it turns out if insert the values using script such as psql or query tool in pgAdmin, the error does not happen, i.e., it only happens if using GUI of pgAdmin.
Example script:
UPDATE public.""mytable"" SET alias='{a, b}' WHERE id='myid'
But as the GUI is much easier to modify values, so really want to figure it out. Any idea?
",<postgresql><pgadmin>,861,2,7,2370,3,18,36,62,2083,,44,2,12,2019-05-13 10:13,2019-09-17 16:51,,127,,Basic,9
52143396,JDBC SQL Server : The value is not set for the parameter number,"I received the following error from the code that calls a stored procedure from java code:    
  Exception Trace {} org.springframework.jdbc.UncategorizedSQLException:
  CallableStatementCallback; uncategorized SQLException for SQL [{call
  test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, ,
  ?, ?, ?, ?, ?)}]; SQL state [null]; error code [0]; The value is not
  set for the parameter number 11.; nested exception is
  com.microsoft.sqlserver.jdbc.SQLServerException: The value is not set
  for the parameter number 11. at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
  at
  org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:1095)
  at
  org.springframework.jdbc.core.JdbcTemplate.call(JdbcTemplate.java:1131)
The application is deployed on WAS 8.5.5 and using jdbc driver version 4.2. On restarting the server this issue did not occur again. The following call statement generated looks to be incorrect. There are consecutive commas without ? between them.
  {call test.usp_xxx_GetCompanyDetails(?, ?, ?, ?, ?, , ?, , ?, ?, ?, ?,
  ?)}
The stored procedure has 10 parameters. Following is the definition of the stored procedure:
CREATE PROCEDURE [test].[usp_xxx_GetCompanyDetails]
(
    @ANumber        int,
    @CompanyId      int,
    @UserRole       varchar(15),
    @RequestId      varchar(100),
    @CompanyCode    varchar(5),
    @BaseSystem     varchar(5),
    @PType          varchar(20),    
    @PId            varchar(40),
    @IsActive       bit,     
    @responseData   xml OUT
)
Following is the java code that makes a call to the stored proc. It is using spring data to make the call.
 private String executeProc(Integer aNumber,Integer companyId, String baseSystem, 
                String role,String companyCode,String requestId, String pType,String pId,
                boolean isActive ) throws SQLException {
            SQLXML responseData=null;
            Map&lt;String,Object&gt; inputParams= new HashMap&lt;&gt;();
            inputParams.put(""ANumber"", aNumber);
            inputParams.put(""CompanyId"", companyId);
            inputParams.put(""UserRole"", role);
            inputParams.put(""RequestId"", requestId);
            inputParams.put(""CompanyCode"", companyCode);
            inputParams.put(""BaseSystem"", baseSystem);
            inputParams.put(""PType"", pType);
            inputParams.put(""PId"", pId);
            inputParams.put(""IsActive"", isActive);
            inputParams.put(""ResponseData"", responseData);
            Map&lt;String, Object&gt; result = this.execute(inputParams);
            String responseXMLString = ((SQLXML) result.get(""ResponseData"")).getString();
            return responseXMLString;
        }
What could have gone wrong.
",<java><sql-server><jdbc><sql-server-2012><websphere>,3077,0,35,1235,0,16,36,59,5994,0,62,2,12,2018-09-03 5:02,2018-09-03 5:09,,0,,Advanced,32
52537741,multiprocessing / psycopg2 TypeError: can't pickle _thread.RLock objects,"I followed the below code in order to implement a parallel select query on a postgres database:
https://tech.geoblink.com/2017/07/06/parallelizing-queries-in-postgresql-with-python/
My basic problem is that I have ~6k queries that need to be executed, and I am trying to optimise the execution of these select queries. Initially it was a single query with the where id in (...) contained all 6k predicate IDs but I ran into issues with the query using up > 4GB of RAM on the machine it ran on, so I decided to split it out into 6k individual queries which when synchronously keeps a steady memory usage. However it takes a lot longer to run time wise, which is less of an issue for my use case. Even so I am trying to reduce the time as much as possible.
This is what my code looks like:
class PostgresConnector(object):
    def __init__(self, db_url):
        self.db_url = db_url
        self.engine = self.init_connection()
        self.pool = self.init_pool()
    def init_pool(self):
        CPUS = multiprocessing.cpu_count()
        return multiprocessing.Pool(CPUS)
    def init_connection(self):
        LOGGER.info('Creating Postgres engine')
        return create_engine(self.db_url)
    def run_parallel_queries(self, queries):
        results = []
        try:
            for i in self.pool.imap_unordered(self.execute_parallel_query, queries):
                results.append(i)
        except Exception as exception:
            LOGGER.error('Error whilst executing %s queries in parallel: %s', len(queries), exception)
            raise
        finally:
            self.pool.close()
            self.pool.join()
        LOGGER.info('Parallel query ran producing %s sets of results of type: %s', len(results), type(results))
        return list(chain.from_iterable(results))
    def execute_parallel_query(self, query):
        con = psycopg2.connect(self.db_url)
        cur = con.cursor()
        cur.execute(query)
        records = cur.fetchall()
        con.close()
        return list(records)
However whenever this runs, I get the following error:
TypeError: can't pickle _thread.RLock objects
I've read lots of similar questions regarding the use of multiprocessing and pickleable objects but I cant for the life of me figure out what I am doing wrong.
The pool is generally one per process (which I believe is the best practise) but shared per instance of the connector class so that its not creating a pool for each use of the parallel_query method.
The top answer to a similar question:
Accessing a MySQL connection pool from Python multiprocessing
Shows an almost identical implementation to my own, except using MySql instead of Postgres.
Am I doing something wrong?
Thanks!
EDIT: 
I've found this answer:
Python Postgres psycopg2 ThreadedConnectionPool exhausted
which is incredibly detailed and looks as though I have misunderstood what multiprocessing.Pool vs a connection pool such as ThreadedConnectionPool gives me. However in the first link it doesn't mention needing any connection pools etc. This solution seems good but seems A LOT of code for what I think is a fairly simple problem?
EDIT 2: 
So the above link solves another problem, which I would have likely run into anyway so I'm glad I found that, but it doesnt solve the initial issue of not being able to use imap_unordered down to the pickling error. Very frustrating. 
Lastly, I think its probably worth noting that this runs in Heroku, on a worker dyno, using Redis rq for scheduling, background tasks etc and a hosted instance of Postgres as the database. 
",<python><postgresql><sqlalchemy><psycopg2><python-multiprocessing>,3558,4,43,545,2,6,25,56,5165,0,21,1,12,2018-09-27 13:07,2018-10-08 13:33,2018-10-08 13:33,11,11,Advanced,32
52096692,Change sequelize timezone,"I want to make restful app in nodejs
Server: centos 7 64x
Database: postgresql
Additional: express, sequelize
Table: datetime with timezone
When I selecting rows with sequelize from database, created_at column gives me wrong time. 5 hour added to datetime.
I change timezone configuration of centos to +5 (Tashkent/Asia)
Also change postgresql timezone configuration to +5
Datetime is correct in database when shows.
But when I select it converts to like this
""createdAt"": ""2018-08-12T17:57:20.508Z""
In database column shows this
2018-08-12 22:57:20.508+05
config.json
""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""zablet"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""timezone"": ""Tashkent/Ashgabat"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    }
}
index.js
'use strict';
var fs = require('fs');
var path = require('path');
var Sequelize = require('sequelize');
var basename = path.basename(__filename);
var env = process.env.NODE_ENV || 'development';
var config = require('../config/config.json')[env];
var db = {};
if (config.use_env_variable) {
    var sequelize = new Sequelize(process.env[config.use_env_variable], config);
} else {
    var sequelize = new Sequelize(config.database, config.username, config.password, config);
}
fs
    .readdirSync(__dirname)
    .filter(file =&gt; {
        return (file.indexOf('.') !== 0) &amp;&amp; (file !== basename) &amp;&amp; (file.slice(-3) === '.js');
    })
    .forEach(file =&gt; {
        var model = sequelize['import'](path.join(__dirname, file));
        db[model.name] = model;
    });
Object.keys(db).forEach(modelName =&gt; {
    if (db[modelName].associate) {
        db[modelName].associate(db);
    }
});
db.sequelize = sequelize;
db.Sequelize = Sequelize;
module.exports = db;
updated config.json
{
    ""development"": {
    ""username"": ""postgres"",
    ""password"": ""postgres"",
    ""database"": ""postgres"",
    ""host"": ""127.0.0.1"",
    ""dialect"": ""postgres"",
    ""define"": {
        ""charset"": ""utf8"",
        ""dialectOptions"": {
            ""collate"": ""utf8_general_ci""
        },
        ""freezeTableName"": true
    },
    ""dialectOptions"": {
        ""useUTC"": false
    },
    ""timezone"": ""+05:00""
}
}
How can I select rows from database in correct timezone format?
",<node.js><postgresql><express><sequelize.js>,2401,0,66,553,4,11,31,67,44370,0,37,2,12,2018-08-30 12:07,2018-08-30 13:53,2018-08-30 13:53,0,0,Basic,2
58905555,"Cannot connect to MySQL database (running on WSL2) from Windows Desktop Application ""MySQL Workbench""","I set up MySQL on Windows Subsystem for Linux (WSL2 version).  I'm relatively new to MySQL, but I have confirmed the following: 
It is running (ps ax | grep mysqld returns a value)
It is running on default host 127.0.0.1
It is running on default port 3306
To login to the mysql shell, I use the command sudo mysql -u root -p.  Without sudo, I am unable to login to the shell.
I assume that this issue has something to do with the host that the MySQL service is running on, but I have no idea how to change that and properly connect.  Below is a screenshot of the connection setup in MySQL Workbench.
And below is the error that I get when I use the settings shown and my root user password.
",<mysql-workbench><windows-subsystem-for-linux>,691,3,5,2164,2,17,26,58,23856,0,116,6,12,2019-11-17 21:51,2019-12-06 19:10,,19,,Basic,9
53800062,"""expected zero arguments for construction of ClassDict (for numpy.dtype)"" when calling UDF that returns FloatType()","I believe it is related to this one: Spark Error:expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
I have a dataframe
id col_1 col_2
1 [1,2] [1,3]
2 [2,1] [3,4]
I want to create another column that is a cosine distance between col_1 and col_2.
from scipy.spatial.distance import cosine
def cosine_distance(a,b):
    try:
        return cosine(a, b)
    except Exception as e:
        return 0.0 # in case division by zero
And I defined a udf:
cosine_distance_udf = udf (cosine_distance, FloatType())
And finally:
new_df = df.withColumn('cosine_distance', cosine_distance_udf('col_1', 'col_2'))
And I have the error: PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)
What did I do wrong?
",<python><dataframe><pyspark><apache-spark-sql>,771,1,17,4480,11,46,76,35,9896,0,102,1,12,2018-12-16 6:52,2018-12-16 7:10,2018-12-16 7:10,0,0,Advanced,32
54454797,where IN clause - multi column (querydsl),"I have three integer values of two pairs.
I would like to use this as a list of IN in the WHERE clause.
  (2019, 5) (2019, 6) (2019, 7)
I want to use the above two-pair list in a query like this:
SELECT
    *
FROM
    WEEK_REPORT A
WHERE
    A.user_id IN ('test2','test5' ) AND
    (A.year, A.week_no) IN ((2019,4),(2019,5),(2019,6));
To that end, I wrote the source as follows:
// lastYear=2019, lastWeekNo=4
Tuple t = JPAExpressions.select(Expressions.constant(lastYear), Expressions.constant(lastWeekNo))
    .from(weekReport)
    .fetchOne();
// currentYear=2019, currentWeekNo=5        
Tuple t2 = JPAExpressions.select(Expressions.constant(currentYear), Expressions.constant(currentWeekNo))
    .from(weekReport)
    .fetchOne();
// nextYear=2019, nextWeekNo=4
Tuple t3 = JPAExpressions.select(Expressions.constant(nextYear), Expressions.constant(nextWeekNo))
    .from(weekReport)
    .fetchOne();
return queryFactory
    .select(weekReport)
    .from(weekReport)
    .where(weekReport.user.eq(user)
        .and(Expressions.list(weekReport.year, weekReport.weekNo).in(t, t2, t3)))
    .fetch();
However, the correct result is not output and an error occurs.
java.lang.UnsupportedOperationException: null
    at com.querydsl.jpa.JPASubQuery.fetchOne(JPASubQuery.java:66) ~[querydsl-jpa-4.1.4.jar:na]
I looked it up in the official document but it does not come out.
Is there a way?  
Thank you.
",<mysql><sql><querydsl><in-clause>,1402,0,30,323,0,2,9,67,2476,0,0,4,12,2019-01-31 6:44,2019-01-31 6:53,,0,,Basic,10
52803014,"Sqlite with real ""Full Text Search"" and spelling mistakes (FTS+spellfix together)","Let's say we have 1 million of rows like this:
import sqlite3
db = sqlite3.connect(':memory:')
c = db.cursor()
c.execute('CREATE TABLE mytable (id integer, description text)')
c.execute('INSERT INTO mytable VALUES (1, ""Riemann"")')
c.execute('INSERT INTO mytable VALUES (2, ""All the Carmichael numbers"")')
Background:
I know how to do this with Sqlite:
Find a row with a single-word query, up to a few spelling mistakes with the spellfix module and Levenshtein distance (I have posted a detailed answer here about how to compile it, how to use it, ...):
db.enable_load_extension(True)
db.load_extension('./spellfix')
c.execute('SELECT * FROM mytable WHERE editdist3(description, ""Riehmand"") &lt; 300'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(1, u'Riemann')]
With 1M rows, this would be super slow! As detailed here, postgresql might have an optimization with this using trigrams. A fast solution, available with Sqlite, is to use a VIRTUAL TABLE USING spellfix:
c.execute('CREATE VIRTUAL TABLE mytable3 USING spellfix1')
c.execute('INSERT INTO mytable3(word) VALUES (""Riemann"")')
c.execute('SELECT * FROM mytable3 WHERE word MATCH ""Riehmand""'); print c.fetchall()
#Query: 'Riehmand'
#Answer: [(u'Riemann', 1, 76, 0, 107, 7)], working!
Find an expression with a query matching one or multiple words with FTS (""Full Text Search""):
c.execute('CREATE VIRTUAL TABLE mytable2 USING fts4(id integer, description text)')
c.execute('INSERT INTO mytable2 VALUES (2, ""All the Carmichael numbers"")')
c.execute('SELECT * FROM mytable2 WHERE description MATCH ""NUMBERS carmichael""'); print c.fetchall()
#Query: 'NUMBERS carmichael'
#Answer: [(2, u'All the Carmichael numbers')]
It is case insensitive and you can even use a query with two words in the wrong order, etc.: FTS is quite powerful indeed. But the drawback is that each of the query-keyword must be correctly spelled, i.e. FTS alone doesn't allow spelling mistakes.
Question:
How to do a Full Text Search (FTS) with Sqlite and also allow spelling mistakes? i.e. ""FTS + spellfix"" together
Example: 
row in the DB: ""All the Carmichael numbers""
query: ""NUMMBER carmickaeel"" should match it!
How to do this with Sqlite? 
It is probably possible with Sqlite since this page states:
  Or, it [spellfix] could be used with FTS4 to do full-text search using potentially misspelled words.
Linked question: String similarity with Python + Sqlite (Levenshtein distance / edit distance)
",<python><sqlite><full-text-search><levenshtein-distance>,2431,4,30,42385,103,394,707,36,8839,0,3760,2,12,2018-10-14 13:12,2018-10-17 17:42,2018-10-17 17:42,3,3,Basic,10
56321781,Hangfire causing locks in SQL Server,"We are using Hangfire 1.7.2 within our ASP.NET Web project with SQL Server 2016. We have around 150 sites on our server, with each site using Hangfire 1.7.2. We noticed that when we upgraded these sites to use Hangfire, the DB server collapsed. Checking the DB logs, we found out there were multiple locking queries. We have identified one RPC Event  “sys.sp_getapplock;1” In the all blocking sessions. It seems like Hangfire is locking our DB rendering whole DB unusable. We noticed almost 670+ locking queries because of Hangfire.
This could possibly be due to these properties we setup:
   SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
   QueuePollInterval = TimeSpan.FromHours(5)
Each site has around 20 background jobs, a few of them run every minute, whereas others every hour, every 6 hours and some once a day.
I have searched the documentation but could not find anything which could explain these two properties or how to set them to avoid DB locks.
Looking for some help on this.
EDIT: The following queries are executed at every second:
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
select distinct(Queue) from [HangFire].JobQueue with (nolock)
exec sp_executesql N'select count(*) from [HangFire].[Set] with (readcommittedlock, forceseek) where [Key] = @key',N'@key nvarchar(4000)',@key=N'retries'
irrespective of various combinations of timespan values we set. Here is the code of GetHangfirServers we are using:
  public static IEnumerable&lt;IDisposable&gt; GetHangfireServers()
    {
        // Reference for GlobalConfiguration.Configuration: http://docs.hangfire.io/en/latest/getting-started/index.html
        // Reference for UseSqlServerStorage: http://docs.hangfire.io/en/latest/configuration/using-sql-server.html#configuring-the-polling-interval
        GlobalConfiguration.Configuration
            .SetDataCompatibilityLevel(CompatibilityLevel.Version_170)
            .UseSimpleAssemblyNameTypeSerializer()
            .UseRecommendedSerializerSettings()
            .UseSqlServerStorage(ConfigurationManager.ConnectionStrings[""abc""]
                .ConnectionString, new SqlServerStorageOptions
            {
                CommandBatchMaxTimeout = TimeSpan.FromMinutes(5),
                SlidingInvisibilityTimeout = TimeSpan.FromMinutes(30),
                QueuePollInterval = TimeSpan.FromHours(5), // Hangfire will poll after 5 hrs to check failed jobs.
                UseRecommendedIsolationLevel = true,
                UsePageLocksOnDequeue = true,
                DisableGlobalLocks = true
            });
        // Reference: https://docs.hangfire.io/en/latest/background-processing/configuring-degree-of-parallelism.html
        var options = new BackgroundJobServerOptions
        {
            WorkerCount = 5
        };
        var server = new BackgroundJobServer(options);
        yield return server;
    }
The worker count is set just to 5.
There are just 4 jobs and even those are completed (SELECT * FROM [HangFire].[State]):
Do you have any idea why the Hangfire is hitting so many queries at each second?
",<c#><sql-server><backgroundworker><hangfire>,3178,4,37,8900,6,84,109,67,12970,0,1086,3,12,2019-05-27 7:46,2019-06-03 8:46,,7,,Basic,10
50238568,How to group by time bucket in ClickHouse and fill missing data with nulls/0s,"Suppose I have a given time range. For explanation, let's consider something simple, like whole year 2018. I want to query data from ClickHouse as a sum aggregation for each quarter so the result should be 4 rows. 
The problem is that I have data for only two quarters so when using GROUP BY quarter, only two rows are returned.
SELECT
     toStartOfQuarter(created_at) AS time,
     sum(metric) metric
 FROM mytable
 WHERE
     created_at &gt;= toDate(1514761200) AND created_at &gt;= toDateTime(1514761200)
    AND
     created_at &lt;= toDate(1546210800) AND created_at &lt;= toDateTime(1546210800)
 GROUP BY time
 ORDER BY time
1514761200 – 2018-01-01
1546210800 – 2018-12-31
This returns:
time       metric
2018-01-01 345
2018-04-01 123
And I need:
time       metric
2018-01-01 345
2018-04-01 123
2018-07-01 0
2018-10-01 0
This is simplified example but in real use case the aggregation would be eg. 5 minutes instead of quarters and GROUP BY would have at least one more attribute like GROUP BY attribute1, time so desired result is
time        metric  attribute1
2018-01-01  345     1
2018-01-01  345     2
2018-04-01  123     1
2018-04-01  123     2
2018-07-01  0       1
2018-07-01  0       2
2018-10-01  0       1
2018-10-01  0       2
Is there a way to somehow fill the whole given interval? Like InfluxDB has fill argument for group or TimescaleDb's time_bucket() function with generate_series()  I tried to search ClickHouse documentation and github issues and it seems this is not implemented yet so the question perhaps is whether there's any workaround.
",<sql><clickhouse>,1570,2,36,11928,18,90,141,35,15729,0,1945,4,12,2018-05-08 16:46,2018-05-14 8:04,2018-05-14 8:04,6,6,Basic,2
52186125,Moving Wordpress site to Docker: Error establishing DB connection,"Ive been making new sites with Wordpress &amp; Docker recently and have a reasonable grasp of how it all works and Im now looking to move some established sites into Docker.
Ive been following this guide:
https://stephenafamo.com/blog/moving-wordpress-docker-container/
I have everything setup as it should be but when I go to my domain.com:1234 I get the error message 'Error establishing a database connection'. I have changed 'DB HOST' to 'mysql' in wp-config.php as advised and all the DB details from the site Im bringing in are correct.
I have attached to the mysql container and checked that the db is there and with the right user and also made sure the pw is correct via mysql CLI too.
SELinux is set to permissive and I havent changed any dir/file ownership nor permissions and for the latter dirs are all 755 and files 644 as they should be.
Edit: I should mention that database/data and everything under that seem to be owned by user/group 'polkitd input' instead of root. 
Docker logs aren't really telling me much either apart from the 500 error messages for the WP container when I browse the site on port 1234 (as expected though).
This is the docker-compose file:
version: '2'
services:
  example_db:
    image: mysql:latest
    container_name: example_db
    volumes:
      - ./database/data:/var/lib/mysql
      - ./database/initdb.d:/docker-entrypoint-initdb.d
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password123 # any random string will do
      MYSQL_DATABASE: mydomin_db # the name of your mysql database
      MYSQL_USER: my domain_me # the name of the database user
      MYSQL_PASSWORD: password123 # the password of the mysql user
  example:
    depends_on:
      - example_db
    image: wordpress:php7.1 # we're using the image with php7.1
    container_name: example
    ports:
      - ""1234:80""
    restart: always
    links:
      - example_db:mysql
    volumes:
      - ./src:/var/www/html
Suggestions most welcome as Im out of ideas!
",<mysql><wordpress><docker><docker-compose>,1991,2,28,377,2,8,17,71,34865,0,21,8,12,2018-09-05 13:17,2018-09-05 13:59,2018-09-05 13:59,0,0,Basic,3
58735389,Pyspark SQL Pandas Grouped Map without GroupBy?,"I have a dataset that I want to map over using several Pyspark SQL Grouped Map UDFs, at different stages of a larger ETL process that runs on ephemeral clusters in AWS EMR. The Grouped Map API requires that the Pyspark dataframe be grouped prior to the apply, but I have no need to actually group keys.
At the moment, I'm using an arbitrary grouping, which works, but results in:
An unnecessary shuffle.
Hacky code for an arbitrary groupby in each job.
My ideal solution allows a vectorized Pandas UDF apply without an arbitrary grouping, but if I could save the arbitrary grouping that would at least eliminate the shuffles.
EDIT:
Here's what my code looks like. I was originally using an arbitrary grouping, but am currently trying spark_partition_id() based on a comment below by @pault.
@pandas_udf(b_schema, PandasUDFType.GROUPED_MAP)
def transform(a_partition):
  b = a_partition.drop(""pid"", axis=1)
  # Some other transform stuff
  return b
(sql
  .read.parquet(a_path)
  .withColumn(""pid"", spark_partition_id())
  .groupBy(""pid"")
  .apply(transform)
  .write.parquet(b_path))
Using spark_partition_id() seems to still result in a shuffle. I get the following DAG:
Stage 1
Scan parquet
Project
Project
Exchange
Stage 2
Exchange
Sort
FlatMapGroupsInPandas
",<python><pandas><apache-spark><pyspark><apache-spark-sql>,1262,1,14,1666,0,14,28,79,1863,0,162,1,12,2019-11-06 17:16,2020-02-11 15:32,2020-02-11 15:32,97,97,Intermediate,19
61419449,Unable to Instantiate Python Dataclass (Frozen) inside a Pytest function that uses Fixtures,"I'm following along with Architecture Patterns in Python by Harry Percival and Bob Gregory.
Around chapter three (3) they introduce testing the ORM of SQLAlchemy.
A new test that requires a session fixture, it is throwing AttributeError, FrozenInstanceError due to cannot assign to field '_sa_instance_state'
It may be important to note that other tests do not fail when creating instances of OrderLine, but they do fail if I simply include session into the test parameter(s).
Anyway I'll get straight into the code.
conftest.py
@pytest.fixture
def local_db():
    engine = create_engine('sqlite:///:memory:')
    metadata.create_all(engine)
    return engine
@pytest.fixture
def session(local_db):
    start_mappers()
    yield sessionmaker(bind=local_db)()
    clear_mappers()
model.py
@dataclass(frozen=True)
class OrderLine:
    id: str
    sku: str
    quantity: int
test_orm.py
def test_orderline_mapper_can_load_lines(session):
    session.execute(
        'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
        '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
        '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
        '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
    )
    expected = [
        model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
        model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
        model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
    ]
    assert session.query(model.OrderLine).all() == expected
Console error for pipenv run pytest test_orm.py
============================= test session starts =============================
platform linux -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.13.1
rootdir: /home/[redacted]/Documents/architecture-patterns-python
collected 1 item                                                              
test_orm.py F                                                           [100%]
================================== FAILURES ===================================
____________________ test_orderline_mapper_can_load_lines _____________________
session = &lt;sqlalchemy.orm.session.Session object at 0x7fd919ac5bd0&gt;
    def test_orderline_mapper_can_load_lines(session):
        session.execute(
            'INSERT INTO order_lines (order_id, sku, quantity) VALUES '
            '(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),'
            '(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),'
            '(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14)'
        )
        expected = [
&gt;           model.OrderLine(&quot;order1&quot;, &quot;RED-CHAIR&quot;, 12),
            model.OrderLine(&quot;order1&quot;, &quot;RED-TABLE&quot;, 13),
            model.OrderLine(&quot;order2&quot;, &quot;BLUE-LIPSTICK&quot;, 14),
        ]
test_orm.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
&lt;string&gt;:2: in __init__
    ???
../../.local/share/virtualenvs/architecture-patterns-python-Qi2y0bev/lib64/python3.7/site-packages/sqlalchemy/orm/instrumentation.py:377: in _new_state_if_none
    self._state_setter(instance, state)
&lt;string&gt;:1: in set
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = &lt;[AttributeError(&quot;'OrderLine' object has no attribute '_sa_instance_state'&quot;) raised in repr()] OrderLine object at 0x7fd919a8cf50&gt;
name = '_sa_instance_state'
value = &lt;sqlalchemy.orm.state.InstanceState object at 0x7fd9198f7490&gt;
&gt;   ???
E   dataclasses.FrozenInstanceError: cannot assign to field '_sa_instance_state'
&lt;string&gt;:4: FrozenInstanceError
=========================== short test summary info ===========================
FAILED test_orm.py::test_orderline_mapper_can_load_lines - dataclasses.Froze...
============================== 1 failed in 0.06s ==============================
Additional Questions
I understand the overlying logic and what these files are doing, but correct my if my rudimentary understanding is lacking.
conftest.py (used for all pytest config) is setting up a session fixture, which basically sets up a temporary database in memory - using start and clear mappers to ensure that the orm model definitions are binding to the db isntance.
model.py simply a dataclass used to represent an atomic OrderLine object.
test_orm.py class for pytest to supply the session fixture, in order to setup, execute, teardown a db explicitly for the purpose of running tests.
Issue resolution provided by https://github.com/cosmicpython/code/issues/17
",<python><sqlalchemy><pytest><python-dataclasses>,4540,2,89,169,0,0,9,65,2442,0,20,2,12,2020-04-25 0:23,2021-04-04 13:22,2021-04-04 13:22,344,344,Advanced,41
